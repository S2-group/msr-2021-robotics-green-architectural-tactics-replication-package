[
{"title": "Research about the battery in robots", "thread_contents": ["Dear community,", "We are researching on how to make robots programming more efficient and of a better quality. If you are into robot programming, specially drones, please help us by filling this survey. It takes no more than 3 minutes.", "Kind regards", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/research-about-the-battery-in-robots/5566"},
{"title": "Outdoor autonomous delivery robot powered by ROS 2", "thread_contents": ["Hi all,", "We released the last-mile delivery robot. Autonomous driving robots in the outdoors are difficult to test due to various regulations, but we can now go outside the world with permission from the Korean government.", "\u201cUnder the latest regulatory exemption, an autonomous driving robot developed by Korean robot manufacturer ROBOTIS. will be allowed to operate on sidewalks along with pedestrians starting next year. Under the current law, outdoor self-driving robots are categorized as \u201cvehicles\u201d thus cannot run on sidewalks or crosswalks but only on roads. The government, however, allowed an exemption for the delivery robot given the fact that its maximum speed is only 1.5 kilometers per hour, which is too slow to run on roads with other vehicles.\u201d -PulseNews -", "This robot was developed to deliver goods and food along the sidewalk outdoors, with the know-how of the previous development of the TurtleBot 3. The robot uses ", ", so we are happy to share this wonderful news with our ROS community.", "The TV news below contains official permission from the government for outdoor delivery robots, unfortunately in Korean. ", " ", " ", " ", " ", "\ucd95\ud558\ud569\ub2c8\ub2e4! that it awesome.", "Congratulations release outdoor autonomous delivery and certified robot sandbox. Brilliant job being made!", "Great job! I hope to see it soon in the street ", "Congratulations, Great Job!!!", "\nif i may, whichi ROS2 version is used for? dashing?", "Hi, Tomoya", "\nWe have used various ROS 2 versions. We use the latest version for development and testing, but we use the LTS version for our formal services. ", "got it, thanks for the quick response!", "Do y\u2019all have any videos of the product out in the field? I would love to see some candid shots of how you put it all together.", " We are preparing a video of the robot\u2019s overall concept and operation. We will share it with the ROS community when it is released.", "Hi Pedro", "\nCan you please also share presentation with me.", "I will be indeed delighted.", " attached the news article as PDF in case someone can\u2019t reach the URL", "\n", "\n", " (908.3 KB)", "\nthis is so cool\ud83d\ude01 ", "\n", " thank you ,i can browse the url now", "I share a new related video. Um \u2026 It\u2019s in Korean because it\u2019s a local broadcast in Korea.", "\nPlease understand. ", "\n", "Can I get my coffee delivered next time I\u2019m in Seoul at Samsung HQ? ", "Thanks for sharing the video. The sound effects made this fun to watch (even if I only understood about 0.1% of the Korean).", "Great job! That little robot looks lovely ", "Nice work!", "Can I ask if you are using the new ROS2 navigation stack?", "Great! could i ask what ROS version is used on this?", " Thanks, since our driving environment is mostly outdoor, we have developed and used a computer vision processing program for autonomous driving.", ", We are using ROS 2 Dashing and preparing for the next LTS version.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/outdoor-autonomous-delivery-robot-powered-by-ros-2/12024"},
{"title": "Gapter: Research and Education Drone with ROS", "thread_contents": ["Gaitech is proud to announce the release of its educational drone: Gapter, a shortcut for Gaitech Copter.", "Gaitech is proud to announce the release of its educational drone: Gapter, a shortcut for Gaitech Copter.", "\nGapter is an unprecendanted drone with was carfeully designed to meet the requirements of researchers, students and teachers as an educational and research aerial platform.", "\nIt supports Robot Operating System (ROS), the MAVLink protocol, and the mavros wrapper layer between ROS and MAVLink.", "Gapter EDU support Robot Operating System. ROS Users can easily develop applications with Gapter using ROS. In addition, an educational ROS package with several demos was developed for Gapter.", "Furthermore, Gapter Supports Arduilot and the MAVLink Protocol. It can be easily controlled using MAVLink ground stations such as QGroundControl and Mission Planner and to define autonomous missions either through Internet or Telemetry devices.", "Gapter can also be connected to Internet using 3G or 4G connection. This allows you to control anywhere and anytime through the Internet.", "From hardware perspective, Gapter is based on the Pixhawk autopilot and reenforced with an embedded computer Odroid XU4, which is proven to be more powerful than other single board computers, like Raspberry PI, typically used in other drones.", "For computer vision and obstacle avoidance application, Gapter comes with a 3D vision sensor with two stereo camera and to a laser range finder used to avoid obstacles and navigate more safely.", "This is in addition to  a comprehensive documentation and software packages that allows a user to easily start working and developing with Gapter.", "Gapter is an innovative platform for research and education.", "\nFor more information, please contact us at ", "Where can I buy this kit?", "please contact gapter at ", " and we will follow up with your request.", "\nAlso, please see our offer here", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gapter-research-and-education-drone-with-ros/1663"},
{"title": "ROS-Industrial Community Meeting September 24th!", "thread_contents": ["Hello all! We would like to extend an invitation to those interested to attend a web-based ROS-I Community meeting on Tuesday, September 24, at 9AM Central Time US. This meeting will seek to convey a number of interesting developments, both within the Consortium-leading organizations, and two guest presenters from member organizations.", "Intel\u2019s Matt Hansen will share the latest with Navigation2, which he recently shared a ", " of Navigation2 working on hardware. Next up we will have the ", " project ", ", represented by PPM AS, present their latest work brining a ROS-based framework for planning, monitoring, and control of multi-pass robotic applications.", "Registration is required, and spots are limited. We will also be recording the meeting, so should you not be able to attend, the video of the meeting will be available. Meeting participation info is below.", "When: Sep 24, 2019 09:00 AM Central Time (US and Canada) - 90 Minute Duration", "Register in advance for this meeting:", "\n", "Update to members and affiliates on activities relevant to industrial open-source projects/applications. Agenda: Welcome, ROS-I Americas Update (Program/Tech), ROS-I EU and AP Updates, Intel's Matt Hansen on ROS2 Navigation, and PPM Robotics on...", "\n", "After registering, you will receive a confirmation email containing information about joining the meeting.", "Spots are limited, so please register sooner rather than later!", "Matt R.", "\nROS-I Consortium Americas Program Manager", "Hello All! For those that attended, please feel free to take this survey! We hope to gather your feedback to continue to make these meetings as valuable as possible!", "Thanks again!", "Take this survey powered by surveymonkey.com. Create your own surveys for free.", "Matt Robinson", "And\u2026 we now have a playlist over at the ROS-I YouTube channel. So if you missed the meeting or want to rewatch any of the talks, you can find them here: ", "Thanks again to the speakers and the attendees!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-industrial-community-meeting-september-24th/10738"},
{"title": "Autonomous Selfie Drone by Crazyflie using Deep Learning Models", "thread_contents": ["Post retracted at the request of the author due to business-related sensitive information contained in this post.", "The information is expected to be re-announced with more information in the future with more details.", "Hi, nice project ", "Is all the processing done on a jetson tk1?", "We implement this on a Linux box with more power and then port it for Jetson TX2.", "Running on Tk1 might be possible if we rewrite some code for arm 32bit and low gpu power.", "(Also, we plan to optimize this for embedded board without gpus. )", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/autonomous-selfie-drone-by-crazyflie-using-deep-learning-models/2414"},
{"title": "PICA: Drone Made to Learn ROS with MCU and FPGA", "thread_contents": ["If you are a new learner of ROS and want to own your R&D drone? Do you want to learn ROS along with MCU and FPGA? PICA would be the choice for you to experience science, research and education all together on one plate form. PICA is specially designed for Research and development and it would give you a thorough insight of ROS. In a very short period of time you can be skillful in the field of ROS Robotics. PICA is a highly capable platform for Unmanned Aerial Vehicles. PICA is packed with serious processing and computing power, deploying an advanced FPGA, MCU and on-board computation module. The Education Package would give you full access to a learning environment. It has tutorials and instructions inside, and could help you to leap a quick start towards robo-science.", "Gaitech Robotics have also a perk for the customers, to support the consumer, there are learning courses available, which includes not only the hardware training but also software drills will be available.", "PICA is now available on ", " !", "Hello,", "Nice to see a new drone open source! Is your autopilot new or based on PX4 or ArduPilot ?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pica-drone-made-to-learn-ros-with-mcu-and-fpga/2542"},
{"title": "Regional Robotics Sailing Winter Academy at Zhejiang University", "thread_contents": ["Sailing robots are autonomous surface vehicles (ASV) that use wind power as the only propulsion source. The robot can sailing on the sea on a much longer duration compared to the battery-powered counterpart and potentially could serve as a platform for the long-term ocean observation. In order to help teams get start with robotics sailing, we are organising a two days winter academy for students and professionals.", "\nThe lecture will be given by Yu Cao (Southampton Sailing Robot Team) and Prof. Chao Xu (Zhejiang University).", "\nDec.15 and 16, 2018 @ Zhejiang University, China // Free of charge", "\nRegistration can be made online via ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Introduction on robotics sailing \u2013 history and the challenges", "The physics on sailing craft", "Hardware and software structure of an autonomous sailing robot", "Software tutorials on the basics of ROS, python and git version control", "Sailing robot path planning and lower level control", "Sailing demonstration given by ZMART team (subject to weather condition)"], "url": "https://discourse.ros.org/t/regional-robotics-sailing-winter-academy-at-zhejiang-university/6980"},
{"title": "Openpose Implementation for Mobile / Embedded", "thread_contents": ["Implementation of Openpose for realtime : Human Pose Estimation", "tf-openpose - Openpose from CMU implemented using Tensorflow with Custom Architecture for fast inference.", "This will run on your laptop or embedded devices (even without gpu).", "It seems to be a good technique to create various application examples.", "[Call for join on opensource project]", "I recently re-implemented CMU\u2019s openpose which is one of the best model to estimate human pose in realtime with powerful 'GPUs\u2019", "\nWith that reimplementation, I have made a couple of changes like tuning network architecture.", "Especially, by using mobilenet\u2019s \u2018Depthwise Separable Convolution\u2019, the model can be improved to run realtime even in an low-power embedded device or laptop without a GPU.", "But I think, there are much room for improvement in speed and accuracy. So I\u2019m finding crews to help this opensource project to research further. Anyone can join via online.", "Anyway you might recommend to extract skeleton points as 3D point coordinates?", "Sorry for the late reply. Actually, I\u2019m working on it although It will take some time\u2026", "I added ROS support on this project as well as other features.", "See ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/openpose-implementation-for-mobile-embedded/3031"},
{"title": "Proposal - New Computer Vision Message Standards", "thread_contents": ["Hello computer vision users,", "Please help us define a new set of computer vision specific ROS messages by reviewing the ", " and providing your feedback, either here or on the repository.", "At OSRF, we are in the process of defining a new standard set of ROS messages for the computer vision community, and we\u2019d like your help. This need was identified from our computer vision ", " as a first step towards improving the ROS computer vision ecosystem, so thank you for the feedback!", "The end result of this effort may be a new message package in ", ", a REP, or both. Our goal is to capture as many common computer vision use cases as possible, with the exception of navigation. (We feel that navigation and localization are already well-defined by the community and ", ".) Object recognition and image classification are two primary targets we are hoping to hit, and we want to cover both 2D and 3D use cases.", "The ", " we have created is very much a work in progress, and only with your feedback can we make it better. Any feedback is welcome, bu here are a couple of questions I have identified:", "Thanks!", "Nice effort! What about also define interfaces for annotations?", "We also try to define \u201cgeneric\u201d interfaces:", "image_recognition - Packages for image recognition - Robocup TU/e Robotics", "However, it\u2019s hard to capture things as:", "I think you would also like to define something as detection or feature groups that belong together. Maybe take a look at how ", " does it.", "Currently, what is the difference in roles between the two poses in ", " vs ", " nested inside it?:", "As in, what is the relationship that would afford the use the nested ", " to convey the detection\u2019s frame_id?", "I suppose this is a larger question of semantics or dichotomy, but perhap I\u2019m of the thought that classifications derive from detections, such as ROI\u2019s, as opposed to viersa. In whichever case, I think the relationship should be made clarified if we are starting to nest standard message types.", "To just throw this out here, I\u2019ve been using SPENCER recently, and I\u2019m beginning to really appreciate the message type layout they\u2019ve used. Perhaps we could take some hints from the project:", "\n", "spencer_people_tracking - Multi-modal ROS-based people detection and tracking framework for mobile robots developed within the context of the EU FP7 project SPENCER.", "\n", "Hi,", "\nwhen talking computer vision message standards one thing comes to mind, features.", "\nIt is not unusual to have different nodes exploiting the same kind of features (think SIFT/SURF etc), so that rather than extracting several time the same features, a single \u2018extraction\u2019 node does the job and publishes them. They are in turns exploited by (several?) others. A standard message may not be straightforward and I\u2019m not sure this problem fits the scope of this proposal, however it certainly would be useful.", "Cheers.", "I think features should be regarded as an implementation detail of the classifier/detector and should therefore specifically not exposed in these messages.", "I really like this proposal, there should be a standard for this in my opinion.", "In detail, what I like more about Reinzor\u2019s message definitions, is that there is a shared ", ", which takes the role of the ids+scores combination in ", "I think using the CategorialDistribution reduces a little bit on complexity on the user end, but the difference is very small. Another benefit is that the message definition is reusable.", "This topic\u2019s header is \u2018Computer Vision Message Standards\u2019 however the discussion seems to focus on classification. Did I misunderstood the point or is the title not appropriated ?", "It\u2019s not clear to me if the proposal supports per-pixel segmentations. There is the ", " field of ", " and ", "  which might be used for segmentations, but from the documentation I\u2019m not entirely sure if it is also meant for segmentations or not. The name ", " is a bit confusing to me.", "There might also be other types of detection besides bounding boxes and segmentation that I\u2019m not currently thinking of, though the two seem like a pretty solid base for now.", "Why XML? Why not YAML or JSON, or completely implementation defined? Or what about the name of a tree of parameters on the ROS parameter server?", "It indeed says computer vision and not classification/detection only. My bad.", "\nWhat kind of messages would be needed for tasks beyond those 2?", "No problem ", ", it\u2019s just that I have a specific use case in mind. It is as follows:", "\nLocal features (a point and a descriptor -> SIFT/SURF etc) are one of the basic component of CP and is used for geometry algos as well as for appearance-based algos. In feature-based Visual-SLAM (e.g. ORB-SLAM) you rely on feature both for the poses estimation (geometry) and place recognition (appearance). Those two tasks can be executed in parallel threads. Assuming you are using the same features for both tasks one could communicate a ", " (or such) to the other.", "\nIt is something (a ", ") I have been hackily doing here and there, feeding different classifiers - different processes for that matter.", "\nI am just wondering here if a standardized way of moving such objects around would not make sense ?", "ps : To be fair local features are also used from other sensor readings (e.g. laser scan, point cloud) so my question my be a little out of the scope of this thread.", "Thanks for the awesome feedback, everyone! I\u2019ll try to address everything that was brought up.", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general ", " package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific ", " or similar.", ", thanks for linking to your message definitions! I think that annotations are already covered under the existing implementation. You could provide the bounding box coordinates in a ", " message, and the most likely label as the only result in the class probabilities. If we want to add other information, such as color of the outline, etc. then maybe this would be a better fit for ", " or another package.", "On another note, is human pose estimation standardized enough to make a custom message type for it? Or is it best described by a TF tree, arbitrary set of ", ", or some other existing ROS construct? I\u2019m thinking of the fact that different human detectors provide different levels of fidelity, so it might be difficult to standardize.", ", My idea with having two poses is that the bounding box could actually have a different pose from the expressed object pose. For example, the bounding box center for a coffee mug might have some z-height and be off-center wrt the body of the mug, but the expressed object pose might be centered on the cylindrical portion of the mug and be at the bottom. However, maybe it makes sense to forego the bounding box information, as this could be stored in the object metadata, along with a mesh, etc.", "On the topic of nesting, I\u2019m open to the idea of flattening the hierarchy and having Classification/Detection 2D/3D all include a  new ", " message. I\u2019m not sure how much message nesting is considered standard practice, so I\u2019ll look at some other packages to get an idea.", ", I like the idea to add a standardized ", " or other similar message, as long as there is some common baseline that can cover a lot of feature types. From my own understanding of visual features, there\u2019s usually a lot of variation in how the feature is actually defined and represented, so I\u2019m not able to find a \u201clowest common denominator\u201d from my own experience. If you feel there\u2019s something there that could be broadly useful, please feel free to post it here or make a pull request. I agree with ", " as well, although many classifiers use features internally, this should be hidden in the implementation except in special cases like the SLAM case described.", "I didn\u2019t design the current messages to support per-pixel segmentation, and I\u2019ll have to look into how that is usually represented to get a good idea of how to craft a message for it. My initial guess is that it will be a separate message type from ", " and ", ".", "On the topic of the parameter server, I think it\u2019s worth having a discussion about representation format. From talks with other OSRF folks, I don\u2019t think it\u2019s a good idea to use a tree of parameters; a single parameter would be better. For example, if you are loading the ImageNet class names, that\u2019s 1000 items on the parameter server, just to store the names. Add object meshes, sizes, etc., and it could balloon very quickly.", "While JSON/XML/YAML might work equally well in terms of expressive power, with XML, we can be sure that both C++ and Python will have the ability to read the database. TinyXML is already included as a low-level dependency in ROS C++, but the same can\u2019t be said for a YAML or JSON parser. Rather than allow people to use whatever\u2019s convenient, I think it\u2019s worth it to restrict/recommend everyone to use a format that can be parsed from more languages. We could do it in the REP, but not enforce it, so if someone really wants to use YAML in their Python-only implementation, they could do so. That\u2019s my position, but I\u2019m interested in hearing other ideas.", "I\u2019ve updated the repository with changes as discussed above \u2013 ", " and flattened the message hierarchy accordingly.", "On the topic of dense pixel segmentation, is there a reason that ", " is inadequate?", "In regard to pixel labeling, I\u2019ve also seen ", " used as a means to publish, along with some custom structure to define the mapping between pixel values and labels on a separate topic. It would be cool to also have a message type to publish a array of convex bounding polygon verticies with label IDs. That\u2019s\u200b a common use case when labeling regions of an image, and would be good compressed representation to transmit instead for classification modalities that utilise that format.", "For pixel-based segmentation, I imagine a message PixelSegmentation.msg like", "where the pixel value of each pixel in the mask corresponds to an index in the results-array.", "This looks like it would be a clean implementation. Just to be sure (since I\u2019m a segmentation newbie), the size of ", " would be the number of pixels in the ", "? There\u2019s a distribution for each pixel?", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general vision_msgs package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific classification_msgs or similar.", "I am not an expert in this field, but I would like to clarify whether the classification and detection messages are specific to 2D image processing, or if they can also be used for 3D point cloud processing or even 2D laser scan processing. If there is a possibility that they may be used outside of image processing, then perhaps a ", " or similar package actually is appropriate. Just something I think should be considered.", "A CategoricalDistribution for every ", ". E.g pixel [45, 89] has value 21. That means it\u2019s labeling can be found at", "That CategoricalDistribution e.g. determines that pixel is either a pear or banana with corresponding values in the distribution.", "Pixel [45, 90] also has value 21, referring to the same CategoricalDistribution, though pixel [56,94] has value 2 so refers to results[2], which says that pixel is most likely an apple etc.", "The max value of the image +1 corresponds to the length of the .results-array.", "Is it likely that many pixels in the image will have identical distributions? It seems that \u201capple\u201d pixels near the edge of the apple would have a different probability distribution than those near the center. All the ML-based segmentation systems I\u2019ve seen either predict a single output class for a pixel (such as a binary classifier), or they produce probability vector", "It seems like a bit of a halfway solution to define a small set of distributions that the image uses as an index, then transmit that set with every result. I feel that these two options would work based on use case:", "The image is segmented in some small finite set of output classes, which do not have probability distributions that vary in space/time: use an Image message where the lookup value of the pixel is the output class. If desired, static probability distributions for each class can be communicated in a one-time fashion, such as via a single CategoryDistribution[] message, or via the parameter server", "The output segmentation includes varying probability distributions that are calculated per-pixel or per-small region: use a CategoryDistribution of length the size of the image, where each pixel has its own unique distribution that may change every frame.", "Let me know if I missed something! If you have some code available for a use case, that\u2019s really helpful. I\u2019m currently in the process of writing example classifiers to use the Classification/Detection messages and finding it a useful exercise.", "3D point cloud processing generally falls under the topic of \u201ccomputer vision.\u201d But I had not considered laser scan processing, good point. The package name will probably be subject to review from more senior OSRF architects, and we\u2019ll keep that in mind!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Are there major use cases or edge cases not covered by this set of messages?", "Is this set of messages broad enough to encompass both handcrafted and machine learning-based approaches to computer vision?", "\n", ": as a result from ", "\n", "Poses e.g. ", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/proposal-new-computer-vision-message-standards/1819"},
{"title": "Hardware Accelerated Depth Camera Simulation", "thread_contents": ["Hello,", "I\u2019ve made a depth camera simulation that utilizes OpenGL for hardware acceleration, but otherwise has fairly minimal dependencies. It is most useful when you want to quickly simulate depth scans but don\u2019t really need the full blown power of Gazebo.", "I welcome feedback and contributions.", "\n", "I wonder if any others users have need of a headless (or offscreen) rendering tools. Maybe machine learning applications?", "Hi Jonathan,", "Some years ago I developed a very similar library ", ", in the context of my PhD thesis. The library also used OpenGL for fast computation of virtual laser scanners on a 3D environment. These virtual scans were used as \u201cexpected measurements\u201d and were compared with actual measurements for a particle filter map-based localization ", ". The tool worked pretty well, and very fast, allowing to update hundreds of particles at rates of 5-10Hz.", "I also developed the version for depth cameras, which I wanted to use later for camera-gripper calibration for robot manipulator applications, see picture below:", "\n", "The main feature of this package, specially designed for 2D lidar simulation, was that the rendering window, which can be obviously hidden, was set with a size according to the resolution of the sensor you want to simulate, so no extra pixels are rendered. This resulted with very small windows to simulate lidars, since they typically have lower angular resolutions than cameras. Small rendering windows lead to fast renderings, so fast depth computations.", "Just wanted to post here, so we can be in touch, and to point out two potential applications of this approach (mobile robot map-based localization and camera-hand calibration)", "If I have some time I\u2019ll try to dive a little bit in your code. Feel free to contact me for further discussion.", "best", "Andreu", "Math details about the library can be found in the SIMPAR\u20192010 publication:", "\nCorominas Murtra, A., Trulls, E., Mirats Tur, J.M., Sanfeliu, A. ", ". Lecture Notes in Artificial Intelligence -LNAI6472. International Conference on Simulation, Modelling and Programming for Autonomous Robots (SIMPAR\u201910). Darmstadt, Germany. November 2010.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/hardware-accelerated-depth-camera-simulation/4817"},
{"title": "Path planning with external axis", "thread_contents": ["We\u2019ve been using ROS for additive manufacturing quite a lot in the last months (see ", ").", "We are now interested in working on revolution parts; the big difference is that we want to use a 2 axis positioner and combine/synchronise the movement of the position and the robot.", "A typical example would be:", "\n", "I want to build the green prism on the grey cylinder, the grey cylinder is attached to the positionner and can be turned around it\u2019s axis (X / red axis on the image).", "What does it take to create a trajectory to build this part accounting that:", "I\u2019m not sure about how to do that, because the part can be moving.", "\nIn other words, I\u2019m not sure if the path planning algorithm has to decide which orientation should the positioner take or if MoveIt has to determine the orientation of the positioner depending on the trajectory fed.", "Any direction, idea, pointer would be appreciated!", "Interesting question, and something we\u2019ve been starting to look at here as well (not too actively yet though).", "We\u2019re not welding, but winding. Same difference, although a winding motion is potentially infinite.", "Has anybody done a similar thing with ROS on industrial robots?", "\nI would like to estimate how much time I would need to test this and get things running.", "I\u2019m currently working actively on a very similar problem, but not within ROS-I at the moment.", "\nMy goal is to plan trajectories for 6 or 7 DOF welding robots, mounted on rails, creating up to 3 additional axes.", "Last year we did some experimenting [1] with the descartes [2] package of ROS-I. Because of my lacking C++ skill\u2019s and there being no interface for highly redundant robots in MoveIt!, I started experimenting from scratch in 2D. [3]", "For your problem, my initial guess (based on the approach I\u2019m trying now) would be:", "Feel free to contact me. I\u2019m open to collaboration, as far as I can contribute something.", "\n(Coincidently, we even have a Kuka robot with a two-axis positioner in our lab [5] ", ", but I never used it.)", "\n", "\n", "\n", "\n", "As stated above, the problem that you are facing is a more general instance of a constrained motion planning problem than what ", " can express. Although not immediately helpful, recently an update has been pushed to OMPL [1] that introduces generic, geometrically constrained motion planning with sampling-based motion planners [2] (you can view a highlight reel of some of the available demos here at [3]). The code is available at [4].", "This new framework allows for geometric constraints to be phrased as functions of a robot\u2019s state, ", ", which are satisfied when ", ". Your problem can be expressed as a geometric constraint on the 8-DoF system of the positioner and the end-effector. It could possibly look something like:", "where ", " is some metric in SE(3). Unfortunately, right now there is no interface through ROS to access these new features (i.e., through ", " or others), but a bespoke solution is possible. It would require creating a state space for your robot and a constraint, and then running your favorite motion planner on the constrained state space (for your case, probably an asymptotically-optimal one, optimizing for minimal end-effector movement).", "Feel free to let me know if you are interested in pursuing this route or if you have any trouble using OMPL.", "[1] ", "\n[2] ", "\n[3] ", "\n[4] ", "Zach, those OMPL additions look very cool. I like the sampling augmented with a little optimization for exploration. It\u2019s not necessarily true that something like Descartes can\u2019t express the problem, however. Like OMPL, at it\u2019s core it\u2019s just a way to build and search graphs. For example, Descartes trying to move a 6DOF arm around the perimeter of a part that can turn: ", "The problem is your sample space will grow out of control very quickly and so will your planning times\u2026 You might just try iterative inverse kinematics (e.g. call set from IK over and over) and see where it gets you. It\u2019s at least quick to try.", "I have lots of ideas but, frustratingly, I can\u2019t really share too much at the moment. Even past motion planning, I\u2019d budget a couple of weeks for debug and testing of your new motion interface.", "Gijs, the problem of infinite windup might be addressable in the Descartes (or other planner) space if you specify that your start and end joint pose are the same. If your process allows the robot to get back then you could just continually duplicate that motion.", "Thanks you for these very nice answers ", " I\u2019m happy to see that this is a topic of intereset, as many industrial robots are used with linear axis and turn tables this is a key topic to drive ROS-I into our factories.", "This really doesn\u2019t look like a feature that would be done in two days work!", "I never used Descartes but I wanted to give it a look since a long time, that\u2019s what I\u2019ll be doing right now.", "I think it would be nice that this discussion continues with the advances of everyone in the field, I\u2019ll try what you have suggested and give it a look.", "At the moment we are not starting a project or anything concrete in that direction, it is something we are studying, the main idea is to gauge the amount of work to get a simple example running, and get 3D printing running with this stuff.", "This would be something interesting to figure out\u2026", "\nIf you plan a path as if you were planning to weld on a 2D plane then the 2D plane (and dimensions) would be the flattened cylindrical surface you\u2019ll be welding on.", "\nThen interpolate the rotation of the positioner with the \u201cplanned\u201d y movement (green?) (involving PI and the radius). So put a gear ratio between the y-position and the positioner, and do not move the welding torch in the y direction. You will then get metal added concentrically with the cylinder.", "Doing something similar with ", " example of the infinite spool winding:", "\ndepending on the winding angle, I\u2019d see if you can interpolate the x position (along the rotarion axis) with the (winded) wire position/length. So that will be the driving (virtual) axis. Then the spool will have to generate a constant torque otherwise the wire will be too \u201cloose\u201d around the spool, or the wire will snap. so for 1 lenth of wire the x-position will be the width of the spool, and the next length the x-position to be reached will be zero. and repeat that.", "\nOr maybe even simpeler, travel back and forth between the spool, forget about the wire position altogether and link the wire speed to the cartesian speed of the end effector.", "I\u2019m happy to see that this is a topic of intereset, as many industrial robots are used with linear axis and turn tables this is a key topic to drive ROS-I into our factories.", "I thought I\u2019d clarify here a bit: I believe ", " axes are supported quite well. I\u2019ve got 3 setups myself that use linear axes, it\u2019s not something that requires any really special configuration or planners (OMPL fi can cope with this just fine fi, and so can Descartes).", "A turntable however is something I\u2019m not sure about, but only because I\u2019ve never configured such a system for use with ROS. Not necessarily because I don\u2019t think it\u2019s possible / supported right now.", ", I\u2019m curious about how we should configure the kinematic system to make ", " work with 6dof + turn table. I feel like this is something different to 6 dof + 1 linear track and IKFast is not supporting this, right?", "Also, I think the youtube link that you shared above is not working\u2026 Would you mind fixing that? I\u2019m looking forward to checking this demo!", "Video ", " wanted to show is available here: ", "Thanks ", " ! Do we have any example code for this? If we can set up a robot model with ikfast configured, I believe ", " won\u2019t have any difficulty working with 6dof + turntable case.", "I would like to try what ", " mentioned above. In other words, I want to use ", " for redundant robots, where the redundant joints are sampled/discretized. I have plenty of idea\u2019s for the sampling part, it is with the actual implementation that I\u2019m stuck. I\u2019ve looked through the source code of MoveIt! and ", " but I\u2019m not sure what the best approach is.", "Approach 1. would make it available to other planners. On the other hand, you could argue that sampling is the task of the motion planner and therefore approach 2. is better. Approach 3. is somewhere in the middle.", "I cannot find a lot of information on the topic using google or looking through Github issues.", "\nCan someone help me with pointers / interesting links / insights / discussion?", "Has someone found a clean solution or made code available for this example?", " and I are working on some demos for ROSCON that will include a slightly retooled interface to Descartes that makes it easier to express problems like this. Give me a few days and I should be able to start posting some reasonably useful stuff.", " , Does this look somewhat representative?", "This is exactly what I meant to do ", " Looks awesome!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The robot has 6 axis", "The part is on a 2 axis positioner (we would be using only 1 axis for the beginning)", "Moving the positioner is prioritised to minimize robot orientations changes", "Create a kinematic chain from the workpiece to the robot tool (therefore ignoring the dynamics).", "Create some kind of wrapper for the inverse kinematics of this chain that can return a sampled set of all possible solutions. The sampled space is very big, but I think it could be manageable for off-line planning. (If the ranges of the positioner are not to big. Like, smaller than 30 degrees or something, sampling at 1 degree intervals, and not to much freedom on the end-effector to keep to total solution space manageable.)", "Add a custom cost function to descrates that not only minimizes joint motion, but also takes into account the priority of avoiding orientation changes. (We already experimented with adding tool orientation to the cost function, but the approach was quite hacky [4].)", "Hope for the best. Descartes seems to be in active development at the moment. In the indigo version, we often encountered a lack of memory. Computation time was never really an issue when using the IKFast solver.", "Add functionality to the MoveIt! IK plugin to return a certain number of possible solutions for a redundant robot. There seems to be a lot of functionality present in moveit_core/kinematics_base to work with redundant joints, but I\u2019ve not figured out how it works yet.", "Let ", " handle the sampling of the redundant joints. Use separate planning groups for the redundant and non-redundant parts of the robot. This approach seems particularly useful for external turntables.", "Use a separate package for the inverse kinematics and add redundant joint sampling there. (For example ", " Only use MoveIt! for collision detection.", "Some combination of the above. or other approaches I did not think of."], "url": "https://discourse.ros.org/t/path-planning-with-external-axis/4451"},
{"title": "LIDAR of Turtleblot3 starts spinning once power on?", "thread_contents": ["Just wondering is it normal that the LIDAR starts spinning when I switch on TB3???", "In my understanding, the LIDAR should start spinning after we roslaunch turtlebot3_bringup turtlebot3_robot.launch\u2026", "Anyone face the same condition?", "Hi,", "Since October, 2017, the LIDAR which comes with the TurtleBot3 gets the firmware that makes automatically run from booting on. It is not a malfunction.", "Look here:", "and hopefully (as Tully always says):", "The TurtleBot category here on ROS Discourse is the right forum for general discussions.", "For questions with direct answers or debugging please use ", " with the tag turtlebot or turtlebot3 with waffle or burger depending on your model.", "There are guidelines for asking questions at ", "Thanks", "Leon", "Thanks Leon for your reply and the reminder of ", " ^^", "hello, I am also having the same problem. have you solved this problem? if you solved please help me to get out of this\u2026", "\nUnfortunately, spinning the LDS with power up is an inherited feature of the sensor firmware itself since 2017.", "ok.", "\nDoes turtlebot3 needs host computer?", "\ncan you help me to do without host computer if possible\u2026", "Generally if you are going to need a machine to ssh into the turtlebot and kick-off a launch file. I would suggest that if you have general turtlebot questions you ask Robotis directly or use ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["#include <hls_lfcd_lds_driver/lfcd_laser.h>", "#include <std_msgs/UInt16.h>", "\n", "namespace hls_lfcd_lds", "{", "LFCDLaser::LFCDLaser(const std::string& port, uint32_t baud_rate, boost::asio::io_service& io)", ": port_(port), baud_rate_(baud_rate), shutting_down_(false), serial_(io, port_)", "{", "serial_.set_option(boost::asio::serial_port_base::baud_rate(baud_rate_));", "\n", "// Below command is not required after firmware upgrade (2017.10)", "boost::asio::write(serial_, boost::asio::buffer(\"b\", 1));  // start motor", "}", "\n", "LFCDLaser::~LFCDLaser()", "{", "boost::asio::write(serial_, boost::asio::buffer(\"e\", 1));  // stop motor", "}", "\n", "void LFCDLaser::poll(sensor_msgs::LaserScan::Ptr scan)", "{"], "url": "https://discourse.ros.org/t/lidar-of-turtleblot3-starts-spinning-once-power-on/3332"},
{"title": "Using NUC with Kobuki", "thread_contents": ["Hi,", "It seems to becoming harder to find netbook size laptops to use with the Kobuki base and so I\u2019m looking into using the Intel NUC (e.g. NUC6i5SYH). I want fully autonomous operation where the Kobuki can dock and charge its batteries and the PC, so I wondered if anyone was aware of a power solution e.g. external battery that can connect to the Kobuki 19v output AND will allow the NUC to get a reading of the battery level e.g. via USB. Alternatively, is it possible to use a dc-dc converter with one of the other Kobuki power outputs (e.g. 12v 5A) to power the NUC?", "Or if anyone knows of a good choice of small laptop to use then that might be preferable.", "Thanks,", "\nLee.", "Hi,", "Have a look at the ", ", it\u2019s a DC-DC converter as well as UPS (6-30V input, 6-24V output). It supports multiple battery chemistries (e.g. Li-Po, SLA) and has an adjustable DC output.", "It has a USB interface, and Linux drivers, supporting ", ".", "I\u2019ve been using this device on my robot, I wrote a very simple Python ROS interface for this, which just publishes the charging state and individual Li-Po cell voltages. I\u2019ll upload this script and link it on this thread when I get the chance (I\u2019m currently away from my robot).", "Alex", "Thanks for the response. After posting I did end up looking at that and it looks like a good solution. Good to know that you have it up and running and that the Linux drivers work etc.", "I\u2019m thinking of using 6 of these batteries:", "\n", "7926", "\n", "\nwhich based on my very quick calculations should drive a NUC for about 6 hours (assuming NUC consumes 15W)?", "I asked the OpenUPS company if they could recommend any enclosures - did you find one that works well?", "Hi,", "we have a NUCi7 on our Turtlebot. We ordered an i5 but got an i7 due to", "\nsome issue while ordering. The i5 had a notebook powerbank with it which", "\nis not powerful enough for an i7 (it lasts about 30 minutes). Thus we", "\nare now testing a custom LiIon battery. It seems to be able to run the", "\ni7 for about 4-5 hours now but we are still working on a battery status", "\nintegration. Additionally, we need about 4-6A for charging the robot", "\nnow. From the internal wiring, this should not be a problem, but the", "\nwall charger only gives us 3.2A. So we also still need to get a powerful", "\nwall charger (right now using adjustable lab power supplies).", "Once the battery runs well, I can send you our setup. The OpenUPS looks", "\nvery nice as well, especially the already integrated battery status and", "\ncharging state. If you deploy it to your Turtlebot, could you give", "\nfeedback about it?", "Best,", "\nLasse", "Hi!", "We are also using Turtlebot (version 1) with an i7 NUC which actually requires 19V / 65 W. So far, we didn\u2019t use any DC/DC converters or something similar, but instead, we powered the NUC directly from the 12V / 5A output port on the Kobuki.", "This is certainly not ideal, and we experienced the NUC shutting down because of a \u201cprocessor thermal trip\u201d after some time (sometimes only 30 mins when the Kobuki battery would still last for long). First we thought it\u2019s a problem with the NUC (old thermal paste or something), but later we realized that the output voltage from the Kobuki is monotonically decreasing and will be too low at some point. This seems to have caused the overheating problems then.", "Either ", " or ", " seem like good solutions for this problem, but I\u2019m not very experienced with DC-DC converters or UPS and I\u2019m glad for any advice.", "Can anyone tell me if this would be a good buy for more reliable mobile NUC operation?", "We had a similar problem and solved it by purchasing a power bank. It charges from the kobuki base, provides 19V to the NUC and does not kill the Kobuki battery. We use one of the 19V connectors on the back of the Kobuki and make an adapter to charge the powerbank when the robot is on the charger.", "This is the one we used: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-nuc-with-kobuki/489"},
{"title": "A do-it-yourself Turtlebot", "thread_contents": ["I am teaching a class to Computer Science students (clever software but not hardware types). I thought it would be a good experience to actually build or own turtlebot-like robot, not literally from a kit, but from a pre-acquired set of parts that I knew would go together. It would give them a more visceral experience of building. Our goal is to do mobile navigation with ROS, so we are also using off-the-shelf Robotis Turtlebot3 but they are very small and a \u201creal\u201d Turtlebot2 costs like $2K. Any links or suggestions from those of you who have traveled the same road?", "(Not sure if this question should\u2019ve been on ", ".)", "Hi pitosalas, while not exactly the same specs as Turtlebot, I am working on a ROS + OpenCV Raspberry Pi based robot kit called ROSbots - ", ".", "It is a differential drive robot that has a camera, wheel encoders, and a motor driver, and off course runs ROS and OpenCV pre-installed.", "Would that fit your needs?", "Jack", "Hi Jack", "Yes that\u2019s along the lines but I am looking for something a bit bigger and with a more powerful main computer. Do you have any other leads? Thanks!", "Pito Salas", "\nBrandeis Computer Science", "\nVolen 134", "If you don\u2019t want to build the TurtleBot2 from a kit, the full designs are available to build it from scratch at: ", " Many people have done that. And while you\u2019re at it you can modify the design for your usecase or application.", "Hi Pito,", "I apologize if you had already thought of this but if you can tolerate the network latency and your robot isn\u2019t too sensitive to slightly \u201clate\u201d sensor data (but still accurately timestamped), you can always have a more powerful computer (ie laptop or desktop) run the more computationally heavy ROS nodes as secondary processors - no need to have these processors actually on the robot (which just sucks up more power and requires heavier duty motors). Just another architectural thought to help your students keep cost down while still being able to build their own full fledged ROS robot.", "Happy to continue to hear your thoughts, and \u201cdiscourse\u201d (harhar!).", "Cheers", "\nJack", "The TB2 \u201cfull designs\u201d look quite intimidating. Did you say that the components are available to buy or is the idea that I literally 3d print the parts as specced out there?", "Hi Jack. Yes I was aware of that option but I also am aiming (maybe mistakenly) for building up the experience and skills to allow us to make a robot that will be able to carry a small load (1kg say) and eventually go outdoors. I know that\u2019s a lot. But that\u2019s why I am looking for something with a base the size of an iRobot create or something else\u2026 Thoughts?", "Hi Pito,", "You can check out ", " . It\u2019s not exactly a Turtlebot but it offers a \u201cblueprint\u201d for building a variety of DIY ROS compatible robots (2WD, 4WD, Mecanum Drive, and Car-Like steering) . The platforms are computer agnostic so you can choose a much capable board.", "Hope this helps. Cheers!", "You can buy individual parts for the Turtlebot 2 from Dabit Industries for around $800/kit plus the cost of a computer", "For a cheaper cost, you can grab a iRobot Create2 base ($200), iRobot Create2 Mounting Kit ($125), Orbbec Astra ($150), and some computer.", "\n", "**The Kinect and iRobot Create2 are NOT included in this kit \u00a0 This is a kit meant to screw directly into the iRobot Create2 Boss locations, and provide an adaptable and scalable base for all sorts of fun projects! \u00a0 Included in this kit: 1 x Middle...", "\n    ", "\n", "As for computers:", "For SBCs, you can take a look at:", "\nRaspberry Pi3: ", "\nASUS Tinker Board: ", "\nODROID-C2: ", "As for laptops, you can check out the following recommended Lenovo laptops:", "\nrefurbished x240 at $419:", "\n", "\n  \n  \n  \n  \n  ", "\n", "\n", "\nLenovo 11e at $369: ", "Hi Pito, you got some great suggestions from the community. Last but not least, while not exactly a kit but definitely a platform you can build on (capable of huge payload) - check out the Magni platform from the awesome folks at Ubiquity Robotics -", "\n", "Jack", "Besides the Ubiquity platform, you can download both their RPi sd card or Virtual Box images. The small Loki platform may become available this Spring if crowd funding is successful.  Also check out the SV-ROS github for how to convert a Neato Botvac to a Turtlebot like platform. I published two articles in Servo Magazine entitled Roll your own Turtlebot. So I think it is easiest to go with an existing platform. Putting together a platform from scratch, with motors, encoders and controllers from scratch just takes too long.", "Some of the autonomous racing folks are building around a rc car platform, but I don\u2019t think that\u2019s good for education. I believe Cousera used the Neato in their course on intro to autonomous robots.", "Disclaimer, I coordinate SV-ROS, and am a member of Ubiquity Robotics", "Good luck. The only way to get better at robotics is to build better robots.", " - do you happen to have pdfs of your two articles? Reading them online in servo is hard ", " Thanks!", "I can send you the botvac article plus youtube videos of  interest.  I have to look for the original Roll Your Own Turtlebot article. (about 3 laptops ago!)   Sending raw odt:", "Roll Your Own Turtlebot Part II", "By Alan N. Federman (Dr. Bot)", "Several years ago I wrote a \u201cServo\u201d article showing how you could convert an old Roomba vacuum cleaner and a Microsoft Kinect into a robot training platform capable of teaching yourself  ROS, the Robot Operating System from Willow Garage.  Willow Garage has now closed, and Clearpath Robotics can sell you a complete brand new Turtlebot for about $2100. Recently OSRF and Robotis has announced a less expensive Turtlebot, but this is still a bit out of the range for most serious amateurs  What if you could easily build the equivalent of a Turtlebot for under $300?  I am going to show you how easy it is to convert a Neato robotic vacuum cleaner into a fully functional training platform in less than a day.  Very little hardware skill or special tools are needed. Everything is available COTS, and the software is all Open Source.", "What you need to get, if you don\u2019t already have them:", "A laptop or wifi connected desktop running Unbuntu. This should be at least 14.04 and running ROS Indigo, but it would be better to upgrade to the same versions if running cross platform. ROS versions are usually matched to Ubuntu releases.", "\nA Neato Botvac or equivalent (I have seen used XV-12s for under $200, and new basic models for under $300)", "\nA Raspberry Pi 3 (camera is optional but highly recommended) ~$50", "\n16 Gig Sd Card for Pi ~$10", "\nRechargeable 5v power pack (Those for recharging cell phone are fine) ~$20", "\nUSB cables for battery pack to Pi (micro) and Pi to Botvac (can be mini or micro depending)", "\nSmall scraps of aluminum or a tin from can.", "\nSmall scraps of flat plywood or acrylic", "\nVelcro, double sided tape or other easy to remove adhesives", "Step 1 Modifying the Botvac", "Depending on your model, you may chose to ignore any hardware modifications entirely. Then if you mess up, you can just use it to cleanup your house!  I removed the brushes, the dust bin and used a stip of metal to disable the bin detector switch (See Photo 1)", "STEP 2 Preparing the Pi and attaching to Botvac", "Artfully arrange the Pi, battery pack and optional camera on a 6\u201d by 6\u201d flat piece of wood or plastic. Attach with double sided tape. On the bottom of the assembly, attach a piece of Velcro or similar quick release fastener. Attach the matching Velcro to the top of the Botvacs Lidar unit. Lastly plug in the USB cables.  You might want to charge your batteries.  It would be a shame to have all the software loaded and than have to wait to test it.", "STEP 3 Loading the software onto the PI.", "At the time of this writing, an official version of Ubuntu 16.04 was not available fro the Pi 3. I used the Ubuntu Mate (pronounced \u201cma tay\u201d) version. Instructions for loading Mate are found here:", "Download a copy of Ubuntu MATE", "And follow the instructions for 16.04 \u2013 Raspberry PI 2/3.", "You can use an HDMI TV and attached keyboard to initially set up the Pi, using the Graphical Environment.  It also helps to have a direct Ethernet connection when doing the initial set up, because you need to load a lot of software initially. Using the Desktop, it is pretty easy to get WiFi working.", "I suggest creating an 8 gig image on a 16gig SD card. After the initial software is on and you can bring up a graphical desktop, follow the intro screen and click on Raspberry PI info \u2013 it will enable you to expand the image to 16gig. It also will allow you to configure your WiFi.  I suggest you still use the Ethernet Connection, but you can at this point open a terminal, type sudo graphical disable, and then use ssh over WiFi to complete the installation.", "Once Ubuntu is working, continue loading ROS onto the PI,", "(", "\nYou may have to maintain your Ubuntu distributions; the following commands are useful:", "sudo apt-get update", "\nsudo apt-get upgrade  (must run both in sequence)", "and sometimes to clear dpkg errors:", "sudo dpkg \u2013configure -a", ")", "Summary:", "sudo sh -c \u2018echo \u201cdeb ", " $(lsb_release -sc) main\u201d > /etc/apt/sources.list.d/ros-latest.list\u2019", "sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116", "sudo apt-get update", "\nsudo apt-get install ros-kinetic-desktop-full", "(if -full is not available, just get ros-kinetic-desktop)", "sudo rosdep init", "\nrosdep update", "\necho \u201csource /opt/ros/kinetic/setup.bash\u201d >> ~/.bashrc", "\nsource ~/.bashrc", "\nsudo apt-get install python-rosinstall", "ROS Catkin Workspace installation", "mkdir -p ~/catkin_ws/src", "\ncd ~/catkin_ws/src", "\ncatkin_init_workspace", "\ncd \u2026", "\ncatkin_make", "And then edit .bashrc to change the source /opt/ros/kinetic/setup.bash to ~/catkin_ws/devel/setup.bash", "\nalso it helps to add the following line if the Pi is hosting the robot:", "export ROS_MASTER_URI=http://$HOSTNAME.local:11311", "Where \u201c$HOSTNAME\u201d is the name you have in the /etc/hostname file", "Your /etc/hosts file should look like this:", "127.0.0.1\tlocalhost", "\n127.0.1.1\t\u201cyour hostname\u201d", "::1     ip6-localhost ip6-loopback", "\nfe00::0 ip6-localnet", "\nff00::0 ip6-mcastprefix", "\nff02::1 ip6-allnodes", "\nff02::2 ip6-allrouters", "This will support ROS networking.", "You also may wish to install \u201cchrony\u201d to synchronize the time different \u2018nodes\u2019 are running at, this is because the Pi doesn\u2019t have a real time clock, and if your Wifi is not connected to the Internet, the Pi will have the wrong time.", "Next I suggest you load the following ROS packages into your catkin_ws/src workspace:", "ROS by Example part one (RBX1) from Patrick Goebel", "     This should go on both your laptop and the PI", "and", "the SV-ROS Botvac nodes courtesy of mostly Mr.  Ralph Gnauck", "Repository of packages and info for the SV-ROS Intro To ROS training series - SV-ROS/intro_to_ros", "follow the instructions in the README files to install and test.", "Example", "cd ~/catkin_ws/src", "git clone ", "  (this also should go on both)", "cd \u2026", "catkin_make", "TESTING", "On the laptop:", "roscd teleop", "If nothing is found,", "sudo apt-get install ros-kinetic-teleop-twist-keyboard", "On the Botvac, turn on the pi, and sign on via a terminal window from the laptop.", "I like to launch a custom base only node on the Pi", "roslaunch bv80bot_node bv80bot_njoy.launch (code included at the end of this article)", "Then you should hear the Neato Lidar unit start to spin.", "On the Laptop open up another terminal window and", "set up the ROS_IP  and ROS_MASTER_URI environment variables via the \u2018export\u2019 command.", "Test to see if you are getting topics:", "rostopic list", "and scans", "rostopic echo /scan", "Finally launch teleop", "rosrun teleop_twist_keyboard teleop_twist_keyboard.py", "You should be able to drive your robot.", "If you open RVIZ in another window, you should see the LIDAR returns.", "What Next?", "With just this simple robot, you can begin to learn how to accomplish advanced robotics tasks and begin to learn the subtleties of autonomous navigation. Because of the Neato\u2019s XV-11 LIDAR unit, you can simultaneously accomplish localization and obstacle avoidance. Support for webcams and the Raspberry Pi Camera are available through ROS nodes. I have gotten teleop via a blue tooth joystick to work through the laptop, but not directly on the Rpi.  Please note that though the Rpi has 4 USB slots, there is seldom enough power to run more than the Botvac interface and a WiFi dongle. A typical USB webcam will draw too much current and crash the Rpi.", "With a WiFi connected phone and some ingenuity, you would be able to issue voice commands. So you can call up your home robot from the office and ask it to find the cat. The Botvac is a little underpowered for bringing you a snack from the kitchen, but when the next more powerful platform is available, you\u2019ll know just how to program it.", "Figures and Code", "LISTING 1  /catkin_ws/src/intro_to_ros/bv80bot/bv80bot_node/launch/include/bv80bot_njoy.launch", "LISTING 2  Terminal output from launching startup nodes:", "roslaunch bv80bot_node bv80bot_njoy.launch &", "rostopic list", "/button", "\n/cmd_vel", "\n/cmd_vel_mux/active", "\n/cmd_vel_mux/parameter_descriptions", "\n/cmd_vel_mux/parameter_updates", "\n/joint_states", "\n/mobile_base_nodelet_manager/bond", "\n/odom", "\n/raw_cmd_vel", "\n/robot_cmd_vel", "\n/rosout", "\n/rosout_agg", "\n/scan", "\n/sensor", "\n/smoothed_cmd_vel", "\n/teleop_velocity_smoother/parameter_descriptions", "\n/teleop_velocity_smoother/parameter_updates", "\n/tf", "\n/tf_static", "Figures:", "   Fig 1  XV-12 dustbin removed", "  Fig 2  XV-12 Name Plate", "    Fig 3  XV-12  Brush Removed", "   Fig 4 RPi 3 mounted", "Youtubes of Turtlebot I from create base and voice control ~2013-2014? :", "Other Youtubes of interest:", "   Botvac 2015", "  Magni 2016", "  Loki 2015", "Very interesting and thank for your information. I am interested in building a robot that can carry a larger weight (>100kg). But I am lost when Raspberry Pi control the motor because, in comparison with Turtlebot, this robot will have a more powerfull motor. In your article it does not appear any other MCU, so my question is how is the motor controled?", "\nThank you.", "The Botvac has its own microprocessor, motor drivers, etc. The Pi connects via a USB cable. There is no interface required. This was not the case on the original Turtlebot 1 which required a USB to serial TTL converter.", "I am going to make your project and i am a beginner in ROS can you provide more details to build this project", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-do-it-yourself-turtlebot/3978"},
{"title": "Next ROS Quality Assurance Working Group Meeting for Oct. 2019", "thread_contents": ["Dear ROS community,", "This next ROS QA working group will take place on  ", ". The agenda below:", ":", "\n==============Conference Bridge Information==============", "You have been invited to an online meeting, powered by Amazon Chime.", "=================================================", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Update on the QA Dashboard initiative", "Update on Code Review for ROS2 pilot for these repositories:\n", "ros_comm", "ros_control", "navigation", "ROS2 rcl_cpp", "ROS2 actionlib", "\n", "Discuss next quality initiative"], "url": "https://discourse.ros.org/t/next-ros-quality-assurance-working-group-meeting-for-oct-2019/10806"},
{"title": "Upgrading Turtlebot3 with better LIDAR", "thread_contents": ["Hello all,", "LDS-01 LIDAR that comes with turtlebot3 is good for small indoor places. But when I try to use it autonomously in larger indoor places it starts to behave unreliable.", "\nSeems like LDS-01 range is not good enough. My plan is to upgrade my Turtlebot with better LIDAR.", "What LIDAR would you suggest me for price up to $350?", "consider adding video.  Landmark navigation works well for longer distances and outdoor environments.  After all this is what humans do when driving a car.  They look out the window and see that big green building and know that is where to make the left hand turn.     I think vision is best for a certain scale and lidar best for getting through a doorway without hitting the jambs.", "There is another class of vision-based navigation that is different, this is where they convert a stereo 3D image to simulated LIDAR data.   I am ", " talking about that.   I mean that you recognize the landmarks and enter them into a list.   Then when the landmark is re-recognized the list is consulted.    This can be robust if multiple landmarks are in sight and location is determined by triangulation.", "The same camera data can be used by other algorithms for \u201cvisual odometry\u201d possibly using optical flow", "Then as the robot approaches a wall or door the lidar data becomes useful again.", "That said, a lidar upgrad is conceptually simpler", "Hi Chris,", "\nthanks for replay ", "I also plan to use Intel R200 in combination with LIDAR. But I plan to convert its 3D data into 2D laser scan.  With LIDAR I also have issues when robot cannot recognize obstacles that are higher than LIDAR can detect.", "\nBy adding R200 I hope it will detect these obstacles more reliably.", "\nDo you have more information about using 3D camera as you described above?", "I still think that my LDS-01 is bottleneck of my robot. ", " looks like very good LIDAR for its price.", "\nRange is 3.5x better, sample rate is 4.5x better and accuracy also looks better.", "\nWhat do you think, is there any better LIDAR with similar price?", "Yes, that is a common \u201ctrick\u201d, you convert 3D camera data so that it looks like LIDAR data then you don\u2019t have to change the SLAM software.   But the ONLY advantage is not having to change the SLAM software.", "There is another use of the vision camera where the most simple explanation goes like this:   The robot rolls into a kitchen and vision based software recognizes the front of a stove.  Then the SLAM system says \u201cin am in front of the stove\u201d.   There is no point cloud in this method.", "If you clapse the 3D image to 2D then you are also tosing out most of the information", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/upgrading-turtlebot3-with-better-lidar/6603"},
{"title": "Guidelines on how to architect ROS-based systems", "thread_contents": ["Hi all,", "\nin collaboration with researchers of the Carnegie Mellon University and the Vrije Universiteit Amsterdam, we defined a set of 49 guidelines about how to architect ROS-based systems. You can find the guidelines and the steps we performed for extracting them ", ".", "Since we are just starting to do this kind of research on ROS, any possible feedback is ", " welcome!", "\nWhat do you think about this kind of research? Do you find it useful? Is there some other aspect of ROS which you think is worth this kind of activities?", "The full dataset we used for extracting the guidelines from GitHub repositories is available ", ".", "Thanks!", "Ivano", "This is absolutely fantastic. Would you be interested in presenting your results to the quality working group?", "These guidelines are stunning. Thanks a lot for sharing.", "Many thanks for sharing this! I read the whole paper and found lots of useful information! I will allow to share it on ", " as I\u2019m sure many people will find it useful.", "Here are some things I think I would find useful mingled with some suggestions:", "Many thanks for putting this together!", "Hi Katherise, sure, please let me know when it will be and I will check if I can manage to participate.", "Thanks for the invitation!", "Many thanks for sharing this! I read the whole paper and found lots of useful information! I will allow to share it on ", " as I\u2019m sure many people will find it useful.", "Here are some things I think I would find useful mingled with some suggestions:", "Many thanks for putting this together!", "Thanks Mateusz!", "\nIt\u2019s a pleasure to be included in the Weekly Robotics mailing list.", "About your mentioned points:", "External packages: I agree, this may also create maintainability issues since then you need to manually check if the added packages have been updated, right?", "This shouldn\u2019t be too much of an issue actually:", "The second bullet is also less of a problem when using Git submodules, but I tend to not use those (for various reasons).", "Comment/suggestion: this would be a great post for ", " category, instead of here in ", ". You could consider moving the post there.", ": could you perhaps also say a bit more on the repository selection process? The paper doesn\u2019t seem to explain how you selected the initial set of repositories (ie: before you started pruning).", "External packages: I agree, this may also create maintainability issues since then you need to manually check if the added packages have been updated, right?", "This shouldn\u2019t be too much of an issue actually:", "The second bullet is also less of a problem when using Git submodules, but I tend to not use those (for various reasons).", "Right, it all makes sense, thanks Gijs for the hints.", "Comment/suggestion: this would be a great post for ", " category, instead of here in ", ". You could consider moving the post there.", "Yes, indeed, it started as an announcement and it is becoming a full discussion now ", ": could you perhaps also say a bit more on the repository selection process? The paper doesn\u2019t seem to explain how you selected the initial set of repositories (ie: before you started pruning).", "Sure. The initial set of target repositories has been defined by reusing two other research contributions:", "Thanks again for the suggestions!", "Many thanks for the launch files paper! Will definitely have a look at it!", "External packages: yes, if you use submodules then you need to manually sync them with upstream, especially if there are some interesting developments in there for you.", "Bagfiles: I know that chossing \u201ctop platforms\u201d in unbiased manner is a challenge but maybe you can select them based on number of stars in the repo? Or you can be biased and say: these platforms do everything correctly in our view, check out examples of their raw data.", "I\u2019m now realizing that the bagfile data project I\u2019m proposing might not necessarily be fit for a research project but having something similar to ", " but for Robotics could be very interesting. ", " maybe OpenRobotics could lead such effort? I\u2019d be up for writing a chapter on some platform if this was to become a thing.", "Sorry I went even more off topic here ", "Many thanks for the launch files paper! Will definitely have a look at it!", "External packages: yes, if you use submodules then you need to manually sync them with upstream, especially if there are some interesting developments in there for you.", "You are welcome! And thanks for the clarification on the external packages.", "Bagfiles: I know that chossing \u201ctop platforms\u201d in unbiased manner is a challenge but maybe you can select them based on number of stars in the repo? Or you can be biased and say: these platforms do everything correctly in our view, check out examples of their raw data.", "In empirical software engineering there are many ways in which this bias may be controlled. For example, ", " is a paper about the do\u2019s and dont\u2019s related to the treatment of third-party GitHub repositories for scientific purposes.", "I\u2019m now realizing that the bagfile data project I\u2019m proposing might not necessarily be fit for a research project but having something similar to ", " but for Robotics could be very interesting. ", " maybe OpenRobotics could lead such effort? I\u2019d be up for writing a chapter on some platform if this was to become a thing.", "Sorry I went even more off topic here ", "This is ", " interesting for me (and for my research group) and we are definitely willing to help and put effort in this!", "Awesome paper, thanks!", "This is probably in the paper and I just missed it, but do you have any recommendations on the best way to document the system architecture? The paper says:", "\u2026 documented architecture (i.e. the system is described in terms of nodes, services, topics, and their configuration).", "Are these best described in a single README file or as ROS node graphs with text descriptions or do you have some other example of great architecture documentation that the rest of us can follow?", "Is there one particularly excellent architecture that could be used as a starting point for most other robot architectures? Perhaps something like the Niryo One five layers architecture of external communication, command, motion planning, control, and hardware? Or are most robots unique enough to require independently designed architectures? Thanks!", "The robot operating system (ROS) is ", " de-facto standard for robotic software.", "I can\u2019t wait for this to be published for me to be able to cite that zinger.", "Thanks Peter, here are my thoughts on your points.", "This is probably in the paper and I just missed it, but do you have any recommendations on the best way to document the system architecture? The paper says:", "\u2026 documented architecture (i.e. the system is described in terms of nodes, services, topics, and their configuration).", "Are these best described in a single README file or as ROS node graphs with text descriptions or do you have some other example of great architecture documentation that the rest of us can follow?", "My experience with this study is that people document the architecture of their systems in a very heterogeneous way, ranging from raw node communication graphs, to box-and-line diagrams done in Powerpoint, etc. Actually, in the dataset I collected a direct link to each Architecture documentation, you can refer to the \u201cSA documentation\u201d column ", " and explore.", "About recommendations, I think that the best architectures (in terms of documentation) are the ones combining a graphical representation (of both nodes, topics, and pub/subs) and a textual description of the main responsibilities of each node. Personally, I also like those in which you have a high-level architecture description at the beginning and then details are provided in a top-down fashion, such as in ", ".", "Is there one particularly excellent architecture that could be used as a starting point for most other robot architectures? Perhaps something like the Niryo One five layers architecture of external communication, command, motion planning, control, and hardware? Or are most robots unique enough to require independently designed architectures? Thanks!", "This is difficult to say. I think it is doable to have some kind of reference architecture for robots belonging to specific domains (e.g., delivery robots, or industrial arms), but it really depends on the requirements and regulations of each domain. If we want to be generic and cover all application domains, maybe the Niryo One layered architecture is a good candidate, but I doubt it will be able to cover all possible needs. Funnily enough, in these weeks I am working with a group of researchers between Italy and Sweden on a generic platform for (1) specifying reference architectures of arbitrary application domains (e.g., delivery robots) and (2) automatically checking the compliance of specific architectures against the reference architecture of their domain. I will keep you posted about it, if you are interested ", "Cheers!", "Ivano", "Funnily enough, in these weeks I am working with a group of researchers between Italy and Sweden on a generic platform for (1) specifying reference architectures of arbitrary application domains (e.g., delivery robots) and (2) automatically checking the compliance of specific architectures against the reference architecture of their domain. I will keep you posted about it, if you are interested", "I would be very interested, thanks!", "The MoveIt documentation looks fantastic. I imagine it takes a lot of manual work to make it look that nice. Do you think it is possible to generate system architecture documentation automatically, even if it does not look quite that good, in a standardized way that makes it easy to compare architectures and subsystem compatibility? Maybe something like rqt_graph. I do not know how much could be determined statically and how much would need runtime information. Or does proper documentation require high level information that you cannot get from a set of packages alone? It seems like if you have enough information to check the compliance of an architecture then you would have enough information to automatically create documentation for it.", "Maybe something like rqt_graph. I do not know how much could be determined statically and how much would need runtime information.", "Here goes another shameless plug.", " is able to extract the computation graph statically - to some extent.", "An automatic documentation generator is something I have been meaning to do for a while, but never got the opportunity. I might have some time in a couple of months.", "Andr\u00e9 Santos", "Hi Peter,", "\nas soon as we will publish the paper I will come back to you with a copy of it. ", "About the automatic generation. There are approaches and some of them are pretty cool (see Haros by Andr\u00e9 in his other message), but the problem is still very hard, specially with ROS where the \u201carchitectural connectors\u201d emerge only at run-time, basically when you publish/subscribe to specific topics. Interestingly, in ROS it is \u201cfairly simple\u201d to identify the main components of the architecture, but the connects are difficult to extract.", "And yes, it could be fantastic (also for us, researchers) to have a standard notation for representing the architecture of robotics systems and to be able to compare architectures across systems, types of robots, and application domains. This would open up for large-scale studies where we could extract even more details information from the system, even more than architecting guidelines!", "HAROS is able to extract the computation graph statically - to some extent.", "HAROS looks really interesting thanks! Do you have plans to extend it to ROS 2? Could it work with just vcstool yaml files or does it need more information in its configuration files?", "Interestingly, in ROS it is \u201cfairly simple\u201d to identify the main components of the architecture, but the connects are difficult to extract.", "Would the ", " help with extracting all of the necessary connections between components?", "Do you have plans to extend it to ROS 2?", "Not right now, although a few community members have contributed in that regard.", "\nIn a few months I might get into another project which, likely, will require ROS 2 support at some point.", "Could it work with just vcstool yaml files or does it need more information in its configuration files?", "It does need its own YAML files with a little extra information, unfortunately. That is also something I want to address whenever I have the time to start working on v4.0.", "Btw, thanks for sharing that PR with the Node IDL! I had no clue it was a thing, and it might be useful for HAROS, since I already have a similar mini language going on.", "Btw, thanks for sharing that PR with the Node IDL! I had no clue it was a thing, and it might be useful for HAROS, since I already have a similar mini language going on.", "Please do review that PR, we\u2019re developing the code behind it now. It sounds like we have similar interests, and I\u2019d love to make sure your use-case is empowered.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["I\u2019d be very interested to learn about the depth of launch files used in the analyzed projects. Quite often we have launchfiles including other launchfiles. Wonder what is the maximum depth that can be found in the wild", "I\u2019d also mention some guidelines on adding external packages. Ideally we want to list them as dependency and make rosdep handle it for us but sometimes packages are not released or we need to fork them in which case submodules are helpful (what I usually use but I know some people use some other repo managers)", "What would be a killer for me is if someone took top 5-10 \u201ctop repos of platforms\u201d and recorded bag files with representative data. I think this would be very useful for \u2018good practices\u2019 as you can quickly see how the whole stack works what kind of sensor data is provided and in what format etc.", "I\u2019d be very interested to learn about the depth of launch files used in the analyzed projects. Quite often we have launchfiles including other launchfiles. Wonder what is the maximum depth that can be found in the wild", "I\u2019d also mention some guidelines on adding external packages. Ideally we want to list them as dependency and make rosdep handle it for us but sometimes packages are not released or we need to fork them in which case submodules are helpful (what I usually use but I know some people use some other repo managers)", "What would be a killer for me is if someone took top 5-10 \u201ctop repos of platforms\u201d and recorded bag files with representative data. I think this would be very useful for \u2018good practices\u2019 as you can quickly see how the whole stack works what kind of sensor data is provided and in what format etc.", "Launch files: there is a paper from 2015 about how ROS launch files are developed, see it ", "\n", "External packages: I agree, this may also create maintainability issues since then you need to manually check if the added packages have been updated, right?", "Bag files recording: this is a very good idea and it is technically doable. I am wondering about how to (i) choose the \u201ctop platforms\u201d in a replicable and unbiased manner and (ii) how to generate scenarios which are \u201crepresentative\u201d of the common usages of those platforms. I will think about it in the coming days.", "worried about continuity (ie: external repositories disappearing): ", " and point to your fork", "worried about reproducability (ie: avoid tracking moving upstreams): specify versions (ie: commits/tags, not branch names) in scripts/", "/", " files (avoid using scripts for seeding workspaces btw)", "worried about continuity (ie: external repositories disappearing): ", " and point to your fork", "worried about reproducability (ie: avoid tracking moving upstreams): specify versions (ie: commits/tags, not branch names) in scripts/", "/", " files (avoid using scripts for seeding workspaces btw)", "\n", ": it is an analysis tool for scanning dependencies among ROS packages in GitHub, GitLab, and Bitbucket. The first step of the analysis tool consists in performing queries in those three platform and look for repositories taggeg with the ROS topic. Also, rosmap checks the official ROS Index in the rosdistro-repository and considers all repositories mentioned in there as well. In my research I am using rosmap only for the search step.", "\n", ": it is a database (either as MySQL or mongodb) containing a periodically-built mirror of all projects\u2019 metadata in GitHub. I am querying GHTorrent by selecting all repositories which are either tagged with the \u201cROS\u201d topic or their name (or description) contain the \u201cROS\u201d substring. ", " you can see the specific SQL queries I performed in GHTorrent."], "url": "https://discourse.ros.org/t/guidelines-on-how-to-architect-ros-based-systems/12641"},
{"title": "Discussion on detection of people and obstacles in a dense crowd", "thread_contents": ["Hi,", "I\u2019m not at the point of being able to ask a proper question yet. However, I am starting on a project that will hopefully work as a telepresence robot for a friend of mine who is housebound. This would, hopefully, allow her to attend science fiction conventions.", "The problem is that science fiction conventions are extremely crowded and I don\u2019t want the robot to be able to run into anybody or anything, at least no more than a human would in a similar situation.", "The robot will have a lot of sensors to detect objects at various levels, but I think that vision is the best way to detect people in this situation. And I\u2019m sure that it won\u2019t help that many of the people will be in costume.", "Does anybody have any ideas about how to approach this problem?", "The planner to determine the route of the robot would include the above data, in addition to the goals of the remote user and also keeping me within a certain distance. But right now I am only concerned about detecting the \u201ccrowd.\u201d Finding a way through the crowd is different problem. It is similar to the problem of a child in a similar circumstance with orders to keep their parents in sight and not to run into people.", "Yes, it would be nice if the robot were fully autonomous.  But actually I think a robot like this one would be teleoperated, that is moved with a joystick.    So there is a human driver using the vision system remotely.    Still there would be some lag and human drivers are not perfect.   I think it might be good enough that if the robot detected an imminent collision would simply apply the brakes hard and stop.  The human driver would then figure out what to do.", "I\u2019m using the common HC-SR04 as a last-resort collision sensor.  In theory my robot should never crash as there are sensors to prevent this.  But hc-sr04 is a backup.   Inside the base controller there is a loop that runs once for every twist message.  But the base controller looks at the distance reading from the ultrasonic sensors and decides if it \u201cwants to\u201d execute the twist message.   This decision is made completely outside of ROS.  The point is that we should only detect an obstacle if there is a failure in the ROS based system so the base controller, the process that actually commands the traction motors does the \u201cpinging\u201d itself and will refuse to power into a fixed object.", "You may or may not want to use this same design but if operating in a crown and the robot owner is not within arm\u2019s length with a hand ready to punch an \u201ce-stop\u201d bottom you need maybe TWO not one fail safe checks.   In other words if the ROS planner says \u201cgo\u201d it also must have an OK from TWO independent non-ROS based systems.  Perhaps the second one is a mechanical switch that detects physical contact between a bumper bar and the obstacle.    For your robot  I\u2019m thinking of a ring that encircles the robot and is held by springs and if a spring moves say 1/4 inch it means that your vision system, the humans driver and the ultra sonic sensors have all failed to detect the obstacle.  So this system that uses mechanical switches disconnects the motors from power using a mechanical relay (no software in the loop)", "Safety is hard.  The easy task is to design the machine to be safe when it is operating as designed.  The harder task that is 100% required to operate remotely in a crowd is that the machine remains safe even after unanticipated failure modes.    As an example I had an electrical fire last week in a prototype.  Lithium batteries have high power density and I ended up vaporizing a power cable.  My design was not fail safe, in that obviously there was a failure mode that could cause a fire.", "So your anti collision system must work even if there is a bug in the software and even if there is a mechanical fault.  Vision is CLEARLY to complex to be unconditionally safe but could be a very good primary system given enough  redundant backups.", "Back to my work, I\u2019d like to be able to use one camera but I\u2019m undecided.  getting 3D data from one camera requires very good motion estimation.  I don\u2019t think my IMU and odometry will be good enough so I may need stereo vision.   I think in your mixed indoor/outdoor use case, moving in a crowd you will need stereo vision to get usable depth.  Your obstacles are all moving and you will need to snap the pair of images simultaneously, not sequentially.", "Summary:  I think stereo vision is a great primary sensor.  But for remote operation you\u2019d better have multiple independent backups that are each so simple they can\u2019t fail and being truly independent the chances of both failing at the same time is the product of the probabilities, a tiny number.", "Telepresence seems easy at first, basically it is a remote control car with a webcam glued to the top but the problem is that telepresence by definition means the operator is not present.  I think that implies extreme reliability and safety.", "People tracking for people who don\u2019t look like people all the time, interesting!. In a crowded environment no less.", "You might look into the ", " (who had a robot driving around on Amsterdam Schiphol Airport: ", ")", "Following people in a less crowded environment is a task in RoboCup@Home. At TechUnited, we use vision to detect a person and then a laser scanner at torso-height to track the operator.", "Thank you very much, Loy.", "And I forgot to mention: while most of the people in the crowd are adults, there are many children and probably a few R2D2 robots. This is going to be an interesting project. I suppose I will start out with collisions in my motorhome where there is just me to damage. Then I will add in other factors by inviting other people in and putting random boxes down to simulate a changing environment.", "Then I might bring Groucho outside and see how he handles there, with me close by with a remote cut-off switch. Unfortunately, I have to carry him in and out of the motorhome. This puts an upper limit on his size and weight, though I will make him able to be split into multiple pieces if possible.", "Jay", "I agree about the need for safety.", "In some of my previous robots, I had a \u201creflex system\u201d that handled some failure cases. This was a simple processor that took in input from certain sensors and stopped the robot and then sent a message to the planner about why it had stopped. Until the planner sent the reflex processor a \u201cchill out\u201d message, the robot didn\u2019t move.", "The reflex processor killed the motors with relays so that nothing could turn them on until the reflex processor was satisfied.", "I used touch-sensors and floor sensors to make sure the robot wasn\u2019t heading over some stairs.", "These were very simple robots compared to Groucho and they didn\u2019t use ROS.", "I usually design my robots\u2019 brains with multiple functions/subsystems. The reflex processor is the one that is guaranteed to be a separate processor that is very simple.", "I will use multiple segments to my touch-ring. This will give me a better idea of where the robot was touched the object. I may even have certain touch-sensors control the motor relays themselves after a given amount of pressure.", "I have two Intel RealSense cameras that I plan to use for Groucho. I also have six webcams coming so I\u2019ll be able to handle using stereo vision if need be. I am still designing Groucho\u2019s head. I want it to look metallic while also looking like a caricature of Grouch Marx. Plus I will take some ideas from ", ". This is the most expressive head I\u2019ve seen while still looking like a robot. I have 12 Dynamixel AX-12 servos that can be used for just the head if need be.", "There will be at least one camera in the head for additional vision at a taller height. There will be at least 3 webcams in Groucho, in addition to the two RealSense cameras.", "I\u2019ll be using an Intel NUC (latest generation) for Groucho\u2019s main brain. I may preprocess some of the cameras with another computer if that is needed. Or perhaps use a second NUC for vision processing. At this point, I haven\u2019t done much vision processing so I will have to do more before I can say anything. The base will have enough room for several processors.", "DangerousThink, AKA DT, Jay", "The Jack Robbot program at Stanford is using ROS to understand the robot SLAM in social situations.  For example, A person would never walk between to other people having a conversation.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/discussion-on-detection-of-people-and-obstacles-in-a-dense-crowd/2109"},
{"title": "Kobuki base external power supplies: can be set OFF by default?", "thread_contents": ["When you switch on the robot, the external power supplies already provide current, until you publish OFF state on mobile_base/commands/external_power topic.", "Can be somehow disabled by default on robot switch on and then enabled by publishing ON on mobile_base/commands/external_power topic?", "Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/kobuki-base-external-power-supplies-can-be-set-off-by-default/271"},
{"title": "TurtleBot 2e", "thread_contents": ["As many of you may have seen we\u2019ve been working hard on improving support of ROS on smaller embedded computers during the recent release ", " the product of this I\u2019m happy to announce the TurtleBot 2e, an embedded version of the TurtleBot 2.", "Documentation of the TurtleBot2e can be found on github at: ", " and you can get it in pdf format ", "The TurtleBot 2e is a new revision of the TurtleBot primarily defined by replacing the netbook with a single board computer(SBC)", "\nsuch as the 96 Boards CE computer, the DB410c.", "The other big change is that we\u2019ve added support for the ", " depth sensor.", "\nThis is important because both the sensors, the Microsoft Kinect and the Asus Xtion, we previously used as standard sensors are now no longer in production. This upgrade is available to all TurtleBot users. There are some ", " If you have trouble with the Astra take a look there. As well as visiting Orbbec\u2019s forums ", "We\u2019ve validated the basic demos such as gmapping, following, and joystick teleoperation are functional on the DB410c.", "\nHowever there may be tuning needed for different SBCs.", "Please try things out! And any contributions for improvements to the documentation code etc are always welcome.", "\nSo jump in and try it out if you already have a TurtleBot 2 and an SBC.", "\nThe more people who are involved the better this will become.", "Dabit Industries has the TurtleBot 2e for presale.", "\"TurtleBot 2 is an open robotics platform designed for education and research on state of art robotics. It is also a powerful\u00a0 tool to teach and learn ROS (Robo", "also the accessories pack: ", "email: ", " if you have any questions or need help with anything.", "We just ordered the pieces to build the heat sinks, expect to get them done in 2-3 weeks for the robot to ship.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/turtlebot-2e/307"},
{"title": "[TB3] How to leverage the Intel ROS Project", "thread_contents": ["I brought up the Intel ROS Project and don\u2019t know how to test it, port it to the TB3, or leverage it.", "\n", "I installed the Intel ROS Project because the diagram on it home page (see link above) suggested I should be able to use it to do interesting things. It also assures that the Movidius and RealSense APIs work well together. It was difficult to install because of the number of packages and instructions were written using different documentation, installation and test conventions. But I managed to complete that install after a couple false starts.", "In the following paragraphs I document installation and test status to provide confidence that the install is promising. I am currently stuck. I don\u2019t know what the next steps are.", "I executed the following in separate terminal windows. They executed without crashing and I could display an interesting rqt_graph.", "rviz displays \u201cNo Image\u201d in its Image window. Clearly I have a publisher/subscriber message mismatch. My /camera/realsense2_camera node is publishing /camera/realseanse2_camera_manager/bond. I see nothing in the rqt_graph subscribed to this message.", "/moving_object subscribes to the /object_analytics/tracking message from /object_analytics and it also subscribes to /tf_static from /camera. This demonstrates that a lot is working. rqt_graph image below.", "QUESTIONS", "The rqt_graph indicates that I have a fundamental disconnect about how to integrate the capabilities I have installed. Where do I go from here? Is there an example implementation and description that I can use to integrate all this into a proof of concept?", "I think rviz is supposed to display the output of the RealSense camera. How do I make that happen?", "I don\u2019t have a well defined project. I\u2019m having fun learning something new.", "My current setup supports Kobuki. I have a tb3. What messaging capabilities must I port to tb3 to start that process? Does this question even make sense?", "Support appears to be migrating to ROS2 more quickly than I expected. I am hoping to get a basic understanding of the ROS Intel Projecting using ROS1 before moving to ROS2. The transition will probably take me several months so I am getting this project to work in ROS1 will provide the reenforcement I need while I work on that transition. I want to see the power of the ROS Intel Project to motivate me. Is that reasonable?", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Movidius and RealSense ROS demos work.", "I had to install the Kobuki message package before I could complete the ROS Moving Object install: ros-kinetic-kobuki-msgs.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/tb3-how-to-leverage-the-intel-ros-project/8655"},
{"title": "Design By Contract", "thread_contents": ["What do you think about adding \u201cDesign By Contract\u201d functionality to ROS?", "\u201c", " is an approach for designing software. It prescribes that software designers should define formal, precise and verifiable interface specifications for software components, which extend the ordinary definition of abstract data types with preconditions, postconditions and invariants.\u201d ", "\u201cDesign By Contract (DBC)\u201d can dramatically decrease the effort during software integration and dramatically increase the overall system reliability.", "\u201cDesign By Contract (DbC)\u201d is a built-in feature of many recent programming languages. Usually the verification of these interface specifications is supported on different levels of abstractions during \u201cdebug\u201d builds at runtime. ", " supports runtime checking of contracts on the interface, the class and the (member) function level for example:", "On the class level the caller of a member function ensures defined preconditions. When the preconditions are fulfilled the member functions guarantees proper functioning and defined postconditions. Invariant checks ensure that an object remains in a valid state during runtime. The validity of the internal state (e.g. class members) is checked after the execution of the constructor, before the execution of the destructor, before and after the execution of a public member function. [Cehreli, Ali: ", ", IngramSpark, 1st edition, 2017, p. 218 and 386 or ", "]", "In ROS one could think of node level contracts w.r.t. \u2026 (impact of DbC on timing):", "Checks w.r.t. to timing could be heavily impacted by the prrocessing overhead of checks for complex message types. However in some circumstances (comparably low processing overhead due to checks) they could at least give some rough estimate w.r.t. to \u201cdynamic\u201d node dependencies.", "My two cents.", "\nIt sound to be a good idea, if a missing requirement genrate a warning, rather than an assertion/exception.", "Some of the things you described seems to be related more to Quality of Services, rather than Design by Contract, but I might be wrong.", "The more I learn modern C++, more I realize that the best \u201ccontracts\u201d can be usually implemented using \u201cstrong types\u201d.", "\nI am in favour of using something similar to ", " in ROS messages, some kind of metadata attached to the topic itself that is NOT transmitted every time and is immutable.", "But I am aware that this is another topic\u2026", "I have often put some thought into providing such an idea over the years, but I\u2019ve never come close to putting in enough effort to actually do it. ", " I\u2019d love to see such work move forward so I\u2019m happy to see someone putting in the work!", "Having said that, I think that it would be a major effort to achieve for ROS 1, but quite doable for ROS 2 because many of the things you want to establish as the \u201ccontract\u201d for a data-flow-based software component can be implemented using the DDS QoS policies. It would need some thought put into how and where to specify the contracts and then how to translate that into QoS settings. On from that things get more complicated. DDS provides facilities to be informed when a QoS policy has been violated, but how to respond to that is probably going to be very application specific. (In classical Eiffel-style DbC, the contracts are essentially asserts that raise exceptions when violated at run time.)", "This can be done using the DDS ", " QoS policy.", "This could be done in the message IDL and an assert in the topic publish API. There has been occasional talk in ROS 2 discussions of adding allowable ranges to the IDL, but I don\u2019t think it\u2019s gone anywhere. However if it is going to be node specific it would probably be better done purely in the topic publish API, with a node declaring when it sets up the publisher what allowable ranges the message can hold. This would be fine for run-time checks, but it would make it a lot harder to check contracts statically than having them in the IDL. I think that the need for a unique message definition for every node would be too prohibitive to put contracts in there, though. Other than perhaps things that everyone agrees are sensible for that particular message.", "This can also be done using the DDS ", " QoS policy.", "Same comment as for the publishing side above.", " QoS policy again.", "Now we\u2019re getting close to Eiffel-style DbC. These can be done now by putting asserts in your service callback, but what you really want is a way to notify the caller that there was a problem fulfilling the service call due to a contract violation. This would probably require extending the way services are implemented.", "The RPC over DDS specification, which was finally published in April this year and hopefully the OSRF\u2019s DDS vendors will rapidly support, does not provide any QoS policies specific to services (only ways to specify existing QoS policies on a per-interface level). Therefore ROS2 would need to decide how to deal with contract violations themselves.", "Same as above.", "Again same as above.", "So the things that you are requesting are doable in ROS 2 using a combination of the ", " QoS policy and adding some features to the API for specifying pre- and postconditions. I do not think it would be a huge level of work, but there would need to be a focus on how the API will work to make it clear what is happening, and making sure that the performance is both minimal and zero-able (i.e. all checks can be turned off).", "The more I learn modern C++, more I realize that the best \u201ccontracts\u201d can be usually implemented using \u201cstrong types\u201d.", "Design by contract encompasses more than can be specified using strong types. It includes the behaviour of the function called. Even in a language where functions strictly have no side effect, there are parts of this that cannot be expressed by ensuring the types are correct, unless every function takes and returns types unique to that function that exactly define its input and output spaces. For functions that do have side effects (such as class methods) and situations where the side effects are the whole point (many service calls and many data-flow-based nodes), types will not cover everything you want to check.", "I am in favour of using something similar to boost::units in ROS messages, some kind of metadata attached to the topic itself that is NOT transmitted every time and is immutable.", "I think that this would be a useful feature. Adding units to data is a powerful way to catch a common class of potentially fatal errors, and it can be done in a way that has minimal impact on performance.", "Re-posting here because replying from email breaks\u2026", "IMHO There are many many known ways to improve software quality in general, and ROS in particular, but many limiting factors, mostly the lack of resources ( including each and everyone motivation and time ).", "So I think the focus should be on applying the software development methodologies that are most likely to bring big benefits with relatively little investment, picking from the list of already proven existing software systems in other areas.", "The contracts as described seems to be a \u201cweak\u201d version of a specification + model checker (check ", ") that could also be integrated with a ROS system, but the effort required for the potential users is probably prohibitive\u2026", "Before doing contracts, I would first focus on proper, static (since message structure is static), strong, typing (despite the default weak/dynamic typing of the supported languages, it is doable using existing libraries, like for ", " and ", " - and even ", ").", "My personal top two wishes are :", "static&strong typing for ROS message fields (typing helps, and even more when things are distributed). It s a first step\u2026 later we could do much more, like add external dynamic typechecker that can check communication during execution (for types, contracts or stronger formal systems)", "erlang VM integration ( ROS messages as a port, communicating with speed number crunching C++ code, and able to use the erlang VM for all the distribution concerns ) - especially for ROS1.", "I have often put some thought into providing such an idea over the years, but I\u2019ve never come close to putting in enough effort to actually do it. ", " I\u2019d love to see such work move forward so I\u2019m happy to see someone putting in the work!", "I have my roots in the domain of embedded software development. Most languages used there like C/C++ are lacking a built-in support for \u201cDesign By Contract\u201d (one exception is e.g. Ada) and try to compensate the lack e.g. with coding standards suggesting to define the interfaces in a \u201ccontract\u201d like manner as part of the built-in code documentation. But in comparison with language built-in support all these measures are very weak. To get to the point\u2026 this topic is keeping me busy for quite a while ", ".", "Having said that, I think that it would be a major effort to achieve for ROS 1, but quite doable for ROS 2 because many of the things you want to establish as the \u201ccontract\u201d for a data-flow-based software component can be implemented using the DDS QoS policies. It would need some thought put into how and where to specify the contracts and then how to translate that into QoS settings. On from that things get more complicated.", "ROS is a new technology for me. But after reading a bit about the ROS2 design I thought the proposal for \u201cDesign By Contract\u201d would fit better into the ROS2 design GitHub repo than into ROS1 (and ", ") in the first place. Unfortunately I do not know enough about ROS2 to implement something reasonable on my own yet. However I am very interested in contributing code if it is ensured that it is no waste of time. (Means contributing code w.r.t. some reasonable up-front design.)", "This can be done using the DDS DEADLINE QoS policy.", "I will dive deeper into DDS and QoS policy the near future.", "DDS provides facilities to be informed when a QoS policy has been violated, but how to respond to that is probably going to be very application specific. (In classical Eiffel-style DbC, the contracts are essentially asserts that raise exceptions when violated at run time.)", "(In D one can decide whether to use \u201cassert\u201d, \u201cstatic assert\u201d or \u201cenforce\u201d checks. \u201cassert\u201d checks throw an AssertError which is no Exception but an error during runtime. \u201cstatic assert\u201d checks do the same but at compile time. Both are usually enabled during debugging only.", "\n\u201cenforce\u201d checks throw exceptions at runtime which which can be handled and are most suitable for public interfaces.)", "This could be done in the message IDL and an assert in the topic publish API. There has been occasional talk in ROS 2 discussions of adding allowable ranges to the IDL, but I don\u2019t think it\u2019s gone anywhere. However if it is going to be node specific it would probably be better done purely in the topic publish API, with a node declaring when it sets up the publisher what allowable ranges the message can hold. This would be fine for run-time checks, but it would make it a lot harder to check contracts statically than having them in the IDL. I think that the need for a unique message definition for every node would be too prohibitive to put contracts in there, though. Other than perhaps things that everyone agrees are sensible for that particular message.", "Unfortunatelly I do not know much about IDL as well right now. From a conceptional point of view the possiblility for static checking considering a single source of specification should be favored. Considering the \u201cstatic\u201d behaviour of the node interface only the IDL seems suiteable for me. What do you mean \u201cif it is going to be node specific\u201d exactly? In case of exotic node topic message types?", "Will \u201cdynamic\u201d aspects like the \u201cguaranteed\u201d publish rate of topics is gooing to be defined in the IDL as well? If so then the IDL could be a suitable place for these kind of contract information in the first place as well. However if one thinks further about situations where the \u201cguaranteed\u201d publish rate shall not be constant but dependent on the node state during runtime (in common if the behavior of the node depends on its state during runtime) the IDL seems not that suitable anymore.", "Now we\u2019re getting close to Eiffel-style DbC. These can be done now by putting asserts in your service callback, but what you really want is a way to notify the caller that there was a problem fulfilling the service call due to a contract violation. This would probably require extending the way services are implemented.", "In case a \u201cstrict\u201d contract check (analog to D \u201cassert\u201d checks throwing errors) would fail one as programmer should be notified somehow about that (e.g. logger). In case of \u201cweak\u201d contract checks failing (analog to D \u201cenforce\u201d checks throwing exceptions) the caller should be notified directly.", "The RPC over DDS specification, which was finally published in April this year and hopefully the OSRF\u2019s DDS vendors will rapidly support, does not provide any QoS policies specific to services (only ways to specify existing QoS policies on a per-interface level). Therefore ROS2 would need to decide how to deal with contract violations themselves.", "I will dive deeper into DDS and QoS policy the near future.", "So the things that you are requesting are doable in ROS 2 using a combination of the DEADLINE QoS policy and adding some features to the API for specifying pre- and postconditions. I do not think it would be a huge level of work, but there would need to be a focus on how the API will work to make it clear what is happening, and making sure that the performance is both minimal and zero-able (i.e. all checks can be turned off).", "I consider the possibility to \u201cdisable\u201d performance overhead essential as well. The possibility to keep performance overhead minimal would depend on the communicated data type to a great extend I guess. I would love to contribute.", "The contracts as described seems to be a \u201cweak\u201d version of a specification + model checker (check TLA+) that could also be integrated with a ROS system, but the effort required for the potential users is probably prohibitive\u2026", "Yes, it\u2019s specification and model checking. If the effort required for the potential users is prohibitive depens heavily on the domain and environmental conditions they are acting in. I think one should give every possible user as much optional technical possibilities to work with as possible. If users make use of the concepts offered is their choice.", "Before doing contracts, I would first focus on proper, static (since message structure is static), strong, typing (despite the default weak/dynamic typing of the supported languages, it is doable using existing libraries, like for C++ and Python - and even LISPs).", "I agree that static strong typing is very important. However from an integration point of view I wouldn\u2019t consider \u201cDesign By Contract\u201d less important. DbC helps to avoid \u201chigher level\u201d interaction issues in addition to typing issues. But as DbC would require many features to be most effective w.r.t. to effort static strong typing could probably be achieved faster.", "[\u2026] I think one should give every possible user as much optional technical possibilities to work with as possible. If users make use of the concepts offered is their choice.", "Freedom-", "-choice supporters would probably disagree with you here.", "From a maintenance pov this is also not a very popular sentiment.", "I am one of those \u201cFreedom-from-choice\u201d supporters ", "Freedom-from-choice supporters would probably disagree with you here.", "From a maintenance pov this is also not a very popular sentiment.", "I am one of those \u201cFreedom-from-choice\u201d supporters ", "I am a freedom-from-choice supported as well ", " . I would stick to freedom-from-choice w.r.t. all \u201cinternals\u201d of a framework. However from a framework user perspective it is probably not always possible or reasonable to beeing forced to define contracts in practice.", "For everyone which is interested to get hands dirty: There is a ", ".", "Before doing contracts, I would first focus on proper, static (since message structure is static), strong, typing (despite the default weak/dynamic typing of the supported languages, it is doable using existing libraries, like for C++ and Python - and even LISPs).", "I am interested in this topic as well. But I would prefer to discuss only DbC in this thread. If you can convince me that I prioritize it higher than DbC I will be with you ", "This does not fit into this thread as well but what is the benefit of erlang VM integration?", "This can be done using the DDS DEADLINE QoS policy.", "For more information about the DDS DEADLINE QoS policy refer to page 95 in the ", ".", "Unfortunatelly I do not know much about IDL as well right now. From a conceptional point of view the possiblility for static checking considering a single source of specification should be favored. Considering the \u201cstatic\u201d behaviour of the node interface only the IDL seems suiteable for me. What do you mean \u201cif it is going to be node specific\u201d exactly? In case of exotic node topic message types?", "This relates to the way the message definition language is used in ROS. It defines data types, not node interfaces. Contracts are much more likely to be specific to node interfaces than to the messages, which are intended to be generic and highly reusable. Because ROS doesn\u2019t currently have a node interface definition language, there is not yet a suitable place to specify contracts.", "Furthermore, some contracts may be specific to a particular implementation of a node, and so wouldn\u2019t fit in a node interface specification intended to be reused by many different implementations (although then I would argue that the nodes with different contracts should not be considered interchangeable and so should be using different interfaces).", "Contracts aim is to enforce complex dynamic properties of a system.", "\nTypes aim is to enforce simple (usually) static properties of a system.", "Therefore I am of this point of view : ", "\nNotice how the complexity increase from top left to bottom right.", "So, before trying to do something complex (which means heavy maintenance, and likely to be unused until it is perfectly optimized), I would focus on the doable, lighter side of things.", "I also agree with ", " and would like first to see more strict enforced message types, before thinking about their combination in an IDL, how this would behave dynamically, and how to enforce some behavior and prevent others\u2026", "\nRight now the message field type is too ambiguous (\u201cnode N can subscribe to a message M with a field int, but actually there will ever be only even numbers there\u2026\u201d except the developer of N don\u2019t know that, unless he goes through the code of all nodes publishing M)", "This does not fit into this thread as well but what is the benefit of erlang VM integration?", "Don\u2019t try to reinvent the wheel, reuse 30 years of expertise in distributed system programming. There are already a bunch of people working on these questions in a distributed setting, and some tools available : ", "By the way, isn\u2019t this thread some kind of ", ". Which problem exactly are you planning to solve with DbC ?", "Always nice to see another Erlang fan! That language is such a pleasure to program in.", "would like first to see more strict enforced message types, before thinking about their combination in an IDL, how this would behave dynamically, and how to enforce some behavior and prevent others\u2026", "Right now the message field type is too ambiguous (\u201cnode N can subscribe to a message M with a field int, but actually there will ever be only even numbers there\u2026\u201d except the developer of N don\u2019t know that, unless he goes through the code of all nodes publishing M)", "Some would say that this is one thing contracts are meant to check\u2026 but I think that depends on where you draw the line between what is a contract and what is the type. But I think you are correct in saying that this sort of information really needs to be available and checkable. It is helpful to have in documentation but far more beneficial to developers (and safer) for it to be automatically checkable.", "Types/Contracts : In my mind, \u2018Design by Contract\u2019 was an informal concept/practice introduced a few years ago because type systems of most languages at that time was insufficient to guarantee correct program behavior. But it is fundamentally the same thing\u2026", "\nExcept that we have researched ", " for a while now, whereas \u2018", "\u2019 is probably not what you would expect after learning about DbC\u2026", "These days I am following ", " and ", " to ", " ", ".", "Contracts aim is to enforce complex dynamic properties of a system.", "Types aim is to enforce simple (usually) static properties of a system.", "That summarizes the difference between types and contracts. And that is exactly about why I didn\u2019t propose types here: In my experience the hard to find defects tend to have their root cause in implicit, incomplete or missing definitions of the dynamic characteristics of here in ROS, node interactions.", "This relates to the way the message definition language is used in ROS. It defines data types, not node interfaces. Contracts are much more likely to be specific to node interfaces than to the messages, which are intended to be generic and highly reusable. Because ROS doesn\u2019t currently have a node interface definition language, there is not yet a suitable place to specify contracts.", "Unfortunatelly that is exactly what I found out when looking into the ROS2 sources. One could add deadlines for topics that (a) do not change or (b) do change over node runtime they could be (a) defined and/or (b) updated via the rmw C API which wrapps the DDS DynamicData API or the statically generated DDS functionality from the IDL definitions. However as you said: The interface considers the aspects of the message description languages IDL only, not a node description language. And a node description language would be required to add functionality which would be most benefitial.", "By the way, isn\u2019t this thread some kind of X-Y problem. Which problem exactly are you planning to solve with DbC ?", "That\u2019s right. The question should be: \u201cHow can I prevent from introducing defects into/detecting defects in distributed ROS systems which have their root cause in the dynamic interaction of several ROS nodes?\u201d I am biased and did not propose ", " because that seems hard to implement for distributed systems. DbC or actually model checking based on kind of a node description language seems to be cheaper to me.", "I might be stating the obvious here, but still worth reminding everyone I think\u2026", "How can I prevent from introducing defects into distributed ROS systems which have their root cause in the dynamic interaction of several ROS nodes?", "Don\u2019t build a distributed (==multiprocess) system if you don\u2019t ", ". Programming language elements (functions, classes, libraries, packages) are made for composing correctly in all sorts of ways, and there is usually theoretical background, tooling, conventions, processes, to help you satisfy the cognitive biases you didn\u2019t know you had. No distributed software system that allows you to control the distribution graph, has anything equivalent to that currently. ROS is no exception (actually erlang might be the only exception).", "\nExample : A whole part of Operating System design is to ", ", and most recent OSes are ", " ? This is opposed to the features suitable for a distributed system, which by definition needs process cooperation, and where controlling when each process can be interrupted, or not, is really useful. In one process, in one language, all these problems vanish.", "If you have to build a distributed system, congratulations, you are doing distributed system research. This is not robotics and there is a different set of assumptions coming along in that context.", "\nExample : most existing and widely-used distributed software systems rely on the fact that a message, a unit of computation \u201ctask\u201d, is atomic and ", ". That requirement usually cannot be met in a robotic platform, because of side effects on the real world, the whole point of it. Painful lesson after a year working on ", " - .", "For the rest of us having to do both distribution and real world side-effect, I feel the most promising way, is still ", ". But, as far as I know, it is still a software research topic on its own.", "Regarding ROS, the best bet is likely to integrate/interface/implement ROS with the existing programming language that provide the feature that you need, instead of trying to integrate \u201cthat awesome language feature\u201d into ROS (because it implies re-implementation and proactive maintenance from ROS community for something that is not purely robotics related)", "For DbC, I\u2019m thinking if you get around implementing a Eiffel-based ROS interface/integration/implementation, you might find some interesting changes needed in ROS itself, even in REPs, in order to make that possible without compromising ", ". I\u2019m thinking these changes would likely be worth it for ROS, especially in the long run.", "\nDisclaimer: I\u2019m currently following the same path with Python, improving how ROS integrate with it along the way, and finding basic problems where I didn\u2019t expect to\u2026", "But ultimately, writing a \u201csolid\u201d software project is a matter of computer science and software engineering expertise, so general software theory, knowledge and tools apply there. It\u2019s not a problem specific to robotics, and therefore robotic science and tools (like ROS) are not focusing on it.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["if node is topic publisher (postcondition checks)\n", "(\u201cguaranteed\u201d topic publish rate)", "\u201cguaranteed\u201d topic message type values", "\n", "if node is topic subscriber (precondition checks)\n", "(\u201cexpected\u201d topic reception rate)", "\u201cexpected\u201d topic message type values", "\n", "if node is service server\n", "(\u201cguaranteed\u201d service response time) (postcondition check)", "\u201cexpected\u201d service request message type values (precondition check)", "\u201cguaranteed\u201d service response message type values (postcondition check)", "\n", "if node is service client\n", "\u201cguaranteed\u201d service request message type values (postcondition check)", "\u201cexpected\u201d service response message type values (precondition check)", "\n", "if node is action server\n", "(\u201cguaranteed\u201d feedback transmission delay after goal request has been received) (postcondition check)", "(\u201cguaranteed\u201d result transmission delay after goal request has been received) (postcondition check)", "\u201cexpected\u201d action goal message type values (precondition check)", "\u201cguaranteed\u201d action feedback message type values (postcondition check)", "\u201cguaranteed\u201d action result message type values (postcondition check)", "\n", "if node is action client\n", "(\u201cexpected\u201d feedback message delay after goal request has been transmitted) (precondition check)", "(\u201cexpected\u201d goal message delay after goal request has been transmitted) (precondition check)", "\u201cguaranteed\u201d action goal message type values (postcondition check)", "\u201cexpected\u201d action feedback message type values (precondition check)", "\u201cexpected\u201d action result message type values (precondition check)", "\n", "if node is topic publisher (postcondition checks)\n", "(\u201cguaranteed\u201d topic publish rate)", "\n", "\u201cguaranteed\u201d topic message type values", "if node is topic subscriber (precondition checks)\n", "(\u201cexpected\u201d topic reception rate)", "\n", "\u201cexpected\u201d topic message type values", "if node is service server\n", "(\u201cguaranteed\u201d service response time) (postcondition check)", "\n", "\u201cexpected\u201d service request message type values (precondition check)", "\u201cguaranteed\u201d service response message type values (postcondition check)", "if node is service client\n", "\u201cguaranteed\u201d service request message type values (postcondition check)", "\u201cexpected\u201d service response message type values (precondition check)", "\n", "if node is action server\n", "(\u201cguaranteed\u201d feedback transmission delay after goal request has been received) (postcondition check)", "(\u201cguaranteed\u201d result transmission delay after goal request has been received) (postcondition check)", "\u201cexpected\u201d action goal message type values (precondition check)", "\u201cguaranteed\u201d action feedback message type values (postcondition check)", "\u201cguaranteed\u201d action result message type values (postcondition check)", "\n", "if node is action client\n", "(\u201cexpected\u201d feedback message delay after goal request has been transmitted) (precondition check)", "(\u201cexpected\u201d goal message delay after goal request has been transmitted) (precondition check)", "\u201cguaranteed\u201d action goal message type values (postcondition check)", "\u201cexpected\u201d action feedback message type values (precondition check)", "\u201cexpected\u201d action result message type values (precondition check)", "\n", "\n", "\n", "\n", "\n", "erlang VM integration ( ROS messages as a port, communicating with speed number crunching C++ code, and able to use the erlang VM for all the distribution concerns ) - especially for ROS1.", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/design-by-contract/2405"},
{"title": "[TB3] problem to connect wiimote to Turtlebot 3", "thread_contents": ["I have two issues:", "\nIssue#1:", "\nIn Teleoperation chapter:  ", ", for every listed remote controller, all operations are marked as \u201c[Remote PC]\u201d. Should it be \u201c[Turtlebot]\u201d instead? We need to connect the remote controller directly with TB3, in order to control TB3 via the remote controller. It has nothing to do with the \u201cRemote PC\u201d. Please clarify this.", "Issue#2:", "\nI am not able to connect wiimote to TB3.", "\nHere is what I did:", "\nAt first, run:", "\nsudo apt-get install ros-kinetic-wiimote", "\nthen:", "\nrosdep install wiimote", "\nthen:", "\nrosmake wiimote", "Everything is fine. No error found.", "Then I try to pair wiimote with TB3. I get the following error:", "$ rosrun wiimote wiimote_node.py", "\nPress buttons 1 and 2 together to pair (within 6 seconds.)", "\n(If no blinking lights, press power button for ~3 seconds.)", "\nWiimote read error", "\nRead error (nunchuk cal)", "\n/opt/ros/kinetic/lib/python2.7/dist-packages/wiimote/wiistate.py:157: RuntimeWarning: divide by zero encountered in divide", "\nself.acc = WIIReading((self.accRaw - self._accCalibrationZero) / (self._accCalibrationOne - self.accCalibrationZero), self.time)", "Then I added some logs in wiimote_node.py. and I found that the following line is not working:", "wiimoteDevice = wiimode.WIIMote.WIIMote()", "I went through the source code of wiimote module: ", " and then found that the following line is causing the problem:", "\nself._wm = cwiid.Wiimote()", "\nSee ", "Anyway, I am able to pair wiimote directly with Ubuntu. But I am not able to pair it with TB3.", "\nHave you seen similar problem? Any solutions?", "Thank you very much.", "\n\u2013Kening", "Issue#1:", "In Teleoperation chapter:  ", ", for every listed remote controller, all operations are marked as \u201c[Remote PC]\u201d. Should it be \u201c[Turtlebot]\u201d instead? We need to connect the remote controller directly with TB3, in order to control TB3 via the remote controller. It has nothing to do with the \u201cRemote PC\u201d. Please clarify this.", "The tutorials are designed for you to plug your device joystick into the Remote PC. You can modify the configuration.", "With respect to your wiimote issue that doesn\u2019t seem to be TurtleBot related. The TurtleBots are running Ubuntu so they should work as any other Ubuntu computer. I\u2019d suggest that you search for similar issues and if nothing is found ask on ", "One side note:", "Here is what I did:", "At first, run:", "sudo apt-get install ros-kinetic-wiimote", "then:", "rosdep install wiimote", "then:", "rosmake wiimote", "If you\u2019ve used the binary installations you shouldn\u2019t need to use ", " to install dependencies. And the wiimote package is a catkin package so you shouldn\u2019t be trying to use ", " to build it. If you\u2019re still having trouble when you ask on ", " please make sure to include your full environment setup etc. Including what code you\u2019ve checked out and what environment setup files you\u2019ve sourced.", "If you can reproduce a problem in the constructor of the Wiimote class, a ", " is the right place to follow up.", "Hi ", "As ", " said that, Teleoperation examples are tested in Remote PC and RP3. But we recommend that this examples are run into the Remote PC due to stable bluetooth connection.", "\nSo ", " have to ", " TurtleBot3 before start Teleoperation examples.", "And I checked \u2018sudo apt-get install ros-kinetic-wiimote\u2019 is working and \u2018wiimote_node\u2019 is more reliable than \u2018wiimote_node.py\u2019. I changed some information about wiimote teleoperation in ", ".", "Give it a try and let me know how it works.", "Thanks", "\nDarby", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-problem-to-connect-wiimote-to-turtlebot-3/2251"},
{"title": "ROS Quality Assurance Working Group meeting minutes - March 2018 Meeting", "thread_contents": ["Two instances have taken place this month. The first was the 7th and the second was the 14th", "A list of quality metrics was presented for discussion. The list was compiled based on ", " ", ". The objective is to define a set of metrics to use in order to \u201cmake ROS packages quality visible\u201d.", "These quality metrics have been discussed:", "A decision has been made to put the list into a google document (", ") and share it with the group to allow people to contribute to the list development.", "We invite people to take initiative and contribute to the development of spreadsheet. Your contributions will make it happen!", "Hi,", "I would like to contribute to this working group in the future. Kudos for the initiative!", "My 2 cents is that it would be useful to prioritize our quality metrics. A list of 29 elements might be intimidating for people which are doing the transition from \u201cit just works\u201d to professionally written code.", "I guess we can approach it in the 80/20 rules, ie. 20% of this recommendation can already solve 80% of typical issues.", "Clang sanytizers and Unit Test for example are extremely powerful best practices, IMHO.", "Davide", "I agree, it is an ambitious list. We should prioritize and have an iterative approach to the implementation. We should also think of the audience of these metrics, i.e. How they may interpret these metrics?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "Build", "Unit Tests", "Unit Test Crash", "Unit Test Coverage [%]", "Code style violations", "Logic errors and warnings", "Cyclomatic complexity", "McCabe complexity", "Afferent coupling", "Efferent coupling", "Clang AddressSanitizer and LeakSanitizer", "Fuzzy testing by \u201cchaos node\u201d", "Comment to code ratio", "Status", "README", "Wiki", "Getting Started", "Other resources", "Number of closed issues", "Time to close issue", "Activity on issues", "Other status (eg: wont-fix, etc.)", "Number of open issues", "User rating (Star rating and feedback)", "Maintainability Index", "Depth of Inheritance", "Class Coupling", "Lines of Code", "Cyclic includes", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-quality-assurance-working-group-meeting-minutes-march-2018-meeting/4226"},
{"title": "[TB3] TurtleBot3 Burger Assembly Video", "thread_contents": ["Hi all,", "I\u2019ve been planning to post this assembly video[43minute] from long ago and today is the day!", "\nSometimes reading assembly manual seems hectic and often confused.", "\nI have to confess that connecting power pin to the Raspberry Pi 3 board required me some googling :\u2019(", "\nAlthough this video cannot be a perfect manual, saving at least one Raspberry Pi board would be awesome.", "Also, there is an amazing series of ", " recorded by Michael Overstreet so go ahead and check his videos as well.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-burger-assembly-video/2340"},
{"title": "[URDF-NG] ROS2 URDF2 discussion", "thread_contents": ["What does the next version of URDF look like?", "There was", " around URDF and SDF in 2.0.", "Later a ", ". Summary ", ".", "The discussion so for seems to be leaning towards harmonizing URDF and SDF (using SDF as a foundation).", "What are the next steps? What are the important details?", "My bold position is that ROS could adopt SDF wholesale. Most, if not all, of the wishlist items for URDF2 are already captured in SDF. It would be a shame to reinvent the wheel.", "I have a feeling this idea is somewhat controversial. If so, would someone be willing to speak to potential issues associated with ROS using SDF?", "It would be a shame to reinvent the wheel.", "Didn\u2019t SDF do that, given that people were trying different ways to extend URDF already (mimic tags, srdf, etc.)? ", "Jokes aside, I think SDF\u2019s ability to have frames with multiple parents in the kinematic chains is a distinct and fundamental advantage over URDF, which happens to also be a requirement of sorts when working with physic engines. However, I do think it shows that there are appropriate situations in which yet another standard makes sense.", "If so, would someone be willing to speak to potential issues associated with ROS using SDF?", "In order to use SDF in ROS, there needs to be a way to do tf with graph (rather than tree) based kinematics, i.e. how to handle multi-parent reference frames with tf. This is brought up every time this is suggested and though some ideas have been discussed in person, there\u2019s never been a well formed proposal on how to deal with that.", "Specifically ", " and I have talked at length about spanning trees for the SDF graph and how they could be used to address this. Maybe ", " can speak to that point.", "There may be other issues, but that\u2019s the fundamental one that comes to mind for me.", "Wheels are strange beasts, and like to be reinvented. Gazebo did have an XML format for describing robots and worlds before ROS existed. In fact, URDF looks surprisingly like Gazebo\u2019s original format\u2026", "The graph issue is a good point. Adding something to SDF to break a graph into a tree sounds like a reasonable approach.", "An impopular position perhaps, but personally I would really like to see ROS2 use an established scene/robot description format if possible.", "SDF is a good improvement over URDF already, but it\u2019s still a custom format, with almost no uptake outside the ROS-Gazebo universe. I know of one (export only?) plugin for a (commercial) 3D modelling tool. Newer Gazebo versions have the model editor, but that is still limited.", "With ROS2 using an established and industry grade middleware as the foundation to build its communication abstraction on top of, it would be great to see if we can do something similar for some other key parts, the robot description format being one of them.", "So I have just consumed most, if not all, of the discussion so far (hurray for lazy Fridays) including the recording of the meeting. Here are a few observations:", "All the formats seem to have arisen from fire-fighting small problems in small problem domains rather than intentional design. Everyone participating in the last year of discussions is representing their stakeholders and their requirements; but the conversations seems to be \u201cHow can we make the current robot description techniques less broken?\u201d rather than \u201cWhat set of stakeholders and requirements will give us the best foundation for the future?\u201d", "Given the number of people volunteering to do anything, moving to and incrementing on SDF seems like the best bet.", "At the heart of a robot description, the most important thing is that the mental model, and any given element of that model, is documented in enough detail so that you, and I, and everyone else understand the context and agree on what the element is. After that we need a defined data representation, with a common, easy to implement format being nice to have.", "I\u2019m seeing a general confusion between configuration (robot A has a gripper and robot B does not) and the dynamic state of things (robot A picks up an object and we add a rigid constraint between the pose of the EE and object to represent that). There is nothing wrong with bootstrapping any dynamic models with the robot description, but they are a different thing.", "There was some discussion about moving the robot_description parameter to a latching topic. Fine idea. Outside the scope of a robot description format.", "The robot description is a storage/wire format. There is a point in time when all data retrieval and parsing is finished at which point there is a robot model in memory. What happens after that point is application specific. Declaring which end effectors are available is part of the robot description. Signaling the switching of tools or the mounting of a tool at t=0 is not part of a robot description.", "There has been some discussion on compose-ability, extensibility,  and whether a core description (or well defined sub-descriptions) should be used. Dependency hell issues were raised with regards to plugins, URI\u2019s and the like.", "I don\u2019t have anything new to add to that discussion. The core description thing seems to be an artificial issue created by assumptions about the responsibility of the tools using the description to their downstream users. The robot description should not care about those third party consumers. Any parseable and coherent subset of the robot description should be fine.", "Dependency hell is not a reason to disallow dependencies. If managing external connectivity or packaging of multiple elements is beyond your project, then you should keep everything in one file.", "There was discussion about xacro, template engines, and GUI editors. I think these are outside the scope of the robot description, beyond indicating a format that has good libraries available.", "One final observation is that while it appears that SDF is the easiest path, the SDF creators have a mental frame that constrains their thinking. Nothing wrong with that, but it\u2019s worth taking a careful survey of others using rigid body robot models. People deploying robots in experiments or the field. People transferring CAD or optimizer designs. I think most types of users have been participating in the discussions so that\u2019s good.", "In order to use SDF in ROS, there needs to be a way to do tf with graph (rather than tree) based kinematics, i.e. how to handle multi-parent reference frames with tf. This is brought up every time this is suggested and though some ideas have been discussed in person, there\u2019s never been a well formed proposal on how to deal with that.", "Specifically ", " and I have talked at length about spanning trees for the SDF graph and how they could be used to address this. Maybe ", " can speak to that point.", "I\u2019m really hitting a mental wall with why a graph topology is a problem. Which frames have multiple parents (or rather, how is it possible for a frame to have multiple parents)? Can you elaborate the problem?", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the order joints are in the file would be fine no?", "(EDITED to clarify the question)", "I\u2019m really hitting a mental wall here. Which frames have multiple parents? Can you elaborate the problem?", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the parent-child relationships would be fine no?", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "An impopular position perhaps, but personally I would really like to see ROS2 use an established scene/robot description format if possible.", "What formats do you have in mind?", "SDF is a good improvement over URDF already, but it\u2019s still a custom format, with almost no uptake outside the ROS-Gazebo universe.", "Open source projects are difficult to track. Based on word of mouth and interactions with other people I can say that simulators, such as Moby and Drake, use SDF. Organizations like FIRST, Robocup, and NASA use SDF. There is also a solidworks to SDF exporter, and AutoCAD has expressed interest in developing their own exporter. None of this means SDF is good, but are people who rely upon SDF.", "Can you clarify what you mean by \u201ccustom format\u201d and \u201cestablished format\u201d. To me it seems that a format created for a particular project (let\u2019s say SDF and URDF) can become an established format through general acceptance. This acceptance is an indicator that a format is worth using, or improving upon. Whereas a format created by an organizing body (let\u2019s say Collada) does not make the format good.", "Newer Gazebo versions have the model editor, but that is still limited.", "Yes, it is limited. The approach has bee to develop and release incremental improvements, rather than wait until a feature complete version is ready. We\u2019d love to have help with the model editor.", "All the formats seem to have arisen from fire-fighting small problems in small problem domains rather than intentional design.", "Just to clarify, both SDF and URDF have been carefully crafted by groups of people. It\u2019s difficult to foresee all potential use cases and problems. What may seem like fire-fighting is the normal processes of incremental improvements.", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "Yes, tf requires a tree representation, but many robot configurations can have cycles. It\u2019s also jsut a good idea to support representation of a graph as a tree.", "I would say the only other one we should consider is collada.  This is because it is an industry 4.0 standard.", "If SDF is it then we need push to get it as an industry 4.0 standard as well", "industry 4.0 standard", "What is involved in becoming an industry 4.0 standard?", "I have not looked into it.", "Paul Hvass or Shaun Edwards might know more about this.", "Gjis?", "I\u2019m guessing ", " is thinking of kinematic structures with cycles in them, like grippers with parallel / coupled links, delta robots, etc.", "Yes.", "Since it\u2019s a rigid body model it seems like any spanning tree would give you a valid results. The default spanning tree via the order joints are in the file would be fine no?", "One strategy would be to pick an arbitrary spanning tree (first in order, last in order, random, etc.). However, you could also let the user specify the spanning tree, something that ", " was pretty interested in doing. Also, when you are dealing with a distributed tf system you can easily get into a situation where two spanning trees can disagree. This isn\u2019t an issue when the description is being used in a simulation because the simulation ensures that the spanning trees agree (or at least it should) and it\u2019s able to do so because it is not distributed and it has a \u201cperfect\u201d model.", "So allowing the user to pick, with a reasonable default if they don\u2019t care, seems best. But there are a lot of details about how to represent this and communicate it in the ROS graph and in the API.", "There was some discussion about moving the robot_description parameter to a latching topic. Fine idea. Outside the scope of a robot description format.", "Both of the previous two points, by the way, are examples of why I have to disagree with your general sentiment that how the description is transmitted and used in context is out of scope. The description isn\u2019t used in a vacuum and I don\u2019t think it should be designed in one either. I do understand the desire for the format to be portable to different frameworks and therefore it\u2019s design shouldn\u2019t be unduly influenced by just one of those possible frameworks, but I think you can accomplish that while considering how it will be used if you\u2019re conscious of that fact.", "One strategy would be to pick an arbitrary spanning tree (first in order, last in order, random, etc.). However, you could also let the user specify the spanning tree, something that ", " was pretty interested in doing. Also, when you are dealing with a distributed tf system you can easily get into a situation where two spanning trees can disagree. This isn\u2019t an issue when the description is being used in a simulation because the simulation ensures that the spanning trees agree (or at least it should) and it\u2019s able to do so because it is not distributed and it has a \u201cperfect\u201d model.", "A spanning tree export is certainly possible, and something that I think we should explore. But a naive spanning tree computation will lead to significantly degraded performance of tools like tf. As mentioned, it needs to be consistent between all participants. For good performace the tree needs to be both deterministic and also consistent over time. If the tree changes topology you loose the ability to interpolate between time updates. There\u2019s also value in having the tree be similar in topology to the physical linkages since traversing fewer joints to compute a transform will result in less error accumulation.", "My opinion on what should be done in the short term has changed over the past few months. While I still find it annoying that Gazebo and ROS do not have a common format, I don\u2019t believe that switching to SDF for all ROS applications (or vice versa) makes sense, because:", "On top of these technical issues, there is the higher level decision of who decides what goes in the spec, and what the process is for that. One reason we have so many different robot description formats is that many of the interested parties prefer to have the flexibility to decide on their own what goes into a format. For TF/moveit/rviz/etc to share the same description format with gazebo (for example) people from all of those groups would need to have input into the spec. ", " would you be ok with changes to sdformat being chosen by a committee, where gazebo was only one of several participants?", "Longer term, I do think that defining a format (or set of formats) that are broadly used in the robotics community is extremely important, but I\u2019ll post some thoughts about that separately in ", "EDIT: Removed reference to \u201clong term section\u201d - I\u2019m going to post those thoughts on the next gen robot description thread.", "Switching to SDF doesn\u2019t provide most of the things that I want out of a robot data exchange format", "Do you mean the action of switching, or that there are a bunch of features/characteristics missing? If features, what\u2019s missing in SDF?", "I believe switching to any new format would require significant effort.", "\nlibsdformat has conversion from URDF to SDF, and the reverse is partially", "\ncomplete. This would make a transition less onerous.", "What are the items that SDF is missing?", "SDF elements are already chosen by committee. That committee happens to be", "\ngazebo developers. Interested parties are welcome to propose changes, and", "\ncomment on pull requests. Are you talking about a more formal committee?", " i mean the act of switching", " yes - switching to any new format would require significant effort, which is why I\u2019m suggesting not switching.", "I do mean that I the committee would have to be not just gazebo people. Yes, anyone can propose changes and offer input, but for something so crucial as a robot description format, I think that the make up of the committee that has actual decision making power should reflect the people who use it, and so if URDF switch to SDF, then i would expect the committee with decision making power for sdf to include people from developers for the various tools that now use URDF, as well as the developers for gazebo.", "To be clear I\u2019m not asking for that - I\u2019m bringing it up as an example of why I don\u2019t think we should switch from URDF to SDF.   I think that SDF is an excellent format created by engineers who know their stuff and  who have a clear application in mind that guides their decisions.", "I\u2019m still really struggling to see how the graph causes issues. Does anyone have any example cases? This feels like it\u2019s straightforward and a non-issue to me and given the people here I must be missing something.", "Here\u2019s my assumptions:", "\nLoading a graph into a tree with a hand written parser can be done with a brute force loop detection with just a few lines of code, and given the typical number of loops there is no performance problem. So no penalty for the little guy.", "\nIf one is working with a large graph structure that would cause a performance impact, then they have probably have the ability to write more elegant loop detection.", "\nAll spanning trees are equivalent and it never matters which one is given to TF.", "Where am I going wrong?", "A spanning tree export is certainly possible, and something that I think we should explore. But a naive spanning tree computation will lead to significantly degraded performance of tools like tf.", "How does the computation for breaking the loops in the graph interact with TF and change TF\u2019s performance?", "As mentioned, it needs to be consistent between all participants. For good performace the tree needs to be both deterministic and also consistent over time. If the tree changes topology you loose the ability to interpolate between time updates.", "What naive algorithms for breaking a graph into a spanning tree are not deterministic and not constant in time? Can you come up with some code that gives non-deterministic results when applied successively to the same graph or any expected evolution of that graph? Is this just a theoretical problem? (I understand a tree topology change causes a real problem).", "There\u2019s also value in having the tree be similar in topology to the physical linkages since traversing fewer joints to compute a transform will result in less error accumulation.", "How much error are you really going to accumulate (or save) on any robots in current (or in URDF2\u2019s future) use with TF, with a non-optimum spanning tree?", "I think that the rigid body assumption (or the mathematical nature of a frame) means that there is no spanning tree that is better than any other from a topological view.", "From a problem solving view there is a benefit if I can get the upstream tools to use the same spanning tree as my tools (maybe this is what you meant about matching topology to physical linkages?) and we have a solution for that in annotated graphs. I didn\u2019t see any proposals, but I would expect that a \u2018loop\u2019 element, or something similar would close the loop and all tools reading \u2018joint\u2019 elements would just naturally see the spanning tree that is the \u2018best\u2019 from the users view.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Switching existing tools that use URDF over to SDF would require a large effort. Tons of tools/libraries/etc use URDF, and switching would be non-trivial.", "Switching to SDF doesn\u2019t provide most of the things that I want out of a robot data exchange format", "As ", " and ", " have pointed out, TF and associated tools need to have a tree in order to work. There are solutions (provide a way to specify a root node for the tree, or to specify the entire tree), but my feeling is that this is only the first of many edge cases that would be run into.", "The existing tools for taking a robot described by a URDF, converting it to SDF, and spawning it in gazebo work \u201cwell enough\u201d for me.", "This isn\u2019t just a question of urdf/sdf. On top of those I have an SRDF for moveit, yaml files for describing other robot-specific formats that aren\u2019t in any spec yet, etc."], "url": "https://discourse.ros.org/t/urdf-ng-ros2-urdf2-discussion/511"},
{"title": "Turtlebot3 - Joule stalled", "thread_contents": ["Hi", "Turtlebot 3 - Waffle \u2026 Joule - after installing Joule following the official documentation (Ubuntu 16.04 LTS) : everything works fine\u2026problem: Joule stops working/running after approx 10min up - independently if operated on 12V/3A external power supply or with the battery provided with the Turtlebo3 kit. Logs don\u2019t show any weird symptoms\u2026", "Any ideas? pls help\u2026", "Tnx in advance", "\nml", "First thing to check, what is the temperature doing? Is the module overheating?", "Also, does anyone know what the replacement for the Joule is, now that they are EOL?", "\nThanks ", "Tnx Ilia for the hint\u2026checking continously temp via /sys/class/thermal zones 0-7 - max. 45C \u2026 assuming that isn\u00b4t the root cause. Keep on digging\u2026Best ml", "\u2026quick update: if you push the Power button (SW2) the \u201cstalled\u201d system recovers - even the Unix sessions\u2026", "\n\u2026did I miss something here - kind of \u201csleep mode\u201d /   hibernation mode - some BIOS settings?", "Interesting.  Our bot was doing the same thing, with symptoms just like a software crash, so that\u2019s what I assumed.  Maybe it is going into a sleep mode of some sort.", "I experience the same stalling that you describe.  Have you managed to figure out a workaround?", "Hey ashoed,", "unfortunately not - tried all 3 BIOS versions, including the recommended ", " \u2026 same negative results.", "Any luck on your side?", "Best ml", "Hi Michael,", "Have you checked to see if there are settings within Ubuntu 16.04 for screensaver or power savings?", "You should also check if you can update to the latest Linux kernel.", "If you can type  ", " in the terminal on your Intel Joule and post the output, I may be able to load up my Intel Joule with your versions and check to see if I encounter the same issue.", "Please keep us updated on any other efforts you may try.", "Hi MyNameIsCosmo,", "tnx for your support\u2026some progress but not 100% done yet:", "all updates based on \u201capt-get update\u201d installed", "HDMI not working - need to work with cli", "uname -a:", "\nLinux turtle 4.4.0-1000-joule #0+joule21-Ubuntu SMP PREEMPT Thu Mar 16 14:46:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux", "following your hint \u201cpower saving\u201d -> focussing on Ubuntu not Joule I did the following:", "sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target", "-> unix processes will not get stopped/suspended anymore -> seems ok !", "\n-> issue: every 30min the network manager gets a request to \u201csleep\u201d (syslog) -> wifi connection goes down", "\n-> can\u2019t restart it automatically (service network-manager restart -  nmcli network on - ifup -a)", "No clue what causes the request for network manager to sleep\u2026given that it is reproducible it seems a \u201cfeature\u201d. Any idea how to tackle it? Seems I didn\u2019t disable all \u201cpower saving\u201d options (or what soever causes the nm shutdown\u2026) - can\u2019t find anything on the net that fixed it\u2026", "Every hint/comment welcome\u2026", "SOLVED: guess the \u201cGUI\u201d environment triggered the request to sleep after 30min - given that I run the Joule headless: disabling \u201cGnome\u201d and \u201clightdm\u201d did the trick - system up and running for hours (didn\u2019t reverse the masking of sleep, suspend, hibernate, hybrid\u2026)", "\nThank to all of you for the hints - Best Michael", "Had the same issue, disabling gnome and lightdm did the trick! Thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-joule-stalled/2739"},
{"title": "OpenCR and encoders", "thread_contents": ["I want to use my OpenCR board to interface with two quadrature encoders for odometry and wanted to get your thoughts on different approaches.", "I did some searching and the STM32F7 is capable of reading quadrature encoders (I\u2019m not sure how many yet) but this may conflict with the current firmware settings on the board.", "\nAnyone know if the Low-power timer (LPTIM1) is being used for anything?", "\n", "My other option would be to use external counters and interface to them via SPI\u2026", "\n", "Other methods I should consider?", "I\u2019m leaning towards the external counters but  is there anything I\u2019m missing that would interfere with the above shield?", "Any suggestions would be greatly appreciated!!!", "Have you looked at the Turtlebot3 source? You can find it in the OpenCR code. The turtlebot3 uses encoders for odometry. It uses dynamixel\u2019s sdk, which you can definitely find the source for.", "I have slowly been working my way through the source, but haven\u2019t made it to the odometry section yet. The Dynamixel motors are \u201csmart\u201d servos and perform their own position/velocity control and just report this info back the the Opencr board.", "Sorry I wasn\u2019t very clear with my plans. I want to use the Opencr board exactly as the TurtleBot3 does, but swap out the Dynamixel\u2019s for larger motors with encoders. This will require the Opencr board to take on the servoing of the wheel motors and calculate odomerty on its own. I feel comfortable modifying the source to do this but wanted to see if anyone had advice or suggestions on adding encoders to into the mix.", "I have experimented with the STM32 processor and did want to use it\u2019s builtin ability to read quadrature encoders.   I had to give up because of a conflict over timber use.", "But I did find the CPU has zero trouble at all with VERY high interrupt rates  I have four motors each spins up to about 11,000 PRM and has 64 pulses per revolution.   The processor can handle the maximum rate and still do quite a lot else.  that would be (4 x 11,000 x 64)/60 interrupts per second.    That said the interrupt handler in C++ is very short and fast.", "You can further speed  up the  processing by a large factor if you remember the last time the motor changed directions and then you can ignore that the encoder is using quadrature.  And your handler becomes \u201ci = i + direction\u201d  Just one statement.", "So the simplest way to to do the encoder handing inside an interrupt handler in software.", "But if you can use the STM32 counter hardware that just seems so much more efficient but in my case I ran out of timers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/opencr-and-encoders/2335"},
{"title": "Cameras with 360\u00ba FoV in ROS", "thread_contents": ["Hi,", "I noticed that more and more consmer-grade cameras with 360 FOV are available (Samnsung Gear 360, Nikon KeyMission, kodak pixpro 360, etc.)", "Wonder if anyone successfully managed to do live streaming from these cameras in Ubuntu and to use them in robotic application.", "Cheers", "Davide", "Hi,", "Disclaimer : self-promotion ^^.", "A few years back I was playing with a 360\u00ba imaging using two uEye camera UI-3240CP \u2013 IDS-Imaging each mounted with 185\u00ba FoV fish-eye lens.", "\nYou can find short videos ", ".", "\nSince then plenty of nice camera got to the market as you mentioned, unfortunately I didn\u2019t get to play with any of them.", "Cheers.", "I\u2019m not sure about those cameras, but Econ Systems makes a rig for the TX1 that supports 6 cameras to provide 360FOV.", "I can only talk about the Kodak PIXPRO SP360 (and 4K). They work as normal USB cameras. But connecting them via USB doesn\u2019t give them enough current to keep on forever (they drain more than they charge). Maybe fixable with some kind of powered USB cable?", "My little work can be found here: ", "I have a short experience with SP360 (firmware version 1.0.5). I think it is important to highlight the differences of SP360 and SP360 4K since AFAIK only the SP360 4K version appears as V4L2 device.", "The USB interface in SP360 serves only for storage access purposes. The only way I have found to stream the live image is setting the cam in WiFi mode and sniffing the MJPEG stream. Some instructions on that can be found ", ". Since this stream is actually meant for live preview on a mobile app, the quality and resolution is far from the cam is actually capable of. Although the cam has an HDMI output, it is only enabled in playback mode and does not provide live feed.", "The SP360 4K has a webcam functionality and enables live stream to HDMI (now the external capture cards are also an option).", "Hi,", "does anyone know if the ", " is compatible with the ROS this node or at least with Linux?", "I think ", " was being too modest. Not only did he get the Kodak 360 camera streaming, he wrote RViz plugins to blend and display a spherical image (requires 2 cameras, ~1 hemisphere each) and  use a VR headset.", "Spherical image:", "\n", "VR:", "\n", "Hi, we are using ", ".", "\nWe can use ", " package to obtain image via USB streaming.", "Hi!", "Is the RICOH Theta S working well with the UVC driver on Linux?", "\nWe bought a RICOH Theta V only to realize that it is using the UVC1.5 driver which is not supported. I can\u2019t seem to find any other confirmation besides your post about the compatibility of the \u201cS\u201d model, using UVC1.1, and it would be great if you can confirm that it is working fine before we buy another camera ", "\nThanks!!!", "Hi,", "We are using RICOH Theta S with libuvc_camera, using UVC1.1 (Motion Jpeg mode).  The frame is a  combined 1280x720 image at 14fps. I think you can use other package like gscam or usb_cam.", "I didn\u2019t know about RICOH Theta V, but do you mean you cannot make Theta V work with current driver and packages? I expected V model is upper compatible to Theta S\u2026 but checking the spec sheet now, it seems support only H264 streaming\u2026 (it means we need UVC1.5 or higher, which is not supported by libuvc ", ")", "Thank you for the confirmation! We will go ahead with the S model then.", "I will check periodically what the status of support for uvc1.5 is and post it here when I can make the V model work. I assumed compatibility as well, but they dropped support for 1.1, which is my bad, I should have checked before.", "I seriously doubt my boss will allow me to allocate time to develop the driver, but if it happens I will post the news here.", "It\u2019s wonderful if we can use UVC1.5 and H.264 encoding. I\u2019m waiting for good news!", "what do you mean with combined 1280x720 image, the equirectangular 360\u00ba panorama?", "Also, how does the integration work? Can I plug it to a computer and have the computer handle everything ( turning it On/Off, modes, data handling, etc)", "Has anyone had any success running the Theta S or V with UVC1.5?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/cameras-with-360-fov-in-ros/1833"},
{"title": "New ROS-compatible 3D sensor based on stereo vision", "thread_contents": ["Nerian Vision Technologies has released SceneScan \u2013 a new ROS-compatible 3D sensor system that is based on stereo vision. Unlike conventional depth cameras, stereo vision is a passive technology, which also works robustly in bright daylight and over long distances.", "SceneScan is an embedded system that uses a powerful FPGA and state-of-the-art image processing algorithms to convert the imagery of a stereo cameras or two conventional cameras into a depth map or 3D point cloud. This can be done at frame rates of up to 100 fps and image resolutions up to 3 megapixels.", "Together with SceneScan, Nerian also released the new Karmin2 stereo camera. Both devices work optimally together and form a fully featured 3D depth sensor system.", "You can learn more about SceneScan at:", "\n", "More information about Karmin2 is available at:", "\n", "A ROS node for SceneScan has already been released and is available from the ROS package servers. Details can be found in the ROS wiki at:", "\n", "Really looks like a very nice product, the fact an FPGA is embedded to do the processing really is awesome!", "\nAs usual, I\u2019m always disappointed to never see a public price tag on these products.", "Your ROS package looks clean and you are providing clear/read-able documentation/examples. I\u2019m pretty sure many people will be interested in your sensor because the integration time in a ROS application is close to zero.", "Best of luck to you with this product!", "Yes, prices are available upon request only, but we do have a competitive pricing scheme. Prices do vary depending on the configuration. A full system consisting of SceneScan, cameras and optics is already available at well below EUR 3,000.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-ros-compatible-3d-sensor-based-on-stereo-vision/3251"},
{"title": "[TB3] TurtleBot3 with Laser Distance Sensor (LDS)", "thread_contents": ["Hi everyone ", "\nTurtleBot3 release come just next month.", "\nAre you curious about the LDS on the TurtleBot3?", "LDS can be rotated 360 degrees.", "\nIt is a key part in create slam and navigation.  even create 3D map!!!", "\nAnd less than half the price of a lidar sold on the market. But the functions are almost the same.", "You can see it soon. ", "\nThank you ", "This is the cheapest 360 degrees LiDAR in the world I know. ", "\nFor more information; Please see the detailed spec on TurtleBot3 wiki.", "\n", "The design reminds me of the RP-LIDAR (", ").", "Can you comment on expected uptime? E.g., if I begin to operate the sensor and do not stop, how long before the device may need to be rebooted?", "I am motivated to ask because I noticed recently that after approximately 3 days of continuous operation of an RP-LIDAR that I have, the motor stopped and would not return to operation until after I cycled power. I plan to conduct a more thorough study of product lifetime and durability of the RP-LIDAR.", ", do you have any information on the cost?", "I think it is hard to expect long operation life in low-cost LiDAR.", "\nI will ask the engineer about the expected time for the operation time. ", "After the launch of TurtleBot 3, the products for each accessory are going to start selling in June.", "\nThe price is still undecided, but we think it is under $150. ", " did you get any data about expected lifetime under continuous operation?", "The following attachments include the contents like basic performance, measurement performance, mechanism layout, optical path, data information, pin description, command. But, I can\u2019t find information about the lifetime from the manufacturer.", "14.56 KB", "Ask you engineers what is the mean time between failures (MTBF) data they should have and hopefully share.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tb3-turtlebot3-with-laser-distance-sensor-lds/1644"},
{"title": "Building a better TurtleBot3", "thread_contents": ["Hi everyone-", "I\u2019ve written a ", " about TurtleBot3 assembly and component placement. If you\u2019re just building your TurtleBot3 take a look and let me know what are some of your own suggestions related to component placement.", "Thanks in advance", "Hi Dragan,", "I\u2019ve been setting up a TB3 burger this week with ROS2. I was wondering if you would want to chat for a minute about how the experience has been for you.", "Hi ", ",", "can you jump on IRC on freenode? A bunch of us from Canonical are in ", "Hi ", "\nThis is a very impressive article. We seem to be able to use ROS more easily in a variety of ways with a little more thought. ", " Thank you very much and I will consider how to make it easier for users.", "Glad more people are using tmux/screen for their bringup sequences ", " definitely helps.", "Hi ", "-", "thank you and feel free to ping me anytime you want to bounce ideas around.", "Thanks", "Hi ", "-", "yes it most definitely helps and the tmuxinator scripting is a great addition. There a couple of tmux extensions that also help with session save and restore, so if you modified a session you don\u2019t lose that when you come back\u2026 Definitely a great tool.", "For interested parties search for tmux-resurrect or tmux-continuum plugins on github", "A few things that I found missing is about upgrading the default computer which is raspberry pi to a more powerful one such as Intel NUC or NVIDIA Jetson TX. In order to do processing from depth and tracking cameras.", "I have myself tried to interface NUC directly via USB to OpenCR. But, it only works after I re-upload the Arduino firmware from the master branch of OpenCR.", "But, it behaves slightly different than the latest binary release for raspberry pi. Also, each NUC and OpenCR requires its own power in order to move the dynamixel motors. As the NUC doesn\u2019t seem to supply enough power via the USB.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-a-better-turtlebot3/10533"},
{"title": "Waffle - HDMI no signal", "thread_contents": ["Hello,", "I need help installing Ubuntu on TurtleBot3 Waffle.", "\nI flashed Joule BIOS with version 193 (since 1J2 didn\u2019t worked and seemed a lot of people had trouble with it).", "\nLooked good :", "However after that I can\u2019t get any screen with micro HDMI-HDMI cable, still no signal.", "\nI plugged power supply one openCR, keyboard, mouse and USB with Ubuntu on USB hub (which is connected to the Joule). All leds are green, plus the white/blue one.", "Any suggestion ?", "\nThanks", "Hello r00t,", "\nThe below link is the method that worked for us to recover the joule.", "\nIt seems like the sequence of connecting and disconnecting cables do matter.", "\n", "Thank you for your reply.", "Tried everything, still can\u2019t get it working.", "\nFirst connecting type C USB, first connecting power supply, waiting 10min first boot before connecting HDMI, really everything. With both bios version 193 and 1J2. Even re-done exactly like in the video tutorial. Did it with two different Joule board (we got three of them), same thing.", "Flashing is always fine, but can\u2019t get that output working.", "Anyone with an other idea ? (Hardware is fine, test moves worked)", "According to McCool, sometimes specific monitors would not connect to the Joule.", "\n", "\nWere you able to connect them with your current monitor before updating firmware?", "Yep, it was working before updating\u2026", "I\u2019m trying to reproduce the problem with my Joule and if I figure out what the problem is, I\u2019ll share the solution for it.", "Well, I took my chance installing directly the \u201cAlternative Ubuntu for Joule\u201d on a second Joule not flashed (since this way I did have acces to BIOS to boot on USB), and it works properly\u2026 (ROS install and turtlebot teleop working with no problem)", "\nSeems weird 'cause there must be a reason the guide tells us to flash before doing it, but i am not complaining, it works.", "I\u2019ll keep trying make the first one work, and come back to you if I find a solution.", "Thanks ", "Hello r00t,", "As far as I understand, BIOS 193 is required in order to install Ubuntu with a USB flash drive. Since it was one of requirements from Ubuntu Developer(", "), I didn\u2019t doubt about updating the BIOS firmware.", "\nAt least you figured out your solution with the second Joule which is definitely a good news ", "\nPlease let me know if your second Joule has different BIOS version other than 193(", ") so I can also try with the same version.", "\nCheers!", "I was able to use joule-firmware-2017-06-26-1J2-public, specifically the debug bin file.  My initial flash with the release file would not boot, nor display anything on screen.  With the debug bin the board does run, although bootup times are very slow.", "That said, the Joule board has proven to be unstable.  I\u2019m unsure on the root cause, be it BIOS, Ubuntu, or hardware, but since Intel has discontinued the board I don\u2019t expect much in the way of software improvements.  At present, I\u2019ve swapped over to a Raspberry Pi, and have an Odroid XU4 which will eventually end up in the Turtlebot.  I\u2019ll go back to the Joule at some point, but it\u2019s not a long term solution for development.", "Hello,", "\nI got the same problem. The board was displaying the bios using hdmi but when I flashed bios version 193 I could not get hdmi output anymore. The flashing process didn\u2019t give any errors. I also tried newer bios versions with same negative results.", "\nI can connect to the board using serial connection. It seems to recognize the display when I execute xrandr -q. If I reboot the board it displays the whole boot sequence. These are the last commands before stopping:", "[   12.378641] Bluetooth: RFCOMM ver 1.11", "\n[   12.820056] HDMI HDA Codec ehdaudio0D1: HDMI: failed to get afg sub nodes", "\n[   12.827761] HDMI HDA Codec ehdaudio0D1: Failed in parse and map nid with err: -22", "\n[   12.836166] HDMI HDA Codec: probe of ehdaudio0D1 failed with error -22", "It seems to be some sort of hdmi error. Any idea how to correct the issue?", "\nThanks", "Just a data point.", "\nMy Joule running 193 works with my Dell and Samsung monitors, with a generic type A to type A HDMI cable, and generic type A to type D adapter.", "\nIt won\u2019t work with my Elecrow monitor, its included type A to C cable, and the above generic type A to type D adapter", "\nIt will work with my Elecrow monitor, its included type A to C cable, and a Rocketfish type A to D adapter.", "Ed", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Downloading BIOS", "Clearing NvStorage\u2026"], "url": "https://discourse.ros.org/t/waffle-hdmi-no-signal/2687"},
{"title": "Don't miss EU funding for ROS developments via ROSIN FTPs - Next cut-off dates: June 14 & Sept 13, 2019", "thread_contents": ["The EU H2020 ", " has the goal to advance open-source robot software for industry and the robotics community as a whole. One of the main activities of the project is a grant program with a total amount of ", " for Focused Technical Projects (FTPs) on ROS software development.", "Sofar, we have funded almost 40 FTP projects. Many (intermediate) results can be found here:", "\n", " Do you have a good idea for a ROS/ROS-Industrial related project and want to work on ROS(-I) software components, documentation, standardisation or a related topic?", " Are you or your company located within the European Union (or any of the ", ")?", " Then submit an FTP proposal and apply for a ROSIN grant by following the steps outlined in the ", ".", "FTP proposals may be submitted all year long but are evaluated 3-4 times a year.", "The next cut-off date are: ", " .", "Proposals are short (a few pages) and concise. Applicants are guided through the process by an application wizard and a guide is provided.", "To submit your FTP, please visit ", ".", "If you have any questions about the process, whether your idea or project would qualify, send me or one of my colleagues a message either through ROS Discourse or by email (see the ", " on the site for addresses).", " Too much text and no video?", "\n", " Watch our project coordinator explaining ROSIN project and it\u2019s funding opportunities at last year\u2019s ROSCon in less than 200 seconds", "\n          ", "\n", "or follow the 30 minutes presentation from ROS Industrial Conference (followed by four 15 minutes presentations of FTPs already funded by ROSIN):", "\n            ", "\n", "As a result of discussions at ROSCon2018, ROS-Industrial Conference 2018 and exchange of ideas in early 2019, we would like to solicit \u201c", "\u201d (or pre-baked FTPs) for upcoming ROSIN FTP calls in June and September. These suggestions shall give a focus on the topics we fund with our EU funding, but is not meant to not limit the scale and scope of future FTPs. Please check (upcoming) ", " from more than 38 ROSIN FTPs, to see what projects are in the pipeline or have been finished.", "\nWhich manipulator is suitable for a specific task? What is the ideal location of an object (e.g. to be polished) within a workcell for a given manipulator? Compare different solutions (in terms of computation time, energy consumption) for a specified task. Currently within MoveIt! it is possible to benchmark OMPL planners only ", "\nCurrently, mobile bases are approximated with floating joints within MoveIt! and it is not possible to execute these trajectories. Only other possibility is to plan for the arm (MoveIt!) and the mobile platform (navigation stack) separately. ", "\nAlthough FTP in similar direction has been approved (Move-RT): ", "\n(MoveIt!'s transition to ROS 2.0)", "\nMake MoveIt\u2019s interfaces more generic which will simplify integration of standalone components (kinematics and collision checking). and also external planners (Descartes) and optimizers (TrajOpt).", "\nKinematicsBase API", "\n", "\n", "\nCollision checking API", "\n", "\nAllow for \u201cpartial\u201d SRDF definitions to use arms with a combination of end-effectors / mount objects (tables, mobile base), similar to XACROs? Have a unified robot description (URDF, SDF, SRDF, COLLADA). Multiple formats introduces repetition of link names, etc.", "\nTiming and memory analysis tools for ROS nodes.", "\nAlthough some tools have been mentioned in the link: ", " ,", "\nit may not be the best solution for multi threaded, multiprocess applications", "\nEspecially, (most commonly used) Valgrind seems to be mainly for single-threaded applications and runs on VM, which distorts the actual results.", "\nA tool by OSRF (", ") has not been updated since 2014.", "\nHardware-in-the-loop / model-in-the-loop testing", "\nPackage versioning (possibly a REP)", "Kind reminder for the European ROS Community.", "\nOur next cut-off date is approaching.", "\nWe will be happy to evaluate all FTP proposals that are submitted before ", " end-of-day.", "\nNext cut-off is then on September 13, 2019 .", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/dont-miss-eu-funding-for-ros-developments-via-rosin-ftps-next-cut-off-dates-june-14-sept-13-2019/8999"},
{"title": "Installing Ubuntu 18.04 arm64v8 on Raspberry Pi 3", "thread_contents": ["Would anyone else like to see a arm64v8 image of Ubuntu 18.04 image for the Raspberry Pi 3 Model B? I gave a good stab at it Sunday afternoon, but got a little lost in the weeds given its been awhile since I made a 14.04 image for my old Raspberry Pi 1, and trying to catch up with the changes of things.", "I have a majority of a one-line install image build script, but just can\u2019t get over irregularities with ubuntu\u2019s server install scripts and patching grub bootloader. My hacked together script thus far:", "\n", "Clobbering of bootstrap scripts for setting up raspberrypi images - ruffsl/raspberrypi_bootstrap", "\n", "My goal was to make a Ubuntu image I could use to test the arm64 ROS2 Boncy binaries on, and do some profiling and testing on; as I didn\u2019t feel like building or cross compiling ROS2 myself. But this has sort of turned into a deeper rabbit hole. Figured I\u2019d share my stalled progress with the embedded folks here, or find out if anyone else already has a working image.", "Other recent related discussions/wikis:", "\n", "\n", "I actually did spend some time getting ROS2 working on the Raspberry Pi 3b+ a few months ago.  I took two different paths to get there:", "Hope this helps a bit; let me know if you need additional information, since I elided my detailed notes here for brevity.", "Any reason to use ubuntu instead of debian ?", "Otherwise there is a debian arm64 for raspberry pi 3 :", "\n", "A 64-bit OS for the Raspberry Pi 3. Contribute to bamarni/pi64 development by creating an account on GitHub.", "\n", "\nI do not test it for the latest pi 3b but there is a fork for it.", "The installation of the OS is straight forward. Then we can install ros2 directly from the ros2 repo :", "\n", "The Robot Operating System, is a meta operating system for robots. - ros2/ros2", "\n", "If it is on any helps\u2026", "This  ", " Xubuntu 18.04 installs and works pretty fine on Raspberry Pi 3 B+", "Follow the instructions here: ", "\n", " The installer is designed to be run from a flash usb drive and installed to another device (SD card, hard drive).", "To make it lightweight, you will have to manually remove all the packages that you don\u2019t need.", "I built a ROS 1 image for Turtlebot 3 on Gentoo that was 64 bit a while back, I\u2019ll link here once I dig up the post.", "I think ", " was attempting to make an Ubuntu image, though, and was hoping for 64 bit.", "I\u2019ll say, however, that 64 bit arm is not necessarily a great thing on the raspberry pi\u2026 The main benefits: you get 16 more registers than 32 bit mode and you get better compiler optimizations (the pi 3 has vectorization, ", "), but you ", " have the < 1 GB of RAM which makes doing most things in a 64 bit system quite difficult\u2026 I think this is one of the reasons the raspberry pi people don\u2019t use 64 bit mode by default.", "Don\u2019t get me wrong: I use my raspberry pi 3 in 64 bit mode, have compiled ROS on Gentoo, and have been able to do a full GMapping demo with Turtlebot3. It\u2019s not useless, and it\u2019s not painfully slow. But doing any memory-intensive thing is awful.", "I have to concur. I don\u2019t see the necessity for 64bit on the Pi3, the downsides outweigh any perceived benefits. FWIW there is also a semi-official Ubuntu Core 18 ", " but for the reasons ", " and more have listed, we haven\u2019t pursued an official release.", "Yes there is need for 64 bit Ubuntu for RPI.", "64 bits are needed because every one around will not continue to develope 32 bit platform version just because there is one 32 bit board that may use it\u2026", "\nRPI is not the only microcomputer with GPIO, there are many \u201cfruit\u201d like boards, that work better, with better hardware, more RAM, possible SSD\u2019s and so on\u2026", "I had to choose another board, because making Ubuntu 64 18.4 server working on rpi is problematic and not much straight forward.", "\nFor my project and production devices we use Odroid H2 , without fighting we can use any distro, and really powerful hardware.", "So if there would be \u201ceasy way\u201d  ready to flash image i would use it. And just check how many people is looking for Ubuntu 18 64 bits server for ARM on forums\u2026 really many.", "For future reference or for others who end up finding this thread, the beginning of this week Ubuntu released preinstalled server images for Raspberry Pi, including a 64-bit ARM image for Raspberry Pi 3.", "The preinstalled-server image allows you to unpack a preinstalled version of Ubuntu onto a target device.", "Additionally, the ubuntu wiki for Raspberry Pi has also been updated to include some more notes and setup guides for hardware acceleration, desktop GUI installation.", "These are not Ubuntu Core images, but the \u2018classic\u2019 deb based image.", "I\u2019ve just tested the ", " image on a Raspberry Pi 3B. Installing and running ROS2 Crystal from the released debian binaries for arm64 from OSRF are working well. The ros2 cli is a little slow, but that just seems to be python on arm in general. I\u2019ve starting testing out turtlebot3 ros2 packages from source, such as the hls_lfcd_lds_driver, and it\u2019d been smooth sailing.", "The only hiccup thus far has been the bluetooth. Thankfully the broadcom brcmfmac drivers for the wifi that come preinstalled are working fine with ", ", but the ", " package seems to be missing. While one can ", ", I\u2019m having trouble maintaining a pared connection using bluetoothctl and a ps4 controller for ros-joy due to missing dependencies. ", " , would you know where I could ticket about the ", " package?", "Hey ", ", sorry for the delay, I was out for a few days. I asked Foundations about this, and they said to please log it right ", ". It is a known issue, but they would appreciate a bug. If you wouldn\u2019t mind linking here once you\u2019ve done so, I can give it directly to them.", "If you wouldn\u2019t mind linking here once you\u2019ve done so, I can give it directly to them.", "I\u2019ve created a bug report for it here:", "No bluetooth on 18.04 for raspi3 arm64", "\n", "I\u2019ve been using this image on some Raspberry Pi robots:", "\n", "(Update 11/2/2018) Updated image with 2018\u201310\u201309 Stretch Lite Raspbian\u2026", "\n    ", "\n", "But it\u2019s a somewhat crippled version of ROS, e.g. no tf and no repo to apt-get install binaries of ros-kinetic-* packages. Building all those fairly standard things from source takes ages on a Pi.", "Also if the above image is installed on a Pi Zero W, the Wi-Fi doesn\u2019t work.", "I would definitely love to see an 18.04 image with full-blown ROS ", "I believe for bluetooth support, you can use the packages from raspbian, ", " (some other packages may be needed). This is how the Ubiquity Raspberry Pi image works, albeit for Xenial armhf.", " Ubiquity Robotics maintains a Raspberry Pi Image with Ubuntu 16.04 + ROS Kinetic available here ", ". You might be interested.", "Rohan", "\nUbiquity Robotics", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["I recompiled all of ROS2 on the 32-bit Raspberry Pi distribution that is the default for Ubuntu 18.04.  This worked fine, with the caveat that you have to build everything sequentially since some things take too much memory.  I ended up using ", ".  The big downside to this approach is that the ROS2 CI doesn\u2019t regularly build or test against 32-bit ARM, so things may not work exactly as expected.", "I switched the Ubuntu 18.04 distribution running on the Raspberry Pi 3b+ over to 64-bit, and then was able to install ARM64 packages.  Unfortunately, this is a bit of a process (it took me about 2 hours to do).  The basic steps are:\n", "Follow the instructions at ", " to install Ubuntu 18.04 32-bit on the Raspberry Pi 3b+ (this is a bit of a process on its own, but just follow through that guide)", "Once you have successfully booted the 32-bit distribution, we can start the steps to switch to 64-bit.  The basic instructions for switching the processor mode and kernel over to 64-bit are also at ", "\n", "After rebooting, you\u2019ll be running a 32-bit userland on a 64-bit kernel.  To switch to a 64-bit userland, you need to manually install all of the ", " versions of the packages and removing the equivalent ", " versions.  I ended up starting with leaf packages and making my way towards core packages, but it may be easier to start the other way around.  In any case, after dealing with some minor fallout from doing that, and rebooting, I had a full 64-bit kernel and userland that I was able to install ROS2 packages arm64 packages on.", "\n", "Please find Mongo DB that is made for ARM and 32 bits - hard to find ? well \u2026 32 bit version was killed a yer or two ago, so if You need good noSQL database You need 64 bit system, same with many other tools.", "\n32 bits are going to go to heaven not because rpi needs it (with 1 gig ram it does not)."], "url": "https://discourse.ros.org/t/installing-ubuntu-18-04-arm64v8-on-raspberry-pi-3/6094"},
{"title": "[TB3] The Turtlebot3 Teleoperation Example", "thread_contents": ["Hello ", "The Turtlebot3 would be teleoperated by various devices.", "\nWe tested it using several wireless devices e.g. PS3, XBOX360, ROBOTIS RC100, etc.", "\nThis example is operated by ROS on Ubuntu mate 16.04 with Raspberry Pi 3 (except that it tested by LEAP Motion) and OpenCR which controlls Dynamixel XM-430.", "See you soon with next video ", "Darby Lim", "I would like to use my upcoming Waffle as a part-time telepresence robot, controlled over the internet. This is specifically so that a housebound friend of mine can go to a science fiction convention. This couldn\u2019t just be a standard remote-controlled robot-with-a-camera and face. Science fiction cons are usually extremely busy and even real people have problems not running into other people, especially in the dealer\u2019s rooms.", "The Waffle would be modified with a few more avoidance sensors and a tall head. This should make it approximately 3.5-4 feet tall.", "I am assuming that the avoidance routines would have to be done on-robot and that my friend would be able to \u201csteer\u201d the Waffle with a goal and the Waffle would choose how to implement this goal.", "I\u2019d use a larger lithium battery so that it could last at least an afternoon or day.", "It would go along with me (or I along with it) so that it wouldn\u2019t get \u201clost.\u201d", "I\u2019m also assuming that I might have to run the Internet off of a cell phone rather than wifi so that it might have lower bandwidth and higher latency than normal.", "Has anybody made a telepresence robot specifically for these conditions?", "Hi ", "Thank you for your interest in TurtleBot3", "\nYour project is really nice! We expect a lot of people like you to transform TurtleBot3 into the robot they want.", "So We have already make some friends of TurtleBot3. I think that your robot is simillar as ", "This friend are help your project ", "Thanks.", "Thank you for the suggestion.", "I\u2019ve looked at Carrier and it doesn\u2019t seem to have the sensors that I need. It needs the capability of traveling on uneven floors as well as perhaps some grassy areas.", "Maybe a combination of Carrier and Monster. It must be extremely stable because I\u2019m sure it will be bumped a lot in a crowd. Think of any kind of crowded situation where there are small tables and thin maze-like passageways. That is an SF con\u2019s dealer rooms.", "And to make human recognition more difficult, many of the people will be in costume and there will be other robots there.", "Part of the reason for the head is that I can put additional sensors there to locate things that a floor-level sensor would miss. The other reason is so that people will think of this device as a person. Another reason is so that the human on the other end of the link can see through the camera on the head.", "I might need to plan on making the head a variable distance high. For portability, it might need to be short, but for telepresence, it might need to be tall. I\u2019ll have to see if the camera I\u2019ve ordered can see in a wide enough field of vision or I might need to make the head more complex than I\u2019d like for now.", "Just thinking out loud.", "Hi Routiful", "I wanted to buy the waffle but it is out of stock.", "I would like to build the \u201ccarrier\u201d do you have the complete partlist\u2026 the list i found here doesnt seem complete ", "And do you have the instruction for building the carrier?", "Cheers", "\nian", "Hello ", "  ", "Thank you for your inquiry.", "\nTB3 waffle was sold out last month. ", "\nBut you can buy it next year!!", "We don\u2019t officially support instruction for building series of TB3 Friends. However, you can refer to ", " in OnShape.", "Thanks", "\nDarby", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Related packages and applications", "\nros-kinetic-teleop-twist-joy", "\nros-kinetic-teleop-twist-joy-drivers", "\nros-kinetic-joy", "\nros-kinetic-rosserial-python", "\nros-kinetic-teleop-twist-keyboard", "\nros-kinetic-wiimote", "\nros-kinetic-leap-motion", "\nturtlebot3_core (developed)", "\nlearning_wiimote (modified)", "\nrosleapmotion (modified)", "\nOpenCR Arduino", "\nAndroid App (ROS Teleop)"], "url": "https://discourse.ros.org/t/tb3-the-turtlebot3-teleoperation-example/865"},
{"title": "How to setup environment to test latency of publisher and subscriber scenarios", "thread_contents": ["I want to setup an environment where I can check latency of different publisher and subscriber scenarios in different architectures.How can I do it??", "Here is a recent 2018 paper from Robotics and Autonomous Systems that touches on this topic:", "Robotic software frameworks simplify the development of robotic applications. The more powerful ones help to build such applications as a distributed \u2026", "In order to reproduce the network scenarios described before (localhost, Gigabit ethernet and WiFi networks), we take advantage from Linux NetEm [46]. This module allows the simulation of network delays. Moreover, we are able to simulate bandwidth and packet loss.", "[46] S. Hemminger, Network Emulation with NetEm, in: Linux Conf Au, 2005. URL ", ".", "Looks like they\u2019ve also released there code for the experiments here:", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-setup-environment-to-test-latency-of-publisher-and-subscriber-scenarios/4167"},
{"title": "Claiming maintainership for ros-perception/{pcl_msgs, perception_pcl, ...}", "thread_contents": ["Hi,", "Long time stalker of ros-perception, fairly new maintainer on image_pipeline here.", "There\u2019s an effort going on to get PCL items in ROS released that have stalled out mostly from PRs going un-reviewed and unmerged.", "Discussion here", "\n", "\n", "Examples", "\n", "ROS package containing PCL-related messages . Contribute to ros-perception/pcl_msgs development by creating an account on GitHub.", "\n", "\n", "\n", "While I could ask for just maintainer access to these repos, in practice I want to take on some maintainership of the larger ros-perception ecosystem as I have backgrounds in nearly all the packages there and I\u2019m taking the attitude ", ". I\u2019m not necessarily promising that I can port everything, but I am committing to getting reasonable PRs submitted reviewed and merged, particularly as relates to ROS2.", "So I\u2019d like to request to be added to the ros-perception org with write access. Any projects with active maintainers I don\u2019t plan on getting in the way.", "Pinging here, as I\u2019m trying to release PCL msgs into Dashing / Eloquent. ", " can you bump the versions on the ", " branch in pcl_msgs so I can continue with the release here (", ") in the meantime while I\u2019m waiting on this to be discussed above?", " We want to let the maintainer of pcl_msgs have a chance to respond and/or do the releases themselves.  In this case, it looks like you\u2019ve already opened ", ", so we\u2019ll see if the maintainer responds and go from there.", "Sounds good!", "(Something something 20 characters)", " pinging back, its been 2 weeks. We want that PR into pcl perception and release the messages for eloquent.", "Update: Awesome we got pcl_msgs shipped thanks to ", ". Off to perception_pcl", "Steve", "Hi ", ", as I\u2019m sure you\u2019ve picked up, my attention span for perception_pcl maintainership has been low. I\u2019m not spending much time on perception ", " ROS2 at the moment. If you\u2019ve got the time and motivation to help review and clean up the ROS2 porting/releasing/etc, I\u2019d be happy to tag you in.", "Sure thing, I\u2019d like to help get this out!", "Could the powers that be (", "?) please add ", " to ", "?", "Done! Happy maintaining!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/claiming-maintainership-for-ros-perception-pcl-msgs-perception-pcl/10813"},
{"title": "ROS2 Embedded SIG meeting #4", "thread_contents": ["Hi all,", "The next Embedded SIG meeting will take place next July 29, at 18.00 CEST.", "We have created a pull that will serve as the basis for discussion in the meeting:", "We will use google hang out for the meeting: ", "Great, I would be there on this time.", "thanks,", "In the meeting on July 29th at 4pm UTC / 6pm CEST / 9am PDT, we also plan to discuss about", "widening the scope of the working group (the poll shall serve as a basis for that discussion)", "introducing a regular schedule (e.g., every four weeks on a fixed weekday)", "Regarding technical topics: We will provide an update on the micro-ROS Dashing Diademata support (cf. ", "). Other technical topics are warmly welcome! Let\u2019s use this thread to compile the agenda together.", "There were questions about the scope of the Embedded SIG. Correspondingly, n this meeting, we will put a big emphasis on this topic.", "Therefore, ", ". Also, please again look at the poll: ", " and try to answer it before the meeting.", "If you\u2019re missing something, please comment!", "Thank you for organizing the meeting. I will join it with my colleague Shoji Morita. He will answer the poll this weekend.", "Ishu Goel (my clolleague at Nobleo) and I are also joining for the meeting tonight. I was wondering what the expected duration of the meeting is?", "Usually we aim for 1 hour. This meeting might be a bit unusual and thus could take longer, but I\u2019ve already heard from at least one person that they are even more time-constrained.", "So we\u2019ll probably time-box it (meaning, we\u2019ll close it after 1 hour and postpone any remaining business to the next, which might then happen rather sooner than later).", "We have met yesterday, thanks to everybody who attended and also those who replied to the poll!", "The main result from the meeting is that we have extended the scope to also include more powerful devices (e.g., those running Linux), and that in the future, we will meet regularly: On every 4th Monday of the month. Exact times are to be announced.", "For more details, see the ", " (131.6 KB)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "SIG Scope: Software, Hardware, etc\u2026", "Organisation of the SIG: Schedule of meetings, organizers, \u2026", "Performance analysis status update", "Micro-ROS Update: Dashing Support, hw platform update"], "url": "https://discourse.ros.org/t/ros2-embedded-sig-meeting-4/9959"},
{"title": "Is Turtlebot the right platform for us?", "thread_contents": ["I am thinking of Turtlebot for an intro robotics course I am designing. I am wondering about a few things:", "p.s. is this list active or is there another list where there\u2019s more lively discussion?", "Also looking at ", "Hi ", "Thank you for your interest in TurtleBot3.", "First, you can check it ", " on wiki page. It shows what is in the box. TurtleBot3 officially supports ROS. It already uploaded in ", ". Furthermore, OpenCR that is included in TurtleBot3 is applied Arduino IDE. The source code and hardware already uploaded in ", "Second, it varies from person to person. Averagely TurtleBot3 burger takes 1~2 hours and waffle takes 2~3 hours.", "Third, we provide Raspberry Pi 3 and empty microSD card. So users can install what they want. We recommend Ubuntu mate 16.04 LTS for ROS compatibility.", "Thanks", "Thanks! A few followup questions:", "Thank you!", "Hi again!! ", "The assembly instruction is provided four language including English, Chinese, Japanese and Korean. The main language is English. you can check this ", "When you get the new TurtleBot3, OpenCR has latest firmware but other boards(Raspberry Pi 3 or Intel Joule) doesn\u2019t have operating system. So, we provide all setup process in ", ". Users can easily follow this procedures.", "The ", " is ROS embedded system. The development environment for OpenCR is wide open from Arduino IDE for students.", "TurtleBot3 burger has Raspberry Pi 3 and TurtleBot3 waffle has Intel Joule board. If you order TurtleBot3 burger, you can take all components, Dynamixel, OpenCR, Lidar sensor and Raspberry Pi 3.", "Shipping will start August 1, you can receive it in the order you ordered. We will try to deliver as soon as possible.", "Some people struggle for installation of ROS. So we prepare ", ". This file will help them.", "ROS is open source community during 10 years. It has ", " and many ", " for supporting users. It might be help you when you have any errors during operating robot.", "Thanks.", "Thanks.", "The assembly instruction is provided four language including English, Chinese, Japanese and Korean. The main language is English. you can check this Assembly Manual", "Double check the link. I don\u2019t think it has more than a few pages of English. Or just send me the pdf to avoid any misunderstanding!", "When you get the new TurtleBot3, OpenCR has latest firmware but other boards(Raspberry Pi 3 or Intel Joule) doesn\u2019t have operating system. So, we provide all setup process in TurtleBot3 wiki pages. Users can easily follow this procedures.", "The OpenCR is ROS embedded system. The development environment for OpenCR is wide open from Arduino IDE for students.", "What does the ROS embedded system do? Does it mean that I don\u2019t try to run ROS on the Arduino?", "Shipping will start August 1, you can receive it in the order you ordered. We will try to deliver as soon as possible.", "Can you give me a realistic estimate, because I actually need it on July 1 for my course. How much after Aug 1 do you guess?", "Thanks!", "Pito", "Hi ", "I share ", ". You can check that the all contents are provided English.", "I posted your second question in ", ".", "About shipping in US reference please contact to ", " (For more detail about shipping please contact ", ") The person interested will help you.", "Thanks.", "I think that the usefulness of the Turtle3 for a beginning robotics course would depend on many things. As somebody who has worked integrating technology into university-level education, I would suggest that ROS ", " be introduced into a beginning robotics course unless this is a graduate-level course.", "Part of this for me is the definition of \u201cbeginner.\u201d The skill level of the students depends on their background. If they are all skilled programmers and understand basic mechanics, then perhaps \u201cbeginning robot course\u201d might be a good place to introduce ROS.", "ROS does have a lot of code that is available for use by the programmer. However, it is a large and complex system by itself. It could easily take the entire course to explain ROS to students who are not already versed in programming.", "With ROS the students are able to program much more complex robots because they don\u2019t have to reinvent the wheel multiple times. ROS covers many very difficult subjects like SLAM and navigation. Many sensors already have the code written to use them.", "The Arduino IDE comes into play because the OpenCR is a separate board which handles some of the low-level processing of the motors and perhaps some sensors. The OpenCR is not able to run ROS, but it can send and receive ROS messages. Therefore to ROS, it looks like a ROS node and the main computer board doesn\u2019t care that it is a separate board.", "The other problem is the class\u2019 budget. Even the Burger costs quite a bit for many educational budgets. One could make it a requirement that the student buy their own but that is a lot for some students.", "I hope this helps.", "Jay", "Hi Jay", "Thanks for the great info! I have some followups inline\u2026", "I think that the usefulness of the Turtle3 for a beginning robotics course would depend on many things. As somebody who has worked integrating technology into university-level education, I would suggest that ROS not be introduced into a beginning robotics course unless this is a graduate-level course.", "It would be a senior/grad level course, the students would all be pretty good programmers already but not in C++. It would be either Java or Python for their previous experience. Also this is a \u201cstructured\u201d independent study course so they would be expected to do a lot of digging themselves.", "Part of this for me is the definition of \u201cbeginner.\u201d The skill level of the students depends on their background. If they are all skilled programmers and understand basic mechanics, then perhaps \u201cbeginning robot course\u201d might be a good place to introduce ROS.", "They will be medium skilled programmers, but likely know little about mechanics. They could be strong in math and would have to learn the mechanics, matrices, transformations, and all that.", "But: I too was concerned about choosing ROS. I have done a lot of software myself and I guessed just reading about it that ROS might be delicate to configure and get to work. I installed it on a new linux laptop without too much trouble but this is just for the first few tutorials. The way these things go, I can imagine that once I got to going beyond that something would stop working and it would be hell to figure out why.", "ROS does have a lot of code that is available for use by the programmer. However, it is a large and complex system by itself. It could easily take the entire course to explain ROS to students who are not already versed in programming.", "With ROS the students are able to program much more complex robots because they don\u2019t have to reinvent the wheel multiple times. ROS covers many very difficult subjects like SLAM and navigation. Many sensors already have the code written to use them.", "The structure of the course is a little unusual: I intend (hope) to make it something that is given every semester, but where the work of students in one semester is used as the foundation for the next, thereby allowing us to shoot for something really hard. That would be a robot that could navigate autonomously out of one office, out the building, down a path to another building, into it and to another office. I first thought this was a \u201csolved problem\u201d and my ambition was just to retrace what already had been done. But now I think it is actually not a solved problem. So, I don\u2019t know if we will achieve (even over several semesters) that goal but it\u2019s a nice inspiring goal to put in front of us.", "I describe all that because I think that to accomplish this we need either to use ROS or write a lot of \u201creal time OS\u201d code ourselves.", "That being said, I am already planning to get students warmed up in the basic world of robotics with a smaller project on an mBot. The smaller project, still hard I think, is to get the mBot to drive in a 2M square pattern, without line following but using sensors to correct and adjust the route. This I do think should be tractable without ROS.", "The Arduino IDE comes into play because the OpenCR is a separate board which handles some of the low-level processing of the motors and perhaps some sensors. The OpenCR is not able to run ROS, but it can send and receive ROS messages. Therefore to ROS, it looks like a ROS node and the main computer board doesn\u2019t care that it is a separate board.", "Excellent. Then my next question is what software is on OpenCR? In other words, what does it do beyond what a bare arduino fulfilling that function do? Also what about using an OpenCR ONLY and then a wifi connection to a bigger linux computer running ROS?", "The other problem is the class\u2019 budget. Even the Burger costs quite a bit for many educational budgets. One could make it a requirement that the student buy their own but that is a lot for some students.", "I\u2019ve gotten a small grant to help pay for this. Still I think the class as a whole would have only 1 or 2 robots. The class initially will be kept between 2 and 8 students (I actually don\u2019t know how many students will be interested.)", "I hope this helps.", "Extremely. With that background, where do you stand on the Turtlebot3 for my objectives? From other info I got the impression that the \u201cburger\u201d with a PI would not be powerful enough and that I would need to get a \u201cWaffle\u201d. Or maybe a \u201cburger\u201d configured with the other available CPU.", "All further thoughts and insights would be greatly appreciated!", "Pito", "Seems like with a budget that can barely afford two small robots, you\u2019re asking for too much here.", "I would suggest using more leverage (e.g a bigger community which already has a lot of resources\u2026 like a Turtlebot 2), or reducing some scope. If you make a \u2018robot\u2019 with an arduino and a servo, im sorry but that\u2019s not really a robot course. it\u2019s a course on servo actuation. It\u2019s got to be a ", " more complex than that.", "I highly recommend demonstrating kinematic chains (e.g arms), different drive types of robots, different types of sensors, and different types of algorithms which can navigate, move, and orchestrate the robot.", "Just keep in mind, please, that a robot is a SYSTEM as much as an entire laptop is a system. This also means a microcontroller inside a mouse, is not a laptop. (which is what is proposed by only using something as small as a microcontroller and a servo.", "Given also that the students learn from the teacher, I recommend getting a lot more knowledgeable real fast before they come along. Otherwise, you may want to step back again and reconsider using community leverage where there are those more knowledgeable that can help out. If you\u2019re just learning about ROS, there\u2019s no way in hell the students will be able to solely rely on you for all their questions.", "So, you need a super community supported robot/system (you HAVE to use ROS here, given the above constraint). I\u2019ve answered that for you.", "You should also spend your budget on something that makes a lot more sense and is an entire robot system. If the class size is 8-9 students, they can take turns loading software on it, or all be ssh\u2019d into it in the same time. They don\u2019t all need their own robot. That\u2019s a bit grandiose.", "Also, don\u2019t bullshit around on anything that\u2019s not a full computer. ROS works best on an Ubuntu 16.04/14.04 operating system and not on an ARM device. You\u2019ll want to not be constrained by performance when picking and choosing modules. it will cost you a lot more in time and grief than the difference in price between a RaspBerryPI and a $700-800 intel NUC/mini-ITX form factor computer.", "With a platform like this, you don\u2019t have to worry about where to go next. Software on a proper computer will take you to the sky and limit of research in robotics (mostly). Given you\u2019re teaching introductory ROS, a platform like this could easily give you 4 semesters of course data, and with thought, 2-4 more.", "At least give the students a fighting chance. If you get the kids (If they\u2019re graduate level) working on an mBot to do \u2018research\u2019 or \u2018learn on\u2019, the mBot is nothing like the robots in industry or used in academia, you aren\u2019t setting them up for a future or helping them out by showing them whatever you\u2019re coding on an mBot.", "I can tell you that if I saw an mBot on a resume or information about it (unless otherwise extremely spectacularily innovative or doing something NEVER done before), I would assume that it was a highschool project or something someone did in a weekend that showed no real effort put in towards robotics.", "My 2C.", "I\u2019m sorry, Pito, but I don\u2019t have the OpenCR board yet. I believe that it is programmed like an Arduino, in the same languages, but it uses an M0 ARM chip which means there may be some incompatibilities.", "I did use several of the \u201cTeensy 3.x\u201d boards which are also ARM-based using the Arduino IDE. These worked fine, but not all Arduino libraries were compatible. Most of the libraries were compatible due to the fine people that created the board. I think that all the official libraries were compatible (at the source level), but some unofficial libraries weren\u2019t.", "I thnk it comes down to the programmers that programmed the core code for the OpenCR.", "Unless there is something that the OpenCR board gives us that the Teensy 3.x doesn\u2019t, I would have preferred that they went with the Teeny\u2019s. The Teensy is very small and still does more than an Arduino, packing a lot into a breadboard-friendly dual-inline package. They are also inexpensive.", "I will wait and see before final judgment.", "I am currently going to start experimenting with a UDOO x86 ultra. This is a quad-core x86 (faster than the Joule) with a built-in Arduino onboard. I should be able to use it for both the main processor and the sensor board for a robot.", "I haven\u2019t had a chance to play with my own mBot yet.", "Jay", "Hi Jay", "Thanks for the info\u2026 I will investigate further about OpenCR. Also thanks for the reference to Teensy 3.0. and UDOO x86 ultra. Will check them out!", "Pito Salas", "\nBrandeis Computer Science", "\nFeldberg 131", "Hi there (name?)", "Thanks for lots of good insights\u2026 Comments inline:", "Seems like with a budget that can barely afford two small robots, you\u2019re asking for too much here.", "I would suggest using more leverage (e.g a bigger community which already has a lot of resources\u2026 like a Turtlebot 2), or reducing some scope. If you make a \u2018robot\u2019 with an arduino and a servo, im sorry but that\u2019s not really a robot course. it\u2019s a course on servo actuation. It\u2019s got to be a bit more complex than that.", "I assume you mean the ROS community vs. the arduino one? Yes, I totally agree, that\u2019s why I am starting to participate here! Do you consider Turtlebot3 a good candidate? Because that\u2019s the one I was digging deeper on. (I thought the 2 was discontinued?)", "I highly recommend demonstrating kinematic chains (e.g arms), different drive types of robots, different types of sensors, and different types of algorithms which can navigate, move, and orchestrate the robot.", "Gotcha.", "Just keep in mind, please, that a robot is a SYSTEM as much as an entire laptop is a system. This also means a microcontroller inside a mouse, is not a laptop. (which is what is proposed by only using something as small as a microcontroller and a servo.", "Of course: I am well aware of that ", "Given also that the students learn from the teacher, I recommend getting a lot more knowledgeable real fast before they come along.", "Working on it. But I don\u2019t exactly agree with your model. I think the teacher creates the environment to allow students to learn and to teach themselves. Also by the way note that this is more of an independent study course.", "So, you need a super community supported robot/system (you HAVE to use ROS here, given the above constraint). I\u2019ve answered that for you.", "I think ROS is amazing, I agree, especially because of the community. (The software architecture of ROS itself seems to me a little overly complex but that\u2019s just my first impression. I know I have to get deeper.)", "Also, don\u2019t bullshit around on anything that\u2019s not a full computer. ROS works best on an Ubuntu 16.04/14.04 operating system and not on an ARM device. You\u2019ll want to not be constrained by performance when picking and choosing modules. it will cost you a lot more in time and grief than the difference in price between a RaspBerryPI and a $700-800 intel NUC/mini-ITX form factor computer.", "Good advice. I heard elsewhere also that Pi was underpowered for the job. Intel Joule?", "With a platform like this, you don\u2019t have to worry about where to go next. Software on a proper computer will take you to the sky and limit of research in robotics (mostly). Given you\u2019re teaching introductory ROS, a platform like this could easily give you 4 semesters of course data, and with thought, 2-4 more.", "At least give the students a fighting chance. If you get the kids (If they\u2019re graduate level) working on an mBot to do \u2018research\u2019 or \u2018learn on\u2019, the mBot is nothing like the robots in industry or used in academia, you aren\u2019t setting them up for a future or helping them out by showing them whatever you\u2019re coding on an mBot.", "My thought (maybe wrong) was this: a simple, successful experience, where they confront face to face the fact that a robot interacts with the real world and won\u2019t just do what you tell it, i.e. even drive in a straight line. Do you not buy that?", "Thanks much. So in summary, I wonder about your opinion about the Turtelbot3 with the Joule. And of course I am keen to continue the discussion to learn more from you!", "Thanks,", "Pito", "Perhaps if you first prepared a relatively simple project for the students by having them use a specific part of ROS to do what you wanted.", "For example, the suggestion of having the robot roll in a simple square. Though I might suggest doing line or maze following. Both of these seem more ", " for students than rolling in a square which seems more like a homework problem.", "Yes, I agree that all of the above are homework problems, but people react better when there is some fun and just a little competition involved, IMHO.", "If you teach the general concepts of ROS and then concentrate on the parts of ROS that the students would need in order to achieve the goal, ROS might work out.", "After I do the basics to learn ROS with the Waffle as ordered, I will attempt to replace the Joule with a NUC and give the Joule to my preordered Burger. After this, I will keep those machines for demos and build something more complex and fun.", "Hi Jay", "Perhaps if you first prepared a relatively simple project for the students by having them use a specific part of ROS to do what you wanted.", "I assume the idea of doing something like that on an arduino robot you feel is not the best use of time. My second project I was designing is for them to create a map of a small part of the building and then using the ROS simulator train the simulated robot to find it\u2019s way from my office to an office down the hall.", "For example, the suggestion of having the robot roll in a simple square. Though I might suggest doing line or maze following. Both of these seem more fun for students than rolling in a square which seems more like a homework problem.", "A maze is much more fun, good idea. I thought that not using line following but making them think about 2d geometry as they try to do a straight line, using some kind of sensor to find distance to walls might be a little more challenging and interesting. Line following seems too easy or am I thinking about this wrong?", "If you teach the general concepts of ROS and then concentrate on the parts of ROS that the students would need in order to achieve the goal, ROS might work out.", "After I do the basics to learn ROS with the Waffle as ordered, I will attempt to replace the Joule with a NUC and give the Joule to my preordered Burger. After this, I will keep those machines for demos and build something more complex and fun.", "Great idea for a progression! Thanks and keep any suggestions coming!", "Pito", "A maze is much more fun, good idea. I thought that not using line following but making them think about 2d geometry as they try to do a straight line, using some kind of sensor to find distance to walls might be a little more challenging and interesting. Line following seems too easy or am I thinking about this wrong?", "Well, I\u2019d like to see you personally get it done in 3 months with starting with 0 ROS or robotics knowledge ", "If you\u2019re going to ever do distance sensors, you have 3 options:", "Given your price range, I\u2019d try and go for ", " if it fits the budget, and don\u2019t bother with ", " other than for proof of concept/want people to feel the pain while learning. Actually a combination of them all would be a good way to demonstrate that your algorithms are theoretically bounded by the quality of your sensor data =)", "Pito, design the course and what you want to show first. The robot you need will be decided by that. If you are unsure of what the course needs in it/how to go about that, lets take this offline/another thread. I\u2019ve built many a robots and i think I can help you here with coming to a close on what you need to show the students.", "Working with point clouds for distance measurement using laser or 3d cameras is a big challenge in 3 months along with getting the robot moving.", "A proximity sensor for wall following is more doable in that period.", "I disagree. There are many platforms out there where it\u2019s one launch file (e.g one cmdline type in) to bring up a screen to show you what point data it is seeing, and consequently what data you can see from the terminal.", "Given that, you could instruct the students to do stuff with the data using python, C++. In fact, it is the exact same amount of work in ROS to get a prox sensor coming up\u2026 to ROS it doesn\u2019t care, it\u2019s all pointcloud data/laserscan data\u2026", "You aren\u2019t asking them to write an algorithm that extrapolates the depth from the images, it\u2019s given already.", "The reason I\u2019m not a fan of the square is that it is a highly intellectual exercise. My suggestions will encourage competition. I don\u2019t think that you need to say anything.", "Yes, you can do these things with an Arduino-based robot. But I believe they will learn more using ROS.", "One of the most important things is that you understand ROS and the algorithms for your projects. I will be installing", "\u2026Linux on a machine tomorrow so that I can start learning ROS in simulation soon. I have books and the web, but I learn best when I can write code!", "Thanks everyone for this thread, I learned a lot. I will continue providing small updates from time to time on my progress and experience in another topic here.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Exactly what comes in the box. In particular, to what extent is the software, ROS, etc. already configured?", "How many hours from unpacking to riding the bot in a circle?", "Asked in a different way, what software is pre-installed on the Pi?", "The assembly instructions is mostly in a non-english language. How come?", "Given that you say \u201cit is already uploaded\u201d that means that we have to install the software onto the actual robot, which is fine.", "I don\u2019t understand what you said about \u201cApplied Arduino IDE\u201d, can you explain?", "Raspberry Pi is with the Waffle, right? Are you saying that we can easily order a Burger with a Pi? Or what exactly?", "If I order on July 1, when can I expect to receive a Burger?", "ROS seems to be pretty complicated to set up (all the xml files, etc.) And there doesn\u2019t seem to be that much support (i.e. Stackoverflow or similar.) Is this true?", "The best: laser range finder", "Close second: 3d cameras (kinect, intel euclid, intel realsense)", "Last resort: proximity"], "url": "https://discourse.ros.org/t/is-turtlebot-the-right-platform-for-us/1892"},
{"title": "Meta-ros Kinetic runtime issue with roscore", "thread_contents": ["Hey everyone,", "I\u2019m adding ros-meta to a Yocto/OpenEmbedded build of a Linux distribution for a custom Xilinx Zynq based camera board and it looks like I\u2019m running into some runtime dependency issues when running roscore.", "I\u2019m using the experimental kinetic distribution of ros-meta at:", "My bblayers.conf file looks as follows:", "Added the following line to by ", " file:", "Used the following command to create my own image.", "Build completed without any errors.  Created an SD card including the kernel", "\nand file system that includes ROS.", "On the booted Xilinx Zynq based system running the Linux environment I just built, I\u2019m configuring ROS Kinetic with the", "\nfollowing:", "I have the following problems running roscore.", "From what I can find on ROS Answers, it looks like defusedxml was as a fairly recent dependency added to ROS Kinetic.  I assume this dependency probably didn\u2019t make it into the meta-ros dependencies.  Any suggestions on how I go about adding it to the Yocto/OpenEmbedded build of the system?", "Thanks,", "Mike", "Hey Mike,", "Nice to hear that the \u201ccustom Xilinx Zynq based board\u201d is still going ", "Defusedxml is a python package that was added to facilitate adding security to ROS, as it provides protection against a bunch of different XML attacks. This is part of the ", " effort to add some security and hardening to ROS.", "Anyway to get roscore to run, have you tried adding ", " as a dependency here: ", "I have only done some looking around at OpenEmbeded stuff, and never got around to actually using it so I am not sure if this is the \u2018right\u2019 way to fix this, or if it would even work.", "Out of curiosity what is the size of the image that comes out the build process? I would think it would quite small relative to a Ubuntu image.", "Rohan", "Hey Rohan,", "I resolved this issue by adding the following recipe for defusedxml to:", "The contents of the recipe is:", "Then adding a dependency to roslaunch_1.12.2.bb for this python module.", "The following commit is a complete fix.", "The resulting image file for the entire Linux OS including roscore is about 47MB in size which is considerably smaller than an Ubuntu image.  However, this is for the absolute minimum user tools and ROS install.", "Mike", "Hi Mike,", "I am interested in doing something similar using a Microzed, but honestly don\u2019t know where to even start, can you point me to some good resources, so I might be able to do the same?", "Thanks!", "Hi Qnetjoe,", "I would propose that you start as follows:", "Get an understanding of yocto and bitbake following the Yocto Quick Start:", "\n", "\nAfter stepping through that document, you should be able to build and start a small Linux image in the qemu emulator.", "Build and run an image with ROS for the qemu emulator following the instructions at:", "\n", "Build and run a minimal image for the Microzed board, using the microzed configuration provided", "\nin the meta-xilinx layer at:", "\n", "\n", "\nBy the way, I found this by searching on ", ",", "\nand it quickly linked me to:", "\n", "Combine what you learned in Step 2 and Step 3, and build an image", "\nwith ROS for the Microzed board. When combining, you will probably need", "\nto choose the build configuration that is a compromise between the build", "\nconfigurations you used in step 2 and in step 3. This will then probably need", "\ncertain adjustments in meta-ros.", "Here some further advice:", "Get a build machine with a lot of CPU power, RAM and a SSD disk to", "\nbuild images much quicker. You will not get away with some laptop", "\nand some Linux virtual machine. when bitbaking the images.", "Report issues you encounter on the meta-ros github issue tracker. Please", "\nalways provide at least the build configuration you use, and possibly even", "\nthe bblayers.conf file, so that others can reproduce the reported issue.", "I hope this helps.", "Lukas", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/meta-ros-kinetic-runtime-issue-with-roscore/1407"},
{"title": "ROS only simpler", "thread_contents": ["Hello All,", "I apologize if this is not the right forum for this, but I recently started on a project that will use ROS2 for IPC and inter-container communication.", "For a little background, I have been a C++ developer for 20 years, and a C developer for 10 years before that. I have used MQTT on other projects, and fully appreciate what ROS2 offers.", "My first impressions of ROS2 have not been good. Don\u2019t get me wrong, I understand what ROS does, how it does it, and believe it is the right tool for the job, and in spite of the on-going difficulty in using ROS2, we are committed to using it.", "Our trouble stems form the build environment that ROS2 dashing has. I have built the basic tutorials at ", ", and after some trouble, they compile and all is well. The next step in my journey is to incorporate custom messages, and here is where the story has become unhappy. I started following the tutorial at: ", ", and the rails came undone. This tutorial fails to build with cryptic errors. One of our partners put me in touch with their ROS/ROS2 expert, and he is helping me work through the build environment issues. We will solve these issues, but this is pain I can do without. My first MQTT project worked in 30 minutes from start to hello world (with my own custom payload). I have so far invested 2 days of my time and at least a few hours of others time, and still do not have a working basic example of a custom payload with ROS2, in fact, we have so-far been unable to get the above tutorial to run on my development machine.", "From a new dev perspective, I have a few simple suggestions to hopefully help others who may follow in my steps:", "First, The existing build environment is far over complicated for a simple tutorial. As noted, I can\u2019t get the example programs to run because the build environment is broken. This is almost certainly not related to anything that is core to ROS2, and could easily have been avoided. The tutorials should include only a few files, and perform the command line compile by hand (avoid automated build tools: that\u2019s like taking a bazooka to a fly). For a good example of how a tutorial should be: see ", ". Notice that this tutorial requires only gcc, and nothing else, it distills the example down to its most basic form. A similar tutorial for ROS2 would require only the gcc compiler and the translation script that generates the C and C++ header files for the custom messages.", "\nSecond, document the translation scripts as standalone programs so that people who wish to use their own build environment do not have to reverse engineer yours just to extract the custom message translation mechanism. You have a good build system for complex projects (at least I assume it is good as I don\u2019t have it working yet), but it is grossly overkill for most projects, especially learning projects for people who are new to ROS, but have lots of experience as developers. I am in no way suggesting that your build system should be changed, just that the option to skip the build system be supported for those that don\u2019t need/want it, and for the sake of simple tutorials.", "This tutorial fails to build with cryptic errors.", "I know its not the point you\u2019re trying to make but: can you tell us the \u201cwhat was wrong\u201d and \u201cwhat was the error you got\u201d with this one? Documentation can always be easily updated with your experiences ", ". If you find something confusing, others would too. Be the change you want to see, and all that.", "Anyhow, to try to repeat back to you my understanding of your core issue:", "Those are actually very different points than I would have expected someone new to ROS2 to make about the tutorials. That perspective is interesting. This is the right place to share it, thanks for getting  a conversation started.", "I would not consider the custom message tutorial to be on the basic-first-look for some of the issues you bring up. It is a little involved, but if you were to be going through the tutorials in order, the only new macro here is ", ". The other ", " things you would have run into already to get a feeling for in several tutorials before this point.", "A similar tutorial for ROS2 would require only the gcc compiler and the translation script that generates the C and C++ header files for the custom messages", "Those do a bit more than just that, but I understand the sentiment. I have a feeling that someone more knowledgeable than me will come along to explain why it would likely be more challenging to compile from a commandline for new users.", "My higher-level opinion is that we functionally never do that.  Reinforcing the tools and teaching habits from the get go is important. If you tell someone how to compile it with a commandline, you\u2019re going to need to then make another tutorial about how to do it \u201cfor real\u201d. Most users are going to be using ament/colcon to compile their systems, something I\u2019d estimate in excess of 95%. Each additional reference of these tools someone can learn from has substantial value for reinforcing ideas. However, what doesn\u2019t have value, is if they are poorly explained, presented, or ordered in tutorials causing the experience that you had.", "I think that experience has alot of value and we\u2019d all learn alot from a proposal from you about how to better explain these concepts and support new users (ie to move around the tutorials, explain some concepts better, or give more examples presented in another way).", "In addition, adding an advanced tutorial explaining how to compile code without these tools could have value to an experienced developer like yourself that would rather not use the provided tooling.", "Those 2 action items don\u2019t resolve the specifics of your comments, but I think they resolve the spirit of where we can do better so that your comments won\u2019t happen to new users because the build environment will be better presented and working out of the box.", "I recently started on a project that will use ROS2 for IPC and inter-container communication", "The catch here, especially when comparing with MQTT, is that ROS is so much more than just IPC.", "There was quite a bit of discussion about the build tool recently, which might help explain why things are the way they are. There is room for improvement, especially in the documentation, but we do have reasons for why things are like this.", "If you are having problems, then I second the comments by ", ": Seeing those errors and hearing about the detail of the problems will go a long way to helping us find out specifically where we can improve.", "While I agree largely with what ", " and ", " have written in their replies, I just wanted to make sure that ", " doesn\u2019t get the impression we\u2019re trying to ignore his comments or dismiss them with an appeal to tradition (ie: \u201cthis is just how we do things here\u201d).", "The thread ", " linked to was the first thing that came to my mind as well, and I believe there are some good posts in it that ", " may want to read.", "I recently started on a project that will use ROS2 for IPC and inter-container communication", "The catch here, especially when comparing with MQTT, is that ROS is so much more than just IPC.", "This is true, and may not be apparent when starting to work with this infrastructure.", "I just wanted to make sure that ", " doesn\u2019t get the impression we\u2019re trying to ignore his comments or dismiss them with an appeal to tradition (ie: \u201cthis is just how we do things here\u201d).", "Absolutely. We know the tutorials are lacking, but there are so many tutorials and so much stuff we need to cover so hearing from ", " what specifically are the errors and problems will help us improve things.", "Yeah, I agree with the sentiments above.  I don\u2019t think it is a good idea to introduce something that is simpler, only to then backtrack to using the \u201creal\u201d tools.", "But I do agree whole-heartedly that the tutorials are lacking, and making things \u201cjust work\u201d is an important goal for us.  So I would also like to know the particulars of your trouble, and how we can improve the build tools/code/documentation so that the next newcomer doesn\u2019t run into them.", "Follow up", "Good news! I was able (with the help of one of our local CMake experts) to dig back to the root cause of the problems I was having with the build environment.", "At one point, I had been working on just one of the included packages, and saw the CMakeLists.txt, and naturally thought to run cmake . followed by make. This worked fine for that 1 specific compile (except for build files being put in funny places), but after that, cmake had cached the build settings which conflicted with those that colcon was trying to use, and cmake just ignored the colcon setting to use the cache. To fix the problem, all that needed to be done was to clear the cmake cache (the cmake equivalent of make clean).", " shared a link to a general discussion topic about moving ROS2 over to a pure CMake environment.I still contend that for a tutorial, the build environment is a distraction, however pure CMake would help to improve the situation a lot. CMake is quickly becoming the defacto standard for many platforms, and as such, would be easier for some of us to transition to. (Disclaimer: I work for Kitware).", "I appreciate all of the responses, and to be clear, I am not necessarily expecting any particular action, and I am absolutely not suggesting that the default for ROS2 be anything other than the build environment as you have built it. For most (all?) new projects, I think that the current default build environment makes reasonable sense. That having been said, it is always a good idea to give users more flexibility when you can, especially developers, and I can think of several cases where the single build environment would be more hindrance than good. The first and most obvious is for a legacy project where they want to add ROS2. That project already has its own build environment, and porting that entire project to use colcon would be prohibitive at best, and would most likely make the change a non-starter. A second example would be conflicting with other projects that have incompatible build requirements. For example, incorporating anything involving GPGPU processing or multi-language environments generally already have their own build systems in place that, in some cases, do not play nice with CMake. Merging ROS into such a project would be a royal PITA. Granted, these other projects are as much or more at fault for picking a non-standard build system, but I think you see where I am going with this. Another use case is for embedded systems, where one of the packages in a system belongs on an embedded device. Due to the nature of cross compilation, it is almost always easier to have a standalone build system for the cross-compiled parts. As you can see, it is easy enough to keep separate workspaces for each of these examples, but under these examples, the colcon build system provides no real value added.", "One thing I do want to be sure to convey is my appreciation for the documentation.  In my experience good quality documentation is hard to write and hard to find, but the plethora of ROS/ROS2 documentation has been a happy exception to that norm. Being able to find most of the information I need without resorting to google-fu has been very refreshing.", "Thank you again for the wonderful replies. You have built a really powerful tool in ROS/ROS2, and it is clear to me that no matter what decisions are made, there are dedicated and capable people working to make ROS2 better, and that is what really matters.", "That having been said, it is always a good idea to give users more flexibility when you can, especially developers, and I can think of several cases where the single build environment would be more hindrance than good.", "This is an interesting statement, because it is actually the opposite of how I think about the ROS build tools.", "One point that I keep coming back to (and maybe not explaining well) is that tools like Make, CMake, SCons, etc are inherently single-project tools.  That\u2019s fine, that is their goal.  But ROS is set up in a much more federated fashion, where you have many projects that you combine in different ways to achieve your goals.  So while I wouldn\u2019t stand in the way of anybody trying to improve the documentation and tools for the single CMake case, in my mind it just doesn\u2019t fit in well with the federated nature of the ROS ecosystem.", "That being said, I\u2019m glad that you solved your problem.  One thing we could add to the documentation is to make sure that your CMake environment is cleaned up before trying to build.  For the record, what were the commands/actions that you had to take to clear our your environment?", "For the record, what were the commands/actions that you had to take to clear our your environment?", "I restored the package directory to the pre-cmake state. In my case, I accomplished this by deleting the package directory under src, and replaced it with the original from the tutorial. When I ran cmake from the package directory, it built all of the files and directories that colcon would have placed under /build, and put them in the local package directory instead. I had to remove those files (which included the cmake cache information). I probably only needed to remove the CMakeCache.txt, Makefile and the CMakeFiles directory, but it was easier to restart fresh in my case.", "My higher-level opinion is that we functionally never do that. Reinforcing the tools and teaching habits from the get go is important. If you tell someone how to compile it with a commandline, you\u2019re going to need to then make another tutorial about how to do it \u201cfor real\u201d.", "Python is a good example where they do this poorly. The usual intro tutorial is just how to print hello world to the terminal, it explains nothing about importing packages or pip or setting up requirements.txt and setup.py, which is of course how you do it \u201cfor real\u201d", "I hope you\u2019ll forgive the sarcasm but I thought it was the most effective way to get my point across.", "Most of arguments against making ROS simple enough to be able to have simple tutorials center around \u201cbut ROS is federated\u201d, which is actually a sort of catch-22 because the build system enforces this federated model and when someone like OP tries to work outside of it, they get bit.", "I\u2019d argue that Python is also \u201cfederated\u201d, in the sense that there\u2019s a vibrant package ecosystem (which is one of the greatest things about Python and about ROS as well). I think Python/pip could provide a good model for how to do ROS packages and one of these days, when I find the time, I plan to work on a proposal for how ROS packages could work in a more pip-like way that is both friendly to new users but also decipherable for those who want to dive in \u201cunder the hood\u201d.", "That project already has its own build environment, and porting that entire project to use colcon would be prohibitive at best, and would most likely make the change a non-starter.", "You mixing two completely separate parts of the process: the build system (CMake, Python setuptools, etc.) and the build tool (colcon) [see ", "]:", "And as usually you can always mix-and-match: build some packages with colcon, then some manually (however you like), and then another set using colcon.", "Due to the nature of cross compilation, it is almost always easier to have a standalone build system for the cross-compiled parts.", "I don\u2019t think this statement is true. When you want to cross compile any project which consists of more than a single monolithic piece you are facing the exact same challenges as for a native build. Therefore the benefits of a build tool apply just the same. Coincidentally just yesterday a tool to make cross compiling was announced ", " which internally uses ", " for that very reason.", "because the build system enforces this federated model", "See my comment above which clarifies that the build system only has knowledge about a ", " package. So by definition can\u2019t have anything to do with the federated model which means dealing with ", " packages.", "And by design the build tool is optional to use. You are able to build all the packages of a ROS distribution just fine without a build tool. You \u201cjust\u201d have to manually figure out in which order packages need to be build and how to build each individual package. So I hardly see where we enforce the build ", " anywhere (except that doing that kind of work manually sounds like a huge amount of wasted time to me since it can be easily automated).", "I think Python/pip could provide a good model for how to do ROS packages", "I think this comparison isn\u2019t actually applicable. Afaik there is no way in Python to build and install a set of packages where you have the source code in a local directory - and especially do that by honoring their inter-dependencies.", "Great discussion!  I just started ROS2 in November, I didnt have any ROS1 experience. I did find the build system complex even though I am an expert in cmake, but colcon is now working for me too and I like it\u2026just takes time. I struggled like OP\u2026let\u2019s face it ROS is a big ecosystem and requires perseverance to master. I took a udemy ROS2 course, and just kept at it. I feel at ease with the build system now but had the exact same issues as OP on my first custom msgs build. I love cmake and now after only 2 months I really love ROS2\u2026such an improvement over ROS1. I would say keep colcon in the mix, but second improving documentation and tutorials especially given in progressions. Should be spoon fed a bit in the beginning. API-like tech docs can get overwehelming for newbies (like I was).", "I am not necessarily expecting any particular action", "See, but I ", " want actions. I think you bring up a good point in things not being as clear as they should be. Most (if not all) of those tutorials didn\u2019t exist with I learned ROS2, so you have a unique perspective as not only a new user, but a competent software engineer. Most new users don\u2019t come in with 10+ years of experience to draw from ", " . I\u2019d think we would all very seriously learn alot from your stream of consciousness on how to improve the first experience, if we take colcon/ament as a given.", "But it sounds like you got yourself in this situation by going off the beaten path of the tutorials.", "This is an interesting statement, because it is actually the opposite of how I think about the ROS build tools.", "I agree with ", ". If you come from anywhere in C++ land prior to ROS, this stuff is a little weird to deal with. Once you own the fact that you\u2019re just going to use ROS for the long-haul ", ", its a worthwhile investment. But if you\u2019re just wanting to dip your toes in or you\u2019re really only trying to support 1 package, that perspective is very different. I don\u2019t want to argue on the technology components here, mostly because I think my opinion is under-educated, and partially because I don\u2019t really think its the point.", "Howdy, I am also a newer ROS2 software engineer, I started picking it up about 6 months ago. I just recently had to implement custom messages and I found the tutorial lacking in one specific area that I am suggesting gets added.", "The tutorial offers insight in creating a custom message within the same package, and also includes the option for interfacing with a message in an external package, however, the latter is coupled with the former. That means the cmakelists had both internal and external references for custom messages and it took a bit of extra work to determine what was needed for only referencing external custom messages. It would be useful to add an extra step that shows just that. I hope this suggestion was clear!", "Also, while I am here, I was wondering if anyone had a simple answer to whether or not it is possible to compose node components that are in different packages, or even further, different ros workspaces. I dug around and could not find an answer, which leads me to believe that it is not possible, especially with the c++ build and compiling requirements. I could be missing something though, so if anyone has an answer, I would appreciate it!", "Thanks,", "\nRobert", "I was wondering if anyone had a simple answer to whether or not it is possible to compose node components that are in different packages, or even further, different ros workspaces.", "That should work just fine. The only constraint I would see is that both shared libraries need to be compatible in terms of being loadable into the same process (e.g. not linking against the same dependency but with a different version which have conflicting symbols).", "I\u2019d argue that Python is also \u201cfederated\u201d, in the sense that there\u2019s a vibrant package ecosystem (which is one of the greatest things about Python and about ROS as well). I think Python/pip could provide a good model for how to do ROS packages and one of these days, when I find the time, I plan to work on a proposal for how ROS packages could work in a more pip-like way that is both friendly to new users but also decipherable for those who want to dive in \u201cunder the hood\u201d.", "While I agree with the sentiment of \u201cinstall the packages in your system and just use them\u201d, I think your comparison between pip and colcon has flaws:", "Best practice in Python these days (and in Ruby and a bunch of other languages) is to create a virtual environment for every project and every test execution and so on. This allows you to be certain about dependencies, not have to worry about what the system has installed, and keep things in general isolated. A virtual environment is effectively what a Colcon workspace is, in most respects, and this is typically how I use them - I have a bunch of ", " (and now ", " functions and aliases that I use to activate workspaces, including the ROS distribution in use, depending on what I\u2019m working on. It makes me just as happy to do this with C++ code as Python developers are with ", ".", "Python packages usually do not need to be built before you use them, they just need to be present. This means that you do not need to worry about ordering of builds beyond a simple depth-first approach used to grab depedencies from PyPI.", "pip only has to deal with Python, not C++ and Java and Lisp and protentially half a dozen other languages being treated as first-class citizens in a single virtual environment (or system).", "pip only works on a single package and its dependencies, not, as ", " said, a bunch of packages at once.", "Can you either submit a PR to the message tutorial or DM me a summary of what you think needs to be added so I can add it.", "I am also in the process of writing a \u2018hello world\u2019 tutorial, any feedback will be appreciated.", "Build Robot using Robot Operating System (ROS 2) and Gazebo - bunchofcoders/basic_bocbot", "You mixing two completely separate parts of the process: the build system (CMake, Python setuptools, etc.) and the build tool (colcon)", "This distinction feels academic. If there were tutorials or use cases where ament_cmake was used by itself, then it might make sense, but from an outside perspective it appears to be designed to work hand-in-glove with colcon as a single unit.", "And as usually you can always mix-and-match: build some packages with colcon, then some manually (however you like), and then another set using colcon.", "Is this your idea of a reasonable workflow???", "And by design the build tool is optional to use.", "Another academic point that\u2019s missing the spirit of the discussion. Saying the build tool is optional to use is like saying that using either Linux or Windows is optional. It\u2019s also not true, the build system doesn\u2019t generate the setup.bash file.", "I think this comparison isn\u2019t actually applicable.", "I meant it in the sense that if I wanted to learn how to write some Python code, I can go learn how to write Python code. If I later want to publish that code, I can learn how the packaging system works. With ROS, you can either learn how to package the code first and then how to write it, or learn them simultaneously. This is the main criticism, that the package and build system/tool get in the way of writing code, and that perhaps there\u2019s another approach where you can write the code and package it too without having to mix these two things, like the way they do it in Python. Obviously I\u2019m not literally suggesting that ROS move to the exact Python/pip model, just pointing it out as a potential inspiration for a potential rethink to the package system. While ROS has a system that works and is very popular, I think it could do more to get out of the user\u2019s way and I think this would be beneficial to ROS in the long term.", "This distinction feels academic. If there were tutorials or use cases where ament_cmake was used by itself, then it might make sense, but from an outside perspective it appears to be designed to work hand-in-glove with colcon as a single unit.", "It\u2019s just following the ", " - one tool is doing thing - and only one thing: the build system processes one package, the build tool invokes the build system for each package in the needed order. Also since ", " is only providing helper functions to make writing CMake project more convenient it can be used like any other CMake project - which is a very well established and documented process.", "It\u2019s also not true, the build system doesn\u2019t generate the setup.bash file.", "You are wrong about this. ", " packages actually do generate setup files for the package they build. You can find them in the install prefix under ", ". Obviously they only setup the environment necessary for that package. The rational is also straight forward: multiple packages shouldn\u2019t try to write the same setup files in the root of a workspace - that was e.g. a major issue for ", " in ROS 1. Instead a single package - namely ", " - is responsible to produce these prefix-level setup files for e.g. the Debian packages (in which process ", " is not involved at all).", "Obviously I\u2019m not literally suggesting that ROS move to the exact Python/pip model, just pointing it out as a potential inspiration for a potential rethink to the package system. While ROS has a system that works and is very popular, I think it could do more to get out of the user\u2019s way and I think this would be beneficial to ROS in the long term.", "Please consider proposing specific improvements rather than talking in generalities. Otherwise it is just not very likely that something can / will be done about what you would like to see improved.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You believe that the build environment is complex", "Because its complex, some of  the tutorials are unclear", "The  build system is in charge of processing a single package. In the case of CMake that usually boils down to using it as ", " / ", " / ", ". The fact that ROS packages use CMake helper functions provided by ", " are actually not relevant for this since they are an implementation detail when you look at how to invoke the build system. You can achieve the same with plain CMake - it will just take you more lines of CMake code.", "The build tool ", " only determines in which order a set of packages needs to be built and what build system a package uses in order to decide how to build that package. ", " is very modular, if you want to use a build system which isn\u2019t supported atm you can easily write an extension to teach colcon how to invoke that build system. E.g. CMake support is just an extension for colcon which implements the invocation logic mentioned above.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-only-simpler/12619"},
{"title": "Turtlebot3 - Impressions so far", "thread_contents": ["It\u2019s taken longer than expected, but our Turtlebot3 is running.  Here are some brief impressions and lessons learned - contrasting thoughts and experiences are welcome!", "The build instructions are good, but not great.  Adding some emphasis to the orientation of components would be helpful.  I had the motors swapped initially, and it was maddening to get no response from the board, particularly when there is no real display of an error anywhere.  Is there a legend somewhere on the Internet stating what the OpenCR user LEDs mean with the base software?  Maybe it was signaling the problem, but I couldn\u2019t tell.  Also, the build instructions could use a plan view of the OpenCR board showing where things are plugged in.  I know, some of that stuff is available on line, but hunting for it is more difficult than it should be.", "The Intel Joule board is a significant weakness, for several reasons.  First, having the new user flash the board is understandable, but should be highlighted in the instructions well ", " the board is mounted in the bot.  The requirement to reflash the BIOS is also understandable, but maddening - I run Linux, not Windows 10, so finding a machine to flash the BIOS is a pain.  It also seems that more than one person has had to flash the board multiple times before it boots.  Installing Ubuntu - that was worse than it should have been.  How many of us have micro HDMI adapters handy, so we can actually do the install?  Also, how about pointing out in the instructions that charger power supply will also serve as a Joule power supply for the flashing and install?", "More Joule issues - the BIOS checks cause an amazingly slow boot up, particularly for an embedded board.  It also runs pretty darn hot for an embedded board.  Worst of all, it seems quite unstable.  I don\u2019t know if it\u2019s the fault of the Ubuntu distribution or the hardware, but it crashes far too often.", "The OpenCR board seems quite nice, and I like having all of the sensors integrated.  I have had one lock up where communication between the Joule and the OpenCR ceased - no idea where the fault lies.  In that particular case, the bot just kept chugging along until it ran into something.  It would be beneficial to add a power or reset button on the top platform.", "The overall kit quality and part quality are outstanding.  The Dynamixels are quite nice drive units, all of the cabling is well thought out, and the possibilities for expansion are great.", "The Turtlebot wiki page (", ") is very well done.  It lays out the required steps to get the machine running in a clear and logical manner (minus things like provisioning the Joule before build).  I do think it would benefit from a troubleshooting section, including screenshots of what the various steps should look like.", "So, summing up the lessons learned:", "Flash the Joule BIOS before anything else.  You will need a Windows 10 computer, the downloaded BIOS file, and the USB-C to A cable that comes with the Joule.  Be prepared to do this more than once, and to try different BIOS files - the first one didn\u2019t work for me.", "Install Ubuntu on the Joule next.  You will need the power supply, a micro-HDMI cable or adapter, a powered USB hub, a USB memory stick with a bootable Ubuntu image loaded, and a keyboard, display, and mouse.", "Pay obsessive attention to the build instructions, making sure that every part is oriented correctly.  When in doubt, check several pictures in the manual to be sure.", "The ROS learning curve is steep, and a lot of things are taken for granted.  Rviz looks friendly, but it\u2019s definitely not simple.  Reviewing tutorials on the ROS web site is very helpful, but plan to spend a significant amount of time figuring things out.", "Thank you so much for the detailed feedback ", "\nI agree with you for several points in sequence of assembly and software installation.", "\nI hope your feedback can help a lot of users for joining ROS community with TurtleBot3.", "\nWe\u2019ll see what we can improve on our manual.", "\nThanks a lot!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/turtlebot3-impressions-so-far/2907"},
{"title": "Turtlebot 3: Successfully upload Alternative Ubuntu Desktop 16.04 to Joule?", "thread_contents": ["When I set Turtlebot 3, I met some problems:", "\nIt is very hard to upload the Ubuntu image to Intel Joule board. I am not able to get my PC to connect to Joule board through USB-C successfully. And After that I am not able to upload Ubuntu image to Joule board successfully. It is very hard to find any useful info online regarding these issues.", "After some study, I finally got it work. Here I would like to share with you some critical info.", "1, Connect your PC with Intel Joule board via USB-C connector.", "\nBefore doing anything, we need to make our PC recognize and connect to Joule board. We should see \u201cIntel DnX device\u201d under \u201cUniversal Serial Devices\u201d in Device manager. If not, you have a problem.", "\nIf you get it work by following Intel\u2019s description. It is great. If not, it is normal. Here is what you need to do:", "\nAt first, follow the link ", ".  User \u201cvraoresearch\u201d provides a solution. (Thank you, vraoresearch.)", "\nIf you still cannot get it work, here is my suggestion:", "\nStep#1, disconnect 12VDC power supply.", "\nStep#2, Connect PC to USB-B of the Intel Joule board, if you want to see how Joule runs through UART terminal.", "\nStep#3, In Device manager in your PC, select \u201cShow Hidden Devices\u201d. And choose \u201cUninstall device\u201d on all failed devices.", "\nStep#4, Connect PC to USB-C of the Intel Joule board, at the same time press \u201cDnX\u201d button on the Joule board. Wait for PC to detect the DnX device and then release the button. If it does not work, try it multiple times.", "2, Upgrade the BIOS image to Intel Joule board.", "\nThere is a bug in the file DNX/Flash.bat in the downloaded and unzipped Joule BIOS image. \u201cclearRpmbFlag\u201d needs to changed to \u201cClearRpmbFlag\u201d. And its value should be defined as \u201cTRUE\u201d", "When running Flash.bat, you should see the following output:", "\nClearing NvStorage\u2026", "After this, you can disconnect PC from USB-C of Joule Board, and connect the 12VDC power supply, the board should be able to boot up properly, Then you go ahead to follow the description to use a USB flash drive to upload the Ubuntu image to Joule board.", "By the way, the bug I mentioned is found in the version 1J2 of Joule BIOS image. Since Intel discontinued Joule. I believe this is the last release of Joule BIOS image.", "Hi rknlhrqy,", "Thank you so much for sharing your method for solving Joule\u2019s USB-C issue when updating BIOS!", "\nI\u2019ll create a suggested solution link to your post from TurtleBot3 wiki FAQ page if you don\u2019t mind.", "Sincerely,", "\nWill", "Hi Will,", "\nNot at all. It is my great pleasure.", "\nThank you.", "\n\u2013Kening", "I\u2019m sooooooooooooooooooooooooooo happy \u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c\u315c  you are SOOOOOOO genius!!! I have suffered for a long long long!!! time. Because I couldn\u2019t enter the BIOS even after installing the BIOS successfully\u2026 At first I thought that the connection method of my HDMI wire would be a problem, but when I tried to connect the serial port with putty, I could not enter the BIOS too!!! I searched all the questions related to the bios of the joule board to find the answer. \u201cjoule Bios not boot\u201d, \u201cjoule board putty serial bios\u201d, \u201cjoule hdmi not output\u201d etc\u2026 For a very very!!! long~~~~~~ time \u2026 and I was able to solve the problem through \u201c\u201d\"\"\u201cyour answers\u201d\"\"!!!. My BIOS was the latest version, and Flash.bat was False. After I changed this to True, I was finally!!! free from all the problems. I can sleep well now. It\u2019s all for you. I really love you!!!", "Hi kingbob,", "\nI am thrilled by your email.    Thank you very much for the nice words.", "\nI am so glad that it did help people.", "-Kening", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/turtlebot-3-successfully-upload-alternative-ubuntu-desktop-16-04-to-joule/2224"},
{"title": "The MoveIt 2 journey (part 0): why MoveIt 2?", "thread_contents": ["This is the first of a series of articles that we will produce to describe our learning experience and contributions with the MoveIt 2 and ROS 2 communities. The MoveIt! motion planning framework is an open source software for motion planning, manipulation, 3D perception, kinematics, control and navigation. It\u2019s built on top of the Robot Operating System (ROS) and altogether, according to its maintainers, remains the third most popular package in the ROS world. MoveIt! provides an easy-to-use platform for developing advanced robotics applications, evaluating new robot designs, and building integrated robotics products for industrial, commercial, R&D and other domains.", "As MoveIt! evolves beyond R&D environments and starts getting integrated into more and more commercial and industrial products, it\u2019s only natural to look for improvements and seek alignment with groups attempting to use ROS on industrial and commercial environments.", "Over the last years some groups have extended ROS to obtain different degrees of determinism (e.g. [1]), however these attempts ended up with complex and use-case specific architectures that were never fully adopted by industry. As described at [2], ROS started in November 2007 as the development environment for the research-oriented Willow Garage PR2 robot. Due to its research nature, ROS did not considered many aspects relevant for industry. One of such was real-time compliance. ROS was never meant to guarantee deterministic responses, critical for an improved motion planning experience.", "With complete understanding of these limitations, around 2014, Open Robotics started designing a new version of ROS called ROS 2. Such version aimed to deliver upgraded features when compared to ROS 1 and was officially released the 8th of December, 2017. ROS 2 has been extended enabling support for teams of multiple robots (or robot modules), small embedded platforms or primitives to enable real-time robotic systems.", "As one of the companies leading the transition to ROS 2 and as the company behind ", ", the first modular robotic arm that runs ROS 2 natively, the use of MoveIt! seemed natural to us. While MARA can be operated ", " with MoveIt! (see ", ") through the ", " ROS package, it was always our desire to provide the best experience. And this brought us to ask ourselves: Why MoveIt 2?", "Our motivation:", ". While this can be fine tuned, doing so is technically complex. Moreover, it generally requires to have a powerful-enough computer and, even in the best case, unnecessary latencies (due to the layers of abstraction) appear in the overall control chain.", ". ROS 2 delivers upgraded features when compared to ROS 1. It has been extended including support for teams of multiple robots (or robot modules), small embedded platforms or primitives to enable real-time robotic systems. We can particularly highlight the following characteristics:", ". Our ", " technology allows to build robots whose components are all ROS 2-native. It\u2019s only natural that we aim for a motion planning framework that can similarly also operate in a ROS 2-native manner.", ": Open Robotics performed an initial analysis [3,4] of the capabilities one could achieve with ROS 2 in terms of determinism. These articles already highlight the importance of the computer architecture, the underlying operating system or the communication middleware used. All these aspects need to be taken into account when designing a real-time distributed system. This matter was further investigated by our group at [6,7,8,9] where we evaluated and characterized the ROS 2 responses on a (an embedded) system while researching and optimizing the underlying communications middleware (DDS), the Linux Networking Stack, the Linux operating system or the base link layer towards the creation of a firm-real-time capable distributed system based in ROS 2. Focused purely on ROS 2 layers (ROS Client Library (rcl), ROS middleware layer (rmw), etc.), Maruyama et al. [5] conducted an experimental study aimed at comparing the performance of ROS and a preliminary version of ROS 2, under different types of Data Distribution Services (DDSs). More recently, member of Apex.AI wrote a short introduction[10] summarizing optimizations required in the ROS 2 layers such as the use of safe data types, split memory allocation in init and runtime phases, remove all blocking calls or implement real-time safe log output handler among others.", "\nAll these optimizations are but pre-requirements to obtain real-time distributed motion planning over ROS 2. It\u2019s also required to optimize the motion planning framework itself. This will and end-to-end real-time motion planning experience.", "\nMoveIt 2 is the place where to do so. The adoption of ROS 2 facilitates the real-time optimization of the underlying layers and paves the path towards real-time motion planners. Preliminary work (though not connected to ROS 2 just yet) is available at ", " (", ").", ". According to some of its core maintainers, \u201cmany companies and R&D teams report they will not migrate to ROS 2 until the MoveIt 2 port is complete\u201d. Looking at past releases of ROS, this was similarly true when Kinetic was released; months later, when MoveIt! was ready for Kinetic, adoption picked up dramatically.", "Motivated by the technical and strategical benefits, our team announced a few weeks ago ", ". We plan to deliver to the community a first working prototype of MoveIt 2 with MARA, as ", ", and have partnered with ", " to benefit from their expertise in motion planning.", "Our work is happening at ", " and we\u2019re contributing directly to the official MoveIt 2 repository (", ") through Pull Requests. In barely a few weeks, the repository has about 60 starts and active involvement from members of the MoveIt maintainers group. We encourage everyone to jump in and contribute to MoveIt 2.", "We\u2019d like to credit our partners at PickNik for their support in the process. Similarly, we\u2019d like to thank William Woodall, Dirk Thomas, Tully Foote or Robert Haschke for their insights and contributions in this first phase of the project.", "The unofficial MoveIt 2 logos are the result of ", ". We\u2019ve made them available for everyone.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", ": Supported by companies from all around the world that collaborate with open source contributions to make ROS 2 the de facto standard for robot application development.", "\n", ": ROS 2 was built from the ground up to be industry-grade and used in production environments, including high reliability and safety critical systems.", "\n", ": first class support for Linux, Windows, and macOS plus a tiered support model that allows for seamless ports to new platforms, such as real-time and embedded OSs.", "\n", ": Layered architecture which allows for the co-existence of both open source and proprietary solutions while facilitating the comparison of different technologies for each layer.", "\n", ": ROS 2 is ready for use across a wide array of robotics applications, across several domains. From indoor to outdoor, home to automotive, underwater to space, and consumer to industrial.", "\n", ": The default communications method in ROS 2 uses the industry standards DDS and RTPS, which are already widely deployed in a variety of industrial applications, from factories to aerospace.", "\n", ": ROS 2 provides the robotics tools, libraries, and capabilities needed to develop applications, allowing developers to focus on the work that is important for business.", "\n", ": ROS 2 code is licensed under Apache 2.0 License, with ported ROS 1 code under the 3-clause (or \u201cnew\u201d) BSD License. Both licenses allow permissive use of the software, without implications on the user\u2019s intellectual property.", "\n", ": Over 10+ years the ROS project has produced a vast ecosystem of software for robotics by nurturing a global community of hundreds of thousands of developers and users who contribute to and improve that software. ROS 2 is developed by and for that community, who will be its stewards into the future.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "[1] Wei, H., Shao, Z., Huang, Z., Chen, R., Guan, Y., Tan, J., & Shao, Z. (2016). RT-ROS: A real-time ROS architecture on multi-core processors. Future generation computer systems, 56, 171-178.", "[2] Gerkey, B., Why ROS 2.0? Retrieved from ", "\n", "[3] Kay, J., Introduction to Real-time Systems. Retrieved from ", "\n", "[4] Kay, J., Proposal for Implementation of Real-time Systems in ROS 2. Retrieved from ", "\n", "[5] Maruyama, Y., Kato, S., & Azumi, T. (2016, October). Exploring the performance of ROS2. In Proceedings of the 13th International Conference on Embedded Software (p. 5). ACM.", "[6] Guti\u00e9rrez, C. S. V., Juan, L. U. S., Ugarte, I. Z., Goenaga, I. M., Kirschgens, L. A., & Vilches, V. M. (2018). Time Synchronization in modular collaborative robots. arXiv preprint arXiv:1809.07295.", "[7] Guti\u00e9rrez, C. S. V., Juan, L. U. S., Ugarte, I. Z., & Vilches, V. M. (2018). Real-time Linux communications: an evaluation of the Linux communication stack for real-time robotic applications. arXiv preprint arXiv:1808.10821.", "[8] Guti\u00e9rrez, C. S. V., Juan, L. U. S., Ugarte, I. Z., & Vilches, V. M. (2018). Towards a distributed and real-time framework for robots: Evaluation of ROS 2.0 communications for real-time robotic applications. arXiv preprint arXiv:1809.02595.", "[9] Guti\u00e9rrez, C. S. V., Juan, L. U. S., Ugarte, I. Z., Goenaga, I. M., Kirschgens, L. A., & Vilches, V. M. (2018). Time Synchronization in modular collaborative robots. arXiv preprint arXiv:1809.07295.", "[10] Pangercic, D., ROS 2 and Real-Time, Retrieved from ", ".", "[12] ", "\n", "[13] ", "\n"], "url": "https://discourse.ros.org/t/the-moveit-2-journey-part-0-why-moveit-2/8461"},
{"title": "MoveIt GSoC Project 2019", "thread_contents": ["Originally posted on the ", ".", "We are pleased to announce that MoveIt was reselected for a Google Summer of Code (GSoC) grant through OSRF. GSoC is an international program in which Google awards stipends to students who complete a free and open-source software coding project during a three month period.", "Jens Petit (", ") will be working on a collision checking project this summer in conjunction with another project by Omid Heidari (", ") to add the TrajOpt motion planning library.  Felix von Drigalski (", ", ", ") and Bryce Willey (", ", ", ") will be the primary co-mentors along with guidance from Dave Coleman (", ", ", ") and the MoveIt Maintainers community.", "Collision detection is a core necessity in path planning for any type of robot. Currently, MoveIt utilizes the Flexible Collision Library (FCL) for this", "\ntask. Inspired by the motion planning framework \u201c", "\u201d of ROS-Industrial, the aim of the project is to integrate Bullet as an additional collision checking library into MoveIt. This includes additional new features like continuous collision detection (CCD). CCD is useful as many state-of-the-art path planning frameworks like ", " (TrajOpt) rely on it.", "To see the progress of this effort this summer, follow along ", ".", "We would all like to thank GSoC and Open Source Robotics Foundation (", "), as the sponsoring recipient organization, for making this grant possible and to the mentors volunteering their time.", "We are excited to see the progress made this summer!", "Since Jens got introduced to ROS through a lab course of his robotics master degree at TU Munich in Germany, he has been hooked. Building robots with ROS is at the same time powerful and fun. Jens is excited to contribute to such a popular package as MoveIt and happy to be part of a vibrant community of robotics experts. Besides coding this summer, he can be found  practicing Aikido or hiking in the beautiful Alps.", "Hi, I am Nishant Sachdeva. I am looking to work with you people for Gsoc 2020. I have worked with C++ quite a bit and have recently begun working with ROS. I am a student at IIIT Hyderabad and am currently working with the Robotics Research Lab . I would love to be of any help possible and am willing to learn and work on any and all ends possible.", "My github id is nishant-sachdeva", "Hello Everyone, My name is Vedant Mundheda. I want to work with MoveIt for Gsoc 2020. I have some experience with working on ROS and have done a few projects regarding the same. I also work with C++/C in my college projects. I am a keen learner and am willing to put the hard work for any work assigned.", "Vedant Mundheda", "\nIIIT Hyderabad", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Integrate Bullet as a collision detection library", "Implement continuous collision detection using Bullet", "Tutorial on how to implement additional collision checking libraries into MoveIt"], "url": "https://discourse.ros.org/t/moveit-gsoc-project-2019/9579"},
{"title": "Great success for ROS on DARPA Subterranean Challenge!", "thread_contents": ["Just here to mention that ROS powered (AFAIK) 11 out of the 12 teams that took part in the Tunnel Circuit of ", ". Good job, ROS community! Read the great news and watch the awesome videos at ", " and ", ".", "Was it ROS1 in all 11 cases or some of them used ROS2?", "No idea about that\u2026 We (CTU-CRAS) use Melodic everywhere\u2026", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/great-success-for-ros-on-darpa-subterranean-challenge/10391"},
{"title": "ROS@CES2020 Meetup", "thread_contents": ["Hi all, I\u2019m your friendly neighborhood ROS (developer? advocate? TSC rep?) person!", "As CES 2020 descends on us once again in Las Vegas this week, I wanted to setup a casual meeting of the ROS community that may be in attendance. Do you like robots? Craft beer? Well, you\u2019re in luck! We\u2019re going to meet at the ", " inside The Palazzo ", ".", "Don\u2019t like robots? Not sure how you\u2019re reading this, but you\u2019re a strange creature and I like it. Don\u2019t like craft beer? That\u2019s alright, the nectar of the gods isn\u2019t for everyone, but they have good food and cocktails too.", "If you\u2019re interested in attending, shoot me a note or comment below. If we have a large enough crowd I\u2019ve been in contact with the managers to potentially rope off an area.", "Happy Roboting! Hope to see you in Vegas", "Steve - Samsung Research", "I\u2019ll be around and will try to make it, thanks for organizing Steve!", "Did you mean Tuesday the 7th? (Tomorrow)", "If so I\u2019d love to join and I will bring +5 (or 6) ", "Awesome, thanks for organizing!", "Good call. Yes, the 7th! It\u2019ll be good to chat again from ROSCon.", "Sorry, arriving the 8th.", "Great initiative. I have several meetings before but I will try to pass by.", "\n(Btw, who\u2019s going to the Intel Realsense event just before?)", "Great to hear it\u2019s Today (Tuesday the 7th). I\u2019ll see you there!", "By the way, on ", " ", " my company and I are throwing an event here at CES2020 for robotics companies (link below).", "There\u2019ll be a lot of robotics folks there, and we\u2019d love to have any other ROS community friends join. All robots showing up are powered by ROS (including one of ours) ", "Here\u2019s the sign up link. ", " The password is \u201cfreedomrobotics.ai\u201d. If you sign up, message me directly and I\u2019ll make sure you bypass the wait-list!", "I\u2019m up at the bar eating, but reservation for a table at 7!", "I\u2019m in a space invaders shirt. Hard to miss.", "Based on the background, that looks like it was taken in Macau.", "Promise, its Vegas.", "What a con that would be, fake a ROS meetup without pictured folks to corroborate ", "Ask this guy for proof:", "i wish i could be there, i got stuck in some other places in US.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-ces2020-meetup/12181"},
{"title": "Confusion about EOL for Indigo", "thread_contents": ["According to ", ", EOL for ROS Indigo is April, 2019. However, it also says \u201c(Trusty EOL)\u201d, which, according to ", ", is not until April 2022 (End of Standard Support, however, is April 2019). Which is the real EOL date for Indigo? If that date has passed, is the community OK with me updating ", " to remove the green highlight from the Indigo row in the table?", "Indigo is approaching EOL. I announced the previous sync as the penultimate one: ", "Trusty is effectively EOL now. That chart you link to is deceptive. April 2019 has been the EOL date, but there\u2019s now new paid service from Canonical called Extended Security Maintenance which you can use, but it\u2019s not the general release support we\u2019re used to.", "Lets update the Distributions file in a week after the final sync.", "Thanks for the info, ", ". I will wait for final sync.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/confusion-about-eol-for-indigo/9065"},
{"title": "Safety working group meeting and call for participation", "thread_contents": ["The ROS 2 Safety working group will be meeting this week on ", ".", "We haven\u2019t met since early December and we have not done a great job of achieving our goals, so this meeting will be as much about trying to inject some energy back into the group as anything else.", "The safety working group currently is focusing on producing a catalogue of architectural patterns commonly used in safety-critical systems and how to implement them using ROS 2. You can find the catelogue (currently empty) on the ", ".", "We also welcome any other ideas on things the working group could achieve that would help the ROS community use ROS in building safe robots.", "Join Zoom Meeting", "\n", "Meeting ID: 266 509 195", "One tap mobile", "\n+19292056099,266509195# US (New York)", "\n+16699006833,266509195# US (San Jose)", "Dial by your location", "\n+1 929 205 6099 US (New York)", "\n+1 669 900 6833 US (San Jose)", "\n+81 524 564 439 Japan", "\n+81 3 4578 1488 Japan", "\n+49 69 7104 9922 Germany", "\n+49 30 3080 6188 Germany", "\n+49 30 5679 5800 Germany", "\n+34 917 873 431 Spain", "\n+34 84 368 5025 Spain", "\n+44 203 051 2874 United Kingdom", "\n+44 203 481 5237 United Kingdom", "\n+44 203 966 3809 United Kingdom", "\n+44 131 460 1196 United Kingdom", "\nMeeting ID: 266 509 195", "\nFind your local number: ", "I\u2019ll join this one a bit late but will be there!", "+1 for the \u201ccall for participation\u201d, ping to groups involved in security matters.", "involved", "+1  for the \u201ccall for participation\u201d", "Minutes are available here:", "ROS safety working group repository", "Apologies ", " and everyone else in the call I could not join at the end. Something urgent came up. Will try my best on the next telco.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/safety-working-group-meeting-and-call-for-participation/12489"},
{"title": "Using ROS in commercial OpenEmbedded environment?", "thread_contents": ["Can someone suggest resources (web links, blogs, books, etc\u2026) that discuss the implications of running ROS in a commercial product.  Specifically, a commercial product running a Yocto/OpenEmbedded derived custom Linux distribution.  Although, a ROS product based on a commercial distribution of Linux would likely have all the same issues.", "I realize there is a whole spectrum of issues that might be involved.  A few of which might be: handling ROS version issues, ROS system configuration, open source license management and compliance, build and testing management, and then the more nitty-gritty stuff that might be tweaked in packages to better support commercial use where customers expect things to just work without a lot of tweaking on their own.", "I have a feeling a lot of the service and industrial robots that are using ROS are solving these types of issues on their own, but I am wondering if there is a watering hole on the web where people concerned with these issues gather to share knowledge and best practices so ROS support from a variety vendors looks more unified from an end-customers standpoint.", "Thanks,", "Mike", "Hi Mike!", "I\u2019m neither ROS nor OpenEmbedded expert, but I\u2019ve spent some time working with both projects and my gut feeling is that there\u2019s maybe just a couple of companies using ROS on top of OpenEmbedded in production internally at the moment. I\u2019m pretty sure nobody builds real commercial products or something that goes beyond a research projects. And I know what it takes to build a product (In my previous life I worked on the integration system used to build Nokia N9).", "I guess the best way to find out what people are concerned about is to have a look at the programs of ROS (and ROS Industrial) conferences. You\u2019ll see that the majority of ROS users are researchers happy with Ubuntu as a baseline for ROS.", "BR,", "\nDmitry", " - this is something that I am interested in as well.", "Dmitry, thanks for the response.", "For good historical reasons, ROS seems strongly focused around Ubuntu as a base platform, but Ubuntu isn\u2019t really suitable platform for actual embedded products intended to work in a ROS ecosystem.  Its akin to placing an aircraft carrier nuclear power plant into a speed boat.  I\u2019m very happy and grateful to see that others had already blazed the trail on getting ROS working over OpenEmbedded as it at least sets a direction and makes the integration possible, even if it is not easy.", "I\u2019m just at the beginning of this process, but I\u2019ll be happy to share my experience with others as our development progresses on a commercial product based on ROS running on an OpenEmbedded-based custom Linux.  Hopefully I\u2019ll find others on a similar journey with which to compare notes.", "Mike", " I can not say much about ROS on OpenEmbedded. But can give you some advice on OpenEmbedded/Yocto for product development. We are using Openembedded for about 4 years for our camera firmware development. I kow none other system which provides you the services OpenEmbedded do.", "\nWe do automatic builds based on Jenkins and create full software update packages bundled with our application software.", "Best,", "\nChristian", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-ros-in-commercial-openembedded-environment/1428"},
{"title": "Low level encapsulation of Mavros messages using MAVLink", "thread_contents": ["Hi all,", "I\u2019d like to have a go at implementing a UART based Mavros client / server implementation, The client will be running on a homegrown ARM Cortex F4 flight controller and the server side I plan to implement on a more powerful \u201cCompanion Computer\u201d, probably a Raspberry PI.", "I\u2019m struggling to find any detailed information about how other systems perform the encapsulation with MAVLink as I would like to develop code that I could easily re-use with other systems.", "I now have a basic understanding of ROS but I still have a few moderately fundamental questions with regard to Mavros that I\u2019m trying to find answers for. I have started to have a look though some of the implementing code bases such as PX4. However, there is a lot of code and this is taking a while!", "Below are  a few \u201coverlapping\u201d and pretty basic low level questions about the MAVLink encapsulation that I\u2019m hoping someone can answer, or perhaps point me at some appropriate code or a technical description (I have unsuccessfully looked for this).", "Many thanks!", "Hello,", "I didn\u2019t fully understand what you tried to do. But here is some clue:", "\nMAVROS is just a bridge between ROS messaging and Mavlink.", "About Mavlink usage. The only need message is really the heartbeat as it contains essential status information and implement the mavlink routing feature. Mavlink C implementation already got big helpers function to help you implement the sender and receiver functions.", "Mavlink common.xml implements a set of common usefull message. If those don\u2019t suit you, you can easily create your own set of message \u2026 but you will need to compile MAVROS yourself to include your messages. MAVROS only undestand message from ", "You will find more help here :", "\n", "\n", "It\u2019s trying to say: \u201cPreArm: Compass not calibrated\u201d  This is me trying to read \u201cuseless\u201d heartbeats \ud83d\ude13  At least I can add that Pixhawk also sends warning messages without stream request.  Thanks for sharing!  I\u2019ve been following old tutorials for...", "\n", "Thanks for your reply.", "I understand how MAVLink works and have a good API level understanding of ROS.", "Essentially I\u2019m trying to determine how a serial Mavros bridge works, but at the lowest level. i.e. How to implement a client capable of talking to an other Mavros compatible systems via a serial port using MAVLink as the transport layer. I suspect that it\u2019s straightforward, but it\u2019s difficult to determine precisely without any documentation or by looking though existing code bases like PX4.", "I have now brought a Pixhawk that I intend to setup and use with the existing Mavros tools. Once working, I\u2019ll monitor the serial comms and this will hopefully fill in the gaps in my current understanding of how it works.", "ROS appears to be very well documented at all levels, but as far as I can tell this isn\u2019t the case for Mavros. Once I\u2019ve figured it out I will post a description just in case anyone else ever needs build their own implementation.", "Meanwhile, any low level insights would be gratefully received!", "Mavros only write mavlink message on serial nothing more.", "\nWhat you want is basic mavlink routing. You want to send a message from a mavlink device to another", "\nMavros will connect to only one mavlink ID.", "\nSo you need :", "\nMavros ( id=1,cmpnt=255)< - - uart - - > device (1,1) <\u2013uart\u2013> device2 (2,1) <-- uart\u2013> mavros (2,255)", "\nNormally, mavros filter messages by id via ros plugin api. You can easily add more plugin to handle more message or catch messages from /mavlink_from", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Which MAVLink message ID or IDs are used?", "Are these custom MAVLink messages? i.e. in the range 180 - 229 and thus application specific.", "Do distinct Mavros message get encapsulated as specific custom MAVLink message?"], "url": "https://discourse.ros.org/t/low-level-encapsulation-of-mavros-messages-using-mavlink/7247"},
{"title": ".NET bindings for ROS2", "thread_contents": ["This is just a place to continue the discussion that started in ", "It\u2019s great to have better place for discussions than the github issue tracker.", "As already said before I would like to join efforts regarding a .Net wrapper. In the mean time I will go on with my own wrapper.", "\nI was able to make progress with my wrapper ( ", ") the last few days and I\u2019m trying to implement services tomorrow. Nevertheless as mentioned ", " I\u2019m currently running into some major issues with arrays.", "But I still believe that a fairly flat wrapper without some glue code should work fine in C# and might might also provide better performance. .Net provides a nice mechanism called pinning which allows for a certain set of data types to  pass a pointer of a managed memory section to unmanaged code without doing the usual copy.", "From a usability point of view I think it would be nice be create an api with the same names and usage like the rclcpp. So users can easily switch from c++ to c# or the other way round.", "As already said before I would like to join efforts regarding a .Net wrapper. In the mean time I will go on with my own wrapper.", "That sounds great, I fully agree that a single wrapper is much better for users, and please continue with your efforts! I don\u2019t know if the way I wrote the wrapper is the most .NET idiomatic way, I\u2019m fairly new to the platform/language. My only advice is that you could reuse empy, the templating engine used in rclpy and rcljava (and pretty much all of ROS2), since that\u2019ll help get you more contributors and to better integrate rclcs with ROS2.", "But I still believe that a fairly flat wrapper without some glue code should work fine in C# and might might also provide better performance. .Net provides a nice mechanism called pinning which allows for a certain set of data types to  pass a pointer of a managed memory section to unmanaged code without doing the usual copy.", "I\u2019m far from being a .NET expert, so given that you\u2019re more knowledgeable about this, I trust your judgement ", "  My wrapper is just a .NETified version of my Java wrapper, so I don\u2019t know of any better way of passing structures around.", "From a usability point of view I think it would be nice be create an api with the same names and usage like the rclcpp. So users can easily switch from c++ to c# or the other way round.", "That\u2019d be great, though the recommended path is DDS implementation <-> RMW <-> RCL <-> Language-specific library. rclcpp is meant to be the C++ client library, not a foundation library upon which other libraries can be built. For example, rcljava is built on top of RCL, despite the bindings being written in C++, because that way there\u2019s a clear separation between the base layers and the client libraries.", "My only advice is that you could reuse empy, the templating engine used in rclpy and rcljava (and pretty much all of ROS2), since that\u2019ll help get you more contributors and to better integrate rclcs with ROS2.", "My intention is to finaly use the templating engine. But when I took a first look at how it is used in the rosidl_generator_c it looked terrible complicated. So I decided to hack my own message generator.", "I\u2019m far from being a .NET expert, so given that you\u2019re more knowledgeable about this, I trust your judgement", "I wouldn\u2019t call myself a .Net expert, I actually spend the most time the last two year coding in C++, but there\u2019s some stuff that much easier to do in C# than in C++.", "That\u2019d be great, though the recommended path is DDS implementation <-> RMW <-> RCL <-> Language-specific library. rclcpp is meant to be the C++ client library, not a foundation library upon which other libraries can be built", "My .Net wrapper codes is built upon the RCL. I just wanted to say that it would be nice to have roughly the same naming/usage pattern like in the rclcpp. Building a wrapper upon C++ code would be very very difficult.", "Hi,", "\nI was able to make some progress with my wrapper. As already said on my post on the mailinglist ( ", ") most of the essential features are working now.", "\nI even wrote some examples: ", "But nevertheless I\u2019m running into some problems with the memory management when using arrays. I wrote a fairly detailed article about that in the documentation of my rclcs ( ", " ). Could you perhaps take a look at it and/or tell me how you solved the problem, when to free manually allocated memory inside the struct that wraps an array?", "My idea was to create something like a Message wrapper around the struct, which prevents the struct from beeing copied. The usage of a message would then be:", "Message<test_msgs.msg.Dummy> msg = new Message<test_msgs.msg.Dummy>();", "\nmsg.Data.field name = \u2026", "But I think this isn\u2019t very elegant. But having to free your memory by hand also isn\u2019t very nice.", "Sorry for the slow response.", "I just had a look at ", " I see you found a way to solve this, that\u2019s great!", "The way I solved it is a byproduct of converting between C and Java structures, since the Java messages are pure Java and are only converted to their C counterparts when they are published/received, I can control how and when to allocate/deallocate.", "Do you still have to explicitly deallocate the wrapped structures in C#? Are structs automatically destroyed in C#?", "Hey,", "\nmy solution was to create a (autogenerated) class wrapper around the message struct. The wrapper contains a single instance of struct. For simple types and arrays the code generator generates getter and setter properties which directly operate on the struct (with some nice enhancements for dealing with strings and arrays). Only nested types were a bit nasty to implement.", "In order to solve the memory allocation/deallocation problem the wrapper implements the IDisposable interface. This means at destruction (and on calling .Dispose() ) the wrapper calls .Free() on the struct. In the Free method the struct steps via reflection through all fields check if they also have Free method and calls it.", "\nSo it\u2019s still necessary to free unmanaged memory, because I need to allocate/deallocate unamanged in order to be able to pass unbounded arrays. The main difference is: the user doesn\u2019t have to call .Free() manually.", "I think it might get a bit clearer by having a look at the generated message:", "\n", "\n", "\n", " could you perhaps put your .Net wrapper code on github? I\u2019m still running into some troubles with my code on windows and wanted to have look at your code. (Even though we are using different approaches it would be nice to see if the problems I\u2019m running in are due to ros2 on windows or are caused by my own code)", " hey, sorry for the slow response, I totally forgot about this. I just made the repo public ( ", " ), it\u2019s not as complete as yours, but hopefully you may find it useful. Things you might want to consider:", "Instead of using ", ", use ", " / ", " (see ", "). The reason is that with ", " you can\u2019t pick which RMW to load at runtime, so right now ", " uses whatever RMW implementation is the default. It works fine if you only have one RMW implementation (such as FastRTPS), but it won\u2019t if you have additional ones.", "The CMake cruft I had to add is rather ugly. .NET Core will again switch to a new build system, so I added support for dotnet\u2019s ", ", but also for the current ", " format on Windows. Eventually everything will consolidate into only one build system ( ", " and ", "), but for the time being I added support for both ", " and ", " projects.", "Works on Windows (tested on Windows 10 and Visual Studio Community). I originally added support for ", ", but I\u2019m afraid that after a few changes for supporting Windows, I broke the support for Linux. It should be fairly straightforward to fix, just a few tweaks to the CMake scripts, I think.", "Let me know if you need anything else, and of course, feel free to take any code you may find useful, it\u2019s all Apache licensed.", "Instead of using DllImport, use LoadLibrary / dlopen", "I don\u2019t really see the advantage of using dlopen directly. The .net way would be to create a class that contains the DllImport statements foreach RMW implementation and that is implemented against an interface or baseclass. Than you have some logic that decides at runtime which concrete class should be used and create your code against the interface.", "\nOtherwise you are likely to mix some code that decides which rmw implementation to use with the code that actual call the native function. I think that would be a rather ugly design.", "Also I couldn\u2019t experience any problems with multiple RMW implementations (I have FastRTPS and OpenSplice installed). I would be suprised if there where any problems because \u201cnormal\u201d enduser applications just link to the default rmw too.", "The CMake cruft I had to add is rather ugly", "The implementation into the buildsystem is still something I\u2019m kind of afraid of. I will have a deeper look into your CMake code, but I think that\u2019s a point I could really use some help.", "Works on Windows (tested on Windows 10 and Visual Studio Community).", "I need to do some further debugging with my implementation on windows. I\u2019m running into some troubles with strings on Windows, which aren\u2019t passed correctly into the native code.", "Thanks for publishing your code. I\u2019m sure I will find some useful parts in it.", "I don\u2019t really see the advantage of using dlopen directly. The .net way would be to create a class that contains the DllImport statements foreach RMW implementation and that is implemented against an interface or baseclass. Than you have some logic that decides at runtime which concrete class should be used and create your code against the interface.", "Sure, that\u2019s doable, but the way ", " is currently laid out only works with a single RMW implementation. For example, ", " loads ", " ( ", " ), over which the user doesn\u2019t have any control. If you have both OpenSplice and FastRTPS installed, you can\u2019t pick which one to load.", "Otherwise you are likely to mix some code that decides which rmw implementation to use with the code that actual call the native function. I think that would be a rather ugly design.", "I guess that\u2019s a matter of different tastes, but that\u2019s the design we did for ", " and thusly, the one I followed for ", ". But given that the .NET mechanism for using native libraries is more powerful, it may not make sense to use the same design with .NET, so I don\u2019t really know. The approach you described sounds really clean and should work fine.", "Also I couldn\u2019t experience any problems with multiple RMW implementations (I have FastRTPS and OpenSplice installed). I would be suprised if there where any problems because \u201cnormal\u201d enduser applications just link to the default rmw too.", "How do you choose which RMW implementation to use? I might have missed something, but it seems to be me that ", " loads ", " directly and you can\u2019t pick which one to load at runtime.", "From my current understanding also a c++ program written by an end user will usually link only against the default rmw. I\u2019m aware that it is possible to link against a non-default rmw but I\u2019m not sure that this is the intended use for endusers. (There\u2019s an issue tracker on github where we discussed this matter and my understanding was that linking against non-default rmw is usually used for testing purposes)", "\nSo the way I\u2019m switching the rmw implementation at the moment is simply recompiling ros2 with the RMW_IMPLEMENTATION variable defined.", "If desired I will implemented a mechanism that allows choosing the rmw implementation in C# at runtime. Perhaps ", " or ", " could comment on this.", "\nImplementing the pattern I described in the previous post, will also have the advantage that it\u2019s possible to have special code paths for different plattforms. (For example different library names on windows) So I will probably go for that.", "At the moment there is unfortunatly a lot of other stuff to do for the wrapper. For example I\u2019m currently refactoring my message generator to use the C# CodeDom (or in future the Roselyn code synthesize interface), furthermore I need to fix the issue relating to windows. And there are still a lot of unit-tests that needs to be written. So it might be a while before I will implement a mechanism for choosing the rmw in the C# code.", "Update on rclcs!", "\nI finally found some time (university is more time intensive than one might think) to work at the rclcs again.", "The next big thing I\u2019m going to do is provide a simple abstraction layer inside the rclcs to implement different codepaths for linux and windows.", "I\u2019m still looking for helpers who might want to help porting to Windows!", "Next update:", "I implemented multiple codepaths for linux and windows that get chosen at runtime. That should make a working implementation on windows much easier.", "\nFurthermore I fixed some memory cleaning bugs and cleaned up some classes.", "\nThe demonstration workspace now also uses the dotnet_cmake_module.", "Another update:", "I implemented  support for msbuild projects. That means the user has almost the same workflow like developing a standard .net application (Code and debug in visual studio or monodevelop) . It also means that I don\u2019t need that many different code paths in the cmake part of the build system.", "Then I managed to compile ros2 on windows and my testing workspace without any problems. Running some simple testprograms that simply creates a node and spins it seem to work fine now.", "\nIn order to solve that I needed to force 64bit builds. (otherwise windows thinks - ohh let\u2019s run this .net application as 32bit program and crashes when I try to call functions the the rcl)", "Next I\u2019ll try setting up a simple publish and subscribe example on windows.", " you are my hero - thanks for developing a C# sharp library for ROS.  Will this be based on .Net Core?", "I\u2019m new to ROS and trying to pick up Python but for me, C# is the most beautiful language out there and the IDEs for C# are always great.  Having to learn another language has been putting me off ROS for ages.  Can\u2019t wait to test drive your binding.", "I\u2019m happy to see that someone is interessted in the C# wrapper. But first - I didn\u2019t put any work in this for 2 month or so. It probably won\u2019t work out of the box. Also I couldn\u2019t get it to work on windows the last time I tried.", "Regarding .net core. It is based on the normal .net framework but the classes used should also be available in .net core. So it would just be building against .net core instead of normal .net.", "For serious developments I recommend using C++ or Python with ROS2. You will have to put more work into this wrapper before it can be used properly.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "The rclcs finally works again with the current rcl", "I moved to the dotnet_cmake_module esteve provided (Will merge the corresponding branches of the rclcs after ", " merged my PR to the cmake module)", "The did a complete rewrite of the message generator using the C# CodeDom interface", "\n** To be fair: I\u2019m still running into major trouble regarding nested messages. It\u2019s really hard to generate them and provide a memory save wrapping interface without to much overhead. (But I\u2019m working on this)"], "url": "https://discourse.ros.org/t/net-bindings-for-ros2/460"},
{"title": "Zethus - Browser based realtime ROS data visualization", "thread_contents": ["Hi all,", "Rapyuta Robotics team is happy to announce the open-source release of Amphion and Zethus, two javascript libraries to visualize ROS data from remote websocket endpoints in the browser. Zethus is the browser equivalent of RViz, built on top of Amphion. Amphion can be seen as an alternative to ros3djs with enhancements to design and performance.", "\n", "\nRoboticists familiar with RViz will find it very similar. Just connect to a websocket endpoint and click on Add Visualization.", "\n", "\nDevelopers can also include it in their web applications. Instructions can be found on ", "\nGetting started with any visualization only takes a few lines of code.", "\nSyntax for each visualization is available ", "We extensively use them to build powerful browser-based user interfaces to develop and operate our robotics solutions built around ", ". Here are some examples for inspiration.", " ", " ", " ", "\n", "We\u2019ve added support for PlanningScene, DisplayTrajectory and a lot of Image encodings. (", ", ", ")", "\nWe received contributions from our friends for Point, Range and Wrench (", ", ", ", ", ")", "\nWe also made improvements to the performance of PointClouds and Images. (", ", ", ")", "\nAnd rectified bugs in the fixed frame selection. (", ")", "\nThe current features in development include support for interactive markers.", "\nWe\u2019re working on widgets ", " ", " which should see significant additions in the upcoming weeks", "We sincerely hope this work helps you to build powerful user interfaces. We will be very happy to hear feedback and welcome any contribution.", "We\u2019ll be delighted to receive PRs, suggestions, advice and ideas from the community so we can together make it better.", "A good place to start is the Issues section on Github", "\n", "Visualization objects for ROS messages based on three.js. Primarily built for use in Zethus - rapyuta-robotics/amphion", "\n", "\n", "Realtime robot data visualization in the browser. Contribute to rapyuta-robotics/zethus development by creating an account on GitHub.", "\n", "We\u2019re committed to the browser as a medium for ROS visualizations. It is in line with our vision to make robots more accessible. We believe this is just the beginning of ROS based web visualizations.", "Best regards,", "\n", "I love all these projects moving alot of the RVIZ tools to the web! I really hope one day (soon) we deprecate rviz in favor of one of them.", "I\u2019m curious on your thoughts comparing and contrasting Cruise\u2019s webviz / worldview projects (", "). Obviously by just supporting non-bagged data this is immediately more easy to play with. I\u2019d love to hear your thoughts on each.", "From what I notice about the current state of Webviz and Zethus, they vary on the purpose they were initially built for.", "WebViz seems to focus on timeseries data analytics. It has support for graphs and logs. And the panel interface is a good way to anaylze a single dataset across time or to compare two graphs on different panels. But it doesn\u2019t support a robot model.", "Zethus is more suited as a visualization than as an analytics tool. It only has a single 3d scene. It provides nothing to see the x coordinate of pose data. The current focus is on the visual representation of the robot itself - robotic arms, agvs, drones etc. To visually see how the robot moves and to interact with it on the 3d interface itself. There are plans to show graphs, rosout and other text logs. And also to support other data sources (ROSBags as one of them) but these features are currently not in active development.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/zethus-browser-based-realtime-ros-data-visualization/10489"},
{"title": "Lecture notes specifically for \"Programming Robots with ROS\"", "thread_contents": ["For better or for worse I\u2019ve concluded that the best \u201ctextbook\u201d for teaching ROS to Python developers is \u201cProgramming Robots With Ros\u201d by Quigley, Gerkey and Smart. It is a few years old and uses Indigo. Many of the code and other instructions don\u2019t work exactly as written. But the sequence of explanation and examples is really excellent.", "My question: have any of you developed course notes, powerpoints or similar to go with that particular book that you\u2019d be willing to share? I am starting to write some myself to see how it goes and thought I would reach out.", "If you see this book, you may change your mind", "\n", "Hi everyone, \ud83d\ude42  I\u2019m happy to announce a new ROS book: \u201cROS Robot Programming, A Handbook is written by TurtleBot3 Developers\u201d. Now, this book has been published English and Chinese versions. You can download the pdf of this book.  The authors of the...", "\n    ", "\n", "I\u2019ve looked very closely at that book and its quite good. However it uses only C++ while I think for teaching purposes python is much better suited.", "For teaching students they should be well versed in both C++ and Python, and perhaps any other language tricks of the trade. It would be doing them a disservice as with ROS in my experience I actively use both, all the time, sometimes both in the same day to achieve a goal.", "I have been teaching ROS with C++ at least two times at Universities, and it was clear that students struggled more with C++ syntax and build requirements that with ROS concepts.", "\nLast ROS courses I used Python with student with zero background on Python and it was much more easier for them to grasp faster ROS concepts.", "\nSo, I think, unless you have students with good background in C++, it will be painful for them.", "\nIt seems with new C++ 11 supported in ROS2, programming in C++ with ROS will be less tedious.", " I totally agree and have reached the same conclusion. Our students know Java and Python (and many know additional languages. But C++ is a rare bird.) I do agree that to get a \u201cjob\u201d in the ROS world you probably are going to need to know C++ but for teaching Python is the way to go.", "(Anyway I have no desire or intention to start a debate on this. Everyone has their own experiences.)", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/lecture-notes-specifically-for-programming-robots-with-ros/5729"},
{"title": "Information for those using Raspberry Pi, Ubuntu Xenial, and ROS Kinetic", "thread_contents": ["This is \u201chopefully\u201d a point in time statement:", "The following refers to images from this source: ", "For those who are using or plan to use the Ubuntu 16.04 Xenial server builds for Raspberry Pi 3 (and 2), it is important to immediately disable ", " and avoid ", ".", "There is a temporary solution to prevent a system from upgrading critical kernel files: ", "There are known issues with various patches which have been introduced after 1034. These patches make the Raspberry Pi unstable - primarily during the boot process. The result is the Raspberry Pi will sometimes freeze during boot. Several re-attempts (power cycling) and the system may eventually boot. Occasionally even this is insufficient.", "By installing the original images and not permitting upgrades of core kernel files, the RPi + Ubuntu Xenial + ROS Kinetic is stable.", "Background:", "Launchpad thread: ", "[spoiler]", "\nUPDATE: 2017-02-15: There is a better workaround available. Rather than use the \u201chold\u201d feature, users may edit the config.txt and change the device_tree_address (", "). The result should be the following lines:", "\n", "Even with this improvement, completing apt-get dist-upgrade will break wlan0.", "UPDATE: 2017-02-16: The config.txt has been repeatedly reported to work; however, I get boot issues.[/spoiler]", "UPDATE 2017-02-17: The update from the 15th has proven unreliable. The suggestion of using a non-server version from ubuntu-pi-flavor-maker is proving more promising.", "UPDATE: 2017-02-15: There is a better workaround available. Rather than use the \u201chold\u201d feature, users may edit the config.txt and change the device_tree_address. The result should be the following lines:# set extended DT area    #device_tree_address=0x100    #device_tree_end=0x8000    device_tree_address=0x02008000", "Just to be clear, we would modify the ", " file to comment out the old device_tree parameters, then only set the ", " to be ", ".", "In the launchpad thread you linked to, there was some references to ", ":", "\n", "\nDid you try those yet?", "\nDoes say the ubuntu-mate-16.04.2-desktop-armhf-raspberry-pi.img encounter that same issues?", "\nI\u2019m going to try it with ROS in a bit.", "I did not attempt the alternate image because of the statement at the end of the thread \u2026", "Shuhao (shuhao) wrote on 2017-02-13:\t", "\nThe images listed on ubuntu-pi-flavour-maker and the one listed on ", " is exactly the same. The same issue should exist on both systems.", "The non-server ubuntu-pi-flavor-maker builds are built using a different process, and don\u2019t seem to have this issue. They come with rpi-update and can easily run the RPi \u2018mainline\u2019 kernel (not the Ubuntu fork).", "I have been using lubuntu-16.04.2-desktop-armhf-raspberry-pi.img.xz with no issues.", "Rohan", "Today I am getting undiagnosed issues so I will try using the desktop image and then disable the GUI to have a facsimile of a server image.", "[quote=\u201crohbotics, post:4, topic:1205\u201d]", "\nThe non-server ubuntu-pi-flavor-maker builds are built using a different process, and don\u2019t seem to have this issue.[/quote]", "I can confirm. I have a functioning instance of my LoCoRo project. I started with the Lubuntu image from ubuntu-pi-flavor-maker and disabled LXDE. Then treated like an Ubuntu server image. Everything worked as expected.", "The Ubuntu Bugs thread has demonstrated to me that \u201cofficial Ubuntu support\u201d for the Raspberry Pi is dubious at present and has an uncertain future. When the April drop of Ubuntu 17.04 occurs, things should be clearer one way or another.", "For ROS, it would be best for the community if it supported buildfarm installable packages for a \u201csupported Raspberry Pi operating system\u201d. This translates to Raspbian. I\u2019ve been trying to promote Ubuntu but I have to concede, its not an ideal situation for ROS.", "I have talked with Canonical about the problem of Raspberry Pi support. They told me the lack of work on the ubuntu raspberry pi is do to the focus on ubuntu core. I agree with their logic. There is little reason to run a full version of ubuntu on a RPI.", "I do see the attention to Ubuntu Core. This is why my previous conclusion was to suggest ROS to consider Raspbian or even one of the other fully supported Linux distributions for the Raspberry Pi audience.", "Here is where I see a divergence between ROS and Ubuntu as far as the Raspberry Pi is concerned.", "The majority of Raspberry Pi owners (and the focus of the Raspberry Pi org) is the experimentor, student, tinkerer, maker, learner, creator.", "Ubuntu Core appears to focus on IoT which is more of a commercial direction.", "Unless a lot more packages are constructed as Snaps, it leaves very little usefulness for a large segment of the Raspberry Pi user base. (You can\u2019t just \u201capt-get install\u201d on Ubuntu Core.", "Is ROS considering a focus on Ubuntu Core?", "My personal activity with ROS is more \u201cresearch\u201d and \u201ccreator\u201d and not IoT. None of the Taspbery Pi based projects I\u2019m involved in are looking at Ubuntu Core.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/information-for-those-using-raspberry-pi-ubuntu-xenial-and-ros-kinetic/1205"},
{"title": "SBS (Smart Battery System) SMBus over USB?", "thread_contents": ["Hi all, this isn\u2019t ", " ROS-related, but I\u2019m trying to integrate a SBS laptop battery into a ROS-based robot I\u2019m building, and it\u2019s been a difficult road. Wondering if anybody here has had any experience with this and could share their perspective.", "I\u2019ve had minor amounts of luck with a few approaches", "I\u2019m hoping to discover a fairly \u201cout of the box\u201d solution to reliably retrieve data from the SMBus. The built-in state of charge measurement, and other metrics, is really attractive from these smart batteries. However, I don\u2019t have a lot of experience implementing these protocols on microcontrollers, so it\u2019s fairly daunting to try and get it working reliably.", "Once the communication is working, I\u2019m looking forward to making a node to talk to the SMBus communication layer and publish BatteryState messages", "For the curious I am currently using the following battery and charger from RRC - they\u2019re a bit pricey but seem really nice", "\n", "4S1P Standard-Batteriepack RRC2054 mit 14.40V/3.45Ah/49.70Wh. H\u00f6chste Performance, weltweit zugelassen, direkt verf\u00fcgbar. Entwicklungskosten & Zeit sparen!", "\n", "\n", "RRC-PMM240 Lademanagement-Modul als integrierte L\u00f6sung zum Laden unserer Standard-Batteriepacks. Mit 240W max. Ausgangsleistung & 82W max. Ladeleistung.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Arduino using ", ", which let me speak SMBus (Wire library doesn\u2019t format communication correctly for smbus) - but the library seems to not handle the multi-master situation well and freezes up the communications quickly", "I\u2019ve also tried this little project ", " which also seems to freeze up on me"], "url": "https://discourse.ros.org/t/sbs-smart-battery-system-smbus-over-usb/12836"},
{"title": "ARM TechCon: Best Contribution to an Open-Source Software Project", "thread_contents": ["Hello, everyone. ", "I\u2019m pleased to announce the news that we got finalists of the Arm TechCon 2017 Innovation Awards.", "\nI think this is an achievement and good news for our ROS community.", "TurtleBot3, in cooperation with OpenRobotics and ROBOTIS, was awarded ", " at \u2018ARM TechCon2017\u2019 in Santa Clara, California from October 24 to 26.", "\n", "\n", "OpenCR in TurtleBot3 is powered by an Arm Cortex-M7 and be able to connect the servos and battery. TurtleBot3 take advantage of Raspberry Pi\u2019s full capabilities as a central computing system for the robot platform. The hardware, firmware and software of TurtleBot3 are open-source allowing users to download, modify and share source codes. All components of TurtleBot3\u2019s 3D CAD data are available for 3D printing.", "\n", "\n", "\n", "Although we missed the final prize, we are proud to announce that TurtleBot3, which was only five months, was finally nominated for an innovation in the open source software project at an international event. We are also proud to be recognized as a platform by many people who want to learn the ROS.", "We are especially grateful to have a great opportunity and wonderful project with Tully, Morgan, and Brian. I want to make a lot of fun things for the ROS community in the future. ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/arm-techcon-best-contribution-to-an-open-source-software-project/3129"},
{"title": "ROS client library for the Go programming language (Golang)", "thread_contents": ["Hello, some weeks ago i published an open source ROS client library for the Go programming language, written in pure Go and without external dependencies:", "ROS client library for the Go programming language - aler9/goroslib", "In my company we use ROS in a lot of fields, from robotics to image processing, but the deployment is sometimes difficult, since:", "Go is one of the languages of choice when writing microservices and containerized applications, and there are already a couple of ROS implementations for Go, but they have some problems:", "Some months ago i decided to rewrite everything by scratch, and this library is the result. It should have all the base features (topics, services, messages) and it is already powering some nodes in production-grade environments.", "ROS2 support is currently missing, but i hope to find time and reasons to start working on it too.", "this is great work!!!", "Go is one of the languages of choice when writing microservices and containerized applications, and there are already a couple of ROS implementations for Go, but they have some problems:", "\u2026", "ROS2 support is currently missing, but i hope to find time and reasons to start working on it too.", "Would be cool to have a ROS2 client lib written in pure Go.", "Nice job! I\u2019ve been using go for 3 years. Go is solid on type, error handling and application deployment. Hope this project goes well and I\u2019m very willing to make contributions!", "Nice work! ROS2 support would be great too. We have been using Go for robotics for several years at my work. We have not used ROS yet, but are now planning to start using ROS2. It would be great to be able to continue to use a lot of our existing Go code when we move to ROS2. We maybe able to contribute to this effort.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["the standard library is very big, and therefore uploads can take much time", "the standard build chain makes very difficult the generation of static executables", "the standard library forces to choose between C++ and Python", "rosgo (", ") is unmaintained", "rclgo (", ") is a wrapper around a C++ library, a non-ideal situation", "rosgo (", ") is unmaintained", "rclgo (", ") is a wrapper around a C++ library, a non-ideal situation"], "url": "https://discourse.ros.org/t/ros-client-library-for-the-go-programming-language-golang/12792"},
{"title": "TechRepublic: The hottest thing in robotics is an open source project you've never heard of", "thread_contents": ["Commentary: The Robot Operating System (ROS) doesn't get a lot of press, but it increasingly powers the robots upon which industrial automation and other functions depend.", "With discussion at ", "Never heard of that thing\u2026 what is it?", "Yes Loy, what are you on about? What is a ROS?", "\nOh and b.t.w. Thilo:", "The Robot Operating System (ROS) doesn\u2019t get a lot of press,", "Looks like you should be even more active on all forms of social media than you already are ", " haha", "Good questio\u2026", "What is ROS?", "even better question, how come its so widely used, yet the confrences are so small\u2026 last year ROSCON japan and france, didnt attract 300 professionals combined\u2026?", "If its so common among robotic profesionals, why so little attendance?", "Both ROSCon JP and ROSCon FR are local conferences for their respective countries, dealing with a still rather niche area (open-source robotics software), so it is not surprising that they don\u2019t have a huge attendence.", "The global ROSCon attracted nearly 600 people last year, which is similar to the number of attendees that PyCon was attracting 10 years ago. I think this is an impressive number given that the field of potential attendees is smaller (I think there are probably far less robotics engineers in the world than Python programmers).", " Conference in Stuttgart, Germany, could also attract \u201conly\u201d 150 people in ", " and ", " respectively. Such events are just very technical and (so far) we do not attract much more people. But who knows, maybe we need to book the ", " (football stadium) in 2030 for the event \u2026", "\u201cROS\u201d? But it is not a robot nor an operating system. who names this stuff?", "Varies by region? ", " had 570 developers, Taiwan ROS Summer School had 200.", "\n", "The summer schools in China get hundreds of attendees every year as well, seemingly without effort.", "Of course there is a difference between knowing a  conference\u2019s subject, knowing that there is a conference and actually attending a conference. If I had the time & money I\u2019d visit much more conferences than I currently do.", "\nMy guess is that since the Python community is so much larger than ROS\u2019s, ROSCon has a much higher turnout ratio.", "I didn\u2019t meet anyone in the robotics field who didn\u2019t hear of ROS, even if they don\u2019t use it (maybe my pool of samples is just too biased though :P). However, as Geoffrey pointed out, robotics is still a growing, but comparatively small field. Sure, IROS/ICRA grew a lot in the last 10 years, but just check out computer vision conferences since 2012\u2026", "And honestly, I am thrilled by these consistently high numbers of attendees at worldwide ROS-related events. In 2007 we organized a Player summer school in Munich, when Player was already quite established, and I don\u2019t remember there being this many & large events. Such a list was considered long back then: ", " (maybe now a \u201ccould be using ROS but don\u2019t\u201d list would be shorter :P). Then in 2010 we organized a ROS Fall School, where it was still about getting people on board. Then ROSCon started in 2012, and now there are regional conferences, tons of events, etc.", "The title is just clickbait for non-roboticists, in the field ROS does not need an introduction, which is amazing progress! Keep up the great work ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/techrepublic-the-hottest-thing-in-robotics-is-an-open-source-project-youve-never-heard-of/12229"},
{"title": "Workshop on Security and Privacy in Robotics - Call for contributions", "thread_contents": ["========================================================================", "========================================================================", "IMPORTANT DATES", "Submission deadline: March 31, 2020", "Notification: April 15, 2020", "Camera-ready deadline: April 25, 2020", "Submission site: ", "SCOPE", "The workshop on \u201cSecurity and Privacy in Robotics\u201d addresses recent security", "\nand privacy challenges with the robotic systems. The trend of integrating", "\nrobots with information and communication technologies (ICTs) such as cloud", "\nservices and IoTs has imposed cybersecurity risks. Interdisciplinary", "\napproaches that bridge cryptography, communication networks, data sciences,", "\ncontrol systems, and robotic operating systems will be examined and discussed", "\nto address the emerging challenges. This workshop will gather experts in this", "\nemerging area of research and provide a perspective on relevant challenges", "\nand opportunities for the academia and industry. The workshop will focus on", "\nrecent advances in areas such as ROS security, cloud robotic security, and", "\nprivacy issues in robotics. It will give the audience an overview of the", "\nsystematic security solutions to robotic systems in these areas and provide a", "\nplatform to discuss new research directions. Conclusions will be summarized", "\nin a report released to the community.", "TOPICS OF INTEREST INCLUDE (BUT ARE NOT LIMITED TO)", "SUBMISSION GUIDELINES", "The (WiP) session provides an opportunity to present and discuss", "\nnew challenges and visions, showcase early research results, and", "\nexplore novel research directions. The specific aim of the WiP", "\nsession is to provide a forum for timely presentation, discussion", "\nand feedback for novel, controversial, and thought-provoking ideas.", "Demonstration submissions should list any special requirements", "\n(tables, power, wireless connectivity, etc.) n a separate page to", "\nthe submitted abstract (see the instructions below). This page is", "\nnot part of the technical content of the abstract, can be formatted", "\nat the discretion of the authors.", "Authors are requested to submit original, unpublished abstracts of", "\nno more than 500 words or 2 pages of extended abstract. All submissions must be", "\nin PDF format and uploaded through the online system", "This workshop aims to addresses recent security and privacy\u00a0challenges with the robotic systems. The trend of integrating robots with information and communication\u00a0technologies (ICTs) such as cloud services and IoTs has imposed...", "The submitted abstract describing the work or demo will be", "\nevaluated based on technical merit, innovation, and the potential", "\nto stimulate lively discussions at the conference.", "For any further questions on the submission or workshop format,", "\nplease don\u2019t hesitate to contact us (email addresses given below).", "WORKSHOP ORGANIZERS", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "Robot security architectures", "Secure deployment of robotic systems", "Accountability", "Safety and Security modeling of robot systems", "Lessons learned from practice", "Demonstrations of practical solutions", "Cloud operated robots", "Cryptography and Applications in robotics", "Cross-disciplinary topics", "Resilient robotic systems", "Security of multi-agent robotic systems", "submission of work-in-progress", "submission of demos", "Quanyan Zhu, New York University, USA, quanyan.zhu@nyu.edu", "Stefan Rass, Universitaet Klagenfurt, Austria, stefan.rass@aau.at", "Bernhard Dieber, Joanneum Research, Austria, Bernhard.Dieber@joanneum.at"], "url": "https://discourse.ros.org/t/workshop-on-security-and-privacy-in-robotics-call-for-contributions/12780"},
{"title": "Best ARM board for ROS", "thread_contents": ["I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black.  beaglebone does not have the horsepower to run ROS (moveit,etc).  I think that a quad core A53 might be the minimum required.", "Anyway comments welcome.", "Shawn", "HI ", ",", "I\u2019ve tried all those and my advice for new projects would be to have a look at the new NVIDIA Jetson TX1 module. IMO it\u2019s by far the best ARM embedded board where to run ROS and friends.", " how is the TX1 compared to the TK1?", "I have used the TK1 myself, and it has the capabilities to be extremely powerful, however all of nvidia images are given out with low power settings so you have to configure it all yourself (i also noticed that it had a issue with IRQ balancing, which till this day I dont believe got fixed)", "Hi,", "Yes,  the TX1 would be a great board (or the TK1).  I should have clarified and said best board under 120.", "\nI am considering Pine64 but I will have to wait and see about it.  I am a bit disappointed by the RPI3\u2019s OS and lack of 64bit support.", "Hi ", ",", "\nI jumped directly to the TX1 so I\u2019m afraid i can\u2019t comment on that.", "Has anyone used an old flagship phone as a \u201cARM target\u201d? I\u2019d like to use some old android phones that still have a good 2GB of RAM + 64-bit ARMv8 that include a swath of radios, sensors, self contained power supply. Buying one retail might be above your $120 mark, but if you have one siting around in a relatives junk droor or with a cracked screen off ebay\u2026", "I know there is ROS for android with ROSJava, but flashing phones with a more common flavor of Linux and treating as a traditional embedded target has always been appealing to me. I think mobile device hardware support is a bit fractured thanks to device manufactures, so I\u2019ve only seen posts with Nexus and Ubuntu Touch, nothing like an old Samsung I have.", "\nRelevant ROS Answers post: ", "I would like to get an opinion on what people think is the best ARM board for ROS.", "I have tried the dragonboard, raspberry PI3 and beaglebone black. beaglebone does not have the horsepower to run ROS (moveit,etc).", "I was hoping to use the BBB for a mobile robot doing things like SLAM, localization, and navigation (move_base). So no need for image/video processing, only spinning LiDAR. From my testing, it doesn\u2019t look adequate even for that, unfortunately.", "How do people feel about the Odroids? ", "ODROID XU-4 is pretty awesome. It has a USB3 host, and if you have a USB3 peripheral that you need to talk to, I don\u2019t think there are many (any?) similarly-sized and similarly-priced options at the moment.", "+1 for the XU-4. It can run a surprisingly serious ROS setup (motion planning, depth image processing). I\u2019ve just bought a C2 as well; haven\u2019t had a chance to try it out yet though.", "spmaniato", "You can use the BBB but for me using MoveIt it the CPU ran around 80-90 percent during planning.  Which is not good.  Using the DB410C or RPI3 it runs around 60-70 percent.", "I have not used an Odroid, but have used SolidRun cubox and Radxa rock and TK1.", "Since you mentioned depth image processing: did you try connecting an Asus Xtion to the XU-4? Does it work?", "Nice discussion with interesting answers.", "Just to mention my experience:", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in several", "\nprojects.", "\nI did not used MoveIt but I can tell that it is enough to support some SLAM", "\nsystems if the algorithms parameters are well tuned for efficiency.", "\nSpecifically odroid is quite powerful and it is able to execute this kind", "\nof heavy applications fluently.", "Kind Regards.", "we have used xu4 with turtlebot with asus camera", "I also used ROS in rpi2 rpi3, bbb and Odroid XU4 successfuly in severalprojects.I did not used MoveIt but I can tell that it is enough to support some SLAMsystems if the algorithms parameters are well tuned for efficiency.", ", have you ever tried running the navigation stack (i.e., ", " + ", ") on a BBB by any chance? I\u2019ve found that, even with relaxed parameters, it cannot handle it. But I may be doing something wrong. (It can definitely handle ", " plus the laser scan publisher and other drivers. It\u2019s ", " that takes it over the edge in my experience.)", "I also like the ODROID XU-4 for using USB 3.0. On the one hand \u2018Intel\u00ae RealSense\u2122 Robotic Development Kit\u2019 is now available. It is fantastic for the user using RealSense or other USB 3.0 device (but a little bit expensive than other SBC). How about it?", "\n", "check our\n3d model\nHigh performance and low power consumption features of the latest tablet technology", "\n", "Yeah, we also used it with an asus xtion. We set the depth resolution very low (QVGA or QQVGA) mainly because we didn\u2019t need the extra pixels. Overall It worked quite well though.", "i tried Pine64+ and installed Ros from source on it ros indigo with ubuntu 16.04", "\nit is very good", "\nyou can download image from", "\n", "PINE64-_ROSINDIGO_Ubuntu16.04lts_image - image for pine64+ installed on it ROS package From source and Distro Ubuntu 16..04 lts", "\n", " It\u2019s great to know that it works for you with Indigo on the Pine64.", "Thanks for taking the effort to share however I have to not recommend people grab it. Binary images of unknown provenance are potential security risks.", "If you wouldn\u2019t mind it would be great if you could share your experience bringing up Indigo on the Pine64 with as much info as you can remember in a new thread in this category. Also did you try using the Debian Jessie builds with Kinetic from debian packages?", "i tried Ubuntu 16.04 LTS with indigo and i will make it for  Kinetic also", "\ni tried  Kinetic with it but i stopped to make meta-ros yocto core-image-minimal for raspberry pi 2 and it works now i will upload it soon also and i will build Kinetic with Ubuntu16.04 LTS", "I have Kinetic running on Pine64 under debian.  works great.  Kinetic is still missing some packages that should be synced in two weeks.", "All of the ARM A53 boards perform similarly to me.  I do like the 96Boards form factor the best", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Intel\u00ae Atom\u2122 x5-Z8350 Processor (2M Cache, 1.44 GHz up to 1.92 GHz) CPU with 64 bit architecture; Quad Core", "Intel\u00ae HD 400 Graphics", "1~4G DDR3L", "USB 2.0 and USB 3.0"], "url": "https://discourse.ros.org/t/best-arm-board-for-ros/152"},
{"title": "ROSCon JP 2019 report", "thread_contents": ["The second officially licensed Japanese ROSCon event, ROSCon JP 2019, was held in Tokyo, Japan on the 25th of September. ROSCon JP 2019 was, like ", ", held in conjunction with the Open Source Robotics Foundation.", "We sold over 210 tickets, and had 204 participants on the day (excluding sponsors\u2019 invitations and staff), a small growth over last year. The livestream again had a steady 60 to 70 people watching at any one time. Thanks to all the participants, our sponsors and their staff, and most of all the excellent event staff who made sure things Just Happened", ", ROSCon JP was an excellent event for all. Presentation slides and videos will be published on the website in a few weeks.", "Everything at the event, including the invited talks, the submitted talks, the lightning talks, and the exhibitors\u2019 booths, was of even higher quality than last year and showed a level of committment from the local ROS community and sponsors that we can only say makes us, as organisers, very happy to be a part of this community. We consider the event to have been a massive success, building on the success of our first event last year, and feel that the Japanese ROS community has now established a solid foundation on which to grow the event to provide an enjoyable venue for the ever-growing community to come together.", "This year we tried a new addition to the programme, and gave a full-day tutorial on ROS 2 and navigation2 for 51 participants. The morning was taken up by learning the basics of ROS 2, including how to write nodes and use topics, services and actions. In the afternoon, ", " gave a marathon 3-hour lesson on setting up navigation2 from scratch on a real robot, building a map using Cartographer, and then navigating around in it.", "The response to the tutorial was good, and I think the participants all enjoyed the chance to learn about ROS 2 before diving into the community head first on the main conference day. In our follow-up survey, which had 33 responses, every respondee said they would consider participating again.", "The first invited talk was given by Ryan Gariepy of Clearpath Robotics/Otto Motors. Rather than being specific to ROS, Ryan discussed why safety is important for all those robot products that are being produced now that ROS has Solved All Our Problems, and gave tips on where to start. The content of the talk was probably unexpected by the majority of the audience, but the feedback was positve and it was clear that they all were happy to have heard the talk.", "The second invited talk was given by Louise Poubel of Open Robotics. She talked about Gazebo and Ignition, giving some information on where Gazebo came from and where Ignition is going. With the default simulator for ROS going through a transition to greater power (and greater responsibility?), this talk was timely for the audience. And, as usual for a member of the Gazebo team, the talk was given in Gazebo, which apparently was a new thing for most of our participants! There was an audible response from the audience when the first slide turned out not to be a slide.", "Although we provided real time translation from English to Japanese for the participants, Louise gave her entire talk in Japanese.", "ROSCon JP 2019, in keeping with the ROSCon method, established a programme committee and made a public call for submissions. This year we received 23 submissions, a slight drop over last year, but fortunately most were of high quality.", "After review by the programme committee, 13 presentations were selected and given presentation slots ranging from 10 minutes to 30 minutes. (One presentation unfortunately had to drop out.) The presentations were selected covered a broad range of ROS-related topics, such as", "On the other hand, the lightning talks session was full this year, unlike last year where we struggled to get participants. Most likely, having seen last year what it is about, a lot more people wanted to do one this year. The hour was fun for all and every presenter was very enthusiastic. We even had a live demo of a ROS-controlled tiny-scale quadcopter, done perfectly in just a minute and a half!", "To help cover the cost of holding a conference in central Tokyo, where meeting space is at a premium, the organising committee invited sponsors. To our amazement, we sold out of exhibition space and had three companies waitlisted to sponsor!", "As with ROSCon, sponsors were provided with dedicated exhibition space in a separate room from the presentations. This allowed them to give lively demonstrations and hold discussions with participants throughout the day. The exhibition hall was well-attended, with participants talking to exhibitors even during the invited talks.", "ROSCon JP 2019 was sponsored by the following companies and organisations:", "Gold: Analog Devices, The Autoware Foundation, eSOL, iSiD, Rapyuta Robotics, Renesas, Seqsense, Tier IV, Ubuntu", "Silver: Acutronic Robotics, ADLink, Argo, Concurrent Real-Time, Mamezo, Robotis, SoftBank, Whill", "Bronze: AVNet, Fixstars, RT Corporation, TechMagic, TokyoRobotics, Vstone", "To encourage networking between participants, ROSCon JP provided both a catered lunch and a catered reception. Attendees were able to enjoy a good meal without leaving the conference venue, giving them more time to mix with each other and attend the exhibitors\u2019 booths.", "The reception in particular was well-attended, even by those who had to travel several hours to get home that night. Once again we were forced to drive people out of the reception when the hotel wanted their room back!", "A survey of participants at the conclusion of the conference received 109 replies. Here are some select results:", "Building on the 2019 event, ROSCon JP 2019 was a huge success. We saw many repeat visitors, and the buzz around the event beforehand showed us that people were genuinely excited about attending. The event seems likely to continue next year!", "Congratulations to ", ", ", ", and the rest of the ROSCon JP team for putting on another great event!", "Congratulations ", " and everyone else involved in the organization!", "Awesome! I\u2019m glad to hear it went so well, especially the Nav2 tutorial, very cool! ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["using Alpine Linux to rapidly deploy ROS-based software to in-production robots with minimum dependencies;", "how ispace is using Gazebo to assist with designing its moon rover (due for launch soon!);", "the combination of machine learning and Matlab\u2019s ROS support to provide higher-quality highly-functional robot software; and", "two talks on how to do hard real time with ROS 2.", "56% of respondents were engineers at companies. A further 22% were in commercial R&D and 6% in business.", "Just 15% of respondents were on the academic side, with the vast majority of those being students. This shows again that there is a strong commercial interest in ROS in Japan.", "94% of respondents rated the quality of the sessions as high or very high, an improvement over last year. Just 2 people rated them as below average (again).", "By far the majority of participants considered the sessions just right in length and number.", "100% of respondents stated they intend to participate if ROSCon JP is held again. Out of this, 70% were unconditional (i.e. not based on schedule or content).", "28% of respondents expressed a desire to present at the next ROSCon JP. A further 41% stated they would present a lightning talk."], "url": "https://discourse.ros.org/t/roscon-jp-2019-report/10860"},
{"title": "Featured ROSCon 2018 Talks", "thread_contents": ["As we start to get ready for ", " we will be featuring talks from the ", " to help inspire people to submit their own talks as well as give people a sense of what they can look forward to at ROSCon 2019!", "\n", " ", " ", "          ", "\n", "Astrobee is a free-flying robot designed by NASA to operate alongside astronauts inside the International Space Station, where it will carry out scientific and surveying tasks in microgravity. The robot can autonomously mate with a docking station to recharge, as well as perch to existing ISS handrails using a three degrees of freedom arm. Its open source flight software stack is built on ROS Kinetic, uses a delay-tolerant DDS bridge for space-to-ground communication, and is accompanied by a Gazebo simulator that enables researchers to develop and test behavioral algorithms. This presentation covers the the software architecture, challenges faced during the development process, facilities for testing prototype hardware, and broad lessons we have learned over the last three years.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past 10 years, ROS 1 has proven itself to be the framework of choice for prototyping and developing large robotic applications. Many limitations preventing ROS 1 from being used in production applications have been discovered over the years, and after significant prototyping, ROS 2 makes great headway in making ROS 2 suitable for production. Autonomous driving is the next great technology waiting to be realized and transform our society. A full autonomous driving stack is inarguably a large robotic system, and consequently ROS 2 is the right framework upon which to develop a full autonomous driving stack. To prove this, we have developed a part of the autonomous driving stack based on ROS 2.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "          ", "\n", "Over the past half year we have been working on the new launch system for ROS 2 based on roslaunch from ROS 1. The goal of this presentation is to summarize the state of the design document for launch, describe the current state of the reference implementation, and dive into the rationale behind some of the design decisions. The attendee should come away with a deeper understanding of the differences between roslaunch from ROS 1 and the new ros2 launch tool. We\u2019ll also have some short demonstrations to show how these differences might be useful to everyday users.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Over the past couple of years, Open Robotics collaborated with NASA Ames Intelligent Robotics Group to develop the Resource Prospector lunar driving simulator. In particular, a strong focus was placed on generating high quality visual images from the cameras on the simulated rover. This resulted in in several improvements made to Gazebo, including support of high resolution digital elevation maps, improved shadows, integration of custom material shaders, and a new lens flare plugin. This presentation will show the resulting lunar terrain environment created in Gazebo and discuss some of the challenges in modeling the environment.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we show the current state of ROS 2 features through hands-on demonstrations. We guide the developer through a set of features and functionalities of ROS 2 beginning with creating a simple \u201chello world\u201d package and consecutively increasing functionality. We show how to launch multiple nodes, how lifecycle nodes can be used to bootstrap a complete system, through to how the ROS 2 security features can be utilized to secure applications. This talk is meant to highlight the newly available features and tools of ROS 2 and aims to incline developers to start working with it.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "FIRST, or For The Inspiration and Recognition of Science and Technology, is an international organization focused on engaging students through STEM. This year, our team - The Zebracorns - was the first in the high school FIRST Robotics Competition (FRC) to control our robot entirely using ROS. In our presentation, we\u2019ll introduce the unique challenges presented by FRC with restricted hardware options, time, and resources. We\u2019ll talk about our motivation for implementing ROS, the specific application within FRC, and our ambitions for the future of ROS within the FRC community.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "Cruise Automation\u2019s self driving car runs on top on ROS. This talk will share some of the lessons we learned while scaling up the ROS stack to a very complex Robotics problem and 500+ engineers. We will talk about performance, reliability, code organization and health, and the ways we have found ROS to excel or fall short.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", "\n", "Reliable, life-long mapping and localization is an essential component for mobile robotics in continuously changing warehouse environments. We present a system based on Cartographer in which robots run finite-history SLAM for low-latency localization and continuously stream local map updates to a cloud service. The cloud component assembles and optimizes a globally consistent pose graph out of the streaming data of the agents. Magazino piloted cloud-based Cartographer in a customer warehouse using its fleet of mobile picking robots. By sharing the local map changes among each other, the robots were able to maintain their localization accuracy while dealing with dynamic environments.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "On January 11, 2018, Sony Corporation released aibo (", "). aibo that is back on market beyond the time of 12 years constructed via robotics framework named ROS. In this presentation, we introduce examples of development in aibo from the point of view of ROS, starting with introduction of aibo, architecture, embedded technology, real-time optimization, robot development environment, simulation etc.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "RViz is now available for ROS 2, including most of its features. This talk will look back at the migration of RViz from ROS 1 to ROS 2, and present the main challenges and changes performed during the migration. It will focus on a new package, rviz_visual_testing_framework, that makes writing automated UI tests - including the 3D rendering part of RViz \u2013 possible.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "\n", "The  ", "  stack is one of the core components of the ROS Ecosystem. This talk will present lessons learned from maintaining the navigation stack over the past five years, and discuss a new generation of interfaces for increased functionality. This talk will cover: the new  ", "  interfaces, with a new  ", " interface; the locomotor package which replaces move_base and demonstrates new functionality; and ROS 2.0 + Navigation.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "This presentation discusses the building of a task-driven, message-synchronized execution abstraction layer on top of ROS\u2019s message passing, aiming to mimic a deterministic system while continuing to use a non real time operating system. We discuss the overall structure of the framework, as well as the details of various capture and synchronization policies, and the profiling and diagnostics of task executions. We further discuss methods for using simulators and bagfiles in conjunction with the synchronization framework for repeatable testing and issue reproduction.", "Thank you again to our Platinum Sponsor Amazon, and to all our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.ai, Clearpath Robotics, eProsima, Fetch Robotics, iRobot, Microsoft, Rapyuta Robotics, ROBOTIS, Silexica, Tier IV, Toyota Research Institue, and Ubuntu.", "\n", " ", " ", "\n", "In this talk we give a short introduction to Mozilla rr, a powerful C/C++ debugging tool for Linux, and show how to more effectively debug ROS nodes by republishing messages from rr recordings. Mozilla rr serves as a gdb (GNU debugger) replacement which efficiently records the execution of a process and then provides repeatable deterministic debugging of the recording, enabling a very powerful debugging experience with reverse execution.", "Thank you again to our Platinum Sponsor and Gold Sponsors for supporting ROSCon.", "\n", "\n", "\n", "This talk gives an introduction on how to contribute to ROS 2. Specifically this talk shows you how to take a ROS2 feature from a design discussion, to a concept document , to a series of pull requests, through to a released feature. The talk also discusses the process of patching bugs: moving from an ", " bug, through trouble shooting, bug report generation, and finally submitting a patch. If you\u2019ve never contributed to open source patching your own bug is a great place to start and this talk will guide you through the process.", "ROSCon registration is ", "! The deadline for regular price admission is just two weeks away. Get your tickets now before the prices go up.", "                    ", "\n\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops is ", "\n", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for workshops closes today. Submit ", ".", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Applications for the Diversity Scholarship is ", "\n", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Sponsorship opportunities are ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "\n", " The call for talks and videos is now ", " See the ", " for templates and submission links.", "Registration is ", "\n", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first).", "Early registration is ", "! The deadline for early registration is August 24th, 2019 (or until the limited supply of early registrations sells out, whichever comes first)."], "url": "https://discourse.ros.org/t/featured-roscon-2018-talks/9071"},
{"title": "ROS Answers needs your help", "thread_contents": ["Or to be more specific: all of us posting on ROS Answers need your help.", "?", "Not counting the duplicates and the ones that are easy to answer (\u201cdid you source ", "?\u201d, \u201cNo, pkg X is not released into ROS version Y\u201d), I easily see a hundred new questions on ROS Answers each week.", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages. How to tune the PIDs on a ", " based driver fi, what sensors to choose for ", " in an underground cave or whether a 2D navigation stack can actually be used by a 6D drone \u201c", "\u201d.", "A large number of volunteers from the community help out by answering quite a few of these questions, or at least try to help getting closer to a solution \u2013 and they deserve a lot of recognition and thanks for that. I\u2019ve noticed however somewhat of a decline in the number of interactions of what I would call ", " with the ROS Answers community: users that have extensive experience with specific functionality such as mapping, localisation, motion planning, application design, human-robot interaction, behaviour modelling, system architecture, high-volume datastream processing, multi-robot systems, etc.", "It\u2019s exactly those kinds of interactions that make ROS Answers an invaluable resource, and it would be unfortunate if we lose that.", "This post is therefore a ", " (or perhaps even a ", "): if you have experience with one or more packages and you feel you could help out a fellow ROS user that is perhaps just starting out, take 15 minutes at the end or start of your day to check whether there are any questions you could contribute to.", "Even a short comment pointing the poster in the right direction has the potential to save someone hours of debugging or searching. Don\u2019t understimate how much you can help someone, even if you feel you are not a \u201cROS expert\u201d (whatever that may mean).", "And if you\u2019re just starting out yourself, you could consider ROS Answers a good way to get familiar with ROS: pick a question and try to figure out what the answer could be. I\u2019ve learned quite a few interesting things about ROS that way which I would not have known otherwise.", "Thanks for the call ", "! Always good to give back.", "Thank ", " for taking the time to answer a few questions ", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic.  As of this post, only ", " of questions have an accepted answer.", "Indeed ROS Answers is a great community resource and it does rely on the community to give back to make it effective.", "In addition to visiting the site periodically you can also sign up to be notified when specific tags are mentioned. So if there\u2019s a topic that you feel that you\u2019re able to help out on add tags associated with that to the \u201cInteresting tags\u201d section in the side bar and make sure you have \u201cEntire forum (tag filtered)\u201d enabled in your \u201cemail alerts\u201d settings.", "Maintainers we suggest subscribing to tags relating to your packages too. It\u2019s a good way to learn where people are getting tripped up as well as can be an early indicator of a bug or issue with the code.", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic. As of this post, only ", " of questions have an accepted answer.", "I\u2019m not sure how you\u2019re computing your ratio, but I find the ", " a little bit more representative of the state of things. Questions that are \u201cclosed\u201d as duplicate or off topic will never have an \u201caccepted\u201d answer but also aren\u2019t waiting for one. And the outstanding \u201cunanswered\u201d questions are only 12837 out of 40548 total which is closer to 68% resolved.", "If everyone who reads this could try to answer at least one question per week the backlog would be cleared quite quickly.", "As with many things, the answer is ", ". It does ignore closed questions.", "However, the Askbot API isn\u2019t perfect, so I may be missing some values.", "Although I can\u2019t find a reference, there was at least once a coordinated day for burning down unanswered questions wasn\u2019t it? My calendar still marks Septermber 19th is world ", " day (not sure when, not sure if that was the title either).", "If ", "\u2019s verbal argument is not sufficient, here\u2019s a cold hard statistic. As of this post, only ", " of questions have an accepted answer.", "\u2026 And the outstanding \u201cunanswered\u201d questions are only 12837 out of 40548 total which is closer to 68% resolved.", "The original claim was \u201cI\u2019ve noticed however somewhat of a decline in the number of interactions of what I would call topic experts\u2026\u201d. To substantiate, prove or refute it numerically, a ", " in a timeseries needs to be provided.", "\nThe ROS metrics reports are a little more helpful in that respect.", "The current ratio, be it 44% or 68%, is a quite meaningless number without the context to judge it. Maybe even 44% could be quite a lot and a sign of good health of the community.", "Perhaps OSRF needs to hire some support staff if these metrics really are trending down (which we have yet to see proved in this thread). Or, perhaps at ROSCON we get some companies pledging to contribute support hours to the community. There are a lot of companies making a shit ton of money using ROS, it\u2019s only fair they give back, right?", "Next year, in order to get \u201cPlatinum\u201d sponsorship, you also have to prove your company has contributed XX support hours to the community.", "Just an idea.", "Or, for individual supporters/not affiliated, maybe those with enough internet points karma (can\u2019t remember what ROS answers uses) they can get their ROSCON registration fee waived/top contributors full expense paid and sponsored to come to ROSCON.", " do you have suggestions on how to find questions about", "application design, human-robot interaction, behaviour modelling", "in ROS Answers? I\u2019ve been using ROS for creating interactive behaviors and running HRI experiments last couple years. I\u2019ve been wanting to answer some questions in ROS Answers but had difficulty in getting started. I understand there are tags, \u201cSort By\u201d, etc., that I could use to navigate the answers but I had difficulty in spending time searching for questions that are in my domain.", "Any suggestions would be greatly appreciated and thanks for staring this thread.", "Wow, quite some responses.", "First: re: \u201cunproven claims\u201d: I deliberately used the words \u201cI\u2019ve noticed\u201d and nuanced it with \u201csomewhat of\u201d. That makes this a personal observation. I\u2019m in a university, so I\u2019m all for hard numbers and facts, but I don\u2019t have them. I\u2019m also wondering how I would gather such statistics, as unfortunately Askbot doesn\u2019t have a ", " badge.", "Or, for individual supporters/not affiliated, maybe those with enough internet points karma (can\u2019t remember what ROS answers uses) they can get their ROSCON registration fee waived/top contributors full expense paid and sponsored to come to ROSCON.", "Motivation of community members to volunteer for or contribute to certain hard-to-sell tasks is something we\u2019re also looking at in the ROSIN Quality Working group sessions (", "). It\u2019s not trivial. Other than financial incentives a lot of things come down to karma or status.", "We do get quite some input from companies that they will, as part of their hiring process, look at someone\u2019s position and contribution to the community. However, this is typically not so much to gauge what sort of nice guy the candidate is, as it is to see what his technical experience is.", "re: sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money.", " do you have suggestions on how to find questions about", "application design, human-robot interaction, behaviour modelling", "in ROS Answers? I\u2019ve been using ROS for creating interactive behaviors and running HRI experiments last couple years. I\u2019ve been wanting to answer some questions in ROS Answers but had difficulty in getting started. I understand there are tags, \u201cSort By\u201d, etc., that I could use to navigate the answers but I had difficulty in spending time searching for questions that are in my domain.", "Any suggestions would be greatly appreciated and thanks for staring this thread.", "Quite some ROS Answers posts have either:", "This makes it hard to find posts relevant to your topic specifically. I\u2019m afraid I wouldn\u2019t know any real tips that solve your problem unfortunately.", "Personally I\u2019ve found just spending 15 mins quickly reading each new question is enough to get a feeling for whether it\u2019s something you have an affinity with. I also try to retag questions if I feel that\u2019s necessary.", "+1000 for wanting to answer questions btw. You must have experience with quite some packages and infrastructure by now. Don\u2019t limit yourself to HRI ", "sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money", "Just giving my though on this, It\u2019s a good idea but it will maybe launch a \u201cKarma War\u201d from some people to answers all the questions, with a risk of bad answers, just to get Karma (If the OP of the questions mark as answered, It gives Karma nonetheless.", "In addition to visiting the site periodically you can also sign up to be notified when specific tags are mentioned. So if there\u2019s a topic that you feel that you\u2019re able to help out on add tags associated with that to the \u201cInteresting tags\u201d section in the side bar and make sure you have \u201cEntire forum (tag filtered)\u201d enabled in your \u201cemail alerts\u201d settings.", "In a more \u201caggresive\u201d way, we could also setup a small bot who read tag and associate it with the maintener mail on package.txt and send periodic summary of the number of unanswered new questions about the package (or make a post on a specific Discourse channel).", "I think this is a good time to thank gvdhoorn for his great work on ROS answers. I visit that place from time to time and in perceived most of the questions there is at least a helpful comment from him.", "First off, thanks ", " for bringing this up and for all the energy you put into answering questions lately, I\u2019m sure you\u2019ve helped a bunch of people! I\u2019ve been slacking off in answering questions myself, and I\u2019ll try to spend the 15 minutes each day from now on.", "I believe it\u2019s best to encourage people to answer questions out of intrinsic motivations instead of paying them to do so (because it will lead to sub-standard answers). When I started answering questions 7 years ago, what motivated me was:", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages.", "Would it make sense to let a package maintainer know about those that they can be referenced and (later) added in some FAQ section on the packages wiki page?", "A large number of volunteers from the community help out by answering quite a few of these questions, or at least try to help getting closer to a solution \u2013 and they deserve a lot of recognition and thanks for that.", "I got most answers from you ", " so far. I appreciate your effort very very much! Thx!", "I have no real feeling for this outside ", ", but I wonder how many questions that get asked have been answered before. I know from personal experience that I\u2019ve answered the same non-trivial question on multiple occasions.", "Part of this, I think, comes down to search. For example, I often get asked, \u201cCan I use robot_localization with just a GPS and IMU?\u201d If users search for that on ROS Answers, ", ". None of those initial results are all that relevant to the search. If we search using Google, ", ".", "Obviously, part of this is a failure of the package documentation, too, and ", "\u2019s idea to reference the questions in their package wikis is a good one. I\u2019m just wondering if there\u2019s anything we can do to make previous answers easier to find.", "Regardless, I learned a lot from ROS Answers when I started using ROS, and I think we owe it to new users to give them the same level of help that we received. I\u2019ll try to carve out more time to answer questions, and will encourage my team members to do the same.", "sponsored ROSCon: that is a nice idea. That could certainly be a nice way to recognise community contributions without resorting to just handing out money", "Just giving my though on this, It\u2019s a good idea but it will maybe launch a \u201cKarma War\u201d from some people to answers all the questions, with a risk of bad answers, just to get Karma (If the OP of the questions mark as answered, It gives Karma nonetheless.", "This is something we would need to be careful with, I agree. However, we do still have other users and the moderation system. Answers (and questions) can also be downvoted. The idea of this being of course that it\u2019s a self-balancing system (in the end).", "I\u2019ve been slacking off in answering questions myself, and I\u2019ll try to spend the 15 minutes each day from now on.", "My OP was more a call-to-action, as in \u201cwe can do better\u201d, but if you feel you\u2019ve been \u201cslacking off\u201d then that sounds like a good motivator ", "I believe it\u2019s best to encourage people to answer questions out of intrinsic motivations instead of paying them to do so (because it will lead to sub-standard answers).", "As I wrote in reply to ", "\u2019s comment, I believe this can be mitigated by the self-correcting nature of the system with up- and down-votes.", "I do agree completely that intrinsic motivation is best. For some people money is a good intrinsic motivator though ", " But perhaps altruism might be a good one too.", "When all the easy questions were already gone, I started researching answers to questions that I didn\u2019t immediately know the answer to. After a while, I noticed that this really helped me to gain broad understanding of everything there is in ROS, even though that wasn\u2019t my original intent when answering those questions (I wanted to get karma!). I believe this is really the best way to become a ROS expert.", "I completely agree with this one too. It\u2019s basically what I wrote at the end of my (perhaps too long) OP.", "While many are about installation, deployment or integration issues, the ones that make ROS Answers really worth the time are those that ask about advice on configuration of specific packages.", "Would it make sense to let a package maintainer know about those that they can be referenced and (later) added in some FAQ section on the packages wiki page?", "That is a good idea, and would even not be too difficult to implement. It would just take some time.", "Perhaps the easiest would be \u2013 seeing as not all questions are properly tagged \u2013 for someone to browse ROS Answers and collect questions about a certain package which he could then collate and post on the issue tracker of that package / those packages.", "Rather low-tech, but perhaps even already enough.", "I have no real feeling for this outside ", ", but I wonder how many questions that get asked have been answered before. I know from personal experience that I\u2019ve answered the same non-trivial question on multiple occasions.", "Part of this, I think, comes down to search. For example, I often get asked, \u201cCan I use robot_localization with just a GPS and IMU?\u201d If users search for that on ROS Answers, ", ". None of those initial results are all that relevant to the search. If we search using Google, ", ".", "This is definitely true. The number of duplicates or near-duplicates is just about bearable at the moment.", "The search is rather sub-par at the moment: I\u2019ve even opened an issue about that: ", " (which you ", "ed).", "I ", " use Google: ", ".", ": is this an opportunity to revisit ", "?", "Obviously, part of this is a failure of the package documentation, too, and ", "\u2019s idea to reference the questions in their package wikis is a good one. I\u2019m just wondering if there\u2019s anything we can do to make previous answers easier to find.", "Would this perhaps warrant a post over in the ", " category?", "Regardless, I learned a lot from ROS Answers when I started using ROS, and I think we owe it to new users to give them the same level of help that we received. I\u2019ll try to carve out more time to answer questions, and will encourage my team members to do the same.", "Great, thanks ", ". And thanks for the support for ", " ", ".", "Although I can\u2019t find a reference, there was at least once a coordinated day for burning down unanswered questions wasn\u2019t it? My calendar still marks Septermber 19th is world ", " day (not sure when, not sure if that was the title either).", "I\u2019m glad someone still celebrates ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a title that does not accurately (or even at all) reflect the topic / subject of the post", "a poorly chosen set of tags (the obligatory ROS distro tag has helped a bit here)", "an ", "\n", "Helping people. You really get a good feeling for having helped someone who otherwise wouldn\u2019t have solved their problem, and who perhaps has nobody else to turn to for help.", "Getting karma. It sounds silly to pursue fake internet points, but the gamification aspect definitely worked for me. Getting karma and earning new badges by answering questions feels like leveling up in a game.", "When all the easy questions were already gone, I started researching answers to questions that I didn\u2019t immediately know the answer to. After a while, I noticed that this really helped me to gain broad understanding of everything there is in ROS, even though that wasn\u2019t my original intent when answering those questions (I wanted to get karma!). I believe this is really the best way to become a ROS expert."], "url": "https://discourse.ros.org/t/ros-answers-needs-your-help/5147"},
{"title": "Awesome list for professional robotic development with ROS in C++ and Python", "thread_contents": ["Just a bunch of powerful robotic resources and tools for professional robotic development with ROS in C++ and Python. - Ly0n/awesome-robotic-tooling", "\n", "\n", "That is quite the list, very nice! ", "It\u2019s a bit of a plug, but would this be something to post in the ", " category?", "Love it, thanks for doing this! I also have my awesome robotics list: ", " .", " for the ade-cli we now also have a a very good blog post: ", " .", "I missed a docker template in the list and used the ADE base image. thanks ", "\n", "Thank you for the list. This will be great for junior programmer like me.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/awesome-list-for-professional-robotic-development-with-ros-in-c-and-python/11216"},
{"title": "Which board/microprocessor should I use? I want to use ROS Kinetic on Ubuntu 16.04, along with packages like move_base, laser_scan_matcher for indoor autonomous navigation", "thread_contents": ["I am confused as to which board will be able to handle the necessary computation to run autonomous navigation for my turtlebot-like-robot.", "\nI want to run Ubuntu 16.04 OS and ROS Kinetic, and need packages like move_base, laser_scan_matcher.", "\nI\u2019ve heard that Raspberry Pi 3 and its Arm Cortex A53 isn\u2019t enough for the purpose.", "\nWhat are your views?", "That highly depends on your budget, software requirements and space limitations.", "\nThe packages alone also don\u2019t say  that much about the required computation power.", "\nE.g., what kind of LIDAR are you using (how many data points per second does it produce?), what other kinds of sensor (e.g., cameras) are you using?", "\nFor cameras etc. the USB bandwidth might be a limiting factor that has to be kept in mind.", "Without more information, I can only suggest some generally viable small form factor options depending on your computation power requirements:", "\n", " You could try the Pi 4 which is a lot faster than the Pi 3. The 4GB RAM might not be enough depending on your use case but I assume it should be able to handle autonomous navigation with the kind of LIDAR you would put on a Turtlebot-like robot.", "\n", " A mini-PC such as an Intel NUC would surely be enough to handle almost anything you throw at it.", "what kind of LIDAR are you using (how many data points per second does it produce?)", "Hey ", ", I\u2019m using RPLidar A1, and using it at 5Hz with 2000 data points.", "\nNUC seems too bulky, actually. Looking for something much more compact like Pi.", "\nWould you recommend Beaglebone or Odroid XU4?", "Hi, if you don\u2019t have budget constraint you can try UP board or jetson nano or Beaglebone. Working with ROS , Lidar and some kind of application that is to be done like turtlebot alike of robot if designed.", "I agree with ", " on the UP board if budget is not a constraint.", "\nI wouldn\u2019t recommend a Beaglebone for this use case (I assume you want something simple to work with and develop on) because they have very little RAM (unless there\u2019s a model I missed) and that can lead to frustrating issues for inexperienced developers, e.g., you may not be able to compile directly on the board but will have to cross-compile on your host machine and deploy the binaries manually.", "\nA jetson nano really only makes sense if you also have a camera and want to use graphics accelerated algorithms or deep learning otherwise the Pi 4 is faster IIRC.", "\nHowever, if I were you I would try a Pi 4 since you\u2019re not dealing with a lot of data and if the Pi is not enough, it\u2019s a cheap mistake to make and maybe you can still use it for something else.", "\nAlso, the Pi is widely used and has the highest chance of finding someone who can help if you run into issues.", "\nI don\u2019t know how the Odroid compares to a Pi 4.", "Later on, you can still switch to something that is smaller / more energy efficient but harder to work with.", "Looks like you mised the ", " (Cortex A15, 1GB RAM) and the ", " (Cortex A15, 2GB RAM)", "I have had great luck using the TX2 with the ", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/which-board-microprocessor-should-i-use-i-want-to-use-ros-kinetic-on-ubuntu-16-04-along-with-packages-like-move-base-laser-scan-matcher-for-indoor-autonomous-navigation/11488"},
{"title": "Announcing LaMa: An alternative localization and mapping package", "thread_contents": ["Dear ROS users,", "We would like to announce the release of the ", " package.", "\nIt includes a framework for 3D volumetric grids (for mapping), a localization algorithm based on scan matching and two SLAM solution (an Online SLAM and a Particle Filter SLAM).", "The main feature is ", ". You can even run the Particle Filter SLAM in a Raspberry Pi.", "We provide ROS integration with the ", " package.", "Fell free to try it and provide any feedback.", "Very nice. I\u2019ll definitely take the localization out for a spin", "You should also give SLAM a chance ", "Looking forward to test it!", "Hi ", ",", "\nCongratulations on the release!", "\nI am wondering if you have found any differences comparing IRIS LaMa localization to amcl implementation that\u2019s already in ROS. Same for mapping, with comparison to popular ones out there.", "\nI am really curious to know.", "I am wondering if you have found any differences comparing IRIS LaMa localization to amcl implementation that\u2019s already in ROS. Same for mapping, with comparison to popular ones out there.", "\nI am really curious to know.", "Yes, I did compared my solutions with popular ones found in ROS. In the ", " file you can find a few papers where I compare LaMa\u2019s algorithms with solutions such as AMCL and GMapping. But here are my selling points:", ": In general both provide good accuracy but (by default) AMCL does not use all data to compensate for particle filter\u2019s overhead and that can result in some errors. Scan Matching can be 5x times faster or more. I still use AMCL in applications where information is reduced and noisy.", ": I think that GMapping is a wonderful piece of technology but very slow. I remember, back in early 2012, using GMapping online was ", ". LaMa PF SLAM is kinda like a ", " GMapping or ", " GMapping if you activate multi-threading. LaMa Online SLAM is the turbo version, it can generate the Intel map in 5seconds. Here is the result: ", ": I used the ", " to compare with other SLAM solutions and we did good ", ". I believe that ", " also used the same benchmark.", ": ", " is another top reference in robotics. I only developed SDM because OctoMap\u2019s main focus is occupancy grids and I needed more flexibility. The inner structure of SDM is model agnostic and provides the same features for any type of grid map. Those features include Copy-on-Write and Online Data Compression.", "Thanks! BTW that map looks awesome!", "Both mapping and localization work pretty well, but I am particularly happy with the latter.", "Congratulation for the awesome work !!!", "Would you mind sharing a video of your tests? A video is worth a thousand images ", "Looks very interesting!", "\nDid I miss the link to the benchmark results somewhere?", "\nA comprehensive quick overview demonstrating the performance both in terms of speed and accuracy in comparison to the other methods would also help a lot.", "Did I miss the link to the benchmark results somewhere?", "No, you did not miss the link. Some of the benchmarks are in published articles.", "\nBut I can provide a short summary of the results.", "I used this ", " (like other have). This software provides a mean ", " and ", " errors.", "Here is short version of the errors:", "The following table show how long each solution takes to process a given dataset:", "  The values that are presented here were not obtained using the same computer, therefore it may not be a ", " comparison. Nonetheless, the difference in scale is obvious.", "\nThe ", " values are taken from the author\u2019s paper.", "Accuracy is on pair with known (and established) solutions such as GMapping with very good performance. I omitted the ", " but I can say (with confidence) that it also offers good results.", "You should give it a try!", "But GMapping uses Particle Filter right?", "Yes, GMapping uses a Particle Filter.", " congrats for the packages, and thanks for sharing it! ", "What exact ", " files have you used as inputs for the SLAM benchmarking?", "I\u2019d like to know specifically what computers each of those tests were run on, I\u2019m not sure you could say the scale is obvious if its a 5th generation laptop vs an 8th generation desktop CPU and had a fair comparison of settings.", " congrats for the packages, and thanks for sharing it! ", "What exact ", " files have you used as inputs for the SLAM benchmarking?", "Thank you!", "\nFor the benchmark I used the raw log files available at ", ". These are CARMEN ", " log files (if I am not mistaken). To use these logs with ROS I created a small program to convert ", " to ", ".", "I\u2019d like to know specifically what computers each of those tests were run on, I\u2019m not sure you could say the scale is obvious if its a 5th generation laptop vs an 8th generation desktop CPU and had a fair comparison of settings.", "I took the time to redo the tests and you are correct, the scale is not obvious.", "The original GMapping values were taken from a HP ProLiant with two Intel Xeon CPU E5-2640 0 @ 2.50GHz (Max Turbo Frequency 3.00 GHz) running Ubuntu 16.04 TLS.", "Here are some values taken from the same computer, a Thinkpad L480 with an Intel  i7-8550U CPU @ 1.80GHz (Max Turbo Frequency 4.00 GHz) running Ubuntu 18.04.3 LTS.", "Maybe I should also take the time to test ", ".", "I like the direction this thread is taking\u2026", "Lot of us care about the quality of localization and it is hard to figure out the pros and cons of different algorithm, being also Karto one I would add to the list.", "It would be very nice if we figure out a way to make these benchmark easily reproducible, maybe with a Docker image and a Gthub repository where people can add their own algorithm to the test.", "If we dream big, we may even associate a continuous integration machine and get updated about new results!!! ", "Something that I consider valuable is the amount of tuning that a certain algorithm need.", "With Cartographer, I always have the feeling that I can play with hard to understand parameters to make it work. I know that there are parameters that EVENTUALLY will give me an amazing map, but it could be frustrating.", "Something I appreciate of ", " is that it just worked, at least with my dataset.", "Benchmarking different SLAM algorithms (for map/localization quality and speed) is not a straightforward task. Benchmarks such as the ", "  try to be objective (and it kinda is), but, the final result can (and most likely will) be influenced by the ", " of the algorithm. This is most evident If I am the one trying to do the benchmark without fully understanding what the parameters do. And just like ", " said, this can be frustrating.", "Nonetheless, I do believe that benchmarking is necessary. It creates a healthy competition that can result in improvements. For example, the ", " has a leader board where the authors of a solution submit their code with proper parametrization for evaluation. Maybe something like this could exist for SLAM? I usually have to search for papers to find this kind of information.", "Something I appreciate of ", " is that it just worked, at least with my dataset.", "I think that everybody likes things that just work ", "\nThe response of a system to change in parameters was something that I discussed quite often with my colleagues.", "Hi,", "I\u2019ve been playing with this today and wanted to share my very preliminary results.", "I would independently verify that its much lighter weight than other options I\u2019ve seen recently for the optimizer based SLAM option. I\u2019m seeing the CPU grow but overall pretty consistent over short trajectories at around 10% CPU on a 6th gen i7. Over the same trajectories with my package I\u2019m seeing less consistent but generally hovering around 30% \u2013 both with more or less the same memory utilization.", "I\u2019d say though the rastered map image out isn\u2019t as good as slam toolbox and I\u2019m not seeing it accomplish loop closures as responsively. That may not be a big deal for many users. For the datasets it works with, it works pretty well to keep as a reasonable option on the table. For the datasets it doesn\u2019t work with, I have no idea what\u2019s going on. See below, the same robot, on the same day in the same environment 2 datasets were taken, one works fine, the other does this:", "It worked for about 10-20 updates and then just started blowing up. No warnings or errors thrown. I\u2019m also going to have to figure out why LaMa has so much of a CPU drop from Slam toolbox, it looks like it uses much of the same techniques and it may lie in the dependency libraries since I use Ceres as my LM solver & a bunch of outside libraries so I can swap out with new technology trends \u2013 though I\u2019m sure you get a really nice speed up from the distance field work as well.", "Overall I think this is a pretty good option, but needs to expose more of the parameters, documentation, and hardening \u2013 which in SLAM isn\u2019t the hard stuff.", "If there\u2019s any interest in writing and maintaining long term a ROS2 port of this work, I\u2019d support this as a genuine option for us on the ROS2 Navigation Working Group/TSC to consider at for the \u201cdefault option\u201d in ROS2. I think its well written and enables a number of applications on lower power machines, though being able to scale from small examples to 200,000+ sqft facilities remains to be evaluated.", "Edit: I didn\u2019t evaluate the localization stuff.", "\nEdit2: I was thinking about those numbers, which seemed high and remembered that I didn\u2019t build in release mode so those are going to be higher than you\u2019d see in production.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/announcing-lama-an-alternative-localization-and-mapping-package/10916"},
{"title": "Discourse - A very powerful mailinglist", "thread_contents": ["Preface: I know it\u2019s horribly hard to organize the community and osrf tries to do the job, thank you!", "\n< rant >", "\nUntil recently I simply ignored discourse because it looks like a huge soup of everything, sorted by web 3.0 standards and only used by a handful of people. I\u2019m sure I\u2019m not the only one who looks at it that way.", "Now, as OSRF keeps pushing people towards this website by corporate policy (e.g. ", "),", "\nI tried to integrate it with my mailbox.", "\n< /rant >", "\nThis is because I skim through my mails every morning, but do not plan to login to a website everyday just to scan through things I\u2019m probably only marginally interested in.", "This brought up two issues for me:", " here is the constructive criticism you asked me for. Well more like a \u201ccriticism\u201d and a \u201cconstructive\u201d part, but anyway.", "Thanks in advance for pointers to solutions!", "Hi ", " thanks for the feedback,", "Indeed discourse is a more modern front end, it is a little different than many of us are used to but that\u2019s one of the things that is valuable about it.", "In the current state on the mailing lists I often find myself finding threads on nabble rather than the main site when I search. Discourse is focused on both supporting active members via the realtime email notifications as well as being easily searchable and browseable which is highly valuable for the majority of ROS users who are not actively subscribed. From the ", " we have approximately twice as many people with wiki accounts, 5x the users on ", ", and 20x the unique IPs as subscribers to ros-users. We want to make things better for this large majority of the users. Improving browseability and ease of search significantly improves this experience.", "One of the main impetus\u2019 behind the switch was ", " To find that thread, I searched my personal inbox, found the title and then searched online for the archive so I could link to it. Without cheating and remembering exact strings from that thread it\u2019s very hard to find the thread using google search. Discourse is designed to be easily crawled by search engines and the results are picked up quite quickly with good rank.", "We\u2019ve had many reports of people unsubscribing from ros-users due to the volume being too high. As the user community has grown the ros-users mailing list subscriber base has not grown proportionally. Typically it\u2019s  a subcategory of content which individuals are not interested in and they unsubscribe because it\u2019s filling up their inbox too often. Whether it being job postings, development discussions etc with discourse and your personal settings now you can easily pick the topics you\u2019re interested in subscribing to and let the rest be.", "We\u2019ve also seen a trend that our SIG\u2019s get created populated, and then the drift off into a slow quiet decline because they are not discoverable. It requires knowing there\u2019s a mailing list to find, finding the mailing list, browsing the individual archive. Often after the initial impetus has petered out, there are a bunch of people on the list and if a new person joins the members will respond, but rarely does the membership grow after the initial publicity due to the SIGs being hard to discover whereas now they can be a category and new people joining the community can easily discover them through the front page of the forum.", "Discourse can scale and support many more users. If you browse through: ", " you\u2019ll see forums with tens of thousands of users. Clearly that level of activity requires filtering and categorizing, but that\u2019s what discourse empowers the user to do.", "To answer your specific questions:", "In general, how do I send a mail to a given category to start a conversation?", "You should be able to send a message to ros+[categoryname]", " if you are at trust level 1 or higher. To get to trust level 1 you need to enter 5 topics, read 30 posts and spend at least 10 minutes on the site. This is primarily an anti-spam measure. From your ", " badge I see that you\u2019ve already reached level 1 from just looking around the site.", "You can find the ", " in the url if you browse to the category. Note that subcagetories are separated by a ", " not a ", " so it\u2019s ", " where the url reads ", "From your reply on github you implied that the domain did not resolve for you. I received the notification of your post from that domain so as far as I know it\u2019s resolving and operating just fine.", "How do I subscribe to a specific category only? I changed my profile to \u201cmailing list mode\u201d and only category I \u201cwatch\u201d is \u201cMoveIt! Developers\u201d, but still got mails for ", " and ", " . I know that some colleagues face the same problem.", "In terms of filtering, mailing list mode means that you get everything on the forum sent to you, just like a mailing list would. If you want to filter you should turn that off and then you should only get notifications on any categories you\u2019ve setup to watch.", "I never said there are no reasons to use such a system, but anyway thanks for the lengthy elaboration of osrf\u2019s reasoning.", "\nI didn\u2019t know that short thread got so much attention, but it kind of makes sense. It actually made me laugh when someone suggested the user list got too active.", "\nEspecially as it feels like every mail that is sent by someone other than yourself gets a fast reply saying \u201cgo to ", "\u201d, hehe.", "\nI\u2019m pretty sure one day of subscription to the Linux Kernel Mailing List would change their mind (and their way of handling mails). ", "In general, how do I send a mail to a given category to start a conversation?", "You should be able to send a message to ros+[categoryname]", " if you are at trust level 1 or higher.", "\nFrom your reply on github you implied that the domain did not resolve for you.", "I forgot to look up their MX entries(actually just one). I simply weighted it more likely that you made a typo", "\nthan that all mails sent for ", " discourse instance are send over a central mail server.", "\nStill sounds somewhat fishy to me.", "\nSo I suppose it works as you described and I\u2019ll test it eventually. Thanks!", "How do I subscribe to a specific category only?", "In terms of filtering, mailing list mode means that you get everything on the forum sent to you, just like a mailing list would.", "I enabled mailing list mode only recently because I didn\u2019t get a number of replies from ", " .", "\nAlthough I received some other posts from the watched category\u2026", "\nLooking into it a bit, I suspect that discourse did not automatically send mails to me for replies to the thread because the thread got created before I added the category to the watch list (and if I remember correctly also before I created the accout).", "\nI\u2019m not sure this is the case, but it might actually be a bug in the system?", "I turned off mailing list mode again and I expect that everything works as expected now.", "\nIf I notice any further problems, I\u2019ll come back to this.", "Thanks for your time.", "I forgot to look up their MX entries(actually just one). I simply weighted it more likely that you made a typothan that all mails sent for any discourse instance are send over a central mail server.", "This is because we\u2019re using their hosted service. Originally we had to setup our own mail server SPF/DKIM entries etc and we\u2019d done it successfully. But for most of their customers that\u2019s too much so they\u2019ve moved to processing all the emails centrally for the hosted accounts.[quote=\u201cv4hn, post:3, topic:389\u201d]", "\nI enabled mailing list mode only recently because I didn\u2019t get a number of replies from ", " .Although I received some other posts from the watched category\u2026Looking into it a bit, I suspect that discourse did not automatically send mails to me for replies to the thread because the thread got created before I added the category to the watch list (and if I remember correctly also before I created the accout).I\u2019m not sure this is the case, but it might actually be a bug in the system?", "I turned off mailing list mode again and I expect that everything works as expected now.If I notice any further problems, I\u2019ll come back to this.", "Thanks for your time.", "\n[/quote]", "You\u2019re welcome. We want to hear feedback and understand how things are working or not so that we can fix it. Discourse is under active development/maintenance. Well formed bug reports  ", "  can easily be diagnosed, a fix developed, and deployed on the site within a week.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["In general, how do I send a mail to a given category to start a conversation?", "How do I subscribe to a specific category only? I changed my profile to \u201cmailing list mode\u201d and only category I \u201cwatch\u201d is \u201cMoveIt! Developers\u201d, but still got mails for ", " and ", " . I know that some colleagues face the same problem."], "url": "https://discourse.ros.org/t/discourse-a-very-powerful-mailinglist/389"},
{"title": "Intel\u00ae Euclid\u2122 Development Kit is open for pre-order!", "thread_contents": ["Intel\u00ae Euclid\u2122 Development Kit is open for pre-order! (", ") ", "The Intel\u00ae Euclid\u2122 Development Kit features the integration of Intel\u00ae RealSense\u2122 depth camera technology, a motion camera, and an Intel\u00ae Atom\u2122 x7-Z8700 Quad core CPU to produce a compact and sleek all-in-one computer and depth camera in the size of a candy bar! The Intel\u00ae Euclid\u2122 Development Kit is designed to be operable out of the box with pre-installed software including an Ubuntu\u00ae 16.04 operating system and ROS Kinetic Kame, which makes it perfect for Making Robots and other makers projects!", "\nIt is computer combined with a depth camera,  Fisheye camera, IMU, WiFI, and Bluetooth along with some other sensors like GPS and environmental sensors in a very small size. It comes with a battery so it is completely stand alone.", "\nYou can check out the Euclid in action on a turtlebot here: (all processing is being done on the Euclid device)", "Limited quantity is available, so get it before it runs out ", "euclid community site is live: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/intel-euclid-development-kit-is-open-for-pre-order/1828"},
{"title": "Chat for ROS on robotics.stackexchange.com", "thread_contents": ["I created a ", " on ", " for ROS. The main purpose and rationale is to bring experts together to discuss interesting problems (related to ROS) and helping beginners (and non-) in solving their problems in real-time for a more active and hopefully healthy community.", "Isn\u2019t that why we have ", " and ", "?", "Yes, but that is a more real-time alternative which hopefully will attract users from Stack Exchange communities. As a matter of fact, you answered me after almost 1h, whereas in that chat, if someone is constantly there, you can tag that person and that person will be notified and eventually will help faster.", "If you\u2019re really bored, you can hang out in the barren wasteland that is ", "I came to make a comparison between the ", " and Mars (been there forever, may have had life once, now populated by bots). But I checked the channel first and discovered, much to my amazement, that it\u2019s actually been fairly active recently!", "I fully agree! The ", " IRC channel is a nice place to hangout, as is the ", " IRC channel for everything MoveIt!", "How can we get logins to the slack account ?", "In light of the Slack channel being even quieter than the IRC we\u2019d like to officially deprecate it. Setting up the Slack channel was an experiment to see if we could build more community through the new tool. However it seems that it\u2019s been more of a distraction and split peoples limited focus.", "I would like to encourage people interested in realtime chat about ROS to use the IRC channel.", "Bringing everyone together in one public place is the most likely to get critical mass.", "As linked above you can find the ", " channel on freenode. Details are at: ", "Logs are viewable at: ", "And there are multiple web clients if you don\u2019t have an IRC client installed locally.", "\nYou can follow either of these links. (Please change your username)", "How can we get logins to the slack account ?", "This is part of the challenge of slack. We had to setup a special heroku app to allow people to request invites to even be able to view the convesations. Maintaining this is part of the overhead we don\u2019t want to keep incurring for such a low volume.", "In the IRC chat room there\u2019s usually just one person that is capable of answering a few questions, and many times it is inactive. Furthermore, Stack Exchange communities seem to be more visible to a wider range of people with respect to IRC chats. If I had to bet on a growing and sustainable community I would not do it on the IRC chat room.", "The stack exchange chat doesn\u2019t seem to have a client. +1 for irc", "As far as I know, you\u2019re right, so far there isn\u2019t a client for the Stack Exchange\u2019s chats (except clearly the browser). A plus for the chat I created is that it\u2019s really a chat on a website regarding robotics, so, possibly, there we could group a wider range of experts in the field.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/chat-for-ros-on-robotics-stackexchange-com/1841"},
{"title": "New package for mobile manipulation planning", "thread_contents": ["I\u2019d like to announce a new package for mobile manipulation planning! ", "The simple algorithm calculates a robot base location so the end-effector (EE) can reach a desired pose. The method is computationally fast (since it depends on a MoveIt! Cartesian motion plan), simple (~300 lines), and easy to use (requiring just a service to provide the desired pose). There is a basic example included.", "Below, the first image shows a mobile manipulator which is too far to grasp the object of interest. The desired EE pose is shown in green. The second image shows the freshly-calculated base position (red cube) which will allow the robot to reach that pose.", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-package-for-mobile-manipulation-planning/1385"},
{"title": "Robot As a Service, anyone?", "thread_contents": ["We\u2019ve gotten interested in collaborating with our University Library to create a \u201crobot as a service\u201d facility. I am in the process of writing a grant and was curious whether anyone here had done something like that who would want to exchange ideas.", "p.s. I\u2019ve found several related papers which I would attach here if only I knew how.", "Can you define \u201crobot as a service\u201d? E.g., are you referring to situations where someone can get ephemeral and anonymous access to a remote robot, in the style of VM instances in cloud computing?", "you might not be permitted to attached PDFs (security risk), but you can list citations in plaintext.", "I started a list of related work on this topic at ", "Yes. Its kind of like the idea of a virtual remote laboratory, but the lab contains a set of (physical) robots that can be scheduled remotely and used to try different things. Initially they wouldn\u2019t ever leave the lab. Also initially you would have the choice of one hardware configuration. The starting point can be thought more of as a teaching and experimentation environment. At least that\u2019s how we\u2019re thinking.", "Pito Salas", "\nBrandeis Computer Science", "\nVolen 134", "cool!", "I am starting a company that provides exactly this service (", "). We have not made any big announcements yet, but here are two cases we are targeting that might align with your interests:", "physical robots in a well documented workspace to which users get remote access, but to which they (users) can never physically go themselves.", "easy sharing of experimental robots from one lab to another, peer-to-peer, where the documentation depends on what users (peers) provide.", "The main goal of ", " is to support reproducible research. The environments are \u201cwell documented\u201d, i.e., you can build them yourself if you do not want to use the remote access service. The case of ", " includes both live interaction and non-interactive experiments. We have homing code that returns the robots to a set of known initial states at the end of each instantiation, which supports reproducibility.", "The main goal of ", " is to facilitate remotely sharing of heterogeneous robots from research labs, something which has been re-invented by different groups over the past 20 years.", "Hi,", "I\u2019m just wondering what\u2019s the advantage between this approach, versus using a simulation? I know a simulated world is never the same as the real world, but it\u2019s easier, and could provide (almost) as much data as required?", "Arif", "Hi there.", "I\u2019m involved in development a platform allows to create a kind of \u201crobot as a service\u201d: ", ". First platform user created a platform (sic!) for direct drone hiring (", ") for example. I will be glad to exchange ideas too.", "One of the papers ", " wanted to attach is probably this one:", "865.07 KB", "I also vaguely recall that someone, possibly Bosch, had a similar service available way back when the PR2s were first distributed, but I could be mixing it up with the lab from that paper.", "Fair question ", ":  The feeling is that, once you\u2019ve gotten \u201cit\u201d to work with the simulator, you need a real robot. And you may not have the facility or the space or the money to do so. Also depending on your software/technical level you may not have the ability. What I am envisioning (imagining?) is a way to program the \u201cbehavior\u201d of the robot with less than the total flexibility of ROS, but still very powerful, and for a specific more limited domain, at least at first.", "Hi,", "I have two link that can be interesting for you :", "\n", " , a old \u201cgame\u201d where you reserve a robot for some time and control it to do some pick and place task.", "\n", " , an educational robot arm that can be programmed with ", " from a web interface.", "\nI actually use Blockly (more or less an old version of snap) to let people (non-developer) program robots arm at work, it work pretty well.", "Is your vision something like a mix of these 2 links ? Like a ROS controlled robot from snap! with a webcam and reservation system ?", "I also vaguely recall that someone, possibly Bosch", "the first two authors of the paper that you cited were with Bosch.", "as an example with less sophistication than Snap!, there is ", "What I am envisioning (imagining?) is a way to program the \u201cbehavior\u201d of the robot with less than the total flexibility of ROS, but still very powerful, and for a specific more limited domain, at least at first.", "The earliest work of which I am aware is documented in the excellent book Beyond Webcams: An Introduction to Online Robots. Nov 2001. Eds. Ken Goldberg and Roland Siegwart. MIT Press. ISBN: 9780262072250. ", "It describes remote access to lab robots from around the years 1995, 1997, so some screenshots show the Mosaic Web browser, and some of the authors discuss using Java applets and clever tricks to make user\u2019s Web pages refresh (because modern JavaScript did not exist yet).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/robot-as-a-service-anyone/4463"},
{"title": "Self-driving Racing Challenge, car platform provided. Compete \u25cfConnect\u25cf Grow", "thread_contents": ["\nLet\u2019s grow, connect and test self-driving technology in a more interesting way.", "\nDIY Robocars KuaiKai provides car platforms equipped with computing platform, sensors, and by-wire control support for self-driving engineers, and also closed urban roads for racing. Theoretically, you can just save your algorithms to the flash disk and then you are good to go for the racing (", "). If you are still a hobbyist engineer without too much experience on the full-sized self-driving vehicle, you can choose to participate KuaiKai small-sized car onsite racing on the indoor racetrack.", " Car Onsite Racing: Full-sized car platform and hardware are provided onsite by the Organizer. The car platform is already equipped with self-driving engineering support. You come with your self-driving software and algorithms onsite  to compete on closed urban roads for self-driving skills such as obstacle avoidance and S-bend etc. (", " )", " Car Onsite Racing: You can choose to bring your own small-sized car onsite to race (as long as it is self-driving small-sized cars powered by Raspberry Pi, such as Donkey car ", " or use the standard small-sized car platform provided by the Organizer onsite. The racing is for Wheel to Wheel Tournament.", ": Coming Soon (not released yet)", "\nUdacity Self-driving NanoDegree Programs: Worth $1000 USD each", "Full-sized Car Onsite Racing: Participants arrive on May 19th, racing date is from May 20th-26th (the first five days are for installation and test)", "Small-sized Car Onsite Racing:  Participants arrive on May 24th, racing date is from May 25th-26th (the first day is for installation and test)", "\nGuiyang, Guizhou, China  (", ")", "DIY Robocars KuaiKai, creating an unforgettable self-driving racing experience that we all haven\u2019t thought of yet!", "\n", " more details on the racing website page: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ". DIY Robocars KuaiKai is a racing event dedicated to self-driving. Participants come onsite to compete, race, test and optimize their self-driving software and algorithms.", "\n", ". It\u2019s also an international event for global self-driivng startups, students, engineers, developers and enthusiasts to gather together for networking, communication and mutual growth.", "\n", ". DIY Robocars KuaiKai is a special way to mark the progress and milestones in self-driving development through the racing.", "\n", "\n", "\n", "\n", "\n", "\n", "Full-sized Car Onsite Racing: $40,000 USD", "Small-sized Car Onsite Racing:  $15,000 USD", "Human Driver vs. AI Driver Ultimate Challenge (Full-sized Car): $100,000 USD", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/self-driving-racing-challenge-car-platform-provided-compete-connect-grow/4431"},
{"title": "ROS Wiki: announcing extended CI badges in package headers", "thread_contents": ["Greetings ROS users,", "With this post we\u2019d like to update you on a change to the ", " section on the ROS wiki that was recently merged and has been rolled out for packages with a ", " in Melodic.", "As you may have noticed, for packages in Melodic the ", " badge does not only show ", " the source repository has been registered in the ", ", but it will also show the CI status of the repository. The colour of the badge and icons shown on the badge reflect the status and show a summary of the results of tests run during the last build.", "Clicking on the ", " badge shows a drop-down menu with up to five previous runs of the devel job. Clicking on any of these entries will take you to the page of the respective build for that particular repository.", "The idea behind this change is that the CI badge will reflect the actual state of a package like this, and that in turn this will facilitate assessing package state by giving the Jenkins job build status and test results a more prominent place on the wiki.", "There are three different icon-colour combinations, representing three different categories of packages.", "Repositories for which all tests are succeeding and the build ended successfully will get a green badge and a check-mark:", "All tests pass and Jenkins is happy about the build. So are we.", "Repositories with packages that have skipped tests will show a grey badge with a dash:", "While skipped tests do not necessarily indicate something is wrong with a package, they are typically skipped for a reason (they are expected to fail otherwise, require some elaborate setup, are in the process of being updated, etc). Packages with grey badges are most likely perfectly fine to use, but cautious users might want to verify ", " the tests are skipped.", "Repositories with failed tests will get a red badge and a cross:", "A badge like this is an indication that something is not right and one or more tests are failing. Inspecting the build history might be a good idea to try and understand what failed and why.", "The current implementation shows this information per repository, and is enabled or disabled per ROS release (and currently only enabled for Melodic). The former is both a limitation of the way test results are gathered by the buildfarm as well as a limitation of the current implementation. The latter was determined to be a good first step, with a potential extension to allow opt-outs added in a future PR.", "How and whether to take this effort further is one of the topics being discussed in the Quality Working Group meetings held each month, as part of the ", ". Comments, criticism and suggestions are more than welcomed: please open a topic in the ", " Discourse group to discuss.", "PR against ", ": ", ".", "\nPR against ", ": ", ".", "The PR link for the roswiki modifications ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-wiki-announcing-extended-ci-badges-in-package-headers/4930"},
{"title": "ROSCon 2018 Diversity Scholarships: Applications Open", "thread_contents": ["The ROSCon 2018 organizing committee aims for ", " to represent the entire ROS community, which is diverse and global. In addition to promoting technology that is open source, we also strive to ensure that our communities themselves are as open and accessible as possible, since we recognize that diversity benefits the ROS ecosystem as a whole.", "Whoever you are, whatever you do, and wherever you do it, if you\u2019re interested in ROS, then we want you to join us at ROSCon. To help reduce the financial barriers to conference attendance, the ROSCon organizing committee is offering a number of scholarships to members of traditionally underrepresented groups in the tech community. Thanks to the support of the program\u2019s sponsors, these scholarships each include a complimentary conference registration pass and three nights\u2019 accommodation shared with another recipient*. Limited travel support is available for participants whose travel to the conference would otherwise be infeasible**. Please note that all other expenses (including any visa requirements) will be the responsibility of the participant.", "*To maximize the impact of the scholarship funds, scholarship recipients will be asked to share a room with another recipient. Under special circumstances alternative arrangements can be accommodated.", "\n**Participants will be responsible for covering their travel expenses up-front, as the travel support will be provided once at the conference.", "We invite applications from members of groups that have been traditionally underrepresented in the robotics community (including but not limited to: women, LGBTQ+, people with disabilities, people from racial and/or ethnic minorities in the robotics community, and people from developing nations), who may not otherwise be able to attend ROSCon.", "Previous ROSCon Diversity Scholarship recipients are not eligible to re-apply, but we are proud to share this feedback from a participant of the ", ":", "The ROSCon Diversity Scholarship Program provided me with an opportunity that would have been completely impossible without it. I was able to attend my first robotics conference and feel empowered to keep working to try and make a positive impact on this community. Also, it was very encouraging to see so many companies stepping up to promote and enable diversity within their companies and the robotics community. Thank you!", "The ROSCon 2018 Diversity Program is made possible with support from the following sponsors:", "If your organization is interested in getting involved in the Diversity Program, please ", ".", "We also thank the conference Platinum Sponsor, Erle, and Gold Sponsors: Amazon, Clearpath, Fetch Robotics, Google, Locus, ROBOTIS, Tier IV, Universal Robots.", "To apply, ", " by May 6 2018, describing how you are involved with ROS and the robotics community and what you hope to get out of attending ROSCon. Scholarships will be awarded based on a combination of need and impact. Every applicant will be notified of the outcome of their application.", "For more information about ROSCon 2018, including the program, code of conduct, and childcare options, please see ", "Friendly reminder that applications for the ROSCon 2018 Diversity Scholarship are closing on May 6. If there\u2019s someone who you think might be interested, please consider sending them this info directly.", "In case you missed it, the ", ", too!", "ROSCon Diversity Committee", "I unfortunately missed this deadline ", " . It is not possible to apply anymore? How about the conference itself? Is it free for entrance?", "Hi ROSCon Diversity Committee!", "Do you know when do the results of the diversity scholarship come out?", "Thanks! ", "Hello Guys,", "I agree with ", ", I\u2019m also very excited to know when the results will come out.", "Best regards.", "We have received a higher volume of applications than anticipated this year and are in the process of thoroughly reviewing all submissions received. Every applicant will be notified of the outcome of their application.", "Thanks for your patience!", "ROSCon Diversity Committee", "I unfortunately missed this deadline ", " . It is not possible to apply anymore? How about the conference itself? Is it free for entrance?", "(this was responded to via PM)", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/roscon-2018-diversity-scholarships-applications-open/4552"},
{"title": "Please ask all questions on ROS answers", "thread_contents": ["Hi ROS users,", "As a reminder, please ask questions on ", ", following our ", ".", "While bootstrapping ROS 2 we chose to centralize questions on the mailing list for greater visibility. Now that the volume has increased, that becomes unsustainable, so we now request asking any ROS question (ROS 1 or ROS 2) on ", ".", "\nThe ROS 2 core developers are all subscribed to tags related to ROS 2.", "To avoid confusion, when posting new questions, please tag them with the rosdistro you are using (for ROS1: ", ", ", "\u2026 for ROS2: ", ", ", "\u2026). If the issue spans several ROS distributions, please use the more generic ", " or ", " tags.", "Thanks again to all the members of the community making ROS Answers a successful exchange platform with over 20 questions a day answered by the community.", "Your friendly ROS team", "OK, I will , thank you reminder", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/please-ask-all-questions-on-ros-answers/4259"},
{"title": "Magni and Loki available for pre-ordering!", "thread_contents": ["Our ROS robots, are now available on Indiegogo:", "If you could build your robot in hours, what would you build? | Check out 'Magni - Build your robot app in hours, not months' on Indiegogo.", "We are building ROS-powered robots to enable you to build your robot applications faster. You can find more info on our website, ", ".", "A lot of our code is opensource on our Github (", "). Feel free to check it out.", "Rohan", "\nUbiquity Robotics", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/magni-and-loki-available-for-pre-ordering/4145"},
{"title": "Announcing TurtleBot3 Software(v1.0.0) and Firmware(v1.2.0) Update", "thread_contents": ["Hello~!! everyone ", "Do you remember ROBOTIS, Intel and OpenRobotics announced official release of TurtleBot3", "\n@ ICRA 2017 in Singapore?", "Thank you to all who loved TurtleBot3  for the past year ", "\nWe celebrated the first anniversary of TurtleBot3 and prepared software and firmware updates and more powerful WiKi ", "This update considered may issues and requests from users. We are sincerely thankful to them.", "\nTurtleBot3 can get even better through a lot of interest. So please feel free to suggest any functions or ask questions into our ", "\n", "Update gmapping parameters", "Update navigation parameters", "Modified version check", "Add robot model for ", " and ", "(1) Upload new firmware(1.2.0)", "[TurtleBot3] ", " or ", "(2) Download new software (1.0.0)", "[TurtleBot3] ", "[RemotePC]", "(3) Download new software (1.0.0)", "(4) Bringup TurtleBot3", "[TurtleBot3] ", "If you succeeded to firmware and software update, you can watch below information on your terminal", "We are always very appreciative of all users to interest TurtleBot3.", "\nEspecially, Thank you to ", ", ", ", ", ", ", ", ", "\nBecause of your interests and efforts, TurtleBot3 ", " become more better.", "If you need more information about TurtleBot3, please visit our ", " or open ", ".", "Thank you", "\nDarby", "Great,Just amazing.", "\nTQ!", "Darby and all the Robotis Turtlebot3 designers and engineers. Once again thank you for this excellent Update adding many interesting and useful functions to the Turtlebot3. I can report that the Update worked properly on my Burger following the accurate instructions. I look forward to building and connecting the additional Sonar, LED, Bumper and Cliff sensors. Doing several demonstrations to many friends and colleagues I found the hardware and software to be very reliable. A tip: As we are all aware for reliable teleop, Image view, RVIZ, SLAM.Navigation functions, it is critical to have a sound WiFi network which I usually provide with a dedicated \u201cMobile Router\u201d (the one I use has both a LAN and WLAN radio) that gives the Turtlebot3 its own DHCP IP assignments separate from the private or public network,", "Thank you for sharing your experiences ", "Thank you ", "!", "Hello guys ", "We announces you can now download updated firmware(v1.2.1) and software today!!", "The new software includes", "Modified waffle URDF to escape the scan lines hitting the realsense camera", "\n(Issue : ", ")", "Add arguments to simply load multiple TurtleBot3s", "\n(Issue : ", ")", "\n(WiKi : ", ")", "The new firmware includes", "Add param to set prefix of TF", "\n(Issue : ", ")", "Update sensor time for cartogapher", "\n(Issue : ", ")", "\n(WiKi : ", ")", "You are supposed to upload new firmware to OpenCR and download software on SBC(Raspberry Pi 3 or Intel Joule) and Remote PC. Please follow ", " sections on this page to get this.", "You can see that, we are always try to solve issues everyday. These issues make better TurtleBot3 or more simple to launch it. If you have any inconvenience or questions about TurtleBot3, please feel free to open ticket(", ")", "Thanks", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/announcing-turtlebot3-software-v1-0-0-and-firmware-v1-2-0-update/4888"},
{"title": "Why don't we use ROS?", "thread_contents": ["There are people who are not interested in using ROS for their robots.", "Here is one story that I found on Internet!", "Good one! Of course all-in-one software can easily be better than one", "\ndistributed into many parts all living on their own.", "The point of ROS here is agility. If one can afford to spend a lot of time", "\nmaking custom software there\u2019s a high chance it will be better in many", "\nways. If they keep growing their software stack they\u2019ll eventually reinvent", "\nparts of ROS. I\u2019ve seen too many projects going that way,  sad for all that", "\nwasted time of smart ppl.", "That\u2019s a really interesting article with some well-founded arguments against ROS. I had this kind of discussion (a little bit different because we are targeting modular robots) just on last friday.", "\nWhen I started to take a deeper look into robotics I was strictly against ROS in the beginning, because my experience was: It is hard to learn, hard to integrate into an existing non-ROS software stack (custom build system makes it difficult to integrate, you need to start a roscore), and I couldn\u2019t get it working on a system other than ubuntu. Furthermore (this is an issue I still see today) most software that is written for a ROS software integrates ROS in a way that you can\u2019t use it without ROS even if it is just a hardware driver. For example we have a Sick TIM5xx laser scanner. The driver is quite well written, but it took a lot of work to use it without ROS.", "In case of an system that comes from an single vendor, where you\u2019ve got the control about all parts like in your case I think it is totaly valid to say, we don\u2019t need to use ROS. Also we all might agree that there are a lot of problems ROS introduces. (Some I mentioned before)", "But the whole point of using ROS is, that you want modularity, agility and you can rely on a whole lot of ready to use components (Let\u2019s take for example the very powerful TF2 components). Furthermore (as stated in your article) it introduces standard messages for certain kinds of data. That means I can simply replace a Sick Laserscanner with a Hokoyu, start the fitting driver and the system still works.", "I don\u2019t know what kind of robot system you build, but in case other people want to integrate it into their own environment ROS might be a huge advantage.", "Amen!  That story mirrors my own experience.  Neither approach is absolutely better or worse, they are each inherently better at solving different types of problems.  Some realtime problems are just silly to try to handle abstracted away from the source of the events.  But ROS, should not have a problem treating a low level integrated robot as a module by way of introduction to the \u201cROS way\u201d.", "What ROS lacks is a simple clear path (development, tutorial, community outreach) to go from individual low level integrated sumo bots, RC, Arduino firmata, Vex, FIRST, etc to a useful ROS dev environment that handles swarm communication, coordination, and configuration based modular code update on Arduino devices in the field that are not running ROS.", "As others here, I agree with a lot of what the article says about sensor-data integration in ROS and also about the benefits of using micro-controllers directly. We\u2019re actually working on making that work together with ROS better.", "That said, I know that most compute and sensor platforms are far from being the major contributor to the BoM that Pulu makes them out to be. At least in our platforms, while being a significant part, they are from from the major cost driver. Similarly, sensor-data integration, while important, is far from being the main software cost driver. I\u2019ve heard similar things from other vendors.", "However, I know of several low-cost robot platforms which experienced a ", " hard surprise once they tried to tackle ", ", which is a must for operation in human environments with loads. Now these platforms cost several times their original target.", "Looking at Pulu\u2019s current site, I would also guess that they have a lot to still to do in this regard. Their sensors are not safety-rated, and their chassis presents serious crush and tear risks. This kind of work is easy to underestimate.", "The OTHER problem with ROS is that it only address what I call the \u201cmiddle layer\u201d.   It is not good for low level hardware control.  It leaves uses one their own for that.   It is also not good for the higher level decision making that I would call AI.  So you and up building bridges at both ends.", "The other huge complain is the complexity of the dependancies and the build system.  It is such that only \u201cexperts\u201d are able to port ROS to new platforms, even if the platform is very much like Linux, say BSD UNIX or MacOS.  It is basically a house of cards, one tiny problem and nothing works. In theory a distributed system that depends on massage passing should not be so tightly coupled.   I should be able to build a ROS node on using some OS I just invented and plug it into a years old ROS system and it should \u201cjust work\u201d.  Every last bit of ROS should be ablate be build independently and the only interface should be the text of the massages pass.   Itshould not all need to be built in the same work space.", "Perhaps it is a cultural thing and we could correct this by hold a frequent \u201cplug fest\u201d.   This is where people bring devices built using diverse software code bases and test for interoperability.   We see this in other areas why not in robotics", "In my opinion the root cause is over coupling.  entire idea of a \u201cROS work space\u201d is likely the cause of this.", "That said, of course I continue to use ROS, but it is far harder and slower to use then it should be.", "Every last bit of ROS should be ablate be build independently and the only interface should be the text of the massages pass. It should not all need to be built in the same work space.", "A workspace is really nothing more than a directory containing a nr of CMake projects. You should be able to just ", ". There is no requirement for a shared workspace, it\u2019s just convenience.", "In my opinion the root cause is over coupling.  entire idea of a \u201cROS work space\u201d is likely the cause of this.", "Can you give an example of the over-coupling you mention? Big software projects tend to be complex, but if there is some low hanging fruit here that should deserve attention.", "Interesting article, indeed \u2026 also, I agree with most of what people wrote so far. However, I have to disagree with one statement \u201cROS has a steep learning curve / is difficult to learn\u201d. That\u2019s simply wrong from my point of view. Let me tell you why.", "I am working at the University for quite some time now (probably too long ). I watched numerous students come and go. In general, we try to inspire young people to study robotics by giving them the opportunity to play with real  (physical) robots and ultimately implement and integrate their very own small project(s).", "I have never ever had a student that was ", " able to get familiar with ROS. I am ", " saying catkin workspaces are the holy grail. What I like to point out is that the ROS community and tutorials are something that is \u201calready out there\u201d and that is updated regularly \u2014 we, as the \u201cteachers\u201d don\u2019t need to provide all that.", "Moreover, most student projects require a basic software stack that exceeds what can be done by a single student in one term, e.g., writing a camera grabber/driver, image conversion and depth processing, state machine design, you name it. Once a student is familiar with how ROS works, e.g., wrt to workspace layout and compiling/building and deployment, he or she can push the project really fast.", "Of course, we could have come up with our own workspace strategy and use existing (ideally) build tools like CMake (not catkin CMake login), but then diversity kicks in. Some students prefer C/C++, okay no problem using ROS, some prefer Python (more and more students actually prefer Python over C/C++), okay again, they can use the same framework, integrate easily and most importantly they can easily interface.", "To make a point here, IMHO ROS is extremely useful in the education domain.", "Well, is \u201cthe ROS way\u201d is the right way? Probably not (IDK), because as some already said we need to tackle the problems on a lower level. Do we make the situation worse by teaching them \u201cthe wrong way\u201d? IDK. But, as long as it works, and ROS does work on that level, I am happy about it. Maybe this post does not address the original topic (ROS for industrial applications) but I wanted to share these thoughts with you.", "Well we here at Ubiquity Robotics base all our robots around ROS. Our robots will, it seems, be less expensive than the pulurobotics robots and on almost all fronts have greater capability.", "Why did we choose ROS? Well its simply a case of enabling a great user experience and development speed for the user. Before we built ROS based robots we built monolithic designs like the pulurobotics robot. We built an application called party-bot that looks around the room and uses face detection to find people to serve drinks to (see the video on our website ", "). With the monolithic design we had a team of 5 people working on it for several days to get a first prototype. Then it took us ages to tune it and it never really worked very well.", "With the ROS design, it took 1 person 6.5 hours to get to the first prototype. We tuned it in 1.5 hours using the many ROS tools and it worked well fast.", "Yes ROS has a steep learning curve - that\u2019s something we as a community do need to solve. Yes it takes a long time to make all the parts of your robot work properly with ROS - that\u2019s something that is a result of the fact that ROS forces you to think properly about the architecture of your robot.", "To claim though that ROS is too heavy weight, just ain\u2019t so! We too run our entire ROS stack on a raspberry Pi 3. In fact you can download our ROS image for the Raspberry Pi 3 here. ", ". Our software and stack is great for running robots, but also great for just hooking up say a camera or other peripherals. We get terrific performance with ROS on a Raspberry Pi 3, but the best bit is that if the performance isn\u2019t enough its easy to make use of additional compute infrastructure because you can make it all work together with ROS.", "To claim that shoving everything on to one central processor solves all problems with timing ain\u2019t so either nor is it particularly true that you must implement heavy weight timing synchronization mechanisms for every peripheral.", "Personally I love ROS just like I love my car. Now if I am more familiar with a horse and buggy, it might seem a lot easier to use and a reasonable choice, because its true that cars have their own set of problems. However in the end the advantages of my car over more traditional modes of transport are just overwhelming and any problems that currently exist with cars are being worked on by a vast worldwide group of talented engineers. I don\u2019t see that kind of engineering happening with my horse and buggy approach.", "I think that the author has missed an important benefit of ROS for developers of robot applications: the tools.", "The author is building a robot with only concern for the hardware and the low-level embedded software that drives it. This is a use case that ROS is not particularly well suited for. They can be considered correct in arguing that ROS is not for them. (ROS 2 is, or perhaps will be, a different story.)", "But for someone building an application on top of a robot platform, ROS offers something very important: an integrated tool chain with many different tools that can help you design, introspect and debug your software. This may not be relevant when building the hardware-driving embedded software (or it might be, depending on how you work), but it is very relevant when you are dealing with complex data flows and data-processing or planning algorithms that need debugging but aren\u2019t easy to debug when the output is a wall of numbers. Having tools like rosbag and rviz and rqt_* available can massively reduce the difficulty in developing robot software.", "I\u2019m not with a company, but a comment I have heard several times from people who are is that this is the value they get from ROS. They don\u2019t use it because there are lots of existing nodes available (another comment is often that they don\u2019t want those nodes because they are not reliable enough to go into a product). For them, the value is in the improvements in the robot development process provided by having an integration framework with a wide range of tools available that already work. I would be interested in hearing from the company people here what the most important value proposition of ROS is for them.", "Hi, long time lurker here ", "I think this remark of Chris Albertson about low level hardware control is spot on.", "Allow me to shamelessly plug a project which addresses this issue ", "\nI\u2019ve been participating in another Open Source project called Machinekit, which is fills the gap of low level hardware control.", "\nMachinekit is a real-time motion/io control stack. Forked from LinuxCNC a few years ago. It can be used in more applications than CNC control though.", "The gem in Machinekit and LinuxCNC is the HAL layer, the Hardware Abstraction layer. Developing a system is done by configuring it, instead of programming/compiling. One basically wires components together and puts these functions on an execution thread.", "\nThe realtime thread typically has a cycle time of 1ms, but can be faster/slower depending on hardware.", "\nThis means that interfacing with a DC motor, or a stepper motor does not change the system other than choosing a different component and hardware.", "\nOne can change the running realtime system on the fly by adding/removing components and (re)wiring them.", "\nThere\u2019s also are C/C++/Python API so one can interface from a (userland) application with the realtime HAL.", "Machinekit runs on linux platforms, typically Debian, with ARM/x86/x64 hardware, an example would be a Beaglebone Black where the PRU\u2019s do the realtime tasks, a PC with Mesanet PCI FPGA card and daughterboard, or a De0 nano SOC which includes the PFGA. The Mesanet firmware (which is very stable) also runs on this. Making re-use of the industrial Mesanet daughterboards possible.", "So instead of doing path planning in realtime, we can do \u201coff line\" planning with ROS (ROS does the path planning) and we take this trajectory as input.", "\nAn example (prototype, proof of concept) is here: The trajectory is put into a HAL ringbuffer by a ROS node written in Python. Machinekit HAL components then read from the ringbuffer, interpolates the segments and gets the motors moving.", "\nvideo:", "We would love to one day have a generic ros_control node to interface with the HAL layer.", "It can be used in more applications than CNC control though.", "I\u2019ve heard of Machinekit a couple of times, but never seen people outside the industrial community use it. Has anybody used machinekit to drive, say, a mobile robot\u2019s wheels, including Odometry and IMU integration?", "In my time managing a team that used ROS for a large European research project and my time with ROS at home, I think the hardest part of ROS is the steep curve offered by the dependency and build system - make, cmake, gcc, et al\u2026", "I offered a tutorial on ROS once to a couple of Java developers and they stopped speaking to me thinking I\u2019d take them away from their murky maven builds and drown them in make files! ", "While the existing tutorials makes it easy to setup from apt and get going, the hard part comes when you have to build topics with covariance, time synchronization and all the other good stuff to make your robot work.  For example, I remember struggling to ENU transform my IMU for robot_localization and I just couldnt find a location with a good description ", " even on Answers.", "Still I\u2019m a big believer in ROS and experimenting with ROS2 where many of QoS limitations are taken out. I think its the only way of prototyping robots today despite the learning curve. Building monolithic applications is so 2008 but a good option still if you want to be like Golem from the Lord of the Rings ", "Has anybody used machinekit to drive, say, a mobile robot\u2019s wheels, including Odometry and IMU integration?", "Not that i\u2019m aware of. This would typically be a setup where one would configure Machinekit to drive motors (with or without encoder feedback) and read sensor data. Then get the motor feedback and sensor data back to ROS via topics.", "\nDepending on realtime constraints / hardware one could write a component for fast control (in MK, not in ROS), and publish info back to ROS.", "Driving the motors is done by dedicated micro-controllers, at least in the systems I work with. Simple ones take velocities and perform PID control, better ones do Model-Predictive Control (MPC). Often, IMUs are also often directly attached, as they are cheap enough for a while now to be just integrated on the board. Of course, for MPC and for IMU integration some configuration is necessary, but not a lot.", "I\u2019m not sure what role MachineKit would play in such a setup. Would you use MachineKit to generate the software running on the micro-controller?", "What about more complex systems, e.g, whole body motion as in humanoid robots? In order to achieve this, the motors need to be synchronized (I assume), this would require lots of configuration I guess?", "I\u2019m not sure what role MachineKit would play in such a setup. Would you use MachineKit to generate the software running on the micro-controller?", "Machinekit would either have software step generation (with RT-PREEMPT) kernel, or depending on platform the hardware taking care of this by PRU (microprocessor) on a Beaglebone, FPGA on a mesanet pci card / De0 nano soc.", "In order to achieve this, the motors need to be synchronized (I assume), this would require lots of configuration I guess?", "As long as you have the motors driven from the same board this is not really a big thing because you\u2019d have the interpolation done by a HAL component. Then the HAL component sets the velocities, etc. Where movement (rough interpolation) is planned by ROS.", "\nSynchronized moves from different Boards is not available (not yet anyway; execution of the HAL function thread needs to be done from an external source (like a hardware interrupt from fpga), there\u2019s been a bit of experimentation with NTP server in the past, but I don\u2019t know the details)", "software step generation", "I\u2019m not sure what you mean by \u201cstep generation\u201d. The motors we use can either be driven by Pulse-Width Modulation (PWM), and most micro-controllers for this purpose do that in hardware based upon a defined level.", "Given that the exact API on how to set the PWM is MCU specific, I wonder how \u201cgeneric\u201d MachineKit would be. Also, given that the code for this is largely trivial and consists out of writing an input value to the right output register, I wonder what benefit is to be gained from MachineKit.", "I\u2019m not sure what you mean by \u201cstep generation\u201d.", "That\u2019s an example of a component for a step generator for a stepper motor. You\u2019d set velocity or position and stop caring about generating steps (let the hardware deal with that), or use PWM, or getting a velocity/position from encoder for that matter.", "I wonder what benefit is to be gained from MachineKit.", " may u suggest me a way to interface ROS with Arduino so that i can make a robot with my own .", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["re-use of a configuration (read control system) on different hardware", "use components (filters, PID etc) to configure your realtime control behaviour", "defined interaction between realtime and non realtime"], "url": "https://discourse.ros.org/t/why-dont-we-use-ros/3161"},
{"title": "Self-Driving Development Platform Hacking Based On ROS\uff0dFor Engineers and Hackers", "thread_contents": ["\nOpen Source Self-Driving Development Platform Hacking", "\nFIVE Days/ TEN Engineers/ TWO Groups/TWO-mile Park Road Test/TWO Self-driving Vehicles", ":", " is a five-day hands-on workshop during which ten talented engineers (divided into two groups) worldwide collaborate with each other to build an affordable (within $10K excluding various sensors) drive-by-wire development platform for self-driving.", "On the last day, there will be a small and fun race between the two groups. The whole hackathon process and outputs will be open to the public in various ways.", "Meanwhile, the most special and unique experiences are created for global engineers: collaborative development, fabrication workshop, social parties, keynote speech, mentor communication and local exploration etc\u2026 You are bound to get inspired.", "Hacking of car control, both lateral and longitudinal controls, to achieve drive-by-wire", "Self-driving setup and test of 1/16 scale car powered by Raspberry Pi", "Installation and test run of self-driving software Autoware on full-sized car", "\nC-Zone Hackspace, Guiyang, Guizhou, China", "\nMarch 9th to March 13th 2018 (Five days)", "The ultimate goal of Move-It Hackathon is to co-create an open, democratized, affordable and reliable by-wire control development platform for self-driving. More details can be found ", "We\u2019ll make full arrangement for the development components & accessories stuff, also free accommodation & foods, and reimbursement for transportation. Check Move-it FAQ here ", "Surprise and prize for ", " participants!", "\nContact: Email to ", " or call at 0086.18111991219 for any questions or concerns", "Apply Move-it Hackathon ", "This sounds like a cool thing, but does it have anything to do with ", "? If not, I\u2019d really recommend you change the name of the hack-a-thon, since it\u2019s likely to confuse ROS users.", "Hi. Thanks for pointing that out and I really appreciate it. The hackathon is not related to MoveIt! and the name is a coincidence ", " The hackathon aims to hack an affordable development platform for self-driving. I\u2019m so sorry for any confusion", "That really confused me also ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/self-driving-development-platform-hacking-based-on-ros-for-engineers-and-hackers/3716"},
{"title": "Announcing the Hardware Robot Information Model (HRIM)", "thread_contents": ["Hello everyone,", "It\u2019s a pleasure for me to announce the release of the ", " version of the Hardware Robot Information Model (HRIM) available at ", ".", "Briefly,", "\nHRIM is a common interface that facilitates interoperability among different vendors of robot hardware components with the purpose of building robot modules and thereby, modular robots. HRIM focuses on the standardization of the logical interfaces between robot modules, designing a set of rules that each device has to meet in order to achieve interoperability. It tackles the problem of incompatibility between robot components that hinder the reconfigurability and flexibility demanded by the robotics industry. In a nutshell, HRIM presents a model to create plug-and-play robot hardware components. HRIM builds upon the ROS component model and although we envision its expansion to support other framework alternatives, currently a ROS 2.0 implementation is available at ", ".", "In short, the robot modules have been classified in 6 (actually, there\u2019s a seventh, composites that is a work in progress) types of modules which correspond to the task they can perform: sensing, actuation, communication, cognition, user interfaces or power. Each type is composed by sub-types or devices, related to the functionality of the component. For example, a camera is a sub-type of the sensor type. The following image pictures the HRIM component model for each device (or sub-type):", "HRIM has been presented to national experts at the International Standardization Organization (ISO) within the scope of the standardization in the field of robotics, excluding toys and military applications (ISO/TC 299). Particularly, it has been introduced within the ongoing standard ISO/CD 22166\u20131 which treats modularity for service robots.", "\n", "\n", "In addition, Fraunhofer IPA \u2014 the largest research organization for applied research in Europe \u2014 is currently looking at HRIM to adopt it and extend it for several projects. Mirko Bordignon, group manager at Fraunhofer IPA said:", "After years of experience designing, developing, and deploying software on robotic systems, we fully subscribe to the objectives stated in the HRIM manifesto: working towards true \u201cplug and play\u201d hardware modules through a standardized information model, which merges the inputs and feedback gathered from open-source communities with the stability and platform-independence required by standardization bodies. We look forward to contribute our experience to further advancing HRIM towards this goal!", "Similarly, HRIM picked the interest of the ROS-Industrial (ROS-I) Europe consortium during the last ROS-I conference where HRIM was introduced.", "Feedback, criticism and contributions are more than welcome. A complete writeup and description of the release is available at ", ".", "Regards,", "Great work, Victor and everyone working with you! I hope this is adopted widely!", "I have some questions about your relationship with standards.", "Hello ", "!", "How exactly are you compatible with the (draft) ISO 22166-1?", "I/we don\u2019t claim that HRIM is compatible with the draft of ISO 22166-1. Doing so would be incorrect as the ongoing standard covers (or that\u2019s my hope for its final shape) much, much more than a model. As far as I know, the existing draft does not even propose a model and the closest thing included is a pointer to HAL4RT (more about this below).", "What we claim is that HRIM has been ", " and ", " to the experts pushing the ongoing standard. We received feedback already from several of those experts and our hope is that we can inspire some additional content on the working document which currently, IMHO, lacks of software abstractions that facilitate interoperability.", "How are you compatible with HAL4RT? There are two versions of that specification proposed; the first has died and the second was rejected because it doesn\u2019t contain anything of use to robotics.", "Although what you say is right and HAL4RT 2.0 isn\u2019t still official (AFAIK JASA will revise and submit the new version to OMG in June this year but you probably know it better than I do), we based ourselves on the latest draft of HAL4RT 2.0 available (verified just a few days ago with Kenichi Nakamura when he facilitated the last version after I introduced HRIM to him (leading HAL4RT)). We claim we conform with the statements indicated in this last draft and which refer to specific sections within HAL4RT 2.0 document. The conformance of each one of this sections is discussed in Appendix I at ", ".", "Taking into account these aspects, we believe that claiming conformance is valid but probably, stating that we\u2019re on the path to conform (formally) with HAL4RT 2.0 is more correct since as you point out, (HAL4RT 2.0) isn\u2019t official yet.", "The proposed APIs are also not compatible with ROS.", "I\u2019m not really sure we agree on this. Could you mention which sections make you think this way?", "How do you support OpenEL 3.0, which is a private commercial project (not an international standard)? Do you provide an implementation of its APIs? They don\u2019t seem compatible with ROS, to me.", "This is likely the claim I can defend the least since most of the information we found is in Japanese and it was a tremendously hard task to understand it right. In fact, although we looked into it, we were not going to mention anything about it however, a few days ago I got the following from Kenichi:", "At this time, OpenEL 3.0 and HAL4RT 2.0 are almost the same.", "\nSo there is no document for OpenEL 3.0 in English now.", "Everything we saw (and understood), make us think this way as well so on this basis, we included that sentence.", "What we claim is that HRIM has been presented and introduced to the experts pushing the ongoing standard. We received feedback already from several of those experts and our hope is that we can inspire some additional content on the working document which currently, IMHO, lacks of software abstractions that facilitate interoperability.", "Sorry, I misread your post.", "Unfortunately, based on this week I don\u2019t think that 22166-1 is going to go in the direction you and I desire.", "Taking into account these aspects, we believe that claiming conformance is valid but probably, stating that we\u2019re on the path to conform (formally) with HAL4RT 2.0 is more correct since as you point out, (HAL4RT 2.0) isn\u2019t official yet.", "I\u2019m not really sure we agree on this. Could you mention which sections make you think this way?", "OK, after reading that document I understand how you are looking at HAL4RT. I looked at the interfaces you have and I do not think you are correct to claim conformance with HAL4RT 2.0. You can claim that you correlate on capabilities with HAL4RT 2.0, but to conform you would need to have identical interfaces, which you do not. You have similar capabilities (as well as significantly extended capabilities) but you do not match the model defined by HAL4RT 2.0. Which I think is a good thing because your model is significantly better for our needs, in my opinion.", "This is likely the claim I can defend the least since most of the information we found is in Japanese and it was a tremendously hard task to understand it right. In fact, although we looked into it, we were not going to mention anything about it however, a few days ago I got the following from Kenichi:", "At this time, OpenEL 3.0 and HAL4RT 2.0 are almost the same.", "So there is no document for OpenEL 3.0 in English now.", "Everything we saw (and understood), make us think this way as well so on this basis, we included that sentence.", "OK, on that basis if you feel confident then you could claim correlation with OpenEL 3.0 (I wouldn\u2019t claim compliance with something I hadn\u2019t seen; \u201calmost the same\u201d is not the same as \u201cidentical\u201d). I wouldn\u2019t claim it\u2019s an international standard, though.", "If you want help with the Japanese, feel free to ask. ", "For completeness, I\u2019m leaving here a short comment about our updates:", "although we will keep in touch with the groups behind HAL4RT and OpenEL efforts, after an offline discussion, Iweve made an update to the original announcement and crossed over the HAL4RT and OpenEL claims until we clarify exactly the status of those documents/efforts.", "A new version of the paper has been submitted to arXiv and should be available tomorrow. Thanks ", " for the support.", "Needless to say, HRIM will keep growing as we get more and more contributors, supporters and (hardware) devices captured in the model.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["How exactly are you compatible with the (draft) ISO 22166-1?", "How are you compatible with HAL4RT? There are two versions of that specification proposed; the first has died and the second was rejected because it doesn\u2019t contain anything of use to robotics. The proposed APIs are also not compatible with ROS.", "How do you support OpenEL 3.0, which is a private commercial project (not an international standard)? Do you provide an implementation of its APIs? They don\u2019t seem compatible with ROS, to me."], "url": "https://discourse.ros.org/t/announcing-the-hardware-robot-information-model-hrim/3906"},
{"title": "ROS in art installations/projects", "thread_contents": ["I\u2019d love to see a list of different art projects that have used/are using ROS, so there\u2019s something to point artists to if they\u2019re looking to get started on anything remotely ROS-related.", "If there\u2019s already a list of these projects somewhere that I missed, please point me to it!", "Here are some I\u2019m aware of already:", "I\u2019m sure there must be many more, spread throughout the internet, difficult to uncover with search keywords unless you have heard of them already\u2026", "Any ideas are welcome!", "Theatre with PR2s", "I also had some point cloud based pieces accepted to an ", " ", "Check out ", " and ", " from the ", " article", "I was the robotics lead on Musings on a Glass Box. We had 4 robotic buckets powered by ROS, navigation around a room filled with actuated water dripping systems and some custom computer-vision boxes for navigation and tracking.", "These are fantastic! Thanks for the pointers ", " and ", "!", "Theatre with PR2s", "I ought to apologise to my previous supervisor ", " for forgetting to list Roboscopie first up ", " Isn\u2019t it an interesting perspective that a lot of the time just what we consider \u201cthe norm\u201d in robotics looks whacky enough to be considered as art\u2026 e.g. the overlay of the ontology in Roboscopie, and the point clouds that you showed ", " .", "The level of close-contact interaction with the robot in Sky Sky Sky (e.g. the hug) would probably surprise many people. I won\u2019t try to be an art critic here, so I\u2019ll just throw in the word \u201cjuxtaposition\u201d and be done\u2026", "I was the robotics lead on Musings on a Glass Box. We had 4 robotic buckets powered by ROS, navigation around a room filled with actuated water dripping systems and some custom computer-vision boxes for navigation and tracking.", "Hah, I would have loved to have seen those buckets roaming around to \u201ccatch\u201d the water droplets: it\u2019s a clever idea!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " (I\u2019ve been told they used ROS then)", "Potentially an entry to ", " or ", ", though I haven\u2019t seen any mention ROS", "\n", " (mine)"], "url": "https://discourse.ros.org/t/ros-in-art-installations-projects/4923"},
{"title": "2019 ROS COURSE Ultimate Guide : Implement ROS on all Robots", "thread_contents": ["I have been working on robots from couple of years and after graduation I made a course that contains all the ROS essential you need to work using ROS.", "This course is not just simulation ", " I will be Using Cheap Robots (any one can buy) to be controlled by ROS , including raspberry PI , esp-8266 and 2 wheel plastic car with sensors", "After this course you can work with ROS so easily and will start making your upcoming robotics projects powered with ROS.", "packages used are", "\n-rviz", "\n-gazeebo", "\n-turtlebot3", "Find Course on this -->  ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/2019-ros-course-ultimate-guide-implement-ros-on-all-robots/8342"},
{"title": "ROSCon 2018 Informal Meetings of Special Interest Groups", "thread_contents": ["Ahead of ", ", we\u2019re inviting organizers of informal meetings for special interest groups to post in this thread for attendees to see. Note that we don\u2019t have dedicated birds of a feather sessions in the schedule this year, but there are times in the schedule where informal meetings will fit in nicely such as the conference coffee breaks, pre-conference breakfasts, post-conference dinners, etc.", "Template for posts (the \"how to find us\" field can be updated on the day by editing your post):", " of the conference venue can be used for planning the meeting locations. Spaces on the floorplan that are not colored are not reserved for ROSCon\u2019s use.", " 10:30am morning coffee break Day 1, Sept 29th.", "\n", " Grab coffee and meet back near the conference registration desk (", " the \u2018reception\u2019 desk in Europa).", "\n", " Look for a person with a red bun atop her head.", " 16:05 afternoon coffee break Day 1, Sept 29th", " Climb the stairs with your coffee and let\u2019s meet at the lunch area on the second floor (Florencia). (Not sure if outside of lunch-time we are allowed in, worst case you can find us standing outside.)", " Look for a group with a bearded, ponytail guy with a your favourite ", " ROS Hydro t-shirt", " Bring your contribution ideas, requirements and let\u2019s draft up a roadmap!", " 12:30 ", " Lunch, Day 2, Sept 30th", "\n", " FLORENCIA (Lunch Room A)", "\n", " Look for our table(s) where I\u2019ll be wearing a green Ivy cap, and a tan brown SROS shirt.", "\n", " Bring your ideas, concerns, question after having pondered over the security talks from Day 1!", "\n", " IROS 2018 | Mo-TUT-4: Securing Robotics with SROS2 (PM)", "\n", "\n", " 12:30 lunch break Day 1. Sept 29th", " Second floor at the entrance of the lunch area (Florencia),", " Look for the Brazilian t-shirt ", " Day 1, Saturday Sept. 29 from 13:15 - 13:45", " Booth ", " (ROS-Industrial Consortia) in the exhibition area", " Look for ", " and colleagues.", "\n(1) get update from upcoming events in Europe", "\n(2) watch our ROS-I demo", "\n(4) talk about advance manufacturing/industrial applications", "\n(3) ask your questions", ": Day 1, Saturday Sept 29th during the 10:30-11:00 coffee break", ": PickNik\u2019s ", ": Use the booth map, look for robot arms, or find ", ": Meet fellow developers, bring your ideas, and ask pressing questions", "When: Day 1, Saturday during afternoon coffee break", "Where: near registration desk", "How to find us: look for Jihoon ", " near registration desk", "Agenda: rosbridge, robot web tool in general, maintenance, and potentially ros2.", " 12:30 at Lunch, Day 2, Sept 30th", "\n", " FLORENCIA (Lunch Room A)", "\n", " Look for a guy wearing a C-turtle t-shirt with a big scarf and the same guy from the ros_control meetup.", "\n", " We are looking for a friendly chat with fellow AV developers and those interested. What is it you\u2019d use ROS for (and which flavour) and what is it you think could be improved on? What do you find challenging with this field? Do you think the future of AV lies in proprietary software or should be similarly opened up like many of the related research topics are?", "\nEveryone is welcome, we only ask to leave your competitive hats at the hotel for this meetup ", " Day 1, Saturday during the morning coffee break", " Erle Robotics stand.", " In front of to the entrance of the Europa Room (Exhibitor Hall)", "\n", " Modularity and system integration in robotics", ": How can we reproduce robotics results?", " On Sunday, 18:30 after the always funny closing remarks by ", " At The Construct booth ", " just go to the booth n.17", " we are going to discuss about how can we have an environment for reproducing robotics results and benchmark them. Questions to answer:", " we will record the whole audio discussion and publish it on the ", ". I will act as moderator to ensure that we deal with all the points.", " Day 1, Saturday, Sept 29th during 13:20-13:40 Lunch break", " ", " Use the booth map or find the TurtleBot3 at the exhibition area", "\n(1) ", "\n(2) In celebration of the ", ", 100 sets will be distributed free of charge at ROSCon2018", "\n(3) ROS2 demo using TurtleBot3 and XEL Network", "\n(4) Open discussion (e.g. ros2arduino, new ideas, roadmap, any questions)", "  Day 1, Saturday Sept. 29 from 10:30 - 11:00", "  Booth ", " (ROS-Industrial Consortia) in the exhibition area", "  Look for ", " and colleagues.", " 12:30 lunch day 1 (or any other time, really)", " FLORENCIA (Lunch Room A)", " Look for folks wearing bright orange Ubuntu lanyards. I\u2019m afraid I don\u2019t quite have the distinctive beard I previously had. Don\u2019t worry, it\u2019ll come back.", " I\u2019m one of the primary snapcraft maintainers, and other engineers from Canonical are here as well. We\u2019d love to talk about your experience creating ROS snaps, answer any of your questions, and provide any assistance we can!", "  30th Sept at 11:50", "\n", " Florence room", "\n", " Look for the \u201cARM\u201d sign (probably hand written).", "\n", "Because the afternoon coffee break was cut short we\u2019re going to have the meetup this evening at the reception.", "When: At the start of the reception after the group photo (Scheduled 18:10)", "\nWhere: Near the registration desk.", "\nHow to find us: Look for Jihoon ", " nearby.", "\nAgenda: rosbridge, robot web tools in general, maintenance, and potentially ros2.", "Several groups are working on integrating micro controllers better with ROS2. Let\u2019s meet to discuss next steps and joint activities!", "Sunday the 30th at lunch (13:20).", "Upstairs lunch area.", "We\u2019ll try to grab a table right at the entrance and put a sign up.", " Sunday Day 2 1605 (just after lightning talks)", " Overflow viewing area (Madrid 4)", " Steve Peters and Jose Rivero will have a table set up", " Day 2 morning break", " In or around the coffee area near the lifts.", " Look for Geoff Biggs with the messy hair.", "Notes from the Gazebo Interest Group, my apologies for any errors:", "Attendees:", "Andrew Symington ", "Carlos Rosales Gallegos ", "Musa Morena Marcusso ", "Ray Cole ", "Robert L\u00f6sch", "Marcus", "Ruffin ", "Plus many Open Robotics people", "Gazebo not shutting down properly", "Sim of wheeled robots driving in mining tunnels, but the simulation is slow and he doesn\u2019t know why", "Had a problem with robot not falling after attaching robot arm with position controller", "How to learn more about gazebo_ros", "How to unthrottle simulation that could go faster than real-time", "Is the CreateLink function working? (written by ", ", not sure who asked this)", "Spawning models:", "While running lots of simulations, sometimes the simulation fails with a GLX-related error", "FBX support", "Modeling sea-floor", "VisualPlugins", "Camera following", "Camera Z-up", "Domain randomization", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Robot component incompatibility: vendor lock-in.", "Common infrastructure: ", " and ", ".", "Open discussion.", "Is ROS a good framework for reproducibility of robotics results?", "Should reproducibility be based on bags, simulations or real robots?", "Should we concentrate on reproducibility for a very specific robotics problem first, and then attack all other problems, or should we generate a common framework for any type of robotics problems?", "How can we learn about reproducibility in other domains of science?", "How to apply for ", "\n", "Is my project eligible for ROSIN funding?", "Find out what are the main issues for people cross-compiling to Arm", "Explain our proposal to move the CMake toolchain files into the official ROS2 repos and have it as a first citizen in the ROS project (instead of Arm trying to catch-up with broken builds).", "Fixed recently in 9.4.1", "\n", ": flamegraphs?", "This was fixed in gazebo9\n", "\n", "Workaround: use effort controller instead of position controller", "\n", " helped answer this", "real_time_update_rate parameter: modify in sdf world, gz tool or gui\n", "Set from GUI: ", "\n", "\n", "ARIAC example for setting in world fie: ", "\n", "From command-line: ", "\n", "Race conditions if one needs to be spawned before the other (such as a free-floating robot on a granite table, want them to be spawned dynamically so they can be written in urdf)\n", "There is a spawn_model service in gazebo_ros, but it returns too fast", "Shane discussed a special xml tag to insert an SDF", "\n", "The spawn_model service should use quaternions instead of roll-pitch-yaw", "If it\u2019s just gzserver and it doesn\u2019t use rendering, these errors can be avoided by disabling rendering in gzserver (it will also speed it up slightly) by using a bad DISPLAY variable or running inside a docker container that doesn\u2019t have nvidia-docker enabled", "There are lots of existing models on sketchfab, often in FBX format. Bosch is interested in being able to parse FBX models.\n", "Ian has worked on this, but it\u2019s proprietary and complicated. We may come back to it if there is sufficient interest and support.", "\n", "Current workflow is to bake lots of textures into a single mesh, but it\u2019s hard to export from blender.\n", "Is the sea-floor like a DEM? There are some performance advantages to using DEM\u2019s.", "\n", "Underwater plumes are visualized in RVIZ with markers, but want to visualize in gzclient too. Is it possible?\n", "It\u2019s easier with markers. (I didn\u2019t get all of Louise\u2019s answer here)", "\n", "Some sequence of moving camera and then follow an object caused camera to go a bit crazy", "Some people need to use Z-down, so it\u2019s hard to make the camera point upside down. Andrew Symington would like to turn off the camera auto-flip", "Would like to make lots of different environments with variations in which models are present, where they are located, etc. Current approach is a script that prints out different versions of world files.\n", "Embedded scripting inside a world file was pretty useful in the Space Robotics Challenge for randomizing the world. It uses embedded ruby with snippets", "Template: ", "\n", "Instance: ", "\n", "\n"], "url": "https://discourse.ros.org/t/roscon-2018-informal-meetings-of-special-interest-groups/6151"},
{"title": "Fiducial Marker Based Localization System - Package Annoucement", "thread_contents": ["Hello Fellow ROS Users and Developers,", "We are excited to announce our fiducial based localization system ", ".", "We love current LIDAR based localization methods, however they require expensive LIDAR for good results. LIDAR methods are also subject to the \u201ckidnapped robot problem\u201d which is the inability to unambiguously localize ", " in spaces which have a similar layout (e.g. if you move your robot to one of many similar offices it will get lost). Common LIDAR localization packages like amcl need to be initialized with a pose estimate on every run, something that can be difficult to do accurately. LIDAR based methods can also be difficult to tune and set up.", "Our fiducial localization system enables a robot with a camera to engage in robust unequivocal localization based on pre-placed fiducial markers. The node simultaneously maps and localizes with these markers, and is robust against movements of single fiducials. This robustness is due to the fact that it continuously recomputes both the map of fiducials and the error associated with each fiducial. It then computes the reliability of each fiducial based on the estimate error of each fiducial. The required sensor is inexpensive and the method is relatively simple to set up. We use the Raspberry Pi Camera V2 ($25), but any calibrated camera with a ROS driver will work.", "Here is a screenshot of rviz visualizing the fiducial map:", "\n", "This localization method may be used stand-alone or it can be used as a compliment to more traditional LIDAR methods to create unambiguous localization at all times, using a system like ", ".", "For creating and detecting fiducial markers we use OpenCV\u2019s ArUco module.", "\nMore about operation and usage can be found on the ", "Have an issue, or an idea for improvement? Open an issue or PR on the ", ".", "This package will be part of the robots that we will release via crowdfunding on Indiegogo at 1 minute past midnight EST on March 10th 2018 (less than 2 weeks from now).", "The Ubiquity Robotics Team", "\n", "Hi guys, looks like you\u2019ve done some great work.", "I haven\u2019t had a chance to go through your repository, but you might be interested in some work I did a little while back (", "). More of a quick hack, but my method uses a refining tree structure to get a precise camera pose estimate across the whole map.", "Looking forward to a chance to see how you approached the problem!", "Looks like a nice package! RViz markers are always a great addition.", "Have you quantified the position accuracy with the Pi camera? Presumably it depends on how many markers are visible, but are we talking cm or mm here? One other thing, the convention for visible/invisible markers seems flipped to me - I would think that visible markers should be green in RViz, and out-of-frame ones red.", "I can see a lot of uses cases for this. I\u2019m excited to use it for quickly calibrating a manipulator to an environment.", "Cool to see others working on similar systems.", "Taking a quick look at your repo, I don\u2019t see a License file, what is the code licensed under?", "For the Pi camera with 14cm fiducials mounted on the ceiling with a ground based robot, in a well lit room we get position data that is good down to a couple cm. The biggest problem is noise in the pose estimates of single fiducials, as a couple pixels of noise can change the angular estimate wildly.", "We have a method in the works that instead of doing individual marker pose estimates, does a more sophisticated approach using a single pose estimate on all the detected marker vertices. We want to have the data set and testing to measure if it is actually an improvement before merging though.", "Rohan", "You were right about the license, I don\u2019t get too many people looking at the work, so it was never much of a concern. I just updated it with a GNU Public License, which seems about right, although the commercial part never really was my forte.", "Either way, if you\u2019re interested in my work, feel free to contact my directly. I don\u2019t want to hijack your thread any more than I have.", "I had a little bit more of a look through your code, but it was a bit rushed so forgive me if I missed it, do you make any assumptions about marker placement? I found that it really helped my estimations during the marker pose estimate step to have an option to allow aligning the marker with a plane.", "As a lot of the use cases for your markers may be on a single ceiling of constant height, it\u2019s relatively easy to project the marker estimate from the camera such that you can take the pose at the intersection with the common plane. Not sure how the SLAM methods work with this, but it may be worth considering if you haven\u2019t already!", "We do not make any assumptions about marker placement. We have a \u201c2d\u201d mode that assumes that the robot is on a flat plane, and only moves in linear x, y and rotational z.", "We used to use a system that made a lot more assumptions, such as a constant height flat ceiling, but minor variations from this assumption caused the pose estimations to be unstable, and going to 6DOF solved this issue, as well as providing much more flexibility in marker placement (allowing for sloped ceilings, and even placing markers on walls).", "Yes we have characterized the accuracy as Rohan says in normal conditions its good to a couple cm. Our system dynamically computes the accuracy based on the accuracy of all the markers it can see and combines the poses from all the visible markers in making its estimate. If you increase the density of markers the accuracy will improve but it is a n^0.5 process. Moving markers closer to the robot will also increase accuracy as will increasing their size.", "Its important to note that the size of the fiducial can be determined both in principle and practice to less than 1 pixel, based on fitting lines to the edge of the aruco marker square, a pattern that crosses many pixels. Signal to noise and sensible lighting is important to get decent results here.", "The results that we are getting are good for most of the applications we use, that doesn\u2019t mean we aren\u2019t going to continue to improve it. The things that are on the list of proposed improvements include:", "We will of course continue to work on refining this package, but its always great to get the input, ideas and most importantly code commits from the broader community to make this software better.", "First of all: great initiative, and I\u2019d be happy to try this implementation.", "\nI\u2019ve had a short look through your code, so please correct me if I missed pieces.", "\nIs it right that, once you\u2019ve seen a marker, its position will not be updated again?", "\nThe fiducial marker SLAM problem strikes me as a perfect case for Graph SLAM. Is there any reason you are not  using this?", "Great question.", "The positions are continuously updated based on the estimated error of the measurement, as well as the error estimate of the current position estimate of the marker.The position error estimate is based on the estimated error of previous measurements, and the errors on the other markers that are visible at the same time.", "I believe that this approach is similar to many graph SLAM methods, except we are not currently doing any global error minimization at the moment.", "We would like to add bundle adjustment, or a similar global optimization system to both improve the mapping and even adjust the camera calibration to reduce the error in measurement, but we haven\u2019t been able to put the time necessary into it yet. (We do accept PRs if you want to add this ", ")", "Rohan", "\nUbiquity Robotics", "\n", "How would one give the known pose of Aruco markers in an existing map and then use the fiducials localization system to localize the robot?", "Background: We have an existing map (pgm) of our real-world environment.  We would like to position specific Aruco markers in known locations in the real-world, record their poses relative to the map coordinates, and then somehow use the fiducials localization node to localize in the map.", "Probably the best approach is to place the robot in a known location with a known pose (the easiest would be the default starting location of 0,0,0) tell the algorithm that pose (or in the case of the robot being in the default position do nothing) and allow it to build the map.", "Guys, I am new to ROS and fiducial marker tracking. I need a help. When tracking the fiducial, if I move the marker faster then the 3D visualization shows lag and the does not show the continuous movement of marker instead it kinds of jumps and apears when the marker come to static. I will really appreciate if you can help me solve this issue. Thanks,", "Hi ", "I would love to help, but please ask either by creating a issue here ", " or on the Ubiquity Forums here ", ".", "The ROS Discourse is for general announcements or discussion only.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Estimate pose from all the fiducials simultaneously rather than independently computing pose and combining them (as already discussed by Rohan)", "Automatically optimizing camera exposure parameters to maximize accuracy", "Automatically optimizing aruco parameters to maximize accuracy given computational constraints (you can improve accuracy by doing more fitting computations at the expense of computer cycles)", "Automatically optimizing camera calibration data based on the large amount of data that\u2019s available from looking at many fiducials - each fiducial is a bit like a checker board (calibration board) so you should be able to get better results from driving around and looking at many fiducials than from the relatively small amount of data available from a normal camera calibration. This is actually really important for a whole host of reasons not least that manufacturing variances between robots can get frictionlessly calibrated away with the user not having to do anything other than navigate and drive. The improvement also spills over in to other uses of the camera.", "and many many more\u2026"], "url": "https://discourse.ros.org/t/fiducial-marker-based-localization-system-package-annoucement/4050"},
{"title": "ROSIN: Call for Education Projects (EPs) - Deadline: September 14, 2018", "thread_contents": ["The EU H2020 ", " project has the goal to advance open-source robot software for industry and the robotics community as a whole. This includes also to improve educational activities about ", ".", "Similar to the already announced Focused Technical Projects (", "), the ROSIN project grants education activities, so-called Education Projects (EPs), conducted by third-parties (non-consortium members).", "You identify a particular project that you want to support the ROSIN educational activities with and outline a sound and sustainable plan to achieve it. If successfully evaluated, you receive 50% of the total cost to perform the work, provided that you commit to sustain the remaining 50%.", "You can apply at any time, there are 3 cut-off dates per year.", "To find out more about what is being funded and how to apply, check the Education Project section at the ROSIN website: ", "The next cut-off date:     September 14, 2018", "The next cut-off date: September 14, 2018", "That is in exactly 1 month. You may consider this as a friendly reminder to get ready to hand in your proposal of Education Projects in due time.", "Many thanks", "\nThilo", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosin-call-for-education-projects-eps-deadline-september-14-2018/5448"},
{"title": "Discussion on ROS to ROS2 transition plan", "thread_contents": ["All,", "\nI\u2019d like to start a discussion on the ROS to ROS2 transition plan. I know there are ", " users of ROS today, and that ROS2 is not yet fully featured, but I believe we need a plan to get the community to transition on to ROS2 as soon as possible. I believe that ROS2 can and will be better than ROS in the long run, and the sooner we get the community support in getting it ready the better.", "There\u2019s also this: ", "Python 2.7 is going to EOL in 2020, and there are ROS dependencies on Python 2 that would need to be resolved in order to do a 2020 release on Ubuntu 20.04 and other OS\u2019s. That could be a lot of work, that in my opinion, would be better spent on improving ROS2 and porting more packages to ROS2.", "Given that ROS Melodic was just released, and that OSRF is committed to supporting it for 5 years, until June of 2023, it seems we have 4.5 years to move to ROS2, even if another ROS release doesn\u2019t happen in 2020.", "I also realize that there is a planned 2019 release - \u2018Noetic\u2019. Since that isn\u2019t on a Ubuntu LTS, I\u2019m not sure how many in the community will move to that, but that could be discussed also.", "So, I\u2019d like to know other users thoughts. Is 2023 enough time to move fully to ROS2? Do we need a LTS release in 2020? ", "We would need to work together as a community to improve ROS2 to the point that it is ready. I believe we can make that happen soon.", "I\u2019m also in favour of forgoing the 2020 release and putting the effort into ROS2. However, there are certain projects that must be available on ROS2 for it to be viable to consider moving for many people. Not least of which are navigation (which the OP ", " is working on), MoveIt, OpenCV and PCL (or equivalent).", "Jumping ship and burning bridges has never been a great recipe when you are dragging a lot of weight (community) with you.", "I\u2019d prefer ROS releases transitioning  towards ROS2 which of course assumes stable apis/concepts on that end. ROS2 IMHO is potentially facing the challenge of having fragmented communities along the lines of OS-s.", "ROS\u2019s biggest asset has always been, and will always will be the community. Keep that in mind and don\u2019t break it.", "I\u2019d prefer ROS releases transitioning towards ROS2 which of course assumes stable apis/concepts on that end. ROS2 IMHO is potentially facing the challenge of ha fling fragmented communities along the lines of OS-s.", "There has been a lot of discussion about this in the past, and the sort-of consensus was (or at least the decision by OSRF) that from a development and maintenance perspective ", " a fear of breaking ROS1 for ppl currently using it, the only option was a clean slate (see also the end of ", ").", "which of course assumes stable apis/concepts", "Perhaps at a high level we could seek conceptual re-use / transitioning and ", " could help, but not sure whether what you\u2019re suggesting is practical (personally I\u2019d like it too though).", "On Wed, Sep 26, 2018, 06:15 Matt Hansen via ros-users <", "> wrote:", "I\u2019d like to start a discussion on the ROS to ROS2 transition plan. I know there are ", " users of ROS today, and that ROS2 is not yet fully featured, but I believe we need a plan to get the community to transition on to ROS2 as soon as possible. I believe that ROS2 can and will be better than ROS in the long run, and the sooner we get the community support in getting it ready the better.", "One alternative is for the ROS1 community to jointly refuse to migrate to ROS2, keeping current businesses and projects alive and well, with only the Python2 -> Python3 migration costs to pay. Instead of porting any package to ROS2, efforts could be spend on reducing the differences between ROS1 and ROS2 from both ends, to reduce the impact of the community split, and all costs of eventual full migration.", "The longer ROS2 remains in it\u2019s unfinished \u2018non-beta\u2019 release, the more time remains to reduce the migration barrier without splitting the community. So \u2018the sooner, the better\u2019 is not necessarily economically true for the community.", "We should be thinking about this in terms of net new projects. I realize that libraries need to migrate or be redesigned to support new Robotics applications on ROS2 however we shouldn\u2019t be discussing porting Robotics applications from ROS1 to ROS2 generally. Some may choose to do it where it makes sense but I\u2019m guessing its not going to happen broadly.", "If you already have an inflight or working ROS1 application it can, and probably should stay on ROS1 until that application has exhausted its useful lifecycle. Any new or next generation of that application should be targeted to be built on ROS2 and getting the libraries there to support it.", "If we  agree on that as the approach then the question would be: is 2023 enough time to have LTS support on your current robotics project/application and can the community start to move forward with building bridges, tools and porting libraries as you described?", "In my personal opinion, I think it\u2019s time to concentrate on ROS2 rather than managing both ROS1 and ROS2 with fewer resources. I manage over 150 packages of ROS1 Kinetic, but I think it is too hard to manage both ROS1 and ROS2. From the perspective of a company that sells platforms that support ROS, if ROS1 is further supported after Melodic version, it must be mandatory to support ROS1 packages for customer\u2019s request. I agree with the opinion of ", ", I also hope that Melodic version is the last version of ROS1. I think that ROS1 maintainer should give them time to migrate to ROS2 rather than maintain the existing packages for ROS1. If ROS1 is finished with the Melodic version, many maintainers are likely to start working on ROS2.", "I believe it\u2019s ", " too early to start talking about stopping support for ROS1. ROS2 does not yet even have feature parity with ROS1 (actionlib, navigation, you name it). Switching off the lights on ROS1 should be considered after 99% of developers have made the switch, not 5%.", "2023 sounds like a long time, but that doesn\u2019t mean we can afford not to have any releases after melodic. We should continue the usual release cycle until that time at least, because we need new releases to upgrade to new versions of libraries, avoid introducing breaking API changes within a release and so on. Making melodic the last ROS1 release kills ROS ", ", not in 2023.", "Alienating ROS1 users won\u2019t magically turn them into ROS2 users. A monolithic company can simply allocate people to work on project B instead of project A. In an open source community, you have to win people over instead of trying to force them. The latter will simply lead to forks and probably a lot of chaos, confusion and pain for everyone.", "I anticipate a loooong transition period from ROS1 to ROS2. This is why interoperability is key, e.g. it should be made as easy as possible to maintain packages for both in a single repo, with patches being backported between the versions.", "At this point, splitting the community will only harm ROS2. It hasn\u2019t reached a critical mass yet.", "No one is talking about switching off the lights to ROS1 until 2023. The idea is to get 99% of the developers moved well before then.", "I agree with ", ":", "If you already have an inflight or working ROS1 application it can, and probably should stay on ROS1 until that application has exhausted its useful lifecycle. Any new or next generation of that application should be targeted to be built on ROS2 and getting the libraries there to support it.", "If we agree on that as the approach then the question would be: is 2023 enough time to have LTS support on your current robotics project/application and can the community start to move forward with building bridges, tools and porting libraries as you described?", "Declaring Melodic the last ROS1 LTS allows Open Robotics to focus their very limited resources on getting ROS2 to feature complete (Actions, etc) and production quality over the next few releases rather than splitting those resources across projects. Making Melodic the last ROS1 release doesn\u2019t kill ROS now, it kills it in 2023", "As a new robotics company entering the ROS world, having clarity on this sort of point is very important to us.  We\u2019ve decided to go \u201call in\u201d on ROS 2 only.  Anything that we need from ROS 1 that is not being actively ported already is something that we will look at helping with going forward (time and budgets allowing.)", "BTW, we are targeting our first \u201cROS\u201d enabled release for the second half of 2019.", "(I\u2019d like to prefix this by saying I haven\u2019t looked into the ROS bridge recently as we\u2019ve been focused on ROS1 on Windows bring up. If my comments are invalid or otherwise cringeworthy, please disregard.)", "One of the lessons from surviving multiple API transitions in Windows - you will fragment your developer ecosystem if the API set differences are too large. The best API transitions I\u2019ve been through, are the ones where the API transition is gradual. (", " comes to mind)", "Is there opposition to making changes to ROS1 to make it ROS2 aware, in order to make the transition gradual?", "(Admittedly trivial) Examples of these changes:", "Making it so a single workspace can host both ROS1 & ROS2 could help ease the pain. What is the feasibility of making Catkin ", ", and ament ", ", so that you can drop a ros2 node into a catkin build and have it Just Work\u2122?", "Would it be feasible to make roslaunch auto-start the ros bridge when launching a ROS2 node?", "Declaring Melodic the last ROS1 LTS allows Open Robotics to focus their very limited resources on getting ROS2 to feature complete (Actions, etc) and production quality over the next few releases rather than splitting those resources across projects. Making Melodic the last ROS1 release doesn\u2019t kill ROS now, it kills it in 2023", "Well, maybe not now, but in 2019/2020 (when the next normal release / LTS release is due). We need to continue with the regular release schedule past that point for the reasons I outlined in my previous post.", "I fully agree with ", "\u2019s  suggestion to make the API transition gradual in order not to split the developer base. Not out of charity for ROS1, but to give ROS2 a shot at survival; I don\u2019t think it\u2019s a foregone conclusion that ROS2 would win if the developer base is split at this point.", "I\u2019ve used ROS since 2010, and I\u2019ve seen many examples of new APIs being introduced (rosbuild to catkin, tf to tf2, C++98 to C++11, \u2026), and it always took many years until the majority of packages was updated. For example, tf2 was available since at least 2013, and navigation switched to tf2 only last month. Since ROS1 to ROS2 is a much larger step than any of those examples, I\u2019m afraid it will take much longer than that for the majority of packages to migrate to ROS2.", "I fully agree with ", "\u2019s suggestion to make the API transition gradual in order not to", "\nsplit the developer base. Not out of charity for ROS1, but to give ROS2 a shot at survival;", "OSRF can just going to stop the buildfarm for Melodic, then how will the ROS1 community survive? There would have to be a thrid alternative to an OSRF supported ROS1 and an OSRF supported ROS2.", "I fully agree with ", "\u2019s suggestion to make the API transition gradual in order not to", "\nsplit the developer base.", "That was a necessary discussion to lead 3 years ago. Different people pushed for it at the time. See e.g. this discussion: ", "And one reaction was this statement at ROSCOn 2015: ", ", starting minute 46, Brian Gerkey took the mike:", "\n\u201cI\u2019ll just add that one of the things that [\u2026] we talked about is what we colloquially refer to as a library shim. So this is something that you can imagine in any language [\u2026]. It would present a ROS1 API, but under the hood it would call into the ROS2 libraries. [\u2026] There are going to be different migration paths for different use-cases. We\u2019re not a gang of super-villains out to give you a really bad day. We want to make this as useful as possible and make it as easy as possible to migrate.\u201d", "Then at ROSCon 2016, we got this talk:", "\n", " (Minute 29:46) with William basically saying they could not get design a shim that works beyond simple cases. Also talks about experiments to unify the buildsystem (minute 32).", "I don\u2019t think there has been any encouraging development since. Given the TSC committee notes (", "): \u201cWhen will ROS1 be EOL\u2019d? [\u2026] It would be good to have a tentative plan for migration. The longer people can use ROS1 the less motivated they will be to move to ROS2.\u201d, it seems that instead of providing a smooth migration path, just scaring the ROS1 community into all trashing their existing work and investing in rewriting their systems for ROS2.0 is favoured by OSRF and the TSC. Basically make the community pay for OSRFs decision to create ROS2 fast and backwards-incompatible, and worry about migration later.", "So not sure if Brian today would repeat his above joke \u201cWe\u2019re not a gang of super-villains out to give you a really bad day.\u201d in light of all the talk of making Melodic the last release.", "OSRF can just going to stop the buildfarm for Melodic, then how will the ROS1 community survive? There would have to be a thrid alternative to an OSRF supported ROS1 and an OSRF supported ROS2.", "If OSRF were to do that, there would be a fork. All those companies and research labs deeply invested in ROS wouldn\u2019t just say \u201cwell, it\u2019s been fun, I guess we\u2019re just going to do something else now\u201d. Everything necessary to run one\u2019s own build farm is open source, so it\u2019s not like OSRF have a kill switch for ROS in their hands that they can push at any time. But probably there wouldn\u2019t be just one, but multiple forks; also, without a source of funding, it\u2019s unclear to me how the fork would be able to keep up the quality of support that OSRF is providing at the moment. Thus the \u201cchaos, confusion and pain\u201d I mentioned in my earlier post.", "That said, I have confidence in OSRF to do The Right Thing\u2122 (i.e., not trying to actively kill ROS), and I really believe Brian  when he\u2019s saying they\u2019re not a gang of super villains. ", "Your points about the shim are very good. I get the same vibe that it\u2019s not going to happen, but I still believe we would really need one for the reasons that ", " listed. ", "In the absence of a shim (I don\u2019t know whether that is possible or not, but for the sake of this question assume it isn\u2019t), what about a migration tool or script to do 80+% of the migration effort? Would that help ease the transition?", "I agree with Martin. As a company building solutions based on ROS 1, porting all of our code base to a new middleware would be a major effort. Our robots perform mobile picking in warehouses, where performance and reliability demands require a well-understood system. Exchanging the foundation of this very complex system and getting to a similar performance level again will not be an easy step.", "And even if there was a script for doing many of the basic steps, the real work begins afterwards when you discover and fix all the bugs related to a different communication model, bugs in the still fresh and mildly tested libraries, in core components etc that often only appear after long-term operation (cf. some of the bugs in roscore my colleagues have reported and fixed)", "Since many of the new features of ROS 2 are not really required for our use case, we would have to decide whether it makes more sense for us to migrate, or to fork and build, in addition to our own packages, also the required ROS core packages. The latter would definitely not be my preferred outcome, and I hope that we can find either a good migration path or ways for maintaining both versions for a longer time.", "I\u2019m coming late to the discussion here, but it seems like there are a couple points that are being conflated.  I totally agree with ", " that ROS API changes are hard things to live through.  I\u2019ve been using ROS since before Mango Tango, and I shudder when I think of some of the transitions we\u2019ve had to go through.  However, I also agree with ", " that not releasing another LTS after Melodic doesn\u2019t kill ROS next year.  Much of my lab is still on Indigo because some of our robots are locked to particular ROS version, and the pain of upgrading outweighs the lack of recency of our version.  If there were no more releases after Indigo, I wouldn\u2019t have cared.  Of course, everyone\u2019s use cases are different, but I don\u2019t think it\u2019s accurate to say that ROS1 is over if there\u2019s no N-Turtle.  If does, however, start the clock ticking.", "I guess that, for me, it breaks down to the question of \u201cAre the improvements in ROS2 worth the pain of migrating all of my code?\u201d.  We\u2019ve got a lot of code, and that would be a significant pain.  Personally, I think that ROS2 will be better (for me and my students) than ROS1 is, once we get some features in place (looking at you actions and the new nav stack).  Given that we have to upgrade from Indigo next April in any case, I might just bite to bullet and move to ROS2.", "In the end, I think that the question of limited resources might be the most powerful one.  If Open Robotics has N hours to work on ROS, how many should they spend on ROS1 and how many on ROS2.  If we need M hours of work to make ROS2 viable, then the math is simple; more time on ROS1 means a longer time until ROS2 is ready.", "The worst thing we can do is fork.  If this happens (and it\u2019s a non-zero probability event, for the reasons the ", " lists), that\u2019s bad for everyone.", "I think that the issue of emergent bugs that ", " mentions is the thing that worries me most about the transition, since this sort of thing is inevitable.  However, my hunch is that there are going to be fewer of them, since we\u2019re moving to a more robust communication system, and because Real Companies \u2122 with Real Software Engineers \u2122 are now starting to contribute to ROS (hat tip to ", " and his crew, among others).  How many of the emergent bugs in ROS1 were because of code written by grad students that wasn\u2019t properly tested?", "The big question, of course, is whether the new (hopefully) more mature and better-developed parts of ROS2 will dominate the inevitable emergent bugs.  I have no idea, but I\u2019m at least a little optimistic.", "True, a number of components seem better designed, and software engineering practices are used more from the beginning. A problem when porting a complex system built on top is just that the application code, often silently, relies on properties of the communication layers (think for example about when exactly to ask tf for transforms and what to consider beforehand). If such behavior changes (even if it changes to the better), problems will appear.", "That\u2019s probably the crux of it; how many of these implicit assumptions will surface, and how long will they take to find and fix.  I guess that\u2019s an argument for both never moving to ROS2 (so we don\u2019t have to deal with it) and moving to ROS2 immediately (so we don\u2019t write any more code that relies on ROS1 idiosyncrasies).  :-/", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/discussion-on-ros-to-ros2-transition-plan/6155"},
{"title": "Ros_comm - request for help", "thread_contents": ["If you have created a pull request against the ROS 1 ", " repository in the past you have likely experienced a long wait until you have received feedback - and in several cases your patch might not have received any attention yet. Unfortunately this has become a more frequent problem in the recent past (and I am really sorry for that negative experience). So why is that the case?", "I am the assigned maintainer of almost all packages in the ", " repository. Beside that I am also the designated maintainer of almost all ROS 1 packages which make up the ", " metapackage as well as higher level packages like ", ". ", " is standing out here due to its prominent content but other \u201chigh profile\u201d repositories like ", " are similar in this regard. ", " receives by far the most tickets - issues as well as pull requests. In the early ROS days it was often sufficient to file a ticket describing a problem or feature idea and \u201csomeone\u201d would just go ahead and fix / implement it for you. That has been much less the case in recent years. At least on the pull request side I was able to review contributions in regular intervals (something around every few weeks). Unfortunately not only has that interval continuously increased also the time I am able to spend on actually reproducing problems, testing patches etc. has decreased significantly. That resulted in both: patches got merged which weren\u2019t sufficiently tested and introduced regressions as well as many patches which haven\u2019t been \u201caccepted\u201d until now.", "To understand why that is the case I have to mention a bit about my work time at Open Robotics. As you\u2019ve surely noticed over the past few years, our development focus is on ROS 2. We\u2019re still maintaining ROS 1 on a best-effort basis, but my day-to-day priority is shifting more and more towards ROS 2 development. That priority is the result of both our decision as a company on where to focus our energy and the reality of what type and scope of work can be supported by the industry partners and government agencies that supply the bulk of our funding. As a result any time that I do spend on ROS 1 maintenance necessarily takes away from our current projects. And even without targeted funding, the company decides to continuously invest quite some discretionary resources into ROS 1 maintenance (through each of our employees) - consider the effort it takes us every year to prepare a new ROS distribution - beside the continuous work on issues and pull requests, running the buildfarm, etc.", "Open Robotics doesn\u2019t have the resources available to allocate to these tasks so we can\u2019t expect any additional time to \u201cmagically\u201d become available. Luckily there is already one external ROS developer helping to maintain the ", " repo (Thank you, Mike Purvis / Clearpath Robotics!). Without his efforts the current situation would be much worse. But the trend of me having less and less time as well as the load continuously increasing leads to the current unsatisfying situation as well as a not very positive outlook for the future.", "So what can we do about it? Ultimately we need more people to volunteer to help with the effort of maintaining these packages. These contributions can come in many forms: review pending patches (which includes reproducing the problem, applying the proposed patch, reporting that the problem is fixed, additional testing to check for potential regressions), review incoming issues and provide users with help to fix them, help with other maintainer tasks like creating and testing patch releases, considering patches for backporting into \u201colder\u201d ROS distributions which are still being supported, investigating flaky tests, etc. A good way to start is to browse through the existing tickets and subscribe to the notifications on the repo so you will get emailed when new issues / pull requests are coming in, and some may pique your interest. While any kind of help is highly appreciated we are looking for people which want to help for a longer time period and on a regular base with these tasks. If you would like to \u201ccommit\u201d to that kind of support please let us know and we are happy to get you more actively involved.", "I am interesting at maintaining the old package in ROS1 how to involve the great process ?", "I can volunteer some time to help out with some of these tasks. Plus I always wanted to learn more about ROS packaging and the build farm process.", "I\u2019d like to help support this.", "To understand why that is the case I have to mention a bit about my work time at Open Robotics. As you\u2019ve surely noticed over the past few years, our development focus is on ROS 2.", "You left out the part where OSRF decided to make all of ROS2 independent and incompatible to ROS1, causing a split in the community, and duplicating all maintenance efforts. Don\u2019t you think that this is part of the reason?", "So what can we do about it? Ultimately we need more people to volunteer to help with the effort of maintaining these packages.", "You do not mention the possibility of migrating the ROS1 community as a whole to ROS2. But maybe even OSRF has given up hope of achieving that anytime soon after making the migration path so difficult.", "Open Robotics doesn\u2019t have the resources available to allocate to these tasks so we can\u2019t expect any additional time to \u201cmagically\u201d become available. While any kind of help is highly appreciated we are looking for people which want to help for a longer time period and on a regular base with these tasks.", "You really mean OSRF is looking to hand over ROS1 to some other organisation, like an actual open-source robotics foundation?", "So will OSRF make a presentation at next ROSCon about how ROS1 is dead, given ros_comm is unmaintained? Might go nicely along with other presentations showing new ROS1 packages.", "Hi Dirk,", "I would like to help, but I cannot really justify it to my employers at this point, when we are focusing on ROS2.", "Also, I think QA on ROS1 has always been a problem and I don\u2019t see a good solution.", "So, how about trying the idea of a library shim, either instead of or in addition to adding maintainers to ros_comm?", "I mean a ROS1 API on top of rcl. We could transition everybody much more quickly that way.", "I know this has been talked about before, but not realized so far. Not sure why, but if it\u2019s about the effort, a shim ", " be something I and probably many others could get involved in, because it\u2019s about helping with the future ROS.", "Cheers,", "Ingo", "I like Ingo\u2019s proposal of a ", ". I\u2019d be interested in helping on the Python part.", " ", " ", "Thank you for offering your support. Please see ", " where I tried to give a high level overview how to help. Please feel free to ask questions on that ticket.", " ", "If you would like to contribute towards an API shim that would be great. We have had discussions about the topic in the past about how viable / complicated it would be, what to cover, what to do when the existing ROS 1 API can\u2019t be mapped to ROS 2 etc. I would suggest to start a discussion in the \u201cNext Generation ROS\u201d category. I could also create repositories for this effort (e.g. ", ") if you think that would be helpful to get things rolling?", " lets start a discussion, indeed. You may have a better insight of how much work and how doable it is.", "Any updates? Maintenance still seems dead.", "\nSome people did volunteer in this thread, is there an ongoing process to find/add maintainers?", "Any updates?", "\nSome people did volunteer in this thread", "While there were some comments offering help there was unfortunately no actual help contributed afterwards.", "is there an ongoing process to find/add maintainers?", "I am not sure what you mean with \u201congoing process\u201d? We are still hoping for others to help but have reached out numerous time with no effect and don\u2019t know what else to do from our side.", "I am spending every few months a little bit of time on catching up with pull requests. Currently I am still working through the tickets on lower level repositories before getting to ", ". And with the amount of PRs pending in that repo I doubt that I will have time to review all new ones before I have to drop the ball again for another few months. (I am sorry if that isn\u2019t satisfactory but I want to set clear expectations.)", "Just to clear up a potential misunderstanding: The process isn\u2019t to just add maintainers to ros_comm. Instead, ", " has lined out how you can help here: ", "Any updates?", "\nSome people did volunteer in this thread", "While there were some comments offering help there was unfortunately no actual help contributed afterwards.", "Sorry i just read the three volunteer prompts and missed the follow up over the Ros1/2 shim rant.", "\nMaybe everyone else missed that too ", "I am not sure what you mean with \u201congoing process\u201d? We are still hoping for others to help but have reached out numerous time with no effect and don\u2019t know what else to do from our side.", "The steps from the ticket include reading issues and creating pull request. Maybe people did read tickets and created pull requests: what now? Thumps Up PRs?  I can imagine this is why none of the volunteers did help. I fell somewhat lost after reading the ticket - No Offense, just presenting my own thought process. ", "I am spending every few months a little bit of time on catching up with pull requests. Currently I am still working through the tickets on lower level repositories before getting to ", ". And with the amount of PRs pending in that repo I doubt that I will have time to review all new ones before I have to drop the ball again for another few months. (I am sorry if that isn\u2019t satisfactory but I want to set clear expectations.)", "It feels like too much for a single person - ", ": There should be additional maintainers, right?", "There should be additional maintainers, right?", "Probably yes. However, since ros_comm is the most important repo of ROS1, I hope that there\u2019s a vetting process for new maintainers; like, potential new maintainers should first help answering issues, reproducing and testing PRs etc. without maintainer status (as described in the issue mentioned above). Once they\u2019ve demonstrated that they are diligent and know what they are doing, they could get direct push access to the repo.", "The steps from the ticket include reading issues and creating pull request. Maybe people did read tickets and created pull requests: what now? Thumps Up PRs?", "You might want to read through the instructions of the referenced PR which describe in detail what kind of help we are looking for: ", "And no, just \u201creading issues\u201d isn\u2019t helping much - the goal is to comment on issues with the finding according to the triage process described in the above ticket. Just to copy some of the bullets from there:", "Also \u201cthumbs up PRs\u201d isn\u2019t helping much since it is unclear what the reviewer actually did (beside that nobody gets a notification when you add an emoji to a ticket). Maybe the person just wants the problem to be fixed. Again some bullets on what we are looking for:", "I fell somewhat lost after reading the ticket.", "Then please consider to ask for clarification.", "Some people probably want to help, but also might simply be intimidated/lack confidence (similar to what the MoveIt! maintainers ", ").", "Not sure what could help other than to tell people (myself included) to \u201cjust do it,\u201d since your instructions on ", " definitely provide sort of a starting point.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Check if it is still valid", "If it is a bug try to reproduce the reported problems", "Consider creating a pull request to fix the bug / implement the feature request (always ", " the original issue)", "If you don\u2019t have the permission (yet) to set labels or close obsolete tickets please just comment with the suggestion and someone will follow up on it.", "Check if it is still valid\n", "All pull requests must target the latest development branch which is commonly also the default branch (currently  ", " ), the only exception is if a patch is only applicable to older branches and not necessary on the default branch", "\n", "If the patch already has feedback which hasn\u2019t been addressed by the author consider creating a new PR addressing the pending feedback to make the patch mergable (always referencing the original ticket)", "Review the patch which includes:\n", "Check the diff for problems (unnecessary / unrelated changes, incorrect / incomplete logic, API / ABI breakage, code style matching surrounding code)", "Reproduce the problem without the patch", "Apply the patch", "Reproduce that the problem has been addressed and try to make sure that it doesn\u2019t introduce any regressions", "Ensure that it passes the tests", "\n"], "url": "https://discourse.ros.org/t/ros-comm-request-for-help/5840"},
{"title": "Looking for Kobuki power supply / battery charger", "thread_contents": ["I have a Turtlebot2 / Kobuki base but no charger for it. I\u2019ve searched online and can\u2019t seem to find any for sale anywhere. If anyone has a link to where I might purchase one, I\u2019d appreciate it.", "Thanks.", "Hi Matt,", "I looked at the spec sheet and you need an Input: 100-240V AC, 50/60Hz, 1.5A max; Output: 19V DC, 3.16A power supply.", "Here\u2019s an example on Amazon: ", "As for the barrel connector, I measured mine at 5mm. The one linked above is 5.5mm, which I assume would also probably work with a little tough love.", "S", "Thanks ", ", I\u2019ll give that one a try.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-kobuki-power-supply-battery-charger/8754"},
{"title": "Is this the place to discuss bugs and fixes to ROS_serial?", "thread_contents": ["If this is not the place, please point me to the correct place.   Thanks.", "ROS Serial supports several platforms and is inherently multi-platform.   The libraries need to compile on the Arduino IDE, some RTOSes and so on.    This might be the only part of ROS 1 that is actually multi-platform.  Some platforms are more popular than others.  For example, Arduino seems to be ", " popular and others may not be used so much.", "I\u2019d suggest that it is a bug if the included \u201cHello World\u201d style examples fail to compile on a platform using that platforms\u2019  official native build system.  Yes, this is a strict definition of \u201cbug\u201d.  But be this rule ROS Serial is bug-free on Arduino and completely broken on Mbed.", "I have posted some questions on a few forums asking if anyone has built a ROS Serial client on the Mbed platform using the \u201cofficial\u201d Mbed build tools and the current version of both ROS (Melodic) and Mbed.  (5.10+)     I\u2019ve not yet heard from anyone who claims to have done this.", "In an ideal case, the Mbed would work like the Arduino:   A developer would import the ROS_Serial library and a \u201chello World\u201d example  to his IDE click \u201cbuild\u201d and it \u201cjust works\u201d", "Where to discuss (1) if this needs to be changed and (2) possible solutions?", "ROS Serial is a great idea and could be much more useful if it worked better with some more powerful platforms like an STM32F that runs an RTOS.", "Need to discuss if it needs fixing and if so what to do.", "Perhaps we just leave it and move on to ROS 2 as I\u2019m guessing ROS 2 will eliminate the need for ROS Serial.  As people could run a DDS library on the microcontroller.", "As people could run a DDS library on the microcontroller.", "That\u2019s not exactly right ", ".  Well at least not fully. There\u2019s been a few initiatives over the past years. Of most relevance (disclaimer, my team is working on ", "):", "An initial attempt to port a complete DDS implementation to a microcontroller has been made available by Open Robotics at ", " but it hasn\u2019t been maintained for a while.", "Another attempt to get an RTPS (the underlying protocol of DDS) library running in uC is ", ".", "A newer initiative called micro-ROS is ongoing at ", " and proposes a slightly different architecture ", " where it breaks the fully distributed paradigm by introducing a client/server architecture between microcontrollers and an a bridging Linux machine (an SBC for now) for interfacing with the ROS 2 network and hopefully, reaching much smaller microcontroller footprints. Refer to ", " for the middleware implementation.", "If you have some cycles to support any of these initiatives I\u2019d recommend contributing to 2 or 3 Probably in that order since both are complementary and 2 could be used either independently or as one of the communication middlewares within 3 (which would be pretty cool).", "The problem of course is I already have enough projects that I\u2019m  100% occupied.    But I see micro-ROS as the way to go one a follow-on to what I\u2019m doing.", "That project is a ROS base controller in an STM32F \u201cBlue PIll\u201d board.   It connects to ROS1 via  ROS Serial.   The STM32 is configured from the parameter server, subscribes to cmd_vel and publishes TF and /odom.   Nothing new except I want it to be a plug and play ", " device.   The board is under $3 and it will be integrated with a commonly cloned h-bridge.", "The follow up will have it\u2019s own IMU and will fuse with odometry and GPS all on the STM32 F7 processor.  micro-ros would be ideal for this.    Looking at something like this drone flight controller to control a 2 to 6 wheel drive platform.", "\n", "One question:   Is there a preferred development platform for micro-ROS?   What do the developers use themselves?    I\u2019d like to start with the best-supported hardware platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/is-this-the-place-to-discuss-bugs-and-fixes-to-ros-serial/8527"},
{"title": "Announcing new packages for TurtleBot3 in ROS2 (including Cartographer and Navigation2)", "thread_contents": ["Hi guys ", "We are happy to announce new packages for ", "!!", "\nThe packages includes ", "(", ") and ", "(", ")", "Now, you can launch those packages using simple commands in ROS2 Crystal Clemmys with TurtleBot3.", "\nIf you already have TurtleBot3, you could try teleoperation, SLAM and navigation through ROS2 frameworks after you setup for ROS2.", "\nIf you don\u2019t have TurtleBot3, you could load TurtleBot3 into Gazebo simulator and launch everything I said.", "This release only support ", " yet, but we are going to update that packages to support ", " after few days later.", "[Github repo]", "[Issue page]", "[E-Manual]", "\n", "\n", "Please consider to release your packages using ", " in order to provide Debian packages for users so that they don\u2019t have to build the packages from source.", "I want to thank ", " for the great work he did here! I was working on this from the Navigation2 side and bugged him many times for help and updates, and he patiently helped me and worked through the issues to get it working! Thank you!", " Of course, we have plan to release tb3 packages ", " Thank you for your kind comment ", " Navigation2 and your team makes me driven to focus that packages. Thank you again and let\u2019s make more greatness!", "Hi ", ", I am preparing to release the TurtleBot3 packages using bloom in order to provide Debian packages for users. Fortunately, the way to register with ", " is the same as before. One thing I\u2019m curious about is that I have used ", ", but ros2 does not seem to be able to use it. What should I use instead of this? I want to hear your advice. Thanks. ", " Please post any follow up conversation to a different place since this category is only intended for announcements etc. and reaches too many people. Either to the ", " category or ", ".", "One thing I\u2019m curious about is that I have used ", ", but ros2 does not seem to be able to use it.", "I guess the right statement would be: ", " doesn\u2019t support ROS 2 (rather than the other way around).", "What should I use instead of this?", "The official ROS buildfarm has the same capabilities for ", " as for ", ". When releasing your repositories you should make sure to:", "If you don\u2019t want to leverage the official ROS buildfarm you can still use the logic provided by the package which powers it by using the scripts locally or withing a CI provider like Travis: see the ", " for more information about that.", "I am not aware which of the other ", " support ROS 2 atm.", "Cheers,", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["add a ", " entry to ", ". Based on that a ", " job will be created which will build new commits on the target branch shortly after they are pushed (and in case of problems send notification emails to the maintainers listed in the packages).", "choose to enable pull request testing. Based on that a ", " job will be created which will build commits on PRs targeting the specific branch and report back with the status through the GitHub UI.", "Dirk"], "url": "https://discourse.ros.org/t/announcing-new-packages-for-turtlebot3-in-ros2-including-cartographer-and-navigation2/7694"},
{"title": "Addition of Radar-Specific Message(s) to sensor_msgs", "thread_contents": ["I work for a company that is very closely tied to the automotive industry. Many people in our industry are starting to use ROS as the basis for research and development efforts into autonomous vehicles. One of the primary sensing modalities is radar. Unfortunately, there isn\u2019t really a basic message type in ", " that currently fits the output from a radar. The closest available is ", ". However, while there are ", ", ", ", ", " and ", " properties associated with a radar detection, there are also other intrinsic properties like ", " (a measure of the power of the returned signal - also applies to the other light-based sensor readings represented by Range) and ", " (the angle of the detection within the lateral field of view - since radars have horizontal discrimination). Soon, there will also be 3D radars with vertical discrimination necessitating a splitting of ", " into lateral and vertical components.", "Furthermore, many radars do not actually output the raw detection information but only output \u201ctracks\u201d which are filtered and grouped abstractions of single or multiple detections. Because of the tracking over several scans, they contain all the same readings as ", " with the following exceptions:", "We have our own versions of messages representing these data (see our package on Github [1] - specifically RadarDetection and RadarTrack) but I am now aware that these do not comply with REP 117 [2] and would much rather contribute to the standardized set of messages in ", ".", "To the point: Does it make sense to try to extend ", " to include the properties of a radar detection or should a new message be created? What about a track (when they are the only data available)?", "[1] ", "\n[2] ", "I think the best way to go is to create a new message, where you include", "\nthe existing  sensor_msgs/Range  and add the missing fields.", "\nExtending the current  sensor_msgs/Range is not a good idea because it", "\nwould change the message checksum and all hell would break loose for those", "\nalready using it.", "Thanks for the feedback, Procopio. Someone here at ROSCon mentioned this would likely be a problem too but I wasn\u2019t sure how worried the community was about it. I\u2019ll make the pull request.", "Hi ,", "\njust to comment that some radars provide, for each target detected, the Doppler velocity as a raw measurement made directly by the device. So it would be important to differentiate \u201crange_rate\u201d which, as JWhitleyAStuff said, the polar longitudinal velocity of tracks, from the \u201cdoppler_velocity\u201d , which is also a polar longitudinal velocity, but issued from a wave signal processing step, and not issued from an object tracking process.", "\nFrom the robotics perspective, doppler velocity is a raw measurement, like range.", "\nbest,", "\nandreu", "Thanks to everyone for the feedback. I think it\u2019s unavoidable to change ", " in, at least a minimalistic way - e.g. adding an enum value to ", " for RADIO. I\u2019m going to go ahead and make the pull request and we can discuss the naming and architectural semantics there.", " Thanks for starting the thread and also opening ", ".", "Now that common_msgs is relatively mature we generally want to make sure that messages are already in use and have been tested, instead of developing them in the abstract here. There\u2019s some notes on contributing ", "We generally want things to have been validated by real use in the field before merging them into common_messages so that we can be confident that they are useful. Sometimes new messages here will also be a merge of two or more messages to standardize. Such as ", " for the BatteryState message.", "There may be several classes of Radar returns that would be good to clarify/separate. There are potentially low level interfaces (range, intensity, velocity)  as well as high level interfaces (object position, velocity, and scale). Surveying the field of sensors and understanding which ones can be covered by any given interface would be valuable in this process. And I\u2019m sure that there are users with radar messages already that would be great to get their feedback and look for an already tested version of a radar message.", "A good amount of people seemed to be interested in these messages. Just a note that they have  now moved from ", " to ", ". All packages under these repos just got an update and have been released to ShadowFixed.", "They are now available in the ROS official repos. You can ", " from Ubuntu or Debian.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They do not contain \u201camplitude\u201d since this is meaningless for a grouped/tracked single or set of detections.", "They also contain the following additional fields:", "\na. ", ": The lateral velocity of the object within the arc of the field of view of the radar (polar lateral velocity).", "\nb. ", ": A lateral measurement of the \u201ctrack\u201d from the most extreme lateral detections that make up the track.", "\nc. ", ": A measurement of the longitudinal velocity of the track with respect to the detected angle (polar longitudinal velocity).", "\nd. ", ": A measurement of the longitudinal acceleration of the track with respect to the detected angle."], "url": "https://discourse.ros.org/t/addition-of-radar-specific-message-s-to-sensor-msgs/2724"},
{"title": "ROS Developers Podcast (RDP)", "thread_contents": ["ROS Developers Podcast is a podcast for developers who are passionate about ROS. Learn ROS and related technologies. Updated every Monday.", "If you want to share your ROS research results or experience with more ROS enthusiasts in our podcasts, please contact ", ".", "\nIn this episode we talk to Dirk Thomas about ROS 2 and some other details about development tools of ROS 1.", "\nHe will explain about:", "\nIn this episode we talk to David Crawley founder of Ubiquity Robotics.", "\nHe will explain about the idea of using built robots for building more complex robotics products and applications. He will also explain about:", "\nIn this episode we talk to Florian Lier about what his team ToBI is doing to program Pepper the robot for the Robocup@Home competition.", "\nWe will explain the whole architecture they are using to program Pepper with ROS for the competition including:", "\nIn this episode we talk to one of the main developers of the Gazebo simulator: Louise Poubel. She tells us about the ins and outs of Gazebo, how do they organize at OSRF to develop the simulator, why to discard Gazebo 2 and how to install the newest versions. She will also clarify us what are the Ignition libraries and why OSRF is putting so much attention to them for the development of Gazebo. Additionally, she will talk about Gazebo plugins, SDF versus URDF, rendering in Gazebo, skeleton animation and many more things.", "\nIn this episode, Jack Pien will talk about how to use accessible robots based on ROS and OpenCV to learn robotics. Those robots are called ROSbots and Jack is the creator of them.", "\nROSbots come with ROS and OpenCV installed on a Raspberry Pi onboard the robot, ready for the user to create applications for it using the camera and the motor.", "\nThe purpose of those robots is to provide hackable robots, on which you can put your hands on, and implement all those robotics algorithms that you study.", "\nIn this session of The ROS Developers Podcast, I chat with Cecilio Angulo, associate professor at Technical University of Catalonia and director of the Intelligent Data Science and Artificial Intelligence Research Center. We will talk about a project he started to bring Aibo robot back to life again by using ROS.", "\nHe talks about how the whole thing started and why. Then he describes the different parts implemented in the Aibo ROSification and how the Aibo simulation in Gazebo is important for the development. He also describes the application that makes several Aibo imitate to each other by connecting all of them through a ROS node, as well as the modules for accessing different sensors of Aibo. Finally, we talk about how to make a ROSified Aibo reproduce the famous MTN (motion) files that Sone generated containing predefined movements.", "\nIn this episode of The ROS Developers Podcast, we chat with Carlos Rosales, CTO of Beta Robots a company dedicated to apply ROS into real life industrial projects.", "\nHe talks about the simulation model of the softhand, an open source hand with 5 fingers that is flexible and that absorbs collisions. He will also talk about different projects he and his company have been involved including industrial manipulators, perception and grasping. Finally he will provide some information about the Master of Robotics he is doing, including some links to the git of the master and the slides.", "\nIn this episode, we interview Enrico Mingo Hoffman, postdoc researcher at the Humanoids & Human Centered Mechatronics Lab of the Italian Institute of Technology.He will talk about their open implementation of the Stack of Tasks for the whole body control of humanoid (and non-humanoid) robots. He will also explain how their development environment at the lab works, and will discuss some differences between ROS and YARP, another framework for robot programming developed at Italy. He will end suggesting about the creation of an OSRF \u2013 Europe. Do not miss that!", "\nIn this episode, we interview Francisco Martin, associated professor at the University Juan Carlos I in Madrid, where he teaches robotics by using ROS. He is also a participant of the Robocup and the ROCKin competitions. He will explain how have they hacked Pepper the robot by means of using docker machines to cross-compile ROS code for Pepper. Francisco will also tell us about with research work on applying cognitive architectures to the control of robots, using ROS Plan inside Pepper to figure out the situation of the robot.", "\nIn this session of The ROS Developers Podcast, we chat with Sam Pfeiffer, who is a PhD student at University of Technology, Sydney where he leads the team of the University that goes to the Robocup with the Pepper robot. Among other things, he will explain us how he ROSified Pepper for that competition. He will also talk about how other participants of the competition are using Deep Learning to win, and how you can do that with ROS. Finally, he will recommend us some useful tools for programming with ROS, like for example a tool that he coded himself that allows to identify the ROS messages used by a remote ROS system, and create automatically a definition for the ones that your system does not have, so you can talk to the remote ROS system.", "\nIn this episode, Ryan Gariepy CTO of Clearpath Robotics talks about how they use ROS for field service robots. He will explain how he uses vim for coding with ROS their field service robots. He will also express his point of view about the current status of ROS based on his large experience on using ROS for real life service robots. Finally, he will give some recommendations to the new comers and indicate that there is a huge future with ROS at Clearpath. By the way\u2026 they are hiring!", "\nIn this episode, Luca Marchionni CTO of Pal Robotics talks about how they use ROS for their humanoid robots. He will specifically describe how they achieved to control with ROS a human size humanoid robot that walks and required real time control. He will also reveal how they attached the NASA Space Robotics Challenge in order to reach third position at the competition. Finally, he will recommend one tool for ROS developers that helps them in their daily life as ROS developers.", "We released the ROS Developers Podcast episode 24 with an interesting interview to Dejan Pangercic about how they are forking ROS 2 to build Apex.OS, a realtime ROS 2 based operating system for autonomous cars.", "\nYou can listen to the podcast here: ", ", or ", ".", "\nIn this episode we talk to Victor Mayoral, CTO of Erle/Acutronic about their Hardware ROS (or better known as H-ROS). We will talk about how they have implemented this hardware framework for building ROS based robots by combining hardware parts that natively work on ROS (no drivers required in the main computer). He will also explain us how to ROSify any hardware by using their SoM, and how they have applied this procedure to build a collaborative robot called MARA. And everything is working on ROS 2!", "\nIn this episode, we talk to Dejan Pangercic, the CTO of APEX.AI a company that is creating the first fork of ROS 2 to create a new operating system for autonomous cars based on ROS: The Apex.OS. Dejan explains us why ROS2 is the way to go for autonomous cars, how are they modifying the main release of ROS 2 to create a certified version of it for self-driving cars, and how all that integrates with Autoware.", "\nIn this episode, Ross explains how he uses ROS to teach foundations in robotics. He tells about how he synchronizes the teaching of ROS with the teaching of robotics subjects, as well as his use of simulations and the importance of debugging tools for ROS. He also explains why he doesn\u2019t use the ROS navigation stack or MoveIt! with his students.", "\nIn this episode we talk to one of the mothers of ROS, the co-creator of the Turtlebot robot, and the CEO of one of the most important companies of the world that sells ROS based Warehouse robots (Fetch Robotics). Among other things, she will explain about their work to simplify the control of fleets of robots based on ROS1 or how they use simulations in their development process.", "\nIn this episode we talk with Joel Esposito about his research on what is the current state of robotics degrees around the world. Based on a set of questionnaires, Joel obtained a clear view of what is the current situation of robotics degrees around the world and what are its drawbacks. He then proposes how those drawbacks may be overcome and how ROS enters into the picture.", "\nIn this episode we talk with Kyler Laird about how to apply ROS to agricultural machines. Kyler will explain his experiences creating autonomous tractors. He will also talk about the ROS Agriculture group created to spread ROS among farmers.", "\nIn this episode, Juan Jimeno from Singapore talks about his Linorobot suite. The Linorobot project is a suite of materials to allow people to learn about ROS by building your own robot. The project provides all the required elements to construct a program a wheeled robot, and it includes the option to build different types of wheeled robots including differential and ackermann steering types.", "\nIn this episode, Mika Barkan talks about her experience teaching ROS to undergraduated students at Bar-Ilan University from Israel.", "\nShe will explain some problems she is facing:", "\n\u2013 What is she teaching", "\n\u2013 How her students install ROS in their computers", "\n\u2013 Why she doesn\u2019t uses real robots for teaching ROS", "\n\u2013 What are the limitations she is facing for teaching ROS", "\nIn this episode, HaoChih uses ROS 2 and DDS to create IoT products based on ROS 2 while working at ADLink Technology. He will explain some solutions he has been creating while at ADLink like:", "\n\u2013 Adlink_DDS bot", "\n\u2013 Adlink Neuron bot system", "\n\u2013 How they use DDS and ROS 2 for ARM based robotic solutions.", "\n\u2013 He will also talk about his Hypha ROS workshop organized last year in Taiwan.", "\nIn this episode we talk with May Zheng from chinese company Gaitech. She will explain us:", "\n\u2013 Is a very good moment now to sell ROS based products in Asia? How to start doing it?", "\n\u2013 What are the main problems of selling ROS based products in China", "\n\u2013 Which is the biggest market now in China for selling ROS based products", "\n\u2013 Is there any chance that a Chinese ROS appears in China?", "\nIn this episode, we talk about how Peter is using ROS to create rescue robots that win robotics contests.", "\nHe will explain what are the current limitations of rescue robots and why they need a human pilot to drive them. He will also explain how they are doing the pilot interface using ROS. Additionally, he will explain how his team is connecting the hardware with ROS by means of ROS Serial.", "\nMany more details included about several rescue robotics competitions around the world.", "\nIn this podcast we are going to learn how to integrate MATLAB code with ROS code to provide access to your robot to powerful mathematical libraries. Included in the podcast:", "\n\u2013 What is MATLAB and Simulink", "\n\u2013 What is the idea behind usingMATLAB with ROS", "\n\u2013 How does MATLAB provide support for ROS", "\n\u2013 What is the difference between Simulink and Gazebo", "\n\u2013 Useful MATLAB packages that ROS users can benefit from", "\n\u2013 Is MATLAB ready for ROS 2?", "\n\u2013 A competition surprise the guys of MATLAB are preparing for the ROSCON 2018!", "\n\u2013 \u2026 and many more thingsWhat is MATLAB and Simulink", "\nIn this episode we would like to talk about the need that robotics developers have of having a standard for robotics (in terms of API).", "\nBy having a robotics standard, developers can concentrate on building solutions that do not have to be re-implemented whenever the robot hardware changes. Actually, given the middleware structure, developers can disassociate so much from the hardware that they can almost rely 100% of its time in the software realm while developing for robotics", "Released ", " where we interview Martin Pecka about how to use his package ", ". Very, very interesting!", "Released the ", ", where Roberto Guzman from Robotnik talks about using off-the-shelf ROS ready components to build your robot among other very interesting subjects.", "Released ", " where Krzysztof Zurad talks about how to use small, fast and lighweight time-of-flight sensors with drones. You cannot miss it!", "\nYou can also ", "For this week at the ROS Developers Podcast we have ", ". He explained us about a bunch of different projects that he and his team are developing at Bosch. Their aim is to create interesting tools for programming robots. Let\u2019s hear about Micro-ROS, Functional Mock-up Interface, Executor for ROS 2, and many other things\u2026", "This week ROS Developers Podcast is about connecting ROS robots over internet using a P2P secure network. We are talking about ", ". Super interesting!", "You can listen to the ", "In this week\u2019s episode of the ROS Developers Podcast, we talk about ", ".", "\nHope you like it!", "You can listen to the ", "In this week\u2019s episode of the ROS Developers Podcast, you can learn how to create Apple applications (for iPad, iPhone, Mac\u2026) that communicate with ROS by using Swift. Learn about the ", ".", "Learn how to create native applications for IOS that talk to ROS!", "In this week\u2019s episode of the ROS Developers Podcast, we are going to learn about what are behavior trees, why are they better than state machines and how can we use them inside ROS All those questions and many more will be answered by Davide Faconti, the creator of the BehaviorTree.CPP set of libraries", "In this week\u2019s episode of the ROS Developers Podcast, we are at the ROS Industrial conference (Europe) to meet Christian Henkel who presented at the conference the dockeROS library. DockeROS allows to dockerize ROS applications and sends them to remote robots. In this interview, Christian will explain to us how to do that, how to test the dockeROS and how do they differ from snaps.", "Very cool topic in the last episode of the ROS Developers Podcast. We interviewed Gonzalo Casas Software engineer at Gramazio Kohler Research of ETH Zurich. There, Gonzalo has developed the ROSlibPy a library that allows to create Python programs that can connect to ROS without having to install ROS in your system. It is very useful to interact from Linux, Windows or Mac machines with ROS robots.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "Why roscd bring us to the devel directory instead of to the compilation directory.", "Why they decided to change from rosbuild to catkin_make", "What can be used ROS buildfarm for", "Why ROS 2 is necessary", "How to use the ros1_bridge to connect ROS 1 programs with ROS 2 ones", "Some examples of ROS 2 applications already working", "What is the ROS 2 roadmap", "\u2026 and many more things", "\n", "\n", "\n", "What their robotics platform provides off-the-shelf, in terms of hardware and software", "Their own ROS distribution for Raspberry Pi", "Their already training deep learning network for recognizing common objects from the Raspberry Pi", "\n\u2026 and many more things", "\n", "\n", "\n", "In which computer to run which code", "ROS wrappers for NaoQi", "How to use a good Pepper simulation using Morse simulator", "Which environments to use to develop robot behaviors, and where to find them", "How they are integrating with automatic regression tests", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-developers-podcast-rdp/4505"},
{"title": "Creating web user interface for ROS powered robots with Bootstrap 4", "thread_contents": ["Hi ROS community!", "We\u2019ve just published a new ", " where we describe how to create a modern-looking web user interface for ROS powered robots thanks to Bootstrap 4 framework and Robot Web Tools.", "\n", "\nEnjoy!", "Best,", "\nDominik", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/creating-web-user-interface-for-ros-powered-robots-with-bootstrap-4/5731"},
{"title": "ROSCon 2018 Program Published", "thread_contents": ["The ROSCon Organizing Committee is happy to announce the ", " has been published on the main ROSCon website: ", "We\u2019re looking forward to seeing these talks in September.", "If you haven\u2019t registered already, there\u2019s just 2 weeks before the early registration deadline.", "Registration site can be found ", ". Get your tickets before the price goes up!", "See you in Madrid!", "Thank you again to our Platinum Sponsor, Erle, and to all of our Gold Sponsors: Amazon, Apple, Clearpath, Eprosima, Fetch Robotics, Google, Locus, Microsoft, ROBOTIS, SICK, Tier IV, Toyota Research Institute, Universal Robots.", "From the program:", "\n\u201cSupported by robotics development companies, academic researchers,", "\nthe U.S. Government, ", ", ROS-Military is a multi-faceted approach to effectively", "\nbringing the collaborative success of the ROS ecosystem to U.S.", "\nmilitary robotics \u2026\u201d", "Interesting to see this talk in ROSCon and advertise OSRF", "\ninvolvement, in particular the same year as Google makes news by", "\nterminating a DoD contract", "\n(", ").", "BTW, What the next killer-application build in ROS with the support of OSRF?", "I\u2019m very disappointed to see OSRF supporting the U.S. military (or any military for that matter) as well. I get that there\u2019s money to be made, but I believe a non-profit charity should not be in the business of war.", ", I appreciate your concern and understand your perspective. I\u2019d like to add some context to the discussion.", "Open Robotics would not exist today without the support of the US Department of Defense (DOD). Back at Willow Garage around 2010-2011 we often talked about creating a separate organization to specifically focus on open source development of ROS & Gazebo. What was missing was a way to fund such an organization. A contract from DARPA to develop simulation software for the forthcoming DARPA Robotics Challenge (DRC) allowed us to leave Willow and start OSRF in 2012. We were helped by a generous donation from Willow to cover our initial operating expenses, but it was the DRC contract that made it possible to build a team and create a sustainable organization around it. The DRC program alone supported ~10 people working on Gazebo and related projects for more than 3 years.", "When Willow was winding down in early 2013, we were able to hire the core ROS team and thereby take responsibility for ROS itself only because of a grant that we had through the National Robotics Initiative. The grant was 1/3 funded by the US Army, with the rest funded equally by NASA and the National Science Foundation (NSF). Taken together that funding supported ~4 people for 3 years to maintain ROS and to start work on ROS 2.", "It\u2019s impossible to know for sure, but it\u2019s reasonable to wonder whether ROS and/or Gazebo would have become orphaned projects had we not received that DOD funding at that time. Perhaps the community or another organization would have picked them up. But in my opinion, probably not.", "Since that time we\u2019ve worked (and continue to work) on a number of DOD-funded programs from various agencies. We\u2019ve never tried to hide that support; you can find 4 DOD agency logos on our ", ". We\u2019ve also never tried to hide the work that we do for those agencies. Quite the opposite: in all these programs the code and documentation that we develop is 100% open source. To my mind, that\u2019s the best possible use of public money that\u2019s paying for R&D: build technology that can be used by everyone, everywhere.", "In addition, the fact that our government funding, DOD and otherwise, is all channeled into open source products has an important consequence: we cannot work on anything that is classified or subject to export control. As you might imagine, this constraint rules out pretty much anything related to weapons or other offensive technology. We go further, judging each potential project based on the apparent intent of the people running the program and the likely application of the technology. Over the years we have declined many funding opportunities because we found them to disagree with our personal and organizational principles.", "Of course it would be naive in the extreme to imagine that technology that we develop and release won\u2019t ever be used in a manner that we disagree with. I\u2019m quite sure that somewhere out there are weaponized robots running ROS and being simulated with Gazebo. We can\u2019t stop that from happening and still make our software open source; clause 6 of the ", " says, \u201cThe license must not restrict anyone from making use of the program in a specific field of endeavor.\u201d But we can and do choose not to build those robots or simulations ourselves.", "Today, thanks in large part to the maturation of Gazebo and the growing promise of ROS 2, Open Robotics enjoys significant support from non-government sources. In 2013 government funding, primarily from DOD, made up more than 85% of our funding. In 2017 that figure was less than 25%. In the future could we move completely away from government funding? Perhaps, but it\u2019s worth considering all the implications: while we\u2019re very grateful for the support of our industry sponsors, they generally (and understandably) are asking us for near-term improvements and releases that will support their product or service offerings. Those priorities are vital to keep us grounded in the needs of today, but it\u2019s primarily government R&D programs that give us the time and flexibility to take on bigger, riskier, more speculative projects. And in the US, those programs are primarily funded by DOD. I\u2019d prefer to see agencies like NSF have a larger budget, but I\u2019m not in a position to make that happen.", "It\u2019s worth noting that this debate is an old one. For an eloquent argument on the other side, see ", " at Michigan. Ben doesn\u2019t just make the argument, he lives it, which I greatly respect.", " I would like to thank you for that great explanation. My first feeling was the same than ", ", but being able to see where OSRF came from and that you keep an effort on considering all implications helps on my peace of mind.", "Thanks ", " and ", " to put the issue on the table, and thanks ", " for your transparent answer (and for your final reference to Ben Kuipers).", "\nAs ", " says, I\u2019m now more peaceful with the subject.", "Yes, I\u2019m fine as well now. Thanks ", " for clarifying things. I knew some of the back story already, and I don\u2019t have a problem with OSRF taking money from DARPA. After all, DARPA funds a lot of US robotics projects without a direct military application. I am also aware that this funding is what kept the core ROS and Gazebo development team afloat, and that any open source technology can be appropriated for military purposes.", "I think where a line would be crossed (for me; everyone has to listen to their own conscience and draw their own line) would be if OSRF was directly contributing to weapons technology, or to a project with a thinly veiled pretense which would likely result in a weapon. That\u2019s the part of ", "\u2019s anwer that reassured me the most: that all of your technology (in the military context) remains open source, and that you\u2019re judging each potential project on a moral basis. As long as you stick to those principles, there\u2019s nothing to worry about. I think you really have to look at potential partner\u2019s background to distinguish between a project that\u2019s honestly trying to make a contribution to e.g. a Search and Rescue (SAR) robot, and one that\u2019s pretending to develop a SAR robot with the intent of repurposing the technology for Search and Destroy.", "The announcement of a dedicated \u201cROS Military\u201d initiative sounded a bit nefarious, but I guess I\u2019ll have to wait for ROSCon to find out.", "Anyway, let\u2019s not hijack this thread any further than we already have. It\u2019s really about the ROSCon program, and it looks awesome! ", "That\u2019s the part of ", "\u2019s anwer that reassured me the most: that all of your technology (in the military context) remains open source, and that you\u2019re judging each potential project on a moral basis.", "You\u2019ll notice that ", " did not specify what moral basis is being applied. ", " spend a lot of words to explain that taking funds from the DOD is morally ok (or not), which was however not the topic.", "The actual problem arises when OSRF actively supports ROS-Military. ROS MIlitary has the goal of improving technology that helps killing for political goals, and in particular do so for the US government. Due to that, it is political, it takes sides in global conflicts. It\u2019s not immoral for ROS-Military to exist, it is very transparent about it\u2019s goals, including the political aspects. Read the ROSCon program: \u201cThe United States military\u2026\u201d Could not be more transparent.", "But it is impossible to offer any support to ROS-Military without being political. OSRF can take all the money the DOD wishes to throw at it, OSRF can tolerate the existence of ROS-Military, but it should in no case directly support ROS.Military, and from the ROSCon program it seems that it crossed this line, and nothing in ", " s response even denied that.", "So any organisation in this world which thinks about sponsoring OSRF one way or the other now has to consider that OSRF helps ROS Military, and thus helps military operations inside and outside the US that protect the American petrol-fuelled way of life.", "This is different from taking DOD money or supporting civilian DOD funded projects (setting aside the arguments by Ben Kuipers for the sake of the argument).", "ROS-Military is not politically neutral, and it is not civilian. People will die thanks to ROS Military. People will die for political goals of the US. That\u2019s all fair enough to participate in for any individual at their discretion, but not for OSRF, IMHO.", "Hello everyone,", "It was quite hard for me to write a follow-up to ", "\u2019s last reply, so I exchanged some private messages with members of the program committee instead. Still, I fail to understand why ", ", ", " and ", " feel relieved by ", "\u2019s explanation and thus want to give my personal response here in public. I expect it represents the opinion of a significant fraction of the contributing ROS community.", "I agree with a lot of what ", " means to convey, but the way he puts it is neither productive nor easy to agree with. The discussion about whether or not OSRF should be funded by military organizations in the future (too) is a valid one, but it is per se unrelated to the program of ROSCon and thus does not belong here. I am sure we will see a lot of discussion on that at the conference and I believe constructive statements in a different thread are welcome.", "It seems there were a number of submissions for this ROSCon on military-related projects and the program committee had to decide whether or not they want to have these talks at the conference.", "\nLooking through the program, the outcome was to accept two talks with obvious military funding or background, the one by Alejandro R. Mosteo (Centro Universitario de la Defensa, Zaragoza) on Ada support for ROS2 and the one by Jerry Towler, apparently summarizing military projects for the US military.", "I object to the scheduled talk by Jerry Towler.", "The abstract does not describe any project with civil use cases (or any concrete project at all for that matter).", "\nAfter reading it many times, I would expect him", "Don\u2019t get me wrong, I believe this can be an interesting overview talk.", "\n``ROS-M\u2019\u2019 might even be an interesting movement, given they open-source all their code.", "\nI would like to see the presentation as a 20-minutes video on vimeo.", "\nBut I do not want it to be part of the ROSCon conference.", "ROSCon is an event to build and support lobbies and communities around ROS and software projects.", "\nThe committee chooses which lobbies are supported by deciding on talks, within the quality and number of submissions they receive.", "\nBut some communities and lobbies do not mix well.", "\nRyan Gariepy said at the end of last year\u2019s ROSCon something like", "\n\u201cWe would never have imagined to have a conference where we talk about robot coloring books and fully autonomous S-classes at the same event\u201d.", "\nImagining this year\u2019s statement additionally contains \u201cand autonomous convoy vehicles for border patrol\u201d, or anything comparable, makes me cringe.", "\nIn my opinion, military applications of ROS, though they obviously exist, have no place in the worldwide happy-go-lucky, lobby-supporting community that OSRF always advertises at their events.", "Discussing military applications in the field of autonomous robotics breaks a taboo for many academics - especially in places where funding does not in large part come from the domain. At least in Germany, there is a long-standing debate about the so-called ", ", which requires universities that accept it to reject all research and collaboration in projects with military objectives, sometimes even potential military applications.", "All in all, I do not oppose talks by military-related researchers or projects in general.", "\nThey might present novel concepts or software modules that are useful for many civil applications as well, and if they do not only focus on how military robotic systems can benefit from them, they are clearly interesting for every researcher.", "\nBut I believe the ROS community - to a significant proportion represented at (and by) ROSCon - should not feature military applications and a talk titled \u2018\u2018ROS-Military: Progress and Promise\u2019\u2019 crosses that line.", "\nThis has nothing to do with \u2018\u2018hiding objectionable projects\u2019\u2019, it is a matter of public support.", "After hearing the presentation at the conference, I fail to understand the decision by the program comitee altogether. Clearly, there have been alternative proposals with more content.", "There was not a single item of information in the presentation that would be relevant to anyone listening.", "\nThe organizers might want to reconsider how they advertise the event:", "ROSCon is a developers conference, in the model of PyCon and BoostCon. [\u2026] the two-day program will comprise technical talks and tutorials that will introduce you to new tools and libraries, as well as teach you more about the ones you already know.", "The only tangible outcome of the talk is that OSRF and SwRI now have a recorded presentation entry that demonstrates that ROS-Military is apparently a topic present in / and involving the community.", "\nThis might help in future management meetings that decide on project budgets.", "I appeal to the comitee to never accept non-concrete military-related talks in the name of the community again. The community, including almost every participant I talked to, disagrees.", "Hi ", " , everyone,", "Speaking frankly, the ROSCon Organizing Committee (OC) is disappointed in the misalignment between the proposal for the talk in question and the presented content, especially given the concerns which members of the community expressed publicly and privately in advance. We expect that all presenters keep in mind that the ROS community is a global and diverse community, and that certain topics, motivations, and even phrases are going to be received very differently when presenters leave their home countries and their local professional communities.", "We will be making some changes to our review process, but we will ", " be putting in place a blanket ban on any topics at this point in time. We believe that such a policy would be tantamount to ignoring the potential impact ROS may have on the world outside of our labs and workshops. Furthermore, it would be insufficient. Considering the rapidly changing pace of robotics development and deployment, it is all but certain that there will be equally controversial topics proposed in the near future which are unrelated to the military.", "The nuances present in these sensitive topics also mean that face-to-face discussions are more important, not less. Speaking for myself, I had several hours of discussions with community members afterwards, none of which would have happened if I was limited to Discourse. It should also be noted that many of the people whom I spoke with were not objecting to the ", " of the talk, but instead were more concerned with how it was presented. That also suggests to us that our focus should be on helping presenters improve how to express topics which are now far more controversial than the \"rosbuild vs. catkin\" debates found at the first ROSCons.", "The OC will reserve the right to request a substantially complete draft ", " the conference of any presentation which has at its foundation a controversial topic. This includes ", " talks which focus on military initiatives. If you would like an opinion in advance of submission, feel free to contact the ROSCon 2019 OC at the to-be-announced email address.", "We will be making some changes to our review process, but we will ", " be putting in place a blanket ban on any topics at this point in time. We believe that such a policy would be tantamount to ignoring the potential impact ROS may have on the world outside of our labs and workshops.", "+100 to this: personally I\u2019d rather know about something and not like it than ignore it and pretend it isn\u2019t happening.", "topics which are now far more controversial than the \u201crosbuild vs. catkin\u201d debates found at the first ROSCons.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["to introduce \u2018\u2018ROS-M\u2019\u2019, maybe comparable to ``ROS-I\u2019\u2019 but with more centralized funding,", "summarize some non-classified portions of US-military robotics projects in the past,", "show how they can benefit from using ROS,", "explain how they have interesting research problems also in uncontroversial scenarios,", "state that they will open-source everything", "and thus not touch weapon controls."], "url": "https://discourse.ros.org/t/roscon-2018-program-published/5537"},
{"title": "ROSCon 2019 Diversity Scholarships: Applications Open", "thread_contents": ["Whoever you are, whatever you do, and wherever you do it, if you\u2019re interested in ROS, then we want you to join us at ROSCon in Macau October 31st \u2013 November 1st!", "The ROSCon 2019 organizing committee aims for ", " to represent the entire ROS community, which is diverse and global. In addition to promoting technology that is open source, we strive to ensure that our community is as open and accessible as possible. Inclusion and diversity benefit the ROS ecosystem as a whole.", "To help reduce financial barriers to conference attendance, the ROSCon organizing committee is offering a number of scholarships to members of traditionally underrepresented groups in the robotics community. Thanks to the support of the program\u2019s sponsors, each scholarship includes one complimentary conference registration and three nights\u2019 accommodation shared with another recipient*. Limited travel support is available for participants whose travel to the conference would otherwise be infeasible**. Please note that all other expenses (including any visa requirements) will be the responsibility of the participant.", "*To maximize the impact of scholarship funds, scholarship recipients will be asked to share a room with another recipient. Under special circumstances alternative arrangements can be accommodated.", "**Participants will be responsible for covering their travel expenses up front, as the travel support will be provided after the conference has been attended.", "We invite applications from members of groups that have been traditionally underrepresented in the robotics community, including but not limited to: women, people in LGBTQIA communities, people with disabilities, people from racial and/or ethnic minorities in the robotics community, and people from developing nations who may not otherwise be able to attend ROSCon.", "Previous ROSCon Diversity Scholarship recipients are not eligible to re-apply.", "We are proud to share this feedback from past participants of the Diversity Program.", "The ROSCon Diversity Scholarship Program provided me with an opportunity that would have been completely impossible without it. I was able to attend my first robotics conference and feel empowered to keep working to try and make a positive impact on this community. Also, it was very encouraging to see so many companies stepping up to promote and enable diversity within their companies and the robotics community. Thank you!", "ROSCon has been an incredible experience. It is really encouraging to see that everyone in the robotics community is really welcoming and willing to share their expertise. I learned a lot and I met incredible people. This experience inspired and motivated me to contribute more to the robotics community and make a positive impact.", "We also have a blog post of the ", ".", "The ROSCon 2019 Diversity Program is made possible with generous support from the following sponsors:", "Acutronic Robotics", "Apex.AI", "Autoware", "Centre for Healthcare Assistive & Robotics Technology", "Changi General Hospital", "Locus Robotics", "Tier IV", "If your organization is interested in getting involved in the Diversity Program, please contact us: ", "To apply, fill out this ", " by ", ", describing how you are involved with ROS and the robotics community, and what you hope to get out of attending ROSCon. Scholarships will be awarded based on a combination of need and impact. Every applicant will be notified of the outcome of their application.", "For more information about ROSCon 2019, including the program, code of conduct, and childcare options, please see ", ".", "Thank you to the conference Platinum Sponsor, Amazon, and to our Gold Sponsors: Acutronic Robotics, ADLINK, Apex.AI, ClearPath Robotics, EProsima, Fetch Robotics, iRobot, Microsoft, Silexica, Rapyuta Robotics, ROBOTIS, Tier IV, Toyota Research Institute, and Ubuntu.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/roscon-2019-diversity-scholarships-applications-open/8856"},
{"title": "XEL Network first application + Distributing XEL devices 100 set for free in ROScon2018!", "thread_contents": ["Hi, all!", "We released a video about simple application using ", "We set up ROS2 topics for each XEL using the XEL Network GUI tool and created a mobile robot by creating ROS2 nodes that utilize these topics on the PC. This demo will be shown at ROScon2018.", "And,", "\nCelebrating our XEL Network Project Release, we will be distributing 100 sets for free in ROSCon2018.", "The configuration of this free set is as follows.", "\n", "The XEL Network dramatically reduces the learning curve in the use of sensors, ROS2 communication(DDS) and power monitoring, providing an easy-to-use experience.", "If you want to join the XEL Network project, you can always go with us through Github.", "\nWe release almost all the files related to project development for free.", "\nFor example, schematic, firmware source code, etc.", "\nIf we can contribute to creating interesting things, there is nothing more better than this.", "You can find more information from", "For our other similar project, please refer to ", "And, We specially thanks to ", " developers.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["GitHub Repositories :\n", "\n"], "url": "https://discourse.ros.org/t/xel-network-first-application-distributing-xel-devices-100-set-for-free-in-roscon2018/6115"},
{"title": "Introducing the XEL Network : Modular H/W ecosystem over ROS2", "thread_contents": ["Hi!", "I have brought new project news related to ", ".", "\nThe name of this project is ", " is a word derived from the robot actuator ", " and uses the characteristics of DYNAMIXEL communication.", "The ", " of this project is ", ".", "The ", " of this project are as follows.", "The main ", " of this are as follows.", "And our ", " is as follows.", "What has been developed so far is only a simple function, and it requires a lot of function addition, stabilization and optimization.", "But, as one of our goals is a complete open source hardware, ", ". So I post this here that this project is there and it has progressed to this extent.", "Here are the ", " that are currently open.", " on this project, please refer to the following wiki document.", "We specially thanks to ", " and ", " and ", ".", "This is very cool, thanks for sharing ", ". Good to see some additional effort in hardware modularity within the ROS ecosystem. Specially one powered by microcontrollers. I encourage you guys to look at ", ". We are working with a variety of partners to facilitate hardware interoperability and your contributions will be more than welcome.", "Also, this is extremely aligned to the ", " and I\u2019m looking forward to see further collaborations between your group and ours.", "Sounds nice! In order to connect to a broad range of more high-level robotics components you might also have a look at ", ". They even have an open call opening this February that could fund efforts to integrate your components in the ecosystem.", "Also, this is extremely aligned to the ", " and I\u2019m looking forward to see further collaborations between your group and ours.", "Hi, ", ",", "\nI was also very interested in micro-ROS.", "\nPersonally, just like ROS has set the standard for robot development, I hope that a standard will be created for embedded. So, I hope that projects like XEL Network and micro-ROS will contribute positively in this respect.", "In order to connect to a broad range of more high-level robotics components you might also have a look at ", "Hi, ", ",", "\nSounds nice! Thank you for your sharing!", "Great work ", "How dependent is this on the dynamixel protocol? Of course the hardware is, but do the message also contain specifics? btw, which messages are you using?", "Apropos, ", " already mentioned HRIM, which is their proposal on meta-data describing hardware (at least that\u2019s how I understood it ", "). I\u2019ve also seen other work which uses a big YAML file with meta-data. Interoperability and pluggability would greatly benefit if the community could agree on this. How about a meeting on this at the ROSCon?", "Hi, ", ".", "How dependent is this on the dynamixel protocol? Of course the hardware is, but do the message also contain specifics? btw, which messages are you using?", "We chose TTL, RS485 communication to use relatively inexpensive hardware. And since DYNAMIXEL, one of the actuators, uses serial interface and protocol is open, we applied it.", "\nBut finally, we plan to make other communications available through the abstraction layer.", "\nWe are planning Modbus as the second protocol to be implemented.", "\nThe current messages only support some standard messages, and there is a need to worry about adding and customizing messages.", "It is still in its infancy, so there is room for improvement. ", "In addition, ", ". The ROS2 related code (including microRTPS) used in TB3 is the same as XEL Network.", "\nWe want to contribute to ", " support.", "\nSpecifically, we plan to implement a library that can be used in Arduino(like rosserial_arduino).", "\nThe project name is ", "(tentative).", "\nCurrently, the target board is MEGA2560 (8bit) and ZERO (32bit).", "How about a meeting on this at the ROSCon?", "Of course! It would be nice if we had time to talk together.", "Thanks for the info on the protocol. btw, I ", " like the dynamixel protocol and think it\u2019s a great first choice. Just wondering about extensibility.", "Specifically, we plan to implement a library that can be used in Arduino(like rosserial_arduino).", "\nThe project name is ", " (tentative).", "\nCurrently, the target board is MEGA2560 (8bit) and ZERO (32bit).", "I know ", " You may remember I contacted you about that a few weeks ago, but I\u2019ve been on a long holiday in the meantime.", "Anyway, micro-ROS intends to produce both a ROS2 port, and a bare-metal/RTOS implementation. For bare-metal, we are using the rosserial blueprint, like you did.", "In fact, I think your work would be a great base for the bare-metal implementation. I\u2019ve had a look over the code in the OpenCR repository already, and since you\u2019re also using micro-RTPS, it would transfer nicely. We have a few different choices to also support RTOSes (optionally) and other transports, but otherwise, it\u2019s very close.", "\nThat\u2019s mainly what I wanted to discuss at ROSCon, but if you like, we can start earlier.", "I know ", " You may remember I contacted you about that a few weeks ago, but I\u2019ve been on a long holiday in the meantime.", "Of course, I remember you ", " Our country was a traditional holiday until today.", "We have a few different choices to also support RTOSes (optionally) and other transports, but otherwise, it\u2019s very close. That\u2019s mainly what I wanted to discuss at ROSCon, but if you like, we can start earlier.", "OpenCR uses serial only because of hardware limitation, but XEL Network also uses Ethernet and use FreeRTOS. Anyway, it\u2019s very close.", "\nTomorrow we will go out, so it would be nice to talk at ROScon. However, if you mail us your opinion, we will be able to communicate more smoothly.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " of Software Mechanism in Robot Development with Embedded Technology.", "\n", " and ", " robot development through the above goals.", "Support ROS2(DDS using microRTPS)", "Plug And Play", "Daisy chain connection", "Support ROS2 (Publisher and subscriber are implemented)", "Plug And Play (Implemented using the DYNAMIXEL protocol. Additional supplement needed.)", "More sophisticated real-time performance.", "Supports various message types.", "Support GUI program (XEL Manager)", "Support IDE likes Arduino for user customizing.", "Support multi topics on one device. (Currently, only one topic per one is possible.)"], "url": "https://discourse.ros.org/t/introducing-the-xel-network-modular-h-w-ecosystem-over-ros2/6050"},
{"title": "Announcing tf_remapper_cpp and static_transform_mux: More power to your TF!", "thread_contents": ["Have you ever run a ", " on a complex system with tens of nodes subscribing TF? Have you looked at the ", "? ", " Stop wasting energy and CPU cycles by moving to ", ". The API is a superset of tf_remap, so you won\u2019t need to alter anything but one line in your launch file. As a bonus you get a node that works natively with TF2, supports remapping of /tf_static (which the original tf_remap cannot do correctly), you get the ability to \u201cremove\u201d frames, and the node can also work bidirectionally (listen on both old and new TF topics).", "Have you ever faced weird problems with ", "? Yes, there is a big problem - ", "! It only publishes the last message, but ideally it should publish all so-far-seen static transforms. This is where ", " comes into play. Just launch this node early enough and it will take care of your static transforms. There\u2019s one limitation, though - you still have to replay the bag from the beginning (or just play a few seconds of it and then you can seek further if you keep static_transform_mux running).", "Give it a try and report if you find these packages useful!", "Happy transforming ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-tf-remapper-cpp-and-static-transform-mux-more-power-to-your-tf/9219"},
{"title": "[Beta Test] DYNAMIXEL Wizard 2.0", "thread_contents": ["Happy belated New Year!", "We hope you are having a great start of the year.", "Here\u2019s a good news for all of you especially advanced users and Linux users.", "\nAll new DYNAMIXEL Wizard 2.0 is about to be released, so we\u2019d like you to try it and tell us what you think.", "\nThe DYNAMIXEL Wizard 2.0 supports Linux and Windows, and going to support OS X as well in the near future.", "DYNAMIXEL Wizard 2.0 comes with various advanced features that will allow users to get the most out of DYNAMIXEL.", "Below is the list of advanced features \u201ccurrently\u201d implemented in the software.", "Please refer to the below eManual for more information.", "Software can be downloaded at below download link.", "Please feel free to report any bugs or issues.", "\nThanks!", "Well, on windows, it only downloads a file without file extension. This file cannot be run even when I rename it to have exe or msi extension\u2026", "\nYou might have downloaded Linux version.", "\nI just downloaded windows version and which has an .exe extension on it.", "Ah, you\u2019re right, I\u2019ve probably misclicked ", " Thanks", "Is this compatible with the U2D2 usb-serial adaptor sold by Trossen?", "\n", "(I am also back-feeding 12V with a power hub)", "I\u2019ve tried to get the wizard2 software to work on both Linux & Windows 10 with no luck.", "I also downloaded the RoboPlus software and DynamixelWizard1.0 it is not working on Windows10", "I tried scanning 1.0 & 2.0 and every possible Baud rate.", "I also have an Arbotix-M board, and I can use that to talk to the servos, so I know that the servos work.", "Hi ", ",", "ROBOTIS officially distributes U2D2 for DYNAMIXEL and it should work well as long as you have correctly installed the driver(usually Windows installs it automatically)", "\nCould you elaborate more on \u201cwhat did you do and how did it fail\u201d part?", "\nVideo or picture would be very helpful in this case.", "I think I might be experiencing hardware failure. the device was working fine yesterday. With dynamixelwizard2 the scan feature works and the lights blink in the U2D2 but it never finds a servo.", "\nAnd when I try to run dynamixel_workbench_single_manager package  I\u2019m getting \u201cincorrect status packet\u201d", ",", "\nWhen scanning for DYNAMIXEL do you see both TX(green), RX(blue) led on U2D2 blink?", "\nWhat DYNAMIXEL do you use?", "\nAlso, go to Options menu and make sure that you are selecting correct COM port and baudrates.", "Yes both lights blink but it never finds a servo. it doesn\u2019t matter if I connect to an XM540 or and XM430. I purchased another U2D2 and it can see the servos so there is something wrong with this unit", "It looks like there is an issue with the DPI setting in Qt.", "A packet log to be able to view some amount of lines (10000?) of rx/tx packets for diagnostic/debugging purposes would make a great addition.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Graph plotting feature : User can select Control Table items to plot data on the graph", "Packet Monitoring & Generation : User can easily generate packets and log TX/RX packets to analyze", "DYNAMIXEL Diagnosis : Enhanced diagnosis helps to figure out the problem of DYNAMIXEL.", "Enhanced Firmware Updates : Multiple DYNAMIXEL with different ID can be updated at the same time.", "Multi Baudrates, Ports are supported.", "English : ", "\n", "Korean : ", "\n", "Download for Linux : ", "\n", "Download for Windows : ", "\n"], "url": "https://discourse.ros.org/t/beta-test-dynamixel-wizard-2-0/7819"},
{"title": "[E-book] A Guide for Teaching ROS", "thread_contents": ["(Download the free e-book here: ", ")", "  is a very powerful system that  ", " . But at the same time, it is a difficult framework to learn. Even if there are many good tutorials around, the novelty of the concepts is hard for students.", "In this document, we propose you a method for teaching ROS fast ", ", and show you how to change your ROS classes from passive listening to active practicing.", "The method consists of going straight to the core ROS subjects and discards the rest. Also relies on tons of practice (with robots). We have divided the rest of the document into three parts:", "  The theory behind the method", "  How to manually implement the theory for your own courses", "  How to use an already working system online", "This guide is ideal for teachers who may need to prepare a syllabus for a summer ROS course, for a future semester, or for a robotics programming course.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/e-book-a-guide-for-teaching-ros/5828"},
{"title": "Preparing for Lunar Sync 2017-08-07", "thread_contents": ["We\u2019re close to having another set of packages available for lunar.", "\nThe only failing stack right now is ", " on Ubuntu Zesty and Debian Stretch. ", " ", "Given that there are almost 200 packages to sync and no major regression (only discrepancy between platforms), I\u2019m planning to sync soon. I\u2019ll wait a few days for a new release of navigation to come through if possible.", "Thanks to all maintainers releasing these packages in Lunar!", "I\u2019d like to push in a new release of ros_controllers as there were a few", "\nminor features that got added to indigo but not kinetic and lunar. There\u2019s", "\na test missing I have to add and it\u2019s ready for release.", "Navigation has been released (", " ", " ) and we have green jobs on all platforms.", " What is the expected time-frame for adding the test and releasing ", " ?", "The latest releases into Lunar are failing some tests on downstream packages: ", " Please hold the sync until those have been resolved.", "Please hold the sync until those have been resolved.", "Will do, thanks for the heads up", "I\u2019m releasing ", " in a few hours.", "There\u2019s also been a regression identified in kinetic that effects the newly released lunar navigation packages too: ", " It looks like some tests are failing in the new version of ", " (job ", ").", "\nIs it something that should be addressed before the sync ?", "Don\u2019t worry about it, I think we have to rethink how our tests are", "\norganized as running them is becoming somewhat unstable.", "The feature for which I added the test has been around for long on indigo", "\nand never had issues with the tests there.", "Don\u2019t worry about it, I think we have to rethink how our tests are", "organized as running them is becoming somewhat unstable.", "Sounds good ", "There are 2 issues holding this sync right now:", "The tinyxml2 issue has been fixed by hosting a patched tinyxml2 on ", "We\u2019re still waiting on some ros_comm regressions to be fixed and the revert of the SDL libraries in navigation before syncing", " fix has been released and ", " issues have been resolved.", "\nWaiting for the rebuild to complete and will sync afterwards (likely tomorrow)", "The ", " patch release is out: ", "I recommend to wait for a while to let the recent changes soak in the testing repo before the sync in order to allow more testing to happen to identify potential regressions.", "I recommend to wait for a while to let the recent changes soak in the testing repo before the sync in order to allow more testing to happen to identify potential regressions.", "Sounds good, Current state of packages has been in testing for five days.", "\nIs there any outstanding issue that I should be aware of ?", "\nOtherwise I\u2019m planning on syncing soon.", "Is there any outstanding issue that I should be aware of ?", "Not that I am aware of\u2026", "Waiting on the buildfarm to rebuild and will sync after that (likely tomorrow afternoon). Please hold merging lunar releases until then.", "Thanks!", "And \u2026Done! ", "\nThanks everyone for your efforts!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["navigation ", " ", "\n", "tinyxml2 broken on Yakkety, Zesty and Debian Stretch (tracked ", ")"], "url": "https://discourse.ros.org/t/preparing-for-lunar-sync-2017-08-07/2398"},
{"title": "Sphero RVR Needs ROS Interface", "thread_contents": [" has a new robot the RVR. This is a serious, robust, and capable small tank robot for a good price.", "Creating the ROS interface would be a good undergrad or advanced high school project. I\u2019d pursue it myself but have other activities over the next months that limit the time I can spend on it. Plus, bluntly, I don\u2019t know enough about ROS internals, working with URDF, etc to address this.", "Here\u2019s why I think the RVR should receive attention.", "It provides a serial port for control by another controller mounted on the chassis. The serial API provides access to control the motors and read gyro, accel, imu, speed, velocity, and location. Sphero is saying a 1st quarter 2020 release will provide access to the magnetometer and other drive functions. The API also accesses power and drive status, and controls LEDs. The Sphero link above provides access to the documentation on the API.", "Sphero has Python code that runs on the Raspberry Pi. I\u2019ve reverse engineered the protocol for C++. I\u2019ve run my code on an Up Board which is a quad core 1.7 Ghz SBC mounted on the RVR. My code is at ", ".", "If someone is interested I can be of some assistance.", "What do we need to do?", "\nRight now I am using roslibpy to publish topics to rosbridge since RVR is in Python 3 and kinetic and Melodic for Python 2 for the PI images I am using.", "I\u2019m not sure what is needed which is why I posted the message. Maybe someone can suggest needs to be done? Some thoughts / questions;", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Need to be able to use Move Base.", "Is Differential Drive suitable? It needs URDF definitions from looking at the code.", "How to get from Twist to the specific motor commands?"], "url": "https://discourse.ros.org/t/sphero-rvr-needs-ros-interface/12163"},
{"title": "\"Deterministic\" navigation in ROS", "thread_contents": ["We are working on a research project where we are investigating the ROS Navigation Stack in an industrial setting (", "). The partners in the project want autonomy to a certain level. For most areas in a plant, the behaviour of the robot must be predictable.", "We have been looking into commercial solutions such as Navitec and Bluebotics and they have graphical tools to guide the navigation behaviour. In certain areas, the robot is free to navigate. In other areas, the robot is only allowed to follow a virtual line. The tools are some sort of vector-drawing tools, where you can draw routes and areas on top of a map. The planners use this additional information to come up with appropriate paths.", "We are wondering if there are open source tools that can do these kind of things. We did not find them yet. If they are not there, it might be interesting to develop such a tool in the ROS ecosystem.", "It\u2019s very early days still, but we just opened up our in-progress \u201ctraffic editor\u201d for this type of thing. Documentation is currently non-existent, publicly-viewable examples are not there yet, etc., but it\u2019s coming along.", "You can specify a floorplan image, draw lanes on it, trace the walls if you want, and then export the path data to YAML and/or a simulation model for Gazebo. The GUI is built using QtWidgets in C++ and saves all the data to YAML. The exporters are Python scripts.", "Again, it\u2019s just a work-in-progress in a public repo, not a polished product. But I think this style of robot operations is becoming a common use case in many domains, so hopefully the editor can become useful.", "Contribute to osrf/traffic-editor development by creating an account on GitHub.", "This is just an editor for these paths; it doesn\u2019t touch the problem of actually following them with real robots.", "Cheers", "Thank you. Your description is close to what I was looking for, so I will definitely have a closer look at your repository. We want to plan our routes on top of the maps generated by Gmapping or Carthographer. But essentially, those are also just bitmaps. We want the following flow: the robot drives around and builds a map, then the operator draws routes/paths, restricted areas, and free-to-navigate areas, speed zones, etc. on top of that map, and finally, the robot uses that data as input to the planner.", "I think it is logical to separate the editor from the functionality of the planner. The interesting part is to have a common data format for the routes in YAML/XML/etc. Do you use a standard for storing the routes/paths? Currently, we have a group of students working on a planner that follows predefined paths.", "Ps. I tried to build your code, but I got some errors. I will have a look at it tomorrow.", "Currently the annotations are just stored in a YAML format of our own dreaming. There are Python \u201cgenerators\u201d in the repo which process that YAML into other \u201coutput formats\u201d such as Gazebo worlds (XML) or a \u201csimpler\u201d YAML format that is just the navigation data (lanes, etc.), intended to be consumed by nav stacks. We can create generators for any other formats or navstacks; I wasn\u2019t aware of standards for this, but if there are, we can certainly convert the data to whatever format is desired. It\u2019s just Python ", "I added a GitHub Action to the repo now which does an automatic build on Ubuntu 18.04 every code commit, so if the badge is green on the repo, it should build (at least on Ubuntu 18.04). Please create an issue ticket if you\u2019re seeing any build errors with details about your platform. Cheers!", "The build errors were my fault\u2026 I was first trying it out on an Ubuntu 16.04 machine. On 18.04, the project builds without problems.", "This application is really similar to what we have in mind. We are now playing around with it. The application really helps to discuss the requirements with colleagues and we are sharpening our user requirements. I will get in touch for next steps. I think it would be nice to join efforts to further develop the tool.", "Web-based GUI plz.", "Preferably as composable widgets.", "(It would make it easier to integrate the tool into application-specific UIs.)", "Yeah, maybe rev 2 ", "Currently we\u2019re running this tool \u201coffline\u201d to create map files and export various products (simulation models, navstack configs, etc.) in the local filesystem, so it would seem that a web-based approach would make things a fair bit more complex. But maybe that\u2019s just because I\u2019m a dinosaur.", "There is probably room for both \u201coffline\u201d and \u201conline\u201d editors in the ecosystem eventually.", "But maybe that\u2019s just because I\u2019m a dinosaur.", "You, like me, are a dinosaur.", "All the young mammals (including at my company) are running around building beautiful-looking UIs that run locally but are accessed through a web browser.", "I don\u2019t pretend to know how it works (OK, I do a little bit but just because I\u2019m curious), but I do know that they seem to whip up new UIs in a matter of hours using nothing but libraries with weird names they found on GitHub.", "I certainly agree that there is room for both. I\u2019m more interested in the (system) interfaces side\u2026 the data formats taken in and spat out, any online communication channels, etc.", "I am also ok with an off-line tool and personally I have most experience in that direction (Qt and Java). However, I agree that it would be nice to have web-based tools to edit the maps. I saw a demo of the MiR 100 robot and it looked like they already had a web-based UI for editing the map and for fleet management. However, when I look at their website I can\u2019t find anything about it.", "We also came up with two other features that might be interesting for this tool. One is support for .pgm files, because that is the format that ROS uses to store maps. The other is to support curves, because our robots need to have smooth trajectories and no sharp turns. This could also be solved at the planner level, but I think it is better to do that in the editor. I did some experiments in Java with curves and used 4 vertices to define bezier curves.", "Would it be interesting if I integrate the curves in your traffic-editor?", "I have written this very web-based application twice now for different companies, but both in proprietary formats.", "\nWhat you\u2019re looking for, if you don\u2019t want to write your own custom UI/system, is QGIS (", "). At its core, what you\u2019re looking to do is pretty standard GIS stuff. QGIS is designed for these kinds of spatial data workflows. It contains everything you need to digitize, manage, analyze, and ultimately serialize (eg. into GeoJSON) your robot \u201ctraffic plan\u201d.", "Also be aware of QGIS-ROS, which I wrote to help bridge QGIS into the ROS world (", ") ROSCon presentation linked in the readme.", "I strongly encourage trying to utilize available open source GIS tooling (that\u2019s been in development for decades) before deciding to make a ROS-specific flavouring of a subset of these tools.  These aren\u2019t novel spatial data authoring problems we\u2019re trying to solve.", "Thanks ", " ! Yes certainly, adding curves/splines would be great, since they are actually physically realizable by robots. We started off with straight-line segments just because they are super easy and because (in my limited experience) it seems that is what many/most companies are currently doing in their proprietary editors anyway.", "Thanks ", " for the feedback. Indeed, it seems that every robot company has their own internal proprietary editor. Thank you for the pointer to QGIS; it\u2019s an interesting idea to use a GIS system for this. I guess I had been stuck in a mental rut that \u201cGIS is for outdoor large-scale maps expressed in lat/lon,\u201d but I see the overlap with indoor mapping now. GIS seems particularly relevant for indoor+outdoor robot operations (deliveries to loading docks, etc.). I guess the purely-indoor, large-building domain still feels \u201ca bit different\u201d to me, in that multi-level buildings are so much more constrained than entire city maps, so an \u201cintentionally limited\u201d UI subset of something like QGIS might make the tool easier to use and the workflows more straightforward. The input (in my limited experience) is often a pile of PDF floorplans provided by the building operator, rather than satellite imagery or other traditional GIS data. But I\u2019ll definitely dive deeper into the GeoJSON RFC and QGIS to challenge those assumptions. Thanks for the pointers!", "Perhaps there is enough interest in this area to create a new ROS Discourse category called \u201cMulti-Robot Operations\u201d or something like that. It wouldn\u2019t necessarily be locked to a particular robot software platform (ROS1 / ROS2 / various other options) but instead about creating higher-level tools to deal with multiple robots sharing the same space, no matter what software is running on the robots themselves. I\u2019ll create a category proposal now and anyone interested can help evolve the definition and direction. Cheers!", "If you\u2019re going to consider GeoJSON as an option for interchange, just know that you can completely ignore long,lat in the spec and just pretend it\u2019s x,y. Been doing this for many projects well over a decade and can say it works fine, assuming you\u2019re utilizing tools that don\u2019t just assume a CRS like WGS84.  There\u2019s hundreds of geodata formats so pick what makes sense, but resist making your own. GeoJSON is great because of the countless tools and libraries already available for it.", "For QGIS all you need to do is set a custom CRS to a planar Cartesian system (just set everything to zeroes). Everything else just works including the extensive suite of raster and vector tools (see my ROSCon talk for examples of QGIS showing an indoor facility with robots in real-time).", "Orthorectified imagery or PDF floorplans are basically the same thing when you think about it. Set control points from known fiducials and begin drawing. Thinking all the way back to school, we\u2019d just rasterize the PDF and run it through QGIS akin to how one would an unrectified satellite/aerial image.Though often a SLAM map is used as the \u201cground reference\u201d and you draw on your vectors relative to it.", "Yes, there is a lot to be said about a limited UI, which is probably why I keep end up making them. Depends what your goals, timelines, etc. are.  QGIS may fill a gap shorter or longer term, and it\u2019s always a phenomenal analysis and data processing tool.", "Feel free to email me with any questions you\u2019ve got with getting started.", "Thanks! Ticket added: ", "The QGIS approach is definitely interesting. I think it is better to use existing tools if they are available. I will have a look at the software to see if it is possible to use in our projects.", "Currently, we are aiming at ", " as the fleet/traffic manager, because it is open source and one of our partners already has experience with it. However, OpenTCS manages on an very abstract level. It uses a graph of the environment and there is no direct connection between the information in OpenTCS and, for example, a planner in the ROS Navigation Stack.", "Our current approach is to have one data set with maps based on sensor data (from GMapping or Carthographer) with an overlay of the paths and areas where the robot can navigate. From that data set  we can export graph data to OpenTCS and use the whole structure in ROS Navigation.", "It\u2019s been over 8 years since I was involved in anything that used GIS, but I do recall that back then the GIS community was making a ", " big push into indoor GIS. A quick Google search turned up plenty of results so I\u2019m guessing the community didn\u2019t give up. I can\u2019t remember details, unfortunately, but there were open format specifications for buildings and built spaces and things like that.", "I dived into the QGIS software and it really has a steep learning curve. However, based on my experiences it definitely has the functionality to draw vectors on top of raster images. That was what I was looking for. I still have to look into the interoperability between QGIS projects and ROS. Thank you ", " for the pointer to QGIS-ROS.", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "I am still interested in your view on the last part of my previous post. We are looking into data exchange between different navigation systems. Specifically, we want to use OpenTCS with our our robots that are running ROS. The standard is really targetted at this topic. However, the standard is not open source and I can\u2019t find fleetmanagers implementing the standard.", "I found a question on this topic on answers: ", " They have a similar conclusion: not open source and no implementations.", "In our project we need functionality to exchange map data. So, we can implement it according to a standard or we can build our own custom solution. At the moment, I don\u2019t know the best solution yet.", "Word of warning, below is a textwall that may be interesting to some, but may not answer the posters question. Read at your own peril.", "I did some research as part off my Master thesis where I created a map using JOSM (java editor for OpenStreetMap) using indoor mapping plugins and then hosted the database (map nodes and relations) locally (could also be hosted online). A simple python node can then be used to query this database using overpass API. I put in information like different floors, hallways, door colours, which sensors could be used in which area. It was all kind of experimental, but technically possible. It gave me a multi-layered map containing the low level x-,y-, occupancy information, a low level topological map, high level topological map all the way up to a high-level semantic map. I never got to the point of actually using the map for navigation though ", " (the mapping effort was luckily enough to graduate on).", "The advantage of creating such a map is that the robot does not have to think a lot. All of the information about traffic rules and which methods of localization and navigation to use in which area are all embedded in the map. The obvious disadvantage is the insane mapping effort required to get such a map. Additionally there is no standard way to this that I am aware of. I came up with my own model, my own hierarchy and own key-value combinations. I just wanted to share my experience and point out that OSM can be used for indoor mapping as well and that OSM has a pretty big open source community so lots of tools and plugins are available.", "Another student from Germany was working on the same project at the time.", "\nThis could be an interesting read for you (summary of a couple of slides):", "\n", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "There\u2019s nothing to stop you using a closed standard to design open-source software, except in rare situations (such as the AUTOSAR specifications) where the license explicitly forbids it.", "The catch is that only people who have access to the standard will understand your design decisions.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/deterministic-navigation-in-ros/11442"},
{"title": "ROS-A Focused Initiative for Agriculture", "thread_contents": ["Greetings,", "ROS-A is a focused initiative for using ROS in Agriculture.", "The goal is to make close loop autonomous farming a reality.", "We are working on data collection and agricultural data standards.  If you would like to participate in the project we will have a breakout session at ROSCon 2017 to discuss the ROS-A standards.", "Follow us online Twitter ", " or signup to join the conversation at ", ".", "Thank you for supporting open source robotics,", "Matt Droter", "\nFounder", "Tel: 866 565-3025", "\n", "\n", "Hi. I\u2019ve looked at the site and am interested, but am wondering what I\u2019m \u2018signing up\u2019 for. What sort of platform do you use to host the discussion? A forum?", "Additionally: are you primarily focusing on the US, or are users from other countries also welcome?", "Hi,", "Right now we are setup on Slack at ", "US based but this is a global challenge.  Everyone is welcome to participate.", "If you are interested in starting a ROS-A meetup in your location contact us.", "Matt", "I\u2019ve done a lot of work in greenhouse automation and control. As a part of this I\u2019ve done automatic irrigation based on sensor data, HVAC control, supplemental lighting and other things (control of fan rates and pumps).  Is there work being done yet on this front within ROS-A?  If not, is there interest in it under the ROS-A project?", "Kevron,", "There is interest in your project. Can you and I get in touch with each other directly, to discuss possibilities?", "Bill Mania", "Sure.  Can you PM me and we can discuss how to discuss more?", "Hi ", ",", "\nThis is a very interesting initiative. I would like to join and contribute to the project. I am currently involved in a reasonable amount of farming/forestry projects.", "A new SIG datasets group is now active and WE NEED YOU. Please join us on our ROS-A slack -> ", ". Start here ", "I would love to lurk and potentially help out on this effort. I have some experience putting together large forestry data sets and I would love to figure out how to dovetail this effort with the perception working group.", "AFAICT I need someone to invite me to the slack group.", "If our proposal for a new category is accepted, we will keep track of these initiatives in ", " .", "I\u2019ve just posted: ", ".", " ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-a-focused-initiative-for-agriculture/2563"},
{"title": "Guidelines for asking for new package releases", "thread_contents": ["ROS is a large diverse ecosystem but on thing that makes it very powerful is the availability of binary packages on the common supported platforms. It is a large effort to make this happen and much of it comes from volunteers throughout the community.", "Many in the community are not aware of the process for making releases. Here\u2019s a quick summary for background.", "Each time a rosdistro comes out we start from scratch and release packages in dependncy order. A rosdistro release is considered complete and ready to announce when all packages contained in the standard metapackages are available. The metapackages are defined in ROS Enhancement Proposals(", ", most recently in ", "However the packages covered in the metapackages is only a small subset of the larger ecosystem. From those several hundered packages rosdistros can grow to several thousand packages. This process is an incremental process and over time the number of packages in a rosdistro grows as more maintainers make releases. In every release announcement there are almost always new packages made available together with the upgrades. F", "The process is slow because it requires maintainers to release in dependency order. Depending on how many other packages a given package depends upon it may not be possible to try to release it or even test it. This can be especially slow if an underlying dependency has changed and requires code refactoring, and not just running the release process. An example is something like the change from Qt4 to Qt5. all gui packages are in the process of transitioning which is non-trivial work.", "ully integrated systems such as support for a robot like the TurtleBot often come out a few months after the initial rosdistro release date because it needs to wait for all it\u2019s dependencies to be released, and then it will need to be tested to be working before it is released. In this last cycle we pushed hard on the TurtleBot to get it out early, but it\u2019s a comparatively small depndency list compared to the PR2 which often is a year or more behind before it\u2019s fully released on a rosdistro.", "When you make a request keep in mind the standard etiquette guidelines for all ROS communications. ", "\nIt is important to keep in mind that maintainership is done by volunteers throughout the community.", "It\u2019s important to express your interest in a certain package being released but respect that everyone else in the community has other responsibilities and deadlines. Often a simple request is enough to get a package released. A polite request which acknowledges how you will value the work that you are asking the maintainer to do will be way more effective. And if you can help the maintainer prepare the release and test the results that decreases the amount of work that the maintainer needs to do.", "It\u2019s recommended to make requests for packaging as github issues on the ROS package. Sometimes it may need to be on a release repository if it\u2019s a 3rdparty package being wrapped into a ROS package. (For example opencv3)", "If you\u2019re interested in a collection of packages and there may need to be coordination between multiple maintainers the best place is the ROS Release categories on ", " (for example kinetic: ", "You can also raise awareness in the ROS release category if there has not been a response from the github issues. Sometimes email addresses have changed or notifications are not being recieved for other reasons.", "When you\u2019re making a request it can be very helpful to assist the maintainer. Here are some things that can help the maintainer.", "Please note that a maintainer is responsible for shepharding packages through the release process as well as merging validated patches. If you want new features other active development please consider contributing yourself. In the ROS community there is often a correlation between developers and package maintainers, however it is not necessarily the case, and is actually different than many other communities. Also even if the original developer is still the maintainer they may have other priorities and may not be able to focus development efforts on that package at the moment. So if there\u2019s something blocking the release due to an incompatibility or failure pull requests to fix the problem can also significantly expedite the proccess.", "Thank you ", " for clarification (and elaboration).", "Perhaps this should be worth posted on wiki (in case of changes in the future) and linked from ", "?", "I picked this as a location since this guideline is only locally relevant to people posting in this category.", "For visibility I\u2019ve pinned it to this category and linked it from the category overview pages for this and the distro specific sub-categories. If people don\u2019t read the pinned topics I doubt they will also find the generic support guidelines.", "If we need to make changes in the future, as people build up reputation in this site they will gain the ability to edit this and many other things. Currently at least the moderators and admins all have access. If someone would like to suggest a change and doesn\u2019t have access please just reply and someone with access can review and update the top post.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Verify that all dependencies are already released. (If not follow up to find what other releases need to be made first and help with them.)", "Test the to be released package from source and verify that it works correctly on all the target platforms. (This is one of the hardest parts of being a maintainer. As a maintainer you want to make sure that the package is working correctly before you release it. This is especially hard for hardware drivers where you also need access to the hardware to test.)", "Run prerelease tests to test and validate that new releases will work."], "url": "https://discourse.ros.org/t/guidelines-for-asking-for-new-package-releases/618"},
{"title": "Maintainer best practices handling changes through ROS releases", "thread_contents": ["While I\u2019ve been working in the different pull requests in gazebo_ros_pkgs I come up with doubts about how would be the best policy to handle changes in the different branches that we host there for every ROS release. As of now gazebo_ros_pkgs currently support changes in: Indigo, Jade and Kinetic. All of them officially released and supported. Contributors are submitting pull requests against any of the released branches and we usually cherry-pick the changes to the rest of the branches but the whole process is arbitrary and not documented in any way.", "I can see that we have different kind of changes that will impact on users, at least:", "My main point is: in which ROS release would be reasonable to introduce a change that forces consumers of the ROS packages to have to react to adapt to the change having in mind that some of them would be not trivial to detect?", "Thanks", "Some of my ideas: try to define \u201cvirtual types of distributions\u201d that match stability vs new features. We could consider an \u201cstable release\u201d and \u201cdevelopment release\u201d and how these concepts fit into the ROS releases:", "At this moment I would say that ", " would fit perfectly in what we consider a stable release. I\u2019m not so sure about ", ". Probably ", " can fit well into what we can consider a development release. Obviously the upcoming ", " falls into this category.", "Reviewing the types of changes that I listed in my first post:", ": if they fix things clearly broken, stable and development.", ": clearly unacceptable for stable. If we consider ", " as a development release they probably are acceptable although they are have a large impact.", ": given the position of ROS in this topic, they should not represent a problem except for people relying on mixing packages and catkin builds. I would probably exclude stable of this.", ": in most the cases the impact could be really important, even worse than modifying the API since problems could not be directly visible.", ": this category or group is difficult to manage. Take ", " as a good example of how a clear bug fix needs to be handle with care. I would say that have a case by case analysis and decision is a good way but by default anything touching a behaviour in an stable release (even if clearly fix it in the right way) should not be accepted.", "There are other models if we define other \u201cvirtual types\u201d of distributions (for example having ", " as a ", " before changes landed into ", ") but I would like to keep the whole thing somehow simple.", " FYI I bumped this into the Release category as it\u2019s best targeted at maintainers.", "And there\u2019s details per distro here: ", " with LTS vs on-LTS. Note that Kinetic is also an LTS.", "Our original versioning policy was here: ", " but we\u2019ve relaxed this as it was too burdensome.", "As a general guideline I like to refer people to ", "I\u2019ll note that our convention is a little bit different that Semantic Versioning in that we typically use patch releases for bug fixes as well as added functionality, as long as it is backwards compatible. And iterate the MINOR version between ROS distros. This is something we might want to consider changing, and suggesting that we follow the slightly more strict guidelines developed since then in semantic versioning. And then we could recommend that only major version changes go into new distros for stable packages.", "One of the challenges is that we have a lot of packages in the greater ROS ecosystem and they are not all at the same level of maturity.", "At the core we work very hard to maintain full ABI compatibility within any ROS distro for the life of the distro. We also make sure that any API changes have a full deprecation cycle of at least one ROS distro before an API is removed.", "However, there are many users using ROS packages to collaborate fluidly on short term projects and need to have more flexibility. Binary builds are a very powerful tool for easy deployment and collaboration, and these faster moving projects cannot maintain the stability while still being productive. As such we do not mandate a specific policy for all packages. ", " We do have a few states in the rosdistro status field defined in ", " but it\u2019s not really adequate. Better communication of the maturity would be helpful. This was  discussed in the package.xml format discussions but could not reach consensus so was left out of the specification.", "We also try to manage expectations by only promoting packages to be inside the ", " and ", " if we consider them stable.", "As a rule of thumb the more dependencies any package has the more stable it\u2019s development process should be. As every incompatible change requires all dependent packages to update.", "Our buildfarm makes no assumptions about ABI stability for any given release, and can successfully rebuild everything if there\u2019s an ABI change. However, this may break a user who only does a partial system update. We have discussed explicitly versioning the ABI and in the future would like to be able to be  more explicit about it. But ABI versioning has many different approaches and we could not find a consensus during the discussions so deferred the decision.", "At this moment I would say that Indigo would fit perfectly in what we consider a stable release. I\u2019m not so sure about Jade. Probably Kinetic can fit well into what we can consider a development release. Obviously the upcoming Lunar falls into this category.", "+1. But there should also be a point where ", " joins the stable-LTS family. I don\u2019t know when and how that\u2019ll be happen though. Maybe when the number of users of Kinetic surpasses that of Indigo?", "Another concern in this subject I\u2019d like to raise, which ", " did not ", ", is about the new capabilities. While the addition could still cause regression even when AB/PI compatibility is carefully maintained, IMO it\u2019s natural for the most of developers/users to think that new features will become available to the older supported distros. I\u2019ve just seen some examples where new features added to kinetic branch were not backported to that of indigo. I guess the final decision may always be up to the package maintainers but it would be great if discussion/guidance at the core level (like on this forum) is available.", "Formerly, the rule was simple: introduce new features in a new distro, so existing users are not impacted.", "With Indigo LTS, I found myself wanting to support new hardware that was not available when Indigo was initially released. An example is the popular Velodyne VLP-16 LIDAR.", "Since I believe I can add that model with minimal disruption to existing users, I\u2019ve decided to make an exception to the old rule, which made more sense when there was a new ROS distro every six months.", "This discussion is added to ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Bugs (mainly things that are broken)", "Changes to the API", "Changes to the ABI (looking at catkin ws users)", "Changes that affect rostopic/roslaunch/rosparams/\u2026 names or arguments.", "Changes that affect behaviour (some of them are bug fixes)", "\n", " most of the users, any disruptive change should not be permitted only important bug fixes to be committed.", "\n", " brave users expecting fast new features and able to adapt quickly to changes.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/maintainer-best-practices-handling-changes-through-ros-releases/771"},
{"title": "FXIMU Sensor for ROS - Open Source Project", "thread_contents": ["Greetings;", "We made an open source imu sensor for ros, that utilizes FXOS8700 and FXAS21002 sensors from NXP semiconductors, coupled with a Texas Instruments TM4C123GH6PM Arm Cortex based MCU.", "The set of sensors from the NXP semiconductors is said to have 100 times better noise characteristics, then what is available on the market.", "We have ported ROS\u2019s imu_complementary_filter code to run on the TM4C123GH6PM platform, to achieve:", "Since the complementary filter is running at an embedded level, there is nearly no latency. It attaches directly and powered by USB port.", "Calibration is done by putting the unit in calibration mode using the params file, and running a set of scripts provided, the ros publisher is turned into normal serial data forwareded to another virtual serial port using the program socat. So once the values are forwarded to virtual serial port in ASCII format, one can use off the shelf calibration utilities, such as Paul Stoffregen\u2019s MotionCal. Basically you can calibrate your sensor, without taking it off from the robot, or updating firmware,", "We are working on a V2 of the board, that includes cosmetic and ergonomic changes, and the addition of a i2c port.", "It runs extremely well, and has more space for development. I built it for a robot I was building, then noticed it could be useful to others, so decided to develop this as a seperate project. Because the filter is fed at 400hz after being read from i2c, it offers nearly zero latency.", "I have worked with various IMUs (I will not name them here.), including ones that include on-board digital motion processors, and the results we are getting are much better than what is available on the market that is the same class of device.", "Souirce code available at ", "Prior version source code available at ", " and ", "All are welcome to parrallel develop this project, and I provide circuit boards for people who can solder SMD components, along with montage diagrams and BOM. Right now github does not contain schematics nor gerbers, but just because I am building a new version with slight improvements.", "We are also asking for the help of ROS community to further develop and maybe fund this project so we can have them produced in SeedStudio or allike manufacturing house.", "People who like to replicate results are also welcome and will be helped.", "Best Regards,", "\nCan Altineller", "Hello Can,", "I follow your IMU project on different platforms. I would be happy to learn more, especially how to get to such a board or how to create one.", "\nYou say that there are no schematics and gerber-files on Git, because you are working on a new design \u2026", "Is it possible to get the \u201cold\u201d design? I would then check to see if I can etch myself a pcb.", "Best regards,", "\nThomas", "This is fantastic. Just to clarify this IMU supports ROS 1, ROS 2, both? It isn\u2019t immediately apparent in the read me?", "I am really curious about what sort of help you would need to produce these boards and what sort of quantity you would like to produce them at? Is the $20 price a retail price or production price at low quantity? Do you plan to make the boards open hardware, not just open source?", "This is currently ROS1, but I would love to port it to ROS2 as well, with the help of the community.", "20$ is just for parts, at low quantity, i.e. 1 by 1. If produced  by the 1000\u2019s, I would say it would cost 12-15USD.", "I plan to make everything opensource, both hardware and software, and I plan to make and sell the boards myself, and looking for collaborators to maybe pay for production costs at a pcb production house.", "But of course, first some other people need to verify my work and see if it is actually useful.", "Best regards,", "\nCan", "Hello ", "Per your request, I am posting them right now.", "Best regards,", "\nCan", "the hardware files are at ", "This is outstanding. How does it compare to the BNO085?", "Indeed, the BNO055 is very robust against magnetic interference. I wonder if anyone knows the algorithm behind that.", "I tested a BNO055 once. It has drift problem, fximu does not. precision is higher for the chipset I am using.", "Also you can not modify the digital motion processing on the BNO055.", "mpu9150 and bno055 was two chipsets that I was disappointed with the DMP, my original plan was to use a bno055, and dont do a lot of processing in my main loop - the bno055 did not work out for me, so I developed a board with a seperate processor, which I could do the filtering myself.", "In this filter, roll and pitch are not affected by magnetic interference, and it has magnetic interference rejection according to video in ", "I however tested this and found out it is not exactly the same as the video in the original code.", "Setting the mag gain low, protects againist magnetic interference, but not we have the filter code opensourced, we can maybe work on this all together, and make the magnetic interference rejection better.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Up to 400Hz read of acceletometer, magnetometer and gyro sensors", "Up to 400Hz update of the complementary filter", "Output is published at /imu/data and /imu/mag as sensor_msgs/Imu and sensor_msg/MagneticField message types, and output rate is determined by output_rate_divider parameter", "Hard iron and soft iron correction", "Everything is configurable by rosparams, from hard iron soft iron calibration to sensor_read rate, output_rate_divider, to gyro and accel sensor parameters, and all the parameters in imu_complementary_filter. (with the exception of tf) See: ", "\n", "Green led is on when at steady state", "It requires no serial driver, and uses no serial ftdi, but rather emulates a virtual serial port as a usb hid device, provided with the TM4C123", "It has an expansion port that can be used as serial, or analog input,  or gpio. PA0 to PA7 on TM4C123GH6PM", "It costs 20USD to build in components, and is open source."], "url": "https://discourse.ros.org/t/fximu-sensor-for-ros-open-source-project/11339"},
{"title": "Weekend Hardware Happy Hour!", "thread_contents": ["We just wrapped up our first full week of the year and many of us are finally getting the chance to hack on the hardware that we picked up over the holidays. In this thread I would like to know what sort of hardware people received over the holidays, what they did with it (or plan to do with it), and how it all went. If the holidays meant you got a solid week to write code and tinker we would love to know what you made.", "I\u2019ll go first. Over the holiday I picked up a used Oculus Dev Kit 2. I want to see if I can get video from my TurtleBot to stream to the Oculus. My stretch goal is to see if I can get stereo pair of cameras that roughly match the stereo baseline of the human visual system. For the holidays I also received a vintage set of letter stamps that I want to use to bling out my TurtleBot with a brass name badge.", "I wrote some ROS2 drivers for the Ouster lidars! ", "Also bought an Anki Vector. Not really a project but also pictured. Wanted to use it for algorithm development. Ended up annoying me ", " much it stays powered off on my desk.", "Video showing basic setup (feel free to subscribe ", " I\u2019m posting a set of videos on depth cameras and CES soonish ).", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/weekend-hardware-happy-hour/12261"},
{"title": "Perception control unit", "thread_contents": ["We are looking to design platforms which can be used to support (multiple) Lidar and camera based vision subsystem with auto-grade heterogeneous socs, sufficient computing power to handle in-car network communication as well as interfacing with ECU through on board MCU.", "Hey ", "  it is a little unclear what should the people respond here.", "\nAre you looking for algorithms folks to give you requirements?", "Do you want feedback on above sketch?", "Etc?", "Sorry for late response - just got back from travelling.", "Yes, we are looking for two groups of feedback:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Requirements from algorithms and system integration folks regarding the hardware", "Feedback from silicon vendors for chips we can use"], "url": "https://discourse.ros.org/t/perception-control-unit/8472"},
{"title": "Calibration Toolkit: Can't extract points from the grapped lidar for camera to velodyne calibration", "thread_contents": ["Hi Guys,", "Thanks for reading my issue.", "I am new in Autoware and trying to make camera to lidar calibration but I can\u2019t. once I grab a frame and narrow the grapped lidar frame to the Chessboard I can\u2019t specify the area of the chessboard as the green circle not always show up and it it show up and press left button of the mouse it just give me like a dot , and can\u2019t specify area and therefore the calibration fails. I followed the steps on the following link but he can mask any region he want but I can\u2019t: ", "I am using linux ubuntu 16.o4, Autoware version: 1.7, Lidar: Velodyne-16, Camera: mindvision", "Many Thanks", " we removed the calibration toolkit due to many of the problems we had.", "\nWe recommend you try the alternate camera-lidar calibrator.", "Thanks ", " for replay,", "I followed the steps in ", "but still can\u2019t get results. firstly I got the following error:", "I can\u2019t 1. Observe the image and the point cloud simultaneously. like in the following picture", "Thanx", " Seems you haven\u2019t performed both parts of the ", ".", "First, obtain the intrinsic parameters for your camera using the ", "Once you got obtained and saved  the file. You can proceed to ", ".", "Finally, with the complete file, don\u2019t forget to \u201cRegister Camera-Lidar TF\u201d when using in the calibration.publisher", "Thank you ", " ;", "solved the problem, the problem was in camera topic subscription.", "Thanks alot.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/calibration-toolkit-cant-extract-points-from-the-grapped-lidar-for-camera-to-velodyne-calibration/8940"},
{"title": "[WG RP] Autoware Reference Platform Definition Documentation", "thread_contents": ["This document is presenting the intents of the Autoware Foundation (AWF) Reference Platform Working Group (RP-WG). This is a live document, providing to reader a view of the purpose, plan and orientation of the Working Group. This document has to be reviewed by all participants to the RP-WG and will be submitted to the Technical Steering Committee on regular basis.", "Autoware Reference Platform Working Group  ", "Document Information  ", "Use Cases and Configurations  ", "Features  ", "Roadmap  ", "Reference Software Platform  ", "Reference Hardware Platform  ", "In July 2019, AWF TSC launched the creation of Working Groups in order to focus technical topics and oversight them (", "). The Reference Platform WG (RP-WG) has been announced at the same time (", ").", "Co-leaded by Autocore and Kalray representatives, it is calling for participants to contribute to the different activities to be covered:", "Define the initial Reference Platform: hardware and software-wise", "Work on roadmap for alignment of hardware and software with Autoware.AI and Autoware.Auto", "Establish and maintain a roadmap longer term", "Contribute for hardware and software integration", "The principles of the RP-WG can be summarized as being:", "Blockquote The ultimate goal for AWF is to be able to present to its users a system that he can acquire and use for creating prototype vehicles and reduce his time to market for production. It will contain best in class compute platform, including accelerator capabilities to run in an optimized way the latest Autoware.Auto release.", "Several intermediate steps prior achieving and maintaining this goal will be necessary. This is one of the purpose of this Working Group to define these steps and perform them.", "This is working document, regularly updates", "Some parts are are under investigation, under discussion by the Working Group.", "Draft:", "P1 provide CAN/UART capability device drivers (ROS/ROS2) and sensor data synchronize.", "P2 provide LVDS capability and CV/Fusion features for camera data.", "P3 running autoware.ai stack.", "P4 running autoware.auto stack.", "About deployment of P1-P4:", "Support different deployment strategy with different computer platform configuration.", "You can put P1-P4 together in one powerful computer.", "Can also deploy different features in different computer unit.", "Generally, P1 should compatible vehicle network including AutoSAR Stack", "If using multi boards solution, use ETH or TSN as network solution.", "CNN accelerator can be a part of P2/P3/P4 as algorithm needs.", "Opens:", "To be studied", "Will be aligned with Autoware.AI (first) and Autoware.Auto features as suppoted by Hardware.", "To be studied", "The AWF Reference Platform will use Autoware Software.", "Initial platform will rely on Autoware.AI  and support Autoware.auto from early stage.", "To be align roadmap SW/HW", "Demo/dev car configuration of our current autoware.auto development team(Apex)", " : Lexus 450 LH with the ", "DBW interface", " :", "4 ", "(or comparable sensors, e.g. VLP-32C)", "16 Sonar sensors", "4 ", "(180 degree FOV)", " :", "aarch64 computer", "rugged x86-64 desktop computer", "The Kalray MPPA (Massively Parallel Processors Array) is a Manycore architecture allowing to executing compute and acceleration algorithms in an optimized and low power consumption.", "The integration with Autoware.AI consists as of today in offloading the following algorithms onto the MPPA", "Apollo 5.0 sensor configuration:", "\n", "Hardware deployment:", "For clarification, the reference software is the responsibility of the Autoware working group. This working group\u2019s responsibility extends only as far as software drivers for ECU-specific hardware. For anything else, please coordinate with the Autoware working group.", "Fully agreed ", ", no confusion on this point.", "we proposal several different kinds of node (P1-P4) for below reasons:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Moving market to benefit from Autoware Eco-System\n", "From Experimentation to Reference Implementation to Production", "Autoware members have all bricks for Reference Implementation Step", "Experimentation Step is on-going", "Let\u2019s move to Reference Implementation Step", "Define it, plan it, make it", "All Autoware members have a brick", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Sensors supported: need to discuss with community, need to know sensor configurations of different useage.", "Support boards: need to discuss with community.\n", "Current Autoware.auto support IPC and Xavier. It\u2019s nvidia GPU solution.", "Heterogeneous solution: 96boards automotive and standalone accelerator (Autocore and Kalray)*", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Reference Accelerator Boards\n", "PCIe boards with MPPA\u00ae2 Generation, MPPA\u00ae3 upcoming", "Accelerator daughter board modules", "\n", "Reference Programming Software\n", "CNN Inference Optimizer", "CNN Inference Runtime Engine (additional algo support and growing)", "Computer Vision OpenCV", "OpenCL 1.2", "\n", "Hardware Integration\n", "Intel and ARM hosts", "\n", "Software Integration\n", "Additional Use Cases support (Companion, StandAlone)-to simplify System integration", "Linux and Autoware Member RTOS supported", "\n", "Image Detector\n", "CNN based object detection (car/person/\u2026)", "Autoware uses Darknet(YOLO networks)/SSD/RCNN project", "SSD and Yolo V3 CNN have been tested within Autoware, using Kalray inference engine solution on MPPA\u00ae", "Other CNN can also be used", "\n", "LiDAR Localization\n", "Based on \u201cNDT matching\u201d algorithm from PCL library", "Compute car coordinates by matching its LIDAR data to a precomputed map", "Fully implemented on MPPA\u00ae (both initialization and runtime functions)", "\n", "considering next gen E/E arch of vehicle.\n", "P1 could be a gateway device which support different connections and can support gerneral sensor message interface(driver).", "P2 could be a smart camera which are used in current ADAS system. most of them are based on vision purpose SoC.", "P3/P4 are general purpose computing unit.", "different unit target different ASIL level in plan.", "P1(Gateway) and P2(Smart Cam) target ASIL B.", "P3/P4 should support hardware redundancy (as request) and has ASIL D decision maker unit(MCU) inside.", "\n", "enhanced P1/P2 which can deploy some peception features will be benifical to HCP system\u2019s algorithm optimzation. can left the optimize work to device provider.\n", "for example, P1 can filter and fusion lidar data by dedicate hardware package accelerator.", "\n", "if user are using x86 IPC, everything should be working fine and no difference.", "Software requirements for Autoware WG:\n", "code organization (repo split should be better, I think)", "build standard message spec/definiation. it should be very useful for realization of special hardware/accelerator.", "\n"], "url": "https://discourse.ros.org/t/wg-rp-autoware-reference-platform-definition-documentation/9949"},
{"title": "Autoware Working Group 20190904", "thread_contents": ["The next meeting of the Autoware working group will be held at ", ". The meeting information is below.", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: ", "Access Code: 547-575-093", "More phone numbers", "\nAustralia: ", "\nAustria: ", "\nBelgium: ", "\nCanada: ", "\nDenmark: ", "\nFinland: ", "\nFrance: ", "\nGermany: ", "\nIreland: ", "\nItaly: ", "\nNetherlands: ", "\nNew Zealand: ", "\nNorway: ", "\nSpain: ", "\nSweden: ", "\nSwitzerland: ", "\nUnited Kingdom: ", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "I\u2019m trying to catch up with the minutes. The goal of this WG also includes the following, right?", "We need software design for Autoware.", "Autoware API", "\nROS interface (nodes, topics, message types, etc.)", "\nData structure and format", "These are all essentially the same thing, and yes, they are in part covered by this working group, but also in part by other working groups.", "Many others upcoming, such as safety and security", "I think it is likely we will have additional working groups on those topics once we get enough activity and expertise in those areas, but for now we intend to cover them in this working group and rely on the existing ROS 2 working groups.", "I am Seiya Maeda Saitama University.", "\nI attend to next meeting and discuss followings.", "The minutes for the Autoware Working Group meeting ", " have been posted to the wiki:", "Project for managing the Autoware Foundation's open-source activities (working groups, project structure, etc.)", "Unfortunately attendence at the meeting was very poor. I think that the TSC will likely have to take some actions to increase participation in the Autoware WG. Currently we have ", " and virtually no humanpower available to do them.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Autoware API", "Data structure and format", "ROS interface (nodes, topics, message types, etc.)", "Many others upcoming, such as safety and security", "Requirement specification", "Function specification", "System test", "Unit test"], "url": "https://discourse.ros.org/t/autoware-working-group-20190904/10388"},
{"title": "Current status of FreeRTPS?", "thread_contents": ["I\u2019m evaluating the availability of communication between ", " and ", ".", "I tried to make the native-posix version of FreeRTPS communicate with the ROS2 on FastRTPS. But it seems that the native-posix sample(e.g. talker_no_rosidl) won\u2019t be able to communicate with the ROS2(e.g. listener_best_effort).", "Is there anyone who can make FreeRTPS communicate with ROS2 or knows which features to be developed?", "\nIt seems that the talker_no_rosidl is able to add its messages to the internal history of FastRTPS. But the upper layers won\u2019t retrieve the messages.", "Greetings! Thanks for your interest in the project. Development of FreeRTPS has been paused for a while. Current development efforts are using FastRTPS and RTI Connext.", "At some point it would be to resume development of a microcontroller-friendly middleware option, but at the moment we (OSRF) are not spending time on it. Of course, this is all open-source, so if you have time and energy to spend on FreeRTPS, it\u2019s certainly there for the taking. It would need a significant amount of effort, though. Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/current-status-of-freertps/1396"},
{"title": "ROS2: anything equivalent to topic_tools:ShapeShifter?", "thread_contents": ["Hi,", "I am start looking at the source code of ROS2 to understand how message are serialized.", "\nMy goal is to update ", " to ROS2.", "I GUESS that serialization details are defined by the middleware (OpenSplice, eProsilica, etc), aren\u2019t them?", "Is there anything equivalent to ", ".? Is it even possible to have something that works similarly?", "Is there any way in ROS2 to introspect the content of a message at run-time?", "Cheers", "Davide", "No, unfortunately we don\u2019t have that yet. ", " was working on it for this release, but we ran into some issues figuring out how to most efficiently get data in and out of the underlying DDS vendors without (de)serializing. The API we\u2019ll provide will be relatively simple, something like ", " and ", " and some wrapper like ", " in our C++ API, but it\u2019s the \u201chow\u201d that\u2019s held us up.", "But we\u2019re still looking at this issue and it\u2019s in our critical path as we need it to implement rosbag and to implement \u201ctype masquerading\u201d.", "I\u2019ll try to remember to reply here when we get the first version of that new API figured out.", "The other thing I suppose you\u2019ll need is a way to parse the \u201cROS IDL\u201d files, e.g. ", "? We don\u2019t currently have that in anything but Python. I see that you have one for ROS 1 in C++:", "As part of your package, which is awesome ", ", but it would likely need to be updated to support the new syntax we support in ROS 2 (see ", ", specifically the part about bounded strings and arrays).", "Also, it would be awesome to have a separate library for parsing the various interface files in C/C++, so it could be reused everywhere.", "There\u2019s also the possibility to have another, more machine friendly, version of the interfaces. For example, as part of generation at build time we could generate an xml or json file which represents the contents of the corresponding ", " file. Then you could lookup and load that file instead (with an xml or json library), which would save you from needing to writing parsing code, and instead you would only need to interpret the contents correctly.", "This is a feature I\u2019d really like to see well supported in both ROS 1 and ROS 2, it would be especially helpful in making the bridge between the two more convenient to use with non-standard types. So, once we get you unblocked by providing something like ShapeShifter, I\u2019d be really interested in helping you with anything you\u2019re missing to implement it.", "Awesome, I am looking forward for it.", "The \u201cmachine friendly\u201d format you mentioned would (of course) be VERY welcome but not strictly necessary, since I have done already most of the heavy lifting in terms of parsing.", "One quick question. My runtime deserialization was created doing a \u201creverse engineering\u201d of how ROS1 messages are created.", "\nIn DDS is the serialization protocol standardized or middleware dependent? Since DDS is known to have good interoperability, I am tempted to think that the former is true\u2026", "The DDS serialization protocol is indeed standardized. It uses a format called Extended Common Data Representation (XCDR). The most up-to-date reference is the DDS-XTYPES specification version 1.2. See ", "Well, we\u2019re going to need to provide a function which can serialize and deserialize a ", " given an output structure. For example, with ", " it might be like:", "The implementation of this imagined ", " function may end up calling an implementation specific function, or at the very least how it is deserialized should be an implementation detail.", "But of course what you want is more like:", "We have the beginnings of this in ", ", but I\u2019m 100% certain how much is still needed to support this at the user level. We might not be able to use any of that package at the user level, since it isn\u2019t always used in the implementation, but it is at least representing the structure of a message as C++ structs and objects which is part of what we\u2019d need to do. My initial guess is that we would need to have a way to generate the same typesupport structures but given an input string or structure which describes the type.", "So I\u2019d actually say we should try to avoid your package needing to reverse engineer how to deserialize, instead that should be provided by our abstraction layer. This allows us to protect the stuff that comes on top from underlying changes, e.g. if we changed from one version of CDR (the serialization standard used in DDS and ROS 2) to another.", "Just two side notes:", "The roadmap currently contains a task to revisit the message definition format. The ", " format currently used with some minor extensions from ROS 1 might be replaced with something more powerful since the format can\u2019t handle several of the pending feature requests. Just to be consider when spending effort on implementing the current format in different languages.", "While the serialization protocol of the DDS implementations follows a single standard you can\u2019t rely on the rmw impl. to be DDS. There has e.g. be efforts to implement the rmw interface with other middlewares like OPC UA.", "Summarizing:", "THINGS WE MIGHT DO:", "A) I guess that the first step is to add to ROSIDL the equivalent of ", ". Preferably expressed as JSON or XML format.", "B) It must be possible to subscribe to a topic in a \u201cgeneric\u201d way, i.e. bypassing the deserialization step and accessign the raw bytes of the message.", "C) To implement a run-time deserializer, I don\u2019t see any solution but implementing a different code for each protocol ", " No magic wands.", "A) I guess that the first step is to add to ROSIDL the equivalent of ros::message_traits::Definition< Type >::value(). Preferably expressed as JSON or XML format.", "I assume the value should contain the message description in a readable format. Why should that be provided through a language specific symbol? I would suggest to provide this information through the ament resource index which would make it available across languages.", "what I meant is that ros::message_traits::Definition< Type >::value() return a ", "But this static C string contains the information in a easy to parse format that any language can easily parse.", "Something like this ", "what I meant is that ros::message_traits::Definition< Type >::value() return a const char*", "I think exposing this information (even a preprocessed version to actually store e.g. yaml) through the ament resource index would be better in order to make it available to any language.", "ok, I got your point.", "Well, it seems that at least for eProsima, there are some open source pieces of software that can help me with the this task. But it will take me time to dig into the source code", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "Type erased messages are needed for applications such as rosbag, rqt_plot, PlotJuggler, MATLAB importers, etc. Any generic topic subscriber indeed.", "ShapeShifter had all the informations needed to do this: raw bytes buffer, Message definition and MD5Sum.", "ROS1 had just one protocol, whilst ROS2 can potentially have DDS or others."], "url": "https://discourse.ros.org/t/ros2-anything-equivalent-to-topic-tools-shapeshifter/3393"},
{"title": "Introducing ROS2 Sanitizer Report and Analysis", "thread_contents": ["AWS Robotics has been working on ROS2 code quality and infrastructure improvements since January 2019. Our project specifically focused on issues reported by the ", " and ", " C/C++ runtime analysis tools. Issues the sanitizers surfaced include:", "The scope of our project  was to analyze as much of the ROS2 core code base as possible (everything included when building with \u2014packages-up-to system_test), fix issues, develop a process so the community can also fix issues, and integrate sanitizers into nightly CI. These topics, as well as what we have done, are outlined in this document.", "Sanitizers capture very detailed information during runtime about code quality issues and print them to stderr. Many of the surfaced issues are duplicates as the code path that has them can be encountered multiple times. Just by looking at stderr output, it\u2019s difficult to see how many issues there are and which ones are duplicates. To make it easy to see, we implemented a colcon plugin that parses the output of sanitizer issues as they are printed to stderr, deduplicates them, and writes them in a readable CSV. If the same issue is printed 500 times to stderr during colcon test, it shows up as a single CSV line with a count of 500. The line also includes the relevant information needed for debugging the issue.", "When we started the project, we created two reports \u2014 one for ASan and one for TSan \u2014 using the colcon tooling we created. The general workflow for generating the reports is as follows.", "First, ROS2 code needs to be compiled with ASan or TSan enabled. We created colcon mixins to make it easier to do that.", "Then, run the tests with the sanitizer_report event handler enabled. This event handler parses all printed sanitizer issues, consolidates duplicates, and writes them to a CSV file.", "See our ", " for a complete walk-through of building and testing with ASan or TSan enabled.", "We limited our testing to a subset of ROS2 core packages including those in rcl, rclcpp, rmw (Fast-RTPS only), rosidl (Fast-RTPS only), and system_tests repositories, which contain 83,760 source lines of code. While we started with only Fast-RTPS, we will include all DDS implementations in the future as we need to surface sanitizer issues in their associated rmw implementations.", "The raw ASan report created on 2019-05-01 showed that the sanitizer raised 1,128 issues. Our deduplication logic reduced them to 37 unique (root) issues.", "The raw TSan report created on 2019-05-01 showed that the sanitizer raised 7,656 issues. Our deduplication logic reduced them to 61 unique (root) issues.", "TSan report includes errors that originate from ROS2 core packages and from DDS libraries.", "To date, we\u2019ve opened 21 pull requests in ROS2 fixing sanitizer issues in low-level ROS2 Core packages. Our strategy is to fix all sanitizer issues in ROS2 Core packages starting with the lowest-level dependencies and working up the ROS2 stack. While fixing issues, we created a ", " which can be used by the community to discover and fix sanitizer issues. Below is a breakdown of our pull requests.", "With the above fixes, tests in rcutils and rcpputils run without raising any sanitizer issues. The total number of reported ASan issues dropped from 1128 to 117 (an 89% decrease) and the number of deduplicated root issues dropped from 37 to 19.", "Most of the above fixes are in test code, as sanitizers are runtime analysis tools with significant overhead and are only practical to use during tests. They capture issues from any code that is exercised (including test code) and we can\u2019t initially tell if an issue is in source or test code. Though we need to resolve all sanitizer issues for CI to be green, issues in source code are more concerning for production scenarios.", "We reviewed our results with eProsima and they already submitted fixes for the following TSan issues in Fast-RTPS.", "One important aspect of this project was to integrate these sanitizers into the nightly ROS2 CI jobs. Once we have them integrated and all issues resolved (jobs are green), we can begin to block the build if any new issue is detected in these packages (new regressions). You can see the sanitizer nightly jobs here", "Initially, we focused on fixing sanitizer issues in the rcpputils and rcutils packages as they\u2019re at the lower level in ROS2 dependencies and we knew we could address all the issues in these packages within the project timeline. As a result, both jobs are green (they run with zero sanitizer issues). Going forward, we want to work our way up the ROS2 stack, adding packages to these jobs while keeping them green.", "We created new tools to make it easy to use ASan and TSan with ROS2, used those tools to identify issues in the ROS2 core code base, fixed many of those issues, created ASan and TSan nightly CI jobs, and got a few of the base ROS2 packages to green in those jobs. We will continue to improve our sanitizer tools and explore other means of ensuring good quality code.", "We feel we\u2019re in a state where we can also solicit input and involvement from the ROS2 community. We encourage the community to use our ", " to learn how to use the new ASan and TSan tools to improve code quality of ROS2 and any project built on top of it.", "I\u2019d be interested to see this work linked to a steppable simulator. Then, even with the overhead, you\u2019d be able to test application code.", "Thanks for this contribution ", ", ", " and everyone in the AWS Robotics team!", "After putting together a few nights and weekends of free time, I managed to reproduce your work and got some additional understanding on several of the sanitizers for dynamic bug finding. I\u2019ve documented my attempts for ", " while applying it with the Dashing release in ", " (there\u2019s an attempt to run it in OS X but I eventually gave up since I had very limited time, the Dockerfile should help reproduce the setup I\u2019ve used).", "Beyond applying this to the ROS 2 codebase, I thought this could have a decent impact in the ", " development and wanted to see whether I could gain some quick insights including this together with the MoveIt 2 alpha release (which already includes some tests we\u2019ve ported). My second attempt is described in ", ". By solely analyzing ", ", I was able to find several memory leaks so it seems we definitely want to include this in our development process. Not sure how much bandwidth myself or my colleagues will have in the short term but ideally, we should aim to get this into the ", " (raised ", " for it).", "I\u2019d be interested to see this work linked to a steppable simulator. Then, even with the overhead, you\u2019d be able to test application code.", "This would definitely be of my interest as well (at least sounds like a fun project to try during a few  nights). Particularly in combination with Gazebo, such toolset could become a rather powerful helper to diagnose vulnerabilities while simulating the actual robot behaviors intended to happen in the real world.", "I\u2019ve given it a thought but I\u2019d like to ask for advice. I can certainly benefit from additional input. Do you have any intuition ", " (or anyone else) on how you\u2019d start with this in Gazebo? Do you foresee any modification required in Gazebo or rather, simply build the application code with the right set of flags to enable sanitizers\u2019 reports (as demonstrated already)?", "Thanks for trying this out ", "! This would be a great addition to MoveIt 2 IMHO, motion planning can be memory intensive so fixing leaks could lead to noticeable improvements.", "Regarding simulation, a \u201cnormal\u201d Gazebo could communicate with ASan/TSan enabled nodes. The only issue I expect would be that any code relying on timeout values will be fragile as the sanitizers induce a performance penalty. I saw a few ROS 2 tests relying on timeouts failing when ASan or TSan is enabled.", "I think it would work best with code that is designed to run in real-time with known execution times per step. That could possibly allow you to adjust the execution rate to account for the overhead of running the analysers. You would probably need to profile how much overhead the analysers add to your code first.", "I don\u2019t have any ideas off-hand for how to handle timeouts.", "Sorry for replying old topic.", "I\u2019m trying to fix remaining sanitizer issues.", "\nI first tied rcl and found almost of them are about test code but a few are about rcl code.", "I sent PR about these(I related them with [rcl isses 469 \u201cMemory leak in test_subscription__rmw_fastrtps_cpp\u201d](Memory leak in test_subscription__rmw_fastrtps_cpp)).", "Also I found rcl line-coverage by UT is 75.2%.", "\nSome functions are not tested in graph.c, init_options.c, node_options.c, publisher.c, subscription.c, timer.c.", "\nSo I plan to write more tests and send PR.", "The same can be said about rmw.", "\nRmw passes ASan test, but test coverage is 58.6%(lines), 43.3%(functions).", "\nI plan to address them too.", "If there is a goal for test coverage or quality, please tell me.", "Thank you.", "Thanks a lot for your work on this ", "!", "Did you take a look at the (future) ", " ?", "For ROS core packages, there is an ambitious goal set to reach 95% line code coverage.", "On another note, we setup an ASAN nightly job there: ", "Currently, it only builds until ", " as only those few packages are 100% clean.", "\nIf merging your PRs could allow us to get more of ROS 2 core packages green, please consider updating the job to start building more code with ASAN every night by replacing ", " in the following code:", "Thanks again for your help with this!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["lock order inversion (potential deadlocks)", "data races", "heap-use-after-free (accessing heap memory after it is freed)", "memory leaks", "signal handler spoils errno (a signal handler overwrites errno)", "colcon build with flag for ASan mixin: ", "\n", "see ", "\n", "\n", "colcon build with flag for TSan mixin: ", "\n", "see ", "\n", "\n", "\n", "\n", "see ", "\n", "\n", "34 memory leaks", "3 heap-user-after-free", "47 data races", "10 potential deadlocks", "2 heap-use-after-free", "2 signal handler spoils errno", "4 fixes in source code\n", "heap-use-after-free in rclcpp - ", "\n", "memory leaks in rcl\n", "\n", "\n", "17 fixes in test code\n", "memory leaks\n", "rosidl - ", "\n", "rcpputils - ", "\n", "rcutils\n", "\n", "rcl\n", "\n", "system-tests - ", "\n", "\n", "\n", "data race\n", "\n", "asan_build_args = data['build_args_default'].replace('--cmake-args',", "    '--cmake-args -DOSRF_TESTING_TOOLS_CPP_DISABLE_MEMORY_TOOLS=ON') + \\", "    ' --mixin asan-gcc --packages-up-to rcpputils'", "\n", "create_job(os_name, 'nightly_{}_address_sanitizer'.format(os_name), 'ci_job.xml.em', {", "    'cmake_build_type': 'Debug',", "    'time_trigger_spec': PERIODIC_JOB_SPEC,", "    'mailer_recipients': DEFAULT_MAIL_RECIPIENTS + ' ros-contributions@amazon.com',", "    'build_args_default': asan_build_args,", "    'test_args_default': (", "        '--event-handlers console_direct+ --executor sequential '", "        '--retest-until-pass 10 --packages-up-to rcpputils'),", "})"], "url": "https://discourse.ros.org/t/introducing-ros2-sanitizer-report-and-analysis/9287"},
{"title": "Optional Fields in Message", "thread_contents": ["I was taking a look at ", " and ", ". I noticed that there were additions to the .msg format and I started thinking of another addition.", "I propose adding an optional tag in front of the field type in the .msg format.", "I know users generally assign a sentinel value (i.e. NaN or 0xFFFF), when there no value is present; but it would be more semantic to build it into the message.", "In C++14, it could be implemented in the experimental optional implementation. Then, when the code base moves to C++17, the experimental tag could be removed.", "It might be useful for making different measurements in a compatible format. For example, my 1D gyro could produce a 9D imu message with just that one optional measurement field filled in.", "Can you clarify what you goals for optional values are. Do you want to distinguish an unset value from any other value? If the value is not set do you want it to not use any space on the wire? Do you want an older message definition (without the optional field) be compatible with a newer variation of the message (with the optional field added)?", "I don\u2019t understand how C++14 / C++17 is related to this and what you mean with \u201cexperimental optional implementation\u201d. Maybe you can clarify that part.", "Do you want to distinguish an unset value from any other value?", "Yes, the goal is to specify an unset or unused value from another value, but maintain compatibility.", "If the value is not set do you want it to not use any space on the wire?", "I don\u2019t care about the wire representation at this point. I was concentrating on the user perspective.", "Do you want an older message definition (without the optional field) be compatible with a newer variation of the message (with the optional field added)?", "That would be nice and could be another way of getting the same thing. For example, somehow relating the 1D gyro to the 9D IMU message without much overhead or specification. I was mostly thinking about the inherent compatibility with one message type.", "I don\u2019t understand how C++14 / C++17 is related to this and what you mean with \u201cexperimental optional implementation\u201d. Maybe you can clarify that part.", "Sorry, I started to get into the weeds. I like the use of std::array and std::vector in the C++ message API. I thought it would be nice to use std::optional, which is available in C++14 as a pseudo experimental feature: ", ". Basically, some compilers added it in the std::experimental namespace before it was officially released in C++17.", "+1", "I would also appreciate support for optional fields in ROS2. The best use case I can currently think of is to get rid of the ", " message types and merge them into one ", " message with an optional covariance field (not considering backwards compatibility in this case). A node that is only interested in the actual value and not in the covariance information would not need to implement different subscribers for the two types or apply shape shifting. Other tools like rviz could optionally visualize the covariance, if available.", "In general, having native optional field support in many cases removes the need to define different, but very similar messages and - depending on how the presence of optional fields is encoded in the wire format - can also help to reduce wire size because unavailable data does not have to be represented by a full 8-byte NaN double.", "Protobuf 2 also has ", ", but apparently this feature has been removed in ", " for a good reason\u2026", "The ", " has a rant about the required field that elucidates (in part) the history behind it.", "Note also that both Capn Proto and Protocol Buffers 3 still support the notion of optional - Capn Proto via ", " and Protocol Buffers 3 via ", ".", "As for a direct use case, we ran into one recently when wanting to write a ROStful (ROS-REST conversion interface) client. As a client to a REST server api, it may need to set outgoing optional fields and parse incoming optional fields. A fixed ros message is not rich enough to transmit/receive this kind of information.", "The best use case I can currently think of is to get rid of the geometry_msgs/FooWithCovariance message types and merge them into one geometry_msgs/Foo message with an optional covariance field", "While I agree that there may be valid use cases for optional fields, I don\u2019t think this is one of them. In a different robot framework that I regularly use, there is no ", " message, only a ", ", which is the logical conclusion of what you propose and which has these fields:", "In contrast, I like the ROS messages (", ", ", ", ", " \u2026) better. The signature of the topic tells me immediately which fields are used or supposed to be filled. I don\u2019t have to do a ", " or read the code to find out, and if they later decide to use a covariance, the API changes, so my code breaks and I know I have to adjust it.", "Thanks for the pointer to the changes between Protobuf 2 and Protobuf 3. A few other searches found some other interesting opinions about proto2 vs proto3 such as ", " and that the proto3 version is ", ".", "Although the answer that \u201ceverything is optional\u201d is a bit tongue in cheek. I think that it actually has a valuable insight. In particular for the API, some fields may not need to be filled. And this depends completely on the message documentation. Messages are both a datatype as well as a documented standard on how to fill them out. Some messages provide multiple fields but ", ". Similarly some messages use ", " where there are some configurations where some arrays may be empty. Which is basically using the implicit flag of the length of the array, which is an introspectable property of the message.", "Using the empty arrays gains the benefit of not needing to send the empty data fields. (Less the overhead inherent to communicating that the array is empty.)", "Two other advantages of \u201coptional\u201d data fields are the ability to not send the data if it\u2019s not used and \u201cbackwards\u201d compatibility between two versions of a message where an \u201coptional\u201d field has been added and the receiver can automatically drop the extra field that it does not know about.", "Not actually sending blank data can be covered as above by using variable length arrays. And the linked article above from the Capn Proto FAQ covers how the \u201ccompatibility\u201d breaks down quickly.", "The other major change that supporting optional fields as protobuf does is that it requires us to switch to method based access. Such that you must query whether or not a field exists before you ask for it. Our community has shows a strong preference for having member based access to the datatypes for the ability to implement high performance methods using things like iterators without going through accessors or making a local copy that would cause significant overhead.", "With the new bounded arrays, we could pick up a new common message pattern in the case that you want to have an optional field.", "Though when designing messages like this it needs to be kept in mind that adding this complexity will force all subscribers to handle both cases. Or if there\u2019s multiple optional fields the number of cases can grow combinatorically.", "That would be nice and could be another way of getting the same thing. For example, somehow relating the 1D gyro to the 9D IMU message without much overhead or specification. I was mostly thinking about the inherent compatibility with one message type.", "I\u2019m not sure that optional fields are the best approach for this. In this case I\u2019d recommend that using the full 9D message with the appropriate zero entries in the covariance. This means that all the tools can be written against a single message. We\u2019ve developed the same policy for 2D vs 3D poses. Where if you\u2019re working in 2D you still use the 3D points. And then the messages can be rendered following the same code path in all the tools instead of needing to have two different code paths.  This approach has proven very powerful as primatives used by the 2D navigation stack which only supports 2D navigation can be reused when people are starting to do 3D free space navigation and almost all the tools still work.", "As for a direct use case, we ran into one recently when wanting to write a ROStful (ROS-REST conversion interface) client. As a client to a REST server api, it may need to set outgoing optional fields and parse incoming optional fields. A fixed ros message is not rich enough to transmit/receive this kind of information.", "Can you clarify what you mean by this? Are you trying to represent an arbitrary REST message?", "To clarify what ", " was talking about, it is serializing/deserializing from json to rosmsg and vice versa (since most REST apis use json messages), in order to interface ROS systems (a robot) with a potentially useful REST API (a server somewhere).", "Note the ROS system can be a client, but also a server, such as when someone writes a WebApp for a Robot UI, interfacing with ROS, using some \u201crobot REST API\u201d derived from existing ROS services.", "As a Reference, here is how one would specify a message for a REST API : ", " by default all fields are optional. required fields need to be specified explicitly. Same as current protobuf design from what i understood.", "An unset field has a different meaning than a field set to a default value automatically by the message type, without explicit user intent\u2026", "And I would personally add a even more obvious usecase : ", ".", "\nA function can have optional arguments, and since python works with references and implicit typing, a safe simple default for any type is ", "if I have a function :", "and I want to expose it to other nodes via a service :", "Nullable default arguments are core part of the language so it is not unusual to see such APIs in python.", "So I think having some standard way to represent a nullable/optional field in a message would be very useful.", "Can you clarify what you mean by this? Are you trying to represent an arbitrary REST message?", "Alexandre is the technical guy ", "More simply, yes we were trying to represent or parse an arbitrary REST message. I wanted our ROS guys to talk to an external REST service without having to know or use REST.  Once the REST API is known, in most cases we can spec the ROS engineers a set of messages and services they can use that will talk to the REST service via ROStful. Unfortunately that REST service is not always something we design/control, so it will often have optional fields in it.", "Vice versa also happens - if we do have control over the design of the REST api, then the REST lads are then forced to become aware of the non-optional constraint.", "By the way, I\u2019m not necessarily arguing for the increase in complexity such a feature would bring, just presenting a use case where it would have been useful.", "I\u2019m have trouble coming up with a good pub/sub use case for optional. In my example, Instead of a gyro driver producing an IMU message, it probably makes more sense to create a converter. Maybe, type masquarading and an ease of building small message converters has more value right now. Type masquerading is already on the roadmap: ", ".", "I agree that remote function calls could use an optional field. Remote function calls are less common.", "The protobuf history is interesting. I didn\u2019t really get their problem. It seemed to be a testing problem, which they didn\u2019t address. In the Cap\u2019n Proto FAQ, the author mentioned protobuf 2\u2019s required field caused parsing failures when that part of the message definition was modified. DDS is strict on message types as well and will not allow reading part of a message. So the main point seems less relevant here.", "There are a lot of ways to implement optional. Mathematically, setting the covariance to 0 may work if it is a final output; but it will not work if that data needs to be fused. This post is about treating optional as a first class message feature so that the interface is explicit. When it is explicit, the user is more likely to deal with it not being set.", "As a little bit of context, my main motivation for proposing this feature was this talk called \u2018", "\nCppCon 2016: Ben Deane \u201cUsing Types Effectively\"", "\n\u2019 at ", ". The presenter argues why C++17\u2019s std::optional and std::variant are fundamental to types. I thought it would apply to ROS messages as well. I\u2019d argue that we should add something like variant, but I don\u2019t quite understand it yet.", "I think that good cases both for an against optional fields have been made here and elsewhere.", "To add to them, I also think that the Python experience shows the value of optional fields for ", ". It can give an API more expressiveness and make more explicit how it is intended to be used correctly. I agree that it can also add a lot of complexity, but at least for services that complexity may be worth it. I think it depends on how the majority of ROS users are creating services. Are they being used as remote procedure calls, or are they more often used as two-way or pull-style pub/sub communication (e.g. pull a copy of the map only when wanted)? If it is the former, then perhaps optional fields should be considered for service definitions. Of course, that raises the prospect of reducing the compatibility between .svc and .msg definitions, which I think is probably not desirable.", "I have a use case for optional fields. I store a vector map. This map has a name, a few other fields, and list of polygons. The polygons are large. If I just want to rename the map, I want to be able to publish an update message that leaves the polygons unset. That would in turn be interpreted as \u201ckeep the current polygons I already know about\u201d at the receiver. I don\u2019t want a separate update message for each map field. It seems that I would still need a separate message to update individual polygons, but I would still hope to leave fields that shouldn\u2019t change out of the message. JSON typically utilizes this principle: if a property is null for the serializer the property name isn\u2019t included and, vice-versa, when deserializing absent properties don\u2019t change the existing value in the target.", "I have a use case for optional fields. I store a vector map. This map has a name, a few other fields, and list of polygons. The polygons are large. If I just want to rename the map, I want to be able to publish an update message that leaves the polygons unset. That would in turn be interpreted as \u201ckeep the current polygons I already know about\u201d at the receiver. I don\u2019t want a separate update message for each map field.", "I think that this can relatively easily be covered by an update message with variable length arrays of changed elements for each type. Empty would imply no updates. There could also be defined additional flags for each type to tell the merge algorithm the expected policy, \u2018merge\u2019, \u2018replace\u2019, \u2018drop\u2019 etc.", "JSON typically utilizes this principle: if a property is null for the serializer the property name isn\u2019t included and, vice-versa, when deserializing absent properties don\u2019t change the existing value in the target.", "This seems to be conflating the message and the merge logic. If you have an update message and a data type you\u2019re merging it with, the policy of keeping existing values is defined by your merging function and not the message itself or the transport of that message. And as mentioned above the merge logic could be informed by message elements.", "I think part of the challenge is that there are two concepts of optional fields. One is that you can state if an element is set or unset. The other is that you can send and receive somewhat compatibly using a partial definition if the other side only has additional optional fields. (Ala what protobuf2 had but has been cut from protobuf3)", "Rereading this thread it sounds like a lot of what people are looking for is not necessarily the different version compatibility but actually improved API for communicating an element is set or not. Which can be done using the [<=1] idiom, but requires you to be pretty verbose and dereference into the list etc.", "For which we could consider adding syntactic sugar to make the user faceing API a little bit easier to use.", "Clearly something like ", " would be nice. However that\u2019s not available until C++17. And we would want to replicate this across all languages so keeping it unified would be valuable. But however it\u2019s implemented I\u2019d suggest that this is mostly client library syntactic sugar instead of a need to necessarily change the primitives if we use this as an extended API on top of the basic primitives we already have.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["position, cov_position", "orientation, cov_orientation", "velocity, cov_velocity", "angular_velocity, cov_angular_velocity", "sending timeout == 0 (how the ROS1 msg type ", " function initialize it) means return right away.", "sending timeout == None (how the python api is built) means no timeout, ie. run for ever."], "url": "https://discourse.ros.org/t/optional-fields-in-message/991"},
{"title": "ROS2 Intel Movidius NCS Release - Initial Version(V0.3.0)", "thread_contents": ["Hi all,", "We released ", " several months ago and received much feedback from community. Now, we are happy to announce the initial release(v0.3.0) of ROS2 Intel Movidius NCS package. This package is derived from ROS Intel Movidius NCS package and provides the following features for ROS2 with powerful ", ":", "\n\u2022 A service for ", " of a static image file", "\n\u2022 A publisher for ", " of a video stream from a RGB camera", "\n\u2022 Demo applications to show the capabilities of ROS service and publisher", "\n\u2022 Support multiple CNN models of ", " and ", ", including", "This project has been open sourced in github: ", ". Please refer to README file for more details about this project. We have tested it on RealSense D400 series camera and Astra camera. More features, such as multiple NCS devices support, are under development and will be included in next release. Stay tuned. Welcome feedback and participation.", "Hi ", "\nReally nice work and it is great to see more and more packages support ROS2!", "\nI am excited to try this with realsense through ROS2 version driver.", "HaoChih", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["CNN models for object classification\n", "AlexNet", "GoogleNet", "SqueezeNet", "Inception_V1/V2/V3/V4", "MobileNet", "\n", "CNN models for object detection\n", "MobileNet_SSD", "TinyYolo_V1", "\n"], "url": "https://discourse.ros.org/t/ros2-intel-movidius-ncs-release-initial-version-v0-3-0/4349"},
{"title": "Access all Publisher/Subscriptions of a node", "thread_contents": ["Hi for debugging purposes I\u2019d like to access all publishers/subscriptions a node handles. (And get their rcl handle for accessing their fast rtps handle) Is this somehow possible. I couldn\u2019t find a fitting method in the rclcpp::Node", "There is a way to do this, but only a code example:", "Contribute to ros2/demos development by creating an account on GitHub.", "Obviously, this isn\u2019t portable because you\u2019re accessing Fast-RTPS or Connext (or whatever one you\u2019re using) specific interfaces and therefore your program will now only work with that vendor. So for that reason we discourage you from doing it unless absolutely necessary.", " might have more information for you as he did this originally, but I didn\u2019t see a tutorial on the wiki or anything.", "Also, please try to ask questions like this on ", " with the ", " and/or ", " tags in the future.", "I think you misunderstood me. I already found this example.", "\nWhat I\u2019m looking for is a way to get all publishers/subscriptions that were created via rclcpp::Node->create_publisher (or create_subscription). I couldn\u2019t find an interface in the node for achieving this, but as far as I understand somewhere there must be a list which publishers and subscritptions were created.", "Regarding ", ". What kind of questions should be asked here and what kind of questions on ", "? The latest I was aware of was that questions should be asked here instead of github.", "Regarding ", ". What kind of questions should be asked here and what kind of questions on ", "? The latest I was aware of was that questions should be asked here instead of github.", "While we were bootstrapping ROS2 we were recommending asking questions here in the Next Generation ROS category. However as the ROS2 traffic has grown that\u2019s becoming unsustainable and we\u2019ve decided to transition to our standard model of asking questions on ", "The transition was discussed:", "And we recently re-announced it:", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/access-all-publisher-subscriptions-of-a-node/4269"},
{"title": "ROS2: Create Shared Message from ROSCon 2014", "thread_contents": ["Hi,", "I was going through old slides and came across some lines ", " in the slide deck from the state of ROS2 in 2014 [1].", "While that\u2019s clearly been long going since at least beta3, ", ". That seems actually very useful and from some discussions we\u2019ve had on the real-time front regarding getting loaned messages from the middleware to take some reserved memory, this seems like a natural way to do it from a user-level API. Recreating it, or some derivative, could get folks in the work flow that will be necessary later on when that is implemented.", "I see some comments on loaned messages that didn\u2019t quite make it for Eloquent [2], but I can\u2019t really find any reference other than this slide deck to the ", " incarnation of the API in ", ", ", ", or ", " (no tickets or PRs reference it). Clearly [1] and [2] are not directly correlated, just noting the value that a method like this may have for wrapping that concept to a user.", "CC ", "[1] ", "\n[2] ", "I can\u2019t speak to why that specific API is no longer around, but you can find what did land in Eloquent here in terms of zero-copy support: ", "While many of the structures and apis are in place, currently the only middleware that supports it is ", ".  ", " gave a presentation on this at ROSCon 2019: ", " ", "I think that the closest current analogy is the new ", " api: ", " , which should return either a) a shared memory segment or buffer (if the underlying rmw supports it), or b) use a user-provided allocator to internally allocate the message.", "You may also be interested in the ", "I see some comments on loaned messages that didn\u2019t quite make it for Eloquent [2]", " which comments are you referring to in terms of loaned messages which didn\u2019t make it? Or asked differently, which functionalities are you currently missing for working with loaned messages?", "I\u2019m just purely going off the checklist of tasks undex Apex that wasn\u2019t checked with loaned messages.", "I can\u2019t speak to why that specific API is no longer around", "This just seems to be a far more natural user-facing API than ", " for the typical user that really shouldn\u2019t need to care how it was made. It\u2019s just giving them a message to work with.", "This just seems to be a far more natural user-facing API than ", " for the typical user that really shouldn\u2019t need to care how it was made. It\u2019s just giving them a message to work with.", "That\u2019s exactly what ", " does, lends them a message which may or may not be allocated on demand. They can work with that message, and when done they can return the loan.", "If you just want a new message and don\u2019t care where it comes from, then I suggest you just use ", ", right?", "Based on:", "That seems actually very useful and from some discussions we\u2019ve had on the real-time front regarding getting loaned messages from the middleware to take some reserved memory, this seems like a natural way to do it from a user-level API.", "It seems like you do want the middleware to optimize this if possible and if not, give the user a heap message to work with, which again is what ", " does, I think.", "It seems like you do want the middleware to optimize this if possible and if not, give the user a heap message to work with, which again is what ", " does, I think.", "Got it. I think then the remaining comment is then more pedantic in naming.", "I know that\u2019s exactly descriptive about what it does, but I strongly disagree with much of the ROS2 user-facing APIs. I think its far too \u201cpowerful\u201d for the average or novice user to a point of making it unapproachable without detailed knowledge of what\u2019s happening under the hood. Whenever that API review comes around, I\u2019ll give a more formulated argument on it\u2026 I have a reasonable understanding of what\u2019s going on because I sit here doing this all day, but I don\u2019t think that\u2019s representative of the userbase of ROS1.", "but sounds like my question was answered: it basically is there, just under another name, but not widely used as its not widely implemented by RMWs.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-create-shared-message-from-roscon-2014/12736"},
{"title": "ROS2 pcl_perceptions", "thread_contents": ["Hi,", "While working with ros2, happen to find pcl_perceptions for one of the algorithms developed useful. But, this", "\nsupports only with ros1, is there any plans or already available version of the same which supports ros2 native?", " Could you help or guide?", "Porting ", " is a non-trivial effort. Especially it depends on some ROS2 core features that have not been finalized/ported to ROS2 yet. Example: Replace dynamic_reconfigure and its .cfg files, porting filter plugins to components, etc\u2026", "Did anyone contact the maintainers asking about their plans? I guess most people are waiting for ROS2 to be more mature to minimize code rewrite/throw-away, especially huge stacks like perception.", " Could you help or guide on this? I wrote an email to you, about the same. I\u2019m ready to put in the effort, but need some initial ideas of for sanity.", "+1, from ", "pcl_conversions is mainly about type masquerading, which isn\u2019t targeted for a release yet.", "pcl_ros is primarily input/ouput/pipeline nodes. For porting that, I\u2019d really like to see how (dynamic) parameters and managed nodes shake out in the summer 2018 release.", "Costmap2D has a dependency on pcl_perceptions, and I don\u2019t see this on the Crystal Roadmap yet. Does anyone have the bandwidth to port it to ROS2?", "I believe the navigation stack as a whole has dropped pcl_* as a dependency in melodic.", "There is certainly interest in a port/refresh of pcl_* to ROS2, but bandwidth is low at the moment. The only thing outstanding from my excuses in February however is type masquerading, and we can probably agree that that isn\u2019t critical for a port.", "I can confirm that the Melodic branch of navigation is PCL-free, but does depend on the sensor_msgs::PointCloudIterator \u2013 not sure the status of that being ported.", "-Fergs", "but does depend on the sensor_msgs::PointCloudIterator \u2013 not sure the status of that being ported.", "There is a ROS 2 version of PointCloudIterator available at ", "Yes I saw that melodic no longer has this dependency after I posted. Thanks for the replies!", ", ", ", our team at Persistent System is ready to put the effort in porting perception_pcl to ROS2. Please let us know if you will be interested to obtain our assistance in the porting effort.", "\nIt seems no body start on this task. Ref to ", ". If you have resource on this tasks, I would suggest you announce on  ", ", and just start the work.", "There is ROS2 ", " package ported on ROS2 before moving on perception_pcl. The pcl_conversions deprecated on ROS 1, if we port perception_pcl, we could switch to use ", " instead of ", " on ROS2.", "The pcl_conversions deprecated on ROS 1, if we port perception_pcl, we could switch to use perception_pcl instead of perception_pcl on ROS2.", "I don\u2019t quite follow you here, but as far as I\u2019m aware nothing related has been deprecated.", "If you have resource on this tasks, I would suggest you announce on ", ", and just start the work.", "+1 to that. I would welcome any PRs for perception_pcl, and I\u2019m happy to provide feedback, but my bandwidth to do actual ", " is fairly limited.", "A few advance points - while a direct port of perception_pcl to ROS2 has value, what would be much more valuable is an overhaul of the perception_pcl \u2018filter\u2019 nodelets, taking the new ROS2 lifecycle concepts into account. That said, it would be great to have a design discussion prior to diving in.", "I don\u2019t quite follow you here, but as far as I\u2019m aware nothing related has been deprecated.", " yechun:", "It seems to me that questions are more around ", " deprecation. Having said that, we have already started working on perception_pcl porting effort.", ", Do you want me to setup a call with you with our team who is working on perception_pcl? If so, let me know so that I can setup a call.", ", ", " from our team has posted in \u201c", "\u201d for our intend to contribute on this porting effort.", ", as we discussed on PCL_ROS over the email, we at Persistent System are currently evaluating dependencies as a part of our porting effort. As we are planning to have a separate call with you, we are hoping to get further insight and guidance from you on this effort.", "For others, Please let us know if anyone else is working on PCL_ROS porting to ROS2? Our team at Persistent System has already started researching the porting requirements for PCL_ROS. Please let us know if anyone has any concern or like to share any vital information that we will help us in this porting effort.", "Please refer thread \u201c", "\u201d for further information on this effort.", "I agree with ", ", this could use a design discussion.", "While the \u2018filter\u2019 nodelets work, chaining them can be problematic, particularly if a new point cloud arrives before the current one is finished being processed. Perhaps the ROS2 lifecycle could be used to start a new filter chain if, for example, the current cloud is taking longer than usual to ICP. Something similar to a worker/scheduler model.", "It may also be worth looking at ", ".", "In the long term I\u2019d like to see a means of passing a point cloud to a GPU and having it to run a set of filter/operations on it. Perhaps something like shaders in OpenGL\u2019s rendering pipeline or gstreamer\u2019s approach to video processing.", "While the \u2018filter\u2019 nodelets work, chaining them can be problematic, particularly if a new point cloud arrives before the current one is finished being processed. Perhaps the ROS2 lifecycle could be used to start a new filter chain if, for example, the current cloud is taking longer than usual to ICP. Something similar to a worker/scheduler model.", "Indeed one of the design elements for the executor based threading model in ROS2 is that we wanted to be able to support synchronous pipelines. To achieve that we\u2019ll need to develop a custom executor that\u2019s aware of the pipeline flow, but once that is configured it can synchronously call each element in the pipeline. This in combination with the unique pointer publishing API supporting ", " should be quite powerful.", "Hi Paul,", "Hope you are doin good.", "\nWe were busy in migrating some other packages so post-poned work on pcl_ros.", "\nNow we are again back on the pcl_ros work.", "\nAs you suggested, we are working on the design part of it which involves component and ros2 parameter features.", "\nIt would be appriciated if you help us to understand few of our doubts.", "Thanks.", "Hi ", ",", "I believe ros2 launch should handle restarting a node if it crashes, however I can\u2019t find an example of this in source or documentation (", ").", "A \u2018port\u2019 of ROS1 would involve instantiating several lifecycle nodes inside one process. See the \u2018composition\u2019 demos: ", "A more interesting implementation would have an executor aware of the pipeline structure (", "), however I don\u2019t think any implementation or demo of this exists in the wild.", "Correct, this certainly would be valuable to fix in the context of a porting effort.", "Thanks ", "  for your response.", "\nAll these questions were related to existing code of pcl_ros in ROS.", "\nAs per my analysis, nodelet/node monitoring is handled by bond package which is already added to nodelet.", "I am working on design doc for pcl_ros migration to ros2. Also I am checking how these things will be handled in ros2. Once the document is in good shape, will share with you.", "Thanks,", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["In the process of piplelining, if any node crashes or breaks down,how is this handled in ros?", "Pipelining of filters is happening serially or parallely?", "I have observed that all the operations of pcl_ros are not getting tested with either gtest or launch. Please confirm.", "In the process of piplelining, if any node crashes or breaks down,how is this handled in ros?", "Pipelining of filters is happening serially or parallely?", "I have observed that all the operations of pcl_ros are not getting tested with either gtest or launch. Please confirm."], "url": "https://discourse.ros.org/t/ros2-pcl-perceptions/3596"},
{"title": "ROS2 Navigation - Input requested", "thread_contents": ["Hello ROS and ROS2 users and developers! I have been in discussion with David Lu and have reached out to OSRF to start an effort to develop the ROS2 Navigation stack. We are currently investigating the needs for the next-generation Navigation stack in ROS2.", "I\u2019d like to invite you to reply with things that:", "I have my own wish list but I don\u2019t want to seed the discussion, I want to hear other users thoughts.", "Also, if you are an active community member and want to be involved in the development effort, please let us know that also. We have a small team right now, but desire to do this in the open with community involvement.", "How to be involved in the development ?", "Happy to know this kickoff. ", "Per my experience of using ROS Navigation Stack, I would like to add below items into the wish list:", "Thanks for the feedback, a couple questions.", "Peter, I sent you an email, we can talk offline", "This is a rather ambitious list, and perhaps some of the navigation tasks applicable to these maps types may be too domain specific, but I\u2019ll just float some far ideas here:", "I suppose I\u2019d like to see navigation planners that could interoperate map format types that are more memory efficient, compressible, dynamic, human relatable, e.g. less pruly metric based like voxels or occupancy grids. I\u2019d also to see ROS navigation planners generalize beyond the classic 2.5D mobile robot on a planer workspaces or perhaps appropriate other environment data in a map as navigational heuristics like for packbots, quadrotors, ROVs that climb, fly or swim in 6DoF.", "Any dedicated Discourse/Slack or other mean for getting involved ??", "One of the main frustration I experienced with ROS nav stack is the lack of flexibility (e.g. ", " inner state machine) that eventually got partially addressed by the community later on - e.g. ", " let you use the state-machine of your choice under the hood.", "\nSimilarly, as it has been mentioned already, the possibility to use other types of map representation would be awesome.", "I\u2019m interested in a similar desire to extend the typical map layers into more semantic meanings. Would add wifi, ble beacons and other rf land marks to the annotated affordances idea mentioned be ", "I haven\u2019t used the navigation stack, but I understood David Lu to say at the last ROSCon that it doesn\u2019t support Ackermann-steered vehicles. That is an important use case.", "I agree with ", ", decouping the move_base in separate modules in a state machine would greatly improve its flexibility.", "Regarding features, a nice little thing to have IMO would be to be able to pass the goal tolerance (xy and yaw) in the goal message to move base.", "Ok, let me see\u2026", "Things to keep about the current navigation stack:", "Things that I will change:", "\nPlease, correct me if I am wrong. Right now, if you want to use your own planner (local or global), you have to use the C++ API. The class that you provided was extremely convenient to minimize the flow of information and speed up the full Navigation Stack work.", "\nHowever, ROS2 should be more efficient while handling messages right? Is it fast enough to deal with the idea of using a node for each planner? This node could receive all the information throw messages or is too much?", "\nIf this is the case, we should study the possibility of detaching the nav_core and the planners.", "\nMaybe the community will be more comfortable with a publish/subscribing paradigm. Is just a thought, but this will make everything even more modular right?", "Is the navigation stack too dependent on the position or is just my impression? If the robot does not start in his position, everything goes really crazy. The local planner and the local cost map can be more independent of the positioning system, right? Is a bad configuration of my navigation or is like that?", "Things that I will add:", "\nA simple system that tracks common problems. E.g: Hey, I am waiting for a map and it is not coming! Hey, I am sending cmd_vel commands but the position does not change! Things like that.", "A simple undocking algorithm; I do not want my robot to move backward unless a recovery behavior specifies so.  A tiny algorithm at the beginning that moves the robot backward x meters could be handy. Moreover, this can be attached to a simple boolean topic. There, a sensor can publish if the robot is at the docking station or not. If the robot is at the docker, you execute the undocking before moving, if not, just move as usual.", "+1 to the idea of specifying the goal tolerance.", "I\u2019m not an expert navigation researcher but I am a user and an experimenter.", "For me, the biggest desire is for navstack2 to be a navigation framework built using ROS2 principles rather than a monolithic navigation solution, as it is in ROS1. This means defining the separate components (local planner, global planner, local map probider, global map provider, planner supporters such as costmap providers, rescuers, path follower, and so on), defining the interfaces between them, and specifying those as ROS2 messages, services and actions. We should be able to say \u201ca node (or set of nodes) that provides a global planner compatible with navstack2 should publish/subscribe/use these topics, services and actions and provide these parameters, at a minimum\u201d. The key thing here becomes defining the APIs between the different parts of the navigation stack.", "Navstack2 should take advantage of the capabilities of ROS 2 to make things nodes but then keep them in the same process so that message passing is nearly cost-free.", "Being possible to specify via a configuration file what the global planner is, what the local planner is, etc. would also be possible. This is to have it not just be a bunch of nodes with defined interfaces, but be more of a framework where it is simple to build a complete navigation stack without feeling like you are plugging things together manually.", "If we do this, then we can achieve the goal of allowing different planners to be plugged in, etc. that is frequently stated in this thread.", "It will also ensure that it is inherently easy to introspect the internal navigation process, because we can intercept all the messages flying around between the different parts.", "Building on this, navstack2 should then provide a default configuration that provides an equivalent or better navigation functionality to the ROS1 navigation stack.", "As someone who has a strong desire to participate but is something like 1000% over-committed in time, I would also like to see a central place where I can comment on design decisions and implementation choices made when I have time, without losing track of where everything is. Please make a github project so we can make issues and discuss them, if you haven\u2019t already.", "A simple undocking algorithm", "I think this is too application-dependent. But, it should be ", " easy make things like this default behaviour for your robot in navstack2.", "wow,  very detailed, and right, it\u2019s what I meant\u2026 ", "All, thanks for the input so far, keep it coming!", "A few comments.", "Keep the feedback coming!", "To enable autonomous navigation, you have to allow the robots to sense the environment around to create its map and enable collision avoidance, In other words, sensor data input to any algorithm is important.", "\nNow there are various sensor solutions serving for this purpose, for example, Sonar, Lidar, MMW, Vision and so on they serves different preference and it\u2019s possible to sensor fusion with them in the real autonomous navigation. Free to think ROS2 navigation can consider more:", "Late to the party, but here\u2019s my $0.02.  Generally, +1 to all of Geoff Biggs\u2019 coimments.  I\u2019d like to see:", "I\u2019d be interested in having our group here at Oregon State help with some of this, depending on where it goes.", "cheers", "\u2013 Bill", "Not really a navigation stack user myself, so just passing by, but I was surprised ", " wasn\u2019t / isn\u2019t mentioned more. Only ", " mentioned it once earlier in this thread.", "From the ", " at ROSCon and ", " it seems it\u2019s gone into the direction that ", ", ", " and some others sketch:", "Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the ", " and the ", ". For navigation on costmaps see ", " and ", ".", "Would seem to be a good idea to get some input from its maintainers (", " et al.).", "Yes, I was overjoyed when I saw ", " announced at ROSCon last year. I consider it a ", " step in the direction I want to see the navigation stack go. I haven\u2019t had time to try it out myself yet, but I agree with ", " that any effort to develop a new navstack for ROS2 should consider it the biggest input into design.", "Hi all,", "\nWe\u2019ve created a repo for the ROS2 Navigation project here:", "\n", "ROS2 Navigation. Contribute to ros-planning/navigation2 development by creating an account on GitHub.", "\n", "We\u2019re going to collect all design inputs, here, starting with high level use cases and requirements:", "\n", "Please submit your use cases and requirements via pull requests so we can have design discussions there.", "Thanks,", "\nMatt", "A bit late, but I figured I would chime in (having been a maintainer of the navigation stack for going on 5 years now).", "First, I concur with the several comments about better modularity. I\u2019d almost suggest that the ROS2 navigation stack shouldn\u2019t include any planners in the main repo (as is the case with MoveIt). There have been several newer (and probably better) planners developed \u2013 but users assume they should use only the \u201cdefault\u201d planners. At the same time, maintaining a code base that includes \u201call the planners\u201d is just not feasible. Having better modularity, and having things like local and global planners exist in other repos, makes it far easier to have more maintainers involved, and for development to proceed quicker (if you don\u2019t like planner X, go write and release planner Y).", "While splitting those things out to other repos, I would suggest providing some basic/core code to build planners on. At some point a Willow Garage intern started to refactor base_local_planner in that direction \u2013 but it was never really finished.", "On the subject of 2d/3d \u2013 I think there is a fine balance to walk here. While most research is pushing more in the 3d direction, commercialization tends to push towards cheaper/smaller processing power \u2013 and some of the optimization in terms of 2d/2.5d in the navigation stack is important here. While a full 3d mode is awesome, requiring the whole system to always act as 6Dof pose + 3d terrain may make it unusable on smaller platforms like Turtlebot.", "With regards to not being monolithic \u2013 I think this will be a serious challenge. One of the things that ROS1 does a really poor job of is synchronizing the operations in multiple nodes. I\u2019m not sure how much ROS2 really helps in that regard.", "But here\u2019s my most important feedback: we need better testing. One of the reasons we have a hard time merging things in the current navigation stack is that there is just almost NO test code (similar issues with MoveIt). I have spent an enormous amount of time physically testing code in simulation or on real robots to try and be sure something contributed works \u2013 only to find out that it actually breaks some particular feature that someone was using. If you\u2019re going to largely overhaul/rewrite things \u2013 do it in a test-driven way, and make sure those tests are meaningful so that the system can actually be maintained.", "Has anyone looked at what other ROS2 dependencies are missing? AFAIK, there is no equivalent of actionlib yet (which is probably a pre-req to actually building most robot applications in ROS2). I\u2019m also not sure the status of things like parameter management or dynamic reconfigure (highly required for people to actually tune a navigation setup in a reasonable amount of time).", "-Fergs", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You like about the current ROS Navigation stack and would like to see kept the same (or equivalent functionality)", "Things you think can be improved on and would like to see done differently, aka. your wish list", "make a more flexible mechanism for plugin implementations, especially for recovery plugins.", "Support multi-thread and heterogeneous computing", "Adopt AI gene (e.g. Reinforcement learning) into path planning and collision avoidance.", "Support more map types", "it\u2019s better to support 3D path planning and CA", "I think you mean AI Gym, right? If so, are you aware of the ", " project? I have been using it for some RL work in this space. I believe it could be ported to ROS2 fairly easily also.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "3D is on my wish list too, for applications like drones.", "What map types are you thinking in particular? Google Maps? Non-2D occupation grid maps?", "Semantically oriented Maps\n", "Indexed Points of Interest\n", "rondevu points", "moving goals", "\n", "Labeled region boundaries\n", "property borders", "tolls or crossing costs", "exclusion zones", "\n", "Annotated Affordances\n", "Doos, Elevators, Appliances, Chargers", "Departments, Faculties", "\n", "\n", "Vector Maps\n", "Floor plans or 3D scale models\n", "Google maps/earth", "Architectural blueprints", "\n", "Roadways maps\n", "Turning lanes, intersections, crosswalks, etc", "Congestion, Traffic density", "\n", "\n", "Geo Maps\n", "Topological\n", "Elevation and grade", "Underwater terrain", "\n", "Weather\n", "wind and tide velocities", "Dynamic time series forecasts", "\n", "Approximate at scale\n", "WGS84 vs local cartesian", "Alternate ", "\n", "\n", "\n", "All the dynamic reconfigure for sure.", "All the representations in rviz (necessary for tunning as well)", "I will keep the algorithms that are currently implemented in ROS1, if possible with the same parameters. This will make the transition smooth.", "I plan to create a ", " repo for this (hopefully under ", " namepace, working on that), and will start capturing the wish list items and requirements, as well as documenting design decisions, etc. This thread is just the primer for that.", "The many different types of maps requested, to me means we need a more abstract data type for maps, that can represent many different types of maps that might be inputs to the system. I\u2019m not sure what that will be exactly yet, but to me, this illustrates that we need to decouple the map type as much as possible from the system.", "I also agree on the use of ROS2 nodes for the low level plug-ins like global and local planning. That will improve decoupling and make it easier to replace those components with other algorithms, and will also ease the debug effort as pointed out before. We can do this using shared memory pointer passing so that the performance overhead is small.", "general flexibility to smoothly use their output in different phase of autonomous navigation", "how to adapt their combination or sensor fusion well while engaging with ros2 navigation stack", "consideration to certain solution with upcoming trend or innovation to extend, for example, for vision based, it may not require to create map firstly to navigation in the future.", "More pluggability in the elements of the nav stack, and the ability to hot-swap implementations.", "A common API that will allow people to add their own underlying representations.", "The ability to handle time.  I\u2019ve been toying with the idea of a map that changes throughout the day (as corridors become congested, and such), so the ability to integrate this into the system would be important to me as a specific use case.", "The ability to run more than one algorithm at a time, compare the results and mux them.  This is important for localization, where it can be used to compare the performance of two algorithms, or to use different algorithms at different time.", "Factoring things up as finely as possible.  As Geoff pointed out, this should be more lightweight in ROS2", "Ability to develop in Python or C++.", "Use floats or doubles as the underlying representation, not some fixed-point hack.  Actually, it might be nice to use arbitrary underlying representations (of probabilities).", "Some more modern algorithms from the literature.", "Being able to swap maps in and out of core seamlessly, so that I don\u2019t have to keep all of campus in memory at the same time.", "Multiple floors in a building.  Hybrids to let me get from one building to another."], "url": "https://discourse.ros.org/t/ros2-navigation-input-requested/4884"},
{"title": "ROS2 Security Working Group Online Meeting - September 17th, 2019 between 10AM and 11AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on ", " \u2192 ", "Please suggest topics for the next meeting by commenting on this thread.", "\n", " to receive calendar invites in", "\nyour e-mail inbox.", "ROS 2 Security Working Group Meeting Notes Working Group Calendar URL | Google Group for invites  Please join the Google group to get calendar invites to the meeting and be able to comment on this doc. Want to help facilitate this working group?...", "You have been invited to an online meeting, powered by Amazon Chime.", "\nChime meeting ID: 1216476874", "\nJoin via Chime clients (manually): Select \u2018Meetings > Join a Meeting\u2019, and enter 1216476874", "\nJoin via Chime clients (auto-call): If you invite auto-call as attendee, Chime will call you when the meeting starts, select \u2018Answer\u2019", "\nJoin via browser screen share: ", "\nJoin via phone (US): +1-929-432-4463,1216476874#", "\nJoin via phone (US toll-free): +1-855-552-4463,1216476874#", "\nInternational dial-in: ", "\nIn-room video system: Ext: 62000, Meeting PIN: 1216476874#", "Hi everyone, ", ".", "We also have the pleasure to announce that Canonical will be leading the ROS 2 Security Working Group from the next meeting. AWS will continue participating in the WG as we did over the last year.", "Thank you ", " for your excellent stewardship of ROS 2 security! We at Canonical are thrilled to be handed the baton. More details will come soon!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-september-17th-2019-between-10am-and-11am-pdt-utc-7/10380"},
{"title": "ROS2 Security Working Group Online Meeting - August 7th, 2019 between 9AM and 10AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on ", "Please suggest topics for the next meeting by commenting on this thread.", "ROS 2 Security Working Group Meeting Notes   2019-08-01 Meeting Announcement on Discourse   Calendar Invites: Working Group URL Google Group for invites  Generic Node Interface Description @kyrofa  FIXME  ROS 2 Access Control Policies @ruffsl  FIXME ...", "You have been invited to an online meeting, powered by Amazon Chime.", "I\u2019d like to discuss easier policy generation by designing a generic node interface description to be specified by individual packages, thereby allowing an end-user to generate a policy without needing to be an expert in every component being used (think Apache shipping its own apparmor profile rather than expecting the average Apache user to do it).", "We could also discuss the design document for ROS 2 Access Control Policies:", "Sounds great! Looking forward to", "Thanks everyone for your time today!", "\nThe ", " have been updated.", "\nYou can also access the meeting recording from the doc.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Generic Node Interface Description - ", "\n", "ROS 2 Access Control Policies - ", "\n", "ROS 2 Node Sandboxing - ", " (AWS)"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-august-7th-2019-between-9am-and-10am-pdt-utc-7/10146"},
{"title": "ROS2 Real-time Working Group Online Meeting 3 - Oct 2, 2019, 7AM PDT (UTC-7)", "thread_contents": ["Hi real-time folks, time for our 3rd meeting.", "Proposed agenda:", "Last meeting ", " and ", ".", "Respond to this conversation by answering to:", " ", " ", " ", " fyi", "Hi Dejan,", "We (", " and I) will be attending this meeting.", "The version we will discuss is a POC on the latest stable dashing release. We are currently also working on a PR (master) version.", "thanks for taking care of this thread, i would like to  join at this time.", "thanks", ",", "Thanks for the invitation. I\u2019m available.", "\nThese would be my topics:", "Kind Regards,", "\nAndrei", "Hi all, below are the meeting invite details.", "Topic: ROS2 Real-time Working Group Online Meeting 3 - Oct 2, 2019, 7AM PDT (UTC-7)", "\nTime: Oct 2, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "Meeting ID: 571 727 736", "\nPassword: 074189", "One tap mobile", "\n+16699006833,571727736# US (San Jose)", "\n+19294362866,571727736# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 571 727 736", "\nFind your local number: ", " thanks a lot.", "Can you and ", "  then present this in details tomorrow?", " Will you be able to attend the meeting tomorrow (4PM German time)?", "If yes - do you have any update on this topic:", "Repository of tools for static and dynamic code analysis", "Ralph Lange and Jan Staschulat will be there.", "Hi Dejan,", "I\u2019ve made a small presentation that shows the differences between the current executor and our proposal. I think it is easy to follow even for people who don\u2019t have a deep understanding of how the current executor works. I will need about 10-15 min. to explain the executors and their differences, but I will need presenter view for this (screen-sharing). Is this possible? I have no experience with Zoom Meetings.", "The presentation will be quite short, but if people are interested we can have a discussion after the presentation about the possible next steps and features people require. The executor we made shows possible CPU gains and gives an alternative for specific use-cases, but a proper all-round executor will need additional features.", "\nFeatures that are currently on the TODO list:", "If there is no time during the meeting to discuss these things then people can just post their thoughts on the PR ", ".", "Kind Regards,", "Martin", "I will need about 10-15 min. to explain the executors and their differences, but I will need presenter view for this (screen-sharing). Is this possible? I have no experience with Zoom Meetings.", " yes, this is absolutely possible and easy.", " we started with the long list of goals couple of months back: ", ".", "In that thread I proposed  an overall goal which would be to make ROS 2 hard real time. This would then mean that we\u2019d need to \u201canalyse an entire stack, from the hardware platform to the applications written with ROS 2\u201d and improve the realtime across the entire stack.", "However the guys have, rightfully so, ", " that this is simply too much work and it depends too much on the use case and selected underlying stack (ECU, RTOS, network interfaces, \u2026).", "As a result we agreed to work on the following sub-goals:", "Additional goals are of course welcome but we will also need someone to work on them.", "All (12 participants), thanks for attending.", "Old link: ", "New link: ", "Check out zhe rmw implementation - all", "Looking for 1-2 candidates to join Erik and his team in creating ", " on top of  ", "  - ", "Create a system for documentation sharing (GDrive, Github project, \u2026) - ", " is this something that you could maybe do?", "Get the presentation from Martin - ", " could you paste it here?", "Present the mini buildfarm with resource guarantees at Apex.AI - ", "Carlos has been working on the demo that shows some of the real-time characteristics. - ", " can you share the demo with us?", "Dejan to take content and conclusions of this meeting to OSRF and TSC- ", "PRs for the static executor - all have a look and comment", "ROS 2 Real-time WG to meet every 2 weeks (Wednesdays 7AM PST)", " - would you be able to join the meetings for this group?", "Yes, I\u2019m happy to join any meeting you think I could help with, please ping me directly for each meeting you\u2019d like me to attend. If you need me on a regular basis I can do that too, though selfishly I\u2019m trying to stay out of the way as much as possible.", "ROS 2 Real-time WG to meet every 2 weeks (Wednesdays 7AM PST)", "Is it at all possible to shift this? It clashes perfectly with AWF board meetings.", "Hey everyone,", "Discourse won\u2019t let me upload the presentation as a powerpoint and the presentation in pdf format is not really usable (the transition of images on a single slide is how I show the working of the executors). Instead I will share some links here, while I look for a way to share the powerpoint.", " this link contains a README where the executors are explained with flowcharts. The flowcharts + explanations get the ideas across of each executor.", " This is the fork we use for the pull request that is up to date with master (slightly behind by now probably).", " we also made a version that we merged with ", " PR for improved intra_process communication.", "For the people that are not aware of why we worked on this static executor here is a very short history. There is more links in the links pointing to more links if you feel like going on an adventure.", "\n", "\n", "\n", " (related, but less interesting)", "Hopefully this gets everyone what they are looking for. If you want more information or help with the code or understanding the STE or static executor, don\u2019t hesitate to send me a message.", "Your flow chart for your static executor is missing some labels on the decision branches.", "I do not have a lot of experience making flowcharts, nor do I know the exact rules to making them. I hope that despite the inaccuracies they are still readable. Sorry for the possible inconvenience.", "Any diamond is a decision. Each arrow going ", " of the diamond needs a label to say what the condition is for that branch being taken.", "I added the powerpoint that I used during the meeting for my presentation to ", " in the presentation folder. Please let me know if it works by downloading it and looking at it in presentation view (to cycle through the images).", "Sidenote:  The dashing version now uses the nodes\u2019 guard_conditions as event trigger to rebuild the executable list and the wait-set. Making the use of the static executor less restrictive (semi-dynamic, rebuild only when necessary). We will continue work on a master (eloquent) version and look into a multi-threaded version.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Single process, real-time rmw => ADLink", "Repository of tools for static and dynamic code analysis => ", "\n", "Performance testing on target hardware => Apex.AI", "I would propose to hold meetings regularly, every 2 weeks", "Are you available?", "Do you have additional items for  the agenda?", "Can you expand on your agenda items?", "Yes we are available", "We created a POC StaticExecutor for rclcpp that can be added without breaking existing source code. If someone wants to use the StaticExecutor instead of the SingleThreaded executor, this only requires a change to one line of code at user-code level.", "The code and preliminary results can be found here ", ". This StaticExecutor significantly reduces CPU usage. We can quickly explain how we changed the existing executor. We have aligned with Ingo and his team to collaborate on a better executor for ROS2. Our focus is on ", " whereas Ingo and his team are working on proper ", ", both of which are important for RT.", "go via AIs from the last meeting", "discuss probably the goals of the RT WG (Personally I\u2019m not sure I understand them, and during the last meeting I got a feeling that those goals are quite different for the RT WG participants)", "Different scheduling mechanism (Bosch, create RT safe scheduling, add priority)", "Semi-dynamic setup (Nobleo, rebuild the system based on event triggers, i.e. something added to a node)", "Provide a use case description and requirements", "Implement real-time able rmw (single threaded, static executor)", "Present findings on real-time audit in rcutils, rcl and rclcpp and continue improving", "Create a shared repository for tools for static and dynamic code analysis and tracing", "Joe Speed, Erik", "Ralph, Jan", "Dejan", "Andrei", "Carlos", "Martin", "\u2026", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", " - would you be able to join the meetings for this group?", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros2-real-time-working-group-online-meeting-3-oct-2-2019-7am-pdt-utc-7/10741"},
{"title": "ROS2 Security Working Group Online Meeting", "thread_contents": ["Hi all, I\u2019m an engineer at Amazon working on ROS2 and wanted to get in touch with people working on SROS2 and the Crystal release of ROS2. I\u2019ve met with a few folks in the community and want to make security intuitive, easy to configure, and performant in ROS2. There were some great talks at RosCon regarding ROS2 and security and I would like to join forces with people working on security-related items for ROS2.", "Some gaps to be addressed for ROS2 Crystal:", "This is by no means an exhaustive list, purely just what could be the minimum bar for a system based on ROS2. I would really like to get input from folks working on security and ROS2 and want to host an online meeting on October 15th @ 17:00 PDT.", "Please reply if you are interested.", "Some gaps to be addressed for ROS2 Crystal:", "It\u2019s not realistic for ROS2 Crystal as well however worth to be considered for the long time roadmap:", "There is a ", " about how to integrate public repositories into ", " (\u201ccontinous open source software fuzzying as a service\u201d) and about how to make OSS-Fuzz work for private repositories. Google tries to motivate people to integrate projects into OSS-Fuzz with ", "\u2026 probably an interesting model to get people like ", " (", ") involved into ROS2 security improvement ", " . However OSS-Fuzz based Fuzzy Testing addresses low levels of abstraction (source code like ", ", ", ", ", ") the priority in comparison to the other point in the list (higher levels of abstraction like features, \u201csecurity by design\u201d) is quite low. Nevertheless worth to being mention here I guess.", "I would really like to get input from folks working on security and ROS2 and want to host an online meeting on October 15th @ 17:00 PDT.", "Sounds good, count me in!", "\nI could start earlier though, as I\u2019ll have to leave for another meeting after 18:30 PDT.", "\nCan you post a link to the online meeting?", "FYI: Meeting notes from previous security meetup:", "Here\u2019s the meeting info:", "October 15th @ 17:00 PDT", "You have been invited to an online meeting, powered by Amazon Chime.", "Meeting ID: 3393 57 7780", "United States Toll-Free: +1 855-552-4463", "\nMeeting PIN: 3393 57 7780", "One-click Mobile Dial-in (United States (1)): +1 206-462-5569,3393577780#", "United States (1): +1 206-462-5569", "\nInternational: ", "Meeting PIN: 3393577780#", "I completely agree, fuzz testing would be a great thing to add. There\u2019s quite a few more things like HSMs and other ideas that people have thrown out which would be very useful.", "I want to try to capture all of this, however I would also like to primarily focus on what could be there for Crystal as well.", "Thanks for setting this up.  I will call into the meeting.", "I\u2019ll be there.", "I\u2019d like to see some type of threat scenarios listed with the security measures listed for each.", "For example:", "\nScenario 1: Robot running in a public place, connected to wifi with other users on it.", "Those are some admittedly simple examples. I would like to see a more comprehensive list and some type of security answer for each as to how to defend against the threat.", ", it\u2019ll be great if you could share the notes after the meeting (or even better, record it)", "\nI\u2019m sure there\u2019re several individuals and groups that like myself, would love to join however 17:00 PDT is a bit late in Europe.", "Looking forward to see the results of it.", "I\u2019d like to see some type of threat scenarios listed with the security measures listed for each.", "+1 for this approach. Maybe compiling a list or a joint document would do.", "FYI: Back in my ", " slides, page 19, I listed some other possible action items for SROS2:", "I\u2019ve uploaded the session slides, linked inline with the schedule, and additional materials to onto the tutorial website ", ". Unfortunately my digital recorder failed, and I lost the audio I attempted to capture during the presentations, but the added reference publications and linked videos should include the same degree of information.", "Good call ", " I\u2019ll make sure to record the meeting and try to have it available. At a minimum, I\u2019ll make sure to post notes and what we covered here.", "Much appreciated ", " !", " there is a lot of great content in this presentation. Are you actively working on all of the items in here? It appears there\u2019s some overlap here in the parts we\u2019re targeting. Specifically:", "I\u2019d be really interested to see your how you\u2019re thinking about these items and if there\u2019s some way we could collaborate on them.", "Thanks,", "\nRay", "Hi everyone,", " and I will try to also attend the meeting, even if it\u2019s a bit later for us here in Europe.", "We wanted to point out that the RCTF(", ") is already available and open for contributions. As we pointed out on ROSCon, it can be played both online and offline, as the containers for the scenarios are available at ", ", they are also open for modification and contribution too.", "It would be very interesting to propose new scenarios that align with the security strategies of ROS2, so we can both train the security researchers to find weaknesses in misconfigured ROS2 systems, as well as to train developers to take care of the security aspects in ROS2 so they actively enforce them.", "Right now the available scenarios only cover the basic aspects, but we are working to include more complex scenarios, and specially, more focus on ROS2. Feel free to point out any feedback or improvements that you would like to see, and of course, feel free to create new scenarios that you would like to be included!", "Thanks ", " I appreciate you taking the time out to make it, even though it\u2019s a very inconvenient time. I think it would probably be good to schedule a time more amenable to folks in Europe very soon.", "Thanks,", "\nRay", "Thanks for organizing. I\u2019m looking forward to the discussion. -Morgan", "Thanks to everyone that attended last night! It was a great discussion and I\u2019m happy to find out what people are building related to security. In addition to the summary, I have a recording of the session and will post it as well once I\u2019ve got the logistics worked out. Unfortunately since I did not have anything on the screen for most of the meeting, the video is completely black, however the audio is all there.", "We decided to meet again within 2 weeks and I\u2019ll post again once I\u2019ve verified an appropriate time. All future meetings will be held in morning US time so we can make sure to include as wide an audience as possible. If you have problems making the schedule, please let me know.", "I am also posting a separate meeting on Thursday AM PDT to have all the folks I unintentionally excluded from Europe due to the timing of the meeting.", "Reviewed security-related tasks Amazon is delivering for ROS2 Crystal", "Focusing on simplifying configuration, logging, security development, and community education", "Review attached presentation for details", "Ruffin / ", " presented several key items related to SROS2", "Dynamic topic names - could potentially impact security configuration, are teams doing things like this?", "Gerardo / ", " presented what RTI is currently working regarding ROS security", "ROS2 Threat Model", "Apex.ai", "A couple of PRs are out there now for improving how security artifacts are retrieved", "Open question: How do we deal with security failures?", "Meeting again within 2 weeks", "I wanted to get a chance to talk to as many people as possible so I\u2019m having another session Thursday morning for folks in other time zones. In the future, I\u2019ll try to schedule meetings such that we can have a single group, however for this first one, I want to give everyone a chance to talk about what their working on.", "You have been invited to an online meeting, powered by Amazon Chime.", "Meeting ID: 5568 19 1908", "United States Toll-Free: +1 855-552-4463", "\nMeeting PIN: 5568 19 1908", "One-click Mobile Dial-in (United States (1)): +1 206-462-5569,5568191908#", "United States (1): +1 206-462-5569", "\nInternational: ", "SIP video system: meet.chime.in", "\nor", "\nH.323 system: 52.23.133.56", "Meeting PIN: 5568191908#", "I have both the presentation and a recording of the meeting available.", "\n", "Thanks ", " for uploading the recording and the notes from the meeting. See you in the next one!", "We had a great second meeting for the folks in other time zones. In attendance were people from Amazon, RTI, Alias Robotics, UCSD, and Acutronic Robotics. Unfortunately I completely forgot to record the meeting so the only artifact is the summary below.", "I\u2019m tentatively going to schedule the next meeting for October 30th @ 08:00 AM PDT. Please let me know in the next couple of days if this is not a convenient time, otherwise I will post here with the meeting details.", "Alias", "RTI", "Threat model", "Should security be exclusive with performance?", "Does the sensitivity of the data merit the performance hit (tf or odometry)", "Realtime systems", "How do we deal with security failures?", "SROS2 tutorial has a walkthrough on securing Turtlebot3", "Thanks for everyone for attending!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Secure services and parameters", "Secure key storage", "Automated security recommendations and configuration", "Secure, signed configurations management", "Auditing and logging", "External network connectivity", "Security best practices", "Promoting security-driven tests", "Ray Cole", "Secure services and parameters", "Secure key storage", "Automated security recommendations and configuration", "Secure, signed configurations management", "Auditing and logging", "External network connectivity", "Security best practices", "Promoting security-driven tests", "integration of fuzzy testing in to the CI environment", "Click to join the meeting:", "You can use your computer\u2019s microphone and speakers, however, a headset is recommended. Or, call in using your phone:", "Threat - user can potentially start publishing to ROS2 topics, how do we ensure no user can publish to CMD_VEL topic for instance and start driving our robot?", "Threat 2 - instead of publishing to a topic, what if they try to make service requests over and over to create a DoS attack?", "Assistive permission policy generation", "Descriptive connectivity manifests", "Procedural provisioning security artifacts", "Expressive security policy definitions", "Generation, deployment, revocation of PKI", "Distributed logging over networks", "Recording Security Events levels", "Adding additional automated CI tests", "\n", "\n", "\n", "\n", "\n", "\n", "ROS2 Threat Model", "Security specific integration tests for eProsima and RTI", "Security file generator for configuration", "CMake target for generating configuration for testing", "Snapshot tool to generate access control (\u201csecure my system\u201d)", "Recommendations for key/config security", "Security event logging", "\n", "\n", "\n", "Procedurally Provisioned Access Control for Robotics Systems - ", "\n", "Candidate frameworks for automation and testing\n", "Keymint - Meta-build system key generation and signed artifacts - ", "\n", "Abstract communication language (ComArmor) - graph of subjects and objects and generate artifacts at compile time", "Maps object or service to DDS mechanisms", "\n", "\n", "Scrape metadata about DDS at runtime and build security artifacts for compilation", "Want to use static verification and include results in security manifest", "Include all elements, topics/messages/services in the manifest", "\n", "\n", "\n", "Do we need wildcarding or something else to handle salt in topic names?", "Typical configurations have static connections between nodes along known topic names", "\nstarting point for creating their own threat model", "\n", "\n", "\n", "No outstanding plans for ROS2 right now", "Believe existing patterns can be used for services and parameters", "Will depend on design of how these elements are implemented on top of DDS", "If use cases are not covered by existing DDS security specifications, additional support could be needed", "Data tags could be used by the identity layer / constrains on security", "Service mapping is using topics and not DDS service names, which means its unclear if changes are necessary", "What granularity is needed for parameters, all-or-nothing or more granular permissions", "\n", "\n", "\n", "Are there existing threat models for ROS2 out there?", "Will not target a specific robot but be a \u201ccookbook\u201d to enumerate possible threats and provide someone building a specific system", "Will use a document on github for collaborating on the threat model", "Come up with a basic template for a threat model\n", "Possibly multiple templates because the domain is so large", "Start with small number of concrete scenarios and try to expand from there", "\n", "SROS2 issue tracker has several long standing issues on the topic", "Many security papers are already out there to draw upon", "\n", "\n", "\n", "Impossible to get a single threat model", "Use STRIDE for modeling threats ", "\n", "Focusing on several different areas: anomaly detection, data integrity, and static analysis", "Ament plugin for pclint and colcon build package to show code coverage", "Security concerns have been around QNX\n", "Preconfiguring the entire system (baked, signed, and shipped)", "\n", "\n", "\n", "\n", "Currently the keystore directory matches the name", "PR to traverse the namespace to allow multiple packages with the same name to have separate security artifacts", "\n", "\n", "\n", "What should be the behavior when a node failed to authenticate/authorize?", "Depends heavily on the implementation, it may be ok to have reduced functionality or could be a critical safety issue", "Should it be modeled similar to mobile applications, where there is a fallback behavior?", "Should there be specific actions taken on failure?", "Need the ability to run in audit mode to find errors", "\n", "\n", "\n", "Click to join the meeting:", "You can use your computer\u2019s microphone and speakers, however, a headset is recommended. Or, call in using your phone:", "To connect from an in-room video system, use one of the following Amazon Chime bridges:", "\n", "\n", "Current in assessment phase for ROS2", "General check for vulnerabilities", "Interest in collaborating on threat model", "\n", "\n", "\n", "Not working specifically on security for ROS2", "Should parts of DDS need augmentation, happy to collaborate on them", "\n", "\n", "\n", "Collaborate via a wiki on SROS2 repo", "Want to start with a less complex, publicly available system to model as an example", "Could use the Turtlebot3", "Victor @ Acutronic offered to use ", " as a possible alternative", "\n", "\n", "\n", "Need to balance security and performance", "May want to have subset of nodes secure", "May only sign or could be sensitive data", "Publicly known data not very sensitive", "High performance, high through put topics may not tolerate problem", "\n", "\n", "\n", "Someone could reconstruct sensitive information from non-sensitive data", "Reconstruct context based on partial information", "Default should be total security", "Model how does partial disclosure affect the system", "There is a paper in the SROS2 tutorial about security, latency, throughput", "\n", "\n", "\n", "Security on realtime systems could impact the realtime aspects", "Various security related functions that will need to happen", "Handshake could cause some non-deterministic elements which would be detrimental to realtime", "Are there other non-deterministic security related functions that could affect realtime systems?", "\n", "\n", "\n", "Extend lifecycle state related to safety of the component", "Allow system to recover by fixing the issue", "Could have mediator that fixes the issue", "This could have problems if nodes begin requesting permissions not needed before", "Nodes/messages could be marked as critical and cause an error if those messages are not able to be processed due to permission errors", "Would require the CA to live close to the system", "Have specific error modes when permission", "\n", "\n", "\n", "Compilation has problems since there\u2019s not a 32-bit build of ROS2", "Use QEMU to cross-compile", "Problems getting the XRCE agent with security enabled, could not communicate with the XRCE node", "Ended up with insecure XRCE nodes and using the RTI router to connect it to the rest of the secure graph", "Need agent to be able to relay the XRCE traffic under it\u2019s own GUID potentially?", "\n"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting/6393"},
{"title": "ROS2 Security Working Group Online Meeting - Feb 13th, 2019 between 2:00 - 3:00 PM PST", "thread_contents": ["Hi Everyone,", "I work with ", " who has been organizing these meetings in the past. First of all, we\u2019d like to apologize for not organizing this for some time now. I am setting up next WG meeting on 2/13 between 2 - 3 PM PST. We can use first few minutes of the meeting to walk through security tooling we released as part of Crystal, do a quick demo and we can open it up for anything else we want to discuss. As always, We\u2019re happy to take suggestions on an agenda. Please let me know if there\u2019s something you would like to discuss.", "You have been invited to an online meeting, powered by Amazon Chime.", "Meeting ID: 1287 75 7019", "United States Toll-Free: +1 855-552-4463", "\nMeeting PIN: 1287 75 7019", "One-click Mobile Dial-in (United States (1)): +1 206-462-5569,1287757019#", "United States (1): +1 206-462-5569", "\nInternational: ", "SIP video system: meet.chime.in", "\nor", "\nH.323 system: 52.23.133.56", "Meeting PIN: 1287757019#", "Thanks everyone who made time to attend this. ", "  is the link to the recording of the meeting.", "Summary:", "Discussed security tooling discussed as part of ROS2-C: ", "Discussion around how this can be used for a fleet of robots. This out of the box is intended towards making local dev easier. There is still  work needed for fleet management and key/cert distribution across fleet of robots.", "High level discussion about current state of the threat model Amazon is  putting together. We discussed out current approach based on STRIDE (", "). Feedback was mainly around using different hardware platforms to include platform specific threats. We\u2019re hoping to make this available to the community in 2-3 weeks of  timeline (publish on discourse and/or ROS2 design page). Idea is to keep this as a live document where community members can collaborate on including variety of hardware platforms, threats that are not captured and discuss potential mitigations for those threats. We will setup another Security WG meeting in 2-3 weeks timeline to review final version of threat model document.", "Cheers,", "\nRutvik", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Click to join the meeting:", "You can use your computer\u2019s microphone and speakers, however, a headset is recommended. Or, call in using your phone:", "To connect from an in-room video system, use one of the following Amazon Chime bridges:", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-feb-13th-2019-between-2-00-3-00-pm-pst/7639"},
{"title": "ROS 2 Real-time Working Group Online Meeting 7 - Dec 11, 2019, 7AM PDT (UTC-8)", "thread_contents": ["RTWG folks, here is an agenda for our next meeting. Please add your topics if you have them.", "Zoom coordinates: TBD.", "More information from the meeting recording:", "I\u2019ll let ", " add more details on Wednesday!", "Hi Dejan,", "Both Ishu and I are busy/not in the office tomorrow, so we won\u2019t be able to attend this meeting.", "To address points 5.1-5.3:", "\nOur reasoning is as follows: We are very excited that our static_executor has gotten as much attention as it did (this was very unexpected for us) and we hope some of our work will end up in the core of ROS2 (this would be amazing!), but we\u2019ve always seen the static-executor more as a POC w.r.t. \u201cjust\u201d CPU utilization. The executor in ROS2 has some more flaws (i.e. scheduling as highlighted in the paper by Tobias) and our static-executor POC is build on top of legacy that blocks/prevents neat minimal code and further other improvements.", "It is difficult to rework rclcpp, rcl and rmw with everything sitting on top of it and equally difficult from the other perspective to keep up with API changes. We want to see what design OSRF/William/the community can come up with and what changes to ROS2 will (need to) happen to realize that design.", "With the limited manpower we have, we decided the nicest thing we could do was to wrap the static-executor into a separate library for both dashing and eloquent users to use. When we get closer to the F release and the design has been settled on, then we are definitely willing to contribute and help port the static-executor if desired. However, keeping up our PR with master in (what is expected to be) a quite tumultuous time w.r.t. rclcpp, rcl and rmw, is not really possible (especially since both Ishu and I are on different projects atm that take up a lot of our time).", "My apologies for the long explanation, but hopefully this shines a light on our point of view. We hope for your understanding and look forward to the design discussions and of course the final design for F turtle ", "Zoom coordinates: TBD.", "is that available now?", "At all, sorry for being late but here are the meeting coordinates:", "Topic: RTWG Meeting 7", "\nTime: Dec 11, 2019 07:00 AM Pacific Time (US and Canada)", "Join Zoom Meeting", "\n", "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference...", "\n", "Meeting ID: 910 363 819", "\nPassword: 935893", "One tap mobile", "\n+16699006833,910363819# US (San Jose)", "\n+19294362866,910363819# US (New York)", "Dial by your location", "\n+1 669 900 6833 US (San Jose)", "\n+1 929 436 2866 US (New York)", "\nMeeting ID: 910 363 819", "\nFind your local number: ", "\n", "about ", " only to connect specific device/robot, yesterday you mentioned some new feature is submitted by ", "  to DDSI-RTPS, right? is it already ready on ", "?", "could you enlighten me a little bit more? I\u2019d like to catch up this.", "Best", "Hi Tomoya, Yes, Erik ", " added domainTag to ", " and implemented iRobot\u2019s use case in Eclipse Cyclone DDS:", "Find Roomba by serial # among ~1,000 robots on the network", "With ", " things talk if domain and the tag (e.g. robot serial #) are matched.", "Happy to chat about that and other ", " contributions to the ROS community.", "\u201cEclipse Cyclone DDS makes ROS 2 Easier, Smaller, Faster\u201d talk today at ROS-Industrial Stuttgart RICEU2019 is a good overview of the ", " ", " and interesting use cases from the community ", "\n", "thanks, will look into it.", "Meeting minutes (thx ", " for help):", "Video recordings: ", " Jan 8 2020 (skipping the one on Dec 25 2019).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Presentation: SLX ROS Performance Testing Platform by Silexica", "Discourse post on how to measure performance correctly\n", "Jaime\u2019s talk transcribed", "\n", "LET executor ", " => ", " ", " ", "\n", "Architecture of single-process, real-time rmw => ", "\n", "\n", " would now be a good time?", "\n", "static_executor recap:\n", "William suggests to change executor design in ROS 2 F: ", "\n", "Martin agrees and offers the current version as an ", " library", "After the changes in ROS 2 F, we will port some parts from ", " to rclcpp?", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Discourse post on how to measure performance correctly\n", "Jaime\u2019s talk transcribed", "\n", "eProsima published latency tests results, looking at the median value of multiple runs", "using a clone of the system and same configuration as the one used by their competition", "with open source code for reproducibility", "Presentation: SLX ROS Performance Testing Platform ", "  by Silexica/Ben\n", "\n", " Ask Amazon if they want to host HW for performance testing platform (on ", ")", "Ben to give regular updates in the RTWG", "\n", "LET executor\n", "\n", " comments on the rcl PR (", ") will be addressed and then the PR will be re-directed to rclc (", ")", "Bosch will also provide the documentation and the use case description for LET executor", "William is OK with the full restructure of the rclc project", "\n", "Architecture of single-process, real-time rmw\n", "Erik\u2019s comment on memory allocation for the data path in rmw_cyclconedds: ", "\n", "\n", "  Members of the RTWG should provide feedback", "Doing it from scratch vs. building on rmw_cyclonedds => 2nd option is preferred", "Erik thinks it makes sense to implement the zero-copy interface (from Bosch) first", "ROS 2 F => Erik thinks that we should start making use of DDS keys\n", "\n", " take this request to ROS 2 TSC", "\n", "\n", "static_executor work to be done for ROS 2 F:\n", "be able to use more than one executor per node.", "Implement Waitset similar to the one at Apex => ", " to work with William", "Interface clean up around the executor", "\n", "ROS 2 F Roadmap will be finalized early 2020\n", "Input to the roadmap: brainstorming of ROS 2 team@OSRF & others", "\n", " provide your input for the ROS 2 F roadmap", "\n", "DDS direct access (", ")\n", "There exist 2 different solutions/approaches: DDS configuration vs accessing DDS APIs", "Borja\u2019s use case in microROS => configuring vendor specific implementation", "William => 3 ways to expose more DDS features:\n", "out of bound for configuration of the underlying DDS implementation", "programmatically (e.g. rmw_init(payload) which has been done at Apex, but it is not portable across DDS vendors,", "ROS way with lots of abstraction and which must be portable", "AI ", " ", ": provide more concrete set of requirements for this use case", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-2-real-time-working-group-online-meeting-7-dec-11-2019-7am-pdt-utc-8/11863"},
{"title": "Can we chat about breaking changes in stable releases?", "thread_contents": ["Hey folks, Dashing user here. Something concerning happened a few weeks ago that is continuing to have ripple effects in our work. That \u201csomething concerning\u201d is this:", "I\u2019ve read through ", " addressed by that PR and I have no argument with any of it. What I want to discuss is this: why was this introduced in a stable release? An LTS, no less? These marker files didn\u2019t need to be specifically installed, and now all of a sudden they do. Our CI broke without us changing anything, which turned into a flag day trying to find what broke and fix it, holding up releases and development. Several other ROS 2 packages broke, which sadly burned another team member\u2019s time tracking down the same issue.", "This is continuing to effect us, weeks later. My understanding is that we have a policy of breaking changes being okay between releases (e.g. this would have been fine landing into Eloquent), but can someone please explain how this made it back to Dashing?", "We\u2019ve had a few confused users on ROS Answers as well: ", " and ", ".", "I\u2019m not trying to justify it, but from a technical perspective, colcon is not part of the release. It is released for each operating system. So we cannot change this particular item in just Eloquent and not Dashing.", "This level of regression was certainly unforeseen and unfortunate.", "why was this introduced in a stable release?", "In general the pure Python packages (like ", ") are not ROS distro specific. On non-Debian platforms they are installed via ", " and on Debian-based platforms there are ", " packages available across all targeted Ubuntu/Debian distros. As such an update can\u2019t be made available for only ROS Eloquent but not for ROS Dashing (since both target the very same Ubuntu distro).", "If the level of issues would have been foreseen I would guess the change wouldn\u2019t have been merged as is. What made this hard to find was that if you had an existing workspace these marker files already existed so just testing the patch didn\u2019t show any problems. Also on CI for running the tests this wasn\u2019t a problem (and where it was it got resolved by PRs).", "Adding a warning (as now proposed in ", ") before removing the automagic creation of the marker files would have certainly been the better approach.", "Adding a warning (as now proposed in ", ") before removing the automagic creation of the marker files would have certainly been the better approach.", "While the warning is appreciated, it only would have helped track down the issue once everything broke, not prevented the breakage in the first place.", "from a technical perspective, colcon is not part of the release.", "As such an update can\u2019t be made available for only ROS Eloquent but not for ROS Dashing (since both target the very same Ubuntu distro).", "Thank you for the explanation. While there is certainly a technical difference, utilities used within stable releases tend to be lumped into \u201cthe stable release\u201d by us users, especially ones so pivotal as the build system. Breaking those tools feels the same as breaking the release. This seems like an unfortunate design decision, but it\u2019s one we need to own, not hide behind. I\u2019m assuming we can\u2019t/don\u2019t want to change it, but it backs us into a bit of a corner. As far as I can see, there are two possible paths afforded by such a decision:", "To be completely frank, in my opinion (1) would be the responsible ownership of this design. That does not seem to be the path we\u2019re on. If you insist on (2), can we at least come up with a strict scheduled deprecate-then-remove policy?", "Developing colcon is hard because you can essentially never break it as all stable releases are using the same version.", "That is the very same for each and every Python package we ship. And yes, while it is making things difficult we have managed pretty ok for the past ten+ years with that approach.", "Also I don\u2019t think the alternative (releasing different version into each ROS distro) would make things easier. E.g. you wouldn\u2019t be able to call any command until you have sourced a distro (chick-egg-problem) which isn\u2019t really feasible for a build tool which you need to bootstrap your very first build.", "The main issue here is awareness. When we do know changes are breaking stuff we are very carefully considering how to roll them out and/or modify the change to be compatible (e.g. REPs for evolving the rosdistro database).", "If preferred by the community I am absolutely fine with reverting the change for now and only go with the proposed warning. I think though there needs to be a possible transition to move forward at some point (after giving users a transition period and then breaking them if they haven\u2019t acted on it in a visible way). Otherwise we look us into the current state with no possibility to evolve in favor of stability.", "That is the very same for each and every Python package we ship. And yes, while it is making things difficult we have managed pretty ok for the past ten+ years with that approach.", "snip\u2026", "When we do know changes are breaking stuff we are very carefully considering how to roll them out and/or modify the change to be compatible [\u2026].", "I\u2019m a bit stunned by this response. I tried to create this thread to talk about exactly this. Are you implying that you see nothing wrong with what happened? There are no improvements to be had?", "Also I don\u2019t think the alternative (releasing different version into each ROS distro) would make things easier. E.g. you wouldn\u2019t be able to call any command until you have sourced a distro (chick-egg-problem) which isn\u2019t really feasible for a build tool which you need to bootstrap your very first build.", "To be clear, I\u2019m not suggesting that colcon be shipped in the underlay, it could just be a namespaced binary package frozen on a major colcon version (I\u2019ll ignore the fact that this change was introduced in the same major version ", "), ", " or some such thing. The challenge there is having the ability to have both D\u2019s colcon and E\u2019s colcon co-installable.", "If preferred by the community I am absolutely fine with reverting the change for now and only go with the proposed warning.", "I do indeed think it should be reverted, but of course I\u2019m only part of \u201cthe community.\u201d", "I think though there needs to be a possible transition to move forward at some point [\u2026]. Otherwise we look us into the current state with no possibility to evolve in favor of stability.", "I agree, but progress should never come at the expense of your users. I guess we\u2019ll agree to disagree on that point. If you must continue with the current design and if you cannot maintain backwards compatibility for utilities used within stable releases, then can we please establish a deprecate-then-remove policy with a clear schedule?", "it could just be a namespaced binary package frozen on a major colcon version, ", " or some such thing. The challenge there is having the ability to have both D\u2019s colcon and E\u2019s colcon co-installable.", "Your suggestion seems to be very Debian-centric. How should that be done for other platforms (e.g. macOS and Windows) where users install the Python packages from ", "? I doubt there is any precedence in the Python community for doing N parallel releases of the same package based on the major version.", "Anyway the infrastructure we use to release Python packages doesn\u2019t support such a workflow. If you would like to see a drastic change like this that would probably require a thorough design ticket / REP to motivate the change, describe how each and every piece in the pipeline should change as well as coming up with patches to realize that proposal.", "Imo such a drastic change is neither needed nor warranted by this case. As mentioned already before if this problem would have been catched during review it would not have been merged and released in the first place. So the main area of improvement I see is that such changes should be reviewed more carefully - which also implies by more people which have to volunteer to do so.", "I\u2019ll ignore the fact that this change was introduced in the same major version ", "On a side note: following semver when the major version if zero breaks can happen even on a minor version bump.", "That being said: the change was introduced in a patch release. Again: this only happened because we were not aware of that significant regression. Otherwise we would have not only bumped the minor version but actually revised the PR before it got merged and release.", "then can we please establish a deprecate-then-remove policy with a clear schedule?", "A tick-tock cycle is certainly desired.", "One problem is that deprecations as they are done in Python are by default not visible. So commonly nobody notices them during the \u201ctick\u201d cycle and only gets hit when stuff breaks during \u201ctock\u201d.", " introduces an option to change that. The goal would be to recommend to everyone (e.g. CI) who wants to catch deprecations / warnings early to set the flag so something other than the default. Similar to what the Python ", " module recommends to test framework developers ", ".", "For this specific case the PR ", " reverts the change and adds warnings in the case where ", " installs the files implicitly for the package. Please feel free to provide feedback on that ticket (and its open questions).", "How should that be done for other platforms (e.g. macOS and Windows) where users install the Python packages from ", " ? I doubt there is any precedence in the Python community for doing N parallel releases of the same package based on the major version.", "requirements.txt? constraints.txt? No need to release separate pip packages, that\u2019s pretty well-defined as long as you follow semver.", "Anyway the infrastructure we use to release Python packages doesn\u2019t support such a workflow. If you would like to see a drastic change like this that would probably require a thorough design ticket / REP to motivate the change, describe how each and every piece in the pipeline should change as well as coming up with patches to realize that proposal.", "I\u2019m not advocating for a new design so much as pointing out that there are others that better jive with your preferred style of maintenance.", "A tick-tock cycle is certainly desired.", "One problem is that deprecations as they are done in Python are by default not visible. So commonly nobody notices them during the \u201ctick\u201d cycle and only gets hit when stuff breaks during \u201ctock\u201d.", " introduces an option to change that. The goal would be to recommend to everyone (e.g. CI) who wants to catch deprecations / warnings early to set the flag so something other than the default. Similar to what the Python ", " module recommends to test framework developers ", ".", "Excellent! How long of a tick/tock cycle are we talking?", "For this specific case the PR ", " reverts the change and adds warnings in the case where ", " installs the files implicitly for the package. Please feel free to provide feedback on that ticket (and its open questions).", "Thank you for that. I\u2019ve provided some feedback, we can continue discussion on this specific case there.", "requirements.txt? constraints.txt? No need to release separate pip packages, that\u2019s pretty well-defined as long as you follow semver.", "Can you expand on how you would use this to fix the issue for macOS and Windows (or even non-deb based Linux distros)? I think it is easy to point out related features/technology, but difficult to apply it in practice. The \u201cdevil\u2019s in the details\u201d and all that.", "I can imagine a few ways to use those features in our stack/release process but none that jump out at me as being strictly better.", "I think it\u2019s worth reiterating these two points as well: a) it was an unintentional break (we would have done it differently had we known impact), and b) colcon is following semver, but colcon is not >= 1.0.0 yet so the rules are much less strict. So in this particular case, I don\u2019t think using anything but an exact version number constraint would have helped avoid the issue.", "Can you expand on how you would use this to fix the issue for macOS and Windows (or even non-deb based Linux distros)? I think it is easy to point out related features/technology, but difficult to apply it in practice. The \u201cdevil\u2019s in the details\u201d and all that.", "I can imagine a few ways to use those features in our stack/release process but none that jump out at me as being strictly better.", "Fair criticism. Again, though, my point is less to debate the design and more to point out that the design on which you have settled has ramifications that I don\u2019t currently feel are being respected.", "a) it was an unintentional break (we would have done it differently had we known impact)", "Then why not immediately revert and go a different path? I see ", " pull requests to ", " fixing the fallout from this. There were ", " ", " about it. But not until I whined was anything done about it. In my opinion, if it was truly unintentional, it should have been reverted as soon as you noticed breakage.", "I appreciate that things are happening now, thank you both for that. However, I\u2019d like to see some lessons learned and changes made, or this type of thing can easily happen again, further harming ROS 2\u2019s reputation. Defining a tick/tock cycle and sticking to it is a satisfactory change.", "Then why not immediately revert and go a different path?", "Unless I misunderstand (I am not following colcon development extremely closely right now), that is what\u2019s happening. We\u2019re unbreaking people and introducing a warning instead, as soon as possible.", "I see ", " pull requests to ", " fixing the fallout from this, but not until I whined was anything done about it.", "Those were to master (will become Eloquent) which we knew needed to be fixed. Our mistake was failing to imagine what would happen (if anything) to Dashing. Normally we wouldn\u2019t do that. Dirk even asked in a ROS 2 meeting with everyone there and no one thought to bring this up.", "In fact, I think the idea was to make the change and use our nightly to find the problems and fix them, which in retrospect was reckless, but again we didn\u2019t think about the fact that it affected all the distributions immediately. We were in our most commonly correct mindset of \u201cthe latest release only affects master\u201d.", "However, I\u2019d like to see some lessons learned and changes made, or this type of thing can easily happen again. Defining a tick/tock cycle and sticking to it is a satisfactory change.", "Sorry you and others were on the bleeding edge on this one, but I still haven\u2019t heard a suggestion of what to change that we can act on. We already use tick-tock situationally between releases, and in the \u201cextra-distro\u201d python packages where it is possible.", "We would have used that here, but we didn\u2019t notice the issue until too late. I think the plan is to use it now,  retroactively.", "we didn\u2019t think about the fact that it affected all the distributions immediately.", "That\u2019s exactly the ramification of the design choice that I was referring to ", " that I didn\u2019t feel was respected. It\u2019s an easy mistake to make (because you designed it that way), and I see nothing preventing it from happening again. Again, not trying to debate the design, just saying you gotta own it. We\u2019re trying to convince customers that this is the choice for production robotics. \u201cOops, won\u2019t do it again\u201d doesn\u2019t tend to convince customers that well.", "Sorry you and others were on the bleeding edge on this one", "Haha, the stable release is the bleeding edge?", "I think the plan is to use it now, retroactively.", "Alright, thank you.", "Let\u2019s say we created a repo of packages for each maintained ROS 2 distro. These packages are built and their tests are run each time colcon changes. Colcon releases are blocked on those tests being green. That way each time a breakage like this happens, we add a new package/test to ensure it doesn\u2019t happen again.", "Thoughts?", "Then why not immediately revert and go a different path? I see ", " pull requests to ", " fixing the fallout from this. There were ", " ", " about it. But not until I whined was anything done about it.", "Again, making such statements after the fact doesn\u2019t help. Now in retrospect it is easy to state what could have been done differently (nobody argues about that). The numerous PRs were aiming to fix the problems - the scope just got bigger over time - and now we are moving forward with the revert.", "The fact that we do have such an important PR to revert the change pending and it doesn\u2019t get much attention in terms of feedback / review is one part of the problem. When it comes to the build tool or the build system in ROS 2 it often comes down to me doing all the work without others stepping up. I pretty frequently have to beg people to add their +1 in order to get PRs merged. If you ask me that fact is significantly contributing to this problem.", "I think the idea was to make the change and use our nightly to find the problems and fix them, which in retrospect was reckless,", "I don\u2019t think that was ever the intention. A lot of our CI builds turned over just fine because these missing files are only an issue in very specific scenarios.", "Let\u2019s say we created a repo of packages for each maintained ROS 2 distro. These packages are built and their tests are run each time colcon changes. Colcon releases are blocked on those tests being green. That way each time a breakage like this happens, we add a new package/test to ensure it doesn\u2019t happen again.", "We do have numerous CI jobs already. Pretty much all of them can be run (and are being run) for proposed changes. Additionally nightly jobs ensure that the current state works. But as with any kind of automated tests not all cases are covered by those. In some use cases users ran into problem where they used ", " on an installed underlay which lacked some package manifests and therefore didn\u2019t install the necessary dependencies.", "There is a limit to what you can do and cover by automated tests. I am not saying we shouldn\u2019t try. But just stating \u201ccreate a repo with packages and build and test it\u201d isn\u2019t going to cut it imo since it is completely unclear what the difference to the existing CI would be.", "A possibility that is not yet being discussed is the use of virtual environments. Installing colcon and other pure python packages in a virtualenv would allow you to not pollute the system-level site packages.", "Pardon my ignorance, but is it possible to freeze colcon releases for ros2 releases as a separate package instead to putting them in deb files that ", " suggested (or was this what you were suggesting in your latest post)?", "Something like:", "This was users that want the bleeding edge can just use this instead of being on the bleeding edge with the normal package?", "We do have numerous CI jobs already. Pretty much all of them can be run (and are being run) for proposed changes. Additionally nightly jobs ensure that the current state works. But as with any kind of automated tests not all cases are covered by those. In some use cases users ran into problem where they used ", " on an installed underlay which lacked some package manifests and therefore didn\u2019t install the necessary dependencies.", "There is a limit to what you can do and cover by automated tests. I am not saying we shouldn\u2019t try. But just stating \u201ccreate a repo with packages and build and test it\u201d isn\u2019t going to cut it imo since it is completely unclear what the difference to the existing CI would be.", "Look, obviously there\u2019s a gap in the existing tests, which is natural, but we should be able to plug it. Is there already a suite of end-to-end tests that use Dashing + the new Colcon release? If so I\u2019d happily add such a test there.", "I just want a sensible retrospective here. \u201cColcon accidentally broke. We\u2019re awfully sorry about that. In order to help ensure it doesn\u2019t happen again, we _____\u201d. Can we finish that sentence with something other than \u201chope\u201d?", "How long of a tick/tock cycle are we talking?", "Please consider to comment on ", " for this question. I refrained for now from making any suggestions.", "A possibility that is not yet being discussed is the use of virtual environments. Installing colcon and other pure python packages in a virtualenv would allow you to not pollute the system-level site packages.", "is it possible to freeze colcon releases for ros2 releases as a separate package instead to putting them in deb files that ", " suggested (or was this what you were suggesting in your latest post)?", "While that is certainly an option users would need to maintain separate venvs for each ROS distribution. Especially on Debian-based platforms that would mean not using Debian packages for ", ". That in itself posed a huge drawback since the venv doesn\u2019t get updated by the package manager. Imo the overhead pushed onto each and every user significantly outweighs the isolation advantage.", "Is there already a suite of end-to-end tests that use Dashing + the new Colcon release?", "Every existing CI job uses colcon to build a workspace. There are specific jobs for Dashing which by default use the released version of colcon but can be parameterized to use any branch. The problem is that these jobs might not fail for this specific case. Why? Because the tests often don\u2019t require the presence of e.g. the package manifest. The use case described in ", " as an example where the missing manifests were a problem doesn\u2019t exist in the CI jobs which only build a single workspace.", "In the meantime the ", " which had been pending for a while got reviewed, merged and released as ", " 0.1.13 which should be available from Debian as well as PyPI for users to update.", "It really is a shame that Debian doesn\u2019t provide something like Gentoo\u2019s versatile slots and eselect mechanism, but I think it would probably go against the nature of the platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Developing colcon is hard because you can essentially never break it as all stable releases are using the same version.", "Using colcon is hard because all stable releases are using the same version which means non-backward-compatible changes hit all users, even those of stable releases."], "url": "https://discourse.ros.org/t/can-we-chat-about-breaking-changes-in-stable-releases/10689"},
{"title": "ROS2 Security Working Group Online Meeting - May 1st, 2019 between 9AM and 10PM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on  ", " .", "Please suggest other topics for the next meeting by commenting on this thread.", "You have been invited to an online meeting, powered by Amazon Chime.", "thanks, i did not get a chance to jump in today.", "\njust to confirm, is that PST(UTC-8)? or PDT(UTC-7)?", " I am really sorry for the confusion, I meant PDT. I just updated the post in consequence.", "thanks for the confirmation, will be there from Japan.", "Thanks for taking the lead again organizing this ", "! Good to see you around and interested on this topic ", "!", "Hi all, I updated the agenda. Please let me know if this is missing any topic we should cover.", "Hi all, here are some pointers to what we discussed this week.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Safety & Security (Acutronic Robotics to discuss some ongoing work in the intersection of both)", "Database of robot vulnerabilities (discussion, disclosures and a new security article maybe?)", "ROS 2 Security Validation (Ryan / Amazon)", "\n", ".", "Link to the article we discussed during the meeting: ", "\n"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-may-1st-2019-between-9am-and-10pm-pdt-utc-7/8786"},
{"title": "Introducing the Robot Vulnerability Database", "thread_contents": ["Hello ROSers,", "As part of our commitment with security, ", " is glad to introduce the ", ", a community-contributed list of robot vulnerabilities and weaknesses.", "This effort aligns with Alias\u2019 mission to ", " and is the first public step we take towards implementing it. Briefly, we share the belief that vulnerability disclosure is a two-way street where both vendors and researchers, must act responsibly.  We thereby adhere to a ", " (read more about ", ") while other flaws such as simple bugs or weaknesses could be filed at any point in time. We notify vendors of vulnerabilities immediately, cooperate with them and favour a coordinated  disclosure where ", ", or sooner if the vendor releases a fix.", "This policy is strongly in line with our desire to improve the robotics industry response times to security bugs, but also results in softer landings for bugs marginally over deadline. According to ", ", most vendors are ignoring security flaws completely. Similar to us, we call on all security researchers to adopt disclosure deadlines in some form, and feel free to use our policy verbatim (we\u2019ve actually done so, from ", ") if you find our record and reasoning compelling. Creating pressure towards more reasonably-timed fixes will result in smaller windows of opportunity for blackhats to abuse vulnerabilities. Given the direct physical connection with the world that robots have,  in our opinion, vulnerability disclosure policies such as ours result in greater security in robotics and an overall improved safety. A security-first approach is a must to ensure safe robotic operations.", "The ", " is an attempt to register and record robot security bugs including both weaknesses and vulnerabilities (refer to ", ") The current content has been built over the past months and includes at the time of writing more than ", ":", "As contributors of ROS and ROS 2, we have create a particular section for ROS (currently only highlighting ROS 2 flaws) available ", ".  We have committed resources to maintain this list and process flaws while reporting about the status of vulnerabilities at the corresponding ROS 2 Security WG meetings. We invite everyone in the community to contribute and help processing security flaws. Currently and as recorded by our team at RVD, ", ":", "Over the coming months we expect to include several ROS and ROS 2 packages in our pseudo-automatic robot security pipelines and collaborate with maintainers while recording and addressing security vulnerabilities and weaknesses", "We\u2019d like to acknowledge and credit the support we received from the ROSin project which partially enabled the development of this work. In particular, RVD will be used to report the findings of ROSIN RedROS2-I and RedROS2-II FTPs, funded by the European Union\u2019s Horizon 2020 research and innovation programme under the project ROSIN with the grant agreement No 732287.", "Finally, a small disclaimer: ", "BTW, for those interested in learning more about our work but also about security in robotics, overall, we invite you all to attend the ", " that will happen within ROSCon 2019 in Macau!", "Do ", " rules that create world writable devices count as a vulnerability or a weakness?", "Hello ", ",", "Do ", " rules that create world writable devices count as a vulnerability or a weakness?", "IMHO, provided those rules were defaults and/or could be forced into a robot or robot component (remotely or even locally, that would just affect the severity) and take advantage of those devices (as part of an exploit), then according to our ", ",  it could be considered a vulnerability. In that case you should be able to provide it with a score for its severity using ", ". See the discusion ", " for a bit more of context in vulnerability/weakness.", "Our goal with existing reported (and future) flaws is to provide means for reproducing and validating its exploitability. We\u2019ve been prototyping for a while (refer to our early ", " approach) and are currently working on a prototype based in Docker. We hope to release is shortly. The idea is that each researcher reporting a flaw should provide a docker-based image that allows reproducing the flaw, arguing about it and ultimately, facilitating its mitigation.", "Interesting undertaking; we clearly need more attention paid to security issues in robotics.", "To help better understand the goal and how the process works, I have some questions:", "Why do we need a robot-specific database for cybersecurity vulnerabilities? Is there a shortcoming in the widely used ", " system? A ", " in their database shows more than a few entries that would seem to be of the type that you\u2019re proposing to keep track of.", "If disclosure of vulnerabilities is meant to be delayed for up to 90 days, but the community submits new vulnerabilities to RVD as public github issues, then how does the delayed disclosure work?", "When you notify a vendor of a vulnerability, does an email address (e.g., security@) suffice, or do you need require other communication mechanism?", "ROS 2 presents 236 security weaknesses", "I have a (maybe stupid) question: is it common practice for for these kind of vulnerability statistics to include test-only code? While problem in test code should certainly be addressed it looks to me that they aren\u2019t really a defect in the shipped software (since they are not part of the packaged software after the build).", "Another note regarding the published numbers here: the public GitHub tickets seems to contain ", " redundancy. The very same defect is being ticketed up to one hundred times just since the functionality / API is used in numerous packages - for each a separate ticket is created. I would certainly consider that to be only a single vulnerability / flaw (even though it affects many packages).", "Both of the above seems to \u201cinflate\u201d the reported numbers significantly therefore I would suggest to reconsider how to account for those in statistics like this. I would also be interested in the actual (non-inflated) numbers to see where we are roughly at.", "Why do we need a robot-specific database for cybersecurity vulnerabilities? Is there a shortcoming in the widely used ", " system? A ", " in their database shows more than a few entries that would seem to be of the type that you\u2019re proposing to keep track of.", "CVEs are managed by ", ", organizations authorized to assign CVE IDs. ", " (see Requirements at ", "). We are taking (what we think are) the right steps to become a CNA and will soon start submitting CVEs (and provide CVE IDs to reported vulnerabilities within RVD). By no means Alias Robotics intends to ", ", we aim to empower it.", "That said, while we look up to the work that Mitre and many other partners started within CVE, over the past year or so, Alias Robotics identified several limitations and started building RVD accordingly (a robot-", " database of vulnerabilities) and strong barriers to change things. Without getting into an extremely verbose reasoning on the things we\u2019ve tried (and failed) and would like to see improved within CVE for robotics, see below some of the aspects we dislike and consider critical to move forward in securing robots and their components:", ": while ", " is complete right about the fact that the current CVE List provides results when searching for ", " (43 CVE entries), ", " (13 CVE entries) and even the more generic (and misleading) query ", " ( 892 CVE entries), a closer look into results (at least to us) into realizing that finding ROS-related flaws is a challenge. Contributing to categorizing this information better is something we\u2019ve committed to but don\u2019t believe will happen in the short term given the complexity/limitations of how the CVE List works (and where robot-related vulnerabilities are the minority, still) ", ". There\u2019s a lot of work to do on this still and we have internal tickets for it. We plan to separate the existing template for reports into two (weakness and vulnerability, facilitating escalation from weakness to vulnerabilities). We also hope to automate the process of reviewing flaws by using parsers that automatically and periodically review all tickets and report/tag those that are malformed.", ": Let\u2019s take ", " (and related sub-reports within the entry) as an example, for a security researcher to reproduce this flaw and provide a mitigation or simply patch it temporarily in their shop-floor, more information would be required. The intrinsic system integration of the robotics field demands for additional bits of information. Examples include a well defined and appropriate severity (to priorize flaws), a reproducible environment and instructions (if feasible) and likely (though this is a personal feeling), a channel for an open discussion where other researchers might triage/contribute/discard the flaw itself (you will find that most robot-related flaws within CVE List have barely been triaged). ", ".", ": Working with robots is very time consuming. From my experience, anyone that has built a robot with ROS understands the pain of rebuilding workspaces across platforms. This is not a criticism though, it\u2019s likely an inherent characteristic of the complexity of the field and the tradeoff of the modularity of ROS. Mitigating a vulnerability or a weakness requires one to first reproduce the flaw. This can be ", " time consuming. Not so much providing the fix itself but ensuring that your environment is appropriate. ", ". We\u2019re still working on it and hope to make it available to everyone very soon.", ": CVE uses CVSS to report on the severity of vulnerabilities. As we discussed and published a while ago, CVSS has strong limitations when applied to robotics. Simply put, it fails to capture the interaction that robots may have with their environments and humans. This is critical when considering the severity of a flaw and has been discussed repeatedly in the security community. We\u2019ve been thinking about all these aspects for ", ", ", ": while some might disagree, from our iterations we found that the process with CVE is somewhat slow. From our research we found that most robots and robots explored nowadays (specially industrial robots!) are highly vulnerable. We believe that a more dynamic path would facilitate mitigating many of these vulnerabilities and accomplish our mission of ", ". ", " (such as checking prior tickets and invalidate it if repeated, tag it as malformed and request for more information, etc.)", "Alias Robotics has committed resources to all the things listed above but of course, support and contributions are more than welcome. We hope the ", " and its members can find a way to support us in this endeavour. Of course, contributing to the CVE List is something we all should do but in our opinion, that falls short.", "From our humble experience, we don\u2019t see CVE changing several of these aspects (in a somewhat acceptable timeframe). This to us, justifies the launch of RVD. We hope to prove with RVD that our statements regarding robotics are somewhat correct and that more resources should be allocated to it. Hopefully this will provide a much stronger argument to Mitre and other parties within CVE.", "\nThe ultimate reason why we decided to launch RVD is because we hope to demonstrate that some of these features are worth integrating into the CVE List.", "If disclosure of vulnerabilities is meant to be delayed for up to 90 days, but the community submits new vulnerabilities to RVD as public github issues, then how does the delayed disclosure work?", "The disclosure policy applies to Alias Robotics and our engineers. We hope to inspire the community with this policy. It\u2019d be great if other groups and individuals were to adopt it as well but we can\u2019t enforce it.", "Anyone can literally jump into the wild and publish vulnerabilities or even worse, sell them in dark markets (rather common from what we\u2019re observing lately). RVD provides a channel to do it responsibly. An approach could be to list the flaw as a weakness (according to our classification, vulnerabilities are a more elevated \u201cdegree\u201d but all vulnerabilities are weaknesses) and then reach out the vendor/maintainer privately providing more information about the flaw and offer support for its mitigation. Eventually, either after 90 days or after a fix has been shipped, the weakness ticket could be enhanced and the complete exploit could be disclosed turning the weakness into a vulnerability.", "A particular example of this is ", ". ", " and other folks have done an amazing job characterizing that and even disclosing a mechanims to reproduce it. The RVD ticket remains as a weakness because we still haven\u2019t found time to reproduce it and provide a mitigation for it. Once we do so, we will elevate it to a vulnerability.", "When you notify a vendor of a vulnerability, does an email address (e.g., security@) suffice, or do you need require other communication mechanism?", "The e-mail ", " is what we are using so far. We haven\u2019t identified the need of additional communication channels for now.", "That was a bit long, sorry ", " !", "have a (maybe stupid) question: is it common practice for for these kind of vulnerability statistics to include test-only code? While problem in test code should certainly be addressed it looks to me that they aren\u2019t really a defect in the shipped software (since they are not part of the packaged software after the build).", "I don\u2019t think this is stupid at all, you\u2019re right. Without saying that we discard these issues (having good flawless tests is relevant), most of the flaws affecting tests that we\u2019ve processed affect underlying software layers (and not explicitly the test code itself). Also, testing on test-only code gives a first valuable intuition. Of course, use-case specific tests are more appropriate but I doubt vendors or integrators would be willing to open source these up (and if they do we\u2019ll do our best to pick them up!)", "I haven\u2019t processed all the RVD tickets but from the intuition acquired building it, I\u2019d say that currently most of the flaws at RVD do report about these underlying defects. We have limited bandwidth and try to focus on what\u2019s more critical.", "Another note regarding the published numbers here: the public GitHub tickets seems to contain ", " redundancy. The very same defect is being ticketed up to one hundred times just since the functionality / API is used in numerous packages - for each a separate ticket is created. I would certainly consider that to be only a single vulnerability / flaw (even though it affects many packages).", "True. This can be further seen with a closer look at the ", ", mitigating a flaw closed several tickets. We\u2019re working on this. As mentioned in my previous comment above, we\u2019ve got some internal tasks already allocated to do this automatically parsing syntactically tickets daily and reaching a compromise. We don\u2019t have a solution ready unfortunately but it\u2019s coming.", "Both of the above seems to \u201cinflate\u201d the reported numbers significantly therefore I would suggest to reconsider how to account for those in statistics like this. I would also be interested in the actual (non-inflated) numbers to see where we are roughly at.", "All right, noted. We\u2019re slowly building up though and have already filtered out quite a bit. Note that the current tickets represent only a very small subset of the ROS 2 packages (ROS core and navigation2 mostly, we disabled the rest for now) with a ", ". Our security pipelines include several static and dynamic tests. Including the autogenerated reports from static testing tools will increase the current number of flaws by an order of magnitude at least (which again, would be hard to interpret).", "Any advice or disagreements (reasoned please) would be very helpful but the general intuition we\u2019re trying to develop is:", "A relevant number to obtain a quick intuition for the insecurity of ROS 2 would be the number of vulnerabilities open (not mitigated). Does this make sense to you ", "? Also, would it help to point to this thread of conversation from the RVD README.md file for further intuition?", "Happy to join the great (and very relevant) discussion points on this thread. Just sharing some thoughts:", "I may add, that conversely to what ", " was stating, CVE covers a yet somehow ", " of Robot-specific vulnerabilities. Very little commitment has been shown so far both by security researcher and robot manufacturers at least when it comes to reporting CVE\u2019s and there is vast amounts of work to be done. RVD is an attempt to systematize this workflow, which complements and feeds vulnerability records maintained by the competent authorities and serves as supporting documentation.", "I\u2019d like to share as well additional challenges we faced ourselves in Alias Robotics when digging into the actual records in vulnerabilities. For example, when we type \u201crobot\u201d in the CVE browser, most of the references will refer back to ROBOT (Return Of Bleichenbacher\u2019s Oracle Threat) which as 19-year old vuln on the RSA encryption which in most cases, does not necessarily apply to a robotic system (won\u2019t in all cases I\u2019ve inspected). Making an emphasis in the fact that we report actual \u201crobot - specific vulnerabilities\u201d is and will be an additional challenge to segregate from other more \u201cIT related\u201d flaws, as ", " points out.", "Similarly, I do believe that ROS2 adoption can greatly benefit from the transparency in the security workflow proposed within RVD.  Weaknesses can be separately inspected and mitigations adopted, all in a trackable and reproducible manner, so ROS2 resources and be kept up to date and secure ", " when used. Of course, there is tons of work to be done still and community contributions will be super-welcome!", "I think this is a very important and highly needed initiative. The potential consequences of an insecure robot are very concerning.", "\nI support the idea of a robot-specific collection but I also agree that it needs to be well maintained.", "However, I think it es even more important to raise the visibility of such a platform. Otherwise ist usefulness will be very limited. OEMs, System Integrators and researchers alike should be aware of this and ideally actively taking part in the process.", "\nI absolutely welcome that Alias is taking the lead here, but elevating this initiative to a broader support by other players would be important. All the issues discussed above (90-days deadlines, scoring, \u2026) could be agreed-upon rules. What are your plans for this and what would be options?", "In any case, we will also actively contribute to RVD in the future.", " ", " Thanks for the detailed rationale.", "If RVD in intended to act as a more responsive and more detailed front-end to CVE, then the concern I\u2019m describing below can be mostly ignored. In that case maybe we can eventually team up with MITRE to improve the CVE feature set based on what our community finds useful in RVD.", "I\u2019m concerned that we might be claiming ", " by saying that robots need their own security flaw scoring and reporting systems. Robots are complex, sure, but there are plenty of physical, actuated things in the world that are controlled by software that might contain vulnerabilities. Are we following the example of other domains that have their own scoring and reporting systems or are we striking out on our own here? What do organizations working in automotive, building infrastructure, factory equipment, medical devices, or other \u201ccyber-physical\u201d fields do?", "Regarding the poor search results available for robot/ROS in CVE today: can that be attributed to the fact that approximately nobody is yet reporting flaws in these systems anywhere? Presumably once we get the community to consistently report their findings, the CVE database would come to contain much more useful information.", "To be clear: I\u2019m very enthusiastic about finding, reporting, and mitigating security flaws! But after 20 years of personally arguing that robots are special and so we need our own X (for many values of X), then living with the resulting maintenance burdens, I\u2019m also eager to reuse existing systems and approaches wherever possible.", "I still have not seen a good reason why we need to strike out on our own. I would rather leverage the work of the NVD and MITRE so people can reuse existing tooling, process and procedures.", "I would say the NVD is lacking in robotics specific CVEs because people have not submitted issues. We have opened 3 CVEs with MITRE this year for ROS packages:", "ROS is just packages on top of an operating system, it would be like Apache standing up a new vuln database just for Apache  projects instead of using MITRE.", "Cheers,", "\n-Joe", "I\u2019m concerned that we might be claiming ", " by saying that robots need their own security flaw scoring and reporting systems.", "Slightly off-topic, but: this is something me and my colleagues ", " and ", " also started wondering. This will probably also come up in our ROSCon presentation (", "), but I just wanted to add that at this point we\u2019re not sure whether CVE is sufficient for robot related bugs/vulnerabilities or whether issues in robot software are actually ", " different that they should get their own classification.", "I guess I wonder why ROS is special and a CVE would not be sufficient to handle a security issue? I mean from a design perspective it sends messages over the network and runs on Linux(for the most part). How is this different from an issue with MQTT?", "This seems like a bit of vanity project, ", " .", "I\u2019d rather we leverage the work and efforts of MITRE.", "Cheers.", "\n-Joe", "Regarding the poor search results available for robot/ROS in CVE today: can that be attributed to the fact that approximately nobody is yet reporting flaws in these systems anywhere? Presumably once we get the community to consistently report their findings, the CVE database would come to contain much more useful information.", "I want to echo this point. There\u2019s actually a PR aspect of this to consider: mature products have CVEs. It\u2019s part of life these days. Security is only recently becoming more of a concern in ROS. As that grows, so will the CVEs, and the perceived maturity of ROS 2. That\u2019s one of the reasons we\u2019ve been submitting them!", "If RVD in intended to act as a more responsive and more detailed front-end to CVE", "I think this is a good way to put it. We certainly tried our best to avoid reiventing the wheel. Our intention is to get aligned as soon as possible with MITRE and the CVE List. Becoming a CNA will help  voice out our opinions (to some extend) and we hope to remain constructive on what needs to change to facilitate securing robots and robot components. RVD is a fast-track we\u2019re taking.", "Our experience (coming from a robotics background and) having tried these tools for a period of time is that they\u2019re not sufficient. I\u2019d be interested in other roboticists from the community sharing their views as well.", "When it comes to scoring mechanisms for the severity of vulnerabilities, ", " (", ") was built by researching what other (robotics) related areas were demanding and wasn\u2019t being met. [", "] or [", "] are among the ones cited while building it. The white paper above discusses it in more detail and proposes a scoring mechanism that takes in consideration aspects that directly apply to self-driving cars and other similar autonomous devices.", "Any help interfacing with MITRE will certainly be very helpful!", "I would say the NVD is lacking in robotics specific CVEs because people have not submitted issues. We have opened 3 CVEs with MITRE this year for ROS packages:", "I applaud this action. This is great and we certainly encourage everyone involved in security to follow a similar approach and commit resources to file reports in the CVE List. As pointed out above, we certainlly will. The big question we asked ourselves when designing RVD was, \u201cAs a roboticist/security researcher, what do I need and find more useful to ", " flaw A in a robot?\u201d  (ROS  specifically, here)\". The core is mitigation.", "I did a quick search on the first ID (", ") you listed above but it\u2019s undisclosed. My guess is that the same patterns critized above about reports in the CVE List are in those reports (or maybe not and I\u2019d be gladly surprised!).", "ROS is just packages on top of an operating system, it would be like Apache standing up a new vuln database just for Apache projects instead of using MITRE.", "This is not what\u2019s being proposed here, at least not within RVD. It\u2019s implicit on its name \u201cRobot Vulnerability Database\u201d. It\u2019s not ROS-specific, it\u2019s for robots. You may claim the same and that robots are a sub-class of hardware which doesn\u2019t deserve its own treatment. Well, I would then object and indicate that according to several sources CVE has currently serious issues capturing vulnerabilities that affect to hardware.", "\nOne only needs to parse the CVE List, compare the density (of hardware vs software reports this year) and the \u201cvalue\u201d on the content of these reports to draw some conclusions.", "One point I\u2019d like to make is that ", ". This is not the feeling we get with the CVE List. We advocate to bring these mechanisms to the CVE List. When reading the CVE List, we felt that many reports where terrible and didn\u2019t help at all reproducing the flaw.", "This seems like a bit of vanity project, ", " .", "Nobody is trying to replace CVE ", ", see reasoning above.", "Are we following the example of other domains that have their own scoring and reporting systems or are we striking out on our own here? What do organizations working in automotive, building infrastructure, factory equipment, medical devices, or other \u201ccyber-physical\u201d fields do?", "Consider ", ", particularly their ", ".  Each advisory may contain multiple vulnerabilities but each vuln links to a CVE.  Also important to note that each vuln has a standard CVSS score to support prioritizing.  Both CVE and CVSS are mature, communicate well, are deeply integrated into vuln management tools.", "most of the flaws affecting tests that we\u2019ve processed affect underlying software layers", "I think I have seen several entries which are only identifying flaws in test code, e.g. using the public API incorrectly / insufficiently. So in these cases the defect is not in the used code but in the test itself.", "Anyway my suggestion would be that it would be helpful if those would at least be moved / accounted for in a separate category to draw a more precise picture how many problems actually affect the code used by applications.", "But after 20 years of personally arguing that robots are special and so we need our own X (for many values of X), then living with the resulting maintenance burdens", "This is my daily life right now\u2026 I really cannot recommend it!", "I may have missed this, but how does the RVD propose to avoid:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "there will be a significant amount of weaknesses reported for ROS 2, several referring to quality bugs (opposed to security ones)", "flaws that are exploitable will be listed as vulnerabilities (note that we don\u2019t have yet a single vulnerability for ROS 2 (neither closed ones, which to us means, mitigated))", "CVE-2019-13445 - potential integer overflow", "CVE-2019-13566 - potential string overflow", "CVE-2019-13465 - potential iterator cause buffer overflow", "CVE-2019-13445 - potential integer overflow", "CVE-2019-13566 - potential string overflow", "CVE-2019-13465 - potential iterator cause buffer overflow", "Duplication of reports between CVEs and the RVD?", "Robotics engineers needing to watch two databases to get all possible vulnerabilities?"], "url": "https://discourse.ros.org/t/introducing-the-robot-vulnerability-database/11105"},
{"title": "Multi-robot coordination: Distributed synchronization of industrial robots through ROS 2", "thread_contents": ["When working in a network with various computers, each has a clock that provides them time. In most cases, being connected to the same network requires that all have the same time reference in order to coordinate their tasks. Here is where time/clock synchronization plays an important role. We can define time synchronization as the different processes or  ", " .", "To accomplish  ", "  between these  ", "  we need to set a very accurate reference. Despite selecting a good base time, hardware clocks use to count time at different rates. Furthermore, the quality of the oscillator is also a factor that has to be taken into account in order to avoid what is known as drift clock. To avoid or mitigate this, there are various options we can choose, for example using  ", "  (Global Positioning System) that provides an accuracy of tens of nanoseconds,  ", "  (Network Time Protocol) widely employed nowadays to set the clocks of our PCs and  ", "  (Precision Time Protocol) which offers great accuracy through local networks. PTP is the one chosen for our demo.", "Precision Time Protocol, which is the one used in the video above, was defined in 2002, but the second version was released in 2008 as IEEE 1588-2008 standard. Thanks to PTP we can obtain accuracies in the range of  ", "  without the need of a GPS system which is a great advantage when GPS signal is not available (e.g. indoor applications) or the cost of the receiver is not affordable. PTP has a Master-Slave architecture and defines the messages between them where the Master provides the time and the Slave synchronizes to the first. The aim of these messages exchanges is to fix the existing offset between the Master and the Slave.", "In most robotic applications several sensors and actuators are used. This means that there should be a coordination between  ", "  and  ", " . For example, if we think on an Advanced Driver Assistant System (ADAS), the wide variety of sensors available and the need of a coordinated and fast response forces to have a very accurate  ", " .", "Another good example where time synchronization plays a critical role is in a  ", "  and  ", " . Let\u2019s suppose that we have a laser scanner with its motor and a camera; the lack of coordination/synchronization between them would result in a poor quality point cloud.", "Finally, if we are interested in  ", "  from our modules or robots, it is needed to achieve a right clock synchronization so that we can plot that data in graphs and the information is coherent.", "In this case, we show the use of time synchronization for  ", " . Having a common reference time in a distributed system allows to coordinate motion accurately between robot arm motors, different robots or between robots and an external process. Everything is powered by the ", ".", "In both setups, all the devices are synchronized using the  ", " . The Open Robot Controller ( ", " ) acts as the  ", "  providing the reference clock for all the robots. All the robots are connected to the ORC by using a PTP capable switch. Inside  ", " , each  ", "  module works as  ", "  by using a Peer-to-Peer ( ", " ) delay mechanism. In order to coordinate the robot motion, trajectories are sent with a scheduled timestamp. The H-ROS SoM executes the motion accurately at the specified time.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/multi-robot-coordination-distributed-synchronization-of-industrial-robots-through-ros-2/9288"},
{"title": "How to support ROS2 on MacOS?", "thread_contents": ["We have at least one user of Navigation2 who is trying to run on MacOS. We haven\u2019t had the resources to set up any CI or do any testing using MacOS.", "So, I wanted to start the discussion of how we can support this in the most scalable way? I believe that the ROS build farm is building packages for Mac, but I\u2019m not sure what testing is being done? Ideally, I\u2019d like to leverage the build farm and not have to test every change on a Mac as well as Ubuntu.", "If anyone has a solution for this that doesn\u2019t involve a lot of manual setup and support, I\u2019d love to hear it.", "I believe that the ROS build farm is building packages for Mac, but I\u2019m not sure what testing is being done?", "macOS is a \u201ctier 1\u201d platform, see: ", " builds an archive every night and every release for macOS. Also, pull requests for any change to repositories in the ", " file must pass on macOS as well. We have nightly jobs for macOS also. We treat macOS the same as Linux and Windows in that respect.", "Currently we\u2019re using an old version of macOS, but we have plans to update our farm\u2019s machines in the near future.", "Ideally, I\u2019d like to leverage the build farm and not have to test every change on a Mac as well as Ubuntu.", "I\u2019m not sure what you\u2019re asking for here, you mean depend on the build farm testing all of your dependencies or\u2026?", "If you\u2019re going to support macOS you\u2019ll either have to build every change against macOS (to prevent regressions) or build periodically (nightlys) and tackle regressions as they come up.", "If anyone has a solution for this that doesn\u2019t involve a lot of manual setup and support, I\u2019d love to hear it.", "Travis offers builds for macOS. I haven\u2019t tried it, but it might be possible to just download our nightly archive (", ") and do the steps in our \u201cbinary install for macOS\u201d tutorial (", ").", "I think there are some other options as well, e.g. setting up an appveyor server on your own hardware: ", ".", "Thanks for the help William!", "So If we want our code to build automatically on macOS in the build farm, we need to add our project to the ros2.repos file, is that correct? If so, we can submit a PR for that.", "So If we want our code to build automatically on macOS in the build farm, we need to add our project to the ros2.repos file, is that correct? If so, we can submit a PR for that.", "I believe there was talk of adding a new nightly job on ", " that additionally tests the navigation stack, rather than adding it to the ", " file, simply because our CI is already over 2 hours, so we\u2019re trying to keep the list short until we have a more scalable solution (people are working on that for ", ").", " were we not talking about a navigation nightly at one point? Or am I imagining that?", "As for packaging, I suppose we could try to add it to packaging but not CI, which is the case for the ros1 bridge for example. We also have some \u201csupplemental\u201d repos files which can be used in various ways when running jobs on ", ", e.g.: ", "were we not talking about a navigation nightly at one point?", " and ", " were we not talking about a navigation nightly at one point?", "We\u2019ve been working on CI jobs for navigation2 but not via ", ".", "\nWe\u2019ve been using the CI jobs feature of ros_buildfarm to run builds here: ", "Our efforts have been on getting ros_buildfarm into a state where we can formalize that job and improve the CI experience on ", " rather than putting more onto ", ". ", " doesn\u2019t currently have support for non-Linux platforms.", "I don\u2019t think that we have the capacity to sustainably build community projects on ", " as part of the omnibus packages ether via a broader repos file for packaging jobs or via an additional \u201clayer\u201d of builds on top of the current packaging jobs. This is due in part because of raw compute resources available and in part because managing that in the medium term sounds like it would eventually turn into re-implementing a bunch of what we have for rosdistro distribution management and if we\u2019re going to do that I would rather focus on the long term goal of migrating all packaging from being .repos file based on ros2/ci to being rosdistro distribution.yaml-based via ros_buildfarm.", "I don\u2019t know your CI infrastructure very well (bare minimum TBH), so can someone tell me how I should proceed? If adding to the ros2.repos file is not the way to go, then what is?", "I don\u2019t know your CI infrastructure very well (bare minimum TBH), so can someone tell me how I should proceed?", "I think ", "\u2019s point is that we don\u2019t have the resources to provide macOS jobs for everyone in the community, and I agree. We\u2019re already running into cases where nightly jobs don\u2019t finish until well into the workday for us.", "I was thinking a single nightly job might be possible, but the reality is that we\u2019re pretty much at capacity and don\u2019t have plans to expand it.", "On the other hand, ", " is meant for the whole community (and is where the nav nightly\u2019s are run), but it currently doesn\u2019t support anything but Linux. I think the plan is to eventually have Windows and even macOS for ", ", and in a way that can scale properly, but not in the near future.", "If adding to the ros2.repos file is not the way to go, then what is?", "I don\u2019t have a solution for you other than to point you to resources that might help you setup your own macOS testing (travis, appveyor, or your own instance of our ", ").", "OK, I\u2019ll look into the Travis option for now.", "Then maybe we can discuss getting Windows and MacOS support into ", " soon. ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-support-ros2-on-macos/10451"},
{"title": "ROS 2 Eloquent Elusor call for testing and package releases", "thread_contents": ["Hello all,", "The ROS 2 team is preparing for the release of Eloquent Elusor and we\u2019d like to invite you to help us by testing out the alpha or releasing your own packages to Eloquent.  For more information on the schedule for the release of Eloquent Elusor, consult the ", ".", "ROS 2 is available with multiple RMW implementations on multiple platforms and architectures. Depending on your system configuration, you can help us out by doing any of the following:", "After testing, reply back here to let us know what worked and what didn\u2019t work, along with what platform you\u2019re on and how you installed ROS 2.", "If you run into problems, all the usual recommendations apply. You can ask questions on ", " with the  ", "  tag or report issues on ", " or the relevant package repository. Be sure to follow the contributing guidelines and issue templates.", "If you maintain a package you would like to release in ROS 2 Eloquent we encourage you to do so by following the ", ".", ", ", ", and ", ",", "\nThe ROS 2 team", "And the packages included in the first sync:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Thanks for sharing this ", "! I will test this out and share this post. ", "Hi ", ", on my box there is no ", " package yet. The build farm status page is all green though, indicating the sync is completed. Is this expected? I can get ", " packages alright from shadow-fixed.", "The last stage of the pipeline for deploying to the mirrors was not yet setup for the main repository. I\u2019ve added it and the packages should now be accessible from the main repository.", " ", " Is it possible to configure the testing mirror deployment as well ?", "\nThe nightly docker images are using the testing repository and would need a sync to start building again.", "Thanks!", "It\u2019s done. I didn\u2019t realize it hadn\u2019t been done already.", "With many packages being updated for the Beta, I have done another sync to main:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " on your preferred platform. Be sure to use the  ", "  branch version of the repos file (described ", ") as Eloquent is still in pre-release.", "Install and use the ", " available for MacOS Sierra, Windows 10, or Ubuntu 18.04 Bionic.", "\n", " as they become available. (", ", ", ")", "Follow the ROS 2 ", " with one or more different RMW implementations.", "Run some ", " and ", ".", "ros-eloquent-action-msgs: 0.8.0-1", "ros-eloquent-action-msgs-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-cpp: 0.8.0-1", "ros-eloquent-action-tutorials-cpp-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-interfaces: 0.8.0-1", "ros-eloquent-action-tutorials-interfaces-dbgsym: 0.8.0-1", "ros-eloquent-action-tutorials-py: 0.8.0-1", "ros-eloquent-actionlib-msgs: 0.8.0-1", "ros-eloquent-actionlib-msgs-dbgsym: 0.8.0-1", "ros-eloquent-ament-clang-format: 0.8.0-1", "ros-eloquent-ament-clang-tidy: 0.8.0-1", "ros-eloquent-ament-cmake: 0.8.0-1", "ros-eloquent-ament-cmake-auto: 0.8.0-1", "ros-eloquent-ament-cmake-clang-format: 0.8.0-1", "ros-eloquent-ament-cmake-clang-tidy: 0.8.0-1", "ros-eloquent-ament-cmake-copyright: 0.8.0-1", "ros-eloquent-ament-cmake-core: 0.8.0-1", "ros-eloquent-ament-cmake-cppcheck: 0.8.0-1", "ros-eloquent-ament-cmake-cpplint: 0.8.0-1", "ros-eloquent-ament-cmake-export-definitions: 0.8.0-1", "ros-eloquent-ament-cmake-export-dependencies: 0.8.0-1", "ros-eloquent-ament-cmake-export-include-directories: 0.8.0-1", "ros-eloquent-ament-cmake-export-interfaces: 0.8.0-1", "ros-eloquent-ament-cmake-export-libraries: 0.8.0-1", "ros-eloquent-ament-cmake-export-link-flags: 0.8.0-1", "ros-eloquent-ament-cmake-flake8: 0.8.0-1", "ros-eloquent-ament-cmake-gmock: 0.8.0-1", "ros-eloquent-ament-cmake-gtest: 0.8.0-1", "ros-eloquent-ament-cmake-include-directories: 0.8.0-1", "ros-eloquent-ament-cmake-libraries: 0.8.0-1", "ros-eloquent-ament-cmake-lint-cmake: 0.8.0-1", "ros-eloquent-ament-cmake-mypy: 0.8.0-1", "ros-eloquent-ament-cmake-nose: 0.8.0-1", "ros-eloquent-ament-cmake-pclint: 0.8.0-1", "ros-eloquent-ament-cmake-pep257: 0.8.0-1", "ros-eloquent-ament-cmake-pep8: 0.8.0-1", "ros-eloquent-ament-cmake-pyflakes: 0.8.0-1", "ros-eloquent-ament-cmake-pytest: 0.8.0-1", "ros-eloquent-ament-cmake-python: 0.8.0-1", "ros-eloquent-ament-cmake-ros: 0.8.0-1", "ros-eloquent-ament-cmake-target-dependencies: 0.8.0-1", "ros-eloquent-ament-cmake-test: 0.8.0-1", "ros-eloquent-ament-cmake-uncrustify: 0.8.0-1", "ros-eloquent-ament-cmake-xmllint: 0.8.0-1", "ros-eloquent-ament-copyright: 0.8.0-1", "ros-eloquent-ament-cppcheck: 0.8.0-1", "ros-eloquent-ament-cpplint: 0.8.0-1", "ros-eloquent-ament-flake8: 0.8.0-1", "ros-eloquent-ament-index-cpp: 0.7.1-1", "ros-eloquent-ament-index-cpp-dbgsym: 0.7.1-1", "ros-eloquent-ament-index-python: 0.7.1-1", "ros-eloquent-ament-lint: 0.8.0-1", "ros-eloquent-ament-lint-auto: 0.8.0-1", "ros-eloquent-ament-lint-cmake: 0.8.0-1", "ros-eloquent-ament-lint-common: 0.8.0-1", "ros-eloquent-ament-mypy: 0.8.0-1", "ros-eloquent-ament-package: 0.8.2-1", "ros-eloquent-ament-pclint: 0.8.0-1", "ros-eloquent-ament-pep257: 0.8.0-1", "ros-eloquent-ament-pep8: 0.8.0-1", "ros-eloquent-ament-pyflakes: 0.8.0-1", "ros-eloquent-ament-uncrustify: 0.8.0-1", "ros-eloquent-ament-xmllint: 0.8.0-1", "\n", ": 1.12.1-1", "ros-eloquent-builtin-interfaces: 0.8.0-1", "ros-eloquent-builtin-interfaces-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-camera-calibration-parsers-dbgsym: 2.2.0-1", "\n", ": 2.2.0-1", "ros-eloquent-camera-info-manager-dbgsym: 2.2.0-1", "\n", ": 1.0.0-1", "ros-eloquent-cartographer-dbgsym: 1.0.0-1", "\n", ": 1.4.0-1", "ros-eloquent-class-loader-dbgsym: 1.4.0-1", "ros-eloquent-common-interfaces: 0.8.0-1", "ros-eloquent-composition: 0.8.0-1", "ros-eloquent-composition-dbgsym: 0.8.0-1", "ros-eloquent-composition-interfaces: 0.8.0-1", "ros-eloquent-composition-interfaces-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-compressed-depth-image-transport-dbgsym: 2.2.0-1", "\n", ": 2.2.0-1", "ros-eloquent-compressed-image-transport-dbgsym: 2.2.0-1", "ros-eloquent-connext-cmake-module: 0.8.1-1", "\n", ": 1.2.0-1", "ros-eloquent-console-bridge-vendor-dbgsym: 1.2.0-1", "\n", ": 2.1.2-1", "ros-eloquent-cv-bridge-dbgsym: 2.1.2-1", "\n", ": 0.1.0-1", "ros-eloquent-cyclonedds-dbgsym: 0.1.0-1", "ros-eloquent-demo-nodes-cpp: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-dbgsym: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-native: 0.8.0-1", "ros-eloquent-demo-nodes-cpp-native-dbgsym: 0.8.0-1", "ros-eloquent-demo-nodes-py: 0.8.0-1", "\n", ": 2.2.2-1", "ros-eloquent-depthimage-to-laserscan-dbgsym: 2.2.2-1", "ros-eloquent-diagnostic-msgs: 0.8.0-1", "ros-eloquent-diagnostic-msgs-dbgsym: 0.8.0-1", "ros-eloquent-domain-coordinator: 0.8.0-1", "ros-eloquent-dummy-map-server: 0.8.0-1", "ros-eloquent-dummy-map-server-dbgsym: 0.8.0-1", "ros-eloquent-dummy-robot-bringup: 0.8.0-1", "ros-eloquent-dummy-sensors: 0.8.0-1", "ros-eloquent-dummy-sensors-dbgsym: 0.8.0-1", "\n", ": 1.0.0-1", "ros-eloquent-eigen3-cmake-module: 0.1.1-1", "ros-eloquent-example-interfaces: 0.7.1-1", "ros-eloquent-example-interfaces-dbgsym: 0.7.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-client-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-server: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-action-server-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-client: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-client-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-composition: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-composition-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-publisher: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-publisher-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-service: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-service-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-subscriber: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-subscriber-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-timer: 0.8.0-1", "ros-eloquent-examples-rclcpp-minimal-timer-dbgsym: 0.8.0-1", "ros-eloquent-examples-rclpy-executors: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-action-client: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-action-server: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-client: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-publisher: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-service: 0.8.0-1", "ros-eloquent-examples-rclpy-minimal-subscriber: 0.8.0-1", "ros-eloquent-fastcdr: 1.0.11-1", "ros-eloquent-fastcdr-dbgsym: 1.0.11-1", "ros-eloquent-fastrtps: 1.9.0-3", "ros-eloquent-fastrtps-cmake-module: 0.8.0-1", "ros-eloquent-fastrtps-dbgsym: 1.9.0-3", "ros-eloquent-foonathan-memory-vendor: 0.1.0-1", "\n", ": 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-msgs-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-plugins-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-gazebo-ros-dbgsym: 3.4.0-1", "\n", ": 3.4.0-1", "ros-eloquent-geometry-msgs: 0.8.0-1", "ros-eloquent-geometry-msgs-dbgsym: 0.8.0-1", "ros-eloquent-gmock-vendor: 1.8.9000-1", "ros-eloquent-gtest-vendor: 1.8.9000-1", "\n", ": 2.1.2-1", "ros-eloquent-image-geometry-dbgsym: 2.1.2-1", "ros-eloquent-image-tools: 0.8.0-1", "ros-eloquent-image-tools-dbgsym: 0.8.0-1", "\n", ": 2.2.0-1", "ros-eloquent-image-transport-dbgsym: 2.2.0-1", "\n", ": 2.0.0-1", "ros-eloquent-interactive-markers-dbgsym: 2.0.0-1", "ros-eloquent-intra-process-demo: 0.8.0-1", "ros-eloquent-intra-process-demo-dbgsym: 0.8.0-1", "ros-eloquent-joy: 2.3.2-3", "ros-eloquent-joy-dbgsym: 2.3.2-3", "ros-eloquent-kdl-parser: 2.2.0-1", "ros-eloquent-kdl-parser-dbgsym: 2.2.0-1", "\n", ": 2.1.0-1", "ros-eloquent-laser-geometry-dbgsym: 2.1.0-1", "ros-eloquent-launch: 0.9.1-1", "ros-eloquent-launch-ros: 0.9.1-1", "ros-eloquent-launch-testing: 0.9.1-1", "ros-eloquent-launch-testing-ament-cmake: 0.9.1-1", "ros-eloquent-launch-testing-ros: 0.9.1-1", "ros-eloquent-launch-xml: 0.9.1-1", "ros-eloquent-launch-yaml: 0.9.1-1", "\n", ": 2.2.0-1", "\n", ": 1.0.0-1", "ros-eloquent-libyaml-vendor-dbgsym: 1.0.0-1", "ros-eloquent-lifecycle: 0.8.0-1", "ros-eloquent-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-lifecycle-msgs: 0.8.0-1", "ros-eloquent-lifecycle-msgs-dbgsym: 0.8.0-1", "ros-eloquent-logging-demo: 0.8.0-1", "ros-eloquent-logging-demo-dbgsym: 0.8.0-1", "\n", ": 2.0.2-1", "ros-eloquent-map-msgs-dbgsym: 2.0.2-1", "\n", ": 3.2.0-1", "ros-eloquent-message-filters-dbgsym: 3.2.0-1", "\n", ": 2.0.2-1", "ros-eloquent-move-base-msgs-dbgsym: 2.0.2-1", "ros-eloquent-nav-msgs: 0.8.0-1", "ros-eloquent-nav-msgs-dbgsym: 0.8.0-1", "ros-eloquent-opensplice-cmake-module: 0.8.1-1", "\n", ": 3.2.0-1", "ros-eloquent-orocos-kdl-dbgsym: 3.2.0-1", "ros-eloquent-osrf-pycommon: 0.1.8-2", "ros-eloquent-osrf-testing-tools-cpp: 1.2.1-1", "ros-eloquent-osrf-testing-tools-cpp-dbgsym: 1.2.1-1", "ros-eloquent-pendulum-control: 0.8.0-1", "ros-eloquent-pendulum-control-dbgsym: 0.8.0-1", "ros-eloquent-pendulum-msgs: 0.8.0-1", "ros-eloquent-pendulum-msgs-dbgsym: 0.8.0-1", "\n", ": 2.4.0-1", "ros-eloquent-poco-vendor: 1.2.0-1", "ros-eloquent-python-cmake-module: 0.8.0-2", "\n", ": 1.0.2-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "\n", ": 1.0.7-1", "ros-eloquent-qt-gui-cpp-dbgsym: 1.0.7-1", "\n", ": 1.0.7-1", "ros-eloquent-quality-of-service-demo-cpp: 0.8.0-1", "ros-eloquent-quality-of-service-demo-cpp-dbgsym: 0.8.0-1", "ros-eloquent-quality-of-service-demo-py: 0.8.0-1", "ros-eloquent-rcl: 0.8.0-1", "ros-eloquent-rcl-action: 0.8.0-1", "ros-eloquent-rcl-action-dbgsym: 0.8.0-1", "ros-eloquent-rcl-dbgsym: 0.8.0-1", "ros-eloquent-rcl-interfaces: 0.8.0-1", "ros-eloquent-rcl-interfaces-dbgsym: 0.8.0-1", "ros-eloquent-rcl-lifecycle: 0.8.0-1", "ros-eloquent-rcl-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-rcl-logging-log4cxx: 0.3.2-1", "ros-eloquent-rcl-logging-log4cxx-dbgsym: 0.3.2-1", "ros-eloquent-rcl-logging-noop: 0.3.2-1", "ros-eloquent-rcl-logging-noop-dbgsym: 0.3.2-1", "ros-eloquent-rcl-logging-spdlog: 0.3.2-1", "ros-eloquent-rcl-logging-spdlog-dbgsym: 0.3.2-1", "ros-eloquent-rcl-yaml-param-parser: 0.8.0-1", "ros-eloquent-rcl-yaml-param-parser-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp: 0.8.0-1", "ros-eloquent-rclcpp-action: 0.8.0-1", "ros-eloquent-rclcpp-action-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-components: 0.8.0-1", "ros-eloquent-rclcpp-components-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-dbgsym: 0.8.0-1", "ros-eloquent-rclcpp-lifecycle: 0.8.0-1", "ros-eloquent-rclcpp-lifecycle-dbgsym: 0.8.0-1", "ros-eloquent-rclpy: 0.8.0-1", "ros-eloquent-rclpy-dbgsym: 0.8.0-1", "ros-eloquent-rcpputils: 0.2.0-1", "ros-eloquent-rcpputils-dbgsym: 0.2.0-1", "ros-eloquent-rcutils: 0.8.1-1", "ros-eloquent-rcutils-dbgsym: 0.8.1-1", "\n", ": 2.2.0-1", "ros-eloquent-resource-retriever-dbgsym: 2.2.0-1", "ros-eloquent-rmw: 0.8.0-1", "ros-eloquent-rmw-connext-cpp: 0.8.0-1", "ros-eloquent-rmw-connext-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-connext-shared-cpp: 0.8.0-1", "ros-eloquent-rmw-connext-shared-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-fastrtps-shared-cpp: 0.8.0-1", "ros-eloquent-rmw-fastrtps-shared-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rmw-implementation: 0.8.0-4", "ros-eloquent-rmw-implementation-cmake: 0.8.0-1", "ros-eloquent-rmw-implementation-dbgsym: 0.8.0-4", "ros-eloquent-rmw-opensplice-cpp: 0.8.0-1", "ros-eloquent-rmw-opensplice-cpp-dbgsym: 0.8.0-1", "ros-eloquent-robot-state-publisher: 2.3.0-1", "ros-eloquent-robot-state-publisher-dbgsym: 2.3.0-1", "ros-eloquent-ros-environment: 2.4.0-1", "ros-eloquent-ros-testing: 0.2.0-1", "ros-eloquent-ros-workspace: 0.7.1-2", "ros-eloquent-ros1-bridge: 0.8.0-2", "ros-eloquent-ros1-bridge-dbgsym: 0.8.0-2", "ros-eloquent-ros2action: 0.8.1-1", "ros-eloquent-ros2bag: 0.2.0-1", "ros-eloquent-ros2cli: 0.8.1-1", "ros-eloquent-ros2component: 0.8.1-1", "ros-eloquent-ros2doctor: 0.8.1-1", "ros-eloquent-ros2interface: 0.8.1-1", "ros-eloquent-ros2launch: 0.9.1-1", "ros-eloquent-ros2lifecycle: 0.8.1-1", "ros-eloquent-ros2msg: 0.8.1-1", "ros-eloquent-ros2multicast: 0.8.1-1", "ros-eloquent-ros2node: 0.8.1-1", "ros-eloquent-ros2param: 0.8.1-1", "ros-eloquent-ros2pkg: 0.8.1-1", "ros-eloquent-ros2run: 0.8.1-1", "ros-eloquent-ros2service: 0.8.1-1", "ros-eloquent-ros2srv: 0.8.1-1", "ros-eloquent-ros2test: 0.2.0-1", "ros-eloquent-ros2topic: 0.8.1-1", "ros-eloquent-rosbag2: 0.2.0-1", "ros-eloquent-rosbag2-converter-default-plugins: 0.2.0-1", "ros-eloquent-rosbag2-converter-default-plugins-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-storage: 0.2.0-1", "ros-eloquent-rosbag2-storage-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-storage-default-plugins: 0.2.0-1", "ros-eloquent-rosbag2-storage-default-plugins-dbgsym: 0.2.0-1", "ros-eloquent-rosbag2-test-common: 0.2.0-1", "ros-eloquent-rosbag2-tests: 0.2.0-1", "ros-eloquent-rosbag2-transport: 0.2.0-1", "ros-eloquent-rosbag2-transport-dbgsym: 0.2.0-1", "ros-eloquent-rosgraph-msgs: 0.8.0-1", "ros-eloquent-rosgraph-msgs-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-adapter: 0.8.0-1", "ros-eloquent-rosidl-cmake: 0.8.0-1", "ros-eloquent-rosidl-default-generators: 0.8.0-1", "ros-eloquent-rosidl-default-runtime: 0.8.0-1", "ros-eloquent-rosidl-generator-c: 0.8.0-1", "ros-eloquent-rosidl-generator-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-generator-cpp: 0.8.0-1", "ros-eloquent-rosidl-generator-dds-idl: 0.7.1-1", "ros-eloquent-rosidl-generator-py: 0.8.0-1", "ros-eloquent-rosidl-parser: 0.8.0-1", "ros-eloquent-rosidl-runtime-py: 0.8.0-1", "ros-eloquent-rosidl-typesupport-c: 0.8.0-2", "ros-eloquent-rosidl-typesupport-c-dbgsym: 0.8.0-2", "ros-eloquent-rosidl-typesupport-connext-c: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-c-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-cpp: 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-cpp-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-cpp: 0.8.0-2", "ros-eloquent-rosidl-typesupport-cpp-dbgsym: 0.8.0-2", "ros-eloquent-rosidl-typesupport-fastrtps-c: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-cpp: 0.8.0-1", "ros-eloquent-rosidl-typesupport-fastrtps-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-interface: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-c: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-c-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-cpp: 0.8.0-1", "ros-eloquent-rosidl-typesupport-introspection-cpp-dbgsym: 0.8.0-1", "ros-eloquent-rosidl-typesupport-opensplice-c: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-c-dbgsym: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-cpp: 0.8.1-1", "ros-eloquent-rosidl-typesupport-opensplice-cpp-dbgsym: 0.8.1-1", "\n", ": 1.0.5-1", "\n", ": 1.0.1-1", "\n", ": 1.1.0-1", "\n", ": 1.0.2-1", "\n", ": 1.0.5-1", "\n", ": 1.0.5-1", "ros-eloquent-rqt-gui-cpp-dbgsym: 1.0.5-1", "\n", ": 1.0.5-1", "\n", ": 1.0.2-1", "ros-eloquent-rqt-image-view-dbgsym: 1.0.2-1", "\n", ": 1.0.2-1", "\n", ": 1.0.7-1", "\n", ": 1.1.0-1", "\n", ": 1.0.5-1", "ros-eloquent-rqt-py-common-dbgsym: 1.0.5-1", "\n", ": 1.0.0-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.0-1", "\n", ": 1.0.1-1", "\n", ": 1.0.0-1", "\n", ": 1.0.0-1", "ros-eloquent-rttest: 0.8.0-1", "ros-eloquent-rttest-dbgsym: 0.8.0-1", "\n", ": 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-common-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-default-plugins-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-ogre-vendor-dbgsym: 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz-rendering-dbgsym: 7.0.1-1", "ros-eloquent-rviz-rendering-tests: 7.0.1-1", "\n", ": 7.0.1-1", "\n", ": 7.0.1-1", "ros-eloquent-rviz2-dbgsym: 7.0.1-1", "ros-eloquent-sensor-msgs: 0.8.0-1", "ros-eloquent-sensor-msgs-dbgsym: 0.8.0-1", "ros-eloquent-shape-msgs: 0.8.0-1", "ros-eloquent-shape-msgs-dbgsym: 0.8.0-1", "ros-eloquent-shared-queues-vendor: 0.2.0-1", "\n", ": 1.0.1-1", "ros-eloquent-sqlite3-vendor: 0.2.0-1", "ros-eloquent-sros2: 0.8.0-1", "ros-eloquent-sros2-cmake: 0.8.0-1", "ros-eloquent-std-msgs: 0.8.0-1", "ros-eloquent-std-msgs-dbgsym: 0.8.0-1", "ros-eloquent-std-srvs: 0.8.0-1", "ros-eloquent-std-srvs-dbgsym: 0.8.0-1", "ros-eloquent-stereo-msgs: 0.8.0-1", "ros-eloquent-stereo-msgs-dbgsym: 0.8.0-1", "\n", ": 2.2.1-1", "ros-eloquent-teleop-twist-joy-dbgsym: 2.2.1-1", "\n", ": 2.3.1-1", "ros-eloquent-test-interface-files: 0.8.0-1", "ros-eloquent-test-msgs: 0.8.0-1", "ros-eloquent-test-msgs-dbgsym: 0.8.0-1", "ros-eloquent-test-osrf-testing-tools-cpp: 1.2.1-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-dbgsym: 0.12.0-1", "ros-eloquent-tf2-eigen: 0.12.0-1", "\n", ": 0.12.0-1", "\n", ": 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-msgs-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-py-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tf2-ros-dbgsym: 0.12.0-1", "\n", ": 0.12.0-1", "ros-eloquent-tinydir-vendor: 1.1.0-1", "ros-eloquent-tinyxml-vendor: 0.7.0-1", "ros-eloquent-tinyxml2-vendor: 0.6.1-1", "ros-eloquent-tlsf: 0.5.0-1", "ros-eloquent-tlsf-cpp: 0.8.0-1", "ros-eloquent-tlsf-cpp-dbgsym: 0.8.0-1", "ros-eloquent-topic-monitor: 0.8.0-1", "ros-eloquent-trajectory-msgs: 0.8.0-1", "ros-eloquent-trajectory-msgs-dbgsym: 0.8.0-1", "\n", ": 1.0.1-1", "ros-eloquent-turtlesim-dbgsym: 1.0.1-1", "\n", ": 1.3.0-1", "ros-eloquent-uncrustify-vendor-dbgsym: 1.3.0-1", "\n", ": 2.1.0-1", "ros-eloquent-unique-identifier-msgs-dbgsym: 2.1.0-1", "ros-eloquent-urdf: 2.2.0-1", "ros-eloquent-urdf-dbgsym: 2.2.0-1", "ros-eloquent-urdfdom: 2.2.0-1", "ros-eloquent-urdfdom-dbgsym: 2.2.0-1", "\n", ": 1.0.4-1", "\n", ": 2.1.2-1", "ros-eloquent-visualization-msgs: 0.8.0-1", "ros-eloquent-visualization-msgs-dbgsym: 0.8.0-1", "\n", ": 7.0.0-1", "ros-eloquent-yaml-cpp-vendor-dbgsym: 7.0.0-1", "AWS RoboMaker", "Amazon B9", "Amazon ROS Contributions", "Anup Pemmaiah", "Chris Lalancette", "Claire Wang", "Dan Lazewatsky", "David Gossow", "David V. Lu!!", "Dirk Thomas", "Dorian Scholz", "Eclipse Foundation, Inc.", "Emerson Knapp", "Ethan Gao", "Ivan Paunovic", "Jacob Perron", "John Shepherd", "Jose Luis Rivero", "Juan Pablo Samper", "Karsten Knese", "Michael Carroll", "Michel Hidalgo", "Miguel Company", "Mikael Arguedas", "Pete Baughman", "Scott K Logan", "Shane Loretz", "Steve Macenski", "Steven! Ragnarok", "Steven! Ragnar\u00f6k", "Ted Kern", "Tully Foote", "Vincent Rabaud", "William Woodall", "ros-eloquent-ament-cmake-version: 0.8.1-1", "\n", ": 1.0.9000-1", "ros-eloquent-cartographer-ros-dbgsym: 1.0.9000-1", "\n", ": 1.0.9000-1", "ros-eloquent-cartographer-ros-msgs-dbgsym: 1.0.9000-1", "ros-eloquent-cyclonedds-cmake-module: 0.4.1-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-config-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-errors-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-io-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-sigslots-lite-dbgsym: 1.0.3-1", "\n", ": 1.0.3-1", "ros-eloquent-ecl-time-lite-dbgsym: 1.0.3-1", "ros-eloquent-examples-rclcpp-multithreaded-executor: 0.8.1-1", "ros-eloquent-examples-rclcpp-multithreaded-executor-dbgsym: 0.8.1-1", "\n", ": 0.12.1-2", "\n", ": 2.2.1-1", "\n", ": 2.2.1-1", "\n", ": 1.0.0-1", "ros-eloquent-pcl-msgs-dbgsym: 1.0.0-1", "\n", ": 1.2.1-1", "ros-eloquent-rmw-cyclonedds-cpp: 0.4.1-1", "ros-eloquent-rmw-cyclonedds-cpp-dbgsym: 0.4.1-1", "ros-eloquent-ros2trace: 0.2.9-1", "ros-eloquent-ros2trace-analysis: 0.2.1-1", "ros-eloquent-sophus: 1.1.0-1", "ros-eloquent-system-modes: 0.1.5-1", "ros-eloquent-system-modes-dbgsym: 0.1.5-1", "ros-eloquent-system-modes-examples: 0.1.5-1", "ros-eloquent-system-modes-examples-dbgsym: 0.1.5-1", "\n", ": 2.2.1-1", "ros-eloquent-theora-image-transport-dbgsym: 2.2.1-1", "ros-eloquent-tracetools: 0.2.9-1", "ros-eloquent-tracetools-analysis: 0.2.1-1", "ros-eloquent-tracetools-dbgsym: 0.2.9-1", "ros-eloquent-tracetools-launch: 0.2.9-1", "ros-eloquent-tracetools-read: 0.2.9-1", "ros-eloquent-tracetools-test: 0.2.9-1", "ros-eloquent-tracetools-test-dbgsym: 0.2.9-1", "ros-eloquent-tracetools-trace: 0.2.9-1", "ros-eloquent-action-tutorials-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-interfaces-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-action-tutorials-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-actionlib-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-actionlib-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-clang-format: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-clang-tidy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-auto: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-clang-format: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-clang-tidy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-copyright: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-core: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-cppcheck: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-cpplint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-definitions: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-dependencies: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-include-directories: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-libraries: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-export-link-flags: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-flake8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-gmock: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-gtest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-include-directories: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-libraries: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-lint-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-mypy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-nose: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pclint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pep257: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pep8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pyflakes: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-pytest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-python: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-target-dependencies: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-test: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-uncrustify: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cmake-xmllint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-copyright: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cppcheck: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-cpplint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-flake8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-index-cpp: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-index-cpp-dbgsym: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-index-python: 0.7.1-1 -> 0.7.2-1", "ros-eloquent-ament-lint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-auto: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-lint-common: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-mypy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-package: 0.8.3-1 -> 0.8.4-1", "ros-eloquent-ament-pclint: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pep257: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pep8: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-pyflakes: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-uncrustify: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-ament-xmllint: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-camera-calibration-parsers-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-camera-info-manager-dbgsym: 2.2.0-1 -> 2.2.1-1", "ros-eloquent-common-interfaces: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-composition: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-composition-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-compressed-depth-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-compressed-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "ros-eloquent-connext-cmake-module: 0.8.1-1 -> 0.8.2-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-cv-bridge-dbgsym: 2.1.2-1 -> 2.1.3-1", "\n", ": 0.1.0-1 -> 0.1.0-3", "ros-eloquent-cyclonedds-dbgsym: 0.1.0-1 -> 0.1.0-3", "ros-eloquent-demo-nodes-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-native: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-cpp-native-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-demo-nodes-py: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.2-1 -> 2.2.4-1", "ros-eloquent-depthimage-to-laserscan-dbgsym: 2.2.2-1 -> 2.2.4-1", "ros-eloquent-diagnostic-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-diagnostic-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-map-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-map-server-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-robot-bringup: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-sensors: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-dummy-sensors-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-client-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-action-server-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-client-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-composition: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-composition-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-publisher: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-publisher-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-service: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-service-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-subscriber: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-subscriber-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-timer: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclcpp-minimal-timer-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-executors: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-action-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-action-server: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-client: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-publisher: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-service: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-examples-rclpy-minimal-subscriber: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-fastrtps: 1.9.0-3 -> 1.9.2-1", "ros-eloquent-fastrtps-dbgsym: 1.9.0-3 -> 1.9.2-1", "ros-eloquent-foonathan-memory-vendor: 0.1.0-1 -> 0.3.0-1", "ros-eloquent-geometry-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-geometry-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-image-geometry-dbgsym: 2.1.2-1 -> 2.1.3-1", "ros-eloquent-image-tools: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-image-tools-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.0-1 -> 2.2.1-1", "ros-eloquent-image-transport-dbgsym: 2.2.0-1 -> 2.2.1-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "ros-eloquent-interactive-markers-dbgsym: 2.0.0-1 -> 2.0.1-1", "ros-eloquent-intra-process-demo: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-intra-process-demo-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-launch: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-ros: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-launch-testing: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-testing-ament-cmake: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-testing-ros: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-launch-xml: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-launch-yaml: 0.9.1-1 -> 0.9.3-1", "ros-eloquent-lifecycle: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-lifecycle-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-logging-demo: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-logging-demo-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 3.2.0-1 -> 3.2.1-1", "ros-eloquent-message-filters-dbgsym: 3.2.0-1 -> 3.2.1-1", "ros-eloquent-nav-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-nav-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-control: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-control-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-pendulum-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.4.0-1 -> 2.4.1-1", "ros-eloquent-quality-of-service-demo-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-quality-of-service-demo-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-quality-of-service-demo-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rcl: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-action: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-action-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-lifecycle: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-lifecycle-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-logging-log4cxx: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-log4cxx-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-noop: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-noop-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-spdlog: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-logging-spdlog-dbgsym: 0.3.2-1 -> 0.3.3-1", "ros-eloquent-rcl-yaml-param-parser: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcl-yaml-param-parser-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rclcpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-action: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-action-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-components: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-components-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-lifecycle: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclcpp-lifecycle-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclpy: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rclpy-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rcutils: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rcutils-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rmw: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-shared-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-connext-shared-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-dynamic-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-shared-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-fastrtps-shared-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-implementation: 0.8.0-4 -> 0.8.1-1", "ros-eloquent-rmw-implementation-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-implementation-dbgsym: 0.8.0-4 -> 0.8.1-1", "ros-eloquent-rmw-opensplice-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rmw-opensplice-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-robot-state-publisher: 2.3.0-1 -> 2.3.1-1", "ros-eloquent-robot-state-publisher-dbgsym: 2.3.0-1 -> 2.3.1-1", "ros-eloquent-ros-environment: 2.4.0-1 -> 2.4.1-1", "ros-eloquent-ros1-bridge: 0.8.0-2 -> 0.8.1-3", "ros-eloquent-ros1-bridge-dbgsym: 0.8.0-2 -> 0.8.1-3", "ros-eloquent-ros2action: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2bag: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-ros2cli: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2component: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2doctor: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2interface: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2launch: 0.9.1-1 -> 0.9.2-1", "ros-eloquent-ros2lifecycle: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2msg: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2multicast: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2node: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2param: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2pkg: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2run: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2service: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2srv: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-ros2topic: 0.8.2-1 -> 0.8.3-1", "ros-eloquent-rosbag2: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-converter-default-plugins: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-converter-default-plugins-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-default-plugins: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-storage-default-plugins-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-test-common: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-tests: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-transport: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosbag2-transport-dbgsym: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-rosidl-adapter: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-cmake: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-c: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-c-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-generator-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-parser: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-runtime-py: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-connext-c: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-c-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-cpp: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-connext-cpp-dbgsym: 0.8.1-1 -> 0.8.2-1", "ros-eloquent-rosidl-typesupport-interface: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-c: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-c-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rosidl-typesupport-introspection-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 1.0.2-1 -> 1.0.3-1", "\n", ": 1.0.2-1 -> 1.0.3-1", "ros-eloquent-rqt-image-view-dbgsym: 1.0.2-1 -> 1.0.3-1", "ros-eloquent-rttest: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-rttest-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-common-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-default-plugins-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-ogre-vendor-dbgsym: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-rendering-dbgsym: 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz-rendering-tests: 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "\n", ": 7.0.1-1 -> 7.0.2-1", "ros-eloquent-rviz2-dbgsym: 7.0.1-1 -> 7.0.2-1", "ros-eloquent-sensor-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-sensor-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shape-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shape-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-shared-queues-vendor: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-sqlite3-vendor: 0.2.0-1 -> 0.2.1-2", "ros-eloquent-std-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-srvs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-std-srvs-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-stereo-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-stereo-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 2.2.1-1 -> 2.2.2-1", "ros-eloquent-teleop-twist-joy-dbgsym: 2.2.1-1 -> 2.2.2-1", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-dbgsym: 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-eigen: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-msgs-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-py-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tf2-ros-dbgsym: 0.12.0-1 -> 0.12.1-2", "\n", ": 0.12.0-1 -> 0.12.1-2", "ros-eloquent-tlsf-cpp: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-tlsf-cpp-dbgsym: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-topic-monitor: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-trajectory-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-trajectory-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "\n", ": 1.0.1-1 -> 1.0.2-1", "ros-eloquent-turtlesim-dbgsym: 1.0.1-1 -> 1.0.2-1", "\n", ": 2.1.2-1 -> 2.1.3-1", "ros-eloquent-visualization-msgs: 0.8.0-1 -> 0.8.1-1", "ros-eloquent-visualization-msgs-dbgsym: 0.8.0-1 -> 0.8.1-1", "Amazon B9", "Amazon ROS Contributions", "Anup Pemmaiah", "Arne Nordmann", "Chris Lalancette", "Christophe Bedard", "Claire Wang", "Daniel Stonier", "David Gossow", "Dirk Thomas", "Eclipse Foundation, Inc.", "Erik Boasson", "Ethan Gao", "Ivan Paunovic", "Jack O\u2019Quin", "Jacob Hassold", "Jacob Perron", "John Shepherd", "Juan Pablo Samper", "Julius Kammerl", "Karsten Knese", "Michael Carroll", "Michel Hidalgo", "Miguel Company", "Mikael Arguedas", "Paul Bovbel", "Pete Baughman", "Scott K Logan", "Shane Loretz", "Steven! Ragnarok", "Steven! Ragnar\u00f6k", "Ted Kern", "Tully Foote", "Vincent Rabaud", "William Woodall"], "url": "https://discourse.ros.org/t/ros-2-eloquent-elusor-call-for-testing-and-package-releases/10923"},
{"title": "Rmw_coredds: new RMW implementation for Crystal and Dashing", "thread_contents": ["Hi.", "We developed a new RMW implementation package using CoreDDS from GurumNetworks. GurumNetworks, Inc. is officially registered as an OMG member and DDS vendor. We believe that this new implementation can provide an opportunity to try more RMW implementations and decide which one fits your needs the most.", "\nThis RMW implementation contains two projects:", "rmw for coredds: ", "typesupport for coredds: ", "These packages can be built and installed just like the other rmw and typesupport packages.", "To try this, you need a trial copy of CoreDDS, which can be found on our ", ". Before building these packages, you need to set an environment variable ", " to specify where CoreDDS is located. After that, another environment variable ", " must be set to where CoreDDS license file is located. By default, it should be ", ". Please follow below in your terminal.", "Once everything is ready, you can use colcon to build the packages.", "or", "No additional arguments are required.", "After building the packages, we suggest you to set an environment variable ", " and set the entry ", " to ", ". The RMW will publish/subscribe messages properly without this, but some APIs like ", " may not work as intended.", "Please note that these packages are still work in progress. Some features are not fully supported and we are working on them. These packages support ROS2 Crystal Clemmys and Dashing Diademata. Also, supported architectures are amd64, arm64, and arm32.", "Welcome to our ROS community!!! ", "One of the biggest changes in ROS 2 is that it supports the industry standards DDS and has a \u2018", "\u2019 feature that does not depend on one vendor. So the ROS Middleware interface (a.k.a. rmw) contains the key abstractions required for standard DDS and functions to support them. There are four vendors known to our community so far:", "Today, ", " from ", " was introduced and we have a stronger diversity. The ROS 2 LTS version, Dashing Diademata, is now available, and ROS 2 will be applied to a wide range of robots. There will be a variety of usage environments and applications, and if we have support from these powerful DDS vendors, we will be able to choose from a variety of features depending on the situation.", "Once more, I congratulate you and welcome you to the ROS community. ", "Thank you for your welcome!", "We will diligently follow up ROS 2 LTS versions and master versions also. Please try our CoreDDS trial and rmw_coredds also. I am sure you will have a good expression about that.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "eProsima\u2019s ", "\n", "RTI\u2019s ", "\n", "ADLink\u2019s ", "\n", "Eclipse\u2019s ", " (former ADLink OpenSplice)"], "url": "https://discourse.ros.org/t/rmw-coredds-new-rmw-implementation-for-crystal-and-dashing/9421"},
{"title": "Introduction of Persistent Systems to ROS2 initiative", "thread_contents": ["Hello ROS Community,", "I\u2019ve written the response below to answer questions posed over the past few weeks about who I am and why we are trying to help port packages from ROS1 to ROS2.", "Persistent Systems (BSE & NSE: PERSISTENT) is a global leader in Technology Services. We build software that drives the business of our customers\u2019 business; enterprises and software product companies with software at the core of their digital transformation. We have a global presence in 15+ countries,  9000+ Employees, 25+ years building software-driven business. Our FY2017-18 revenue was USD 470.55 Million. For more information, please visit ", ".", "Persistent Systems has been funded internally and externally to build out our Robotics expertise and we have formed an initial engineering team focused on ROS development. We will be developing software for our internal projects and for our customers around the globe that will leverage ROS as a foundation, so we have a sincere interest in improving ROS and contributing to the advancement of ROS2. At this time, we have on-boarded an initial team of software engineers to work on this project, initially porting packages from ROS1 to ROS2, and we are committed to investing time and effort to contribute to ROS2.", "Regarding package selection and maintenance, we selected the top 40 most commonly used packages. Our initial thought was that we would help people migrate packages that they were maintaining on ROS1 and wanted to have available on ROS2 but didn\u2019t have the bandwidth to port themselves.  We assumed the owners would accept the help if we submitted the pull request. I was reaching out to the community at large only because we didn\u2019t want to duplicate any other ongoing porting efforts \u2013 if anyone has a suggestion to improve this, please let me know. We are available to help answer questions in the community after these packages have been ported, as the original maintainer sees fit.", "Our team is learning here and we welcome your feedback as we engage with the community. Please let me know if you have any questions.", "Regards,", "\nPrasenjit Dan", "\nCall : (+1)-714-902-3074", "\nProgram Manager", "I applaud your willingness to put such a large quantity of resources into helping push ROS2 along.", "I think there are a couple of things you should consider in your plan:", "Just plain making a package work in ROS2 is useful, but ", ". You should take advantage of as much of the new capabilities allowed by ROS2 as possible. This means things like restructuring the design of nodes to be composable, taking advantage of QoS and deciding the best default QoS for each topic, using local parameters, and using read only parameters where it makes sense (when they become available). ", ", so doing it as part of the porting process is valuable.", "Yes, this.", "This is so important, it cannot be stated often enough.", "As nice as plain ports are, they will never be what the pkgs could be if they were only conceptually ported instead of ", " (I know that doesn\u2019t work that way \u2026 if only), and when there is a plain port, there will be much less of an incentive to do anything about the ported pkg as it\u2019s there and it works \u2026 well enough.", "Thanks ", ", ", " for your valuable input. Point noted.", "Consider giving some resources to porting tools to ROS2. As valuable as nodes are, tools are far more so.", "Can you give an example of some tools you\u2019d like to see ported?", "All the GUI tools, especially the ros QT tools, are very useful. Each one is small but they all combine to provide a powerful toolchain. Even the robot monitor GUI tool is massively useful.", "On the one hand, I agree. This is a great opportunity to do a lot of refactoring and optimization that is now possible thanks to the features of ROS2. On the other hand, I am strongly in favor of incremental development. You\u2019re almost suggesting to go for a re-implementation, meaning big steps, premature optimization, the introduction of new bugs and changes in behavior. This makes the step towards ROS2 bigger instead of smaller. Also, I think we should trust the community to contribute to the restructuring of nodes, especially after many people moved to ROS2. JMHO", "Completely agreed, This is absolutely our way of thinking as well. ROS1 to ROS2 is a significant step and I appreciate everyone\u2019s way of thinking. We are also looking back on capacity. We arefocusing on what we can contribute that will be extremely valuable to the community, and incremental development or rather incremental steps are our key focus at this time.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Porting a package from ROS1 to ROS2 is an ideal opportunity to revise the implementation in major ways. You should coordinate with the package\u2019s developers to make sure you are not rushing ahead with just making it work in ROS2 and removing that chance from them in case they have plans.", "Just plain making a package work in ROS2 is useful, but you should do more than just port. You should take advantage of as much of the new capabilities allowed by ROS2 as possible. This means things like restructuring the design of nodes to be composable, taking advantage of QoS and deciding the best default QoS for each topic, using local parameters, and using read only parameters where it makes sense (when they become available). Once code is available working in ROS2 I think it is less likely that such restructuring will be done, so doing it as part of the porting process is valuable.", "Make sure the maintainer of the package is willing to also maintain and make ROS2 releases. If the maintainer is only interested in releasing for ROS1, then a ROS2 branch may get neglected.", "If you port an unmaintained package, you should be willing to take on maintainer responsibilities and do the releases for that package. (This would be a ", " thing to help with, by the way.)", "As you do your work, also keep an eye out for things to improve in the core libraries. Then post issues for them. Or, even better, make those improvements and post pull requests. It sounds like you have a lot of resources and devoting some of them to helping with ROS2 feature development would be much loved, I think.", "Consider giving some resources to porting tools to ROS2. As valuable as nodes are, tools are far more so.", "Just plain making a package work in ROS2 is useful, but you should do more than just port. You should take advantage of as much of the new capabilities allowed by ROS2 as possible. This means things like restructuring the design of nodes to be composable, taking advantage of QoS and deciding the best default QoS for each topic, using local parameters, and using read only parameters where it makes sense (when they become available). Once code is available working in ROS2 I think it is less likely that such restructuring will be done, so doing it as part of the porting process is valuable."], "url": "https://discourse.ros.org/t/introduction-of-persistent-systems-to-ros2-initiative/6656"},
{"title": "ROS2 Security Working Group Online Meeting - Apr 17th, 2019 between 2PM and 3PM PST", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on ", ".", "Tentative agenda and meeting details are below.", "Suggestions for other topics are very welcome!", "You have been invited to an online meeting, powered by Amazon Chime.", "Chime meeting ID: 7596499512", "Join via Chime clients (manually): Select \u2018Meetings > Join a Meeting\u2019, and enter 7596499512", "Join via Chime clients (auto-call): If you invite auto dial-in as attendee, Chime will call you when the meeting starts, select \u2018Answer\u2019", "Join via browser screen share:  ", "Join via phone (US):  ", "Join via phone (US toll-free):  ", "International dial-in:  ", "FYI - ", " ", " ", " ", " ", " ", "Thanks for organizing Thomas! We\u2019ll be there. How can we su suggest items for the agenda?", "Thank you Thomas for leading the organization once again! Eager to participate from Alias Robotics on this. We have some work to share with you guys.  Can you please add the following Items to the agenda:", "2.3 Alias Robotics update about threat modeling for industrial robots.", "2.4 Alias Robotics update with ROSIN security analysis for ROS 2 based on our tools.", " - done!", "\n", " feel free to reply to this thread and I\u2019ll edit my post.", " feel free to reply to this thread and I\u2019ll edit my post.", "Great, thanks a lot. Here\u2019s my ", " then, editing your agenda above:", " and ", ", you may be interested on this.", "As ", " pointed out, in collaboration with Acutronics Robotics, we have been extending the threat model developed by Amazon Robotics to include an industrial robotic Arm. In this, case, the MARA modular robot arm. We have submitted the PR to ROS 2 design. Feedback is very much appreciated!", "\n", "\n", "Hi ", ", sorry for the late ping.", "If there is some time left today I\u2019d like to ask whether this group here would be interested to perform such ", " on such a ", " in Autoware?", "D.", "Thank you everyone for your time today!", ".", "The next meeting is already planned in two weeks as we decided today and the time is hopefully more Asia-friendly. ", ".", " - did we spend enough time on your questions today? If not, let\u2019s add your questions to the next meeting agenda.", " thanks, I think that we are fine. To recap what you said:", "Is that correct?", "Otherwise we in Autoware currently do not have anyone that has security background, so we were looking for someone that could help us get started. But I guess we can try on our own first and ask for help here if needed.", "Yes! I\u2019d suggest to also to read the section \u201cIncluding a new robot into the threat model\u201d in ", "For 4. we\u2019d be open to add pointers to URLs about how to secure RTOS but we just don\u2019t want the doc to become a guide to secure RTOS as this is out of the scope of the doc. You can see, for instance, what we wrote around NTP attacks. We point to \u201cgood practices\u201d but they are not directly described in the doc.", "Apologies I couldn\u2019t participate yesterday ", " and the rest of the group ", ". Terribly overloaded these last few weeks trying to fix some internal matters. I\u2019ve noted down in my calendar the next meeting and I\u2019ll put together some slides to kick off the next meeting with our contributions so far while discussing in a bit more detail ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " comments and feedback", "Next steps for the security work:\n", "AWS RoboMaker team planned work for Q2", "How to collaborate and tackle security risks as a community?", "Alias Robotics update about threat modeling for industrial robots.", "Alias Robotics update with ROSIN security analysis for ROS 2 based on our tools.", "\n", "Safety & Security  (Acutronic Robotics to discuss some ongoing work in the intersection of both)", "Database of robot vulnerabilities  (discussion, disclosures and a new security article maybe?)", "\n", "\n", "\n", "\n", "\n", " comments and feedback", "Next steps for the security work:\n", "AWS RoboMaker team planned work for Q2", "How to collaborate and tackle security risks as a community?", "Alias Robotics update about threat modeling for industrial robots.", "Alias Robotics update with ROSIN security analysis for ROS 2 based on our tools.", "\n", "\n", " (Acutronic Robotics to discuss some ongoing work in the intersection of both)", "\n", " (discussion, disclosures and a new security article maybe?)", "We can use this ", " as a reference and apply it to our car and use case", "We can create a PR like this ", " once we have done our own threat analysis", "The difference between your threat model and ", " is in that the latter are guidelines on how to make a secure robot and the threat model checks whether implementation of the guidelines is correct (that is there is not attack surface exposed)", "You try hard to focus on ROS 2 framework only (as oppose to e.g. RTOS)"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-apr-17th-2019-between-2pm-and-3pm-pst/8561"},
{"title": "ROS2 Security Working Group Online Meeting - June 7th, 2019 between 9AM and 10AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on  ", "  .", "Please suggest topics for the next meeting by commenting on this thread.", "Note: if we do not have any item to cover by Thursday, June 6th, this meeting will be postponed.", "You have been invited to an online meeting, powered by Amazon Chime.", "Hi everyone, I will cancel tomorrow\u2019s meeting as we don\u2019t have any agenda item scheduled.", "I will be traveling until the end of this month (June) so we will restart the security working group meeting once I am back unless someone would like to step in. Please send me a message if you\u2019re interested!", "Thanks everyone!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["TBD"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-june-7th-2019-between-9am-and-10am-pdt-utc-7/9309"},
{"title": "ROS2 Security Working Group Online Meeting - August 21st, 2019 between 9AM and 10AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on ", " \u2192 ", "Please suggest topics for the next meeting by commenting on this thread.", "\n", " to receive calendar invites in", "\nyour e-mail inbox.", "ROS 2 Security Working Group Meeting Notes Working Group Calendar URL | Google Group for invites  Please join the Google group to get calendar invites to the meeting and be able to comment on this doc.  2019-08-21 (next meeting) Meeting Announcement...", "You have been invited to an online meeting, powered by Amazon Chime.", "\nChime meeting ID: 3204163012", "\nJoin via Chime clients (manually): Select \u2018Meetings > Join a Meeting\u2019, and enter 3204163012", "\nJoin via Chime clients (auto-call): If you invite auto-call as attendee, Chime will call you when the meeting starts, select \u2018Answer\u2019", "\nJoin via browser screen share: ", "\nJoin via phone (US): +1-929-432-4463,3204163012#", "\nJoin via phone (US toll-free): +1-855-552-4463,3204163012#", "\nInternational dial-in: ", "\nIn-room video system: Ext: 62000, Meeting PIN: 3204163012#", "Please suggest topics for the next meeting by commenting on this thread.", "Given we ran short of time during the last meeting:", "Thanks everyone for your time today!", "\nThe ", " have been updated.", "\nYou can also access the meeting recording from the doc.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-august-21st-2019-between-9am-and-10am-pdt-utc-7/10220"},
{"title": "ROS2 Security Working Group Online Meeting - Nov 30th, 2018 @ 10:00 AM PST", "thread_contents": ["There\u2019s been lots of updates since the last meeting including new security based CMake macros to ease developing secure systems. As always, I\u2019m happy to take suggestions on an agenda, however I never run short of questions.", "Please let me know if there\u2019s something you would like to discuss, otherwise I will propose a subject.", "You have been invited to an online meeting, powered by Amazon Chime.", "Meeting ID: 6170 43 7479", "United States Toll-Free: +1 855-552-4463", "\nMeeting PIN: 6170 43 7479", "One-click Mobile Dial-in (United States (1)): +1 206-462-5569,6170437479#", "United States (1): +1 206-462-5569", "\nInternational: ", "SIP video system: meet.chime.in", "\nor", "\nH.323 system: 52.23.133.56", "Meeting PIN: 6170437479#", "Please let me know if there\u2019s something you would like to discuss, otherwise I will propose a subject.", "I\u2019d like to suggest adding time for discussing Security for ROS2 Actions to the agenda. It seems the current proposal relies upon composing action interfaces from dedicated ROS2 topics and services. Should these dedicated topics and services for actions be namespaced into the same DDS topic mapping as ordinary ROS2 topics and services, I suspect this crossover of interfaces will make securing or compartmentalizing/isolating permissions to specific ROS2 interfaces difficult as middleware layer; i.e. the colliding of resource identifiers from supposably different ROS2 interfaces will hinder the fedelty at which users can construct policy permissions.", "To cut to the chase, see my review comments starting ", ".", "Perhaps some others involved in action proposal/implementation could also join this week\u2019s meeting to help shed light on the challenges in rectifying this architecture concern. Ping: ", " ", " ", " ", " ", " ", " ", " ", " ", "Sounds good, I\u2019ve reached out to a couple of the people on the list, hopefully they can help spread the word. In lieu of any other suggestions, I\u2019ll plan on this being the topic of discussion.", "Hopefully some of the other people involved in the actions stuff will be able to join. The meeting is during the few hours of the night when I really do need to sleep.", "FYI: relevant PR to the ", " design article addressing actions:", "Sorry gbiggs, I plan on recording the meeting and making it available in addition to my notes. I\u2019ll try to vary the times in order to be able to get as many folks as possible.", " ,  as per the action item out of the today\u2019s meeting, could you pm me with a draft a draft of community survey on security use cases for separating privilege between topics, parameters, actions, and services? I\u2019d just like to iterate on that to make sure we are asking questions that would be informative.", "Feel free to loop me in on the draft if you want a review from outside the immediate security sphere.", "Link to the recording of the meeting: ", "I\u2019ll try to come up with something we can use as a starting point. I would prefer to just use the discussion on the ros2/design wiki unless there are objections. Having the discussion openly would given us the widest possible audience.", "Does anyone have any guidance on the format of the survey? I haven\u2019t been around for something like this in the past and don\u2019t know what the community would be most likely to respond to.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Click to join the meeting:", "You can use your computer\u2019s microphone and speakers, however, a headset is recommended. Or, call in using your phone:", "To connect from an in-room video system, use one of the following Amazon Chime bridges:"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-nov-30th-2018-10-00-am-pst/6953"},
{"title": "ROS2 Security Working Group Online Meeting - May 15th, 2019 between 9AM and 10AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on  ", "  .", "Please suggest topics for the next meeting by commenting on this thread.", "You have been invited to an online meeting, powered by Amazon Chime.", "TBD", "For the majority of meetings over the past year, we\u2019ve been quite engrossed in threat modeling, vulnerability disclosures, cybersecurity impacts on safety; the byproducts of which have really helped hammer out a nice and extensive threat model document! Still, much of the specifics thus far haven\u2019t really been very ROS2 specific, nor to robotic software in general given most of the tech industry at large has been grappling with the same security ramifications of distributed computing and cyber physical systems for many years.", "If the main objective for this is to document modern security hygiene and inform our community on adopting best practices, I\u2019d recommend we focus on curating/indexing existing references and external resources for educating ROS users on cyber security topics, while merely highlighting/footnoting the divergent requirements and security strategies specific to robotic domains that uniquely distinguishes itself from traditional cloud networks and IoT infrastructures. It\u2019s just that there is so much expert literature and specialized counsel already accessible about secure computing/networking. So hashing out the same general material is perhaps not as productive as compared to narrowing our scope to topics particular to robotics or merely just ROS2.", "On that note, I\u2019d like propose allocating a bit of of the agenda during the ", " security working group for SROS2. Having received several PMs and emails from eager individuals and organizations looking for ways to contribute to SROS2, and to better delegate TODO items, I want to coordinate with the rest of the working group on polishing some ideas and leverage the expertise other developers. I\u2019ve been carving out a few orthogonal areas for development, giving high level overviews on potential features, including motivations, previous approaches, as well as existing challenges. To link some of them here:", "Thanks Ruffin, I updated the agenda with your proposal.", "\nThere is no other item to be discussed tomorrow so far so we may have a meeting shorter than usual.", "Thanks Ruffin for bringing up sros2 to the agenda. I will unfortunately be unable to attend the meeting today. There are a couple extra SROS2 action items that could be interesting to touch base on (maybe next meeting?) such as automated testing of security features, documentation, development practices and granularity in securing topics (e.g. topic A should be encrypted, topic B should only be signed etc)", "+1 for the topics proposed by ", ". Acutronic Robotics is particularly interested in ", " and ", ".", "Sounds like both topics could fit nicely within the ", " section at ", "Thanks to everyone who attended today!", "During the meeting, we went through the different ideas and potential designs. Most of the content is captured in the tickets, but there was some additional questions / comments (see below).", "\n", "\n", "One challenge for this task is to define an appropriate strategy to bundle those files.", "What is ", " in the profile config files?", "\nThis is an AppArmor variable, expended to the ROS install directory. This is not e.g. an environment variable.", "Are profiles always read from the system?", "\nAdditional profiles can also be injected using CLI tools.", "\n", "\n", "Dropping OpenSSL could mean dropping support for third-party OpenSSL engines providing access to the keys in HSMs?", "\nPrivate keys could be flashed on HSMs by manufacturers.", "\n", "\n", "Subscribe to a secure logging topic with permissions issues, etc. Can be used to improve the policy.", "Is there any overlap between ROS 2 logging and DDS logging?", "\nActually not that much overlap, separation between application layer and transport layer is good to have.", "\n", "\n", "Having a tool similar to aa-genprof / aa-logprof in AppArmor world,", "\nThe application is audited first and a \u201ctemplate\u201d config file is generated by the tool, asking the", "\nuser to confirm if each rule is OK. It also can server as a \u201clinter\u201d / suggest improvements.", "\nWe could implement the same for DDS ACL policies.", "Could be implemented either using the DDS logging modules or by scraping DDS discovery information.", "There are also some idea about allowing ROS 2 nodes to declare IDLs containing the list of topics they public/subscribe to, actions, services, parameters, etc.", "Thomas: this would be interesting as it would allow us to run linting / static analysis on applications (are all IDLs compatible), add additional properties to topics like rate limitation and automate e2e testing up to a certain point (IDL declare this node listens to ", " , a test could be generated to check this is actually the case).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["SROS2 improvements suggestions (", ")"], "url": "https://discourse.ros.org/t/ros2-security-working-group-online-meeting-may-15th-2019-between-9am-and-10am-pdt-utc-7/9011"},
{"title": "Call for contributions: ROS 2 Node.js client", "thread_contents": ["Hi there,", "I want to invite anyone who is interested in developing robotic programs by web technology to contribute to the ", ".", "Node.js is a JavaScript runtime which is powerful, across-platform and easy to develop. Also, by leveraging npm, it has the largest distribution platform. The Node.js client of ROS 2, ", ", is just the one based on Node.js, not the node in ROS ", " and the rclnodejs started to support the ROS 2 since its first stable release - Ardent Apalone in 2017 (long long ago). I also presented the project in person on ", ".", "The latest release of rclnodejs (0.10.0) has supported the Dashing Diademata. Along with the increasing maturity of ROS 2, more and more developers in the ROS ecosystem would like to migrate their projects to ROS 2 gradually, and I believe the requirement of having a Node.js (JavaScript) client will be stronger than anytime before. The project rclnodejs is managed by the ", " organization . If you have web developing experience in ROS 1 era, you must hear of it. It offers a bunch of useful modules to bridge ROS to the web world. For the rclnodejs, it has a thin layer to wrap the rcl library, and 96% test coverage to ensure the quality. What\u2019s more, we have Windows/Linux/macOS CIs to verify every PR and release.", "Unfortunately, there is still a gap. I submitted an ", " to describe the functions which are absent till now. I hope anyone who cares the web technology can help to contribute.", "Thanks!", "\nMinggang", "We at Rover Robotics are interested in helping to support web tools for ROS 2. We are currently working on web tools for setting up a robot (connecting it to wifi, setting UDEV rules). Then we will be moving on to higher level features like creating / editing / saving maps, visualizing camera feeds with bounding box overlays.", "We think that it would be nice to modularize these components into apps and have a ROS 2 web app store for posting to and installing from.", "Hi Nick,", "I am very glad to hear from you, thanks!  I strongly agree that integrating the web components into ROS 2 is valuable.", "Currently, we have two components designed for ROS 2, that\u2019s ", " and ", ".  Others are suitable for ROS 1 only, you can check ", ". But I want to point out that some repo are not active, please notice that if you want to reuse them in the future.", "Meanwhile, the OSRF once expressed that they want to port the current ROS 1 components to ROS 2, see ", ", but I don\u2019t know the current status of it.", "Finally, you are welcome to contribute to the components of ROS2, and you can also contact ", " who is very helpful to discuss your thoughts.", "Meanwhile, the OSRF once expressed that they want to port the current ROS 1 components to ROS 2, see ", ", but I don\u2019t know the current status of it.", "The porting effort is still ongoing. The latest PR is from today: ", "Hi Dirk, thanks for your update!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/call-for-contributions-ros-2-node-js-client/9742"},
{"title": "ROS2: Security tools for development and production", "thread_contents": ["Hi all, my team at Amazon is looking for your input on security tools!", "Developers looking to secure their robots and actively develop new nodes will have to manually create keys and policies, as well as debug their node connectivity should security not be set up properly. Conversion from an unsecure DDS to secure DDS may also have a performance impact, therefore developers will need to test and develop with security enabled for production.", "SROS2 has done an excellent job creating their own wrappers to demonstrate the security capabilities of FastRTPS and RTI Connext. With this in mind, we want to bring forth three tools to make security quicker and less error prone for developers.", "\nThis is error prone for developers since we often use docker containers, different machines, direct development on the robot, and all the amazing ways we like to create robots.", "\n", "\nWe would like to present a cmake macro for generating the key and keystores during build time. Our macro uses SROS2 on the back end, but the macro definition should stay the same should a team want to define their own security generator. This enables each developer to have the expected authentication without having extra steps at run time. The ROS security environmental variables will still need to be set outside the macro.", "\n", "\n", "\n", "\n", "\n", "\n", "\nA release engineer or developer would need to inspect the node graph, all services, and soon actions to create policies for each node. We are considering creating two tools for easing deployment to production security and debugging.", "\n", "\nDevelopers will be using ROS2 for a myriad of applications, and teams will want to know the effects of choosing certain providers over others, and the level of security they need. ApexAI provides their repository (", ") which we will use to run a series of performance tests with and without security to prevent regression and understand system impact.", "\n", "Thanks for putting this together! Although not formally a command line tool, our team at Alias Robotics has been working on the Robotics CTF (RCTF). An online environment for penetration testing. RCTF can run both in our servers or locally through Dockerized scenarios (", ").", "We are making very active use of this with our clients and are more than happy to contribute with support and maintenance to the community.", "\nIn addition to the availability of the RCTF, we\u2019d be happy to put person-power and create custom scenarios for sensitive security aspects within ROS 2.0. This way, evaluation is simplified and easily accessible to a wide variety of security researchers", "\nStatus: In Code Review", "\nDeadline for release: 11/7/2018", "\nStatus: Blocked on Node Graph API", "\nDeadline for release: 11/14/2018", "\nStatus: Started for eprosima Fast-RTPS", "\nDeadline for release: 11/14/2018", "\nSee: ", "\nStatus: Upcoming", "\nDeadline for release: 11/16/2018", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A node graph inspection tool will be available to take a snapshot of the node graph and generate policies for each node. It will print feedback on nodes which advertise services and actions, which developers will use to add permissions for client nodes.", "(OPTIONAL) rqt_graph modified with permissions, such that a developer can inspect their system and visualize the access control of their nodes."], "url": "https://discourse.ros.org/t/ros2-security-tools-for-development-and-production/6487"},
{"title": "RMW layer for Eclipse Cyclone DDS", "thread_contents": ["Hi everyone,", "The Eclipse Cyclone DDS project (", ") at long last has received an RMW implementation, which can be found at ", ". Eclipse Cyclone DDS is an Eclipse Foundation project and so a true open-source implementation of DDS. That it has excellent performance as well hopefully makes it a rather attractive proposition ", "I\u2019m hoping that this RMW layer will eventually be accepted as a ROS2 package, move to the ROS2 repositories and become part of the set of packages that is built by default, but until then, it\u2019ll be available in this location. Also, it is really still very young, so please test the hell out of it and then be gentle with me when you report the issues you will inevitably encounter \u2026", "To build/use it you need to have a recent commit of Cyclone DDS installed somewhere, say in $CMAKE_INSTALL_PREFIX. Actually building the RMW implementation then follows the standard procedure: clone it in a ROS2 workspace source directory, set CycloneDDS=$CMAKE_INSTALL_PREFIX/share/CycloneDDS and build with colcon as usual.", "update: Eclipse Cyclone DDS has been refactored to build easily with ROS 2 using colcon. Drop it in and build it, try it and give feedback. We really like GitHub issues\ud83d\ude01  ", " in ROS 2 Eloquent.", "tip: if your wifi has problems with multicast, then configuring DDS to use multicast only for discovery (SPDP - Simple Endpoint Discovery Protocol) can fix that", "cyclonedds ", " for SPDP", ", ", " says: ", " today gives you a pretty decent small message latency over loopback on 8 year old 3.5GHz Xeon E3-1270 running Ubuntu 16 \u2026 you see it comes out rather well for the small ones, even though it uses the loopback interface, and fares not so badly against the latencies mentioned earlier using a shared memory  ", "  for the large ones.", "[0] median latency as measured by doing round-trips as fast as possible and halving the measured round-trip time, using reliable communications \u2026", "\n[1] keyless topic of one int32_t, the others have an int32_t key field", "\n(always set to 0) and an octet sequence;", "\n[2] this is with a fragmenting threshold set large enough to not fragment the ones that fit in a single UDP datagram; with the default setting the numbers are worse.", "I\u2019d like to add that we\u2019ve been testing Nav2 with CycloneDDS, it\u2019s looking very reliable when running our system tests 100\u2019s of times. ", "We saw some pretty good results with Nav2 on CycloneDDS as well. ", "ROS 2 dashing w ", " powered ", " + industrial controller are working reliably via WiFi at the Intel + ADLINK + AWS RoboMaker exhibit at Pack Expo despite 666 WiFi APs covering that section of Las Vegas Convention Center. ", " says \u201cthat is a devilish number of APs\u201d", "ros2/rmw_cyclonedds working reliably under challenging wifi conditions is consistent with ", " and other ros2 contributors", "ROS2 RMW layer for Eclipse Cyclone DDS. Contribute to ros2/rmw_cyclonedds development by creating an account on GitHub.", "\n", "\n", "\n", " found 666 APs", "\n", "Awesome! Congrats on the demo! ", " ", " gave terrific talk at ROScon about iRobot using ros2/rmw_cyclonedds, said it beats all iRobot performance targets for low latency, reliability and low RAM. They worked out how to scale fleets using cyclonedds and contributed a new intra-process manager to dashing.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rmw-layer-for-eclipse-cyclone-dds/9552"},
{"title": "[CANCELLED] ROS2 Security Working Group Online Meeting - May 28th, 2019 between 9AM and 10AM PDT (UTC-7)", "thread_contents": ["Hi everyone,", "We are planning to hold the next ROS 2 Security Working group meeting on  ", "  .", "Please suggest topics for the next meeting by commenting on this thread.", "Note: if we do not have any item to cover by Monday, May 27th, this meeting will be postponed.", "You have been invited to an online meeting, powered by Amazon Chime.", "Is there a calendar invite for these? I would like to start attending but I keep missing the discourse announcements.", " Sure, let me try to find a solution for this.", "I use google calendar to schedule and send invites for the Navigation2 WG, can you do that?", "We\u2019re not using Google Calendar here. I\u2019m trying to see if OSRF could host this for us. I\u2019ll update this thread once I have a solution implemented.", "Hi all,", "\njust a reminder that if we do not have any agenda item by Monday EOD, I\u2019ll cancel the meeting and we\u2019ll meet the week of 06/10.", "Thank you!", "As we do not have any agenda item for tomorrow, I am cancelling the meeting.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["TBD"], "url": "https://discourse.ros.org/t/cancelled-ros2-security-working-group-online-meeting-may-28th-2019-between-9am-and-10am-pdt-utc-7/9158"},
{"title": "Missconfiguration leading to missing package?", "thread_contents": ["Hi, I am still doing my first steps with a custom buildfarm. Now I got stuck during the \u201clocal\u201d build when runing the script generated by \u201cgenerate_devel_script.py\u201d. The code gets checkout out fine and building and testing distro packages like rviz etc works. However with my own package it thows:", "[\u2026]", "\nResolved the dependencies to the following binary packages:", "During handling of the above exception, another exception occurred:", "Traceback (most recent call last):", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 198, in ", "\nrawpkg = self._cache[key]", "\nKeyError: \u2018ros-kinetic-moveit-ros-planning\u2019", "During handling of the above exception, another exception occurred:", "Traceback (most recent call last):", "\nFile \u201c/tmp/ros_buildfarm/scripts/devel/create_devel_task_generator.py\u201d, line 251, in ", "\nmain()", "\nFile \u201c/tmp/ros_buildfarm/scripts/devel/create_devel_task_generator.py\u201d, line 116, in main", "\nget_binary_package_versions(apt_cache, debian_pkg_names))", "\nFile \u201c/tmp/ros_buildfarm/ros_buildfarm/common.py\u201d, line 144, in get_binary_package_versions", "\npkg = apt_cache[debian_pkg_name]", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 200, in ", "\nraise KeyError(\u2018The cache has no package named %r\u2019 % key)", "\nKeyError: \u201c", "\u2019\u201d", "Is the something wrong with my buildfarm configuration or did I make a mistake in my package.xml?", "\nOr can I somehow refresh the cache", "sudo apt-get install ros-kinetic-moveit-ros-planning works fine\u2026", "Some hint on possible causes would be helpful even if it is just a guess.", "I don\u2019t know how you have setup your own buildfarm and what packages you are actually building. But looking at the official build farm and the status of the ", " package: ", "The public repo contains an older release of the package (the blue boxes in the X64 and X32 columns). So if you are using that apt repo you can successfully install the Debian package. But the current version fails to build (red boxes in the X64 and X32 columns) and therefore there is no Debian package available in the building and testing apt repos.", "On the official farm that is caused be the latest OpenCV3 release which fails to build the sourcedeb (see ", "). And that package is a transitive dependency of ", ". So until a new OpenCV3 is being made that problem will stay the same.", "Ok, thanks for the quick answer!", "Seems that I still lack essential understanding of the buildfarm mechanisms I will try to dig depper into it. But so far I understood that:", "Did I get this right so far? The only possible thing is to wait for a fix?", "Could I get my package to green using travis as described in: ", " ?", "Did I get this right so far? The only possible thing is to wait for a fix?", "Could I get my package to green using travis as described in: ", " ?", "It doesn\u2019t matter where you run it. If a package that your package depends on is not successfully building, your devel job cannot install all the dependencies, and thus your package cannot be successfully built.", "You have a few other options:", "Note that we\u2019ve ", " release in the main rosdistro and a rebuild is in progress so if that\u2019s the only issue blocking your tests, the wait for a fix approach is likely to work in the short term.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["liblapack-dev", "ros-kinetic-catkin", "ros-kinetic-joint-state-publisher", "ros-kinetic-moveit-core", "ros-kinetic-moveit-ros-planning", "ros-kinetic-pluginlib", "ros-kinetic-robot-state-publisher", "ros-kinetic-roscpp", "ros-kinetic-rviz", "ros-kinetic-tf-conversions", "ros-kinetic-urdf", "ros-kinetic-xacro", "\nTraceback (most recent call last):", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 194, in ", "\nreturn self._weakref[key]", "\nFile \u201c/usr/lib/python3.5/weakref.py\u201d, line 131, in ", "\no = self.data", "\nKeyError: \u2018ros-kinetic-moveit-ros-planning\u2019", "If I currently declare in my package a <build_depend> to moveit_ros_planning the devel job will fail since moveit_ros_planning does currently not build successfully.", "Because it does not build successfully its debian does not appear in ", " and ", " and therefor can not be apt inside the devel job.", "The result will be that my package won\u2019t get \u201cgreen light\u201d until moveit is fixed and thus I case I would be part of the official buildfarm it would not get included in the official ros release.", "You can wait for a fix.", "You can try to help fix the issue in the dependency. On your custom buildfarm you can choose to override versions of packages, such as rollback or patching repos too.", "You can find a way to eliminate that dependency and then you won\u2019t be blocked by it."], "url": "https://discourse.ros.org/t/missconfiguration-leading-to-missing-package/1898"},
{"title": "ARIAC code release updates", "thread_contents": ["The software for competing in ARIAC 2018 has been released.", "See ", " for competition documentation, installation instructions, and tutorials for working with the software.", "New ARIAC software release (excerpt from ", "):", "First qualifier has been released (excerpt from ", "):", " Submissions for the first qualifier will be uploaded via secure online workspaces. All registered teams must contact ", " to have their workspace prepared in advance of when they intend to submit. If you are planning a submission for the first qualifier and do not yet have a secure workspace, you must contact ", " immediately or you risk missing the submission deadline.", "After the first qualification task closes the ", " topic will become classified as a cheat.", " The IIWA14 will be used in all future trials; the UR10 will no longer be used. The IIWA14 presents the same ROS interface but ", ". This was announced as an upcoming change on 17 February 2018.", " The workcell environment, particularly the shipping container, shelving and the conveyor belt, has been updated to accommodate the working area of the IIWA14. ", " The dimensions of the shipping boxes, storage bins, and products have not been modified.", "The controller has been updated to avoid an issue with joint 7 getting stuck, and to relax tolerances on the trajectory controller (", ").", "To test out the proposed changes, add the ARIAC pre-release repository with:", "and then run ", ". Your ARIAC version will be updated to 2.1.5, which will become the standard version if no regressions are reported. Please report regressions on the issue linked above.", "When you are finished testing we suggest you remove the ", " added in the above command.", "The software for competing in ARIAC 2019 (", ") has been released!", "Please see ", " for instructions as they have changed, especially if you have installed a prerelease. This version is feature complete. Follow this thread to get notified of future bug fix releases.", "Other links", "The software for competing in ARIAC 2019 (", ") has been released!", "\nPlease see the ", " and ", " for more information.", "This release contains the trial config files for the Qualifier.", "\nPlease see the ", " to learn how qualification works and the ", " to learn how to practice on the Part A trial configs.", "\nQualifier submissions will be accepted until ", ".", "Other links", "The end date of the 14th conflicts with those on the ", " site (", "):", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " See ", " for how to launch the trials. Summary of new trial config files (more details in the files themselves):\n", "\n", ": high priority order interruption at a \u201cconvenient\u201d time.", "\n", ": high priority order interruption at an \u201cinconvenient\u201d time.", "\n", ": order requiring products to be flipped (", ").", "\n", ": order requiring more products than available for use.", "\n", "\n", " See ", " for details.", "\n", " The arm can no longer pass through the support frame around the storage bins. This was advised as an upcoming change on 17 February.", "\n", " In response to the additional workcell collisions, the logical camera range has been reduced.", "\n", " This would happen if the ", " service was called before the simulation had finished loading.", "\n", " See ", " for details.", "\n", " See ", " for how to enable/playback simulation state logging for debug purposes.", "\n", " ", " now covers rules/scoring metrics in addition to software changes.", "\n", " Fixes for the simulation state logging (low RTF during recording, no arm inserted during playback) will be resolved in the next Gazebo release.", "\n", " The ", " product\u2019s yaw is not distinguishable with perception sensors, but its yaw is evaluated by the scoring algorithm. This will be corrected for, either by making the yaw perceivable or by having the scoring algorithm ignore the yaw.", "\n", " In particular, info on the storage bins, quality control sensors, potential products, and potential order count. See ", " for the added details.", "\n", " ", "/competition specifications have been updated to clarify that sensors can only be placed in static locations around the environment, they cannot be attached to the arm.", "\n", " Docker images used for the ", " are currently using gazebo7.7 to avoid state log recording issues, but will be swapped to the next gazebo version when it is released (within the next week).", "\n", " A fix for the simulation state log playback freezing with gazebo8.3 will be resolved in the next Gazebo release. This impacts teams looking to playback state logs from the automated evaluation setup. See ", " for how to install gazebo8.2 as a temporary fix.", "\n", " After the first qualification task closes the ", " service will become classified as a cheat. Teams will need to use sensors to infer the products available in storage bins.", "\n", " In ariac2.0.4, models in the environment of a particular type have sequential IDs. The naming system will be modified so that model IDs are randomized. As a result, logical cameras/quality control sensors will not publish TF frames, which include model IDs, correlated to the number of models in the environment.", "\n", " In ariac2.0.4, the conveyor can be controlled before the competition has been started. This functionality will be removed and must not be exploited by teams.", "\n", " For all future trial config files released (including qual1b), products in the storage bins will no longer have sequential IDs.", "\n", " Fixed misalignment of the vertical bars in the support frame around the storage bins.", "\n", " It is now asymmetric on top and bottom. Pulleys (and all other products) will always start un-flipped in the storage bins.", "\n", " By default the UR10 has joint limits of ", "; there is now an option to use joint limits of ", ", which provides better controller performance ", ". It can be enabled by adding ", " to your team\u2019s config file ", ". Participants that do not wish to enable this option do not need to make any changes to their system. The use of this option is permitted in submissions for the first qualifier.", "\n", " The ", " Docker image used in ", " has been updated to use gazebo8.4 now that the logging issues have been resolved.", "\n", " Teams must have had a secure workspace created by competition controllers before they can upload their submissions for ", ".", "\n", " See ", ". Note that these may change between rounds of the competition.", "\n", " Clarified that only automated evaluation metrics will be used for ", " and that any participants found violating submission guidelines (e.g. by starting the conveyor belt before starting the competition) will not be permitted to qualify for the Finals.", "\n", " ", ", ", ", ", ", ", ", ", " topics/services documentation added/updated in ", ".\n", "Clarified that requesting that the drone deliver any unknown shipment type will cause the drone to remove the box without scoring it.", "\n", "\n", "\n", "\n", "\n", " Re-runs for ", " will be permitted only in the event of simulation bugs such as reported issues ", " and ", ", at the discretion of the competition controllers. Any issues caused by competitors\u2019 code does not warrant a re-run.", "\n", " ", " has been updated to clarify that simulation state logging will be enabled in the first qualifier trials via the ", " setting in the the trial config file. If the config file that you are testing with in the automated evaluation setup does not have logging explicitly enabled ", ", please add that line to the config file to enable logging.", "\n", "\n", "\n", "\n", "\n", " Tutorials involving control of the robot arm have been updated for the switch to the IIWA14.", "\n", " Read/use ", " for practice.", "\n", " Read/use ", " for practice.", "\n", " The config file that was used for evaluating Part B of the first qualifier is in ", ". Congratulations to the teams that qualified for the Finals.", "\n", "\n", "The ", " service and the ", " topic have been re-classified as cheats.", "The quality control sensors now publish anonymized model names instead of the types of the faulty products that are detected. As a side effect, the model numbering scheme has been updated; any randomized faulty product IDs in custom-defined trial configs will need to be updated.", "\n", "\n", " Use of the devel space is now supported for users building from source (contributed by ", ").", "\n", " The ", " now highlights that sample config files are available for practicing with.", "\n", " Read/use ", " for practice. Note that re-connecting to some sensors during development will cause them to resume publishing data, but this functionality is blocked in the automated evaluation setup.", "\n", " This is the break beam positioned at the end of the conveyor belt by default.", "\n", " Service calls to set the conveyor belt power before the competition has been started will fail.", "\n", " The ", " now detail that sensors can be placed in any free space in the workcell, they do ", " need to be mounted such that they are touching the conveyor belt/support frame of the storage bin. Sensors must be used in a realistic manner and must not exploit any simulation technicalities such as the logical camera seeing through obstructions.", "\n", " See ", " for details. Teams that did not participate in the first qualification task, and teams that participated but did not qualify, are still eligible to participate in the second qualification round.", "\n", " ", ", which would occur occasionally for some participants, should now be resolved.", "\n", " The external walls of shipping boxes have been widened to make ", ", where boxes would fall into the conveyor belt, less likely to occur. The internal dimensions of the boxes are unchanged.", "\n", " The ", " have been updated to highlight that Gazebo 8.4 is required to avoid issues with state logging recording/playback. ", "\n", "\n", " ", " has been added, which summarizes the released agility challenges, including additional details on the \u201csensor blackout\u201d challenge.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", " The arm simulation model/controller has been updated to avoid an issue with joint 7 getting stuck/causing instability, and to relax tolerances on the trajectory controller (", "). ", "\n", "\n", " The ", " page now clarifies that products must be placed onto the base of the shipping box to be counted for scoring, not on top of other products.", "\n", " Two \u201cpre-release\u201d versions of 2.1.5 were released to users for testing prior to yesterday\u2019s ariac2.1.5 release to improve IIWA14 controller performance: 2.1.5~pre1 and 2.1.5~pre2. The version released yesterday was 2.1.5~pre2; ariac2.1.6 rolls back to 2.1.5~pre1 based on feedback from participants that it provides better arm performance.", "\n", " As mentioned in ", ", 500 sim seconds will be used for qual2b. The ", " now clarify that this information is not broadcast by the ARIAC server. For the Finals, as with the Qualifiers, the time limit will be set as a fixed value for all trials, which teams will know in advance. There are no time limits for individual orders.", "\n", " The config file that was used for evaluating Part B of the second qualifier is in ", ".", "\n", " The simulation model of the conveyor belt has been updated to  make ", ", where boxes fall into the conveyor belt, less likely to occur. There should be no other observable impact to users.", "\n", " The ", " has been released, including details on what to expect in the Finals, dry-run testing on the competition machines, and the schedule for the process.", "\n", " Structure of the ", " directory output by the automated evaluation setup is now described ", ".", "\n", " The config files used for the Finals of the competition are in the ", " directory. ", ". ", "\n", "\n", " (This change was included in the version of ARIAC used for evaluating the Finals).", "\n", " Closing/collection of shipping boxes during log playback now works correctly during state log playback.", "Registration: ", "\n", "Main NIST ARIAC Website (General information): ", "\n", "Competition rules: ", "\n", "Software documentation: ", "\n", "Support Email: ", "\n", "Forum for discussion: ", "\n", "Register to compete: ", "\n", "Main NIST ARIAC Website (General information): ", "\n", "Competition rules: ", "\n", "Software documentation: ", "\n", "Support Email: ", "\n", "Forum for discussion: ", "\n"], "url": "https://discourse.ros.org/t/ariac-code-release-updates/4009"},
{"title": "Project Azure for Kinect (4th gen Kinect)", "thread_contents": ["I\u2019m stealing ", "\u2019s (and/or ", "\u2019) thunder a bit, but just thought I\u2019d start a topic about the \u2018new Kinect\u2019 that MS has just announced: ", " (random link to VentureBeat article). MS site here: ", ".", "Some impressive specs (copied from the VB article):", "The depth image resolution will probably require GPU accelerated processing, as the Kinect2 was already enough to bring many machines to their knees.", "Edit: some other articles/links:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["1024\u00d71024 depth image resolution", "Highest Figure of Merit (highest modulation frequency and modulation contrast resulting in low power consumption with overall system power of 225-950mw)", "Automatic per pixel gain selection enabling large dynamic range allowing near and far objects to be captured cleanly", "Global shutter allowing for improved performance in sunlight", "Multiphase depth calculation method enables robust accuracy even in the presence of chip, laser and power supply variation", "Low peak current operation even at high frequency lowers the cost of modules"], "url": "https://discourse.ros.org/t/project-azure-for-kinect-4th-gen-kinect/4729"},
{"title": "Reliability, safety, security, maintenance and support in ROS", "thread_contents": ["In the last edition of ", " there is an ", " (1.3 MB) with the title ", " on ROS and especially on the differences between the \u201cROS approach\u201d and \u201cexisting robot manufacturers approach\u201d. The article is written by Henk Kiela, professor in Mechatronics and Robotics. He raises some interesting points and gives advice for both ROS developers as \u201cnon-ROS developers\u201d. I am very interested in your opinions on this article.", "It is an interesting article.", "The main takeaway for me is that ROS community prefer to go broad (the more, the better) whilst a typical robotic product need to do deeper.", "In the specific context of mobile robot navigation, it is not surprising that some commercially available solutions are better, since they could focus full time on a single use case.", "Additionally, I believe that ROS was designed from the ground up as a easy-to-learn framework for non experts; this is, at the same time, its biggest strength and weakness, since the openesses of the system also makes very hard to deal with safety.", "About the comparison table: I do not agree with a few points and it feels a little bit biased, even if I agree on most of the items (80% maube?)", "\nThere is one in particular that I consider very interesting:", "Open source is \u2018free\u2019; the effort to make it a reliable and safe system is unpredictable.", "This is unfortunately true; I have multiple examples of situations in which I could have been better served, in the long term, by an expensive COTS solution rather than the open-source \u201cfree\u201d one.", "I have been complaining a lot about the open source model and the issue of sustainability. I like open source software (hey, I have my own OSS projects!), but it is naive to expect that:", "Thanks for sharing the article.", "Davide", "Hi,", "In my humble opinion, I do not agree with the focus of the article. I think you can not compare a final product of two companies with ROS, even if we limit it to their navigation capabilities. ROS provides a base where to build concrete applications, and it is the responsibility of the person who develops a final application to complete the generalities of ROS to adapt it to a final solution.", "There are many robot companies that have robots navigating reliably and safely in real environments using ROS. We have a Pepper navigating 24x7 in a real office environment (", ") in which, of course, there was a lot of work of adjusting parameters.", "Of course, this requires a lot of work to adjust parameters, but no more than those that would have to be adjusted in any other navigation system, with the advantage that in ROS there is a community that develops and tests software that you do not have to implement from scratch .", "Anyway, thanks for sharing,", "\nFrancisco", "Thanks for sharing, it\u2019s an interesting article.", "I have to grudgingly agree with most of the points he makes. Right now it\u2019s very difficult to integrate ROS safely in an industrial grade robot, having either to encapsulate the cell in a parallel, isolated safety system or making ROS act as a kind of supervisor, encapsulating more low level and safety critical systems.", "I see the value of focusing ROS on scientific research and foregoing more mundane features such as these, but support from industrial manufacturers could really ensure the longevity of the project.", " great that you brought this up for discussion and thanks!", "A few thoughts from my side: Henk summarized pretty nicely the relevance of ROS and some of the issues related to building robots from ground up. One must note that Prof. Kiela has several decades of experience in this area and a deep industrial insight. However, to the best of my knowledge, he\u2019s not very actively involved in the ROS community (at least it didn\u2019t feel that way last time we spoke!).", "He presents ROS 2.0 as", "a project was started recently, incorporating the new", "\nrequirement for mission-critical functionality and indeed safety", "Arguably, some members of the community will object since ROS 2.0 started quite a few years ago (e.g. see ", "). Nevertheless, ROS 2.0 has indeed become official within the last year. Much has happened since though!", "In addition and as pointed by ", ", I disagree with several aspects exposed in the comparison made in table 1. E.g., ROS is indeed being used in safe environments and just recently, at ROS-Industrial Conference, a speaker shared quite a few details about their safe setup while using ROS (1)  and how they did it (they followed an isolated safety system-approach making the ROS setup as a behavior coordinator, as mentioned by ", " above).", "Of course, this doesn\u2019t mean they \u201ccertified ROS\u201d (completely) and one should questions whether this makes sense at all. Selecting, however, individual ROS packages (or stacks) and adapting them for complying with a particular set of guidelines is something that several companies are already doing.", "A store of ROS-certified modules could provide a similar", "\nfunction to the user community. The certification should", "\nprovide minimum qualifications for the performance,", "\nsafety, security and maintainability of a module. Such a", "\nscheme could be adopted for ROS 2.0 in the future, but this", "\nshould also be done right away for ROS Industrial.", "I think it\u2019s relevant to note here that by modules, Henk here refers to both software and/or hardware modules. Certification of such is an ongoing discussion and new standards like ISO 22166, hopefully, will provide some answers. A pre-requisite and one of the aspects required for modularity (and hardware reconfiguration) is interoperability. In the article, it\u2019s claimed that \u201c", "\u201d in ROS. I also disagree with this. Furthermore, there exists new projects like ", " that aim to facilitate interoperability by defining a common information model that generates artifacts in an MDE fashion (while offering a structure that allows for its extension in other robotic frameworks).", "I think you can not compare a final product of two companies with ROS, even if we limit it to their navigation capabilities. ROS provides a base where to build concrete applications, and it is the responsibility of the person who develops a final application to complete the generalities of ROS to adapt it to a final solution.", "I find this statement pretty interesting ", " and while I don\u2019t fully disagree when it comes to  \u201c", "\u201d , I actually have a pretty strong opinion about the usefulness of ROS (ROS 2 in particular) to compare different individual components. Our team, often, finds that comparing two different pieces of hardware of the same kind (e.g. two cameras or two actuators aimed for a similar task) is actually pretty hard. Specially because things like communication interfaces and APIs differ substantially (which makes system integration a complete hell most of the time).", "Our approach for the last years has been to \u201cfind common ground\u201d to attack these comparisons. Typically, assuming you have component A (", ") and component B (", ") with their respective interfaces, let\u2019s say ", " and ", ",  to compare them. Often, you\u2019d go ahead and create an abstraction layer on top of ", " and ", " called ", " that allows you to speak to both components and effectively make the comparisons. This way, you can inspect which component ", " or ", " actually performs better for your needs.", "To us, ", ". I think this is one of the core principles of ROS and also, the reason why we selected it when we started building ", " years ago covering not only logical and electrical interfaces but a wide variety of aspects required to go from a component to a module (note that modules imply certain characteristics including interoperability).", "Now, back to your statement, I believe it depends very much on the \u201cfinal product\u201d (assuming we\u2019re speaking of robots, final robots) itself. E.g., we started a while defining ", ". While this is still a work in progress, we managed to interface with several 6DoF arms (", ") and that provided quite a bit of insight that we later used in the development of our latest robots so I\u2019d argue that you can actually compare ", " final products with ROS.", " I\u2019m very interested to hear what\u2019s your reaction to this since as I said, we\u2019re still exploring this path and we certainly could use additional insight.", "ROS is indeed being used in safe environments and just recently, at ROS-Industrial Conference, a speaker shared quite a few details about their safe setup while using ROS (1)", "  Is there a recording of this talk?", "there is an inofficial ", " of the talk \u2026 and there will be a publication of the talk via the official ", " channel with dedicated playlists for each day of the conference \u2026 I will keep you posted\u2026", "there is an inofficial ", " of the talk \u2026", "I think ", " means the talk from Georg Heppner and Fabian Fuerst (Flexible Automotive Assembly with Industrial Co-workers).", "Georg Heppner and Fabian Fuerst (Flexible Automotive Assembly with Industrial Co-workers).", "that was on Day 3, the playlist for ", " is not yet finished. Sorry, it looks like you will have to wait until January 2019\u2026", "  That is great to hear.  Thanks for making it available.", "Hey there, happy new year.", "As ", " said, it\u2019s the talk about Flexible Assembly. AFAIK it\u2019s not ready just yet. Let\u2019s wait until ", " and his colleagues publish it.", "Would be happy to keep discussing it then.", "Cheers!", "Here is the ", " of ROS-Industrial Conference 2018 in Stuttgart, Germany.", "The talk on Flexible Automotive Assembly with Industrial Co-workers", "\nby Fabian Fuerst, Opel, and Georg Heppner, FZI", "\ncan be ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The quality of an horizontal OSS software match the quality of a vertical and commercial one.", "That it is possible to create high quality software without a sustainable way to pay full-time, highly-skilled developers."], "url": "https://discourse.ros.org/t/reliability-safety-security-maintenance-and-support-in-ros/7146"},
{"title": "Fully Open Source (Industrial) Robotics Controller", "thread_contents": ["I\u2019m unsure if this is the right forum to discuss ROS related business ideas or not, if not please let me know where the right place is.", "I have this vision of a fully open source industrial robotics controller based on ROS-I. This includes hardware and software, basically, everything that you need to drive a robot from the motor drivers up or alternatively to drive a robot with an existing proprietary controller.", "I have the feeling that it\u2019s still quite hard to get started on using ROS for an industrial application, because of many factors, not only involving the complexity of the ROS platform.", "What\u2019s missing from ROS at the moment are following pieces. I\u2019ll also add my ideas on how to solve them.", "The opportunity I see here is to build such a platform. For a client, this platform would give following benefits, compared to using a proprietary robotics controller.", "How to make money with this business?", "The goal of this project is to make using industrial robots more affordable and accessible by leveraging the power of ROS.", "Status: Idea, partially proof of concept", "I would very much appreciate your feedback if you think this is a reasonable idea and what kind of applications you see for such a platform.", "I\u2019d suggest to wait for ROS 2 and consideration of TSN in DDS. ", " could be interesting for you. From the papers conclusion:", "\u201cFor  higher  layers,  we foresee a contending landscape where the integration of TSN", "\nin different middleware solutions focused on interoperability", "\nsuch  as  OPC-UA  and  DDS  promise  to  deliver  a  bottom-up", "\nreal-time communication solution.\u201d", "I love the idea of an end to end open robot controller. The nature of my work (contract research at SwRI) means that custom hardware is almost never allowed for safety, cost, and support reasons. I think you will find a lot of folks here that have similar restrictions.", "What a lot of us ", " collaborate on is the user facing end of your proposed stack: the programming language, virtual pendant, and communication interface between PC & hardware. I\u2019m just musing now, but recently we\u2019ve seen more robot vendors provide a \u201cdumb servo\u201d interface to their hardware. We should have (but do not) the basic software in place to easily replicate what robot controllers can do now: jog, point to point move, cartesian move. Save paths, load paths.", "If we have our own controller, that is ROS based, it almost eliminates the need for a robot command language.  However, I do think there is value in providing a programming interface that is more familiar to industrial robot programmers.  We are actively developing something called ", ", that attempts to simplify programming, while still benefiting from power of ROS.", "Additional must-haves:", "How to make money with this business?", "This is by far the harder question.  An open source controller still requires a robot.  Not everyone is going to build a custom robot (or not enough to make it a worth while business).  If the controller worked with off the shelf robots, that would help, but I don\u2019t see getting the cooperation of existing robot vendors as very likely.", "Very interesting topic indeed! Thanks ", " for bringing it up and ", " for referring to our previous work (we will release before ROSCon a follow up on that!).", "We actually looked at a somewhat similar idea a while ago and have an ", " prototype that interfaces with two of the most common collaborative robots seamlessly (in our opinion, ", " is indeed right about the opportunity here). From a modularity perspective (which is what we\u2019re mostly interested in), such a concept (an Open Source (Industrial and ", ") Robotics Controller) is something very appealing but we did not obtain good input from the robot vendors we spoke with (which I guess was expected).", "We already expressed this to ", " directly but to motivate others to contribute, we from Erle Robotics wouldn\u2019t mind allocating some resources and contributing with such a project.", "E.g.: if there were to be enough interest, we could produce a limited number of our existing ", "  and distribute it to those interested parties.", "If the controller worked with off the shelf robots, that would help, but I don\u2019t see getting the cooperation of existing robot vendors as very likely.", "I wouldn\u2019t be so pessimistic about that. If they see it as a way to sell more robots, they might be more interested. You need to approach them with the idea that it will expand the market for their robots, rather than as something to cannibalise their controller business. I\u2019ve seen it work before.", "Very interesting topic indeed! Thanks ", " for bringing it up and ", " for referring to our previous work (we will release before ROSCon a follow up on that!).", "You are welcome. I totally agree with the papers statement and appreciate the effort invested into modularization concepts HRIM, H-ROS, etc as well. I am looking forward to the follow up.", "First: I\u2019d love to have a full open source stack! Specifically the part where you just want to have a robot do some things.", "For this idea to succeed (you want to make money doing this too), you need to know your targets. Will that be companies? A robot manufacturer (startup?) looking to offload support to a community? A manufacturing company which makes products? Why would such a company decide to use another platform if they already have spent serious money on a robot brand, including educating staff and writing programs (sometimes processes are signed of by their customer, which will make it even less interesting to switch)?", "In my opinion, in industry, users need an easy way of programming so they \u201cget things done\u201d. These users don\u2019t go designing robots, they don\u2019t have the knowledge/time/money, they don\u2019t go tinkering, they don\u2019t have the time/knowlege/task to dive into writing nodes and worrying about dds, realtime etcetera. They have a task and want to use a tool to solve a problem.", "\nCompanies themselves struggle to hire staff who can perform these (for the ROS community simple) tasks. On a normal level, coming from school. These men and women do not have a university degree.", "I think that one of the big missing things in a full stack is the programming language and easy programming for a factory user. All your points about the hardware side are doable.", "If you want to have success:", "How to make money with this business?", " I was thinking about something end-user facing for the programming language, like a turnkey solution that works similar to what typical industrial robot controllers offer. But maybe I\u2019m thinking to much \u201cin-the-box\u201d on this topic.", "\nAnd yes, support for off-the-shelf robots is a necessity in my opinion, even if you don\u2019t get the support of the vendors themselves, the vendors provide ROS support, so it shouldn\u2019t be too hard to add support for their robots. But it would be of course better if the vendors collaborate, selling more robots, for additional user groups, could be an opportunity for them as well.", " Thank you for contacting me. I\u2019m very interested in hearing and seeing more at ROSCon.", " I agree.", "RT support of the middleware is definitely helpful, but I don\u2019t think it\u2019s a minimum requirement for such a controller. As pointed out, Machinekit or any other motion control layer needs to be RT capable, but the middleware itself could also operate in userland.", "The RT support of the middleware, however, is necessary when we are talking about distributed motor control. Which is something ", " is targeting for example.", "I agree. Supporting closed source robot hardware would be possible, even without the vendors actively helping with the integration.", "On the other hand, vendors could be interested in offering ROS support not only as a software package but also on the controllers they sell. Just musing.", "Dear ", ",", "thank you for your post . And thank you ", ", ", ", ", ", ", ", ", " & ", " for adding valuable answers to this thread.", "And to answer your introductory questions: Yes, ROS Discourse is the right forum to discuss this ", "From a point of view of ROS-Industrial Consortia (in the Americas, Europe and Asia-Pacific) you mention a lot of issues that we 've looked at it in the past, and as my previous speakers/posters have already expressed: some of it is tempting, most of it could be possible, and all have been on the radar (which does not make it less right or wrong).", "As you remember from our last collaborations (thanks again for your support at sprint tech workshop & ", " at IPA), the devil is in the details (e.g. see comments from ", " like: safety, cost, and support reasons). Therefore, we use official industrial robot controllers.", "If you want to challenge this (and think, you can find potential customers that don\u2019t have that restrictions), I personally would encourage you and others to further investigate in developing a fully Open Source Industrial Robotics Controller. In that respect: great offer, ", ", for offering to allocate some resources and contributing with such a project.", "So far there was not a sufficient alignment of the ", " resources involved in such effort (think about the safety certification costs alone) to pull this off. If anybody has more links, please add them to this post/thread.", "As you and ", " are both based in Europe, you could even apply for some EU funding for a Focused Technical Projects (", ") via the ROSIN project (", ") and give it a try.", "Good luck!", ", ", ", ", ", and Thilo", "If people coming from school knowing this programming language and if you\u2019re lucky some hardware/software concepts, you\u2019ll also solve the problem of companies trying to find educated personnel. Might be an opportunity for the ROSIN project and education.", "As you and ", " are both based in Europe, you could even apply for some EU funding for a Focused Technical Projects (", ") via the ROSIN project (", ") and give it a try.", "+1 for this. Thanks ", " and ", ". After the reactions, we are considering doing so. I\u2019m thinking more in the ROSin FTP direction though. Although the percentage funded is lower, it certainly (at least it does to me) matches a bit better the objective of a potential project (maybe someone could argue this?).", "Creating a small consortium where each partner had access to a different industrial robot and could contribute with their corresponding abstractions for a common Open Robotics Controller (ORC?, ", ")  may make sense.", "If there\u2019s anyone interested, please say so.", "Very interesting idea indeed. I like the suggestion of ThiloZimmermann for a EU funding. So, If ", " and ", " are considering to apply it. Can I propose to joint this proposal as an academy collaborator? Thanks.", "Hello ", ",", "Can I propose to joint this proposal as an academy collaborator? Thanks.", "Thanks for your interest. That\u2019s certainly very feasible. The current status of the project proposal is as follows:", "Currently, the group is composed by:", "We will need a bit more of information about you and your group but feel free to write me if you feel you could contribute. Please specify which robot targets you\u2019d be willing to use for the project.", "Hi ", " ", " I\u2019ve been working on and off on an OS robot (hardware, so that dimensions can be adapted for each case), running on Machinekit called ", " Currently I\u2019m working towards changing the design from prototype for use in practical colleges in NL. I too would like to enquire how can collaborate this project.", "Hi ", " ", " I\u2019ve been working on and off on an OS robot (hardware, so that dimensions can be adapted for each case), running on Machinekit called ", " Currently I\u2019m working towards changing the design from prototype for use in practical colleges in NL. I too would like to enquire how can collaborate this project.", "Pretty interesting! Why don\u2019t you write me directly at \u201cvictor at ", "\u201d and describe a bit more the contributions you have in mind, the entity you belong to and the robot targets you have in mind?", "Thanks!", "Hi ", ",", "It looks an interesting initiative!", " is willing to collaborate in this project. We can contribute with our mobile robots!", "We\u2019ll write you directly.", "Thank\u2019s for your idea, I really agree!", "\nThe good news is: There is an industrial controller (controller + IO-system + Linux + Real-time + \u2026) available which supports ROS.", "\nPlease send a short e-mail for more information (", ") or have a look a this ", "Very nice and exciting topic! I would love to make a robot and understand everything about how it works ", "From my experience the only thing that makes a robot industrial is the fact that companies are able to buy it and legally use it (certifications / safety). It\u2019s not about weight, payload, speed, range, being collaborative or even how the maintenance will be done, these are to be discussed after a certification is issued.", "The dream robot for me would be:", "I think there is a real marker opportunity in making cheap capable robots, robots that do not cost much (mechanics, electronics) but have nice software inside, which would easily allow tasks that known industrial robots struggle with (vision etc.).", "For every robot that I have used so far the problem was always in the software:", "I think one big barrier is the mechanics cost: making a decent robot is complicated and expensive.", "\nThere is already a lot of open source robots that can be 3D printed, however most of them use stepper motors or servomotors which may not be the best if scaling up the robot. It could be a great start using these projects with AC or DC servos instead of servomotors.", "I can help for:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["a robot command/programming language, equivalent to what you have for industrial robots - that\u2019s only necessary if you want to skip the vendor\u2019s controller", "An easy to use online programming HMI, well, that\u2019s doable.", "A low-level real-time capable platform to drive the robot if you are not interested in using the vendors\u2019 controller. ", " work for this.", "A reference hardware platform. I know applications are diverse, but having a common ground to start from just makes things a lot easier. An RT Linux capable platform, preferably with low-level IO access, which is powerful enough to support CV and machine learning applications should do. Turns out the ", " is a good candidate.", "No vendor lock-in.", "Easy and intuitive programming, thanks to the great HMI and teaching capabilities.", "Leverage the whole ROS eco-system for your application.", "Rebrand the product for your application.", "A reference platforms makes it easy to get started, more focus on your core business.", "Sell the hardware/software as a turnkey solution.", "Provide consulting services around ROS, robotics hardware and so on.", "Use dual licensing for commercial and open source applications.", "Provide paid premium support for the platform.", "a robot command/programming language, equivalent to what you have for industrial robots - that\u2019s only necessary if you want to skip the vendor\u2019s controller", "Trajectory generation, planning, and smoothing at the joint level is also missing.  This would have to be added to the low level controller in order to achieve industrial robot level motion.", "Collaborative functionality.  Might as well jump into this growing market, rather than focus on legacy applications.", "you need ", " that companies can buy, proving they can get going. They are mostly interested knowing if the tool can do the job, and what it yields them. Managers mostly decide about purchasing these tools. Concepts as ROS, Open Source or vendor lock-in are second round considerations.", "staff must be able to work with your stack solving a problem, They will help leverage the decision if they tell their Manager that they are able to work with the tools. Possibly even tried out a demo without hardware.", "If people coming from school knowing this programming language and if you\u2019re lucky some hardware/software concepts, you\u2019ll also solve the problem of companies trying to find educated personnel. Might be an opportunity for the ROSIN project and education.", "do not underestimate developing and selling a product, and providing support if you\u2019re going to provide hardware/software as turnkey solution.", "consultancy would be where you have a starting point.", "Open source hardware\n", "Mechanics (dimensioning, CAD / drawings, most interchangeable parts as possible)", "Electronics (CAD / BOM)", "\n", "Open source software\n", "ROS native", "Access to the closed-loop control (monitoring, configuring etc.)", "\n", "Certified", "Not being able to run tasks concurrently", "Robot not following the setpoint (not because of the mechanics)", "Missing (paid) options", "Low memory", "ROS driver not exposing all controller functions", "And the list goes on\u2026", "The mechanics", "The software (from real time to GUI)"], "url": "https://discourse.ros.org/t/fully-open-source-industrial-robotics-controller/5832"},
{"title": "TurtleBot3 software and firmware update and 'waffle_pi'", "thread_contents": ["Hello everyone ", "I announce that TurtleBot3 is huge updated!!!", "\nThis update considered many issues and requests from users. We are sincerely thankful to them.", "\nMore interest makes more progress. If you have any issues or suggestions, please feel free to get ", "Existing users need to download new version in master branch of turtlebot3 and turtlebot3_msgs", "Direct download in repository ", ", ", "or using command line", " sudo rm -rf turtlebot3/", "\n$ git clone ", " sudo rm -rf turtlebot3_msgs/", "\n$ git clone ", "Open Arduino -> Toos -> Board: -> Board Manager\u2026 -> Update (v 1.0.15)", "Best regards!", "\nDarby", "I assume the Waffle Pi is a lower-cost waffle with a Raspberry Pi instead of the Joule. This is great news!", "Is there any progress on a Burger or Waffle version that comes with no sensors or compute boards, for those of us who have all that stuff already?", "Hi ", " thanks alot for the update.", "\ni have a question\u2026 Do we need to update the OpenCr board? and How? i get \u201cChecksum does not match\u201d error . thanks alot\u2026 ", "Hello ", "You can follow below instruction", "or you can find more detail in ", "Thanks", "\nDarby", "I am interested in using the new software update on my TB3 and thank you for these instructions. Are there also updates planned or available to the documentation to describe the ROS commands necessary to show the new Publisher /diagnostic, /battery_state information and to connect a speaker to the RasPi3 or OpenCR to hear the generated sounds? Ross", "Hello ", " ", "We have a plan to be compatible with ROS2 though we don\u2019t have any details.", "\nWe provide a ", " and it will be added more information soon for these software updates.", "\nAdditionally, you don\u2019t need to connect separate speaker due to OpenCR already has it ", "Thanks", "\nDarby", "Darby, Following your good instructions and compiling the revised catkin_ws/src, I easily updated the ROS SBC TB3 and OpenCR software and very pleased that it all works well, including \u201crostopic echo /diagnostics\u201d node that is very informative. I look forward to the new wiki to learn how to connect the new /sound Subscription that roswtf reports as unconnected. I note there is a reference to a Adafruit display driver in the TB3 github-Is that a future additional feature? A small request is make available the pdf version of the previous wiki.Thank you and your colleagues for their hard work and producing an excellent, fun and educational robot :).", "I\u2019m still getting a checksum mismatch. I upgraded to OpenCR 1.0.15 and also tried 1.0.16. I upgraded with turtlebot3.git and on turtlebot3_msg.git on both my RemotePC and TurtleBot.", "This is the execution", "\n<", "\n$ roslaunch turtlebot3_bringup turtlebot3_core.launch", "\n\u2026 logging to /home/eepp/.ros/log/5a7d18a4-0545-11e8-9b6b-080027c0cb1e/roslaun", "\nch-orras-3438.log", "\nChecking log directory for disk usage. This may take awhile.", "\nPress Ctrl-C to interrupt", "\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS", "NODES", "ROS_MASTER_URI=http://10.0.0.159:11311", "process[turtlebot3_core-1]: started with pid [3447]", "\n[INFO] [1517266129.977476]: ROS Serial Python Node", "\n[INFO] [1517266130.051262]: Connecting to /dev/ttyACM0 at 115200 baud", "\n[ERROR] [1517266132.304481]: Creation of publisher failed: Checksum does not ma", "\ntch: 427f77f85da38bc1aa3f65ffb673c94c,d537ed7b8d95065b6c83830430b93911", "\n[INFO] [1517266132.362502]: Note: publish buffer size is 1024 bytes", "\n\u2026", "\n/>", "Nice! we are preparing updated wiki including your requests.", "\nThanks you for your interest ", "Hello ", " ", "Have you set network config??", "\nYou can check how to config network btw RemotePC and TB3 on ", "Best regards", "\nDarby", "ROS guys ", "Thank you for your interest on TB3 ! (I am happy as if i get a sweet coffee)", "\nBut ", " is not proper page to create issue ", "If you have any question for TB3, please use Github ", " or ", ".", "\nYou can meet me in there ", "Thanks", "\nDarby", "Darby,", "Thanks for your quick response. This is how I set it up", "Remote PC", "$ ifconfig", "\nenp0s3    Link encap:Ethernet  HWaddr 08:00:27:c0:cb:1e", "\ninet addr:10.0.0.159  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.159", "\nexport PATH=$PATH:$HOME/tools/arduino-1.8.5", "turtlebot3", "$ ifconfig", "\nwlp1s0    Link encap:Ethernet  HWaddr a0:c5:89:4b:3a:e3", "\ninet addr:10.0.0.158  Bcast:10.0.0.255  Mask:255.255.255.0", "$ tail .bashrc", "\nexport ROS_MASTER_URI=http://10.0.0.159:11311", "\nexport ROS_HOSTNAME=10.0.0.158", "\nexport TURTLEBOT3_MODEL=waffle", "Ed", "I followed the instructions in 7.1.5  for Porting OpenCR1.0 to Arduino IDE. I want to reason through my issue. I\u2019m not familiar with Arduino checksum generation. I connected my OpenCR to my Remote PC. I ran the Arduino IDE from there and loaded the bootloader. So I assume I can load the OpenCR from any machine as long as I have the right bootloader version. Where and how are the checksums being generated and compared? Will I have the same bootloader checksum as everyone else in the world? If that is correct, someone should be able to tell me if I have the correct one and tell me which side is incorrect. If not correct, does the checksum depend on my hardware and/or software.", "ed", "Hi Darby", "Just wanted to confirm when you pointed to this article for updating, you meant: ", "Is it possible to update the board- manager as defined in the wiki using dfu-util?", "Or you meant some other steps such as: ", "Can you please confirm?", "Thanks", "\nSandip", "Hi ", " ", "You don\u2019t need to enter dfu mode.", "\nIf you have used OpenCR, you just update it using ", ".", "Please follow below link, it will help you", "Thanks", "\nDarby", "Thanks Darby for the reply,", "\nI updated the Board Manager on Arduino IDE to 1.0.15 (also tried 1.0.16) --> the tried updating the Bootloader using Arduino IDE (but failed) with the following error message:", "I made sure the", "lsusb", "is showing the STMicroelectronics (NOTE: There is no DFU mode written as you mention in the emanual - but this entry in lsusb starts showing up only after DFU mode triggered on using Boot + Reset button)", "Can you let me know some tips to go ahead with debugging?", "Thanks", "\nSandip", "Hi ", ",", "This is a space for discussion, so the question is not appropriate.", "\nI will continue on the issue page of the link below.", "\n", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "\n", "Thanks!", "Exactly same Checksum error here - before I start digging\u2026was it solved? any ideas? Tnx Michael", "BIG SORRY - my fault\u2026didn\u00b4t read the error messages after uploading correctly\u2026no issue anymore!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["turtlebot3_controller - include RC100(for remote control) library", "turtlebot3_diagnosis - include diagnostic functions", "turtlebot3_motor_driver - include DYNAMIXEL SDK", "turtlebot3_sensor - include functions for IMU, battery, magnetic filed and analog Input", "/version_info - Contains the hardware, firmware and software information", "/battery_state - Contains battery voltage and status", "/magnetic_field - Contains magnetic field information", "/diagnostic - Contains self diagnostic information", "/sound - Output beep sound", "/motor_power - Dynamixel torque on/off", "/reset - Reset odometry and re-calibration IMU", "add Sound.msg", "Simple command makes USB setup", "It shows state of IMU, motor, lidar, battery, button and version information", "Add Waffle PI", "Now, we are preparing new version of TurtleBot3 called ", ".  Meet Waffle PI in ", " before you get an it.", "Software (v 1.0.0)", "Firmware", "I did a roscore on my RemotePC", "a ssh into the TurtleBot", "did a \u201croslaunch turtlebot3_bringup turtlebot3_core.launch\u201d on the TurtleBot", "/rosdistro: kinetic", "/rosversion: 1.12.12", "/turtlebot3_core/baud: 115200", "/turtlebot3_core/port: /dev/ttyACM0"], "url": "https://discourse.ros.org/t/turtlebot3-software-and-firmware-update-and-waffle-pi/3729"},
{"title": "[ISO] TurtleBot Kinetic ISO - Beta Testers needed!", "thread_contents": ["After significant work refactoring our build systems for maintainability, we are pleased to announce that ", " is now available for beta testing on x86_64.", "The Intel RealSense driver ", " with 16.04.5 or later", "The ROS repository is pre-configured in ", " and most commonly used ROS packages should be pre-installed.", "The ROS environment  is pre-configured", "\n", " runs ", "ISO defaults to KHA1 Loadout in ", "Manufacturers will be able to have custom ISOs to install a loadout matching the  equipped hardware. ", " rules for these devices are preconfigured.", "Updates to 18.04 are currently disabled pending work on Melodic.", "Please submit pull  requests against ", "Did we miss installing your favorite package?", "Is this a priority for anyone?", "How much effort should we spend adding things like the rqt tools to the launcher?", "\nShould we prioritize Melodic / ROS2 support?", "Prior to ", " we built a fairly robust ", " for upstart.", "\nSystem bootup automation still needs to be  refactored for launching", "ISO adds the user to  group  dialout", "\nCan we stop setting ", " permissions for devices?", " we talked NetworkManager into helping to configure ROS networking. This needs to be updated and tested with Xenial. The current thinking is  to move  this into a pure python package, but depending  on interest this work could be delayed to Melodic.", "Please file ISO  bug reports ", "After a Kinetic release we hope to quickly produce a Melodic release followed by some sort of ROS2 release", "The TurtleBot Kinetic ISO has been sponsored in part by ", "It now ships with automatic startup and network auto-configuration", "\n", "Systemd Robot Initialization. Contribute to LucidOne/robot_systemd development by creating an account on GitHub.", "\n", "\n", "ROS Network Autoconfiguration. Contribute to LucidOne/network_autoconfig development by creating an account on GitHub.", "\n", "Uploaded!", "This release includes a basic GUI for ", "\n", "ROS Robot Status Indicator. Contribute to LucidOne/robot_indicator development by creating an account on GitHub.", "\n", "Updated ", " to support GUI for launching new systemd roslaunch units", "\nHoping for a release this week before starting on Melodic support.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Kobuki Mobile Base", "Hexagonal Plate Stack", "Orbec Astra 3D Camera", "Laptop Battery /sys/class/power_supply/BAT1", "We are planning to use an indicator like the volume control in the top right", "\nGUI to enable and disable startup", "\n", " + ", " launched by default", "\nEnables RViz to display pointclouds after first boot without opening a terminal", "ROS Startup", "\nAs ", " user?", "\nAs user ", "?", "\nWhat happens If the initial user is not named ", "?"], "url": "https://discourse.ros.org/t/iso-turtlebot-kinetic-iso-beta-testers-needed/11095"},
{"title": "ROS Quality Assurance Working Group meeting minutes Kick Off Meeting - 10/01/2018", "thread_contents": ["ROS Quality Assurance Working Group meeting minutes Kick Off Meeting", "\nTime: 9 a.m. UTC and 5 p.m. UTC", "Participants:", "\n9 a.m. UTC Group", "5 p.m. UTC Group", "Notes:", "\nROSIN quality assurance (QA) initiatives were discussed. Below is a summary of the discussion. The following problems and solutions were discussed:", " (31.0 KB)", " (15.3 KB)", " (70.1 KB)", "Problem: There is a lack of a centralized source for community quality assurance practices, knowledge, and collaboration.", "Problem: The quality of packages is not visible.", "Problem: Inconsistent practice of code review", "Problem: Recruiting maintainers is a \u201creal problem\u201d for ROS and ROS-I. This has led to an increasing number of orphan packages. This is a capacity issue within the core team. The team is struggling to attract new maintainers. The team capacity does not reflect the maintenance effort required. This is also applicable to non-core packages. There is a lack of willingness to contribute to packages\u2019 maintenance. It is a challenge to attract and retain new maintainers.", "On maintaining orphaned packages", "Recruiting more maintainers", "I recommend using the github tools for code reviews.", "GitHub is where people build software. More than 28 million people use GitHub to discover, fork, and contribute to over 85 million projects.", "Where can I get informed about future meetings of this initiative?", "I\u2019ll send you the Doole link for next meeting to your email. I have your email. I believe I sent you for the first meeting.", " ah, I see. Yes, I got a doodle inquiry, but did not respond. So probably that\u2019s why I didn\u2019t get the result\u2026", "In the future, I\u2019ll know to respond. For me personally, I would welcome getting the meeting date in any case, regardless of whether I responded or not, but I can see how other people might feel differently.", " I don\u2019t know if I\u2019m still on time to be included in the next meetings, but I\u2019m also interested and glad to help if possible.", ", you more than welcome. Just send me your email and I\u2019ll include you in the next invite.", ", I\u2019ll make sure you get all the invites.", "Can I get invite too please: ", ".", "thx, D.", "Im interested in the meeting too. ", "Cheers,", "\nJihoon", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Adam Alami", "Akshay\tJain", "Andrzej Wasowski", "Geoffrey Biggs", "Kei Okada", "Adam\tAlami", "Aaditya Saraiya", "David Bensoussan", "Dirk Thomas", "Gijs van der Hoorn", "Ian McMahon", "Luca Marchionni", "Matt Droter", "Shaun Edwards", "Victor Lopez", "\n", "\n", "Solutions:\n", "Quality Hub: Would inform about existing practices and would be a central \u201cgo-to\u201d place for QA knowledge sharing (documentation of QA practices)\n", "Discussion:\n", "Make the content of the website educational and easy to digest.", "The content should capture the knowledge most engineers do not already have.", "The website should be incorporated into the existing infrastructure (i.e., Wiki, ROS Answers).", "\n", "\n", "Quality Discourse: A dedicated QA forum\n", "Discussion:\n", "A chapter was created for Quality Assurance.", "\n", "\n", "\n", "\n", "\n", "\n", "Solution:\n", "Make ROS packages\u2019 quality visible.", "\n1. Discussion:\n", "A \u201cQuality Stamp\u201d was suggested. We can use a script (leverage existing Github feature) to generate the stamp.", "Enforce the stamp creation in the distribution process.", "\n", "\n", "\n", "\n", "\n", "Solution:\n", "Energize the code review process.\n", "Discussion:\n", "It was recommended to use the combination of a tool and peer review.", "It was suggested to create a website (i.e., similar to ", ") dedicated to code review.", "Motivation was discussed. What would motivate community members to do code review? A reward system similar to the \u201cKarma\u201d system was discussed.", "Review and update the current standards.", "Possibly provide tutorials on how to review a pull request.", "\n", "\n", "\n", "\n", "\n", "\n", "Solutions:\n", "Propose and implement a funding model for the maintenance activities.", "Organize periodic campaigns to recruit new maintainers for both core and non-core packages.", "Define an onboarding process for both core and non-core community members.", "Document the onboarding process, including online educational materials (i.e., tutorials).", "Implement the onboarding process.", "Formalize the code ownership process.", "\n1. Discussion:\n", "Reward maintainers with Github Bounty.", "Identify a sustainability strategy.", "The possibility of using ROSIN FTPs to finance maintenance was discussed.", "\nLinks:", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-quality-assurance-working-group-meeting-minutes-kick-off-meeting-10-01-2018/3664"},
{"title": "Integration testing in ROS (2)", "thread_contents": ["We are currently working on a library of algorithms for autonomous (", ") driving based on ROS 2.", "For this library we detailed how we do unit and static analysis testing: ", " and are quite happy with it.", "What we would like to improve though is an integration testing. As a first step we consolidated (", ") and documented how integration testing is being currently done for ROS 2: ", ".", "However there are several problems with ", " described integration_test framework:", "To improve upon above we would like to propose the following improvements", "Secondly, we would also like to propose and get your thoughts on the additional types of unit and integration tests to be written.", "Fault injection tests", "\nThese tests aim at increasing the code coverage by introducing fault into the code path, in particular error handling code path which are rarely executed in normal tests.", "There are three potential places where faults can be injected in runtime:", "If fault injection test is adopted, testing code must be removed in release. Otherwise it could be utilized to perform attack.", "Random input tests", "\nROS 2 nodes are tested against independent random input data. The output, if it exists, doesn\u2019t have to be meaningful as long as the program handles it correctly. If an unexpected exception arises then it means there\u2019s a fault in the program. Random input tests are also used to avoid biased testing.", "Chaos tests", "\nIn distributed systems, chaos tests are introduced to test the system\u2019s capability of withstanding turbulent conditions in production. It\u2019s could be both hardware or software based test. It works best on systems with redundancy.", "Property based-tests", "\nAlso named as QuickCheck. Property is here defined as a ", " (", "). With property-based tests, developers don\u2019t specify the test samples. Instead they write the rule of test and tools will generate the test samples automatically and randomly.", "Example:", "\n", "Very good article about ", " in Go.", "Mutation tests", "\nCertain statements of code are changed to check if the test can find the error. This can simulate typical coding mistakes like wrong operator or variable name.", "Types of mutation tests:", "Good resource: ", "\nA C++ mutation framework: ", "I do not have experience with ROS 2, but your points regarding integration testing sound valid.", "Do above tests make sense?", "Yes, at least for the major part. I have been working on random testing and property-based testing myself, at the node/integration level, although for ROS 1.", "I had this idea of expanding mutation testing in ROS, in which the ROS primitives could be mutated themselves in a meaningful way (i.e. redirecting topics, messing with queue sizes, changing callback functions to a simple ", " or ", "), although I am not sure how useful that would be in practice.", "Is it possible to pack them all into one single framework?", "Is there any advantage to packing all these different kinds of tests into a single framework? It could prove hard to do so in a way that is intuitive for the user.", " could you provide some input here since you are currently doing integration tests for navigation stack?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The tested components cannot be started in a deterministic sequence since ", " is being used", "The framework lacks the ability of an orchestrated startup and coordinating of different components", "There is many flaky tests and it is hard to attribute flakiness to either bad tests, unreliable testing framework or CI system that does not provide guarantees.", "Move integration_test framework to roslaunch2. This should take care of the deterministic startup and state transitioning within the nodes.", "Determine whether a test needs certain guarantees (e.g. timing). In yes it should then probably run on the dedicated hardware, and if not, a cloud CI like ", " is OK. This should eliminate the concept of flaky tests. Tests should either be passing or failing.", "Add more automated debugging tools to the framework, eg., tshark for network packets capturing, perf, memory tools, valgrind and other tools for profiling.", "\n", "\n", "\n", "Data source, where the data is collected. Eg. simulated sensor failure, corrupted data or duplicate data.", "Communication. Eg. UDP packets get lost, duplicated or order of arrival are reversed.", "System hardware, eg. memory data corruption, unstable time source or memory allocation failure.", "\n", "\n", "\n", "\n", "\n", "\n", "Start the whole stack and define a \u201csteady state\u201d as normal behavior.", "Introduce some real world possible failures like disk full, power outage or network going down.", "Test if the services of other components can be uninterrupted or switched to redundancy.", "\nThe harder it is to disrupt the steady state, the more confident we are at the robustness of the system. If weakness is uncovered, now we have a concrete target to improve.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Statement mutation:", "\nCut or paste some lines of code. Most likely it wouldn\u2019t compile. This is highly manual rather than automated.", "Value mutation:", "\nValues of primary parameters are modified.", "Decision Mutation", "\nControl flow is reversed.", "\n", "\n", "Do above tests make sense?", "Are any particular flavors of tests missing?", "Is it possible to pack them all into one single framework (we struggle with this thought honestly)?", "What other use case do you have that need better integration and unit testing?", "Are there other integration testing framework from non-robotics world worth to inspect?"], "url": "https://discourse.ros.org/t/integration-testing-in-ros-2/6859"},
{"title": "[TB3] ROS 2 Dashing Release", "thread_contents": ["Hi ", "We are happy to announce TurtleBot3 ROS 2 Dashing Release.", "This updates includes", "When you visit our ", ", you can find step by step from installation to launch tele-operation, cartographer and navigation2  ", "If you have any issue or questions, please feel free to get new ticket on github issue page.", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "We specially thanks to OpenRobotics Memeber,  every ", " developer and ROS community,", "\n", " at ", " , ", ", ", ", ", ",  ", " at ", ", ", " at ", " and My colleague ", " and ", " at ", ".", "Great stuff, thank you very much!", "Great, many thanks. ", "\nI will have to try working with Jetson Nano.", "Here\u2019s video\u2019s about Cartographer and Navigation2 ", "[ROS 2 Dashing Diademata Cartographer]", "[ROS 2 Dashing Diademata Navigation2]", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["OpenCR(Embedded Board) communicate with ", " by ", "\n(For more detailed, please refer to attached picture below)", "\n", " package was ported to ROS 2 Dashing", "Added some services (/sound, /motor_power, /reset)", "Added some parameters", "Applied message filter to calculate ", " topic"], "url": "https://discourse.ros.org/t/tb3-ros-2-dashing-release/10364"},
{"title": "ROS 2 Dashing Diademata Released!", "thread_contents": ["We\u2019re happy to announce the ROS 2 release Dashing Diademata!", "We\u2019re especially excited to let you know that Dashing Diademata is the first long(er)-term support (LTS) release for ROS 2. After several years of development, and following a big boost in productivity over the past half year from new contributors, including the ", ", we\u2019ve reached a level of maturity with ROS 2 such that we\u2019re extending the support period for Dashing to be two years, through May 2021.", "So whether you\u2019re looking for a platform on which to build a new application, or planning to migrate an existing ROS 1 system, Dashing should be your starting point. Over the coming two years, we\u2019ll be providing patches for Dashing. While we can\u2019t guarantee API compatibility between ROS distributions, for the updates to Dashing we aim to maintain API and ABI stability. This matches what we\u2019ve done in the past with ROS 1 LTS distributions.", "To get an idea of what\u2019s in this release and how to update existing code from ROS 2 Crystal, be sure to read the ", ".", "Here are a few features and improvements we would like to highlight in this release:", "We\u2019re looking forward to getting your ", " and ", ", and to hearing about your new applications based on Dashing! If you have demonstrations of Dashing from your own work that you can share, feel free to post in this thread.", "We also invite you to ", " in Dashing! A huge thanks to all those who\u2019ve already participated in our ", ".", "And finally the name of the next ROS 2 release scheduled for November 2019 will be:", "Your friendly ROS 2 Team", "P.S. Show your color and get a dashing ", ".", "Congratulations for this milestone! ", " would like to thank everyone in the community and specially the folks from Open Robotics who took part into this release and helped putting it together.", "It\u2019s been about 5 years contributing to ROS 2 for some of us here at Acutronic. This LTS release makes us specially proud of what\u2019s been achieved. Our latest contributions are summarized in ", ". We\u2019d like to celebrate this launch by sharing with you some of the demos we\u2019ve been putting together over the last period. All of them based in ROS 2 and powered by ", ", which uses ROS 2 native hardware. Enjoy!", "Here we present the demonstration of a sensorless collision detection system for the MARA modular robot using some of the moveit_core submodules of MoveIt 2. The whole system is based in ROS 2 and has been tested using the Dashing Diademata pre-release while leveraging the real-time capabilities that our team is developing as part of the ", " communication bus for robots.", "For more on this, refer to ", ".", "We present the first demonstrator of the capabilities of MoveIt 2 by showing how to plan to a joint-space goal and how to reproduce it with Dashing. Refer to ", " for the alpha release of MoveIt 2.", "Read more in ", ".", "Often researchers of AI benchmarking formal methods vs sub-symbolic ones find hurdles when it comes to the robot interfaces (real vs simulated). Here we demonstrate how through our contributions (", " framework and ", " toolkit), the transfer of a learned policy from simulation to a robot has become easier. The video shows how we can replicate the behavior demonstrated in simulation accurately using a real robot and with the same ROS 2-powered interfaces.", "Get to know more about RL in ROS 2 ", ".", "Getting different robots to coordinate together precisely is one of the challenges of system integration. This becomes specially tedious when each robot uses proprietary interfaces. A complete ", ". We demonstrate here how 5 robots running ROS 2 natively  are controlled with a single computer achieving millisecond-level precision, while the network is being challenged with simulated depth sensors. Through ROS 2, the H-ROS robot bus provides real-time and synchronization allowing to control simultaneously 30 (5x robots, 3x 2DoF joints each) joints.", "Read more in ", ".", "Synchronization and repeatability are essential for industrial robots to be reliable. These MARA modular arms coordinate precisely thanks to the ", " which empowered by ROS 2 is able to provide sub-microsecond and distributed synchronization.", "Read more in ", ".", "We look forward towards what ROS 2 is bringing to the overall robotics ecosystem and hope to continue contributing. Cheers for Dashing release and cheers for ROS 2!", "Congratulations on the LTS release! Thank you very much to all contributors and to the team at Open Robotics!", " uses the ROS 2 stack to bridge the gap between powerful microprocessors and embedded microcontrollers (MCU). The two major goals:", "Due to the well-designed abstractions in the ROS 2 stack, the rmw and rcl layers may be basically used unchanged on MCUs. On the middleware level, the upcoming DDS-XRCE standard allows the communication from and with MCUs at only a few tens of kilobytes of RAM.", "Although micro-ROS started as a joint endeavor by five companies/institutions ", ", ", ", ", ", ", ", and ", " in the context of a European project, we strive to incorporate the ROS community as early as possible. Join the ", " to learn more!", "As a first community use-case, we brought micro-ROS to an STM32 F4 and created a tiny demo using the Kobuki Turtlebot 2 with it.", "Check it out at ", ".", "          ", "Further use-cases with a modular manipulator, a lawnmower robot, integration of a drone autopilot, and robot operating in smart warehouse will be developed and demonstrated in the next 18 months.", "We have also prepared another small demonstration. In this case, we show a ", ".", "\nCheck the next video to see how it works:", "          ", "Congratulations on the release! Thank you to Open Robotics for their continued work on coordinating all the ROS2 efforts.", "LG Electronics has been building ", ", an autonomous vehicle simulator that is compatible with ROS2. Our simulator is Unity-based and features photorealistic environments, high-performance sensors, and a Python API to control non-ego vehicles, objects, and configurations. Through our use of ros2-web-bridge, anyone can connect their ROS2-based autonomous driving (AD) stack to our tool to test and speed up development. The simulator provides sensor input to the ROS2-based AD stack, and the AD stack provides control commands back to the simulator.", "One way we have been using LGSVL Simulator is for deep learning training. Here we show a video of a lane following test run after training a deep learning model in the LGSVL Simulator. ROS2 is used to run a simplified autonomous driving stack that publishes steering and throttle commands to the vehicle inside the simulator, taking only images from the camera inside the virtual vehicle as input. The end-to-end lane following model was trained in various environmental conditions within the simulator, and is robust to weather, time-of-day, and visibility conditions.", "If you\u2019re interested in getting started with machine learning research using ROS2, the full documentation and guide to this project have been posted ", ", and you can see the code and trained model on ", ".", "We also have a tutorial on getting started with ", " with LGSVL Simulator. Thanks!", "Nice! Is it working faster than with ROS1 bridge?", "Congratulations to the ROS community and to ", " on reaching this new milestone!", "Here at Apex.AI we are pretty excited about this release and wrote this ", " to celebrate.", "As noted in the Blog post, ", " and ", " have now contributed a ROS 2 based 3D perception library for automotive applications to Autoware.Auto, which is shown in this short video.", "For anyone who does not know what we do, in brief, ", " is building ", ", which is API-compatible to ROS 2, runs in hard real-time and is being certified to the highest level of the automotive safety norm ISO 26262 (ASIL-D).", "In case anyone needs some advice with transitioning from ROS 1 to ROS 2, we previously also wrote this detailed Blog post describing ", ".", "Congratulations to Open Robotics on hitting this big milestone. Here at ", " (", ") we are excited to announce our new series of demos that we are creating in conjunction with ", " and ", " to showcase ROS 2 running on industrial-grade, reliable hardware. Our robots were originally designed for SWAT teams so they are built to be really rugged! We have over 10 years of experience in fielding reliable robots and we are excited to show the world what reliable hardware can do when you have reliable software backing it.", "Our demo series will include how to setup an use common packages in ROS 2 such as Google Cartographer, AMCL, RVIZ, and Gazebo. We will be contributing to the sensor drivers needed for these demos to ensure they are reliable for all to use, and lastly we will be hooking this all up to AWS RoboMaker for a professional workflow, professional-grade security and a reliable back-end. Our first full write-up will be released on June 30th.", "\nVisit our website to checkout more about their features and specifications.", "\n", "Cool project!", "I have a working example for publishing and subscribing directly to ROS2 (crystal) from Unity using a custom C# rcl client library and message generator. This approach should have significantly better performance than rosbridge.", "Contribute to DynoRobotics/unity_ros2 development by creating an account on GitHub.", "We\u2019re all excited about Dashing Diademata!", "btw, the ", " logo gets cropped by social media because the image is 718x1000 (portrait). Social media (twitter, linkedin, et al.) need something between square and landscape. So poor Dashing keeps getting his head lopped off", "\n", "Arghh! \u2026 almost, needs to be more landscape ", "\n", " I roughly measured ratio from your LinkedIn screenshot and resized width of canvas of your file according to it:", "\nHere\u2019s a file for sharing: ", "\n(edit: Tested with LinkedIn and Twitter, updated the file)", " is very excited about ROS 2 dashing! We\u2019ve been working with ROS 2 since its inception. Dashing is supported in ADLINK ", "), ", " & ", " used in AVs/industrial/military, and the small & fast ", "\u2019s ", " ", " to which we contribute. Here\u2019s ADLINK ROS 2 dashing demo from IEEE ROS Summit last month with ", "\n", "Here\u2019s more from last month\u2019s ", " put together with the help of many. You\u2019ll notice familiar faces including ", ", also ROS 2 dashing demos. 600 developers & roboticists made it \u2026 the biggest ROS 2 event ever?", "\n", "We are happy to announce TurtleBot3 ROS 2 Dashing Release.", "This updates includes", "For more detailed, please refer to this ", "ROS packages for Turtlebot3. Contribute to ROBOTIS-GIT/turtlebot3 development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " are now the recommended way to write your node. They can be used standalone as well as being composed within a process and both ways are fully support from ", " files.", "The ", " (C++ only) has been improved - both in terms of latency as well as minimizing copies.", "The Python client library has been updated to match most of the C++ equivalent and some important bug fixes and improvements have landed related to memory usage and performance.", "Parameters are now a complete alternative to ", " from ROS 1 including constraints like ranges or being read-only.", "By relying on (a subset of) ", " for the message generation pipeline it is now possible to use ", " files (beside ", " / ", " / ", " files). This change comes with support for optional UTF-8 encoding for ordinary strings as well as UTF-16 encoded multi-byte string.", "Command line tools related to ", " and ", ".", "Support for Deadline, Lifespan & Liveliness QoS", "MoveIt 2.0 ", "\n", "\n", " as Tier 3 supported platform", "Integrate the different types of computing platforms seamlessly", "Ease the portability of ROS code to microcontrollers", "OpenCR(Embedded Board) communicate with ", " by ", "\n(For more detailed, please refer to attached picture below)", "\n", " package was ported to ROS 2 Dashing", "Added some services (/sound, /motor_power, /reset)", "Added some parameters", "Applied message filter to calculate ", " topic"], "url": "https://discourse.ros.org/t/ros-2-dashing-diademata-released/9365"},
{"title": "New packages for Melodic 2019-08-14", "thread_contents": ["We\u2019re happy to announce the next update for ROS Melodic. We have 121 new packages as well as 66 updated packages.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 0.6.14-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "\n", ": 0.1.6-1", "ros-melodic-cob-base-controller-utils: 0.8.0-1", "\n", ": 0.7.0-1", "\n", ": 0.8.0-1", "ros-melodic-cob-bms-driver: 0.7.0-1", "\n", ": 0.6.12-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "ros-melodic-cob-cartesian-controller: 0.8.0-1", "\n", ": 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.8.0-1", "ros-melodic-cob-control-mode-adapter: 0.8.0-1", "ros-melodic-cob-control-msgs: 0.8.0-1", "\n", ": 0.6.14-1", "ros-melodic-cob-docker-control: 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-elmo-homing: 0.7.0-1", "\n", ": 0.6.13-1", "\n", ": 0.8.0-1", "ros-melodic-cob-frame-tracker: 0.8.0-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.0-1", "\n", ": 0.6.6-1", "ros-melodic-cob-hand-bridge: 0.6.6-1", "ros-melodic-cob-helper-tools: 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-model-identifier: 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.8-1", "\n", ": 0.6.14-1", "ros-melodic-cob-object-detection-visualizer: 0.6.14-1", "ros-melodic-cob-obstacle-distance: 0.8.0-1", "ros-melodic-cob-omni-drive-controller: 0.8.0-1", "\n", ": 0.6.14-1", "\n", ": 0.6.14-1", "ros-melodic-cob-phidget-em-state: 0.7.0-1", "ros-melodic-cob-phidget-power-state: 0.7.0-1", "\n", ": 0.7.0-1", "ros-melodic-cob-reflector-referencing: 0.6.8-1", "\n", ": 0.7.0-1", "ros-melodic-cob-safety-controller: 0.6.8-1", "ros-melodic-cob-scan-unifier: 0.7.0-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.6.8-1", "\n", ": 0.6.12-1", "\n", ": 0.6.14-1", "\n", ": 0.8.0-1", "ros-melodic-cob-tricycle-controller: 0.8.0-1", "ros-melodic-cob-twist-controller: 0.8.0-1", "\n", ": 0.7.0-1", "\n", ": 0.7.0-1", "\n", ": 0.6.14-1", "\n", ": 0.7.0-1", "ros-melodic-distance-map: 0.1.0-1", "ros-melodic-distance-map-core: 0.1.0-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-distance-map-node: 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-distance-map-rviz: 0.1.0-1", "ros-melodic-distance-map-tools: 0.1.0-1", "ros-melodic-generic-throttle: 0.6.14-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "ros-melodic-ipa-3d-fov-visualization: 0.6.14-1", "\n", ": 0.3.2-1", "\n", ": 0.3.0-1", "\n", ": 0.3.0-1", "ros-melodic-jackal-viz: 0.3.2-1", "ros-melodic-laser-scan-densifier: 0.7.0-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.6.13-1", "\n", ": 0.1.0-1", "\n", ": 0.0.3-1", "\n", ": 1.2.0-0", "\n", ": 0.6.13-1", "\n", ": 0.1.1-1", "\n", ": 0.1.0-1", "ros-melodic-ridgeback-gazebo-plugins: 0.1.0-1", "\n", ": 0.1.0-1", "ros-melodic-ridgeback-viz: 0.1.1-1", "\n", ": 1.1.4-1", "ros-melodic-rosparam-handler: 0.1.4-1", "ros-melodic-service-tools: 0.6.14-1", "\n", ": 0.0.1-1", "\n", ": 0.2.0-1", "\n", ": 0.2.0-1", "ros-melodic-warthog-viz: 0.0.1-1", "\n", ": 1.11.13-0 -> 1.12.0-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "ros-melodic-cob-msgs: 0.6.10-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "ros-melodic-fetch-bringup: 0.8.6-0 -> 0.8.7-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.6-0 -> 0.8.7-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "\n", ": 0.8.1-0 -> 0.8.2-1", "\n", ": 0.9.1-0 -> 0.9.2-1", "ros-melodic-freight-bringup: 0.8.6-0 -> 0.8.7-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 1.1.2-1 -> 1.1.3-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 2.0.0-0 -> 2.0.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 2019.7.7-1 -> 2019.8.8-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 0.32.0-1 -> 0.32.1-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 0.6.10-0 -> 0.7.0-1", "ros-melodic-rc-genicam-api: 2.2.0-1 -> 2.2.2-1", "\n", ": 1.14.0-1 -> 1.15.0-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "\n", ": 1.0.0-0 -> 1.0.1-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "ros-melodic-rosbridge-msgs: 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.11.2-1 -> 0.11.3-1", "\n", ": 0.5.8-0 -> 0.5.9-1", "\n", ": 1.10.16-1 -> 1.10.17-1", "ros-melodic-test-mavros: 0.32.0-1 -> 0.32.1-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.0.1-0 -> 1.0.2-1", "AWS RoboMaker", "Aaron Blasdel", "Adi Singh", "Alex Moriarty", "Alexander Bubeck", "Alexander Moriarty", "Bence Magyar", "Benjamin Maidel", "Brian Bingham", "Eric Relson", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florenz Graf", "Florian Weisshardt", "Hans-Joachim Krauch", "Jannik Abbenseth", "Jeremie Deray", "Johannes Meyer", "Joshua Hampp", "Martin Pecka", "Matthias Gruhler", "Michael Carroll", "Mike Purvis", "Paul Bovbel", "RDaneelOlivaw", "ROS Orphaned Package Maintainers", "Richard Bormann", "Russell Toris", "Tony Baltovski", "Vladimir Ermakov"], "url": "https://discourse.ros.org/t/new-packages-for-melodic-2019-08-14/10288"},
{"title": "Experiences with Distributed ROS", "thread_contents": ["I am curious to know of any experience people have had running distributed ROS in all shapes and sizes. Have you worked with any of the following solutions (or anything else?), and how did it go? What thoughts or advice can you share?", "As for my personal interest, I have been doing some work with distributed ROS and UAV, and while it is all going well on the ground side, I\u2019m having some severe spikes in latency going through a cheaper wireless router. I\u2019m specifically interested if anyone has any ideas about reliable low-latency wireless links that are easy to interface with ROS.", "The most interesting setup I used was:", "This resulted in a ca. 100ms ping time. With this, I could teleop the robot\u2019s base and arms using the on-board Kinect, so OK data rate as well.", "Lessons learned:", "So I have worked with some rather large distributed systems and best practices are rather hard to say without understanding your topology.", "I have had the best success with routed architecture. Each node master then uses a static packet structure to communicate to the higher level controller (mainly due to the radio link).", "Your spikes are more than likely caused by the inefficiencies of your packet size vs your radio\u2019s frame size. For a test run iperf across your radio link at different packets size. You should be able to see what that most efficient packet sizes are.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Static ethernet network with some kind of switch", "Dynamic ethernet network with some kind of router", "WiFi and ethernet through a router", "WiFi-Only network with a common base station / access point", "WiFi Ad-Hoc network", "Some kind of mixture of anything else?", "a mobile robot with 2 PCs. One of these ran a OpenVPN server", "wired to a router", "that router bridged to a WLAN with multiple access points", "the care home\u2019s (in which the robot worked) network was configured to port-forward to the OpenVPN server", "A control station some 60-70km away VPNed into the robot.", "Tuning a multi-access point WLAN, where the robot roams between APs, is some work. Contrary to my intuition at the time, is that you do ", " want to have each AP at a high power but instead low. That way, the robot switches to the now-closest AP faster, yielding a better connection. WiFi is designed to stay \u2018attached\u2019 to the current AP as long as it gets a signal, even though it may be weak. Dropping a low signal early is better in that case.", "Running the VPN server on the robot is not the way to go\u2026 The server should be on a public IP, the robot behind a firewall."], "url": "https://discourse.ros.org/t/experiences-with-distributed-ros/1768"},
{"title": "CIS ToF Camera Sensor ROS Driver Package Released", "thread_contents": ["Tokyo Opensource Robotics Kyokai Association (TORK) released a new ROS package \u201ccis_camera\u201d (", ").", "This ROS package is a driver package for ", " ( ", " ) ToF (Time of Flight) camera sensor DCF-RGBD1.", "DCC-RGBD1 is a small ToF camera sensor (development kit) that can acquire a wide range of depth images.", "In this package, in addition to the ROS driver for the CIS ToF camera sensor, a sample program of point cloud processing for noise removal, plane detection / removal, an object point cloud extraction and calculating the object frame position. And a launch file for 3D drawing of these processing results in RViz is included.", "Please refer to a documentation in GitHub for usage.", "If you have problems with this package, please report them on GitHub Issues.", "For inquiries about CIS ToF camera sensor hardware, please contact the following:", "For hardware inquiries: Sales Representative, CIS Corporation", "\nEmail address: ", "\nPhone number: +81-(0)42-664-5568", "That\u2019s a lot of work, but it appears to be a solid package. What\u2019s the retail price of the camera module? Would you happen to know of any North American or European distributors?", "Thanks for your comment!", "I have heard the ToF camera is at the mass production development stage.", "\nSo the price is not yet decided and the sales channel is probably under consideration too.", "Please send an Email to CIS Corporation, you will get information on how to obtain  the ToF camera in the present stage.", "Thank you for your inquiry on CIS TOF camera. We are at the very final stage of product validation, and should be able to launch the product soon. We will initially make this product available at amazon.co.jp. Thanks for your patience!", "RGB-D camera from CIS is finally on sale, available now at amazon.co.jp.", "\nIf you are overseas, please write to us directly at: ec-sales@ciscorp.co.jp", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["High accuracy depth images can be acquired in the range of 15cm to 5m.", "Small size H: 50mm \u00d7 W: 55mm \u00d7 D: 35mm (excluding protrusions)", "Simultaneous acquisition of RGB (QVGA) and Depth / IR (VGA) images", "Interface : USB 3.0 (USB 3.0 micro B connector installed: USB power supply is not supported)", "For indoor use", "GitHub Site ( Including a Quick Start guide )\n", "\n", "GitHub Document\n", "\n", "GitHub Document ( PDF )\n", "\n", "GitHub Issues\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/cis-tof-camera-sensor-ros-driver-package-released/11989"},
{"title": "New Packages for Kinetic 2019-08-09", "thread_contents": ["We\u2019re happy to announce 15 new packages and 131 updated packages for Kinetic this week. For full details please see below.", "Thank you to all the maintainers and contributors who helped make these updates possible!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Hi, I\u2019m having issue installing ros after this update. I\u2019m on ubuntu 16.04.", "When I try to apt install ros-kinetic-ros-core after using apt update it attempts to fetch python-rospkg 1.1.9 and python-rospkg-modules 1.1.9. Neither of those exist after this update so it just gives me an error about not being able to find them. I\u2019d expect apt update to cause it to search for 1.1.10, but that\u2019s not happening and I\u2019m confused as to how to fix this. I have a file in my /etc/apt/sources.list.d for ros called ros-latest.list and it\u2019s contents are,", "deb ", " xenial main", "I\u2019ve also mentioned my issue on answers.ros here, ", ", if you think that\u2019s a better place to answer.", "You\u2019re correct that ROS Answers is the correct place for your question following our ", ". Your issues with downloading rospkg via apt are not related to this announcement. And double posting it to multiple forums ends up cluttering everyones inbox.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-distance-map: 0.1.0-1", "ros-kinetic-distance-map-core: 0.1.0-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-distance-map-node: 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-distance-map-rviz: 0.1.0-1", "ros-kinetic-distance-map-tools: 0.1.0-1", "ros-kinetic-drone-wrapper: 1.0.0-1", "\n", ": 0.0.3-1", "\n", ": 1.0.0-1", "ros-kinetic-rqt-drone-teleop: 1.0.0-1", "ros-kinetic-seed-smartactuator-sdk: 0.0.3-1", "\n", ": 0.1.0-1", "\n", ": 0.1.0-1", "ros-kinetic-actionlib-enhanced: 0.0.6-1 -> 0.0.9-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-cob-base-controller-utils: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-bms-driver: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-cartesian-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-control-mode-adapter: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-control-msgs: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "ros-kinetic-cob-default-robot-behavior: 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-cob-docker-control: 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-elmo-homing: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.8-0 -> 0.6.10-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-frame-tracker: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.7.2-0 -> 0.7.3-1", "\n", ": 0.7.2-0 -> 0.7.3-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.5-0 -> 0.6.6-1", "ros-kinetic-cob-hand-bridge: 0.6.5-0 -> 0.6.6-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-helper-tools: 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-model-identifier: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "ros-kinetic-cob-moveit-config: 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-msgs: 0.6.12-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-object-detection-visualizer: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-obstacle-distance: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-omni-drive-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-phidget-em-state: 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-phidget-power-state: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-cob-reflector-referencing: 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.11-0 -> 0.7.0-1", "ros-kinetic-cob-safety-controller: 0.6.7-0 -> 0.6.8-1", "ros-kinetic-cob-scan-unifier: 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.10-0 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "\n", ": 0.6.7-0 -> 0.6.8-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.12-1 -> 0.6.14-1", "\n", ": 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-tricycle-controller: 0.7.6-1 -> 0.7.7-1", "ros-kinetic-cob-twist-controller: 0.7.6-1 -> 0.7.7-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.14-1 -> 0.7.0-1", "ros-kinetic-generic-throttle: 0.6.12-1 -> 0.6.14-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 1.1.2-1 -> 1.1.3-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "\n", ": 0.3.3-0 -> 0.3.4-1", "ros-kinetic-ipa-3d-fov-visualization: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-jderobot-assets: 0.0.1-1 -> 0.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "\n", ": 2.0.1-0 -> 2.0.2-1", "ros-kinetic-laser-scan-densifier: 0.6.14-1 -> 0.7.0-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 2.0.0-1 -> 2.0.1-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-0 -> 0.6.13-1", "\n", ": 0.6.12-1 -> 0.7.0-1", "ros-kinetic-rc-genicam-api: 2.2.0-1 -> 2.2.2-1", "\n", ": 1.0.0-3 -> 1.0.1-1", "\n", ": 0.17.6-0 -> 0.19.3-1", "ros-kinetic-rtabmap-ros: 0.17.6-0 -> 0.19.3-1", "ros-kinetic-service-tools: 0.6.12-1 -> 0.6.14-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.2.1-1 -> 1.2.2-1", "\n", ": 1.0.1-0 -> 1.0.2-1", "AWS RoboMaker", "Adi Singh", "Alexander Bubeck", "Benjamin Maidel", "Brian Bingham", "Fabrice Poirier", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florenz Graf", "Florian Weisshardt", "Jannik Abbenseth", "Jeremie Deray", "Joshua Hampp", "Mathieu Labbe", "Matthias Gruhler", "Nikhil Khedekar", "Paul Bovbel", "Richard Bormann", "Sean Hackett", "Tony Baltovski", "Yasuto Shiigi"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-08-09/10240"},
{"title": "[CFP] ACM/IEEE IPSN 2019 in CPSWeek", "thread_contents": ["The 18th ACM/IEEE The International Conference on Information Processing in Sensor Networks (IPSN)", "===================================================================", "\nApril 16-18, 2019, Montreal, Canada", "IPSN\u201919 is part of CPS-IoT Week 2019, co-located with HSCC, ICCPS, CPS-IoT, and RTAS.", "===================================================================", "The International Conference on Information Processing in Sensor Networks (IPSN) is a leading annual forum on research in networked sensing and control, broadly defined. IPSN brings together researchers from academia, industry, and government to present and discuss recent advances in both theoretical and experimental research. Its scope includes signal and image processing, information and coding theory, databases and information management, distributed algorithms, networks and protocols, wireless communications, collaborative objects and the Internet of Things, machine learning, mobile and social sensing, and embedded systems design. Of special interest are contributions at the confluence of multiple of these areas.", "Like prior years, IPSN 2019 will continue its co-location with CPS-IoT WEEK, the premier venue for research and development of Cyberphysical Systems, and its strong focus on algorithms, theory, and systems for information processing using networks of embedded, human-in-the-loop, or social sensors, as well as new hardware and software platforms, design methods, architectures, modelling, implementation, evaluation, deployment experiences, and tools for networked embedded sensor systems and the Internet of Things. Topics of interest include, but are not limited to:", "In addition to IPSN\u2019s traditional focus, IPSN 2019 will place a particular emphasis on embedded machine learning and computer vision, with a special track centered on these topics. In this case, topics of interest include but are not limited to:", "\nComing soon.", "\nAs in previous years, IPSN will also elicit a review and response process. After the initial review, the program committee may request a short response from selected papers for additional clarifications. The authors will get a short time window of roughly 24 hours to respond. Participation in the response process is not mandatory.", "\nThe two conferences are collaborating to create new synergies, also exploiting their CPSWEEK bi-annual co-location. Prospective authors are thus strongly encouraged to consider these guidelines:", "The TPC chairs of both conferences may agree to move a submitted paper from one to the other if they see a better fit, subject to the authors\u2019 consent. The final decision about where to submit, and therefore what program committee will review the work, ultimately rests with the authors.", "\nPaper Abstract Registration: October 10th, 2018 11:59pm AoE(UTC-12)", "\nPaper Submission Deadline: (Firm*): October, 17th 2018 11:59pm AoE(UTC-12)", "\nAuthors\u2019 rebuttal period December 17th - 19th, 2018", "\nAcceptance Notification: January 16th, 2019", "\nCamera-Ready Deadline: TBD", "\nGeneral Chair:", "\nRasit Eskicioglu (University of Manitoba)", "Technical Committee Chairs:", "\nLuca Mottola (Politecnico di Milan and RISE SICS)", "\nBodhi Priyantha (Microsoft Research)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Sensor data storage, retrieval, processing", "Streaming sensor system tasking and operation", "Coding, compression, and information theory", "Theoretical foundation and fundamental bounds", "Network and system architectures and protocols", "IoT gateway platform architecture and services", "Outdoor, wide-area sensing systems", "Location, time, and other network services", "Programming models, languages, and systems", "Programming models for IoT ensembles", "Modeling, simulation, and measurement tools", "Operating systems and runtime environments", "Applications in health, wellness & sustainability", "Applications in smart cities and urban health", "Experiences, challenges, comparisons of platforms", "Discovery, coordination, and use of IoT services", "Security and privacy in heterogeneous systems", "IoT reliability, adaptability, and dependability", "Technical assessment of emerging IoT standards", "Wearable systems and data processing algorithms", "Sensor-enabled drone platforms and algorithms", "Machine learning and deep learning on sensor data", "New hardware and system design to enable machine learning on sensor data", "Novel embedded machine learning algorithms", "Data related issues, such as methods, tools, and analysis", "Computer vision for resource-constrained and mobile platforms", "Contributions of embedded nature and applying to network segments from the IoT gateway to field devices should be submitted to IPSN. Examples include localization, low-power wireless networking, and embedded data processing.", "Contributions with an end-to-end perspective or applying to network segments from the IoT gateway to the cloud should be submitted to IoTDI. Examples include cloud data processing, edge computing, and systems covering multiple network segments."], "url": "https://discourse.ros.org/t/cfp-acm-ieee-ipsn-2019-in-cpsweek/6240"},
{"title": "Answer ROS Questions Like a Pirate Day 2K18", "thread_contents": ["Avast there!", "\n", "This Wednesday is ", " which our community uses as an arbitrary day to put a focus on resolving issues on ", ", a.k.a. ", ".", "This is a group effort and can be participated in whether you\u2019re a seasoned captain of a ROS stack or a", "\npowder monkey who\u2019s just dipping their proverbial toe into the sea water.", "Here are some ways to help.", "For more on the subject, check out this other thread: ", "Day turned in UTC to Sept. 19th. It\u2019s time, turtles.", " / All questions: 13,259 / 42,211 = 31.4% (UTC 12:08 AM)", "(I feel like what the link above shows is \u201cunresolved\u201d questions (", ").)", "Today the number be down to 12918. Progress, me harties!", "Yarrr\u2026same thing new year. Close your olde questions that are no longer relevant like they were outdated memes.", "Specifically, I be callin out the following ", " users who have more than 30 questions without accepted answers. Avast!", "Kishore Kumar 30", "\nMehdi. 31", "\nlucasw 35", "\nMarkyMark2012 36", "\nS.Yildiz 36", "\nEmilien 37", "\naks 40", "\nNaman 41", "\nAutoCar 43", "\nEdwardNur 45", "\nCerin 49", "\nrnunziata 52", "\nstevemartin 54", "\ndinesh 55", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " - You know what\u2019s fun? Judging people. If there are answers that have been particularly useful to you, even on questions you didn\u2019t ask, go upvote them. Are there burning questions that you also want to know the answer to? Vote for the question. Giving positive reinforcement to the people answering questions can\u2019t hurt.", "\n", " - Fairly straightforward. Look through ", " and find something interesting.", "\n", " -  Go to your user profile and look through the questions you\u2019ve asked. For the ones that are still open, is there an answer you can mark as correct? Or if the question is no longer relevant, consider closing the question altogether.", "\n", " - If you\u2019re an ol\u2019 sea dog and have edit privileges, use your power for good and make sure that people have the correct formatting on their questions."], "url": "https://discourse.ros.org/t/answer-ros-questions-like-a-pirate-day-2k18/6058"},
{"title": "What do you want to see in an educational ROS platform?", "thread_contents": ["We\u2019ve started developing a table top / ground ROS robot for use in educational environments. We\u2019d like to hear from educators that really want to teach ROS based robotics, but don\u2019t feel they can because currently available robots are either too expensive, too complicated or too [insert your adjective here].", "We\u2019d also like to hear from educators that are teaching ROS robotics but want to broaden what they do, or find alternative platforms.", "If you are an educator who offers courses and would like to do so with ROS can you respond to this thread with the one thing that you\u2019d like to see in a educational platform for ROS. If enough people express interest we will do a couple of group calls so we can hear what people would like and make trade-off decisions. Then we will do our best to build what the group wants with open source repos to go along with it!", "Please respond with the one thing you\u2019d like to see in an ROS robot platform oriented for education.", "David", "P.S. Thanks to Tully and Kat for suggesting this!", "I\u2019m not using ROS in education yet but I\u2019ve been looking for a platform like this a while back. I was looking at this from a hobbyst perspective, so a low price was a must (Turtlebot looks good but it\u2019s a bit too expensive if someone wanted to buy it to get into ROS).", "Some of the platforms I looked at didn\u2019t have wheel encoders but I think it\u2019s a very good to have if you are teaching about odometry.", "Here are some things I was looking for in the robot platform:", "Functionality wise I wanted to be able to build a full ROS stack on the platform, starting with the drivers, through ros_control and ending with navigation and localization. I was hoping that with a platform like this I could guide students through all levels of software design for a robot.", "Sorry it\u2019s not a single thing but I thought some of these observations could be useful for you.", "Perfect educational resource to my mind is github repo with whole bunch of small working examples. Each of them compiles, runs, has reach comments and illustrates particular piece of ROS functionality.", "\nE.g.", "\nexample 1: how to use tf2", "\nexample 2: how to use can_bridge", "\nexample 3: how to use pointcloud and access individual points", "\nexample 4: you name it\u2026", "Awesome list! I agree price is the key thing that is missing here.", "How about if we designed it so that you\u2019d have ROS topics shared between a workstation computer and the robot itself and the heavy lifting was done on the workstation? Do you think that would be workable? This is one strategy to keep costs down for the robot - it also is a great example of the power of ROS.", "This reminds me a bit of the vector_ros: ", ". I can see a value in that for learning some high level concepts.", "The reason I was initially looking at low level concepts is because I think there is a niche in this area. At the time I was looking into it I couldn\u2019t find any tutorials on how to create a robot from scratch with ROS (I remember it took me quite a while to figure out how ros_control works).", "If you were to share the topics with the user\u2019s workstation would a user be able to add their own sensors on the robot? Because to me that is the single best thing about ROS and projects like vector_ros are very interesting but because you are not able to extend the robot the educational value might be a bit limited.", "That is definitely possible and workable, given decent WiFi. ", " is nontrivial I guess for complete noobs, but once that\u2019s taken care of, running code from a workstation removes the need to sync code from workstation to the robot.", "Running hardware interfaces of course needs to happen on the robot.", "Two places too look at are the SV-ROS github, which uses a Neato Botvac as a ROS platform, and the ROS by Example books by Patrick Goebel, available from Lulu. It is possible to build a ROS mock Turtlebot using a Raspberry Pi, and a Botvac that has a \u201cLidar\u201d for $300.", "In my experience with new comers (or in some cases even research labs that have been using ROS for years) is that network issues tend to cause them a lot of grief. I think the idea to have some of the logic run on the workstation is good but that would require the existence of very thorough noob friendly tutorials about how to get your network just right.", "Yes network provisioning is usually messy.", "We made it significantly less messy at Ubiquity Robotics by building PiFi. On bootup it scans all the available networks then boots in to AP mode with a unique network name (something like ubiquityrobotXXXX) where XXXX is the last 4 digits of the MAC address.", "You can connect to the robot via AP mode then when you do it presents a list of available networks and you can elect to connect to one of those or you can just stay in AP mode.", "It works well, although educational institutions some times have problems with new WiFi networks and also don\u2019t always make it easy to connect to the available infrastructure networks. Our solution is pretty slick and I can\u2019t think of a better one - but I am all ears for suggestions.", "I have used turtlebot3 for a while and for what it\u2019s intended for (as an introduction to ROS), it doesn\u2019t really justify the price of it.", "In the case of turtlebot3, the heavy lifting such as SLAM, map navigation is done on a workstation. But, I found this rather limiting.", "Ideally the onboard computer can function as a WiFi hotspot, possibly with a 4G/LTE dongle, so it can connect to internet if it needs to. In this case any computer could connect to such network. This will minimize network infrastructure.", "With waffle-pi, beginners could run the examples, launch the ros nodes and configure it with the available setup. But, that\u2019s pretty much the extent of what they can do. If they want to level up eventually, they will seek for better sensors and onboard computer.", "It\u2019s hard to scale the turtlebot or to upgrade it without replacing the core components (raspberry pi 3 model B, the dynamixel motors, etc.). When I plug the OpenCR to an Intel NUC, it doesn\u2019t immediately work out of the box, without some configuration and re-testing the Arduino code. This is what most beginners are not aware of.", "I also noticed people have used more powerful computers such as the Jetson TX2 to run processing onboard because tracking or depth cameras such as realsense / zed are too heavy for a raspberry pi 3, and yet they are quite popular among robotics researcher.", "Perhaps something like the PULP platform, OpenMV camera is a good alternative considering the costs.", "On the software side, it\u2019s not clear how to migrate to ROS 2 while keeping the software stack to be the same. This is if we still want to run the same SLAM / navigation packages. Though I understand it really depends on whether there\u2019s an upgrade on dependencies.", "In short, I would suggest that there should be a guidance on such \u201cupgrade\u201d path and scalability issues. As probably those who started to learn robotics are here to invest their time and skills in the long run.", "My apologies I\u2019m not too familiar with the tb3 but trying to understand why SLAM (I assume against an rplidar) qualifies as a heavyweight algorithm. What\u2019s being used as a default SLAM algorithm for tb3?", "The default setup (as per documentation) is to run the SLAM algorithm and navigation on a remote computer. Running gmapping (the default) or hector with rpildar A3 on the raspberrypi 3 model B is fine and I have tested that. But, I think cartographer is heavier. The other default option is to run frontier_exploration.", "To build the DWA local planner and map_server on the rpi3 itself requires pcl_ros to be installed as a dependency, which is unnecessary in most cases. So, up until now I never run the DWA local planner + map_server onboard.", "Then, once I setup a realsense T265 as a tracking camera to improve odometry, the realsense nodelets used up about 50% of memory on average. Occasionally the realsense camera manager crashes. If I run SLAM onboard.", "If I just use a 3S LiPo 5000mah battery (it\u2019s already larger than the default). The realsense fisheye camera nodes are not streaming their images. I assume there\u2019s not enough power from OpenCR. Although these camera images are fine if I use a larger capacity battery and higher voltage, such as 6S.", "For sure, I would need a better computer and more battery cells + capacity to run something like RTAB-Map with D435 + T265 for example.", "The impression is the rpi3 seem to only be utilised to run the turtlebot bringup or at most gmapping. I am in the process to at least migrate to rpi4 for its USB3 as realsense cameras worked best with USB3.", "At Stanford, one researcher ran a compute intensive Turtlebot II with extra  18v battery packs for a gaming Nvidia equiped laptop.  Runs up to 1 hour autonomous were obtained.", "But, I think cartographer is heavier.", "For what it is worth, I successfully ran cartographer on a ", " attached to a Turtlebot 2.  The caveat was that I had to run it in 2D mode; in 3D mode, it was too heavyweight.  You can see my short presentation from ROSCon 2017 about it ", ".", "Interesting, thanks for sharing your talk!", " does seem to have a better setup from what I\u2019ve seen. It doesn\u2019t have waffle plates. But, they are not really necessary.", "It comes with ", " camera or ASUS Xtion pro live if it\u2019s bought from clearpath robotics. Also dimension is better, while turtlebot3 require additional plates and plate support.", "I\u2019ve been teaching with Turtlebot 2 for several years now. In terms of hardware, it is hard to beat:", "There are a few downsides:", "I\u2019m at the point where I need to re-equip our robotics classroom, and I\u2019m at a loss. It looks like Turtlebot 3 is the default choice at this point, but it doesn\u2019t have any of the pros I list above.", "So\u2026 to answer your question. The platform I\u2019m looking for is something that looks a lot like TB2, with nice clean ROS and ROS2 packages.", "At Ubiquity Robotics we have been running platforms off RPi for a couple of years. We have a simplified navigation node called move-basic that is suitable for student learning and runs on low powered CPUs.", "I going to suggest a few things to make this discussion more productive.", "Please make it clear if you are an educator or not and if so what your student body looks like. Not all students are the same and it helps us if we understand the different constituencies of users.", "If you have a request or an idea please frame it in terms of the subject matter you want to teach not the hardware. Hardware changes from quarter to quarter; fundamentals more slowly.", "I\u2019m not an educator.", "But however we\u2019d like to consider robotics as an abstraction like how software is. It\u2019s not the same, hardware is also an important subject.", "A junior engineer could quickly drop a robot platform that doesn\u2019t do all the things he/she could see on videos of latest research.", "What they couldn\u2019t see is the efforts, workarounds, to make an algorithm work in a certain environment with certain setup of hardware.", "If I would to educate someone, be it a student or an engineer. I would emphasize:", "The JPL Mars rover is probably also a good reference as an educational platform.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Aforementioned wheel encoders", "IMU", "Onboard computer running ROS (extra points for this to be an option, lot\u2019s of people already have embedded computers)", "Ease of integration of custom sensors (consideration for space, robot payload)", "Some built-in DC-DC converters would be great to power external sensors (3.3, 5 and 12V would be superb)", "The size is good.  It is small enough to be safe and portable, but big enough to operate on human-scale problems like office delivery, tour guide etc.", "RGBD sensors provide a nice bang for the buck.  We can do 2d-slam, 3d-slam, computer vision etc.", "Using a laptop for computation makes it much easier to use in the classroom.  Trying to get networking set up correctly and keep it working for multiple robots is a pain.  It is much easier to just write some code on a laptop and plug it in.", "It would be nice if it were cheaper (though honestly, I don\u2019t think the price is unreasonable).", "The ROS Turtlebot packages are not very easy for novices to make sense of.  TB2 is relatively easy for beginners to use, but it is hard for beginners to modify. It would be nice to see an educational platform that serves as a clear, well-documented, example of how to set up a robot with ROS support.", "As far as I can tell, TB2 is going away. It\u2019s not clear if the Kobuki base is even being manufactured anymore.", "\n", "\n", "\n", "\n", "Environments that a robot should operate. This will cover perception, map, navigation, kinematics (if we\u2019re considering a more complicated movement than 2-wheeled differential drive robot)", "Some mechanical / electronics foundation (enough to get something working)", "OS & networking fundamentals, why Python / C++ as a common programming language. Although anyone can use other languages, but these two are the most common.", "Upgrade / scalability problems", "Algorithms", "Software management", "User Interface", "Security (this would be the advanced subject)"], "url": "https://discourse.ros.org/t/what-do-you-want-to-see-in-an-educational-ros-platform/10958"},
{"title": "ROS 2 TSC Meeting Minutes: 2019-06-20", "thread_contents": [", TSC members, community:", "we would appreciate  some feedback on what you think the most valuable contributions among these ones would be (originally listed above):", "In order of preference from most-wanted to not-as-most-wanted:", "It would be great to have ", " in the safety group, since that\u2019s already one of our possible action items.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees\n", "Open Robotics: Brian G, Louise, William", "LGE: Seonman", "Tier IV: Geoff", "eProsima: Jaime", "Intel: Matt H.", "Apex.AI: Lyle", "ROBOTIS: Pyo", "TRI: Toffee", "Bosch: Karsten", "Acutronic Robotics: Victor", "Amazon: Rutvik", "ARM: Filipe", "Microsoft: Sean", "GVSC (TARDEC): Jon Smereka", "\n", "Old business\n", "[2 mins] [Gerkey] ROS Elevator Pitch Brochure\n", "Done, with ", " to update the docs merged (example: ", ").\n", "Need to re-enable the update job to push it public.", "It will land here: ", "\n", "\n", "\n", "[2 mins] [Gerkey] ROS trademark update\n", "Draft use policy under development, led by Geoff with input from Brian G., Jaime, and some others.\n", "Brian to respond to outstanding feedback.", "\n", "Goal: finish first draft by the end of June, go to legal review, then bring to TSC for discussion.", "\n", "\n", "New business:\n", "[5 mins] [Gerkey] ROSCon updates\n", "\n", "\n", "\n", "\n", "[10 mins] [Gerkey] Dashing debrief\n", "How did we do with the release?\n", "What went well?\n", "(Victor) Thanks, their porting went well. Pre-release of Dashing was rather useful (empowered a complete port within a week of time, including external ROS 2 packages for manipulation)", "\n", "What could be improved?\n", "(Matt H. for nav team) Late changes in Fast-RTPS (1.7.2 - 1.8.0) were disruptive, happened in May, still recovering.\n", "We should integrate more Nav tests to avoid breaks like this.", "Nav team has tests which are docker-ized and use gazebo.", "Lifecycle node greatly increased number of topics and services, making nav test stress the middleware in new ways that aren\u2019t caught by existing tests.", "ROS 2 team to integrate a periodic (nightly?) version of Matt\u2019s nav test.\n", "Future goal: distill nav test to its basic stress-testing characteristics without the complexity of nav or gazebo.", "\n", "\n", "(Geoff)  In the future ROS 2 release team could take in results from external CI as well (e.g. Autoware) during the release testing.", "(Victor) geometry2 was released late. Fixed though.", "(Karsten) issues with getting ROS 1 package maintainers to be responsive to changes (specifically ones needed for ROS 2)\n", "Use the existing process, wherein Open Robotics staff and others in the community will help to mediate handovers: ", "\n", "\n", "\n", "\n", "\n", "[10 mins] [Gerkey] New member application(s) discussion", "\n", "Standing updates:\n", "[20 min] [William / All] Roadmap planning - Eloquent Elusor\n", "Master ticket is ready, seeded with Open Robotics\u2019 planned tasks: ", "\n", "List is work in progress, subject to change", "Getting back to rviz to fill in some long-standing gaps\n", "(Matt H.) Trouble with matching QoS types, e.g., for a map publisher with latching-like behavior. Issue to be opened on rviz.\n", "There is an issue already filed here which I believe is the same thing:\n", "\n", "\n", "\n", "Add the ability to do dynamic_reconfigure-like GUI features via rqt for handling params.", "Plan to get performance tests integrated into main CI system", "\n", "(Rutvik) How about creating a user-focused theme for Eloquent, to help motivate which features are to be prioritized? Volunteers to work on that:\n", "Rutvik", "Brian", "Geoff", "\n", "All are invited to add their own planned contributions as comments to the Eloquent master ticket.", "\n", "TSC members updates:\n", "ROBOTIS\n", "Prepare the ", " on ROS 2 Dashing with ", ", ", "\n", "\n", "Apex.AI\n", "From ", " we will for ROS 2 E work on the performance_test part. Major contributions include\n", "Results will be dumped into the SQL-like database", "Multi-process support", "ros2 launch support", "Aggregation of the test results into the test management suite (which will then allow regression monitoring and better bug triaging)", "\n", "Further contributions that we could bring in (items with * are on the ", " we would contribute 1-2 things based on the popular demand):\n", "Static code analysers (we are evaluating LDRA, TrustInSoft)", "Dynamic code analysers (e.g. Silexica SLX C++)", "*Rewritten rmw layer (super easy integration of DDS implementations, exposed full DDS API, 1 copy operation only, \u2026)", "*rmw that includes RTI Connext Micro", "Documentation testing extension to launch_testing (in which the documentation files written in markdown are instrumented such that they become integration tests)", "Presentation of best practices needed for development according to the ISO 26262 (FuSa for automotive)", "*Upstream the velodyne driver from ", " to ", "\n", "Support for ROS2 compilation on QNX", "Configuration management system. We see these use cases which ROS 2 parameters do not satisfy:\n", "Global configuration that many nodes share (e.g. RTOS priorities, vehicle dimensions, \u2026)", "Process level configuration", "Communication configuration (DDS Connext Micro)", "*Waitset implementation in rclcpp", "\n", "\n", "\n", "eProsima\n", "Fast RTPS 1.8.1 to be released next week\n", "Liveliness completed & bug fixes", "\n", "Micro XRCE-DDS 1.1: To be released also, aligned with Fast RTPS 1.8.1\n", "Micro-ROS to follow.", "\n", "Discovery Server: Public release next week.", "Performance Tests: Public end of this month\n", "Dedicated machines.", "\n", "ROS2 Integration Service\n", "Based on OSRF SOSS", "ROSIN Project", "\n", "\n", "Tier IV\n", "We have 8 half-FTEs beginning work on ROS 2 contributions (total 4 FTEs)", "Proposed contributions for ROS 2 E:\n", "Debian as Tier 1 (preferred) or Tier 2 platform", "Real-time publish/subscribe capability via adding new executors and/or implementing callback groups (need to consult William to determine specific work required; will also of course coordinate with real-time WG)", "Implement an rmw library for RTI\u2019s real-time DDS (Connext DDS Cert)", "\n", "\n", "LGE\n", "On Friday (6/21) we should be able to submit a PR to build webOS OSE with ROS 2 dashing release that\u2019s upgraded to OpenEmbedded 2.6 Thud;\n", "We tested it on raspberrypi3 and emulator (qemux86). It depends on OSE releasing the updated OpenEmbedded support.", "\n", "Need help from OSRF to finish the transfer of meta-ros to ros-infrastructure (from bmwcarit) so PR can be submitted", "\n", "Microsoft\n", "Ongoing project to do Windows binary packaging for ROS 2. Idea is to use bloom to produce vcpkg data and then produce chocolatey artifacts. More work needed to better connect vcpkg and chocolatey.", "Working on producing Dashing binaries from MS build farm.", "\n", "\n", "Working group updates:\n", "[3 min] [Matt H] Navigation\n", "Still wrapping up release for Dashing; aiming for end of June.", "\n", "[3 min] [Rutvik] Security\n", "Bug fix in moveit2 ", ". Short write up ", ".", "Planning to dedicate some resources to ensure moveit2 makes use of dynamic bug finding tools.\n", "To be integrated in the moveit2 CI analyzing both the moveit2 code and ROS 2 (base) code ", "\n", "Planning to report and fix bugs as they\u2019re found (to a limited amount of resources)", "\n", "\n", "[3 min] [Lyle] Real-time\n", "ROS 2 real-time workshop was accepted at ROSCon", "\n", " action items:\n", "Dejan to provide a use case description and the requirements", "Find someone that will lead the work on real-time capable rmw", "Get William to present his findings on real-timeness in rcutils, rcl, and rclcpp", "Create a shared repository for tools used for static and dynamic code analysis as well as tracing", "Present Bosch paper and real-time with Callback+Executor in detail in one of the next meetings", "QA RT buildfarm being finalized and prepared for public release.", "First peek available at ", "\n", "\n", "\n", "[3 min] [Geoff] Safety\n", "Held a somewhat better attended meeting (5 participants, so extrapolating we should have half the ROS community by mid-next year)", "There is significant interest in safety, but most people don\u2019t know exactly what is needed or what to contribute", "We have some concrete actions we can work on (see above topic) but are resource-constrained", "\n", "[3 min] [Karsten] Embedded\n", "Need more activity; Acutronic, eProsima are interested to participate", "\n", "[3 min] [V\u00edctor] Manipulation\n", "Moveit 2 alpha release ", "\n", "Moveit 2 and CI ported to Dashing official release ", "\n", "Still submitting PRs upstream but very little response from the moveit community (slowing down contributions).", "Aiming to get pick&place moveit tutorial running in the coming weeks", "Considering different avenues for the future", "\n", "\n", "\n", "Static code analysers (we are evaluating LDRA, TrustInSoft)", "Dynamic code analysers (e.g. Silexica SLX C++)", "Rewritten rmw layer (super easy integration of DDS implementations, exposed full DDS API, 1 copy operation only, \u2026)", "Rmw that includes RTI Connext Micro", "Documentation testing extension to launch_testing (in which the documentation files written in markdown are instrumented such that they become integration tests)", "Presentation of best practices needed for development according to the ISO 26262 (Functional Safety for automotive)", "Upstream the velodyne driver from ", " to ", "\n", "Support for ROS2  QNX (compilation and testing (we actually got python and thus colcon to run on QNX)", "Configuration management system", "3", "5", "8", "4 (We have someone working on this part time; would be happy to follow your lead)", "1", "2", "7", "9", "6"], "url": "https://discourse.ros.org/t/ros-2-tsc-meeting-minutes-2019-06-20/9652"},
{"title": "New packages for Melodic 2019-11-14", "thread_contents": ["We\u2019re happy to announce the next update for ROS Melodic. We have 32 new packages as well as 126 updated packages.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-melodic-cob-default-robot-behavior: 0.7.2-1", "\n", ": 0.7.2-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.3-1", "\n", ": 0.7.2-1", "ros-melodic-cob-hardware-emulation: 0.8.1-1", "ros-melodic-cob-moveit-config: 0.7.2-1", "ros-melodic-dccomms-ros: 0.0.2-1", "ros-melodic-dccomms-ros-msgs: 0.0.2-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "\n", ": 0.4.0-1", "ros-melodic-network-autoconfig: 0.1.1-1", "ros-melodic-seed-smartactuator-sdk: 0.0.4-1", "ros-melodic-swri-profiler: 0.2.2-1", "ros-melodic-swri-profiler-msgs: 0.2.2-1", "ros-melodic-swri-profiler-tools: 0.2.2-1", "ros-melodic-turtlesim-dash-tutorial: 1.0.0-2", "ros-melodic-ubiquity-motor: 0.10.0-1", "ros-melodic-underwater-sensor-msgs: 1.4.2-1", "ros-melodic-underwater-vehicle-dynamics: 1.4.2-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "ros-melodic-behaviortree-cpp-v3: 3.0.7-0 -> 3.1.0-2", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "ros-melodic-cloudwatch-logs-common: 1.1.0-1 -> 1.1.2-1", "ros-melodic-cloudwatch-metrics-common: 1.1.0-1 -> 1.1.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-base-controller-utils: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-bms-driver: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-cartesian-controller: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-control-mode-adapter: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-control-msgs: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-elmo-homing: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-frame-tracker: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-helper-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-model-identifier: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-melodic-cob-msgs: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "ros-melodic-cob-obstacle-distance: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-omni-drive-controller: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-phidget-em-state: 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-phidget-power-state: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-melodic-cob-scan-unifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-tricycle-controller: 0.8.0-1 -> 0.8.1-1", "ros-melodic-cob-twist-controller: 0.8.0-1 -> 0.8.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.0.9-0 -> 0.0.11-1", "ros-melodic-dataflow-lite: 1.1.0-1 -> 1.1.2-1", "ros-melodic-dataspeed-ulc: 0.0.4-1 -> 0.0.5-1", "ros-melodic-dataspeed-ulc-can: 0.0.4-1 -> 0.0.5-1", "ros-melodic-dataspeed-ulc-msgs: 0.0.4-1 -> 0.0.5-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.5.1-2 -> 1.6.5-1", "ros-melodic-file-management: 1.1.0-1 -> 1.1.2-1", "ros-melodic-generic-throttle: 0.6.14-1 -> 0.6.15-1", "\n", ": 2.0.1-1 -> 2.0.3-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.8.7-1 -> 1.8.8-1", "ros-melodic-laser-scan-densifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "ros-melodic-linux-networking: 1.0.13-2 -> 1.0.16-1", "\n", ": 2019.10.10-1 -> 2019.11.11-1", "\n", ": 0.5.1-2 -> 0.5.2-1", "\n", ": 0.5.1-2 -> 0.5.2-1", "ros-melodic-mongodb-store-msgs: 0.5.1-2 -> 0.5.2-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 1.0.13-2 -> 1.0.16-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "ros-melodic-osg-interactive-markers: 1.0.2-1 -> 1.0.2-2", "ros-melodic-osg-markers: 1.0.2-1 -> 1.0.2-2", "ros-melodic-osg-utils: 1.0.2-1 -> 1.0.2-2", "\n", ": 1.0.0-0 -> 1.1.1-1", "\n", ": 2.3.6-2 -> 2.4.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 1.1.4-1 -> 1.1.6-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 2.0.2-1 -> 2.0.4-1", "\n", ": 0.3.0-1 -> 0.4.2-1", "ros-melodic-rosmon: 2.1.1-1 -> 2.2.1-1", "ros-melodic-rosmon-core: 2.1.1-1 -> 2.2.1-1", "ros-melodic-rosmon-msgs: 2.1.1-1 -> 2.2.1-1", "\n", ": 0.5.0-1 -> 0.5.1-1", "ros-melodic-rqt-rosmon: 2.1.1-1 -> 2.2.1-1", "ros-melodic-service-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.1-1 -> 0.8.2-1", "\n", ": 0.8.2-1 -> 0.8.3-1", "ros-melodic-um7: 0.0.5-1 -> 0.0.6-1", "\n", ": 0.4.1-1 -> 0.4.1-2", "ros-melodic-uwsim-bullet: 2.82.1-1 -> 2.82.2-1", "ros-melodic-uwsim-osgbullet: 3.0.1-1 -> 3.0.1-3", "ros-melodic-uwsim-osgocean: 1.0.3-1 -> 1.0.4-1", "ros-melodic-uwsim-osgworks: 3.0.3-1 -> 3.0.3-2", "\n", ": 1.0.2-1 -> 1.0.2-2", "AWS RoboMaker", "Alexander Bubeck", "Benjamin Maidel", "Chris Lalancette", "Christoph R\u00f6smann", "Daniel Miller", "Davide Faconti", "Devon Ash", "Diego Centelles", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Florian Weisshardt", "HXR", "Jannik Abbenseth", "Javier Perez", "Johannes Meyer", "Jon Binney", "Joshua Hampp", "Justin Carpentier", "Kevin Hallenbeck", "Marc Hanheide", "Mario Prats", "Martin Pecka", "Mathias L\u00fcdtke", "Matthias Gruhler", "Max Schwarz", "Micho Radovnikovich", "Nick Hawes", "P. J. Reed", "Richard Bormann", "Rohan Agrawal", "Scott K Logan", "Siddhartha Banerjee", "Vladimir Ermakov", "Yasuto Shiigi", "dfaconti"], "url": "https://discourse.ros.org/t/new-packages-for-melodic-2019-11-14/11494"},
{"title": "New packages for ROS Kinetic Kame 2019-11-15", "thread_contents": ["We\u2019re happy to announce 12 new and 144 updated packages for Kinetic.", "There is one regression of pinocchio due to cmake/pkg-config that\u2019s being actively worked on to be resolved.", "Thank you to all the maintainers and contributors who have helped make these packages available!", "Details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", " can we expect gurobi and coinutils packages to be made available for Kinetic as well?", " I\u2019m not familiar with those ROS packages in this distro or searching any of the other of the ROS distros. If you know of ones, you can reach out to their maintainers to know when and where they\u2019re expecting to make a release.", " they were used as dependencies for ipa_room_segmentation in ROS Indigo", " does not appear to have been released. I found a reference here: ", " and the source code it references doesn\u2019t appear to have either of those packages in it\u2019s dependency list on the ", " branch:", "There are some rosdeps defined related to coinutils now ", ":", "But I\u2019ll return to my previous statement that if you\u2019re interested in having something released into a rosdistro please reach out to the maintainer of the package that you\u2019re looking for. As the release manager I coordinate the releases from maintainers throughout the community. But it\u2019s up to the maintainers to make releases.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 0.7.3-1", "ros-kinetic-cob-hardware-emulation: 0.7.9-1", "ros-kinetic-leuze-description: 1.0.0-2", "ros-kinetic-leuze-msgs: 1.0.0-2", "ros-kinetic-leuze-phidget-driver: 1.0.0-2", "ros-kinetic-network-autoconfig: 0.1.1-1", "ros-kinetic-robot-indicator: 0.1.3-1", "ros-kinetic-robot-systemd: 0.1.2-1", "\n", ": 0.3.0-1", "\n", ": 0.3.0-1", "ros-kinetic-seed-r7-samples: 0.3.0-1", "ros-kinetic-turtlebot-loadout-kha1: 0.1.0-3", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "\n", ": 1.0.3-1 -> 1.1.0-1", "ros-kinetic-behaviortree-cpp-v3: 3.0.7-0 -> 3.1.0-3", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "ros-kinetic-cloudwatch-logs-common: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-cloudwatch-metrics-common: 1.1.0-2 -> 1.1.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-base-controller-utils: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-bms-driver: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-cartesian-controller: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-control-mode-adapter: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-control-msgs: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-kinetic-cob-default-robot-behavior: 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-elmo-homing: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-frame-tracker: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-helper-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-model-identifier: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "ros-kinetic-cob-moveit-config: 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-msgs: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "\n", ": 0.6.8-1 -> 0.6.9-1", "ros-kinetic-cob-obstacle-distance: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-omni-drive-controller: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-phidget-em-state: 0.7.0-1 -> 0.7.1-1", "ros-kinetic-cob-phidget-power-state: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-1 -> 0.7.2-1", "ros-kinetic-cob-scan-unifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.1-2 -> 0.7.3-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.12-1 -> 0.6.13-1", "\n", ": 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-tricycle-controller: 0.7.8-1 -> 0.7.9-1", "ros-kinetic-cob-twist-controller: 0.7.8-1 -> 0.7.9-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.0.9-0 -> 0.0.11-1", "ros-kinetic-dataflow-lite: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-dataspeed-ulc: 0.0.4-1 -> 0.0.5-1", "ros-kinetic-dataspeed-ulc-can: 0.0.4-1 -> 0.0.5-1", "ros-kinetic-dataspeed-ulc-msgs: 0.0.4-1 -> 0.0.5-1", "\n", ": 1.5.1-1 -> 1.6.7-1", "ros-kinetic-file-management: 1.1.0-2 -> 1.1.2-1", "ros-kinetic-generic-throttle: 0.6.14-1 -> 0.6.15-1", "\n", ": 2.0.1-1 -> 2.0.3-1", "ros-kinetic-laser-scan-densifier: 0.7.0-1 -> 0.7.1-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2019.10.10-1 -> 2019.11.11-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.33.0-1 -> 0.33.3-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 1.0.0-0 -> 1.1.1-1", "\n", ": 2.3.6-2 -> 2.4.1-1", "\n", ": 0.7.0-1 -> 0.7.1-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 2.0.2-1 -> 2.0.4-1", "\n", ": 0.3.0-1 -> 0.4.1-1", "\n", ": 0.5.0-1 -> 0.5.1-1", "ros-kinetic-seed-r7-bringup: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-description: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-navigation: 0.2.0-2 -> 0.3.0-1", "ros-kinetic-seed-r7-robot-interface: 0.2.0-2 -> 0.3.0-1", "\n", ": 0.2.0-2 -> 0.3.0-1", "ros-kinetic-service-tools: 0.6.14-1 -> 0.6.15-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 0.7.11-1 -> 0.7.12-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-nodelet: 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-profiler: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-profiler-msgs: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-profiler-tools: 0.1.0-0 -> 0.2.2-2", "ros-kinetic-swri-roscpp: 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-rospy: 2.10.0-1 -> 2.11.0-1", "ros-kinetic-swri-route-util: 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 2.10.0-1 -> 2.11.0-1", "\n", ": 0.6.13-1 -> 0.6.14-1", "\n", ": 0.3.1-1 -> 0.3.1-2", "ros-kinetic-test-mavros: 0.33.0-1 -> 0.33.3-1", "ros-kinetic-ubiquity-motor: 0.9.0-0 -> 0.10.0-1", "ros-kinetic-um7: 0.0.5-1 -> 0.0.6-1", "AWS RoboMaker", "Alexander Bubeck", "Benjamin Maidel", "Christoph R\u00f6smann", "Daniel Miller", "Davide Faconti", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Florian Weisshardt", "HXR", "Jannik Abbenseth", "Jordy van Appeven", "Joshua Hampp", "Justin Carpentier", "Kevin Hallenbeck", "Kris Kozak", "Ludovic Delval", "Marc Alban", "Mathias L\u00fcdtke", "Matthias Gruhler", "Micho Radovnikovich", "Nick Rotella", "P. J. Reed", "Richard Bormann", "Rohan Agrawal", "Scott K Logan", "Vladimir Ermakov", "Yasuto Shiigi", "dfaconti", "hi.kondo", "turtlebot", "\t<buildtool_depend>catkin</buildtool_depend>", "\t", "\t<depend>actionlib</depend>", "\t<depend>cv_bridge</depend>", "\t<depend>dynamic_reconfigure</depend>", "\t<depend>ipa_building_msgs</depend>", "\t<depend>libdlib</depend>", "\t<depend>libopencv-dev</depend>", "\t<depend>nav_msgs</depend>", "\t<depend>opengm</depend>", "\t<depend>roscpp</depend>", "\t<depend>roslib</depend>", "\t<depend>sensor_msgs</depend>"], "url": "https://discourse.ros.org/t/new-packages-for-ros-kinetic-kame-2019-11-15/11527"},
{"title": "Erle Robotics announces ground-based drones Gazebo simulation!", "thread_contents": ["Erle Robotics is glad to announce that the ", " ground drone (powered by the APM autopilot) has been launched and open sourced.", "For those interested, APM (also known as ardupilot) is an open source project (licensed under GPLv3) that aims to create autopilots for autonomous vehicles. As it is nowadays, it supports ground vehicles, aerial vehicles (in different configurations, planes, copters, hybrid vehicles, \u2026) rovers and recently also submarines.", "The plugin has been developed over AurelienRoy\u2019s plugin which, in a nutshell, is an interface between Gazebo and APM:Rover (ardupilot). To show the power of this plugin, here are some basic behaviors we developed and tested:", "            ", "\n", "Additional information and resources:", "I\u2019ve noticed a few ", " questions about car-like path planning ", " - I wonder if anything in this source code would be helpful there?   Or is the ", " usable in this robot?", "Good point ", ", I believe it should.", "\nThe autopilot functions are abstracted within the plugin and If additional functionality is needed, extending the plugin is the way to go.", "We plan on having a look a deeper look at the navigation stack and simulate more behaviors in the coming weeks.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": a simple but powerful enough algorithm to avoid obstacles. This behaviour was tested first under Gazebo and then brought to real life scenario. It uses a 270\u00ba HOKUYO lidar sensor, but all the details and the source code can be found here.", "\n", ": line follower algorithm using the camera. Again, all details as well as the source code can be found here.", "\n", ": SLAM application using hector_mapping node, a powerful SLAM approach that can be used only with a lidar sensor. All the details, including source code are available here. This also was translated to a real scenario and we\u2019ll push more videos soon."], "url": "https://discourse.ros.org/t/erle-robotics-announces-ground-based-drones-gazebo-simulation/171"},
{"title": "New Packages for Indigo Jade and Kinetic 2016-10-24", "thread_contents": ["We have a simultaneous sync of Indigo Jade and Kinetic packages. This includes close to 100 new packages as well as several hundred updated packages. Please see the full details below. And as always thank you to everyone who has contributed to the many updates included!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "#", " to kinetic", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-aubo-driver: 0.2.1-0", "ros-indigo-aubo-trajectory: 0.2.1-0", "ros-indigo-cob-docker-control: 0.6.5-0", "ros-indigo-cob-hand: 0.6.1-0", "ros-indigo-cob-hand-bridge: 0.6.1-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0", "ros-indigo-cv-detection: 0.0.2-0", "ros-indigo-grid-map-pcl: 1.4.1-0", "ros-indigo-ihmc-msgs: 0.8.0-6", "ros-indigo-ihmc-ros-common: 0.8.0-6", "ros-indigo-ihmc-ros-control: 0.5.0-1", "ros-indigo-ihmc-ros-core: 0.8.0-6", "ros-indigo-ihmc-ros-diagnostics: 0.8.0-1", "ros-indigo-ihmc-ros-java-adapter: 0.8.0-6", "ros-indigo-imagezero: 0.2.3-0", "ros-indigo-imagezero-image-transport: 0.2.3-0", "ros-indigo-imagezero-ros: 0.2.3-0", "ros-indigo-libconcorde-tsp-solver: 0.6.7-0", "ros-indigo-libdlib: 0.6.9-0", "ros-indigo-libopengm: 0.6.9-0", "ros-indigo-lpg-planner: 2.0.17-0", "ros-indigo-mrpt-rbpf-slam: 0.1.3-0", "ros-indigo-nao-dcm-bringup: 0.0.2-0", "ros-indigo-reapp-description: 0.1.1-0", "ros-indigo-reapp-msgs: 0.1.1-0", "ros-indigo-ros-type-introspection: 0.3.1-0", "ros-indigo-rwt-image-view: 0.0.3-1", "ros-indigo-rwt-moveit: 0.0.3-1", "ros-indigo-rwt-plot: 0.0.3-1", "ros-indigo-rwt-speech-recognition: 0.0.3-1", "ros-indigo-rwt-utils-3rdparty: 0.0.3-1", "ros-indigo-sick-visionary-t-driver: 0.0.3-1", "ros-indigo-slic: 2.0.17-0", "ros-indigo-visualization-rwt: 0.0.3-1", "ros-indigo-warthog-control: 0.0.1-0", "ros-indigo-warthog-description: 0.0.1-0", "ros-indigo-warthog-desktop: 0.0.1-0", "ros-indigo-warthog-msgs: 0.0.1-0", "ros-indigo-warthog-viz: 0.0.1-0", "ros-indigo-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-indigo-aruco: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-msgs: 0.1.0-0 -> 0.2.0-0", "ros-indigo-aruco-ros: 0.1.0-0 -> 0.2.0-0", "ros-indigo-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-indigo-aubo-description: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-gazebo: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-i5-moveit-config: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-kinematics: 0.1.1-3 -> 0.2.1-0", "ros-indigo-aubo-msgs: 0.1.1-3 -> 0.2.1-0", "ros-indigo-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-indigo-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-indigo-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-drive-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-base-velocity-smoother: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-bms-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-bringup: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-bringup-sim: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-calibration-data: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-cam3d-throttle: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-camera-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-canopen-motor: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cartesian-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-collision-velocity-filter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-command-gui: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-command-tools: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-common: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-control: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-mode-adapter: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-control-msgs: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-controller-configuration-gazebo: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-dashboard: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-default-env-config: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-default-robot-behavior: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-elmo-homing: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-environments: 0.6.3-0 -> 0.6.4-0", "ros-indigo-cob-extern: 0.6.4-0 -> 0.6.9-0", "ros-indigo-cob-footprint-observer: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-frame-tracker: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-gazebo: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-objects: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-generic-can: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-hardware-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-head-axis: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-image-flip: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-interactive-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-lbr: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-light: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-mimic: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-model-identifier: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-monitoring: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-msgs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-object-detection-visualizer: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-obstacle-distance: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-omni-drive-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-perception-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-perception-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-phidgets: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-relayboard: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-robots: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-safety-controller: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-scan-unifier: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-script-server: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-sick-lms1xx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-sick-s300: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-simulation: 0.6.5-0 -> 0.6.7-0", "ros-indigo-cob-sound: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-srvs: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-substitute: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-teleop: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-trajectory-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-twist-controller: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.11-0 -> 0.6.14-0", "ros-indigo-cob-utilities: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-vision-utils: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-voltage-control: 0.6.7-0 -> 0.6.8-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-concert-conductor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-master: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-schedulers: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-link-graph: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-manager: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-service-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-concert-service-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-software-farmer: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-utilities: 0.6.11-0 -> 0.6.11-1", "ros-indigo-concert-workflow-engine-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-indigo-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-downward: 2.0.14-0 -> 2.0.17-0", "ros-indigo-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-indigo-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-indigo-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-indigo-ff: 2.0.14-0 -> 2.0.17-0", "ros-indigo-ffha: 2.0.14-0 -> 2.0.17-0", "ros-indigo-frida-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-gateway-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-indigo-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-indigo-hironx-calibration: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-moveit-config: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hironx-ros-bridge: 1.1.16-0 -> 1.1.17-0", "ros-indigo-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-indigo-husky-base: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-bringup: 0.2.5-0 -> 0.2.6-0", "ros-indigo-husky-robot: 0.2.5-0 -> 0.2.6-0", "ros-indigo-image-common: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-exposure-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-indigo-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-indigo-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-indigo-imu-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-indigo-jsk-apc2015-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-apc2016-common: 1.5.1-0 -> 2.0.0-0", "ros-indigo-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-indigo-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-indigo-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-indigo-julius: 2.0.14-0 -> 2.0.17-0", "ros-indigo-laser-filters-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-libntcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libpcan: 0.6.4-0 -> 0.6.9-0", "ros-indigo-libphidgets: 0.6.4-0 -> 0.6.9-0", "ros-indigo-librealsense: 0.9.2-3 -> 1.11.0-1", "ros-indigo-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mapviz: 0.0.6-0 -> 0.0.7-0", "ros-indigo-marti-can-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-common-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-data-structures: 0.0.12-0 -> 0.0.13-0", "ros-indigo-marti-nav-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-perception-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-sensor-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-marti-visualization-msgs: 0.0.5-0 -> 0.0.6-0", "ros-indigo-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-indigo-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-indigo-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-nextage-description: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-gazebo: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ik-plugin: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-moveit-config: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nextage-ros-bridge: 0.7.8-0 -> 0.7.10-0", "ros-indigo-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-indigo-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-indigo-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-indigo-parrot-arsdk: 3.9.1-6 -> 3.10.1-0", "ros-indigo-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-indigo-pid: 0.0.17-0 -> 0.0.18-0", "ros-indigo-pointgrey-camera-description: 0.12.1-1 -> 0.12.2-0", "ros-indigo-pointgrey-camera-driver: 0.12.1-1 -> 0.12.2-0", "ros-indigo-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-indigo-pr2-bringup: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-camera-synchronizer: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-computer-monitor: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-controller-configuration: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-ethercat: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-robot: 1.6.16-2 -> 1.6.16-4", "ros-indigo-pr2-run-stop-auto-restart: 1.6.16-2 -> 1.6.16-4", "ros-indigo-prace-common: 0.6.4-0 -> 0.6.5-0", "ros-indigo-prace-gripper-driver: 0.6.4-0 -> 0.6.5-0", "ros-indigo-raw-description: 0.6.5-0 -> 0.6.6-0", "ros-indigo-realsense-camera: 1.4.0-0 -> 1.5.0-0", "ros-indigo-ridgeback-control: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-description: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-msgs: 0.1.7-0 -> 0.1.8-0", "ros-indigo-ridgeback-navigation: 0.1.7-0 -> 0.1.8-0", "ros-indigo-rocon-app-manager: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-manager-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-app-platform: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-app-utilities: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-apps: 0.7.13-0 -> 0.7.13-1", "ros-indigo-rocon-bubble-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-concert: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-console: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-device-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-ebnf: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-gateway: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-tests: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-gateway-utils: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-hub-client: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-icons: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-interaction-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-interactions: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-launch: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-master-info: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-multimaster: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-python-comms: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-redis: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-utils: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-python-wifi: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-semantic-version: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-service-pair-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-std-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-test: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-tf-reconstructor: 0.6.11-0 -> 0.6.11-1", "ros-indigo-rocon-tools: 0.1.23-0 -> 0.1.23-1", "ros-indigo-rocon-tutorial-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-rocon-unreliable-experiments: 0.7.10-0 -> 0.7.10-1", "ros-indigo-rocon-uri: 0.1.23-0 -> 0.1.23-1", "ros-indigo-roseus: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-indigo-roseus-tutorials: 1.5.3-0 -> 1.6.0-0", "ros-indigo-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rosping: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-indigo-rtmros-hironx: 1.1.16-0 -> 1.1.17-0", "ros-indigo-rtmros-nextage: 0.7.8-0 -> 0.7.10-0", "ros-indigo-rviz: 1.11.14-0 -> 1.11.15-0", "ros-indigo-scheduler-msgs: 0.7.12-0 -> 0.7.12-1", "ros-indigo-schunk-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-libm5api: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-modular-robotics: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-powercube-chain: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdh: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-sdhx: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.7-0 -> 0.6.8-0", "ros-indigo-statistics-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-swri-console-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-geometry-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-image-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-math-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-nodelet: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-opencv-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-prefix-tools: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-roscpp: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-rospy: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-route-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-serial-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-string-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-system-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-transform-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-swri-yaml-util: 0.0.12-0 -> 0.0.13-0", "ros-indigo-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-indigo-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-indigo-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-indigo-web-video-server: 0.0.4-0 -> 0.0.5-0", "ros-indigo-wfov-camera-msgs: 0.12.1-1 -> 0.12.2-0", "ros-indigo-grid-map", "ros-indigo-grid-map-demos", "ros-indigo-tiago-bringup", "ros-indigo-tiago-gazebo", "ros-indigo-tiago-moveit-config", "ros-indigo-tiago-robot", "ros-indigo-tiago-simulation", "Alexander Bubeck", "Alexander Tiderko", "Andy Zelenak", "Bence Magyar", "Benjamin Maidel", "Daniel Stonier", "David Gossow", "Davide Faconti", "Devon Ash", "Dongwook Lee", "Doug Stephen", "Ed Venator", "Edmond DuPont", "Elliot Johnson", "Felix Messmer", "Florian Weisshardt", "Hitoshi Kamada", "IK Fast Plugin Creater", "Isaac I.Y. Saito", "Isaac IY Saito", "Isaac Isao Saito", "Jack O\u2019Quin", "Jan Fischer", "Jesper Smith", "Jihoon Lee", "Jose Luis", "Jose Luis Blanco Claraco", "Joshua Hampp", "Julius Kammerl", "Kei Okada", "Kentaro Wada", "Kris Kozak", "Liuxin", "Mani Monajjemi", "Marc Alban", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Mikael Arguedas", "Mike Purvis", "MoveIt Setup Assistant", "Nadia Hammoudeh Garcia", "Noda Shintaro", "P. J. Reed", "Paul Bovbel", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Rajvi Jingar", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Scott Niekum", "Sergey Dorodnicov", "TORK", "Takuya Nakaoka", "Thiago de Freitas", "Tianjiang Hu", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "liuxin", "ros-jade-grid-map-pcl: 1.4.1-0", "ros-jade-imagezero: 0.2.3-0", "ros-jade-imagezero-image-transport: 0.2.3-0", "ros-jade-imagezero-ros: 0.2.3-0", "ros-jade-lpg-planner: 2.0.17-0", "ros-jade-mrpt-rbpf-slam: 0.1.3-0", "ros-jade-mrpt-slam: 0.1.3-0", "ros-jade-ros-type-introspection: 0.3.1-0", "ros-jade-slic: 2.0.17-0", "ros-jade-swri-console: 0.2.0-0", "ros-jade-ar-track-alvar: 0.5.3-0 -> 0.5.4-0", "ros-jade-assimp-devel: 2.0.14-0 -> 2.0.17-0", "ros-jade-bayesian-belief-networks: 2.0.14-0 -> 2.0.17-0", "ros-jade-camera-calibration-parsers: 1.11.10-0 -> 1.11.11-0", "ros-jade-camera-info-manager: 1.11.10-0 -> 1.11.11-0", "ros-jade-collada-urdf-jsk-patch: 2.0.14-0 -> 2.0.17-0", "ros-jade-compressed-depth-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-compressed-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-cv-bridge: 1.11.13-0 -> 1.11.14-0", "ros-jade-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-downward: 2.0.14-0 -> 2.0.17-0", "ros-jade-eus-assimp: 0.2.4-0 -> 0.2.5-0", "ros-jade-euscollada: 0.2.4-0 -> 0.2.5-0", "ros-jade-eusurdf: 0.2.4-0 -> 0.2.5-0", "ros-jade-ff: 2.0.14-0 -> 2.0.17-0", "ros-jade-ffha: 2.0.14-0 -> 2.0.17-0", "ros-jade-grid-map: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-core: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-cv: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-demos: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-filters: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-loader: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-msgs: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-ros: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-0", "ros-jade-grid-map-visualization: 1.4.0-0 -> 1.4.1-0", "ros-jade-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-jade-image-common: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-exposure-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-image-geometry: 1.11.13-0 -> 1.11.14-0", "ros-jade-image-transport: 1.11.10-0 -> 1.11.11-0", "ros-jade-image-transport-plugins: 1.9.3-0 -> 1.9.5-0", "ros-jade-jsk-3rdparty: 2.0.14-0 -> 2.0.17-0", "ros-jade-jsk-interactive: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-marker: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-interactive-test: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-model-tools: 0.2.4-0 -> 0.2.5-0", "ros-jade-jsk-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-jsk-rqt-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-rviz-plugins: 1.0.33-0 -> 1.0.34-0", "ros-jade-jsk-visualization: 1.0.33-0 -> 1.0.34-0", "ros-jade-julius: 2.0.14-0 -> 2.0.17-0", "ros-jade-libcmt: 2.0.14-0 -> 2.0.17-0", "ros-jade-libsiftfast: 2.0.14-0 -> 2.0.17-0", "ros-jade-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-data-structures: 0.1.5-0 -> 0.1.6-0", "ros-jade-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-jade-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-mini-maxwell: 2.0.14-0 -> 2.0.17-0", "ros-jade-mrpt-ekf-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-ekf-slam-3d: 0.1.1-0 -> 0.1.3-0", "ros-jade-mrpt-icp-slam-2d: 0.1.1-0 -> 0.1.3-0", "ros-jade-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-jade-nlopt: 2.0.14-0 -> 2.0.17-0", "ros-jade-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-jade-opt-camera: 2.0.14-0 -> 2.0.17-0", "ros-jade-parrot-arsdk: 3.9.1-3 -> 3.10.1-0", "ros-jade-pgm-learner: 2.0.14-0 -> 2.0.17-0", "ros-jade-pid: 0.0.17-0 -> 0.0.18-0", "ros-jade-pointgrey-camera-description: 0.12.1-0 -> 0.12.2-0", "ros-jade-pointgrey-camera-driver: 0.12.1-0 -> 0.12.2-0", "ros-jade-polled-camera: 1.11.10-0 -> 1.11.11-0", "ros-jade-roseus: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-mongo: 1.5.3-0 -> 1.6.0-0", "ros-jade-roseus-smach: 1.5.3-0 -> 1.6.0-0", "ros-jade-rospatlite: 2.0.14-0 -> 2.0.17-0", "ros-jade-rosping: 2.0.14-0 -> 2.0.17-0", "ros-jade-rostwitter: 2.0.14-0 -> 2.0.17-0", "ros-jade-rviz: 1.11.14-0 -> 1.11.15-0", "ros-jade-statistics-msgs: 0.12.1-0 -> 0.12.2-0", "ros-jade-swri-console-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-geometry-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-image-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-math-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-nodelet: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-opencv-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-prefix-tools: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-roscpp: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-route-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-serial-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-string-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-system-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-transform-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-swri-yaml-util: 0.1.5-0 -> 0.1.6-0", "ros-jade-teb-local-planner: 0.5.1-0 -> 0.5.2-0", "ros-jade-theora-image-transport: 1.9.3-0 -> 1.9.5-0", "ros-jade-vision-opencv: 1.11.13-0 -> 1.11.14-0", "ros-jade-voice-text: 2.0.14-0 -> 2.0.17-0", "ros-jade-wfov-camera-msgs: 0.12.1-0 -> 0.12.2-0", "Alexander Tiderko", "Andy Zelenak", "Christoph R\u00f6smann", "David Gossow", "Davide Faconti", "Edmond DuPont", "Elliot Johnson", "Hitoshi Kamada", "Jack O\u2019Quin", "Jose Luis", "Jose Luis Blanco Claraco", "Julius Kammerl", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Mani Monajjemi", "Marc Alban", "Mike Purvis", "Noda Shintaro", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Scott Niekum", "Takuya Nakaoka", "Tony Baltovski", "Vincent Rabaud", "Vladislav Tananaev", "Yohei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "k-okada", "ros-kinetic-dynamic-tf-publisher: 2.1.2-1", "ros-kinetic-grid-map-pcl: 1.4.1-1", "ros-kinetic-image-view2: 2.1.2-1", "ros-kinetic-imagezero: 0.2.3-0", "ros-kinetic-imagezero-image-transport: 0.2.3-0", "ros-kinetic-imagezero-ros: 0.2.3-0", "ros-kinetic-jsk-common: 2.1.2-1", "ros-kinetic-jsk-data: 2.1.2-1", "ros-kinetic-jsk-network-tools: 2.1.2-1", "ros-kinetic-jsk-roseus: 1.6.0-0", "ros-kinetic-jsk-tilt-laser: 2.1.2-1", "ros-kinetic-jsk-tools: 2.1.2-1", "ros-kinetic-jsk-topic-tools: 2.1.2-1", "ros-kinetic-laser-scan-publisher-tutorial: 0.2.3-0", "ros-kinetic-multi-map-server: 2.1.2-1", "ros-kinetic-navigation-stage: 0.2.3-0", "ros-kinetic-navigation-tutorials: 0.2.3-0", "ros-kinetic-octomap-mapping: 0.6.1-0", "ros-kinetic-octomap-server: 0.6.1-0", "ros-kinetic-odometry-publisher-tutorial: 0.2.3-0", "ros-kinetic-point-cloud-publisher-tutorial: 0.2.3-0", "ros-kinetic-robot-setup-tf-tutorial: 0.2.3-0", "ros-kinetic-roomba-stage: 0.2.3-0", "ros-kinetic-ros-type-introspection: 0.3.1-0", "ros-kinetic-roseus: 1.6.0-0", "ros-kinetic-roseus-smach: 1.6.0-0", "ros-kinetic-rqt-wrapper: 0.1.3-0", "ros-kinetic-simple-navigation-goals-tutorial: 0.2.3-0", "ros-kinetic-swri-console: 0.2.0-0", "ros-kinetic-virtual-force-publisher: 2.1.2-1", "ros-kinetic-wts-driver: 1.0.4-0", "ros-kinetic-concert-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-service-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-concert-workflow-engine-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-default-cfg-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-gateway-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-geometric-shapes: 0.5.1-0 -> 0.5.2-0", "ros-kinetic-grid-map: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-core: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-cv: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-demos: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-filters: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-loader: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-msgs: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-ros: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-rviz-plugin: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-grid-map-visualization: 1.4.0-0 -> 1.4.1-1", "ros-kinetic-hrpsys: 315.10.0-0 -> 315.10.1-0", "ros-kinetic-librealsense: 1.11.0-0 -> 1.11.0-1", "ros-kinetic-marti-can-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-common-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-data-structures: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-marti-nav-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-perception-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-sensor-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-marti-visualization-msgs: 0.0.4-0 -> 0.0.6-0", "ros-kinetic-master-discovery-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-master-sync-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-mavlink: 2016.9.9-0 -> 2016.10.10-0", "ros-kinetic-moveit-python: 0.2.17-0 -> 0.2.17-1", "ros-kinetic-multimaster-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-multimaster-msgs-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-nerian-sp1: 1.3.3-0 -> 1.4.0-0", "ros-kinetic-node-manager-fkie: 0.5.8-0 -> 0.6.1-0", "ros-kinetic-pid: 0.0.17-0 -> 0.0.18-0", "ros-kinetic-robot-state-publisher: 1.13.2-0 -> 1.13.3-0", "ros-kinetic-rocon-app-manager: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-manager-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-app-platform: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-app-utilities: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-apps: 0.8.0-0 -> 0.9.1-0", "ros-kinetic-rocon-bubble-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-console: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-device-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-ebnf: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-gateway: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-tests: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-gateway-utils: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-hub-client: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-icons: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-interaction-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-interactions: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-launch: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-master-info: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-multimaster: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-python-comms: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-redis: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-utils: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-python-wifi: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-semantic-version: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-service-pair-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-std-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-test: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-tools: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rocon-tutorial-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-rocon-unreliable-experiments: 0.8.1-1 -> 0.8.1-2", "ros-kinetic-rocon-uri: 0.3.2-0 -> 0.3.2-1", "ros-kinetic-rtabmap-ros: 0.11.8-0 -> 0.11.8-1", "ros-kinetic-rviz: 1.12.1-0 -> 1.12.3-0", "ros-kinetic-scheduler-msgs: 0.9.0-0 -> 0.9.0-1", "ros-kinetic-swri-console-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-geometry-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-image-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-math-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-nodelet: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-opencv-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-prefix-tools: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-roscpp: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-route-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-serial-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-string-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-system-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-transform-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-swri-yaml-util: 0.2.0-1 -> 0.2.1-0", "ros-kinetic-teb-local-planner: 0.6.3-0 -> 0.6.4-0", "AlexV", "Alexander Tiderko", "Andy Zelenak", "Armin Hornung", "Chittaranjan Srinivas Swaminathan", "Christoph R\u00f6smann", "Daniel Stonier", "David Gossow", "Davide Faconti", "Dongwook Lee", "Edmond DuPont", "Elliot Johnson", "Ioan Sucan", "Jihoon Lee", "Kei Okada", "Konstantin Schauwecker", "Kris Kozak", "Marc Alban", "Mathieu Labbe", "Michael Ferguson", "P. J. Reed", "Philipp Kr\u00fcsi", "P\u00e9ter Fankhauser", "Ryohei Ueda", "Sergey Dorodnicov", "Vladimir Ermakov", "William Woodall", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-jade-and-kinetic-2016-10-24/722"},
{"title": "New Packages for Indigo 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of new packages for Indigo. We have 20 new packages as well as over 200 updates available.", "Full details are below. Thanks to all the contributors and maintainers who have made this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cob-helper-tools: 0.6.6-0", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0", "ros-indigo-cob-supported-robots: 0.6.7-0", "ros-indigo-generic-throttle: 0.6.6-0", "ros-indigo-grid-map-costmap-2d: 1.5.2-0", "ros-indigo-grid-map-octomap: 1.5.2-0", "ros-indigo-jsk-pr2-desktop: 1.1.0-1", "ros-indigo-julius-ros: 2.1.4-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0", "ros-indigo-libqsopt: 0.6.11-0", "ros-indigo-moveit-goal-builder: 0.1.0-0", "ros-indigo-opengm: 0.6.11-0", "ros-indigo-parameter-pa: 1.0.0-0", "ros-indigo-rostune: 1.0.5-1", "ros-indigo-safe-teleop-base: 0.0.2-0", "ros-indigo-safe-teleop-pr2: 0.0.2-0", "ros-indigo-safe-teleop-stage: 0.0.2-0", "ros-indigo-service-tools: 0.6.6-0", "ros-indigo-spin-hokuyo: 1.0.0-0", "ros-indigo-urdf-sim-tutorial: 0.3.0-0", "ros-indigo-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-indigo-baxtereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-indigo-can-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-402: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-chain-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-master: 0.6.7-0 -> 0.6.8-0", "ros-indigo-canopen-motor-node: 0.6.7-0 -> 0.6.8-0", "ros-indigo-checkerboard-detector: 1.1.2-0 -> 1.2.2-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-android: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-msgs: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-resource-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-script-server: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-android-settings: 0.1.2-0 -> 0.1.3-0", "ros-indigo-cob-base-drive-chain: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-base-velocity-smoother: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-bms-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-calibration-data: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-cam3d-throttle: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-camera-sensors: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-canopen-motor: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-cartesian-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-collision-velocity-filter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-command-gui: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-command-tools: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-common: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-control: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-mode-adapter: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-control-msgs: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-dashboard: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-env-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-docker-control: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-driver: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-elmo-homing: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-environments: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-extern: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-footprint-observer: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-frame-tracker: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-gazebo-plugins: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-gazebo-ros-control: 0.6.4-0 -> 0.6.6-0", "ros-indigo-cob-generic-can: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-hand: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-hand-bridge: 0.6.1-0 -> 0.6.2-0", "ros-indigo-cob-head-axis: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-image-flip: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-interactive-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-light: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-linear-nav: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mapping-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-mimic: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-model-identifier: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-monitoring: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-msgs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-navigation: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-config: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-global: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-local: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-navigation-slam: 0.6.4-0 -> 0.6.5-0", "ros-indigo-cob-object-detection-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-object-detection-visualizer: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-obstacle-distance: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-omni-drive-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-perception-common: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-perception-msgs: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-em-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidget-power-state: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-phidgets: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-reflector-referencing: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-relayboard: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-safety-controller: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-scan-unifier: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-script-server: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-sick-lms1xx: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sick-s300: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-sound: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-srvs: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-substitute: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-teleop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-trajectory-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-twist-controller: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-undercarriage-ctrl-node: 0.6.14-0 -> 0.6.15-0", "ros-indigo-cob-utilities: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-vision-utils: 0.6.8-0 -> 0.6.10-0", "ros-indigo-cob-voltage-control: 0.6.8-0 -> 0.6.10-0", "ros-indigo-collada-urdf-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-indigo-diff-drive-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-downward: 2.0.20-0 -> 2.1.4-0", "ros-indigo-effort-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-fetcheus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-ff: 2.0.20-0 -> 2.1.4-0", "ros-indigo-ffha: 2.0.20-0 -> 2.1.4-0", "ros-indigo-force-torque-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-forward-command-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-indigo-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-indigo-gripper-action-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-imagesift: 1.1.2-0 -> 1.2.2-0", "ros-indigo-imu-sensor-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-state-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-jsk-2015-05-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-201504-miraikan: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-2016-01-baxter-apc: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc2015-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-apc2016-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-arc2017-common: 3.0.3-0 -> 4.0.0-0", "ros-indigo-jsk-baxter-desktop: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-baxter-web: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-common-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-fetch-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-footstep-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-gui-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-hark-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-jsk-interactive: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-marker: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-interactive-test: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-nao-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pcl-ros: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pcl-ros-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pepper-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-perception: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-pr2-calibration: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-jsk-recognition: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-msgs: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-recognition-utils: 1.1.2-0 -> 1.2.2-0", "ros-indigo-jsk-robot: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-startup: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-robot-utils: 1.0.6-2 -> 1.1.0-1", "ros-indigo-jsk-rqt-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-rviz-plugins: 2.1.0-0 -> 2.1.2-0", "ros-indigo-jsk-visualization: 2.1.0-0 -> 2.1.2-0", "ros-indigo-julius: 2.0.20-0 -> 2.1.4-0", "ros-indigo-laser-filters-jsk-patch: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-libdlib: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libntcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libpcan: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libphidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-indigo-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-indigo-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-naoeus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-naoqieus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-indigo-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-indigo-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-indigo-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-indigo-parrot-arsdk: 3.11.0-0 -> 3.12.6-0", "ros-indigo-peppereus: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-indigo-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-indigo-posedetection-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-position-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-pr2-base-trajectory-action: 1.0.6-2 -> 1.1.0-1", "ros-indigo-pr2eus: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-moveit: 0.3.11-0 -> 0.3.13-0", "ros-indigo-pr2eus-tutorials: 0.3.11-0 -> 0.3.13-0", "ros-indigo-rail-manipulation-msgs: 0.0.9-0 -> 0.0.10-0", "ros-indigo-raw-description: 0.6.6-0 -> 0.6.7-0", "ros-indigo-resized-image-transport: 1.1.2-0 -> 1.2.2-0", "ros-indigo-ros-canopen: 0.6.7-0 -> 0.6.8-0", "ros-indigo-ros-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rosauth: 0.1.7-0 -> 0.1.7-1", "ros-indigo-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-indigo-roseus-remote: 1.0.6-2 -> 1.1.0-1", "ros-indigo-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-indigo-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rosping: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rostwitter: 2.0.20-0 -> 2.1.4-0", "ros-indigo-rqt-joint-trajectory-controller: 0.9.3-0 -> 0.9.4-0", "ros-indigo-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-indigo-schunk-description: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-libm5api: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-modular-robotics: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-powercube-chain: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-sdh: 0.6.8-0 -> 0.6.9-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.8-0 -> 0.6.9-0", "ros-indigo-self-test: 1.9.0-0 -> 1.9.2-0", "ros-indigo-slic: 2.0.20-0 -> 2.1.4-0", "ros-indigo-socketcan-bridge: 0.6.7-0 -> 0.6.8-0", "ros-indigo-socketcan-interface: 0.6.7-0 -> 0.6.8-0", "ros-indigo-speech-recognition-msgs: 4.2.0-0 -> 4.3.0-0", "ros-indigo-surface-perception: 0.1.1-0 -> 0.1.3-0", "ros-indigo-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-indigo-transform-graph: 0.1.4-0 -> 0.2.1-0", "ros-indigo-urdf-tutorial: 0.2.5-0 -> 0.3.0-0", "ros-indigo-velocity-controllers: 0.9.3-0 -> 0.9.4-0", "ros-indigo-voice-text: 2.0.20-0 -> 2.1.4-0", "ros-indigo-jsk-apc", "ros-indigo-jsk-arc2017-baxter", "Aaron Blasdel", "Adolfo Rodriguez Tsouroukdissian", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Bence Magyar", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "David Kent", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hasegawa Shun", "Hitoshi Kamada", "Jan Fischer", "Joshua Hampp", "Justin Huang", "KazutoMurase", "Kei Okada", "Kentaro Wada", "Mani Monajjemi", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Peter Weissig", "P\u00e9ter Fankhauser", "Richard Bormann", "Russell Toris", "Ryohei Ueda", "Sachin Chitta", "Sarah Bertussi", "Shohei Fujii", "Takuya Nakaoka", "Yohei Kakiuchi", "YoheiKakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yusuke Furuta", "Yuto Inagaki", "furuta", "inagaki", "k-okada"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2017-07-25/2306"},
{"title": "New Packages for Kinetic 2017-07-25", "thread_contents": ["We\u2019re happy to announce another set of packages for Kinetic as well. We have 86 new packages as well as 78 updated packages in this sync.", "Thank you to all the contributors and maintainers who make these packages available to the community.", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0", "ros-kinetic-cob-bms-driver: 0.6.10-0", "ros-kinetic-cob-calibration-data: 0.6.7-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0", "ros-kinetic-cob-common: 0.6.7-0", "ros-kinetic-cob-control: 0.7.0-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0", "ros-kinetic-cob-control-msgs: 0.7.0-0", "ros-kinetic-cob-default-env-config: 0.6.5-0", "ros-kinetic-cob-description: 0.6.7-0", "ros-kinetic-cob-docker-control: 0.6.6-0", "ros-kinetic-cob-driver: 0.6.10-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0", "ros-kinetic-cob-environments: 0.6.5-0", "ros-kinetic-cob-extern: 0.6.11-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0", "ros-kinetic-cob-generic-can: 0.6.10-0", "ros-kinetic-cob-hand: 0.6.2-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0", "ros-kinetic-cob-head-axis: 0.6.10-0", "ros-kinetic-cob-image-flip: 0.6.10-0", "ros-kinetic-cob-light: 0.6.10-0", "ros-kinetic-cob-mimic: 0.6.10-0", "ros-kinetic-cob-model-identifier: 0.7.0-0", "ros-kinetic-cob-msgs: 0.6.7-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0", "ros-kinetic-cob-perception-common: 0.6.10-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0", "ros-kinetic-cob-phidgets: 0.6.10-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0", "ros-kinetic-cob-relayboard: 0.6.10-0", "ros-kinetic-cob-safety-controller: 0.6.6-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0", "ros-kinetic-cob-sick-s300: 0.6.10-0", "ros-kinetic-cob-sound: 0.6.10-0", "ros-kinetic-cob-srvs: 0.6.7-0", "ros-kinetic-cob-substitute: 0.6.6-0", "ros-kinetic-cob-supported-robots: 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0", "ros-kinetic-cob-twist-controller: 0.7.0-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0", "ros-kinetic-cob-undercarriage-ctrl-node: 0.7.0-0", "ros-kinetic-cob-utilities: 0.6.10-0", "ros-kinetic-cob-vision-utils: 0.6.10-0", "ros-kinetic-cob-voltage-control: 0.6.10-0", "ros-kinetic-eband-local-planner: 0.3.1-0", "ros-kinetic-follow-waypoints: 0.3.0-2", "ros-kinetic-grid-map-costmap-2d: 1.5.2-0", "ros-kinetic-grid-map-octomap: 1.5.2-0", "ros-kinetic-image-overlay-scale-and-compass: 0.2.1-0", "ros-kinetic-julius-ros: 2.1.4-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0", "ros-kinetic-libdlib: 0.6.11-0", "ros-kinetic-libntcan: 0.6.11-0", "ros-kinetic-libpcan: 0.6.11-0", "ros-kinetic-libphidgets: 0.6.11-0", "ros-kinetic-libqsopt: 0.6.11-0", "ros-kinetic-opengm: 0.6.11-0", "ros-kinetic-raw-description: 0.6.7-0", "ros-kinetic-rostune: 1.0.5-1", "ros-kinetic-safe-teleop-base: 0.0.2-0", "ros-kinetic-safe-teleop-stage: 0.0.2-0", "ros-kinetic-schunk-description: 0.6.9-0", "ros-kinetic-schunk-libm5api: 0.6.9-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0", "ros-kinetic-schunk-sdh: 0.6.9-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0", "ros-kinetic-soem: 1.3.0-0", "ros-kinetic-urdf-geometry-parser: 0.0.2-0", "ros-kinetic-urdf-sim-tutorial: 0.3.0-1", "ros-kinetic-aruco-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-assimp-devel: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bayesian-belief-networks: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-bin-pose-emulator: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-bin-pose-msgs: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-binpicking-utils: 0.1.2-0 -> 0.1.3-0", "ros-kinetic-checkerboard-detector: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-default-cfg-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-analysis: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-common-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostic-updater: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-diagnostics: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-downward: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ff: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-ffha: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-fiducial-detect: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-lib: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-msgs: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-pose: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducial-slam: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-fiducials: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-grid-map: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-core: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-cv: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-demos: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-filters: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-loader: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-msgs: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-pcl: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-ros: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-rviz-plugin: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-grid-map-visualization: 1.4.2-0 -> 1.5.2-0", "ros-kinetic-imagesift: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-3rdparty: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-jsk-pcl-ros: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-pcl-ros-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-perception: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-msgs: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-jsk-recognition-utils: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-julius: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libcmt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-libsiftfast: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-lpg-planner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-magni-bringup: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-demos: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-description: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-nav: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-robot: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-magni-teleop: 0.1.0-0 -> 0.1.1-0", "ros-kinetic-master-discovery-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-master-sync-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-mavlink: 2017.6.6-0 -> 2017.7.7-0", "ros-kinetic-mini-maxwell: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-move-basic: 0.2.0-0 -> 0.2.1-0", "ros-kinetic-multimaster-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multimaster-msgs-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-multiwii: 2.0.0-0 -> 2.0.1-0", "ros-kinetic-nlopt: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-node-manager-fkie: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-opencv-apps: 1.11.15-0 -> 1.12.0-0", "ros-kinetic-opt-camera: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-pgm-learner: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-plotjuggler: 1.1.2-0 -> 1.1.3-0", "ros-kinetic-resized-image-transport: 1.1.1-0 -> 1.1.3-0", "ros-kinetic-rosdiagnostic: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-rosjava-core: 0.3.4-1 -> 0.3.5-0", "ros-kinetic-roslisp: 1.9.20-0 -> 1.9.21-0", "ros-kinetic-rospatlite: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rosping: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-rqt-tf-tree: 0.5.7-0 -> 0.5.8-0", "ros-kinetic-self-test: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-slic: 2.0.20-0 -> 2.1.4-0", "ros-kinetic-test-diagnostic-aggregator: 1.9.0-0 -> 1.9.2-0", "ros-kinetic-thormang3-tools: 0.1.0-0 -> 0.1.2-0", "ros-kinetic-urdf-tutorial: 0.2.5-0 -> 0.3.0-1", "ros-kinetic-voice-text: 2.0.20-0 -> 2.1.4-0", "Aaron Blasdel", "Alexander Bubeck", "Alexander Tiderko", "Austin Hendrix", "Benjamin Maidel", "Brice Rebsamen", "Charles DuHadway (maintained by Benjamin Pitzer)", "Christian Rauch", "Damon Kohler", "Daniel Snider", "David V. Lu!!", "Davide Faconti", "Felix Messmer", "Florian Weisshardt", "Frantisek Durovsky", "Georg Bartels", "George Stavrinos", "Guillaume Autran", "Hitoshi Kamada", "Jan Fischer", "Jim Vaughan", "Joshua Hampp", "Kei Okada", "Mathias Luedtke", "Mathias L\u00fcdtke", "Matthias Gruhler", "Matthias Luedtke", "Nadia Hammoudeh Garcia", "Noda Shintaro", "Piyush Khandelwal", "Pyo", "P\u00e9ter Fankhauser", "Richard Bormann", "Rohan Agrawal", "Ruben Smits", "Ryohei Ueda", "Takuya Nakaoka", "Vincent Rousseau", "Vladimir Ermakov", "Yohei Kakiuchi", "Youhei Kakiuchi", "Yuki Furuta", "Yuto Inagaki", "durovsky"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2017-07-25/2307"},
{"title": "Roboware for ROS beginner dealing with BLDC controller and ROS melodic", "thread_contents": ["hi guys,", "when i was trying to start, first thing in my mind is like \u201chow ROS handle the motor controller?\u201d coz so far as i know (CMIIW), not all BLDC controller is ROS friendly.", "\nso i can only place my bet for any controller which has protocol like CANopen and RS232. trying to plug the controller directly to PC (like intel NUC as the brain of the robot) as well as cheap LIDAR sensor and hoping it will auto magically work by the time ROS installed.", "\n*idiot mode. i know\u2026 i\u2019m very newbie, with only 2 weeks experience in ROS ", "just wondering if roboware is compatible for the latest ROS melodic version and ubuntu 18.04.", "\nfiguring out to work with roboware designer and studio to deal with a chinese BLDC controller. is it possible?", "\nespecially roboware and BLDC controller from the same city in jinan ", "i see chance in roboware designer for any newbies like me to play with simple autonomous robot using ROS. but yes, are all controllers (BLDC, brushed DC) and cheap sensors compatible with roboware and ROS?", "i saw many great example using state of the art lidar (sick, hokuyo, and even $4K velodyne for sale which is still obviously out of my wallet ", " ) while i have eyes only on $200-300 2D lidar (they have ROS driver but still don\u2019t have any idea, but at least i have hope as they said they have ROS driver ", "  ) and intel realsense for 3D vision (i\u2019m sure this camera will work as i saw it in many threats although i\u2019ve never tried this until today).", "so here is my steps as newbie from scratch to autonomous wheeled robot:", "do you think my steps are correct? or i\u2019m still lost in the jungle and still long way to go?", "\nplease kindly guide me by giving me a clue, even a small clue or direction is really appreciated.", "anyway, thanks for creating such this great tool.", "\ni do really appreciate it.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["install ubuntu and ROS (18.04 ubuntu and ROS melodic? or just keep it with kinetic for now?)", "install roboware", "connect BLDC controller to any usb port or serial port (sure connect the motor to the controller)", "connect lidar to any usb port", "connect intel realsense camera to usb 3.0 port", "get the power/battery on", "and play with roboware for doing small baby steps"], "url": "https://discourse.ros.org/t/roboware-for-ros-beginner-dealing-with-bldc-controller-and-ros-melodic/4915"},
{"title": "New Packages for Indigo 2018-07-26", "thread_contents": ["We\u2019re happy to announce 20 new packages and 149 updated packages for Indigo Igloo today.", "Thank you to all the maintainers and contributors who have helped make these available. Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-costmap-cspace: 0.2.3-0", "ros-indigo-ibeo-lux: 2.0.0-0", "ros-indigo-joystick-interrupt: 0.2.3-0", "ros-indigo-map-organizer: 0.2.3-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "ros-indigo-neonavigation: 0.2.3-0", "ros-indigo-neonavigation-common: 0.2.3-0", "ros-indigo-neonavigation-launch: 0.2.3-0", "ros-indigo-obj-to-pointcloud: 0.2.3-0", "ros-indigo-planner-cspace: 0.2.3-0", "\n", ": 0.0.4-1", "ros-indigo-pybind11-catkin: 2.2.3-3", "ros-indigo-safety-limiter: 0.2.3-0", "ros-indigo-track-odometry: 0.2.3-0", "ros-indigo-trajectory-tracker: 0.2.3-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-bms-driver: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-cartesian-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-control-mode-adapter: 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-control-msgs: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-indigo-cob-default-robot-behavior: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-elmo-homing: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-frame-tracker: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.3-0 -> 0.6.5-0", "ros-indigo-cob-hand-bridge: 0.6.3-0 -> 0.6.5-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-helper-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-model-identifier: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-indigo-cob-moveit-config: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-msgs: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-object-detection-visualizer: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-obstacle-distance: 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-omni-drive-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-phidget-em-state: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-phidget-power-state: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-scan-unifier: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.16-0 -> 0.6.17-0", "ros-indigo-cob-twist-controller: 0.6.16-0 -> 0.6.17-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 3.5.4-0 -> 3.6.2-0", "ros-indigo-eus-assimp: 0.3.5-0 -> 0.4.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 9.23.0-0 -> 9.25.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-depth-layer: 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-description: 0.7.13-0 -> 0.7.14-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "ros-indigo-fetch-ikfast-plugin: 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-maps: 0.7.13-0 -> 0.7.14-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "ros-indigo-fetch-navigation: 0.7.13-0 -> 0.7.14-0", "\n", ": 0.7.13-0 -> 0.7.14-0", "\n", ": 0.1.4-0 -> 0.1.5-0", "ros-indigo-generic-throttle: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.11.3-0 -> 1.11.4-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.1.0-0 -> 1.2.0-2", "\n", ": 1.11.14-0 -> 1.11.15-0", "\n", ": 1.11.14-0 -> 1.11.15-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "ros-indigo-novatel-gps-driver: 3.4.0-0 -> 3.5.0-0", "ros-indigo-novatel-gps-msgs: 3.4.0-0 -> 3.5.0-0", "ros-indigo-openni2-camera: 0.3.0-0 -> 0.4.0-0", "ros-indigo-openni2-launch: 0.3.0-0 -> 0.4.0-0", "\n", ": 0.0.24-0 -> 0.0.27-0", "\n", ": 1.6.2-0 -> 1.7.1-1", "\n", ": 0.2.0-0 -> 0.2.1-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-indigo-rc-genicam-api: 1.3.11-0 -> 1.3.12-0", "ros-indigo-robot-controllers: 0.5.3-0 -> 0.5.4-0", "ros-indigo-robot-controllers-interface: 0.5.3-0 -> 0.5.4-0", "ros-indigo-robot-controllers-msgs: 0.5.3-0 -> 0.5.4-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "ros-indigo-roseus-mongo: 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "ros-indigo-service-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "ros-indigo-tf2-eigen: 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.1.2-0 -> 1.1.4-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "\n", ": 1.0.6-0 -> 1.0.9-0", "ros-indigo-xsens-driver: 2.1.0-0 -> 2.2.0-3", "ros-indigo-freight-calibration", "Alexander Bubeck", "Alexander Tiderko", "Alexander W. Winkler", "Andy Zelenak", "Atsushi Watanabe", "AutonomouStuff Software Development Team", "Benjamin Maidel", "Bruno Brito", "Chris Lalancette", "Davide Faconti", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florian Weisshardt", "Francis Colas", "Isaac I. Y. Saito", "Jannik Abbenseth", "Joshua Hampp", "Kei Okada", "Koji Terada", "Martin G\u00fcnther", "Matthias Gruhler", "Michael Ferguson", "P. J. Reed", "Pyo", "Richard Bormann", "Ron Tajima", "Russell Toris", "Sammy Pfeiffer", "Tully Foote", "Vincent Rabaud", "Vladimir Ivan", "William Woodall", "Yohei Kakiuchi", "Yuki Furuta"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2018-07-26/5506"},
{"title": "Using ROS on Nao", "thread_contents": ["Hi there, I\u2019m investigating using ROS with Nao robots for a project but have a few questions that I can\u2019t seem to find answers for online.", "First, do the official ROS Nao packages support Nao v4 or are v5 required?", "Second, it looks like it is possible to have ROS running directly on the robot itself, but is not the preferred method. How feasible actually is it to have ROS installed and running on the robot? How well does it work? Are there pre-compiled binaries for it or would we need to cross-compile it, as suggested by one of the tutorials?", "Any advice about this would be greatly appreciated!", "//Mike", "Hi ", ",", "Thanks for your question.", "\nHowever ROS Discourse is for news and general interest discussions. ROS Answers", "\n", " provides a forum which can be filtered by tags to", "\nmake sure the relevant people can find and/or answer the question, and not", "\noverload everyone with hundreds of posts. So we recommend users to ask their questions there following our support guidelines:", "\n", ".", "\nFor NAO (or Pepper/Romeo) related question you may also find answers to your questions on the dedicated mailing list: ", "To not leave you empty handed ", " :", "Hope this helps,", "Thanks for the tip about answers, and thanks for the information, very useful!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS Nao packages should work on v4 and v5", "Nao has a very limited processing power so running the ROS machinery on top of the Naoqi one gets challenging. But is you don\u2019t do expensive processing you should be able to run some nodes directly on the robot. Some people give additional computing power to Nao to avoid the remote processing (see the ", " project for example)", "There are no pre-compiled binaries AFAIK, but you can use the OpenNao VM to compile your packages for the target architecture"], "url": "https://discourse.ros.org/t/using-ros-on-nao/4110"},
{"title": "New Packages for Kinetic 2018-01-09", "thread_contents": ["We\u2019re happy to announce 22 new packages and 175 updated packages. This includes the restoration of the regressed costmap packages from the last sync.", "Thank you to all the maintainers and contributors who have helped make this possible.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Added Packages", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "\nNow I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "If others already know a short way to find information, please share.", "Cheers", " was bus_server supposed to make it into this sync? I removed it from the kinetic rosdistro after the rename to ", ".", "EDIT: Looks like I forgot to rename the dependency in loki_bringup, ran a new release. Is there anything else needed to ensure proper deletion?", "Is there a way to add hyperlinks to package home pages that would provide some summary what that package is about?", "Now I type google search (example: bus-server), go through a few pages that are unrelated, like general ROS \u201cpackage\u201d installation, and finally hit something usable after 6 links that the search returned. It can be easier than that.", "I totally agree with ", ", I also searched on Google for several packages without direct success.", "The default location to find a package is on the ROS wiki with the package name. So for package ", " which is packaged into the debian package ", " see ", "I opened a ", " as a link in the summary last week. We are targeting to roll it out before the next sync.", "WRT to bus_server as ", " alluded, that was a package ", " but after review was renamed to ", " which is why you can\u2019t find any documentation. There\u2019s an outstanding ticket to ", " but we haven\u2019t had time to implement it.", "The default location to find a package is on the ROS wiki with the package name. So for package cost_map which is packaged into the debian package ros-kinetic-cost-map see ", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "I opened a PR to embed the declared homepage as a link in the summary last week. We are targeting to roll it out before the next sync.", "Very good news, thanks!", "Yes, I know, but it is not always available. A fallback to the Github repo would be nice too.", "An alternative is to check the ", " automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "\nexample: ", " package", "\n", " provides a link to the ", " and the ", "In the case of packages that don\u2019t have a source of a doc entry (like ", ") the status page provides only a link to the ", ".", "On a related note, there\u2019s an unmerged PR that aims to encourage package releasers to create pages on ROS wiki ", " (I haven\u2019t had time to make an improvement).", "An alternative is to check the status pages automatically generated by the ROS buildfarm. They provide a complete list of all released packages including a link to the source repository as well as a link to the wiki page for each package if it exists.", "Oh, really interesting.", "\nI did not know that page.", "\nThanks!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-bus-server: 0.1.1-0", "ros-kinetic-care-o-bot: 0.6.6-0", "ros-kinetic-care-o-bot-desktop: 0.6.6-0", "ros-kinetic-care-o-bot-robot: 0.6.6-0", "ros-kinetic-care-o-bot-simulation: 0.6.6-0", "ros-kinetic-cob-manipulation: 0.7.1-0", "ros-kinetic-cob-pick-place-action: 0.7.1-0", "ros-kinetic-cost-map: 0.3.3-0", "ros-kinetic-cost-map-core: 0.3.3-0", "ros-kinetic-cost-map-cv: 0.3.3-0", "ros-kinetic-cost-map-demos: 0.3.3-0", "ros-kinetic-cost-map-ros: 0.3.3-0", "ros-kinetic-cost-map-visualisations: 0.3.3-0", "ros-kinetic-darknet-ros-msgs: 1.1.2-0", "ros-kinetic-loki-base-node: 0.2.1-0", "ros-kinetic-loki-bringup: 0.0.1-0", "ros-kinetic-loki-demos: 0.0.1-0", "ros-kinetic-loki-description: 0.0.1-0", "ros-kinetic-loki-nav: 0.0.1-0", "ros-kinetic-loki-robot: 0.0.1-0", "ros-kinetic-loki-teleop: 0.0.1-0", "ros-kinetic-pcdfilter-pa: 1.2.0-0", "ros-kinetic-baldor: 0.1.1-0 -> 0.1.2-0", "ros-kinetic-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-kinetic-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-base-velocity-smoother: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-cartesian-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-monitor: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-collision-velocity-filter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-control: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-mode-adapter: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-control-msgs: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-docker-control: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-footprint-observer: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-frame-tracker: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-gazebo-plugins: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-ros-control: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-grasp-generation: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-kinetic-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-lookat-action: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-model-identifier: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-moveit-bringup: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-moveit-interface: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-kinetic-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-obstacle-distance: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-obstacle-distance-moveit: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-omni-drive-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-kinetic-cob-safety-controller: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-substitute: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-trajectory-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-twist-controller: 0.7.0-0 -> 0.7.1-0", "ros-kinetic-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-kinetic-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-kinetic-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-geometry2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-interactive-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-kobuki: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-auto-docking: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-bumper2pc: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-capabilities: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-controller-tutorial: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-description: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-keyop: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-node: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-random-walker: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-rapps: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-safety-controller: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-kobuki-testsuite: 0.7.4-0 -> 0.7.5-0", "ros-kinetic-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-librviz-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-mavlink: 2017.12.12-0 -> 2018.1.1-0", "ros-kinetic-moveit-visual-tools: 3.3.0-0 -> 3.4.0-0", "ros-kinetic-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-opengm: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-pose-cov-ops: 0.1.7-0 -> 0.2.0-0", "ros-kinetic-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-kinetic-robot-localization: 2.4.0-0 -> 2.4.2-0", "ros-kinetic-rviz: 1.12.14-0 -> 1.12.15-0", "ros-kinetic-rviz-plugin-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-rviz-python-tutorial: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-tf2: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-bullet: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-eigen: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-geometry-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-kdl: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-py: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-ros: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-sensor-msgs: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-tf2-tools: 0.5.16-0 -> 0.5.17-0", "ros-kinetic-universal-robot: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-bringup: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-description: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-driver: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-gazebo: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-kinematics: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur-msgs: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur10-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur3-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-ur5-moveit-config: 1.2.0-0 -> 1.2.1-0", "ros-kinetic-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-kinetic-visualization-marker-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-visualization-tutorials: 0.10.1-0 -> 0.10.2-0", "ros-kinetic-xpp: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-kinetic-cob-head-axis", "ros-kinetic-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "D. Hood", "Daniel Stonier", "Dave Coleman", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jorge Santos Simon", "Jose-Luis Blanco-Claraco", "Joshua Hampp", "Kei Okada", "Koji Terada", "Marcus Liebhardt", "Marko Bjelonic", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Rohan Agrawal", "Ryohei Ueda", "Tom Moore", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Wayne Gramlich", "William Woodall", "YoheiKakiuchi", "Younghun Ju"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2018-01-09/3641"},
{"title": "New Packages for Kinetic 2018-07-26", "thread_contents": ["We\u2019re happy to announce 35 new packages and 171 updated packages for Kinetic Kame. The full details are below.", "Thank you to the contributors and maintainers who have helped make these packages available to the community.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-kinetic-arduino-daq: 1.0.1-0", "\n", ": 0.1.2-0", "ros-kinetic-costmap-cspace: 0.2.3-0", "\n", ": 1.3.1-0", "ros-kinetic-ibeo-lux: 2.0.0-0", "ros-kinetic-joystick-interrupt: 0.2.3-0", "ros-kinetic-map-organizer: 0.2.3-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "\n", ": 1.0.1-0", "ros-kinetic-neonavigation: 0.2.3-0", "ros-kinetic-neonavigation-common: 0.2.3-0", "ros-kinetic-neonavigation-launch: 0.2.3-0", "ros-kinetic-obj-to-pointcloud: 0.2.3-0", "\n", ": 0.2.1-0", "ros-kinetic-planner-cspace: 0.2.3-0", "ros-kinetic-pr2-gripper-sensor: 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 1.0.10-0", "\n", ": 0.2.1-0", "\n", ": 0.2.1-0", "\n", ": 0.2.1-0", "\n", ": 0.0.4-1", "ros-kinetic-pybind11-catkin: 2.2.3-0", "ros-kinetic-roseus-mongo: 1.7.1-0", "\n", ": 0.0.4-0", "\n", ": 0.0.1-1", "ros-kinetic-safety-limiter: 0.2.3-0", "\n", ": 1.3.2-0", "ros-kinetic-track-odometry: 0.2.3-0", "ros-kinetic-trajectory-tracker: 0.2.3-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-bms-driver: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-cartesian-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-control-mode-adapter: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-control-msgs: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-kinetic-cob-default-robot-behavior: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-elmo-homing: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-frame-tracker: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.3-0 -> 0.6.5-0", "ros-kinetic-cob-hand-bridge: 0.6.3-0 -> 0.6.5-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-helper-tools: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-model-identifier: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "ros-kinetic-cob-moveit-config: 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-msgs: 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-obstacle-distance: 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-omni-drive-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-phidget-em-state: 0.6.11-0 -> 0.6.12-0", "ros-kinetic-cob-phidget-power-state: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-cob-scan-unifier: 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.8-0 -> 0.6.9-0", "\n", ": 0.6.7-0 -> 0.6.9-0", "\n", ": 0.7.1-0 -> 0.7.2-0", "ros-kinetic-cob-twist-controller: 0.7.1-0 -> 0.7.2-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.6.11-0 -> 0.6.12-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 3.5.4-0 -> 3.6.2-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.2.0-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "\n", ": 0.3.1-0 -> 1.0.0-0", "ros-kinetic-eus-assimp: 0.3.5-0 -> 0.4.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 9.23.0-0 -> 9.25.0-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "ros-kinetic-generic-throttle: 0.6.7-0 -> 0.6.9-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 2.0.2-0 -> 2.0.4-0", "\n", ": 1.11.3-0 -> 1.11.4-0", "\n", ": 0.3.5-0 -> 0.4.0-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.1.0-0 -> 1.2.0-1", "\n", ": 1.12.10-0 -> 1.12.11-0", "\n", ": 1.12.10-0 -> 1.12.11-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 2018.6.6-0 -> 2018.7.18-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.26.0-0 -> 0.26.1-0", "\n", ": 0.1.24-0 -> 0.1.25-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "\n", ": 0.7.8-0 -> 0.8.0-0", "ros-kinetic-novatel-gps-driver: 3.4.0-0 -> 3.5.0-0", "ros-kinetic-novatel-gps-msgs: 3.4.0-0 -> 3.5.0-0", "ros-kinetic-openni2-camera: 0.3.0-0 -> 0.4.0-0", "ros-kinetic-openni2-launch: 0.3.0-0 -> 0.4.0-0", "\n", ": 0.0.25-0 -> 0.0.27-0", "\n", ": 1.6.2-0 -> 1.7.1-1", "\n", ": 0.6.8-0 -> 0.6.9-0", "ros-kinetic-rc-genicam-api: 1.3.11-0 -> 1.3.12-0", "\n", ": 0.5.5-0 -> 0.6.0-0", "\n", ": 0.5.5-0 -> 0.6.0-0", "ros-kinetic-robotnik-msgs: 0.2.3-0 -> 0.2.4-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 1.6.3-0 -> 1.7.1-0", "\n", ": 0.4.11-0 -> 0.4.13-0", "\n", ": 1.1.6-0 -> 1.1.7-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "\n", ": 0.6.10-0 -> 0.6.11-0", "ros-kinetic-service-tools: 0.6.7-0 -> 0.6.9-0", "ros-kinetic-test-mavros: 0.26.0-0 -> 0.26.1-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "ros-kinetic-tf2-eigen: 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 0.5.17-0 -> 0.5.18-0", "\n", ": 1.2.2-0 -> 1.3.2-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.2-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "\n", ": 1.0.0-0 -> 1.1.0-0", "ros-kinetic-tuw-airskin-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-gazebo-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-geometry-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-multi-robot-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-nav-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-object-msgs: 0.0.7-3 -> 0.0.8-1", "ros-kinetic-tuw-vehicle-msgs: 0.0.7-3 -> 0.0.8-1", "\n", ": 1.1.2-0 -> 1.1.4-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "\n", ": 1.0.7-0 -> 1.0.8-0", "ros-kinetic-xsens-driver: 2.1.0-0 -> 2.2.0-0", "Alexander Bubeck", "Alexander Tiderko", "Alexander W. Winkler", "Andreas ten Pas", "Andy Zelenak", "Angel Soriano", "Atsushi Watanabe", "AutonomouStuff Software Development Team", "Benjamin Binder", "Benjamin Maidel", "Brenden Gibbons", "Bruno Brito", "Chris Lalancette", "Davide Faconti", "Dirk Thomas", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Felix Zeltner", "Florian Weisshardt", "Francis Colas", "George Todoran", "Isaac I. Y. Saito", "Jack Kilian", "Jannik Abbenseth", "Jose Luis Blanco Claraco", "Jose-Luis Blanco-Claraco", "Joshua Hampp", "Kei Okada", "Koji Terada", "Markus Bader", "Martin G\u00fcnther", "Matthias Gruhler", "Michael Ferguson", "P. J. Reed", "Pilz GmbH and Co. KG", "Pyo", "ROS Orphaned Package Maintainers", "Raphael Hauk", "Richard Bormann", "Ron Tajima", "Sammy Pfeiffer", "Thomas Le M\u00e9zo", "Tully Foote", "Vincent Rabaud", "Vladimir Ermakov", "Vladimir Ivan", "William Woodall", "Yohei Kakiuchi", "Yuki Furuta", "nick fragale"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2018-07-26/5507"},
{"title": "New Packages for Indigo 2018-01-09", "thread_contents": ["We\u2019re happy to anounce 6 new packages and 168 updated packages for Indigo.", "Thanks as always to all the contributors and maintainers. The list of releases and maintainers is below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ros-indigo-cost-map: 0.3.3-0", "ros-indigo-cost-map-core: 0.3.3-0", "ros-indigo-cost-map-cv: 0.3.3-0", "ros-indigo-cost-map-demos: 0.3.3-0", "ros-indigo-cost-map-ros: 0.3.3-0", "ros-indigo-cost-map-visualisations: 0.3.3-0", "ros-indigo-baldor: 0.1.1-0 -> 0.1.2-0", "ros-indigo-care-o-bot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-desktop: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-robot: 0.6.5-0 -> 0.6.6-0", "ros-indigo-care-o-bot-simulation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-3d-mapping-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-android: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-msgs: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-resource-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-script-server: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-android-settings: 0.1.3-0 -> 0.1.4-0", "ros-indigo-cob-base-drive-chain: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-base-velocity-smoother: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-bms-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-bringup: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-bringup-sim: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-calibration-data: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-cam3d-throttle: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-camera-sensors: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-canopen-motor: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-cartesian-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-collision-monitor: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-collision-velocity-filter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-command-gui: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-command-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-common: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-control: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-mode-adapter: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-control-msgs: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-dashboard: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-default-env-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-default-robot-behavior: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-default-robot-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-docker-control: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-driver: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-elmo-homing: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-environments: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-extern: 0.6.11-0 -> 0.6.12-0", "ros-indigo-cob-footprint-observer: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-frame-tracker: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-gazebo: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-objects: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-gazebo-plugins: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-ros-control: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-gazebo-worlds: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-generic-can: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-grasp-generation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-hand: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hand-bridge: 0.6.2-0 -> 0.6.3-0", "ros-indigo-cob-hardware-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-helper-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-image-flip: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-interactive-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-kinematics: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-light: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-linear-nav: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-lookat-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-manipulation: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-map-accessibility-analysis: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mapping-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-mimic: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-model-identifier: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-monitoring: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-moveit-bringup: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-moveit-config: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-moveit-interface: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-msgs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-navigation: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-config: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-global: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-local: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-navigation-slam: 0.6.5-0 -> 0.6.6-0", "ros-indigo-cob-object-detection-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-object-detection-visualizer: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-obstacle-distance: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-obstacle-distance-moveit: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-omni-drive-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-perception-common: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-perception-msgs: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-em-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidget-power-state: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-phidgets: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-pick-place-action: 0.6.5-1 -> 0.6.6-1", "ros-indigo-cob-reflector-referencing: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-relayboard: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-robots: 0.6.7-1 -> 0.6.8-0", "ros-indigo-cob-safety-controller: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-scan-unifier: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-script-server: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-sick-lms1xx: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-sick-s300: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-simulation: 0.6.8-0 -> 0.6.9-0", "ros-indigo-cob-sound: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-srvs: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-substitute: 0.6.6-0 -> 0.6.7-1", "ros-indigo-cob-supported-robots: 0.6.7-0 -> 0.6.8-0", "ros-indigo-cob-teleop: 0.6.6-0 -> 0.6.7-0", "ros-indigo-cob-trajectory-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-twist-controller: 0.6.15-0 -> 0.6.16-0", "ros-indigo-cob-undercarriage-ctrl: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-utilities: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-vision-utils: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cob-voltage-control: 0.6.10-0 -> 0.6.11-0", "ros-indigo-cost-map-msgs: 0.3.2-0 -> 0.3.3-0", "ros-indigo-dynamic-tf-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-gazebo-grasp-plugin: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-state-plugins: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-test-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-gazebo-world-plugin-loader: 1.0.1-0 -> 1.0.2-0", "ros-indigo-generic-throttle: 0.6.6-0 -> 0.6.7-0", "ros-indigo-grasp-planning-graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-msgs: 1.1.2-0 -> 1.2.0-0", "ros-indigo-grasp-planning-graspit-ros: 1.1.2-0 -> 1.2.0-0", "ros-indigo-graspit-tools: 1.1.2-0 -> 1.2.0-0", "ros-indigo-image-view2: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jaco-graspit-sample: 1.1.2-0 -> 1.2.0-0", "ros-indigo-jsk-common: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-data: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-network-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tilt-laser: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-jsk-topic-tools: 2.2.5-0 -> 2.2.6-0", "ros-indigo-libconcorde-tsp-solver: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libdlib: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libntcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libpcan: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libphidgets: 0.6.11-0 -> 0.6.12-0", "ros-indigo-libqsopt: 0.6.11-0 -> 0.6.12-0", "ros-indigo-moveit-controller-multidof: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-object-handling: 1.0.0-0 -> 1.0.1-0", "ros-indigo-moveit-planning-helper: 1.0.0-0 -> 1.0.1-0", "ros-indigo-multi-map-server: 2.2.5-0 -> 2.2.6-0", "ros-indigo-opengm: 0.6.11-0 -> 0.6.12-0", "ros-indigo-pcdfilter-pa: 1.1.0-0 -> 1.2.0-0", "ros-indigo-raw-description: 0.6.7-0 -> 0.6.8-0", "ros-indigo-schunk-description: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-libm5api: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-modular-robotics: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-powercube-chain: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-sdh: 0.6.9-0 -> 0.6.10-0", "ros-indigo-schunk-simulated-tactile-sensors: 0.6.9-0 -> 0.6.10-0", "ros-indigo-service-tools: 0.6.6-0 -> 0.6.7-0", "ros-indigo-universal-robot: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-bringup: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-description: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-driver: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-gazebo: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-kinematics: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur-msgs: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur10-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur3-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-ur5-moveit-config: 1.1.10-1 -> 1.1.11-0", "ros-indigo-urdf-processing-tools: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-transform: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-traverser: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf-viewer: 1.0.1-0 -> 1.0.2-0", "ros-indigo-urdf2graspit: 1.1.2-0 -> 1.2.0-0", "ros-indigo-urdf2inventor: 1.0.1-0 -> 1.0.2-0", "ros-indigo-virtual-force-publisher: 2.2.5-0 -> 2.2.6-0", "ros-indigo-visp: 3.0.1-1 -> 3.1.0-2", "ros-indigo-xpp: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-examples: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-hyq: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-msgs: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-quadrotor: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-states: 1.0.3-0 -> 1.0.4-0", "ros-indigo-xpp-vis: 1.0.3-0 -> 1.0.4-0", "ros-indigo-cob-head-axis", "ros-indigo-cob-undercarriage-ctrl-node", "Alexander Bubeck", "Alexander W. Winkler", "Benjamin Maidel", "Bruno Brito", "Daniel Stonier", "Fabien Spindler", "Felipe Garcia Lopez", "Felix Messmer", "Felix Zeltner", "Felx Messmer", "Florian Weisshardt", "Francisco Suarez-Ruiz", "Jannik Abbenseth", "Jennifer Buehler", "Joshua Hampp", "Kei Okada", "Matthias Gruhler", "Peter Weissig", "Richard Bormann", "Ryohei Ueda", "YoheiKakiuchi"], "url": "https://discourse.ros.org/t/new-packages-for-indigo-2018-01-09/3639"},
{"title": "New Packages for Kinetic 2019-03-25", "thread_contents": ["We\u2019re happy to announce 24 new packages and 155 updated packages in this sync.", "Thank you to all the contributors and maintainers who have made these packages available to the community!", "Full details are below.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 1.2.2-0", "\n", ": 0.6.10-0", "ros-kinetic-cob-base-controller-utils: 0.7.3-0", "ros-kinetic-cob-tricycle-controller: 0.7.3-0", "ros-kinetic-ddynamic-reconfigure: 0.1.7-0", "ros-kinetic-dynamic-robot-state-publisher: 1.1.1-0", "ros-kinetic-indoor-localization: 0.1.0-1", "ros-kinetic-ipa-3d-fov-visualization: 0.6.13-0", "ros-kinetic-laser-scan-densifier: 0.6.13-0", "\n", ": 1.0.3-0", "ros-kinetic-ouster-driver: 0.1.6-0", "\n", ": 1.0.2-0", "\n", ": 1.0.2-0", "\n", ": 1.0.2-0", "ros-kinetic-pr2-navigation-apps: 1.0.2-0", "ros-kinetic-rdl-msgs: 1.1.0-0", "ros-kinetic-rdl-ros-tools: 1.1.0-0", "ros-kinetic-static-transform-mux: 1.1.0-0", "ros-kinetic-tf-remapper-cpp: 1.1.1-0", "ros-kinetic-usb-cam-controllers: 0.0.3-0", "ros-kinetic-usb-cam-hardware: 0.0.3-0", "ros-kinetic-usb-cam-hardware-interface: 0.0.3-0", "ros-kinetic-uuv-descriptions: 0.6.10-0", "ros-kinetic-uuv-simulator: 0.6.10-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "ros-kinetic-behaviortree-cpp-v3: 3.0.1-0 -> 3.0.6-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "ros-kinetic-care-o-bot: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-desktop: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-robot: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-care-o-bot-simulation: 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cloudwatch-logs-common: 1.0.0-0 -> 1.0.1-1", "ros-kinetic-cloudwatch-metrics-common: 1.0.0-0 -> 1.0.1-1", "ros-kinetic-cmake-modules: 0.4.1-0 -> 0.4.2-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-bms-driver: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-cartesian-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-control-mode-adapter: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-control-msgs: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "ros-kinetic-cob-default-robot-behavior: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-elmo-homing: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.6-0 -> 0.6.7-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-frame-tracker: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-helper-tools: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-model-identifier: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-moveit-config: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-msgs: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-object-detection-visualizer: 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-obstacle-distance: 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-omni-drive-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-phidget-em-state: 0.6.12-0 -> 0.6.13-0", "ros-kinetic-cob-phidget-power-state: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-cob-scan-unifier: 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.6.9-0 -> 0.6.10-0", "\n", ": 0.7.2-0 -> 0.7.3-0", "ros-kinetic-cob-twist-controller: 0.7.2-0 -> 0.7.3-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 0.6.12-0 -> 0.6.13-0", "\n", ": 1.1.3-0 -> 1.1.4-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.0.0-0 -> 1.0.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.1.0-0 -> 1.1.1-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.1-1 -> 1.2.0-0", "ros-kinetic-generic-throttle: 0.6.9-0 -> 0.6.10-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "ros-kinetic-linux-networking: 1.0.12-0 -> 1.0.14-0", "\n", ": 2019.2.2-0 -> 2019.3.3-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "\n", ": 1.0.2-0 -> 1.0.3-0", "ros-kinetic-movie-publisher: 1.2.1-0 -> 1.2.2-1", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 1.0.12-0 -> 1.0.14-0", "\n", ": 2.1.5-0 -> 2.1.7-0", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 1.0.12-0 -> 1.0.12-1", "\n", ": 0.6.9-0 -> 0.6.10-0", "ros-kinetic-rdl: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-benchmark: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-cmake: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-dynamics: 1.0.0-0 -> 1.1.0-0", "ros-kinetic-rdl-urdfreader: 1.0.0-0 -> 1.1.0-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 0.10.1-0 -> 0.10.2-0", "\n", ": 1.12.16-0 -> 1.12.17-0", "ros-kinetic-service-tools: 0.6.9-0 -> 0.6.10-0", "\n", ": 0.0.14-0 -> 0.0.15-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "\n", ": 0.7.9-0 -> 0.7.10-0", "ros-kinetic-test-mavros: 0.29.0-0 -> 0.29.2-0", "\n", ": 1.0.0-1 -> 1.0.1-0", "ros-kinetic-uuv-assistants: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-auv-control-allocator: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-cascaded-pid: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-control-utils: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-gazebo-worlds: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-sensor-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-sensor-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-teleop: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-thruster-manager: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-trajectory-control: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-ros-plugins: 0.6.9-0 -> 0.6.10-0", "ros-kinetic-uuv-world-ros-plugins-msgs: 0.6.9-0 -> 0.6.10-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "\n", ": 1.0.8-0 -> 1.0.9-0", "AWS RoboMaker", "Alexander Bubeck", "Alexander Carballo", "Andres Palomino", "Benjamin Maidel", "D. Hood", "Davide Faconti", "Devon Ash", "Elcin Erdogan", "Felipe Garcia Lopez", "Felix Messmer", "Florenz Graf", "Florian Weisshardt", "Hilario Tome", "Jannik Abbenseth", "Joshua Hampp", "Kevin Hallenbeck", "Luiz Ricardo Douat", "Marko Bjelonic", "Martin G\u00fcnther", "Martin Pecka", "Mathias L\u00fcdtke", "Matthias Gruhler", "Micho Radovnikovich", "Musa Morena Marcusso Manhaes", "Richard Bormann", "Ronald Ensing", "Russell Toris", "Vladimir Ermakov", "William Woodall", "jordan", "yoshito"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-03-25/8455"},
{"title": "Connect two usb2.0 cameras with autoware", "thread_contents": [": ROS Kinetic", "\nubuntu 16.04", "Hi! I encountered a problem when I tried to connect two usb 2.0 cameras with autoware. It seems that autoware always just can recognize /dev/video0, which means autoware just can recognize one camera all the time. Is there someone who knows how to make autoware recognize another camera like /dev/video2 in my case? Autoware still can recognize one camera even after I  deleted the contents of launch file in ros camera driver or the contents of uvc_camera.sh under /Autoware/ros/src/sensing/drivers/camera/usb, which makes me be stuck at the point where I do not know which file I should change so that two cameras can be used at the same time. I would appreciate it a lot if someone can help me figure this out. Please let me know if you need any details I left out here about this problem.", "My ubuntu 16.04 system will show /dev/video0 and /dev/video1 when I just connect one camera to my computer. I guess that it is probably due to the microphone built in my usb camera.", "USB Genric checkbox in Runtime Manager launches uvc_camera.launch under /Autoware/ros/src/util/packages/runtime_manager/scripts.", " Like you mentioned, the current launch script in runtime manager, and the one in the drivers section, are not designed to work with more than one camera.", "\nHowever, you can instead use the launch file provided by the package, and select which device to use", "\n", "\n", "\nFrom a terminal you might execute:", "\n", "\nand select the device you are interested to use.", "\nFinally, you can find more details in the package documentation: ", "\nHope this helps.", "Thank you for your help. It works now after I changed a little bit of the launch file under util directory.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["<node pkg=\"uvc_camera\" type=\"uvc_camera_node\" name=\"uvc_camera\" output=\"screen\">", "  <param name=\"width\" type=\"int\" value=\"320\" />", "  <param name=\"height\" type=\"int\" value=\"240\" />", "  <param name=\"fps\" type=\"int\" value=\"30\" />", "  <param name=\"frame\" type=\"string\" value=\"wide_stereo\" />", "\n", "  <param name=\"auto_focus\" type=\"bool\" value=\"False\" />", "  <param name=\"focus_absolute\" type=\"int\" value=\"0\" />", "  <!-- other supported params: auto_exposure, exposure_absolute, brightness, power_line_frequency -->", "\n", "  <param name=\"device\" type=\"string\" value=\"/dev/video0\" />", "  <param name=\"camera_info_url\" type=\"string\" value=\"file://$(find uvc_camera)/example.yaml\" />", "</node>", "</launch>"], "url": "https://discourse.ros.org/t/connect-two-usb2-0-cameras-with-autoware/8013"},
{"title": "Autoware Calibration Toolkit: Green Circle Too Small", "thread_contents": ["Thanks for using Autoware and for your question. Howevere ask that you please ask questions at the ", " following our ", ". Please pay particular attention to the information we ask you to provide.", "Discourse is for news and general interest discussion. ", " provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["I want to calibrate my camera using Velodyne.", "As title mentioned, when I Grab, the green circle to extract Lidar points is too small then I cannot calibrate it accurately (like the botton right figure show below) because it can only extract couple of points.", "I referred to the YouTube guidelines and the green circle it used is bigger enough to extract many points on the chessboard.", "Is there any ways to modify the size of the green circle?", "I am using Linux Ubuntu 16.04, Autoware version: 1.10, Lidar: Velodyne-16, Camera: GMSL"], "url": "https://discourse.ros.org/t/autoware-calibration-toolkit-green-circle-too-small/11678"},
{"title": "New Packages for Kinetic 2019-06-15", "thread_contents": ["We\u2019re happy to announce 6 new packages and  177 update packages for Kinetic. There is one known regression which removes 3 packages in this sync. The maintainer is looking at the regression and we expect it to be restored soon.", "Thank you to everyone who has helped make these packages available to the community, including the maintainers and contributors!", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "The regressions in abseil_cpp has been resolved thanks to the work of ", " and the ros_type_introspection and plotjuggler have both been restored building on top if it. There was also a new package topic_switch added in the intervening day which is included too.", "Thanks to all ROS maintainers who make packages available to the ROS community. The above list of packages was made possible by the work of the following maintainers:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": 2.1.12-1", "ros-kinetic-flir-camera-driver: 0.1.3-0", "\n", ": 2.1.1-1", "\n", ": 2.6.2-1", "\n", ": 2.6.2-1", "ros-kinetic-rostwitter: 2.1.12-1", "ros-kinetic-abseil-cpp: 0.2.3-0 -> 0.4.1-1", "ros-kinetic-actionlib-enhanced: 0.0.4-1 -> 0.0.6-1", "\n", ": 0.1.1-0 -> 0.1.2-1", "ros-kinetic-assimp-devel: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.7.14-0 -> 0.7.18-1", "ros-kinetic-cob-base-controller-utils: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-bms-driver: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-cartesian-controller: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-control-mode-adapter: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-control-msgs: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-elmo-homing: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-frame-tracker: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-helper-tools: 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-model-identifier: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "ros-kinetic-cob-obstacle-distance: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-omni-drive-controller: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-phidget-em-state: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-phidget-power-state: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "ros-kinetic-cob-scan-unifier: 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.11-0 -> 0.6.12-1", "\n", ": 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-tricycle-controller: 0.7.5-1 -> 0.7.6-1", "ros-kinetic-cob-twist-controller: 0.7.5-1 -> 0.7.6-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 0.6.13-0 -> 0.6.14-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "ros-kinetic-eigen-typekit: 2.9.2-1 -> 2.9.3-1", "\n", ": 9.26.0-0 -> 9.26.0-1", "ros-kinetic-fcl-catkin: 0.5.96-0 -> 0.5.98-1", "ros-kinetic-ff: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 1.1.2-0 -> 1.2.1-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "\n", ": 2.5.18-1 -> 2.5.19-1", "ros-kinetic-generic-throttle: 0.6.11-0 -> 0.6.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.2.1-0 -> 1.2.1-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "ros-kinetic-julius-ros: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.9.2-1 -> 2.9.3-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "\n", ": 1.1.1-1 -> 1.1.2-1", "ros-kinetic-laser-scan-densifier: 0.6.13-0 -> 0.6.14-1", "ros-kinetic-libcmt: 2.1.11-0 -> 2.1.12-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "ros-kinetic-libsiftfast: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 2019.5.20-1 -> 2019.6.7-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "\n", ": 0.30.0-1 -> 0.31.0-1", "ros-kinetic-mini-maxwell: 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "ros-kinetic-nlopt: 2.1.11-0 -> 2.1.12-1", "ros-kinetic-novatel-gps-driver: 3.7.0-0 -> 3.8.0-1", "ros-kinetic-novatel-gps-msgs: 3.7.0-0 -> 3.8.0-1", "\n", ": 2.9.0-1 -> 2.9.1-3", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.7.0-1 -> 0.7.2-1", "ros-kinetic-pgm-learner: 2.1.11-0 -> 2.1.12-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.0-0 -> 2.1.1-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.1-0 -> 2.1.0-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "\n", ": 2.0.0-0 -> 2.1.2-1", "ros-kinetic-quaternion-operation: 0.0.1-2 -> 0.0.3-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "\n", ": 2.6.1-1 -> 2.6.2-1", "ros-kinetic-respeaker-ros: 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.4.4-0 -> 2.4.5-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 1.14.4-0 -> 1.14.6-1", "\n", ": 2.9.2-1 -> 2.9.3-1", "ros-kinetic-service-tools: 0.6.11-0 -> 0.6.12-1", "\n", ": 0.0.16-0 -> 1.3.21-0", "ros-kinetic-slic: 2.1.11-0 -> 2.1.12-1", "ros-kinetic-test-mavros: 0.30.0-1 -> 0.31.0-1", "ros-kinetic-uuv-assistants: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-auv-control-allocator: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-cascaded-pid: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-control-utils: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-descriptions: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-gazebo-worlds: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-sensor-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-sensor-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-simulator: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-teleop: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-thruster-manager: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-trajectory-control: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-ros-plugins: 0.6.10-0 -> 0.6.12-0", "ros-kinetic-uuv-world-ros-plugins-msgs: 0.6.10-0 -> 0.6.12-0", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 0.10.0-0 -> 0.11.0-1", "\n", ": 2.1.11-0 -> 2.1.12-1", "\n", ": 0.2.0-0 -> 0.2.1-1", "ros-kinetic-topic-switch", "Alessandro Tondo", "Alexander Bubeck", "Benjamin Maidel", "Dirk Thomas", "Fabien Spindler", "Fabrice Poirier", "Felipe Garcia Lopez", "Felix Messmer", "Felix Ruess", "Florian Weisshardt", "Hitoshi Kamada", "John Hsu", "Jose Luis Rivero", "Joshua Hampp", "Kei Okada", "Luiz Ricardo Douat", "Masaya Kataoka", "Matthias Gruhler", "Michael Hosmar", "Michael Lehning", "Mike Lautman", "Monika Florek-Jasinska", "Musa Morena Marcusso Manhaes", "Noda Shintaro", "Orocos Developers", "P. J. Reed", "Philipp Schillinger", "Richard Bormann", "Robert Haschke", "Russell Toris", "Ryohei Ueda", "Takuya Nakaoka", "Vladimir Ermakov", "Wolfgang Merkt", "Yohei Kakiuchi", "Yuki Furuta", "Yuto Inagaki", "dfaconti", "k-okada", "matsui_hiro", "nakamichi_d", "\n", ": 2.1.10-0", "\n", ": 1.3.3-1", "ros-kinetic-topic-switch: 0.0.1-2", "ros-kinetic-abseil-cpp: 0.4.1-1 -> 0.4.2-3", "Davide Faconti", "Masaya Kataoka"], "url": "https://discourse.ros.org/t/new-packages-for-kinetic-2019-06-15/9545"},
{"title": "Cozmo with ROS2 without SDK", "thread_contents": ["Hi there,", "for all of you using Cozmo from Anki", "I implemented a simple wrapper for using it with ROS2 without depending on the Cozmo SDK. Please, check it out and let me know what you think:", "At the moment, it allows to move the robot (wheels, head and lift) using a modified version of the teleop_twist_keyboard and publishes the images from the camera to a topic.", "Feel free to contribute to the repo.", "That\u2019s super cool, and the price point is pretty good for the features (~$115US on Amazon). Looks like there is quite a bit more possible. Pycosmo supports:", "Sensors:", "Actuators:", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Hi,", "Good news! So far Cozmo publishes:", "Now that it is working, the rest is quite straightforward (and boring ", ") so some help with the implementation would be much appreciated.", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Not yet but I will try. In theory the pycozmo wrapper I am using for it supports it. I just have to convert the data to ROS2 msgs.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "Cliff sensor", "Accelerometers", "Gyro", "Battery voltage", "Cube battery voltage", "Cube accelerometers", "Wheel motors", "Head motor", "Lift motor", "Backpack LEDs", "IR LED", "OLED display", "Speaker - work progress", "Cube LEDs", "Platform LEDs", "Odometry", "Image", "Imu (orientation, gyro and accel)"], "url": "https://discourse.ros.org/t/cozmo-with-ros2-without-sdk/11689"},
{"title": "Autoware Reference Platform Working Group Meeting #3", "thread_contents": ["Wednesday 30th of October", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Kenji Funaoka (Tier IV)", "Steve Kan (Tier IV)", "Stephane Strahm (Kalray)", "Cheng Chen (Autocore)", "Status on AutoCore board", "Connection with ", "\n", "Discussion on requirements categories", "Status on demo 2020", "Demonstration on-going ROSCon Macau with TierIV\n", "2 PCU boards with PC Simulator\n", "LiDAR stack", "Controlling", "No accelerator", "\n", "Software= Autoware.AI", "\n", "Accessibility of board to be confirmed by Cheng\n", "Requests by Kalray and TierIV", "\n", "Estimated power consumption : 20-30 Watts (without accelerator)", "Accelerator for the demo: Movidius, Google TPU, plus accelerator from Chinese companies", "Goal: ", " activities part of Reference Platform Group", "Yang is leading this ", "\n", "Stephane to contact Yang to make it happen ( + Cheng)", "Reference Platform Group is defining the ", "\n", "Managed by Foundation", "Documentation\n", "Interfaces", "Functionalities", "Integration of software", "Basic guidelines for design", "Performance expectations", "Collection of requirements defining the Reference Design", "\n", "Available through AWF", "\n", "Reference Platform Group will refer to ", "\n", "Actual implementation of Reference Design", "Provided by companies as product Platform", "Autocore PCU is the first Reference Platform according to Autoware Reference Design (upcoming)", "\n", "Suggestion: rename Reference ", " Group as Reference ", " Group\n", "To be submitted to TSC", "\n", "Need to initiate requirements definition process", "Proposal\n", "Collective work\n", "Each member AND each participation shall contribute (Discourse)", "\n", "Organize by categories of requirements for allow better focus\n", "Sensors\n", "TierIV can list sensors currently tested", "\n", "Hardware for application\n", "List of currently used", "\n", "Hardware for acceleration\n", "List of currently used", "\n", "Drivers\n", "\n", "Reference Design Configurations\n", "Demo part of this", "Performance expectations / guidance", "\n", "Software integration\n", "Autoware.AI", "Autoware.Auto", "\n", "Safety\n", "Redundancy considerations", "\n", "Security\n", "to be defined", "\n", "Simulation\n", "Definition of environment for simulation: To be handled by the Simulation team", "\n", "Testing\n", "SIL and HIL approaches", "Define test cases and test scenarios (leveraging system test such as dSpace or others)", "\n", "\n", "Need policy for req management and tool: to be checked with TSC\n", "Steph in touch with Geoff to initiate this", "\n", "\n"], "url": "https://discourse.ros.org/t/autoware-reference-platform-working-group-meeting-3/11451"},
{"title": "Announcing stable release of Slam Toolbox", "thread_contents": ["Hi!", "Over the last 2 years or so a pet project of mine is finally ready for prime time and see get some use. Slam Toolbox is a set of tools and capabilities for 2D planar SLAM. This project contains the ability to do most everything any other available SLAM library, both free and paid, and more. This includes:", "Slam Toolbox for 2D mapping and localization in potentially massive maps - SteveMacenski/slam_toolbox", "For running on live production robots, I recommend using the snap: slam-toolbox, it has optimizations in it that make it about 10x faster. You need the deb/source install for the other developer level tools that don\u2019t need to be on the robot (rviz plugins, etc).", "This package has been benchmarked mapping building at 5x+ realtime up to about 30,000 sqft and 3x realtime up to about 60,000 sqft. with the largest area (I\u2019m aware of) used was a 145,000 sq.ft. building in sychronous mode (e.i. processing all scans, regardless of lag), and ", " larger spaces in asynchronous mode.", "I\u2019d love to see what people think. Features of this have been running live on dozens of robots worldwide. I\u2019ll be the first to admit it needs some refactoring, but the capabilities are there. Take a look, star, and I look forward to the issue tickets and feature requests!", "Steve Macenski, Open Source Robotics Engineering Lead @ Samsung Research America", "Awesome! Can you help us understand either qualitatively or quantitatively:", "It looks terrific, and I am excited to try it on a Ubiquity Robotics Magni when our 3-D TOF sensor is ready.", "One last thing you didn\u2019t include a link to the repo. I presume this is the right one: ", "Hi Dave,", "First, thanks for mentioning to include a link! There\u2019s always something missing when you make an announcement\u2026", "It assumes the \u201cvanilla\u201d mobile base setup of a 2D laser scanner. The testing I have done are with SICK TiM, Hokuyo lidars, and RPlidars, but there\u2019s no reason you couldn\u2019t use a lower cost version and a coarser resolution map. But in general a 2D laser scanner broadcasting a sensor_msgs/LaserScan message.", "On the computational side, I haven\u2019t tried it with something too under powered, I think my weakest robot is a 6th gen i5 and I haven\u2019t had a problem. Its a good question if this would work on something like a Raspberry pi, and the answer would be: I don\u2019t know but I\u2019m open to finding out. I haven\u2019t done anything insanely above the ordinary of 2D laser based SLAM, so you should be fine to use this if you are able to use other 2D slam packages on your platform like Karto, Gmapping, Cartographer, etc. You definitely won\u2019t get 145,000 sq.ft. realtime on that type of platform however. That metric was using a 7th gen i7 mobile NUC processor.", "Nice tool, we\u2019ve been looking to work with something like this.  Anyone interested in using an ", " sensor to make it work with the toolbox?  It provides range information to one or more detected objects in it\u2019s field of view.  Detection range is 0.1 to 20m.  Contact me if interested.", "This is awesome work ", ", thanks for sharing!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Ordinary point-and-shoot 2D SLAM mobile robotics folks expect (start, map, save pgm file)", "life-long mapping (start, serialize, wait any time, restart anywhere, continue refining)", "an optimization-based localization mode (start, serialize, restart anywhere in Localization mode, optimization based localizer)", "synchronous and asynchronous modes", "kinematic map merging (with an elastic graph manipulation merging technique in the works)", "plugin-based optimization solvers with a new optimized Google Ceres based plugin", "RVIZ plugin for interating with the tools", "graph manipulation tools in RVIZ to manipulate nodes and connections during mapping", "Map serialization and lossless data storage", "\u2026 more but those are the highlights", "The types of sensor it expects and the types of sensor it accepts", "The sensor quality it expects and accepts", "The typical computer power needed to do things such as 145,000 square feet real time mapping."], "url": "https://discourse.ros.org/t/announcing-stable-release-of-slam-toolbox/9872"},
{"title": "Hardware requirements discussion", "thread_contents": ["I draft a hardware requirement, let\u2019s discuss in this thread or in hardware working group meeting.", "based on scenario and sensor performance, should involve sensor vendor to discuss.", "Silicon type:", "Board Architecture:", "Reference function design of boards:", "For board hardware interface:", "Type:", "Configuration:", "For MPU side:", "Operation system", "Performance capability", "Autoware should define the dependency of 3rd party software.", "Low level hardware acceleration capability (Optional)", "low level acceleration capabilities are optional, depends on performance benchmark requirements.", "For RT-Core/MCU side:", "Interconnection with MPU and other board", "Operation system", "Feature could be realized inside MCU:", "Autoware do not define which kind of features could be realized by MCU/RT-Core yet.", "Hardware redundancy is not in scope, there are different realizations and we will not choose one. but we believe the interface requirement and RT-Core/MCU requirement will provide the capability to setup user\u2019s own redundancy solution.", "using AutoCore\u2019s PCU as an example to demonstrate hardware design. it\u2019s designed for P1 or P3 usage.", "below is default ethernet configuration of this board.", "Hi,", "Thank you for this draft. Is it for Autoware hardware requirements?", "Do you have any suggestion for hardware supplier for a self driving car? We are planning to buy a car and all the sensors for research at Aalto University. We will use Autoware in this project. I found AutonomousStuff and DataSpeed to provide all the sensors and do the installation and support. Do you have any other suggestion or do you know any other supplier?", "Thanks", " have you checked ", " for the computing? as ", " mentions above", "Yes. I look for a company that can provide all the sensors like ", ".", "Hi, if I may, we at StreetDrone supply fully integrated autonomous vehicles, including sensors, compute and Autoware.AI stack. We can supply full support too and support a range of vehicle form factors.", "\n", "Deploying open, safe, autonomous vehicles in the worlds smartest cities.", "\n", "\nPlease let me know if you\u2019d like to discuss with us- send me a message and I\u2019ll let you know my email.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "USB/GigE based Camera are not for production purpose, but its easy to use.", "Most smart camera is ADAS module.", "at Linux platform, V4L2 is generic interface to get image.", "Image format maybe different by different sensors, need framework to support transform.", "the best solution/framework for transform image format could be GStreamer or OpenCV.", "transport image/video stream to different computer board over Eth, Gstreamer with hardware encode/decode should be much better solution than using ROS/ROS2 to publish image topic. we need avoid transfer raw image by network.", "Lidar", "Most Lidars are ethernet based with UDP traffic.", "in order to improve network performance, Lidar should connect processing board directly or in a dedicated VLAN/LAN by ethernet switch.", "UDP multicast provide capability of redundancy feature of computing board, but also meet safety requirement of network switch.", "GPS/IMU", "Most GPS provide UART interface. (can use usb-uart for development env.)", "Some IMU use CAN bus (2.0 or FD).", "ROS application expect /imu /fix  and original data of NMEA.", "In many use cases, GPS/IMU are connected to MCU, by using IRQ to process high frequency IMU data.", "Radar/Sonar", "most radar/sonar are connected to MCU.", "Summary the interfaces", "Most demo cars are using UART/USB/1000base-T", "LVDS/CAN should must be supported if the ECU/computer board are targeting real product.", "100Base-T1(TSN) maybe the best solution for highspeed data exchange who is targeting real product. (Tesla use 100base-T1 to interconnect media part and AP)", "MPU with RT-Core", "MPU without RT-Core", "Standalone MCU", "should follow 96auto spec.", "at least one MCU/RT-Core onboard, prefer with lock-step features.", "considering the capability of current SoC in automotive world, defined several type of boards:\n", "P1 which focus on sensor connection and sensor fusion. It could be a kind of gateway or domain controller which has good network capabilities.", "P2 which focus on camera data processing. It could be a kind of domain controller or smart camera which support ADAS features.", "P3/P4 are used for main software stack, including later fusion, planning, decision maker.", "\n", "All these part should be connected by ethernet, 100Base-T1 for automotive or 1000base-T are OK.", "Detail network design for deployment is not in the scope.", "You can combine different function in one board or single chip. it depends on the capability of your chip and board.", "\n", "\n", "UART/USB/1000base-T must be supported for current POC setup.", "100Base-T1 with TSN capability and CAN 2.0/FD must be supported.", "LVDS or onboard CSI camera depends on the capability of image processing including ISP/CV/Encode/DNN.", "Need support PPS input from GNSS.", "Should has solution to synchronize sensors.", "\n", "\n", "\n", "If connectint GPS/IMU to MPU, it need 1-2 UART. And need extra UART console for debugging.", "At least 2 ethernet interface to isolate different LAN.", "\n", "\n", "\n", "Linux kernel with PREEMPT-RT.", "Suggest support Debian/Ubuntu filesystem, Yocto as an option.", "It\u2019s optional to support QNX and Vxworks.", "\n", "\n", "\n", "Should support the performance requirement of PCL if support Lidar data processing.", "Should support the performance requirement of OpenCV if supporting Camera/Image data processing.", "Should support the performance requirement of DNN if supporting Deep learning. ", "\n", "\n", "\n", "\n", "\n", "OpenCL, which can power PCL2.0 and OpenCV3.x", "OpenVX", "Gstreamer encode/decode, support MJPG and H.264/265", "DNN accelerator", "CV accelerator (with OpenCV or OpenVX wrapper)", "Neon for ARM", "OpenMP", "\n", "\n", "\n", "\n", "interconnect MPU by onboard ethernet and/or I2C/UART.", "Onchip RT-Core will use RPC tech provided by Silicon vendor.", "If don\u2019t have ethernet interface/IP stack, should has agent on MPU to export capability on middleware.", "must have capability to monitor MPU\u2019s status.", "\n", "\n", "\n", "FreeRTOS or other RTOS", "Prefer support Micro-ROS", "\n", "\n", "\n", "\n", "global time sync\n", "Use GNSS time info as NTP source.", "Use PPS from GNSS as PTP source.", "Support RTC.", "\n", "time stamp of sensor data provided by driver\n", "time/space synchronize of sensors will be developed by user, provide PPS dispatch feature for sensor usage.", "from device driver side, need to clarify the correct time stamp.", "most sensor data are generated in a time slice, and depends on sensor venders\u2019 realization.", "\n", "calculate latency between senser data is generated to algorithm output.\n", "well designed time synchronize infrastructure and device drivers will help prediction algorithm.", "\n", "it use NXP LS1046A as MPU, provide much connectivity and ARM A72 CPU for generic algorithm.", "it use TMS570 as MCU with opensource FreeRTOS, provide capability of real-time/safety features, and CAN.", "it has onboard 5port switch.", "it has extend capabilities for acceleration by M.2 and miniPCIE."], "url": "https://discourse.ros.org/t/hardware-requirements-discussion/12175"},
{"title": "GitHub or GitLab for Autoware.AI", "thread_contents": ["The Autoware Foundation was recently granted free usage of GitLab\u2019s top-tier features, giving us some very powerful development management tools for free through their wonderful ", ". We need to decide now how to make use of these tools.", "Currently the plan that the AWF TSC has agreed to is", "Ambiguously decided is:", "Not decided is:", "The maintainers are in favour of moving as much of Autoware.AI as possible to GitLab because it is easier to use one tool than two and because of the advanced tools we get at GitLab. There are multiple options for dealing with the undecided points.", "The risk of moving is, of course, confusing or losing users, and the fact that the majority of the ROS community uses and is used to GitHub.", "Of important note is that Autoware.AI is currently using GitLab CI, but this will probably not be possible after September when they are planning on ending free support for using GitLab CI for GitHub-hosted code. Additionally, over the next two or three years we expect Autoware.AI to effectively become an empty project that rebrands Autoware.Auto.", " has proposed that the above is not the ideal plan. He proposes that instead we do the following:", "That is the current state of things. Here\u2019s my opinion:", "I think that because the GitLab tools are powerful and useful, we should use them as widely as we can. This means not keeping Autoware.AI development at GitHub.", "I also agree that the stars are a useful marketing point for attracting new users, so they cannot be lightly discarded. We have asked GitLab if it is possible to move the stars, but because they are effectively bookmarks it unfortunately is probably not technically possible except for users who have accounts at both GitHub and GitLab (unlikely for most users).", "I think it is not possible to eliminate our GitHub presence entirely due to the risk of losing users and contributors.", "I worry that leaving a repository at GitHub containing just the ", " file will require pointing users at that repository for installation, which might lead to them filing issues against that repository.", "I think that using group names as project branding is nice but should not drive the name of the group because we will want to have non-Autoware (the software project) projects in the future.", "So I propose that we:", "Leave some kind of repository at GitHub to hold the stars and point new users at GitLab.", "I\u2019ve seen other projects host a read-only mirror of a non-GH repository on GH, exactly for the exposure.", "Then point people discovering that repository (or those repositories) to the Gitlab hosted versions and explain all development and interaction happens there.", "Perhaps that could be something for Autoware as well?", "Does \u201cread only\u201d also apply to issues?", "That depends what you exactly mean by \u201cread only issues\u201d:", "The first I\u2019ve often seen done, the second never really successfully.", "Note that you can\u2019t disable PRs, but you can clearly place a \u201cwarning\u201d somewhere that PRs opened against that repository will be ignored.", "Example: ", ".", "Though I agree that the stars may be important from a marketing perspective, I\u2019d also like to point out that this is a topic that TSC has discussed extensively and reached a decision based on the research that, not only ", " and I as coleads have done, but also by other members of the community.", "Having Autoware.AI and Autoware.Auto in separate website would cement the idea that they are separate projects and fragment even further the community. What we want to achieve in the long term is to make the two converge into one single Autoware brand, and we can do that more effectively if the tools are the same for Autoware.AI and Autoware.Auto (hence the move to colcon and ament in Autoware.AI, for example)", "I\u2019m very much in favor of just keeping a repository on GitHub that only contains the .repos file, in the same way as ", ", and that will also keep the stars.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Put all Autoware.Auto development on GitLab under a group (the equivalent of a GitHub organisation) called ", ".", "Move Autoware.AI code to GitLab under the same group to take advantage of the new tools.", "What presence to leave at GitHub.", "What to do about the thousands of stars that Autoware has at GitHub.", "Move all the code-containing repositories to GitLab and leave just the core ", " repository at GitHub. It will hold the ", " file used to install Autoware.AI and the stars.", "Move all the Autoware.AI repositories to GitLab and leave just the a pointer ", " repository at GitHub. It will hold a ", " pointing people at the GitLab repository and the stars.", "Abandon the stars and move entirely to GitLab.", "Develop Autoware.Auto at GitLab under a group called ", ", taking advantage of the powerful management tools.", "Develop Autoware.AI entirely at GitHub, as it currently is, in an organisation called ", ".", "Use a GitLab group called ", ".", "Use a GitLab sub-group called ", " that holds all the Autoware.Auto repositories.", "Use a GitLab sub-group called ", " that holds the Autoware.AI code repositories (shifted from GitHub).", "Ideally, have the ", " group also contain the core ", " repository that contains the ", " file for Autoware.AI, but I am willing to concede this if I have to.", "Leave some kind of repository at GitHub to hold the stars and point new users at GitLab.", "you can disable the tracker on a repository, then make it clear that issues should be reported on the Gitlab tracker", "as to mirroring issues: that is difficult, as you cannot faithfully mirror things like authorship and other properties between GH and Gitlab"], "url": "https://discourse.ros.org/t/github-or-gitlab-for-autoware-ai/9063"},
{"title": "Autoware TSC meeting minutes for March 20th, 2019", "thread_contents": ["March 20, 2019, 7 AM Tokyo (+1day), 10 PM London, 2 PM California", "Nothing in particular.", "We need to settle on the tool chain to be used by Autoware.Auto for development. This encompasses tools used for management of the development, tools used by the open community for accessing the source code, filing bugs, and making contributions, and tools used for quality management.", "There are several options being considered. Whichever one we choose, the AWF will need to pay for the cost of use.", " (312.7 KB)", "Tier IV has a Velodyne VLS128 available for use by the foundation. What should we do with it?", "Questions from Apex.AI regarding the LGSVL simulator and its license (", ")", "4 posts were merged into an existing topic: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees\n", "Geoffrey Biggs (Tier IV)", "Antonis Skardasis (StreetDrone)", "Dejan Pangercic (Apex.AI)", "Dmitry Zelenkovskiy (LG)", "Esteve Fernandez (Apex.AI)", "Jan Becker (Apex.AI/Autoware Foundation board)", "Joe Buckner (AutonomousStuff)", "John Buszek (AutonomousStuff)", "Kenji Funaoka (Tier IV)", "Nikos Michalakis (TRI-AD)", "Paul Sastrasinh (TRI-AD)", "Shinpei Kato (Tier IV/Autoware Foundation board)", "Stephane Strahm (Kalray)", "Victor Duan (Linaro/96Boards)", "Yang Zhang (Linaro/96Boards/Autoware Foundation board)", "\n", "Minutes: Geoffrey Biggs (Tier IV)", "Opening remarks and new member introductions [2 minutes]", "Projects update: Autoware.Auto/Autoware.AI (Tier IV/Apex.AI) [10 minutes]", "Autoware.Auto toolchain (Tier IV/Apex.AI/TRI-AD) [20 minutes]", "Automotive Grade Linux cooperation (Tier IV) [10 minutes]", "Mapping system and format (Apex.AI) [15 minutes]", "Autoware projects use case (Apex.AI) [15 minutes]", "Heterogeneous computing platform for Autoware (Linaro) [10 minutes]", "Velodyne VLS128 usage proposals (Tier IV) [5 minutes]", "TSC member FTE work (All) [15 minutes]", "Release naming [5 minutes]", "Simulator licensing (Apex.AI/LG) [Remaining minutes]", "Review opinions on the tool chain and information provided by TRI-AD and make a decision on what to use\n", "Geoff, Esteve", "\n", "Coordinate with the Linux Foundation/AGL on their efforts to improve safety certification practices of open-source software\n", "Geoff to take point", "ARM, Linaro, TRI-AD, Tier IV, Kalray to participate as interested", "\n", "Lead improvement of mapping format used by Autoware\n", "Brian Holt (Parkopedia)", "\n", "Produce some proposals for milestone demos that we could set to drive the development of Autoware.Auto and direct resources (choice and ordering to be decided in TSC meeting ", ")\n", "Geoff", "\n", "Coordinate with Tier IV and Kalray to bring their work on algorithm acceleration into the computing platform effort\n", "Yang", "\n", "List planned contributions that go towards Foundation work and can be used to meet the milestone demos (once decided)\n", "All premium members", "\n", "Follow up with premium members who have never attended a TSC meeting\n", "Yang", "\n", "Hire a lawyer with experience in open source licenses to look at licenses for all potential simulators\n", "AWF board", "\n", "1.11 is due to be released on Thursday March 21st.", "Repositories, processes, etc. are being improved for 1.12 and on. See the relevant ROS Discourse threads.\n", "\n", "The Autoware code will be moved to a new GitHub organisation called \u201c", "\u201d", "Branches need to be pruned; many of them have stuff we want to keep so the pruning must be done carefully.\n", "In particular, Apex and AutonomousStuff each have one branch that should be merged.", "Four-way intersection work will be merged from the Apex branch into the AutonomousStuff branch.", "\n", "Updated ADE to ROS2 Crystal", "Ported internal Apex.Auto ray ground filter to Autoware.Auto\n", "\n", "Vehicle interface design discussion\n", "\n", " is possibly also relevant", "All interested in the vehicle interface, please read the above issues and participate in the discussion.", "\n", "Covers repository, issues, task management, CI", "Currently used by Apex.AI\u2019s initial project setup", "Cost-prohibitive and pricing model is not compatible with an open project.\n", "Special program for Non-Profit Organizations", "\n", "\n", "We may qualify for this despite the rule against making money because the foundation itself is non-profit.", "\n", "Apex.AI has experience with this toolset.", "Esteve (Apex.AI) has experience with this toolset.", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "TRI-AD proposes this. They are willing to maintain it.\n", "\n", " (369.1 KB)", "\n", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "JIRA is very powerful for managing complex projects.", "JIRA can integrate with GitHub such that the average user won\u2019t need to touch it (we can limit it to maintainers only), so they won\u2019t need to see a complex tool and can work through the GitHub interface.", "GitLab CI is a good tool and integrates well with GitHub", "Use of GitHub maintains the existing Autoware community and keeps us close to the existing ROS community.", "No CI tool of its own.", "Task management tools are poor; the best third-party replacement (", ") is shutting down in May.", "Linaro: How will we deal with ARM-native CI?", "TRI-AD is going to talk to a GitLab rep about costs; Apex.AI to join that meeting.", "TRI-AD wants the decision to be based on the merits of the tools rather than the need to maintain it or not; they will do any setup and maintenance we need.", "TRI-AD has put together a document describing their work on tool chains based around infrastructure-as-code.\n", "\n", "Esteve is concerned about JIRA being an entry point for users; it\u2019s fine for management but it\u2019s too much for the average contributor. The Apache Foundation used to use it, but everyone hated it and as soon as the foundation allowed Github use, everyone moved there.", "Dejan feels that option 3 is the worst option because of the combination of tools due to the issues syncing between tools. For example syncing between GitHub and JIRA issues is nearly impossible because of the different semantic models. Also problems switching between tools for different tasks.", "Dejan has found that Gitlab (used alone) has produced the happiest engineers.", "AWF board states that they are happy to cover CI cost, but would like a member to cover other costs if possible.", "Tier IV met the Linux Foundation people responsible for their Automotive Grade Linux project recently.", "They have had success in producing a stripped down Linux for use in the entertainment stack in cars.", "They are starting their next two projects now:\n", "The instrumentation stack.", "Unified Autonomous Driving Platform (UADP).", "\n", "They want to cooperate with Autoware and Apollo.", "UADP has significant involvement from Toyota, Honda, Mazda, Suzuki and Subaru. Volkswagen Group and Hyundai have just joined. Component suppliers such as Denso, Harmin and Panasonic are also involved.", "Non-traditional companies are also involved, such as Amazon Web Services and Adobe.", "Two expert groups currently working:\n", "Vehicle to cloud, working on over-the-air updates, map provisioning, etc.", "Vehicle to road infrastructure, working on smart infrastructure.", "\n", "They are also related to/involved in/running ELISA, the project to produce a safety-certified Linux.\n", "BMW and Toyota are major supporters of ELISA.", "\n", "The Linux Foundation and the AGL project want an improved method for certifying open-source software. They believe ISO 26262 is out of date.\n", "Tier IV is interested in coordinating on this for use for Autoware.Auto.", "Probably TRI-AD is as well.", "\n", "They will consider if AGL should join the Autoware Foundation in the summer, once they have figured out their concrete project goals.", "The Autoware Foundation needs to decide if it wants to join the Linux Foundation and participate in AGL and ELISA.", "ARM, Linaro, TRI-AD, Tier IV and Kalray will coordinate on working with their safety-critical efforts.", "I recently talked to several mapping providers: Atlatec, Mandli, Carmera, Parkopedia (Parkopedia in the TSC). Between them Atlatec and Mandli already provide maps as Lanelet2 format and Carmera also is thinking about trying to do that.", "I also concluded, after talking to some mapping experts, ", ".", "On the other hand, Tier IV is creating a Lanelet2 to Autoware vector map format translator (in their recent work for AutonomousStuff at least).", "So I feel that we are getting enough traction between mapping players such that they would finally all converge to a common format.", "But we need someone in the AWF to a) make sure that Lanelet2 is really ok, b) to then come up with the plan and c) to start integrating this into Autoware.", "Questions:\n", "So Brian do you have time to start driving this?", "Are you in a position to talk to above players since potentially you are competitors? Then I can also introduce you.", "Shinpei, Geoff are you OK to start ditching vector map in favor of something that others use (e.g. Lanelet2)?", "\n", "Apex.AI had a lot of problems with the existing Autoware format, for things like not being able to represent certain features, and being inefficient at runtime.", "We need someone from the TSC to take on management of the mapping format stuff.\n", "Shinpei proposes that Parkopedia takes the lead, due to it being their core competency, and Tier IV provides back up due its experience with mapping.", "\n", "Tier IV and AutonomousStuff have got Lanelets2 working with Autoware. OpenDrive is already supported.\n", "Dejan is concerned that this support is only a translator, not using the Lanelet2 format as core.", "Shinpei thinks that being based on open standards is good, but Lanelet2 has information in it that Autoware does not use. So Lanelet2 is a good frontend but we need to figure out how it will be used internally.", "\n", "Mandli is probably going to join the AWF, so they may be able to contribute to this work.", "Several AWF partners are working on the use cases/demos. For instance:\n", "AutonomousStuff/Tier IV on City-pilot", "Parkopedia/StreetDrone/Apex on Autonomous Valet Parking", "TRI-AD on autonomous shuttles/taxis?", "Tier IV on CES demo", "AutoCore/96Boards/Robosense autoware demo on heterogeneous platform (<20 km and avp)", "\n", "It would make sense to focus on one use case where at least the compute ECU and sensors are unified. This will make us faster and allow us to build a really good foundation to then expand into other use cases", "Can we agree on one use case and all work towards it?", "When ROS 1 was first being developed Willow Garage put significant effort into achieving a series of specific, well-defined use cases, the peak of which was the PR2 running a marathon. The recent ROS 2 user stories report identified that the lack of such milestones has hampered development and marketing of ROS 2 - without an eye-catching demo it is harder to demonstrate to users that ROS 2 is robust and useful and mature enough. Autoware must avoid falling into the same trap for Autoware.Auto. Having a single use case at a time that all of us are working toward would unify resources and improve quality.", "We need a series of increasingly complex, capable and robust use cases that each builds on the previous to demonstrate improvements in capabilities and robustness. The initial set of milestones proposed by Apex.AI is a good starting point.", "If we can publish these milestones and hold public demos when they are achieved, this would be good marketing.", "Apex.AI, Parkopedia and LG want to focus on closed environments (valet parking) as the first goal. Tier IV has worked on this area already, so probably interested. StreetDrone also has a project similar to the valet parking demo so probably interested.", "AutonomousStuff wants to stay on the public roads of their CityPilot project.", "TRI-AD does not necessarily want to stay on public roads; it would be nice but they are happy to scale down if necessary. Having something that works end-to-end is better than having a specific use case.", "Tier IV is also working with Kalray on acceleration and parallelisation of algorithms.", "Tier IV can provide hardware logic (FPGA implementations) for some algorithms, such as MVP and Yolo, which uses much less power than GPU-based implementations.", "Yang feels that Tier IV\u2019s work on accelerating the compute aspects is complimentary to their work on the ECU.", "Shinpei wants to get his university (not an AWF member) involved in the work on algorithm acceleration.", "Stephane: What use case is the computing platform project working on?\n", "Linaro: Parking and street driving. Minimum 40km/h driving. This is driving the necessary sensor and computing capabilities.", "\n", "Linaro: If ROS driver and Autoware integration is ready and no one else is applying for then we can use it for demo in China in May probably more realistic", "Apex.AI: Please mount it and drive around, then provide the data.", "What do FTEs from TSC members work on? A while ago there was a following suggestion being made:\n", "AS/Street Drone provides vehicle, urdf, dbw, testing on parking lots", "AutoCore provides a target ECU NOT including BSP and OS - to be confirmed", "Linaro working on aarch64 CI for Arm native development, testing, vehicle interface", "LG provides an integrated simulation environment with the car and the parking lot world model", "Arm provides security", "TRI-AD provides data storage in the cloud and data labeling services", "Parkopedia provides map of the parking lot", "Velodyne provides a vlp128 sensor", "Apex and Tier IV provide all other SW", "\n", "When we spoke to the Linux Foundation about the Autoware Foundation, something they very strongly insisted on was that using FTEs as a measure of commitment to the foundation and/or as a way to pay for membership is a very bad idea. They stated that they used to use this approach, but ran it as an honour system at first. When they started trying to track it, they found that it was very hard to accurately track, and more importantly, most members were not actually providing the FTEs they had promised.", "Apex.AI feels it is time to start tracking results of FTEs, such as code.", "Tier IV proposes assigning member companies to specific projects.", "There are two or three companies who are premium members who have never joined the TSC meetings, so are they actually contributing?\n", "Yang to find out what these companies are doing.", "\n", "LG wants to confirm the parking lot and the car to be used for the valet parking demo so they can produce an accurate simulation.\n", "Esteve and Geoff to coordinate this.", "\n", "\n", "\n", "The proposed milestones need to be taken into account. Contributions should be related to the Foundation\u2019s work and goals on these milestones.", "\n", "We could consider giving each release of Autoware.Auto a name, similar to ROS and Ubuntu release names.", "Options proposed include famous car models, famous racing drivers, and car parts.\n", "The first two may be legally problematic.", "\n", "It seems that if we want to use LGSVL free of cost (i.e. the Personal tier), it\u2019s only possible if the company/organization that develops it has a revenue of less that 100K a year (", "). If the simulator were to be developed under the umbrella of the Autoware Foundation, we\u2019d have the problem, since its finances exceed 100K.", "\n", "\n", "It seems that the license has certain limitations that make it not open source (e.g. it restricts use to ROS1 and ROS2 only (\"(1) build source code or use binaries with ROS1 and ROS2 systems\"), forbids reverse engineering, etc.).", "\n", "\nApex.AI: This is unclear and it is hard to define what a ROS1/ROS2 based system is. e.g. what about rosjava, which is effectively a port of ROS1?", "Do the assets you provide have a separate license? Is it Creative Commons or an open source license?", "\n", "\n", "This sentence in your license: \"Your goods and services (\u201cProduct\u201d) integrated with the Licensed Materials may be publicly demonstrated or exhibited for non-commercial purposes\": this means that an AWF member could NOT take e.g. ", ", integrate it with lgsvl simulator and show the resulting point cloud at CES and try to sell it?", "\n", "\n", "\"(3) application only to files created by LG, skipping default assets and code provided by Unity3D as project scaffolding\" => how can one in this case self integrate a map that e.g. Parkopedia did?", "\n", "\n", "\"application to 3D assets\" - what does this mean?", "\n", "\n", "\u201cThis Agreement governs the use of the confidential, non-public, pre-release LG Simulator Software (the \u201cSoftware\u201d)\u201d - I am not sure why words confidential and non-public are here? Isn\u2019t everything in ", " non-confidential and public?", "\n", "\n", "The license refers to a PRE-RELEASE (section 1.1).\n", "When will there be a final release?", "What will be the license of the final release?", "\n", "\n", "\n", "\"irrevocable license solely for the purposes of ", " using, testing, evaluation, simulation and validation of the Licensed Material (the \u201cPurpose\u201d)\" vs what is further below: Notwithstanding the above, Your goods and services (\u201cProduct\u201d) integrated with the Licensed Materials may be ", " demonstrated or exhibited for non-commercial purposes => these are 2 conflicting statements.", "\n", "\n", "LGSVL wants to offers AV simulator for Autonomous Development for free", "LGSVL intention is to help Autoware Foundation with development process while keeping an opportunity to offer additional premium features in the future (possibly running simulation in the cloud as a service). As a ATF member LGSVL commits resources to provide foundation the best possible simulation tools for free and make sure there are no impediments for using it. Licence limitation are only related to immediate commercial usage: aka selling modified version of simulator or related derivative work (data, models and etc.) See LGSVL Simulator License Terms.\n", "\n", " (57.9 KB)", "\n", "LGSVL will review current license agreement and use own legal adviser to make sure current license is FULLY reflecting intentions.", "See also proposed modifications and clarifications to the license agreement: ", "\n", "LG wants to support developers of autonomous driving systems, but also make sure they have a way to monetise in the future. LG feels that offering the simulator to developers now is important to enable future monetisation.", "LG had ideas for what they wanted to achieve, and they asked their legal team to produce a license agreement that enabled them.", "LG wants people to use the simulator freely except for changing it. They intend for things like maps and so on to be completely changeable without needing to change the simulator itself.", "LG does not want people selling data generated from the simulator, such as LIDAR scans and simulated camera images. However, this does not extend to works produced using the simulator, such as DNN trainings.", "LG believes that the Autoware Foundation is about making the self-driving stack, not the simulator, so they want to keep any potential commercial uses of the simulator for themselves.", "Apex.AI: Why not just use an existing open source license? Using a custom license means that the foundation has to spend money and time getting a lawyer to check the license is OK. We want to use the simulator and LG wants the foundation to use it, so using an existing license would make it easy for all concerned.\n", "LG: Because LG management believes there is a future for the simulator. If they choose the wrong open source license, the project might become unfundable. Most of the tools used and especially the game engine at the core of the simulator are expensive.", "LG is interested in hearing about existing licenses that may achieve what they want but also be easily accepted by users.", "\n", "Apex.AI: The license is not open source. It is shared source.", "LG: We want people to use the simulator, but we want the right to decide how people make money from it.", "The reason LG wants to distribute the code is because they believe that users being able to look at the code and understand how it works (and why it may not be working the way they expect) is important.", "Dejan: The AWF needs to be protected against violating license terms they don\u2019t understand.", "Shinpei: We need to understand what LG intended, so they need to provide that information and answer the above questions by text. We also need to understand if the license is usable by the AWF."], "url": "https://discourse.ros.org/t/autoware-tsc-meeting-minutes-for-march-20th-2019/8515"},
{"title": "Autoware v1.12-alpha.1", "thread_contents": ["The first alpha for the next version of Autoware, ", ", is now available for testing. We encourage all Autoware users to try it out and wring out any remaining bugs before the code-freeze date of June 24th.", "Get the source from ", " and install it using the ", ".", "Docker images are available at dockerhub:", "\n", "The following changes have been made since 1.11.", "The following changes planned for 1.12 have not yet been made (we expect to do a second alpha for these in the next couple of weeks). You can follow progress on the ", ".", "Following on from the first alpha, we now have alpha.2.", "GitLab.com", "The major change in this alpha is the move to GitLab and the split repositories. The new install instructions have not yet been written, but in a nutshell here is how to get and build Autoware 1.12.0-alpha.2 from source:", "\nIt seems like Google Chrome changes the file name to autoware.ai.txt when downloading ", " from above link. Content does not change so just change the filename after download. (Firefox worked fine)", "I created the feedback issue about 1.12.0-alpha.2 release based on our experiment on the real vehicle. Please check and let us know what do you think.", "\n", "We tested 1.12.0-alpha.2 release on the real car, Autonomous Lexus with 4 Velodyne LiDARs. Based on the results of this experiment, the issues to be solved as the fix merge...", "\n", "I\u2019ve updated the instructions to fix the issue that ", " mentions above. There are also instructions there for the upcoming 1.12.0-beta.1.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")", "Split repositories and shift to using ", " for install", "Clean up the quick start documentation", "Download the ", " file ", " and remember where you saved it.", "Run the following commands to download and build Autoware", "\n"], "url": "https://discourse.ros.org/t/autoware-v1-12-alpha-1/9396"},
{"title": "Autoware TSC meeting minutes for February 20th, 2019", "thread_contents": ["February 20, 2019", "Round 1: 0am Tokyo (+1day), 3pm London, 7am California", "Round 2: 10am Tokyo (+1day), 1am London (+1day), 5pm California", "Investigate logistics and cost of safety-critical software development training", "Ack this.", " would you be open to team up with some other companies that  AWF members have contact to (Carmera, Mandli, Atlatec) to come up with the format or with the abstraction layer for Autoware? I think that the mapping and AD industry really, really needs  and now it is the time to do it.", "Awesome. Please have a look at ", " and request if changes are needed.", "Please also pull ", " into this. Him and I have so far looked into Gazebo, LGSVL and he will also talk to Carla folks in the next days. So he will have a really good comparison of the 3 best open source simulators.", " would you guys also consider an AVP as an MVP: ", "? This way we would all work towards the same MVP.", "\nI am actually still not 100% sure. Is CityPilot an AWF project or an AutonomousStuff project? As far as I can understand it is the latter. If the latter - why are we discussing it here? Also, in the spirit of \u201cthe strength of the wolf is the pack\u201d it would be great to work on the SAME use cases/demos/MVPs.", "What do you guys think?", "Please also pull ", " into this. Him and I have so far looked into Gazebo, LGSVL and he will also talk to Carla folks in the next days. So he will have a really good comparison of the 3 best open source simulators.", "Ping Shinpei and find out when they are meeting. I imagine Esteve\u2019s experience will be welcome.", " would you guys also consider an AVP as an MVP: ", "? This way we would all work towards the same MVP.", "I\u2019m not going to speak for TRI-AD\u2019s intentions, but a relevant piece of information is that valet parking is not a big thing in Japan.", "I\u2019m not going to speak for TRI-AD\u2019s intentions, but a relevant piece of information is that valet parking is not a big thing in Japan.", "It\u2019s not a big thing in Europe either, but AVP is an excellent usecase for Autoware.Auto (and Autoware.AI):", " wrote a great article detailing why AVP is the most likely self-driving technology to be deployed in real cars in the short/medium term ", " would you be open to team up with some other companies that AWF members have contact to (Carmera, Mandli, Atlatec) to come up with the format or with the abstraction layer for Autoware? I think that the mapping and AD industry really, really needs and now it is the time to do it.", " I\u2019d be happy to discuss maps with other providers, it makes sense for us to try to develop a common standard that Autoware can use.", "It\u2019s not a big thing in Europe either, but AVP is an excellent usecase for Autoware.Auto (and Autoware.AI):", "That all sounds good to me.", " wrote a great article detailing why AVP is the most likely self-driving technology to be deployed in real cars in the short/medium term ", "Having read that, I can see more of a use case for it in Japan too, now. Outside of the centre of major urban areas like Tokyo, Nagoya and Osaka, there are enough large shopping centres where not having to park your car itself would be nice. Even the centre of a reasonably large city like Hamamatsu has enough large carparks that would benefit from such a feature.", ", given you wrote that article, I\u2019m interested in your thoughts on ", ".", "I\u2019d be happy to discuss maps with other providers, it makes sense for us to try to develop a common standard that Autoware can use.", "Please do so and let us know how you get on. I get the feeling that this is one thing we need to move rapidly on. At the very least the AWF needs to be involved in any discussion that happens.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Attendees:\n", "Geoffrey Biggs (Tier IV)", "Kenji Funaoka (Tier IV)", "Shinpei Kato (Tier IV, AWF board)", "Brian Holt (Parkopedia)", "Victor Duan (Linaro)", "Esteve Fernandez (Apex.AI)", "Adrian Bedford (StreetDrone)", "Antonis Skardasis (StreetDrone)", "Filipe Rinaldi (Arm)", "Yang Zhang (Linaro)", "Dejan Pangercic (Apex.AI)", "\n", "Minutes: Geoffrey Biggs (Tier IV)", "Attendees:\n", "Geoffrey Biggs (Tier IV)", "Antonis Skardasis (StreetDrone)", "Kenji Funaoka (Tier IV)", "Seonmamn Kim (LG)", "Dmitry Zelenkovsky (LG)", "Nikos Michalakis (TRI-AD)", "Cheng Carmark", "Aka Arthur", "Alexander Patrikalakis (TRI-AD)", "John Buszek (AutonomousStuff)", "Shinpei Kato (Tier IV, AWF board)", "\n", "Minutes: Geoffrey Biggs (Tier IV)", "Opening remarks and new member introductions", "3rd WICD - World Intelligent Driving Challenge (Linaro)", "Discourse categories", "Autoware.Auto proposal (Tier IV, Apex.AI)", "Autoware 2019 survey (Apex.AI)", "Replacement for Autoware Slack (Tier IV)", "TSC meeting time slot", "AutonomousStuff proposed meet-up in Silicon Valley", "Project updates", "\n9.1. Autoware.AI (Tier IV)", "\n9.2. LGSVL simulator (LG)", "\n9.3. Computing platform (Linaro)", "TRI-AD/Tier IV taxi project and City Pilot Project (Tier IV, TRI-AD)", "Add a category to Discourse for discussion of map formats\n", "Tier IV", "\n", "Investigate the logistics of getting a car to China to do a demo at the WIDC\n", "StreetDrone", "\n", "Determine what a demo at WIDC will look like and report back to TSC\n", "Tier IV", "StreetDrone", "\n", "Draft an announcement for closing the Autoware Google Groups mailing list and shifting discussion to the generic \u201cAutoware\u201d Discourse category\n", "Tier IV", "\n", "List the specific skills needed for Autoware.Auto development to aid in members finding people who can contribute\n", "Tier IV", "\n", "Investigate logistics and cost of safety-critical software development training\n", "Apex.AI", "\n", "Produce a proposal document for the computing platform project\n", "Linaro", "\n", "The foundation is getting increasingly organised, but we still have room for improvement in our projects.", "The TSC needs to think about how to manage and release and progress Autoware.AI, especially in the time before Autoware.Auto becomes a complete replacement.", "An event held in China by CATARC, which is a company wholly owned by the Chinese government. They have the authority over automotive standards in China, and selling a car in China requires certification from them.", "Event to be held on May 14th to 17th, 2019 at the Meijiang Convention and Exhibition Center in Tianjin.", "Deadline for registration is March 23rd. Each foundation member will need to register individually and get insurance, etc. on their own.\n", "Despite the \u201cindividual\u201d participation, we need to have a consistent message from foundation members.", "\n", "This event reflects their ambition for China to be a leading player in autonomous driving in the world. This is why it is not just a China-focused challenge.", "Companies and projects from China and abroad will be involved in the challenge, including:\n", "Baidu/Apollo", "Tesla", "Audi", "\n", "This is a good chance to improve recognition in China and improve relationships with the Chinese authorities related to autonomous driving.", "The event itself is free but there are costs that must be covered by the participants, such as having insurance coverage.", "Linaro would like to see the AWF join as a group, not just have a few members join individually.", "Shinpei Kato has been invited to give a presentation at the event already.", "If any foundation members are interested in joining the event, please let the foundation know.", "Tier IV is proposing showing a car demo at the event; one proposal is to use a StreetDrone vehicle.\n", "StreetDrone will discuss this idea.", "Logistics of getting a vehicle there and doing a demo will be discussed offline.", "Tier IV is considering taking a Tajima EV vehicle for a demo as well.", "Multiple vehicles in the demo shows the flexibility of Autoware.", "\n", "There are several categories for demos, such as geofenced driving, highway driving, urban driving, etc.", "TRI-AD is still setting up their team so may not be able to contribute.", "Will come back to members when the details of the demo are more concrete to find out what members can contribute.", "These three categories are intended for public discussion related to the above three projects.", "We need to encourage use of Discourse for discussion as much as possible, particularly for longer design-related and project-management-related discussions.\n", "A good rule of thumb for if it should go on Discourse or in a chat is, if it is time critical then use chat, otherwise use Discourse.", "\n", "The Google Groups should be closed in favour of the Discourse general Autoware category.\n", "Groups will be closed to new members and new posts.", "An announcement of the move to Discourse will be made in the Groups mailing list.", "Geoff to write a draft announcement.", "\n", "The foundation may consider in the future setting up an answers site for Autoware. This needs to be decided based on how many technical support style questions we get in the Discourse.", "Goal: Develop an autonomous driving framework that is certifiable, and certify it to ISO 26262 (without killing the Autoware community).", "Background:\n", "We need a certified version of Autoware for use in products that require safety certification.", "The work necessary to overcome the technical debt in Autoware.AI and certify it is greater than the work necessary to do it all again from scratch.", "\n", "Proposal:\n", "Develop Autoware.Auto from scratch following a certifiable development process.", "Transition users from Autoware.AI to Autoware.Auto over time, not as a discontinuity.", "Provide certification evidence to Autoware Foundation members for their use.", "Maintain openness of the software and provide a non-certified version to allow a low barrier of entry for outside contributions.", "\n", "The long-term vision:\n", "Autoware.Auto, a certifiable, safe and secure autonomous driving framework with a controlled development process following safety-critical engineering practices by qualified developers, ready for use in autonomous driving products and services.", "Autoware.AI, a branding of Autoware.Auto for use as a \u201csandbox\u201d project by researchers and other contributors who just want to make the Next Cool Algorithm and contribute it to Autoware, and see it actually integrated and available for use without having to learn safety-critical software development.", "\n", "Parkopedia supports the proposal, and wants to contribute what they have got so far. They have a number of safety engineers from a third party that they work with, and they can provide documents that can kick-start the certification documents.\n", "Including method of operation documents, requirements documents, architecture designs, FMEAs, hazard and risk assessments, etc.", "These documents will go up on the project website anyway, but they will provide it directly too.", "Apart from their proprietary stuff, they are open by default.", "\n", "Parkopedia\u2019s proposed development timeline aligns well with the proposed Autoware.Auto milestones, so they are able to assist in the testing side of things.", "Can Parkopedia provide a map of a carpark for use in the Autoware.Auto development effort?", "Parkopedia: We need to think about how Autoware.Auto will interact with road-level maps, carpark maps, etc.", "Tier IV: We may want to aim for supporting as many map formats as possible. If Autoware only supports a particular set of maps, this is a barrier for new users who might be using another format.\n", "Geoff to add a Discourse category for map formats.", "\n", "Parkopedia: The automotive industry has settled on NDS. It is a well-accepted standard that they use. If Autoware tries to support five different map formats, it becomes much harder to maintain as the formats evolve and possibly diverge.", "Parkopedia is willing to take on responsibility for the map aspects of Autoware.Auto.\n", "They are unsure if they should go straight for Autoware.Auto and aim for certifiable, or if they should go via Autoware.AI.", "Tier IV recommends going into Autoware.Auto if possible because it will be easy to reuse in Autoware.AI, but Autoware.AI is not as easy to reuse in Autoware.Auto.", "\n", "Linaro is concerned about the choice of OS and underlying hardware platform. A decision will be needed relatively soon. Apart from this, they support the proposal\n", "Linaro will discuss in the next TSC meeting.", "\n", "TRI-AD is going through some of the same issues that Autoware.Auto.\n", "They are interested in being able to do this work in an agile way and hope to collaborate on that point.", "They have QA teams but they are using old tools. They are trying to bridge that gap with new tools and techniques.", "TRI-AD is interested in getting the safety-critical development training.", "\n", "When Apex.AI finds out the details of the safety-critical training, we will forward this to foundation members.", "TRI-AD is supportive of of the plan.\n", "Tier IV to go talk to TRI-AD team in person to discuss contributions", "\n", "TRI-AD thinks they can help on the data management side, such as moving data into and out of the car, e.g. maps.\n", "TRI-AD to check if they have access to security experts who can help on the security side of things.", "\n", "Antonis (StreetDrone) previously worked on autonomous forklifts so could help with a scenario in that area (when we get there).", "LG has concerns about the toolchain.\n", "We need to make sure everyone contributing uses the same toolchain.", "Using Docker to provide a validated build environment will be important. Build on the work already done for the ADE by Apex.AI.", "\n", "See the ", ".", "To be discussed in the TSC Slack channel.", "Nikos (TRI-AD) will be meeting the CEO of Slack soon, and can raise the concerns we have about using it for an open community.", "Preference is for a fixed time (no rotation)", "Until daylight savings starts, use the following time:\n", "San Francisco, USA 14:00 PST", "Chicago, USA 16:00 CST", "Washington DC, USA 17:00 EST", "Berlin, Germany 23:00 CET", "Tokyo, Japan 07:00 JST", "UTC 22:00", "\n", "After daylight savings starts, use one of two options:\n", "Option 1:\n", "San Francisco, USA 13:00 PST", "Chicago, USA 15:00 CST", "Washington DC, USA 16:00 EST", "Berlin, Germany 22:00 CET", "Tokyo, Japan 06:00 JST", "UTC 21:00", "\n", "Option 2:\n", "San Francisco, USA 07:00 PST", "Chicago, USA 09:00 CST", "Washington DC, USA 10:00 EST", "Berlin, Germany 16:00 CET", "Tokyo, Japan 00:00 JST", "UTC 15:00", "\n", "\n", "Not discussed", "From a perspective of functionality, Autoware.AI is improving significantly.", "Release 1.11:\n", "Contains many new features.", "In particular, enables much-needed capabilities such as backwards driving.", "A detailed feature list can be found in \u201cAutoware.AI Plan - TSC 20th Feb\u2026pdf\u201d in the TSC meeting ", " folder.", "\n", "Release 1.12 will:\n", "focus on the package and repository reorganisation,", "introduce a Kanban task management tool,", "switch to using vcstools for the installation process, and", "add the ROS 1/ROS 2 bridge as a dependency and add it to launch scripts.", "\n", "Release 1.13 will:\n", "focus on architecture reorganisation, splitting node functionality out into libraries, and conversion to ROS 2 of some packages,", "add Autoware.Auto as a dependency, and", "remove the existing Velodyne driver in favour of the high-quality driver in Autoware.Auto.", "\n", "Release 1.14 will:\n", "focus on conversion of packages and nodes to ROS 2, and", "focus on adding tests made available by use of ROS 2 tools.", "\n", "The release cadence will be set to three months.\n", "Firm due dates for feature freeze, code freeze, etc. will be set with enough time between each to allow for testing and verification.", "\n", "A simulator made with Unity.", "Focuses on simulating the sensors, including LIDAR, radar, various cameras, IMU, and GPS.\n", "The LIDAR simulation is GPU-powered so it can generate up to 128 beams in real time.", "The simulator can be used through rosbridge so that sensor information can be used in ROS and viewed in rviz.", "\n", "The use of Unity means that it is very easy to modify the environment. Their recent efforts are focusing on decoupling simulation from environment definition.", "They are also working on allowing creating a simulator environment from data collected from the real world, i.e. drive around and record LIDAR and camera data and use that to make a simulation environment of the route.", "They are working on an environment that covers San Francisco.", "Running a single simulation over multiple computing nodes in parallel is a goal.", "The simulator has better visuals than other simulators, because this is where they are putting their efforts due to the focus on sensors.", "They are working on an API for remotely managing and interacting with the simulation.", "Their release model is:\n", "Push code to GitHub weekly.", "Make tested binary builds monthly.", "\n", "They also provide a tool to do map annotation in the simulation (e.g. \u201cthis is a footpath\u201d), and that annotation can be exported to an Autoware map.", "The LGSVL simulator and the CARLA simulator have their different strengths. We need to clarify these so we can ensure each simulator is used in an appropriate way.", "Tier IV, LG and TRI to discuss simulators in-person in early March.", "Linaro is working on the proposal document for the computing platform project.", "It will be released soon in the Discourse category for open discussion.", "Apex.AI is finding it difficult to work with NVIDIA because they are not a large OEM, so if this project can provide an alternative computing platform, that is an urgent need now.\n", "Linaro feels that this is somewhere the foundation can make a difference because they feel a lack of a readily-available heterogeneous computing platform is becoming very limiting.", "Tier IV has given up trying to run QNX on NVIDIA\u2019s computing platform because of a lack of support.", "\n", "Tier IV also has contact with Renesas, but has had problems there, too.", "TRI-AD is still looking at what the MVP is for their demo project.", "TRI-AD wants to look at what the requirements are on a vehicle from Toyota, e.g. the Lexus that AutonomousStuff produced.", "Tier IV to share more information in the next TSC meeting after doing work in USA", "Tier IV is working with the Jpn Taxi model, so demos in Japan may be able to use a real taxi vehicle. Tier IV is working with AutonomousStuff to make modifications to this vehicle.\n", "The goal is to have an Autoware-controlled one by July.", "Tier IV proposes doing a demo with TRI-AD under the foundation umbrella.", "TRI-AD and Tier IV agree on using the Jpn Taxi vehicle.", "TRI-AD needs to make sure that whatever AutonomousStuff is doing is approved by Toyota, because the Toyota name will be visible in the demo and so it needs to not have a bad impact on the brand. It is possible that Toyota may require modifications be done by their own accepted shop than by AutonomousStuff.", "\n", "It is desirable for the LGSVL simulator to be used for the demos.\n", "Tier IV has already done work on building a Japanese environment in the simulator using data for Akihabara.", "\n", "Parkopedia: We need to think about how Autoware.Auto will interact with road-level maps, carpark maps, etc.", "Tier IV: We may want to aim for supporting as many map formats as possible. If Autoware only supports a particular set of maps, this is a barrier for new users who might be using another format.", "\n", "Using Docker to provide a validated build environment will be important. Build on the work already done for the ADE by Apex.AI.", "\n", "Tier IV, LG and TRI to discuss simulators in-person in early March.", "TRI-AD is still looking at what the MVP is for their demo project.", "Not on public roads, so while safety is still important, we are minimizing risks, and permits and insurance are hopefully easier to obtain.", "Low speeds.", "Maps for the most part don\u2019t change.", "Very few pedestrians.", "No cyclists.", "We have partners within the foundation that are already working on AVP as a product/service (e.g. Parkopedia), so we can leverage their expertise.", "Simpler to simulate.", "It is a useful and real usecase, it\u2019s not a synthetic demo that won\u2019t have any kind of impact.", "Not on public roads, so while safety is still important, we are minimizing risks, and permits and insurance are hopefully easier to obtain.", "Low speeds.", "Maps for the most part don\u2019t change.", "Very few pedestrians.", "No cyclists.", "We have partners within the foundation that are already working on AVP as a product/service (e.g. Parkopedia), so we can leverage their expertise.", "Simpler to simulate.", "It is a useful and real usecase, it\u2019s not a synthetic demo that won\u2019t have any kind of impact."], "url": "https://discourse.ros.org/t/autoware-tsc-meeting-minutes-for-february-20th-2019/7950"},
{"title": "Technical Steering Committee (TSC) Meeting #10 Minutes", "thread_contents": [" Geoffrey Biggs (Tier IV)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Lee Baldwin (AutonomousStuff)", "Geoffrey Biggs (Tier IV)", "Esteve Fernandez (Apex.AI)", "Kenji Funaoka (Tier IV)", "Brian Holt (Parkopedia)", "Seonman Kim (LGE)", "Dejan Pangercic (Apex.AI)", "Koga", "Nikos Michalakis (TRI-AD)", "Otake (Macnica)", "Stephane Strahm (Kalray)", "Akihiko Tsukuda (eSOL)", "Joshua Whitley (AutonomousStuff)", "Dmitry Zelnkovskiy (LGE)", "Opening remarks and new member introductions", "\n", " Confirmation of previous minutes", "\n", " Action items from previous meeting", "\n", " New logos for the Autoware Foundation and its projects", "\n", " Hiring a software architect", "\n", " Review TSC members list on the AWF website", "\n", " Autoware", "\n", " Map formats", "\n", " Vehicle interfaces", "\n", " ECU/Platform", "\n", " Simulation", "\n", " Working groups participation levels", "Release the safety case example\n", "\n", "Create working group wiki pages\n", "\n", ", ", ", ", ", ", "\n", "\n", "Post the Autoware software architect recruitment notice to any useful forums\n", "AWF board", "\n", "Provide HD Map for AutonomousStuff carpark\n", "\n", ", ", "\n", "\n", "No board members present.", "Minutes approved.", "Release the safety case example\n", "\n", " The partner has added a legal notice which makes them happy to release it, but they used a non-standard language which means anyone who wants to use it may need to get it checked out by a lawyer. Brian is proposing to add CC-BY-SA 4.0 as the license and include their disclaimer of warranty and limitation of liabilities.", "The AWF should make a fuss about releasing them because they are a big potential impact.", "Will discuss offline where to host the examples.", "\n", "Post the Autoware software architect recruitment notice to any useful forums\n", "AWF board", "\n", " Unknown.", "\n", "Create working group wiki pages\n", "\n", ", ", ", ", ", ", ", ", ", ", ",  ", ", ", ",  ", ", ", "\n", "\n", " Only the Autoware WG has a wiki page so far. Other WGs must create their pages urgently.", "\n", "The new logos created for the Autoware Foundation and its projects are now available for use.\n", "AWF logo: ", "\n", "Autoware.AI logo: ", "\n", "Autoware.Auto logo: ", "\n", "\n", " logo: ", "\n", "\n", "The Autoware Foundation is hiring a system architect to lead the design of Autoware.", "Apex.AI has ", ".", "The list of TSC members on the foundation website is out of date.", "If you are not listed, please send your photo and name so we can get you listed.", "Only the primary TSC representative should be listed", "Reminder: The dates for the next release of Autoware.AI have been set by the maintainer team.\n", "\n", "Changes have been made to the governance of the ", " and Autoware.Auto projects\n", "The new approach is based on the Apache Foundation\u2019s approach and aims at having a more open and accessible project governance structure.", "See here for details: ", "\n", "\n", "Autoware.AI updates:\n", "A significant set of new features are being proposed by engineers from Tier IV, including the ability to drive in reverse.", "Some basic safety features are being added to handle the event of NDT matching losing its place. The new features are use of an EKF to detect when the NDT result makes a large jump, and a paramterised limit on the maximum steering angle.", "The submission deadline for MRs is 12 days away.", "\n", "Autoware.Auto updates:\n", "Progress has been slow but is steadily moving forward.\n", "Objection detection is done", "Lidar-based localization super spec\u2019d out ", "\n", "Maps working group very active and we have a map of AS parking lot", "VI interface in Josh\u2019s hands - he will get that done for Lexus vehicle at least", "Path planning and controls - TierIV will get this done", "Simulation - AS parking lot is in LGSVL", "Platform group is also very active and if not there is always ", "\n", "\n", "IMU and GPS drivers will be landing in a few days. These are specific to the xsense NTIG-601 sensor.", "Tier IV has done work to get the ", " in place and allow parts of Autoware.AI to be used from Autoware.Auto. Currently this work is focusing on enabling the localisation stack from Autoware.AI to be used.", "Epics for the first milestone have not seen much tracking, except for the localisation epic being handled by Christopher Ho from Apex.AI.", "Apex.AI proposes that we hold an all-hands-on-deck integration hackathon in the Bay Area in early 2020 (some time by the end of March).\n", "There is interest in attending: Parkopedia will definitely attend and bring two or three people. AS will do their best to attend. Kalray thinks they can send someone. Nikos may be able to send someone.", "The idea is to host it at the Apex.AI and use the AutonomousStuff carpark which is not that far away.", "We still need a vector map of the AS carpark.", "Parkopedia ", " and intends to port this all to Autoware.Auto in their upcoming hackathon in the UK.", "Parkopedia wants the goal of the integration hackathon to be to flesh out all the remaining little bits of the AVP use case (using artifical landmarks, the application-level, etc.).", "\n", "\n", "The Maps Working Group has met 3 times since the last TSC meeting.", "Meetings are organised on Discourse and a regular time of 2pm GMT every second Thursday has been chosen by regular attendees.\n", "\n", "A general architecture for from the perspective of maps has been under intense discussion and represents the 4 ways in which maps are expected to be used:", "\n", "\n", "We expect to make more progress on rationalising the use cases, turning them into requirements and then making a start on the design at the upcoming Autoware Hackathon next week.", "In our last meeting, I was asked to create a survey to send to members of the group about their needs for the Autoware Vehicle Interface layers. I created and distributed the survey but received no responses in the 4 weeks between meetings. In addition, the last meeting was attended by only ", " and myself. After a brief discussion, ", " and I decided that I would distribute the survey to the entire Autoware community. We will discuss how to raise participation in this (and I believe several other) working groups during the TSC call.", "Survey topic: ", "\n", "WG has not been very active in terms of communication and has not managed to hold a meeting since the last TSC.", "Implementation of the Autocore/Kalray reference board BSP is on-going.", "Apex.AI: How is provisioning/deployment done? We want to avoid duplicating how this is done with ROS 2, commercial companies, etc.", "\n", " where they compared Metamoto\u2019s features and LGSVL\u2019s features.", "They hope to have content creation discussions in the next meeting.", "Several working groups do not have much participation and activity.\n", "Notably, Autoware WG, Reference Platform WG, Vehicle Interfaces WG, Simulation WG", "\n", "It is probable that some topics have been split off too early, before there is a critical mass of participants and contributors to power the discussion and work.", "Proposal to improve this is to drop some working groups and roll their topics into one of the two core working groups (Autoware WG and Reference Platform WG).\n", "Vehicle interfaces WG is most likely candidate for this - move its topics to the Autoware WG.", "Simulators working group probably should be, too - move its topics to the Autoware WG.", "\n", "Reference platform WG will not be removed.", "When we start having too much discussion to fit in the Autoware WG meeting, we can split off a WG appropriate to the overflow topics.", "AS agrees with cutting down the number of workin groups, particularly as it will overwhelm people when they see calls for so many working groups.", "Kalray also agrees.", "Working group activities centre: ", "\n", "Shared calendar for working groups: ", "\n", "Please subscribe to the Autoware and Autoware-TSC categories on Discourse, because all discussion for the AWF\u2019s technical activities happens there now.", "Milestones: ", "\n", "All TSC representatives need to take responsibility for driving participation in AWF development activities.", "All FTEs need to be proactive about doing work, rather than waiting for specific instructions. Pick a task from the milestones list and start doing it, and pick a working group and start participating."], "url": "https://discourse.ros.org/t/technical-steering-committee-tsc-meeting-10-minutes/10818"},
{"title": "Autoware 1.12 released", "thread_contents": ["Autoware 1.12 is now available for general use.", "\nTo install it from source, follow the ", ".", "\nPlease note that with this release, Autoware has changed its repository structure and its install method.", "\n", "Docker images are available at Dockerhub:", "\n", "Once you have Autoware installed, you can try it out using some recorded data by ", ".", "This is a list of the major changes in Autoware 1.12:", "There are some known issues in 1.12.", "As always, the current reported issues can be ", " and the current", "\nproposed bug fixes and new features can be ", ".", "Going forward, will the \u201cReleases\u201d section of gitlab be used?", "\n", "GitLab.com", "\n", "Or will release summaries be primarily done here on discourse?", "We haven\u2019t had any discussion about that, but I can say that since Discourse is our primary communications channel we will need to do something here.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["New MPC waypoints follower for more accurate path following", "New Enhanced Kalman Filter (EKF)-based localiser", "Improved decision maker state machine, including support for better stop states and battery charging", "Convex hull prediction", "Support for ROS Melodic Morenia (but see the known issues, below)", "Shift to GitLab as the host of the code", "Split the repositories along functionality lines to improve CI times", "Shift to using ", " to install Autoware", "Numerous bug fixes and smaller changes; see the full list of changes below", "MPC waypoints follower (", ")", "Fix setting files for rosbag demo (", ")", "Modify stop states (", ")", "Fix autoware launcher readme. (", ")", "Split drive state (", ")", "melodic support (", ")", "add battery charging state (", ")", "modify mssion state for fms (", ")", "add install to the resources and plugin.xml (", ") (Fixes ", ")", "Add tests around Openplanner utilities (", ")", "Support full functions of g30esli_interface (", ")", "Update Docker cross-build Image (", ")", "add predicting convex hull (", ")", "Fix Segmentation fault at velocity_set by asynchronous base_waypoints and closest_waypoint (", ") (", ")", "Fix gazebo not found in colcon_release_cross synquacer (", ")", "fix behavior_state in imm_ukf_pda_track (", ")", "runtime_manager: change params definition from int to float (", ")", "Fix don\u2019t show run time in runtimemanager ", " (", ")", "ekf_localizer (", ")", "[fix]: make sure cost is set with expanding_polygon_size (", ")", "Remove points2costmap (", ")", "fix install directive for waypoint extractor (", ")", "Adding install directive for node (", ")", "Fix bug that limits the data rate of DataRateCheckerPlugin (", ")", "Updated paths in quick_start launch files and parameters in default.rviz (", ")", "Remove enablePlannerDynamicSwitch (", ")", "Add .repos file for setting up an Autoware workspace using vcs (", ")"], "url": "https://discourse.ros.org/t/autoware-1-12-released/9817"},
{"title": "Generalized Traffic Light Classification Architecture", "thread_contents": ["Currently, Autoware.ai contains 3 types of traffic light classification systems:", "Each of these assumes that it will be handed two pieces of information:", "These traffic light ROIs (regions of interest) are provided by the ", " (feature projection) node which uses information from the vector map and camera intrinsic and extrinsic parameters to project the bulb locations from the map in 3D space into the 2D plane of the camera\u2019s field of view.", "In developing our own traffic light classification node, we have run into the following issues:", "We think it is possible to provide a general-purpose architecture for traffic light recognition (and other constrained-ROI-type image classification problems) but some pros/cons must be considered for each approach. I\u2019m not going to speak too much to the types or capabilities of neural networks which would fit the bill since that isn\u2019t my area of expertise - I\u2019ll invite my colleague Joe Driscoll to speak on this topic - but more on the overall architecture. I think our intended goals for the new architecture look something like this:", "For implementing an architecture which does the above, these possible options come to mind:", "Well, now that I\u2019ve written a novel on the subject, please provide feedback and suggestions on how we can make an awesome traffic light detector that works well enough to be used in real traffic!", "\nThank you for starting the discussion. I believe a lot of people are interested in the topic.", "About approach 1&3, don\u2019t you still need to calculate ROI from vector_map information even with DNN? Determining the presence of relevant traffic light does not ensure that irrelevant traffic lights are not in the image. If DNN detects multiple traffic lights in the image, you need information to choose which one to look at.", "You are correct. That is a fact that I missed. However, we could use the centroid of the initial ROI estimate from the feature projection node to find the closest detected light from those that the DNN node found.", "It\u2019s really a trade-off between attempting to make the feature-projection node more robust to differences between the idealized world (vector map) and the real world and providing an alternative to feature projection that is less prone to the same real-world pitfalls as feature projection but needs feature projection as an anchor to the idealized world.", " thank you for your comprehensive analysis of the current status and for prividing possible solutions related to the classification of traffic lights.", "Here are some of my comments related to your post:", "If you agree with me, (A) even with current ADAS map format,  the definition of which traffic light applies to which lane is not well defined. Current format defines \u201cclosest lane ID\u201d, but not exactly which one it is, it also only allows the definition of a ", " lane. (not sure about other ADAS formats)", "(B) just like you mentioned previously, current definition of a traffic light is ambiguous. It requires the search of individual \u201clamps\u201d to define a traffic light object. This is not as straightforward as it should be. In my opinion a new layer defining the traffic lights objects need to be added to the ADAS format.", "Now, regarding the 3 of the proposed solutions. I still think that approach (2) of your list is the most \u201cdynamic\u201d, since different nodes can be used to classify regardless the DL framework. However, I also agree that it needs to be improved.", "The recognizer must publish three pieces of information:", "In my opinion, the ", " nodes should only publish an array of results, for each image in a similar fashion to ", ", but containing the Traffic Light classification results.", "Visualization (super_impose, and markers) should be handled in an independent ", " node.", "The recognizer must classify a found traffic light from the image as having one of four states:", "I mentioned above an array of classification results, instead of a single result, so the classifier can handle more complex combinations.", "\nCase in point, only in central Japan these three types of traffic can be found:", "\n", "Example:", "would match to:", "\n", "I believe this should be able to handle different combination in different parts of the world.", "What do you think?", "Is it possible to only emit a single ROI from feature-projection node by a simple filtering rule such as FOV cropping or largest ROI.", " I agree with your assessment of the shortcomings of the mapping format with regard to traffic lights. However, we are unable to modify the existing ADASMap format (it is Aisan\u2019s proprietary format) which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", "). I believe the new format also supports what you are suggesting in regard to multiple lights on a signal. ", " may be able to comment more on this issue.", "I like the idea for ", " but it will run into the following problem:", "In this scenario, because of the latency introduced by the classifier, the ", " cannot correctly identify which raw image is tied to the classification results. If it publishes the most recently-received version of both, it will be superimposing old classifications on new images. The time delay won\u2019t be much but I just wanted to mention this issue.", "Publishing probability with the state would just push the logic of decision to the consumer module. Could cause duplication in the case of multiple consumer modules. IMHO tl modules are the best place to have definitive decision and output a definitive result. Please give a possible scenario where this probabilistic information is useful for a consumer module.", "Approach 1 I really like for the following reasons:", "PS: Crazy idea, draw ROI using perception node, run small NN on cropped ROI. eliminating feat_proj.", " - I agree with your assessment about passing the probability on to another node for the decision. However, these would be very useful for training/tweaking so I don\u2019t think publishing it is a bad thing. We could publish it and just not use it in any downstream nodes.", "Regarding approach 1, I would tend to agree with most of your points. However, as far as using the", "\nperception node for the light ROI, as ", " mentioned, we need to have some measure of certainty about which light applies to the lane we are currently in and we can\u2019t determine this without data from the vector map, thus necessitating ", ".", "which is why Autoware is moving toward a new \u201cAutoware Map\u201d format. This format supports many-to-one relationships between lights and lanes (see page 17 of ", ").", "Actually, we have decided to use Lanelet2 format as IO format and also as internal format in TSC meeting. (AutowareMapFormat will disappear.)", "\nHowever, we would need to add some extension to Lanelet2 format so that it has enough information to minimize the degradation of current Autoware functionalities. I will post the idea about extended format on discourse soon.", "I believe the new format also supports what you are suggesting in regard to multiple lights on a signal.", "Lanelet2 supports a single lane linked to multiple traffic lights and also multiple lanes linked to a single traffic light.", " I also agree that it would be simpler to have tl modules to have definitive decision. However, one of the concerns that ", " is mentioning is that having only four states (Stop, Yield, Go,  Unknown) is not enough in some situations (at least in Japan).", "Even the lighting pattern doesn\u2019t change, whether you are allowed to \u201cGo\u201d or not depends on which way the vehicle is going. In the following example from ", "\u2019s post, the vehicle is allowed to \u201cGo\u201d only straight or to the left, but not to right.", "\n                    ", "\n\n", "Therefore, you would need more than the four states.", "Yes more states should be helpful. Attaching probability to each state, not. I wonder if the following in enough:", "So, I think there are multiple ways to handle the light layout that ", " describes above. Here are a couple:", "Thoughts?", "I am having a hard time imagining how would approach 2 work. What is the image that the classifier will classify? It will have to be an image of a whole light cluster ie: ", "  I don\u2019t think there is a way to subdivide this image any further.", "Also what do you mean by \u201cdirection-based\u201d signals? Who create this signals? Do you mean 3 separate ros topics (left, right, ahead) to publish separately a [r, g, y] state for each?", "Please clarify.", "Maybe I misunderstood the meaning of the lights in your example cluster. I\u2019m not very familiar with traffic lights in Japan. Are each of the arrow lights on the bottom row associated with a single light on the top row or does the entire top row indicate something totally separate from the bottom row?", " will have to correct me. But If I was driving in Japan, I would interpret this as:", "In the above picture:", "Hence the full state of the signal for all 3 directions can only be obtained after recognizing the whole cluster. Hence we wouldn\u2019t be able to subdivide the cluster.", "It looks like the TLD need to be modified for geographical regions anyways. e.g.", "Hence I think it would be more important to design the topics to be flexible enough to cover most traffic rules. Then the implementation can be swapped in and out for different regions.", "WRT the discussion of detection + classification 2 stage pipeline V.S. a single stage pipeline, here are some research on both approaches:", "2 also references a large number of research papers using 2 stage solution, which uses either image processing technique as a first stage, or YOLO-like object detection CNN as a first stage.", "To offer maximum flexibility, I propose the following:", "In a graph", "\n", "To improve ease of internationalisation it would be helpful to split TLD black box into 2 parts,", " - In your revised graph above, wouldn\u2019t the purpose of the \u201cTLD_Vision_Detector\u201d only be to verify that the light exists inside the ROI? Doesn\u2019t this then become a two-stage detection system as was described above? I feel like we might be better off offering alternative implementations of the \u201cTLD_BlackBox\u201d described in your first graph for different countries or just flags which set the country to enable/disable functionality within the node. Thoughts?", "I think Feature_Projection and ROI can be deprecated. Neural networks are capable of detecting and classify traffic lights from the raw image. I left it in the graph to be a \u201coptional\u201d input.", "For example, there could be multiple traffic light in view of the camera. the Vision Detector would  detect all of them. TLD_internationalisation would then take the one \u201cclosest\u201d to the vehicle and output its state.", "This way if special sauce is needed for a particular country, for example right on red in US (default green for turning right), then one need only to modify TLD_internationalisation, and not have to touch any neural network bits in TLD_Vision_Detector.", "That is my intention, but I would have no problem if all this is done as a blackbox inside a single node.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A heuristic approach (", ")", "A DNN approach which utilizes the MXNet framework (", ") and CUDA", "A DNN approach which utilizes the SSD framework (", ") and CUDA", "A list of regions of interest which contain the traffic lights", "A raw image", "The results from the heuristic approach were terrible and nearly unusable in real-world scenarios.", "Both the MXNet and SSD/Caffe detectors contain backbones which were trained using the COCO dataset, which is not available for commercial use. This severely limits the applicability of these nodes since they were designed specifically for these frameworks.", "Much of the functionality is duplicated between the DNN-based nodes.", "Attempting to separate the cropping/ROI extraction functionality of these nodes from the classification functionality leads to either timing issues or chicken-and-egg problems (described in more detail in a bit).", "The ", " node is somewhat naive in it\u2019s cropping mechanisms. Variance in pitch/roll/yaw of the vehicle, multiple bulbs in certain vector map versions, and the field-of-view of the detection camera lead to ROI images which are either too large, too small, only covering a portion of the light, or not pointing at the light whatsoever.", "The recognizer must take in raw images from one or more cameras and detect if a traffic light which exists in the vector map also exists within those images.", "If a traffic light exists in the vector map and is applicable to the currently-occupied lane and direction of travel but is outside the field of view of the camera(s), the recognizer should provide this feedback.", "The recognizer must classify a found traffic light from the image as having one of four states:\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "\n", "The recognizer must publish three pieces of information:\n", "An overall recommendation to the vehicle control system about how to proceed given the detected and classified traffic light(s).", "An image which contains the contents of the raw image plus superimposed squares around the detected traffic lights and their associated classifications (this is currently provided by all classifiers but is only useful for human interaction and not necessary for system functionality).", "A ", " which contains markers for the detected lights and their classification results (again, this is not necessary for system functionality but is provided by all current classifiers).", "\n", "Combine the problems of detection and classification into a single, neural-network-based approach.", "Pros:\n", "Simplifies the architecture immensely", "Reduces compute resources (no image re-publishing, cropping, etc.)", "Gets rid of latency and chicken-and-egg problems", "\n", "Cons:\n", "Difficult to make general-purpose given the feature, language, and library support of the many neural network frameworks that are available.", "Difficult to troubleshoot since the task domain has increased and you can\u2019t obtain partial results.", "People who are using Autoware without a GPU are out of luck.", "\n", "Keep the detection task separate from the classification task but improve ", "'s ROI selection and, in the classification task,separate the actual classifier from the ROS node by either creating the classifier as a library or a separate node with a ROS service call in-between.", "Pros:\n", "Reuses existing code.", "The classification task can be heuristic or DNN-based and the classifier-interface node doesn\u2019t have to care - it just makes a service call or overloaded function call.", "\n", "Cons:\n", "The classifier-interface node can run into latency issues. Whether using a library or separate-node approach for the classifier, the latency between receiving the raw image and ROIs for that image and publishing the superimposed image with classification results is non-trivial. To keep consistency in the published superimposed image, the classifier-interface node has to receive the raw image and ROIs, make a call to the classifier, wait for the classifier to return the result, annotate the image and marker arrays, and then publish them. This can cause incoming images and ROIs to not be processed while the classifier-interface node is waiting for the current classification result. Making this call in any sort of asynchronous way means that you have to manage a list of received raw images and ROIs and coordinate them with the returned classification results.", "If using a library-based approach, only one language can be used and the choice of the language for the classifier-interface node would determine which language would be supported for the classification libraries (e.g., if using C++, can\u2019t use Python for the classifier).", "The detection task essentially remains the same but contains band-aids and hacks to make it better at feature projection in a real-world environment.", "\n", "Use feature projection to either a) determine if a relevant traffic light is in the image(s) AND find the ROI for that light or b) if a GPU is available, just determine if a relevant traffic light is in the image(s) while handing the detection task to a DNN-based node if the traffic light is in the image(s). In addition, separate the classifier as described in 2.", "Pros:\n", "Makes all involved nodes \u201cgeneral-purpose\u201d and usable with or without a GPU.", "Improves the ROI generation with a common approach that is known to produce high-reliability results (the DNN-based detector).", "The placement of traffic lights in the vector map and the yaw/pitch/roll of the vehicle are much less relevant to the detection task because the DNN-based detector only uses the raw image as input.", "The neural network for traffic light detection could be very shallow if only looking for one type of object in the image.", "Each of the stages of detection and classification can be more easily troubleshot and fine-tuned due to increased visibility into the pipeline.", "Same pros as 2.", "\n", "Cons:\n", "More computationally expensive than other approaches because of multiple neural networks running simultaneously and more nodes.", "Same cons as 2.", "\n", "Stop (usually red)", "Yield (usually yellow)", "Go (usually green or blue)", "Unknown", "Camera publishes raw image", "Feature Projection publishes ROIs (or light positions, as suggested)", "Classifier consumes raw image and ROIs (or light positions, as suggested) and produces classification (takes unknown amount of time)", "\n", " consumes raw image and classification", "Simplicity, simplify code and compute graph, reduce large message passing", "More robust, given all problem with current feature extraction node, a DNN approach with some filtering from vector map information should give a lot more robust result. Especially in areas that vector map is inaccurate or lacking.", "Achievability, seems to be a well-explored approach, a lot of networks and training data that can be used. ", ", ", ", ", "\n", "Speed, people running autoware without a GPU should already be in a lot of pain given that perception and tl detection module already reply on NNs. A CPU only simple heuristic node could still be provided by combining current feat_proj and region_tlr nodes, but I suspect the resultant quality would fit nobody\u2019s needs.", "tfd node publish a light color for going forward, turn left, turn right. Default to red. Hence the tfd would be in charge of condensing the different shapes of lights into these 3 channels.", "DecisionMakerNode currently have logic to do straight/left/right recognition by using angle. This node can then make a go/hold decision based on tfd output.", "Have the entire light cluster classified by the tlr in one go. This is difficult to implement because of the required number of statuses for different light combinations and the complete re-training required for the existing neural networks, not to mention the number of images required for each state for each combination of lights for training.", "Create individual, \u201cdirection-based\u201d signals. Using the example image from above, create 3 signals with seperate classification states for each. We would need to add metadata to the mapping format to indicate the direction of travel that each signal controls and then either have the feat_proj choose the correct one based on a \u201cdirection-of-travel\u201d input or have a decision node after the classification decide which to use. The upside of this approach is that the existing tlr NNs would only require a bit of transfer learning to train rather than compete re-training.", "Create \u201cdirection-based\u201d signals as described in 2 but have the classifier learn to distinguish between the different signal types and produce both a signal type and a state classification.", "The top row is the normal traffic light scenario, I.E. 3 lights with R/Y/G. Meaning Stop/Slow/Go for all directions. (I think each position only shows one colour, otherwise, it would be difficult for colour blind people.)", "The bottom row contains modifiers that overwrite the top row instruction. (they are only ever green, they have 2 states, off/green)", "Stop for all directions", "For a left turn, override to Go. For going ahead override to forward.", "In Japan, the green light is blue, the pre-processing of the raw image need to account for that.", "In the US there is right on red, and TLD or other node needs to be able to recognize the (No turn on red) sign.", "In Europe, there is ever only one directional modifier in addition to the main light.", "Join traffic sign and traffic light detection and classification using a single DNN. ", "\n", "2 stage pipeline - ", "\n", "specify a single stage TLD. If multiple stages are required, they can be implemented as internal stages of the TLD.", "feat_proj will continue to exist but improved to take into account the pose of the car", "TLD takes the output of feat_proj as a suggestion but doesn\u2019t wholly rely on the map to provide accurate information", "TLD outputs a go/slow/stop states for each of right/left/ahead directions.", "A decision node will take the output of TLD and path planner to issue STOP/GO signals to the vehicle.", "To deal with geographical differences, TLD node will have different implementation depending on the application.", "Autoware can provide a generic TLD node trained using a publically available database. As there are established research projects in this area, a reference implementation can be done following one of the papers.", "TLD_Vision_Detector to perform purely the task of detecting the lights from an image.", "TLD_internationalisation to integrate information from other sources to condense vision detection into light/right/ahead light signals.", "TLD_Vision_Detector would only deal with running a neural network and is hence very simple.", "TLD_internationalisation would then take multiple signal and discern a left/right/ahead signal."], "url": "https://discourse.ros.org/t/generalized-traffic-light-classification-architecture/9354"},
{"title": "Downloading Dependencies", "thread_contents": ["I looked at ", ", but I didn\u2019t see anything about plans for a new rosdep for ament. Based on the ROS 2 binary installations, I figure that the remote dependencies are managed by brew for Mac, Chocolatey for Windows, and apt for Ubuntu. Are there plans to have a generic tool to download these dependencies based on the package.xml like rosdep? Also, are there plans to generate the necessary files for these dependency managers from a package?", "Unfortunately, the previous version of rosdep didn\u2019t handle downloading repositories directly. It seems useful to enable a complete stack source code build like Mike Purvis\u2019 ROSCon 2016 talk: ", ". Building from source enables cross compiling, link time optimization, cross package bug fixes, native architecture optimization, and dependency artifacts on the fly without a binary server.", "Ultimately, it would be nice to have one command to convert a repository/package(s) into artifact(s). Another tool to help get there would be to convert a package or packages into workspace.", "To answer this properly, I need to layout the workflow we usually use and the different tools we use to accomplish this.", "Here\u2019s the steps first (without details):", "This basic formula is how I start working on everything (except maybe redistributing binaries).", "\nYou can see the pattern in our from source instructions on the wiki:", "And now that list again with more notes in between:", "Ok, so that\u2019s the breadth of what we do with some details.", "\nNow I\u2019ll try to answer you directly by quoting your questions:", "I didn\u2019t see anything about plans for a new rosdep for ament.", "Though ", " does have some ROS specific capabilities, at its core is just takes dependency names, like ", " or ", ", and decides what needs to be done on a system to \u201csatisfy\u201d those as dependencies.", "\nThis is a pretty generic \u201cabstraction\u201d over the OS\u2019s package manager, and is not really ROS specific.", "\nAnd so it should work perfectly fine with ROS 2 as-is.", "That being said we have thought about improvements to ", " and dreamed of removing the the ROS specific stuff from it so that it\u2019s more generic and potentially useful to many other projects outside of ROS.", "\nSpecifically ", " worked on something called ", " while interning at OSRF, and it was meant to be the spiritual successor to ", ":", "xylem - A tool for resolving dependencies in a platform agnostic way.", "Despite his really awesome work, I\u2019ve not had the resources to capitalize on it and push for it to replace ", ".", "\nI\u2019ve always hoped we could make use of it in ROS 2, but it\u2019s really an orthogonal issue as ROS 2 could easily use ", " as well, it would just be nice to use ", "'s new hotness ", ".", "\n", " put together some extensive design documentation and the tool is really close to being ready for use.", "\nIf you\u2019re interested in tools like ", " I highly encourage anyone to have a look at his work:", "So, to answer you, we don\u2019t need a new ", " for ", ", but we\u2019d like to have one ", ".", "Based on the ROS 2 binary installations, I figure that the remote dependencies are managed by brew for Mac, Chocolatey for Windows, and apt for Ubuntu. Are there plans to have a generic tool to download these dependencies based on the package.xml like rosdep?", "Yes, we\u2019ll likely use ", ".", "\nWe are currently working on packaging some dependencies like ", " and ", " for chocolatey on Windows, and still sort of feeling out what the best thing to do there is, see:", "These aren\u2019t official, and will likely move, but for those interested in seeing what\u2019s been tried\u2026", "Unfortunately, the previous version of rosdep didn\u2019t handle downloading repositories directly.", "Actually ", " does have this ability, it\u2019s called \u201crosdep source installer\u201d (I think).", "\nThey\u2019re not used much anymore, because they\u2019re terribly difficult to maintain.", "\nThey looked something like this:", "Since then we\u2019ve deliberately chosen to avoid stuff like this and try to only use OS package managers to resolve dependencies.", "\nThe other point is that you can emulate this by creating a cmake package (or catkin/ament package) which does this \u201cfrom source build\u201d for external dependencies.", "\nWe do this some for ROS 2, for example with ", " (cmake based) because we needed it on Windows and didn\u2019t know how to package it yet, see:", "poco_vendor - CMake shim over the poco library: https://github.com/pocoproject/poco", "Basically, if it cannot find the right version of poco, then it downloads and builds it in place.", " also touched on this topic:", " (search for ", ")", "As for cross-compiling, there are entire toolchains for achieving this, like OpenEmbedded, Yocto, and others.", "\nI am not confident that is a role ", " needs to play, but we can certainly make sure to help support those patterns if we can (i.e. not get in the way).", "Ultimately, it would be nice to have one command to convert a repository/package(s) into artifact(s).", " can provide you with Debian packaging files on a per package basis, and the Debian packaging toolchain can provide you with binary artifacts.", "\nThe same is true for any packaging scheme (Fedora, Homebrew, Chocolatey), in principle.", "\nROS Answers on how to do this locally, see:", "Another tool to help get there would be to convert a package or packages into workspace.", "Not sure what you mean by that.", "\nWe have tools that will build lots of local packages into a single destination folder.", "\nWe have people using this ability to wrap up the result and distribute that, see ", "\u2019s presentation.", "\nI think these tools should continue to work for ROS 2 with little to no changes.", "Sorry for the long post, but hopefully it answered your questions.", "Thanks for the detailed article.", "It\u2019s smart to use a package manager instead of building yet another one. I looked at Conan(", ") and vcpckg(", "), but I guess they are C++ specific.", "For clarification, of that last point. I was thinking of a tool/script that takes a repository/package and builds a workspace around it. Basically, create a workspace folder with a src subfolder and move the package into the src folder. That way ament can run shortly after cloning a repository/package in an arbitrary location. It takes some time to understand the workspace structure and to know how to convert arbitrary packages into runnable nodes.", "I was thinking of a tool/script that takes a repository/package and builds a workspace around it.", "There is no tool that automates so much of the process, but there is an issue on ", " to add an option which would help you do it in a few commands:", "Basically it would be like \u201cI have some package(s) in this folder, and some things installed from binaries, please get me the list of things that are missing for me to build these package(s) I already have.\u201d", "It would be a cool contribution to extend the tool in that direction ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Generate a list of things to download, i.e. \u201cwhat do I need to download to build ", "?\u201d", "Download everything based on that list", "Install the remaining dependencies, e.g. non-ROS things like ", " and ", "\n", "Build everything, in the correct order", "Redistribute binaries", "Generate a list of things to download, i.e. \u201cwhat do I need to download to build ", "?\u201d\n", "We use ", " for this and it draws on data in the \u201crosdistro\u201d", "it generates a ", " file based on your query", "it only gets the location of things that are \u201cROS packages\u201d, i.e. things released in the rosdistro", "it does not figure out how to download things like qt or boost", "it is used in ROS 1 and will likely be used more-or-less unchanged in ROS 2", "can also answer \u201cwhat do I need to download to use ", ", minus ", " and its dependencies?\u201d\n", "you might want to do this so you can install ", " and down from binaries", "\n", "\n", "Download everything based on that list\n", "We use ", " for this (some power users use ", ", we use it in ROS 2)", "Given a ", " (or a similar list of things to go get), it can fetch all the source code", "We\u2019ll likely use both in ROS 1 and ROS 2", "\n", "Install the remaining dependencies, e.g. non-ROS things like ", " and ", "\n", "It can also install binaries for \u201cROS things\u201d if you don\u2019t intend to build them locally and there are binaries for them on the target OS.", "In ROS 1 we use ", " for this, it would install remaining dependencies", "In ROS 2 we just have instructions in the ros2 github wiki on what to install, we don\u2019t use ", " yet", "\n", "Build everything, in the correct order\n", "used to help us build software locally\n", "either to do development or", "to build for a new OS/architecture which lacks binaries", "\n", "software might be anything supported by the build tool, like:\n", "catkin or", "ament or", "cmake or", "python with ", " files", "etc\u2026", "\n", "we use mostly catkin in ROS 1 and mostly ament in ROS 2, but that doesn\u2019t really affect this part of the process", "all result in an \u201cinstall-like\u201d folder which can be used locally", "\n", "Redistribute binaries\n", "One option is to use ", " and release on ", " through rosdistro\n", "\n", " converts the contents of ", " to debian (or others like fedora) packaging files", "this happens on a per package basis (no bundling)", "\n", " uses these files to build binaries and upload them to ", "\n", "\n", "Another option is something like what ", " presented at ROSCon, which builds lots of things at once and bundles the result", "Both options should work for ROS 2, we\u2019re currently looking at any changes that would need to be made for ", " to work on ROS 2 packages (shouldn\u2019t be much)", "\n"], "url": "https://discourse.ros.org/t/downloading-dependencies/1489"},
{"title": "Get ROS2 working on ArchLinux", "thread_contents": [" might be one of the most radical(or just the most?) Linux distributions, with a rolling update strategy, Arch makes everything latest. So we can enjoy the new versions of every package, while at the same time, annoyed by the version compatibilities. But for ROS2, Arch is a great distribution to think about. For Linux distributions beside Ubuntu, the only way getting ROS2 work is to build it from the source code, so a flexible and friendly package manager is really NEEDED to make all dependencies collaborate. That is what pacman(Arch\u2019s package manager) is really good at.", "For now ROS2 doesn\u2019t have a explicit package grouping like ROS1, I mean, something like ros-core, ros-desktop and ros-desktop-full. (Maybe I\u2019m wrong on this point, if there are, let me know and I would be very glad to maintain AURs for ros2 on Arch). So what we get in the post is a full version of ROS2, as you may see, there will be some UI & visualization dependencies like qt5 and rviz.", "Just like Arch Linux, let\u2019s be radical:) We will use the latest Eloquent in this post, the .repos file is from the eloquent branch on github:", "The ", " will lead you to the Arch wiki on ROS2 here:", "For Arch, there is already an AUR package to install ROS2 dependencies: ", ". While it is already outdated, we can still get the basic deps from its installation. So first step, is to install the deps with an AUR package manger(I use yaourt here, you can definitely choose your favorite):", "After that, we install some of the essential deps from official sources, with pacman:", "Before moving on, let me first show you some hair scratching troubles that you may have when building ROS2 by your own. Relax, I\u2019ve already scratched my hair and detected all the mines. It would be easy for you to get them over.", "OK, here comes the first pitfall, the package ", " relies on ", ", which refers to ", ", and ", " interfaces change in the latest 2.34. Since Arch by default keeps everything latest, we have to downgrade the ", " package to version 2.32. Thanks to the design of pacman, this can be easily done with the command:", "colcon is used to build ROS2, according to the Arch wiki, the ", " will install colcon related AUR packages, but these package are not maintained up to date, when building ROS2 with the outdated colcon package, an error complaining \u201cvariable \u2018done\u2019 is assigned before declare\u201d will occur. We can fix this by installing the latest colcon related packages with pip command:", "The package ", " will need some display environment, under Linux, the ", " package is needed, but not listed explicitly above, so install it manually:", "The ", " module will need a qt SIP binding for python, we have installed the qt5 bindings above, while the binding also needs a configuration in path ", ". The latest package ", " does not provide the configs after an architectural change, while the package for python2 still provides it. So here is a trick to make python-pyqt5 work for ROS2:", "After the installation, make sure the path ", " is there.", "We\u2019ve get everything ready for installation, we can now follow the Arch Wiki guide to prepare local environment and get code:", "Then build the project with colcon:", "Follow the Arch Wiki, let\u2019s run a topic pub/sub example:", "Open a new ssh session and sub the toplic, the message \u2018data: Hello World\u2019 should be printed on the screen:", "Something I didn\u2019t mention in the post above, is that some packages are not fully built since the missing of some features, some might give you warnings. That\u2019s OK, that\u2019s what we should think about in the next steps. There are some points I can come up with for the future.", "For now ROS2 doesn\u2019t have a explicit package grouping like ROS1, I mean, something like ros-core, ros-desktop and ros-desktop-full.", "I believe there are, see: ", ".", "Aha, thanks ", " that\u2019s really helpful. I\u2019ll try to build them separately and see if there are issues to fix. ", " So at least we can have ros2_base, ros2_core, and ros2_desktop to maintain.", "If you start working on improving support for a new distribution, consider:", "Add Arch Linux support to ", ". Given the sheer number of packages in ROS, doing it by hand is not really an option, unless you are determined to only ever support a small number of core ROS/ROS2 packages. superflore, like Bloom, rely on ROS internal packaging information (", ") to generate a package for a target distro. The suggestion here is to add Arch as a target. Given the nature of Arch packages, it is likely to be do-able. It is tricky if you\u2019re targeting official inclusion, but that definitely should do the trick for AUR.", "Add/update/maintain ", " keys for everything arch related. ", " is used by ", " to generate the commands required to install system deps for any package (", ") on the target distro.", "\nIt is likely that Arch ", " support could use some help, so adding the keys, and making sure ", " is using whatever makes the most sense on Arch would be a very valuable contribution.", "Hi, Thomas, thanks for the great advices, I\u2019ll look into that, hope we can make Arch a good choice for ROS fans.", "Add Arch Linux support to ", ".", "I seem to remember some earlier interest in this and work towards implementing it (ie: ", " support for Arch).", "May be good to first try and find it ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Make the ros2-arch-deps AUR package updated", "Split the ros2 project into groups(ros2-core, ros2-desktop, ros2-desktop-full, just like ROS1)", "Possibilities on other distributions(like CentOS) outside ROS2\u2019s support list", "Try ros2 on Raspberry Pi Arch Linux, we will need ROS2 on the Arch powered edge devices", "Make the ros2-arch-deps AUR package updated", "Split the ros2 project into groups(ros2-core, ros2-desktop, ros2-desktop-full, just like ROS1)", "Possibilities on other distributions(like CentOS) outside ROS2\u2019s support list", "Try ros2 on Raspberry Pi Arch Linux, we will need ROS2 on the Arch powered edge devices", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/get-ros2-working-on-archlinux/12827"},
{"title": "ROS2 Logging", "thread_contents": ["Hello all,", "My team at Amazon is looking at extending the existing logging functionality in ROS2. I\u2019ve described below what our initial plans are for logging and the justifications. I welcome any feedback on the plan.", "Currently in ROS2 most of the logging implementation exists in the rcutils library. The interface into it is a set of macros that get generated during the build and a few functions in rcutils that back them. A string representing the logger name is used as input to these functions to identify which logger is being used/modified. Each implementation of the RCL then provides its own logging interface. In the RCLCPP library, for example, it is again a set of generated macros that are backed by the rcutils macros. When a logger is created in one of the language RCL libraries, it does not call down into the rcutils library to initialize any state for that logger.", "Loggers can be created in association with a node or on their own. Loggers associated with a node are identical to loggers not associated with the node except that the name of the logger is automatically set based on the associated node\u2019s name/namespace. Logs can be created in a hierarchy based on their name. The hierarchy can be used to adjust the severity level at which a logger operates.", "The logging functionality in the rcutils library currently allows for only a single output function  to be set for all logs. It defines a typedef for the output function header and allows the output function to be changed by calling a setter function with a pointer to a new output function. The rcutils library also includes one output function implementation which sends logs to stdout. The output function cannot be changed for different loggers in the hierarchy. Every logger uses the same output function.", "These are the changes my team is looking to implement.", "Increase the number of output functions that can be set in the rcutils layer", "I would suggest to keep a single output function in the core API (simpler, less memory management required). To enable multiple handlers you could create a new function which is capable to dispatch the call to N other functions.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "Yes, the proposed approach should be able to support fluentd. From what I can see, the easiest way to hook fluentd into this proposal would be to use a ", " input to get the log events sent to to the log files. If more advanced features are needed you could always recompile the nodes and swap out the shared logger library with something that hooks directly into fluentd.", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Yes, that should be possible. Since the shared library is linked at runtime it should be possible to just link in a different implementation. I\u2019ll need to research a little more to see what the most convenient way to do this is. It may still require you to recompile a shared package and source it in your workspace. If other people have thoughts on an easy way to do this I\u2019d be interested in hearing.", "Will the approach proposed here allow for using something like fluentd as an output? For complex robots, a tool like fluentd provides useful facilities for managing, viewing and processing logged information.", "(This is Forrest from the same team in Amazon) Yes our goal is to have an interface so that different types of sinks can be implemented and hooked.", "That said, I would suggest the sink should be fluentd agnostic even if you want to use fluentd to consume the data \u2013 or perhaps I misunderstood you in this case\u2026", "Is it possible to come up with a way to swap out logging systems without needing to recompile nodes?", "Technically this is doable, but we have to be very careful because IMHO logging mechanism should be straightforward to achieve the best robustness possible. Is there a specific use case in your mind that needs such capability?", "I\u2019m thinking about how nodes are typically installed from binaries. If I install nodes from binaries but want to use a different logging daemon to the original node developer, then I lose the ability to install from binaries.", "My understanding is that even when you install Nodes from binaries that ROS dynamically links them from your sourced workspace when they run. That should allow a developer to use some environment variables or some other mechanism to swap out the logger for any predefined ones at runtime or replace the shared library logger by providing their own with the same interface to be linked in instead. I don\u2019t know off hand how difficult or easy that is to do, but it is all possible.", "As noted in a ", ", ros1 logging doesn\u2019t support unicode strings, so it would be nice to support that in ros2.", "We\u2019ve created the first set of pull requests for the new logging features (links below). Our next steps are to work on adding rosout topic capability. As part of that we are planning on porting the Log message from ROS 1.", "For the new ROS 2 message I would like to remove the list of topics the node is publishing on that is in each log message as I don\u2019t think it makes sense to include that with every logged line. I\u2019ll also be adjusting the verbosity level constants to match the values defined in the rcutils package. The final change I was looking at for this message was to move it into the rcl_interfaces package instead of keeping it in a rosgraph_msgs package.", "Links to pull requests below.", "rclcpp: ", "\nrcl: ", "\nrcutils: ", "\n", "A logging implementation for ros2 that using log4cxx.  - ros2/rcl_logging", "\n", "Edit (Nov 14th, 2018 9:45am PST) - Changed the package we\u2019re planning on moving Log message into from common_interfaces/diagnostic_msgs to rcl_interfaces to avoid adding another dependency in rcl.", "Another pull request for the Log message definition.", "The initial implementation of the rosout features is code complete as well and available in the branches listed below.  I\u2019m going to wait to start the pull request for the rosout changes until the existing pull request is complete.", "Common C functions and data structures used in ROS 2 - nburek/rcutils", "\n", "\n", "Library to support implementation of language specific ROS Client Libraries. - nburek/rcl", "\n", "Please go ahead any missing PRs asap (and add a note in it what it is blocked on). We are very close to the API freeze and can\u2019t consider what is not visible anywhere.", "I\u2019ve gone ahead and created pull requests for those features, though I could not find a way to decouple them from the earlier commits being reviewed in the existing pull requests. Once those PRs are done I can rebase these so only the new commits show in the rosout PRs.", "\n", "\n", "\n", "There was some interesting development in rosconsole from ", " to use pluginlib to support different logging sinks, particuilarly to support journald (", "). I wonder if you could consider a similar approach for logging in ROS2, which would provide a familiar mechanism for swapping logging sinks without recompilation.", " is your desire to be able to actually develop custom logging sinks (a reason for using pluginlib), or are you simply suggesting that ROS2 should be able to log to a number of backends which should be configurable at runtime (not necessarily requiring pluginlib)?", "I have a pretty limited number of sinks in mind (stdout, /rosout, journald), but given the fluentd suggestion above (and that ROS2 is cross platform), being able to develop a sink plugin outside of the rcl source tree via pluginlib seems valuable.", "Of course, you can always just write /rosout to journald via an intermediate node, but this can be expensive when intra-process transport is not available.", "it would be very nice if we can choose the backend logging system, cz some vendors have their own specific logging system once it comes to the embedded system.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Increase the number of output functions that can be set in the rcutils layer. We will add new functions to add and remove functions from a list of output functions in the rcutils library, but will not add this capability to the RCL interface. For the initial implementation, we will not add the ability to change the output functions at different logger hierarchies.", "Write a new log output function in the rcutils library that will forward all logs from a logger associated with a Node to a ", " topic on that node.", "Define an interface for a shared library that wraps more power open source logging libraries such as log4cxx. The interface will include initialize/teardown functions, call throughs for when logging metadata changes (such as log levels and formats), and functions to output logs from the rcutils library. We will also define a way of passing through a configuration file location so that the shared library implementation can use the standard config format defined by the library that backs.", "Write a new log output function in rcutils that will call out to a shared library that implements the above interface.", "Write an implementation of that shared library that wraps the log4cxx library. This implementation will include a default configuration that will send all logs to a file.", "Modify the rcutils library so that when it initializes it will hook up the three output functions that now exist in the rcutils (stdout, rosout, and shared library). These will each be able to be individually enabled/disabled via an environment variable.", "Why not add the new interfaces for adding log output functions to the RCL layer so that people implementing their own nodes can hook up additional loggers?\n", "If someone really wants to do this it will be possible by the RCL handle for their node and directly using the rcutils library. The reason for not adding to the RCL interface to make it easy to do in every language is that we do not want to encourage node developers to add their own custom logging in this way. It is better to conform to relying on only the outputs in the rcl/rcutils libraries so that your node can be more easily integrated into other applications. If an application developer wishes to use your node in their application, but wants total control over how logging is handled, it is a better experience for them to only need to adjust the standard output mechanisms natively provided by ROS 2.", "\n", "Why not also add the ability to set different output functions for different logger hierarchies?\n", "We are not providing this as part of the native rcutils library as a simplifier for our initial work. This functionality would be available as part of the shared library output that we are proposing. So anyone who really wanted that level of control could get it for both stdout and file logging by adjusting the configuration file for the shared library logger.", "\n", "Why not send all logs to the rosout topic instead of only those sent to loggers associated with nodes?\n", "Because of the way ROS 2 associates DDS concepts with ROS Nodes, there didn\u2019t seem to be a clean way to setup a general topic on a process that wasn\u2019t associated with an existing Node. Since a process can also contain multiple ROS Nodes, we didn\u2019t think it would be good design to just pick one to have everything published to.", "\n", "Why the shared library?\n", "We went with the shared library approach in order to provide a way to hook into an existing logging library without tying ROS 2 to only using that library. Wrapping an existing logging library will give us the ability to have features such as file logging, log file rotation, and a hierarchy of output sinks without the need to implement all of that in the ROS codebase.", "\n", "Why keep the existing stdout logger implementation in rcutils instead of relying on the shared library to provide the stdout implementation, since most major logging libraries already have those created.\n", "We decided to keep the existing stdout output function because it provides a standard in the cases where someone does not want to rely on the shared library logger. Since you can enable/disable any of the three natively provided log output functions it is easy for a user who wants more control to disable the ROS provided stdout and rely on the", "\n", "Since the shared library will be used in the rcutils layer will it integrate with the custom allocators that the rcl/rcutiles use?\n", "No, we are not planning to provide a hook into the custom allocator for the initial implementation. This could be added later, however, most of the open source logging implementations that are well supported and provide rich features do not provide interfaces for providing your own allocator. We did not want to pass the allocator into the shared library when the shared library wouldn\u2019t actually be able to use it. In the case where someone needs the control over allocation they would need to recompile anyways and at that point could swap out the shared library implementation with a logger of their choosing that provides more control over memory allocation.", "\n"], "url": "https://discourse.ros.org/t/ros2-logging/6469"},
{"title": "IPC in ros2", "thread_contents": ["Does ros2 use inter-process communication when we use it in  linux OS ? What about in case of bare metal micro-controllers where there is no OS?", "In the future, please set the category to \u201cNext Generation ROS\u201d when discussing ROS 2.", "Does ros2 use inter-process communication when we use it in  linux OS ?", "First, I\u2019ll assume you mean \u201cdoes ROS 2 use any operating system IPC mechanisms between nodes on the same machine, rather than sending data over UDP?\u201d, because the term \u201cinter-process communication\u201d only means communication between two processes, but does not imply what the mechanism is (i.e. UDP or TCP can be IPC).", "It depends on the middleware implementation you are using. Currently the default of Fast-RTPS does not use anything other than UDP single cast or multicast. It could, RTPS allows for this in the protocol. I believe some of the proprietary implementations provide a shared-memory based IPC mechanism when on the same machine. Fast-RTPS will hopefully support this in the future.", "What about in case of bare metal micro-controllers where there is no OS?", "There have been some experiments which use RTPS on a OS-less microcontroller to communicate with ROS 2 on a desktop and some other experiments to get our ROS 2 API compiling for microcontrollers, but there is no out of the box support for OS-less ROS 2 at this time. We did enough work to convince ourselves that is possible, but haven\u2019t followed through with sustained support because we didn\u2019t have time, though we would love to do that in the future.", "In that case though, with no OS, you probably would not be using processes so the idea of \u201cInter-", " communication\u201d doesn\u2019t really make sense. You would be using something more like our \u201c", "-process communication\u201d, which can avoid sending the message data over the network (UDP in most cases).", "RTI\u2019s DDS does use shared memory internally when possible, I think.", "Hi sagniknitr,", "Fast RTPS will support shared memory in future releases, is in our roadmap. Regarding microcontrollers we are developing a lightweight version of Fast RTPS based on an OMG future standard for constrained resources devices. Of course it is not a full DDS/RTPS implementation, but a reduced API and different protocol.", "You can see the first prototypes in the ", " use case, and we are working in an release now. This will let you to use ROS2 in micro-controlllers.", "If you want more details, please contact me.", "Jamie,", "Thanks for the update about how Fast RTPS will be evolving.  A few questions, is the prototype with Dronecode you\u2019re referring to the work you did on the ", "?  Additionally, can you point to any public links regarding this new OMG standard for constrained resources? I know that the ", " is publicly available, is a draft of the document you\u2019re referring to available?", "Thanks", "\n- Eddy", "The DDS XRCE (\u201ceXtremely Resource Constrained Environments\u201d because all OMG specifications need a cool acronym) specification isn\u2019t even close to finished yet. The ", " but you have to be an OMG member to get the working documents, unless someone involved is willing to provide them. When the specification is adopted, that becomes publicly available.", "As seems to happen a lot with the DDS specs these days, the RTI/eProsima/Twin Oaks camp and the PrismTech camp have produced their own versions of the XRCE specification and are struggling to reconcile them into a single specification that the OMG can adopt. I won\u2019t wade into the debate over which is better, but I\u2019ll let you draw your own conclusions from the fact that one of the two camps only has one company it it. ", " I hope it\u2019s not going to be the RPC for DDS spec all over again; the arguments over that could be heard in other meeting rooms. But it seems likely that it will be a year and a half or more before the final specification is released.", "Thanks for the informative reply ", ", I had no idea the work was under way.", "I attended the OMG meeting last week, so I took the opportunity to hear what PrismTech and RTI/TwinOaks/eProsima had to say about their competing submissions. The following are the notes I took during their presentations. Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "Prismtech presentation:", "RTI/eProsima/TwinOaks presentation:", "My summary:", "Thanks for the summary ", "!  I\u2019m very interested in how the work progresses", "Thanks Geoff, great stuff!", "Hello ", ",", "First of all I\u2019d like to point out that I am one of the author of PrismTech\u2019s/ADLINK XRCE proposal, thus you may think I am giving you my perspective, in that case I urge you to read original documents to see how what I am providing here are facts. That said, let me make a few rectifications to some of the points above.", "You say:", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure ", " You can find the result of our analysts here ", " but you are welcome to derive them by reading both specs.", "This is also partially correct. We had asked to present what would have been our final proposal in Bruxelles to give a chance to the task-force to digest and one more opportunity to the competing team to join. This has nothing to do with the Vote-to-Vote and the fact that the task-force did not decide to allow the Vote-to-Vote procedure. The OMG has very complex rules\u2026 I know that can seam strange, and they still surprise me after having spent more than 10 years dealing with it.", "The RFP (which I wrote) asks for a protocol not for an API.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "\nAdditionally, you may argue that this may add some complexity in the message parsing \u2013 but again \u2013 I think that checking a flag and deciding wether some field is going to be present or not does not belong to the realm of hard. It is also worth  pointing out that some flags are just informative and don\u2019t require any kind of branching in the decoding. Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes ", " And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple ", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples. Our proposal currently has 4 compared to that of RTI/TwinOaks/eProsima which has 16 bytes. We have had our implementation run on a Makeblock robot using BLE with an MTU of 20 bytes.  You can check the comparison on this deck ", " and should wonder why the competing proposal did not provide a proper analysis of the wire overhead. Additionally when leveraging batching we have a wire overhead of (3+n)/n where n is the number of samples being batched.", "Some other thing worth point out is that the protocol allows for data to be pushed, pulled or periodically pushed or pulled. The write protocol also allow to pace the streaming of data that results from a remote query. This is extremely important when dealing with resource constrained nodes that need to consume data little by little.", "I did not hear RTI saying:", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those. The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP. The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "I\u2019d like to understand why you say this:", "What do you think is our submission cutting out? We are wire efficient yes, extremely wire efficient but at the same tine we support:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions. I\u2019ll also be more than happy to answer any question you may have about our protocol.", "A+", "Dear ", ",", "You are being to nice to PrismTech as everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises. I think it is best for people to look with their eyes at what the two submission can do and make their own decisions.", "On my side I like debates, I like inquisitive minds and I like hard questions. Thus I\u2019ll be more than happy to answer any question on the XRCE protocol proposed by the PrismTech/ADLINK team explain why it is better than RTI proposal\u2026 And actually it is better than RTPS.", "Thus, please let\u2019s start the open debate to dissect the reasons why our proposal is the one which should be voted and by far the better one.", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being ", "A+", "Thanks ", " for jumping into the community in such an energetic manner. I think we all appreciate having one of the authors of the proposals providing feedback. That said:", "Remember while reading this that it is based solely on the presentations. I haven\u2019t read the submissions themselves in any detail yet.", "which answers:", "At this point my question is have  you\u2019ve read both specification? If not, I suggest you do our is available here ", "I think I\u2019ll stop here\u2026 for the time being ", " .", "Hi Angelo (", "),", "Coordinate different companies to get a common view on a complex matter is always hard, and in this case, there are multiple design options leading to different tradeoffs.", "Our submission (RTI, Twin Oaks & eProsima) already aligns the views of three different DDS vendors, and sure we will try to incorporate ideas of your submission.", "Our submission tries to accomplish the following:", "We coded a PoC of our submission, and during the presentation, you asked about numbers. At that point, with no optimizations at all, and in debug mode we answered 43Kb. I asked my team to optimize a little bit the code, and here are the numbers we have now for the client:", "Total Memory Use: 8 Kb", "But we could squeeze that even a little more. We are releasing this as Open Source (Apache 2) so anyone can review the results.", "But again, we are not aiming to be as small as possible. We are covering all the requisites of the DDS-XRCE RFP, and testing our solution in what we consider typical microcontrollers today.", "Regarding the process at the OMG meeting, we (RTI, Twin Oaks & eProsima) didn\u2019t want to confront both specifications and choose one of them, but have the time to incorporate ideas from your submission, and that is why now we have an extended deadline.", "Let me join in the fray. I\u2019m the other author of the PrismTech submission and the one who built our tiny prototype. I wasn\u2019t present at the OMG meetings, and I won\u2019t waste any words on what may or may not have happened there.", "Firstly, I don\u2019t think a contest of bytes of RAM adds real value to the discussion, although of course it is an honourable contest in itself ", " I am surprised that you, ", ", had never even had a proper look at the memory use of your implementation given the purpose of the exercise in the first place, but if it is 8kB now then it is much better already \u2014 if still 7kB overweight ", " In any case, memory use is determined more by the implementation than by the protocol messages.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already. Yet even that is not of such great interest to me in this discussion.", "What really matters in my opinion is a difference in philosophy. The two proposals suggest very different views of what one would ideally want to accomplish.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that. In a sense, it is just a hand-crafted alternative to CORBA with a lower overhead. (Simply using CORBA actually \u201cjust works\u201d if the DDS implementation is faithful to the IDL interface mappings, even if it is ugly.) To me, this route is a pragmatic way of going about satisfying the RFP, but at the same time, an uninteresting one. (Sorry ", " and others \u2026)", "We chose to design a compact protocol that can support what amounts to performing DDS operations remotely, but doesn\u2019t limit itself to it. Instead, it also supports a DDS-like peer-to-peer network with vastly lower overhead, and, in many ways a level of flexibility in specifying what data is of interest (through URIs and selections) that DDS doesn\u2019t natively support.", "All of that would be of little value if it doesn\u2019t perform well or doesn\u2019t scale well. Just like the code size and memory use are mostly determined by the implementation, so is maximum sustainable performance more determined by implementation than by the details of the protocol headers. Size-wise, my prototype can run as a client on an Arduino Uno (8-bit CPU, 2kB RAM). A small test application using the same implementation configured as a peer easily sends ~700k 8-byte msgs/s from one RPi3 to 3 others (CPU is << 100%, network load ~75% of Fast Ethernet, so I really should investigate why it isn\u2019t faster), and goes another order of magnitude faster when run over local loopback on my MBP. That\u2019s better than your typically DDSI implementation. Is this relevant? That depends on whether you have high rate, tiny messages \u2026", "Now my test application doesn\u2019t implement all of DDS \u2014 not even close \u2014 and this is another significant reason why it can do this with only a few kB of code and RAM. At the same time, this is, I believe, where it gets interesting for ROS2.", "As ROS2 has its own middleware abstraction layer that uses only a fraction of the DDS feature set, putting ROS2 directly on our protocol would get you a smaller and a faster system. Smaller and faster usually allows doing more interesting things, even if I can\u2019t say what exactly those interesting things will be.", "Disclaimer: I can\u2019t do run ROS2 over it today, there\u2019s more work to be done on my prototype before it supports all that is required. And I wish none of you would have to take my word for the data I mentioned, but that is not something that is in my power to solve today.", "It looks like I started something of a minor storm moments before starting a holiday\u2026", "I\u2019m thankful that the DDS vendors, PrismTech, RTI, Twin Oaks and eProsima, are all engaged enough in the ROS community to be present on the Discourse board. It is encouraging to future adopters of ROS 2.", "We are Engineer and Mathematicians not priests. Thus we don\u2019t believe we measure", "I wasn\u2019t implying religious believe. It\u2019s an simply expression to describe someone making an assertion. ", " I fully agree that PrismTech\u2019s submission has much smaller messages than the RTI/Twin Oaks/eProsima submission based on the two presentations alone.", "The RFP (which I wrote) asks for a protocol not for an API.", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "First off, the protocol has a single byte header of which only 3 bits are used for flags and the remainder 5 bits for message-id. There are two flags that are used consistently to identify Reliable and Synchronous messages and another few flags used to mark the presence or absence of some information in the message. Personally I don\u2019t find that daunting complex,  it is quite normal in protocol to use flags to this end.", "It is quite common to use flags. TCP is full of them. My concern is that the protocol is, in my opinion, undoubtedly complex and, based solely on the presentations, more complex than the other submission. Complexity and size are often a balance and in this situation we appear to have one submission at each end of the balance.", "Finally, what I can tell you is that with this protocol we have measured performances that  literally blow away RTPS! If you are curious we\u2019ll share the numbers\u2026 Or better make available the code for you to see with your eyes", "With the sorts of overhead you are achieving, I\u2019m not surprised performance is amazing. I\u2019d still like to see numbers, though. ", "And BTW, if our complex specification can fit in 1KB and RTI&Co simple protocol fits in 43KB\u2026 There is something wrong\u2026 Thus either our is not so complex or their is not so simple", "As was stated elsewhere in this thread, eProsima\u2019s implementation wasn\u2019t optimised. Since you said you are trying to bring yours to market already and eProsima claimed theirs was a tech demo, I\u2019m not surprised yours is more optimised and thus smaller. Of course, the numbers are definitely in your favour for RAM usage. But I\u2019m curious how much program memory each implementation requires, too. This is often the limiting factor on embedded microprocessors rather than the RAM usage.", "As I\u2019ve explained several times during previous OMG meeting we have customers count bytes, and in some use cases they are not willing to spend more than 7 bytes wire overhead for data samples.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "But this is far from being true. The AB review should be public and I can ask permission from the AB to post those.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "The truth is that the two reviews of the AB raised questions concerning whether the submission was actually answering the RFP.", "In that case I am very interested in seeing what these AB members wrote.", "The reason why RTI did not want to vote is that their submission \u2013 if selected would have been killed by the AB. That is as simple as that.", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "What do you think is our submission cutting out?", "\u201cCutting down\u201d, not \u201ccutting out\u201d. I meant that you are trying to reduce the size of the messages on the wire as much as possible, at the expense of possibly needing more complex parsing code. I didn\u2019t mean to say that you are cutting out features. It was clear from the presentation that PrismTech supports more features and has more flexibility than the other submission. But there are trade-offs involved.", "Dynamic Discovery (RTI does not, please read their spec!)", "\nPeer to Peer Communication (RTI does not)", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Non IP Transports (RTI does not)", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "Generalised Queries (RTI only supports DDS-like queries)", "Well, it is ", "-XRCE, is it not?", "I hope this was useful in clarifying a few aspects and I am looking forward to get some feedback once you\u2019ll have read both submissions.", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "everyone knows that consistency in design and elegance is seldom achieved through multi-vendor compromises", "While this is true, we are operating at a standardisation organisation, not a rubber stamp provider. There are interested parties beyond just the implementers. We would prefer not to just hold a vote on which submission to go forward with. We would prefer the submitters to actually work together and produce a single submission that combines the best of both without any technological compromises (yes, I\u2019m aware how hard that is).", "I\u2019ll give you another small hint of why our XRCE proposal is an improvement over RTPS\u2026 Do you know how the RTPS protocol deals with discovery? What happens when you have loads of topics, readers and writers in your system and very asymmetric nodes?", "Have you ever tried to do a one shot write in DDS? How much protocol traffic are you going to generate to make that happen\u2026 And how many entities do you have to create?", "I\u2019ll  stop here\u2026 for the time being", "Or, you could provide the answers to those questions, along with the equivalent answers for your submission, so we can see and compare the data to back up your claims.", "Our submission tries to accomplish the following:", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "\nNeither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use.", "This is the strongest impression I got from the presentation, as I said in my own notes. The data model is similar to DDS, which makes adoption by existing DDS users easier, and the ability to implement the protocol in a relatively simple piece of code (which makes it easier to verify and certify) was considered as important as saving bytes on the wire. I\u2019m not sure where the correct balance is between these two requirements, but the PrismTech implementation really gave the impression of being at one extreme.", "testing our solution in what we consider typical microcontrollers today", "This is something that I think is really relevant but that PrismTech have not addressed at all, and RTI/Twin Oaks/eProsima have not addressed enough. What are the typical microcontrollers in use today? What are the target environments for this protocol to be used in?", "The RFP explicitly says this:", "From a high level perspective, the key requirements that a DDS-XRCE implementation has to satisfy are (1) extremely low footprint \u2013 addressing targets with less than 100 Kbytes of RAM, (2) extremely efficient wire protocol \u2013 inducing a protocol overhead of just a few tens of byte over the user data, and (3) support devices that undergo aggressive sleep cycles.", "Both submissions fit within both the RAM usage (with much room to spare) and the protocol overhead.", "More specifically, the actual mandatory requirement is:", "6.5.3. DDS-XRCE protocol overhead, when sending or receiving user data, shall not exceed 24 bytes per packet.", "Let M be the size of the DDS-XRCE message sent out, and U the size of the user data, then M-U <= 24 bytes.", "The precise overhead on the wire is of more interest, as this is fundamental to the protocol. BLE gives you 20 bytes to play with, and a difference of a few bytes of header adds up in that context. Furthermore, as Angelo pointed out, we have customers to whom 8 bytes is too many already.", "Again, this should have been in the RFP if it is so important. That would have saved a lot of trouble. All we got was an evaluation criteria:", "Protocol overhead shall be considered when evaluating submissions. Submissions with smaller overhead are preferable.", "This is a miserably small set of criteria for a complex design space. Even you, ", ", say that protocol overhead is not as important as the design philosophy.", "Which I agree is fundamentally different between the two proposals, and that this is where the root cause lies in the failure to reconcile them.", "The RTI/TwinOaks/eProsima proposal is limited to providing a means for performing DDS operations remotely, and it don\u2019t see how it can do anything other than that.", "The RFP not-so-subtly pushes submitters in this direction. You cannot fault them for taking it at face value.", "I will read both submissions and when I do, I will report back with more technically-informed comments.", "Hello ", ",", "While this is true, the other submission has managed to define an object model as well, and in addition kept it close to the existing DDS one.", "And you see this as a positive aspect? Our model is simpler and more user friendly. For instance, how many people can digest DDS partitions? That said, we have a well defined mapping between XRCE resources and DDS topics.", "Since the submissions have not gone to the AB yet, as far as I know, then there should not be any official AB reviews, which suggests to me that RTI asked for unofficial reviews from AB members. This may be why they are not public?", "Are you an OMG member? If so I\u2019ll forward you the reviews. Both submissions went to the AB and the reviews were posted both on ", " and ", ". If you have access to those mailing list you\u2019ll be able to see them. I also recommend you take a look at those.", "I didn\u2019t catch that even once during the presentation. Next time, put such an important motivating factor in your slides. ", " RTI and co were much better at motivating their design decisions, and that put a positive spin on their submission.", "You probably also should have put that requirement in the RFP, if it\u2019s that important. The other submission cannot aim for a requirement they are not aware of.", "It was impossible as the other vendors did not want to agree on such a low bound. The 24 bytes was the least we could agree on. This is why there is an evaluation on wire-efficiency. This matters were discussed at length, but again I don\u2019t think you attended those meetings, thus you are missing part of the history and the context. In any case, all of those documents are on the OMG archives, thus if of interest you to reconstruct it. Just search for presentation I did on XRCE for almost a year. starting from 2015!", "That may have been RTI\u2019s reason, but the reason the rest of us present voted no is because we still have two vastly different submissions with no apparent readiness to work towards a single one. PrismTech even behaved in their presentation as if they are expecting RTI, Twin Oaks and eProsima to through away their submission and go with PrismTech\u2019s.", "Yes, that is correct as it is since the very beginning that we are trying to do a joint submission. They\u2019ve refused with futile arguments \u2013 if you ask me. We have put lots of effort to trying to join but that has not been corresponded. A pity that you were not in the Bruxelles meeting, otherwise you would have had a taste of it.", "Neither of these are required by the RFP. The RFP heavily directs the submitter towards the style of architecture that RTI/Twin Oaks/eProsima provided.", "Again, you did not attend the end-less arguments we had during the RFP drafting. RTI does not want peer-to-peer in XRCE because they fear it could become as substitute for DDSI-RTPS. Again, this is not something I am inferring, but something that was openly debated during the RFP drafting. We don\u2019t have any issue with that as we think that having a more efficient protocol than DDSI-RTPS for some use cases would be extremely useful.", "Yes, this is something that I was disappointed about. Hearing RTI\u2019s presentation talk about TCP/IP only seemed to rule out using it on things like Zigbee. But on the other hand, perhaps it\u2019s readily adaptable?", "For me that disqualifies completely the submission as in LowPAN environments nobody can afford to use TCP/IP\u2026", "It was very useful. I wish I had had this information during the presentation. I still have not had time to read the submissions in detail and unfortunately will not be able to do so before November, but fortunately we now have until February next year to try and resolve this situation.", "And, as ", " said, having code available would make a difference to how well we can judge things like implementation complexity. ", "I am glad that this helped clarifying the situation, please don\u2019t hesitate to ask any other question. Concerning the code availability we are working on that. I\u2019ll keep you posted.", "A+", "Dear All,", "I wanted to let you know that we have just released under Apache 2 a peer-to-peer implementation of our zenoh protocol called Zeno-He (Zenoh Helium). This implementation fits in about 1KByte of RAM and has 4 bytes wire overhead. This implementation not only is incredibly resource efficient but it is also blazing fast as it delivers incredible point-to-point throughput and low latency.", "The project website is available at ", " and the source code at ", ".", "We will be releasing a brokering system by the end of the year, likewise we be glad to help-out integrating zenoh as one of the protocols supported by ROS2. This could allow to bring ROS2 on micro-controllers!", "N.B. For those of you that are familiar with XRCE, zenoh is the protocol we are proposing for standardisation. But as the standard is not finalised yet, we will keep referring it as zenoh.", "A+", "Kydos,", "Thank you for publishing the Zeno-He library so we can all begin to interact with it.  It is especially useful for the ROS 2 user community to be aware of the effort since it implements the ATLab XRCE proposal.", "I know I would be extremely interested in someone benchmarking Zeno and the proposed epromisa XRCE implementation, and perhaps can find time to do that.", "This is really appreciated ", ", thanks for making this available.", "At present it\u2019s unlicensed code though (", ").", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["They believe that their proposal is more efficient in the wire protocol (trying to save every single byte possible), and supports brokered as well as peer-to-peer communication.", "They have invited the competing submitters to join the PrismTech submission.", "PrismTech believes that they have presented their final submission in Brussels (June) and so were expecting to vote for acceptance in New Orleans (September), but because there are still two competing submissions, and no final submission document was received (only a presentation), this is not possible.", "PrismTech\u2019s reasons for going forward with their own submission:\n", "XRCE tries to target the most wire/power/memory efficient protocol, targeting not just IP infrastructures but a whole range of infrastructures.", "Their submission provides reliability and fragmentation.", "Their current prototype runs on an 8-bit microprocessor with 1 KB of RAM and has a wire overhead of 4 bytes for data samples.", "They got a review from the AB which was favourable (only editorial comments), and they believe that the AB review shows that they satisfy all the mandatory requirements.", "\n", "XRCE applications can be brokered into a DDS data space, or they can discover one another and communicate P2P.", "Their submission is only about the protocol, it does not say anything about the API.", "A DDS-XRCE Agent running on permanently connected hardware provides the access to the rest of the (non-XRCE) DDS domain.", "XRCE provides a data space abstraction in which applictions can read and write data autonomously and asynchronously.", "Data read and written by XRCE applications is associated with one or more resources identified by a URI.\n", "An XRCE resource is a closed description for a set of named values.", "Resource URIs allow for wildcards, which means that more than one resource can be targeted in one definition, which is useful for capturing collections of sub-namespaces.", "A resource with a cardinality of one is called a Trivial Resource.", "Resources also have properties, which are used to attach QoS settings to them, such as \u201cthis resource is transient\u201d or \u201cthis resource is reliable\u201d.", "An XRCE selection is the conjunction of a resource and a predicate over the resource content and properties, used to filter a selection. For example, finding all lights with a luminosity greater than zero. (It seems to provide functionality similar to a very basic subset of SQL.)", "\n", "There is a mapping from XRCE resources to DDS topics.", "Resource serialisation uses the same format as DDS.", "All XRCE messages are a single-byte header and a body.\n", "There are flags in the header for reliability and synchronicity.", "Messages can be \u201cdecorated\u201d by prefixing them with a one-byte pre-header, which determines some of the properties for the following header byte, such as fragmentation.", "The protocol is little-endian.", "Variable-length encoding is used to save space.", "\n", "For addressing, the source address for each message is assumed to be a unique address of the sender.", "The protocol is modular, with a core profile (required for any communication), an optional query profile, and an optional discovery profile.\n", "e.g. If using a communications system like Bluetooth, which already has discovery, the XRCE discovery profile can be removed, saving some space.", "Discovery happens by scouting (sending a scout message to ask for types of nodes to reply, i.e. broker nodes or durability services or peers or clients).", "Other nodes reply to a scout with a HELLO message.", "\n", "After discovery, two nodes need to open a session, which enables publishing and subscribing between those two nodes.\n", "Authentication data is included in the session open message.", "Locator information, telling the other node how it can be reached, is included in the open message.", "The receiving node replies with an accept message or a reject message.", "There is an FSM describing the states a session goes through.", "\n", "Resources and selections are uniquely identified by numerical IDs to save space on the wire.", "A declare message is sent to declare what resources, selections, etc. a node has or will publish or subscribe to.", "Subscriptions can be push, pull, periodic pull or periodic push.", "All data messages and declarations are transported over a conduit (for the session), which is a pair of a reliable and a best-effort channel. Multiple conduits may be used in parallel to avoid head-of-line blocking and allow multicasting.", "One decorator is used to select the conduit for the next message.\n", "It is possible to make this decorator one byte instead of two if the number of conduits is less than five.", "\n", "There is a sync message available to set the next sequence number to expect (e.g. for when not starting at zero).", "The ACKNACK message can acknowledge multiple messages at once, usually up to the given sequence number. It has the option to optionally request retransmission of one or more messages after that point.", "There is a one-shot write function in the protocol, which can do a write of a resource without needing to do any prior registration or discovery.", "This proposal appears to be a very complex protocol with a lot of options in message header structures. An implementation would have a lot of choice points during the decoding of a stream of messages. The \u201cdecorator\u201d idea especially may save bytes on the wire (and in message construction buffers) in some cases, but it complicates the protocol implementation.", "PrismTech are already trying to bring their submission to market as a product.", "This proposal also uses an XRCE agent to provide access from DDS-XRCE nodes and the DDS domain.", "An important use case for them is that devices will tend to mostly sleep, and only wake up occassionally to do some processing, and send and receive data. This means that two devices may never be awake at the same time. This is why they have the agent, which is permanently present and provides a way for XRCE nodes to communicate with each other.", "They say that their proposal focuses not just on the XRCE protocol, but also on the interaction between the XRCE protocol and the DDS domain.", "It is possible for an XRCE agent to behave as an XRCE client to another agent, allowing for a hierarchical structure of agents and clients.", "Their proposal is based on the web-enabled DDS specification, with a DDS-XRCE object model in the agent that has a one-to-one mapping to the DDS data model.", "eProsima have put up a demo on Youtube: ", "\n", "Their demo uses 43 KB of RAM (much bigger than the PrismTech proposal, which can fit in 1 KB).", "The XRCE Agent object model is very similar to the DDS object model, making the mapping very simple.", "XRCE objects are modeled as resources that are addressable by their name and have CRUD operations.\n", "Each resource has a name to address it within the agent and a context; a representation describing the resource; and an ID.", "Resources can be represented as a name, an XML description, or a binary representation.", "\n", "Authentication capability is built into the proposal.", "Types are typically pre-defined profiles in the XRCE Agent.\n", "It is possible to transmit types as binary or XML representations.", "\n", "Message structure:\n", "A message header is either 4 or 8 bytes, depending on if a clientKey is used.", "There is a sub-message header, which is another 8 bytes.", "It is possible to send data in a sequence, meaning the header only needs to be sent once for a bunch of samples.", "\n", "The transport can be message-oriented or packet-oriented.", "CDR and DDS-XML representations are reused.", "Message overhead is typically 12 bytes.\n", "They consider that further reduction in overhead would increase complexity and reduce robustness. e.g. They do not need different code paths for multiple sessions, re-connections, handling variable-length encoding.", "They say it compares favourably to TCP/IP overhead (40 bytes).", "They think they could save 6 bytes from their message header, but the reduction is only 6% in the context of total overhead (when considering the use case of using TCP/IP as the underlying transport protocol) and so is not worth it in the face of increased complexity.", "\n", "They also received 3 AB reviews, which they claim were supportive and did not find any non-editorial problems.", "The submissions are very different. PrismTech is aiming for cutting down the bytes used by the protocol to the absolute minimum at the expense of all else. RTI/eProsima/TwinOaks are favouring some overhead in order to achieve a simpler and more robust protocol and implementation.", "PrismTech\u2019s protocol is overly complex with too many choices during the decoding of a message.", "The extra overhead of the RTI/eProsima/TwinOaks submission is not likely to be a problem in the majority of embedded micro-processors used these days (although I\u2019m not sure how many 8-byte, 1KB-of-RAM micros there are in use in new products). However, their consideration that TCP/IP is going to be the most common transport may not be accurate.", "Dynamic Discovery (RTI does not, please read their spec!)", "Peer to Peer Communication (RTI does not)", "Client to Broker Communication", "Non IP Transports (RTI does not)", "Generalised Queries (RTI only supports DDS-like queries)", "Push/Pull/Periodic-Pull and Periodic-Pull Readers", "Unicast and Multicast communication \u2013 for both client to broker and peer-to-peer", "Durability", "The view ", " provides is a) unbiased, b) the view of a roboticist (that\u2019s what this community is about ", " ! )  and c) based on the information someone got from hearing \u201cyour presentation\u201d. Note the following:", "I believe we all appreciate technical argumentation and slides. I love slides. But what I love even more is code and things that I can reproduce. How can I verify your arguments through experimental results? Is there any open code that supports your arguments? More than bashing around, i think it will do a lot of good to facilitate implementations that others can reproduce in common platforms. Even early stages will do. You might find that you could even get some support (and feedback!) before launching it officially and furthermore, that\u2019ll definitely convince a lot of people on why your approach is better.", "Last but not least, I really hope that the passion you\u2019ve shown answering this thread is shown by supporting and making OpenSplice better.", "Propose a familiar model to the final user, making use of the DDS object model and specifications: Serialization (CDR), representation (XML-DDS), and some ideas of WS-DDS mapping", "Neither the protocol or the API is designed to save every possible bit, but to have something robust, flexible and easy to use."], "url": "https://discourse.ros.org/t/ipc-in-ros2/2619"},
{"title": "Safety-critical WG", "thread_contents": ["Some time ago I was asked to lead a working group looking at the use of ROS 2 in safety-critical systems. These are systems that may potentially cause harm to people or the environment, and I think that most of us agree that a large number of robot applications fall into this category.", "The working group will look at topics including:", "In the interests of getting this thing moving rather than stalling while I try to get a nice, formal proposal together, I\u2019m going to begin with just a call for participation and a time for the first meeting.", "ROS 2 Safety-critical WG", "San Francisco, USA Wed, 8 May 2019 at 15:00 PDT", "\nChicago, USA Wed, 8 May 2019 at 17:00 CDT", "\nWashington DC, USA Wed, 8 May 2019 at 18:00 EDT", "\nBarcelona, Spain Thu, 9 May 2019 at 00:00 CEST", "\nBerlin, Germany Thu, 9 May 2019 at 00:00 CEST", "\nTokyo, Japan Thu, 9 May 2019 at 07:00 JST", "\nCorresponding UTC Wed, 8 May 2019 at 22:00", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: +1 (312) 757-3117", "Access Code: 206-092-957", "More phone numbers", "\nAustralia: +61 2 9091 7603", "\nAustria: +43 7 2081 5337", "\nBelgium: +32 28 93 7002", "\nCanada: +1 (647) 497-9373", "\nDenmark: +45 32 72 03 69", "\nFinland: +358 942 72 0972", "\nFrance: +33 187 210 241", "\nGermany: +49 692 5736 7300", "\nIreland: +353 15 295 146", "\nItaly: +39 0 230 57 81 80", "\nNetherlands: +31 202 251 001", "\nNew Zealand: +64 9 913 2226", "\nNorway: +47 21 93 37 37", "\nSpain: +34 932 75 1230", "\nSweden: +46 853 527 818", "\nSwitzerland: +41 225 4599 60", "\nUnited Kingdom: +44 20 3713 5011", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "One of the things to discuss in that meeting will be the time for and frequency of regular meetings after that, as well as mundane topics like where to keep information.", "As it turns out, safety is not that interesting a topic for most people. Who would\u2019ve guessed? ", "It was just myself and Nick Burek from AWS in the meeting today, but we did throw around a few ideas for  things the working group could do.", "There are many verification tools that would be useful to have supported by ", " in the same way linters are.", "To clarify: the build tool ", " knows nothing about linters. They are being invoked from the build system in either CMake as CTests or in Python packages as unit tests.", "Sorry, I mixed that up. I meant to say ", ", like the existing ament extensions.", "I wanted to be there but the time zone and other problems prevented me from attending ", "I\u2019m still working in the Ada client lib (updating it to Crystal atm). I have little SPARK expertise but it\u2019s definitely one topic in which I want to invest time, so I\u2019ll try to get involved in that part if it ever moves forward.", "If the Ada client library gets completed then that would provide a good base to build a SPARK one on.", "Geoff, sorry we missed the first meeting, we\u2019d like to participate.", " and ", " are looking at safety from the Intel ROS2 team.", "OK, since there have been a few more expressions of support now, let\u2019s take another stab at having a kick-off meeting.", "Let me know your availability through the poll below and we will see if we can find a meeting time that sucks for as few people as possible.", "Doodle is the simplest way to schedule meetings with clients, colleagues, or friends. Find the best time for one-to-ones and team meetings with our user-friendly calendar tool. Get started today!", "Thanks to all those who filled in their preferred times. We will meet at this time:", ".", "If you want a calendar invite, please let me know your email address by PM.", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: ", "Access Code: 205-619-637", "More phone numbers", "\nAustralia: ", "\nAustria: ", "\nBelgium: ", "\nCanada: ", "\nDenmark: ", "\nFinland: ", "\nFrance: ", "\nGermany: ", "\nIreland: ", "\nItaly: ", "\nNetherlands: ", "\nNew Zealand: ", "\nNorway: ", "\nSpain: ", "\nSweden: ", "\nSwitzerland: ", "\nUnited Kingdom: ", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "Hello ", " ,", "I\u2019m trying to log in but getting", "\n", "Time seems right though:", "I\u2019m seeing the same, so you\u2019re not alone.", "Sorry, everyone. Various problems compounded to make me 35 minutes late for the meeting, by which time most people had given up, it appears. Based on the previous poll, I\u2019d like to reschedule for the following time:", ".", "I will post connection information tomorrow. Please let me know as soon as possible if this time doesn\u2019t work for you.", "Here\u2019s the connection information.", "ROS 2 TSC Safety WG kickoff ", "\nWed, 5 Jun 2019 23:00 - 00:00 JST", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: ", "Access Code: 282-071-125", "More phone numbers", "\nAustralia: ", "\nAustria: ", "\nBelgium: ", "\nCanada: ", "\nDenmark: ", "\nFinland: ", "\nFrance: ", "\nGermany: ", "\nIreland: ", "\nItaly: ", "\nNetherlands: ", "\nNew Zealand: ", "\nNorway: ", "\nSpain: ", "\nSweden: ", "\nSwitzerland: ", "\nUnited Kingdom: ", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "Thanks for re-organizing ", ", unfortunately I won\u2019t be able to make it today but will look through the notes if they become available ", "Thank you to those who joined the meeting. We had a good discussion and identified some concrete areas where we can take action. We also identified that our biggest roadblock is, as always, resources.", "Here are my notes from the meeting.", "Thanks for the detailed minutes, Geoff.", "As for your last entry on threading models, you may find something of interest in the Ravenscar profile of Ada, which specifically targets multithreading in high-integrity contexts:", "For those of you interested in formal proof of complete systems, above the source code part, this project has been making the rounds in the Ada community a time ago. I think they used Z for the user interface spec (link to github at the end):", "Apologies for the spam if you were already aware.", "Cheers,", "\nAlejandro.", "Thanks for that information! I\u2019m always happy to be introduced to new samples of using Z to learn from.", "I\u2019d like to set our next meeting in the first week of July. It\u2019s a little way off, but we will start doing more frequent meetings when we start getting more active.", "Here\u2019s a poll for the meeting time.", "Doodle is the simplest way to schedule meetings with clients, colleagues, or friends. Find the best time for one-to-ones and team meetings with our user-friendly calendar tool. Get started today!", "After this meeting, I would like to set a regular meeting schedule. I will send out a poll for that later.", "Thanks to those who provided their availability. I have chosen the time that the most people are available for, which is:", ".", "Sorry for the 7AM start for those on the west coast of the USA.", "Here is the meeting participation information. If you want an invite, please send me a DM with your email address.", "ROS 2 Safety WG", "Please join my meeting from your computer, tablet or smartphone.", "\n", "You can also dial in using your phone.", "\nUnited States: ", "Access Code: 485-233-997", "More phone numbers", "\nAustralia: ", "\nAustria: ", "\nBelgium: ", "\nCanada: ", "\nDenmark: ", "\nFinland: ", "\nFrance: ", "\nGermany: ", "\nIreland: ", "\nItaly: ", "\nNetherlands: ", "\nNew Zealand: ", "\nNorway: ", "\nSpain: ", "\nSweden: ", "\nSwitzerland: ", "\nUnited Kingdom: ", "New to GoToMeeting? Get the app now and be ready when your first meeting starts:", "\n", "Dear all,", "I\u2019m truly sorry but recent developments will keep me out of touch during the Jul 2-3 period, so I will miss the meeting.", "Best,", "\nAlejandro.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Documenting how to use ROS 2 in a safety-critical application", "Use of tools to support the above", "Additional processes, tools and methods needed for building a safety-critical robot that are not currently covered by something in ROS but could be", "How to make the client libraries usable in a safety-critical system, and work on safety-focused client libraries (for example, a SPARK client library)", "Cross-over issues with the QA and real-time working groups for infrastructure, tooling and methods", "Cross-over issues with the navigation and manipulation working groups for sample applications", "Anything else safety-related someone brings along", "The most low-hanging fruit identified was to put some resources towards the ", ". Rust is a powerful systems programming language but it also has features that make it useful for safety-critical and real-time systems. Esteve\u2019s client library exists but more work needs to be done and currently not enough resources are going into it.", "Related to the above would be to get the ", " client library into a usable shape and make it feature complete. SPARK is a variant of Ada designed for dependable systems. We discussed how a minimal SPARK client library could be useful for the safety-critical parts of a robot, especially the lower-level parts such as a monitor.", "There are many verification tools that would be useful to have supported by ", " in the same way linters are. Thought needs to be given to exactly what tools would be useful, with an eye to ones that are going to work well in CI. Design verification tools like TLA+ and Spin are possibly going to be less useful in this situation than implementation verification tools like symbolic execution tools like KLEE.", "More abstractedly, we talked about how it would be useful to have formal specifications of nodes in terms of outputs produced from inputs (think of something like a disjoint transfer function) which could be used to generate test cases that are representative of large input spaces without needing to run huge numbers of tests, helping with combinatorial testing of nodes.", "Alejandro Mosteo, author of the Ada client library. Working with drones, and don\u2019t have a specific safety need but it is something that is of interest. Will continue maintaining the Ada client library and interested in SPARK.", "Brad Baillio, working on autonomous vehicles in off-road situations like mining and agriculture. Safety is an obvious need.", "Matt Droter, ROS Agriculture. Using ROS for farming so trying to figure out how to make it safe, what are the general best practices that can be applied.", "Nick Burek, AWS RoboMaker. Multiple groups at Amazon are doing robotics for warehouses, so the robots are safety-critical.", "Geoff Biggs, working on self-driving vehicles at Tier IV, where the importance of safety is obvious.", "Client libraries:\n", "The Ada client library is working with Bouncy version of ", ", and feature complete for Bouncy. Alejandro is now going to update it to work with Dashing ", ".", "Alejandro plans to start working with SPARK in the near future.", "Rust is intriguing as a language that could be useful in safety, but it is not yet widely used in ROS. Working to classify Rust would be a huge job and probably beyond our capabilities.", "\n", "Tools in the work flow useful for safety-critical development with ROS.\n", "AWS work with TSAN and ASAN is now at the stage that reports are being generated in nightlies.", "The Automated Reasoning Group at Amazon uses a tool called C Bounded Model Checker (CBMC) that", "\nallows you to write proofs against your code and allow you to check the whole valid range of inputs and outputs. It does C++ as well now, but is not great at multi-threaded logic. It is a good tool to find the last bugs in your code.", "The same group is also using many other tools. Nick Burek will provide a list of tools being used.", "\n", "ROS QA WG is working on integrating the High Assurance ROS tool into the build farm.\n", "\n", "There is interest in the work that Apex.AI is doing. Are we duplicating their work, or are we trying to make their work open source? Are we competing?", "No one is particularly interested in and/or has the necessary skills to do formal specifications of nodes.", "Classifying tools used in ROS would be particularly valuable.\n", "We could try to classify launch2 or colcon. Both would be valuable for the community.", "We may be able to try and get Apex.AI on board for classifying launch, in particular its input specification.", "\n", "Most of the things discussed so far are tools that support reliability. What could we do to help people make sure their designs using ROS are safe?\n", "One particularly good idea is to come up with sample structures in ROS for common architecture patterns used in safety-critical systems, such as a 2oo3 architecture, or how a safety monitor should be implemented.", "Document how callback groups and how the threading models work with different executors so that people have a guide for how not to deadlock themselves, for example.", "Provide sample safety cases for ROS-based systems. ROS Agriculture has a small lawn tractor application, which is well-defined and has clear safety concerns. This could be a good sample application to work with. ", "\n", "\n", "Things to attempt:\n", "Document threading models used in ROS 2.", "Classify launch, especially its input specification, to find the things you ", " do with it in a safety-critical system.", "Try CMBC and consider how it might be integrated into ament so it can be easily used in CI and the build farm.", "Develop some sample architectures for using ROS in a safety architecture, and document them.", "(Longer term) Produce a sample safety case for the ROS Agriculture small lawn tractor application.", "\n", "Possible effort contributions:\n", "Alejandro Mosteo can contribute some personal time, and if a student with interest comes along\u2026", "Brad Baillio can contribute some personal time.", "Matt Droter can act as a conduit to the ROS QA WG to coordinate related efforts.", "Nick Burek will try and get some time on the next sprint at AWS for trying CMBC on at least one ROS package.", "Geoff Biggs can contribute work on sample architectures for safety-critical systems, will look into documenting the threading models, and over a longer term will work on the sample safety case with Matt.", "\n"], "url": "https://discourse.ros.org/t/safety-critical-wg/8884"}
]