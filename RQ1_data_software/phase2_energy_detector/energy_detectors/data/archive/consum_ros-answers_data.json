{"title": "Time consumption profiling", "time": "2012-02-23 22:48:36 -0600", "post_content": [" ", " ", "I would like to know, is there such a tool in ROS that I can use it to profile the total duration of time taken (or time-complexity) by each node that I'm running. ", "Thanks in advance. "], "answer": [" ", " ", "If you're looking to perform more in-depth timing analysis of your nodes, you can run the node in ", ". This will allow you to see the time usage of each function in your program, step by step. A node can be run in Valgrind by using a ", ". ", "If I run Valgrind by using a launch-prefix, where do I check the output?", "Use ", " to log to that file. I would then suggest you open that file in a valgrind gui of some kind. I use Kcachegrind on Ubuntu. ", "I've got a permission problem to open the file via Kcachegrind. When I check the generated logfile, it not a read-only file. Any hint for this..", "Are  you running ROS as a different user?", "No, the same user. ", "You could just do ", " to change the permissions. ", "It's not the case of having permission problem, I think, could be something else. Any other possibilities I can open that via  Kcachegrind?", "Valgrind is not a good solution unfortunately because it's effectively a virtual machine that runs a single thread. It greatly affects the way a process runs distorting the costs. @Thomas_D's suggestion of sysprof avoids this issue though.", " ", " ", "Are you looking for the runtime of an executable? In this case start the node using the unix-tool \"time\" (see \"man time\"). If you are using launch files, have a look at the launch-prefix attribute of the \"node\" tag:", "With the above line, I won't be able to know how much time is used for each of nodes I've, right? I've tried it, and it gives only one \"elapsed\"(time elapsed i guess).", "I am sorry, but I do not understand your question. What exactly do you want to measure? \"time\" gives you the duration a program is running from startup until shutdown.", "Yes, it is. Thanks anyway. Only that, I couldn't be able to differentiate which one belongs to which process/node?", "The -f option of time lets you print out arbitrary output. Have a look at time's man-page.", " ", " ", "While Valgrind is nice I tend to use ", " more often. You can install it with:", "This avoids valgrind's vm & single thread distortions, and so in many cases is a much better option.", "I am trying to use sysprof with my ros code, but unfortunately I don't get it to run on ubuntu 12.04 x64. Do you know if it is possible?", "I used sysprof yesterday when running ROS nodes and it worked when I used sudo.", " ", " ", "This way works for me,", "add ", " in your node as a attribute. ", "Then run your program and a log file will be generated. ", "Then start  ", ", you will see all the cool stuff what you have coded :D", "To install "], "answer_code": ["launch-prefix=\"valgrind --log-file=/home/your_user/valgrind.log", "chmod 775 valgrind.log", "<node pkg=\"somepkg\" type=\"somename\" name=\"somename\" launch-prefix=\"time\">\n", "sudo apt-get install sysprof\n", "launch-prefix=\"valgrind --tool=callgrind --callgrind-out-file=/home/project/test.log\"", "kcachegrind test.log", "sudo apt-get install -y kcachegrind\n"], "url": "https://answers.ros.org/question/28383/time-consumption-profiling/"},
{"title": "sound_play consumes high CPU and makes strange voice after some moment", "time": "2011-12-06 12:35:13 -0600", "post_content": [" ", " ", " ", " ", "With ", ", CPU load sometimes reaches 10+ with some of my codes, and sound eventually gets messy and hard to hear.", "I found in Ubuntu's Sound Preference panel that ", " instance keeps populating like in the image below. Sometimes calling ", " function decreases the instance, sometimes not. In my understanding, ", " makes a new instance every time it receives ", " topic. So instanciating itself is not bad but what's bad is the instance doesn't get removed. ", "I haven't figured out the condition where I reproduce the phenomenon. ", " I made for debug, which publishes ", " only regularly with decent amount of sleep after publishing, works well. However, another code of mine that publishes topics irregularly causes an issue above. ", "I frequently see the following error on the terminal ", " runs. But I can't specify when it occurs, even after I briefly looked at ", ":", "Iterative command from commandine like the following doesn't causes the issue until the end.", "Changing voice types on ", " doesn't make any change in terms of this issue.", "Because I see the same phenomenon using both .cpp and .py and topic contents are the same, I suspect there's something in how ", " handles the topics when the interval is irregular/too short, particularly in ", ". But I haven't been successful hacking.", "Environment-1: Ubuntu 11.04 ", "Environment-2: Ubuntu 10.04 ", " (", ")"], "answer": [" ", " "], "question_code": ["sound_play", "sound_play/soundplay_node.py", "stopAll()", "sound_play/soundplay_node.py", "/robosound", "/robosound", "soundplay_node.py", "soundplay_node.py", "[ERROR] Exception in cleanupdict for sound (%SPOKEN_TEXT%): query failed\n", "for i in {0..100} ; do rosrun sound_play say.py 'You got pudding ' ; done\n", "festival", "sound_play", "soundplay_node.py", "electric", "electric", "Turtlebot eeepc"], "url": "https://answers.ros.org/question/12246/sound_play-consumes-high-cpu-and-makes-strange-voice-after-some-moment/"},
{"title": "Image transport to android consumes high memory", "time": "2013-06-14 07:10:25 -0600", "post_content": [" ", " ", "Hello\nI tried to run simple image_transport tutorial on my android 4.0 phone to display image captured by camera. It connects with the master, it displays the image, but the frame rate is extremely low and eventually device hangs itself. It seems that the app is using a lot of memory and therefore GC_FOR_ALLOC pauses the execution of the program. I suppose that each image received is stored instead of being removed after the frame is used.\nCan anybody help me to fix this problem?"], "answer": [" ", " ", "The solution is here:\nhttp://code.google.com/p/rosjava/issues/detail?id=141", "It is needed to make RosImageView a SurfaceView instead of an ImageView."], "url": "https://answers.ros.org/question/65183/image-transport-to-android-consumes-high-memory/"},
{"title": "ROS node consuming 100 % of CPU load . How ?", "time": "2017-08-23 02:28:26 -0600", "post_content": [" ", " ", "I am using ROS - Kinetic , and Ubuntu 16.04 OS .\nROS - node uses a synchronizer for Three messages , to be subscribed from Three other ROS nodes ."], "answer": [" ", " ", "Please provide us with more information - in this case: a snippet that shows how you run your main loop.", "Are you running your own ", " or are you using ", "? If the former: be sure to add a ", " in there, or you basically have a busy-wait.", "Thank You . It is resolved .", "Can you tell us what the cause was? Did you have a busy-while-loop without a sleep?", "Yes . I was using ros::spinOnce inside a while loop  , without rate.sleep() ."], "answer_code": ["while { ..; ros::spinOnce(); ..; }", "ros::spin()", "ros::Rate::sleep()"], "url": "https://answers.ros.org/question/269404/ros-node-consuming-100-of-cpu-load-how/"},
{"title": "I have a startup idea related to ROS and need to consult with the Iranians who are professional to ROS (Preferably residing abroad)", "time": "2018-11-19 02:14:13 -0600", "post_content": [" ", " ", "how can i find Iranians who are professional to ROS?\nif you are interested please follow me so that i can contact you.", " You can try posting on Discourse  ", "  . ", "\u0645\u0646 \u062a\u0627 \u062d\u062f\u06cc \u0628\u0644\u062f\u0645 :)"], "answer": [], "url": "https://answers.ros.org/question/308875/i-have-a-startup-idea-related-to-ros-and-need-to-consult-with-the-iranians-who-are-professional-to-ros-preferably-residing-abroad/"},
{"title": "ros/ros.h header file not found", "time": "2019-10-19 20:29:04 -0600", "post_content": [" ", " ", " ", " ", "Hello, I'm pretty new to ROS and I'm implementing a Service node in C++, the problem is, when I try to include the \"ros/ros.h\" header file, it is not found, and the header file of my service file isn't found either, although it is correctly contained in catkin_ws/devel/include/my_package, I've found that this problem is often caused by an error of configuration in the CMakeLists.txt, but I tried the solutions that were posted in the thread I consulted and it didn't help. My ROS version is Kinetic Kame and I'm using Ubuntu 16.04.", "Here's my CMakeLists.txt:", "And my package.xml just in case:", "I would really appreciate the help, guys", "This could be a result of an unorthodox directory structure. Do you have a \"srv\" folder and an \"include\" folder that contains the mentioned files in the src/my_package directory (e.g. my_package/srv and my_package/include)?", "Oh, my bad, I forgot to mention that, yes, I'm using the suggested folder structure, but the problem was just my IDE's configuration. Thanks a lot for your help!"], "answer": [" ", " ", "I don't see your \"find_package(catkin ...)\" near the start of the posted CMakeLists.txt - I'm actually surprised you aren't getting lots of CMake config errors. Check out the details of find_package on the ", "Oh, yes, actually I erased that line trying to fix the issue desperately lol, but it was meant to be on the file, thank you very much. I found the actual error, it was simply the IDE I'm using that just had trouble recognizing the location of the header files, but when running the node through the Ubuntu console it works just fine. Again, thanks a lot for your help!"], "question_code": ["cmake_minimum_required(VERSION 2.8.3)\nproject(beginner_tutorials)\n\nadd_message_files(\n   FILES\n   Num.msg\n   Sensor.msg\n  )\n\nadd_service_files(\n  FILES\n  AddTwoInts.srv\n  CaracteresComunes.srv\n )\n\n\ngenerate_messages(\n  DEPENDENCIES\n  std_msgs\n )\n\n\ncatkin_package(\n   INCLUDE_DIRS include\n   LIBRARIES beginner_tutorials\n   CATKIN_DEPENDS roscpp rospy std_msgs message_runtime\n   DEPENDS system_lib\n)\n\n\n\n\ninclude_directories(${catkin_INCLUDE_DIRS})\n\nadd_executable(talker src/talker.cpp)\ntarget_link_libraries(talker ${catkin_LIBRARIES})\nadd_dependencies(talker beginner_tutorials_generate_messages_cpp)\n\nadd_executable(listener src/listener.cpp)\ntarget_link_libraries(listener ${catkin_LIBRARIES})\nadd_dependencies(listener beginner_tutorials_generate_messages_cpp)\n\n\nadd_executable(SumarServer src/add_two_ints_server.cpp)\ntarget_link_libraries(SumarServer ${catkin_LIBRARIES})\nadd_dependencies(SumarServer beginner_tutorials_gencpp)\n\nadd_executable(SumarClient src/add_two_ints_client.cpp)\ntarget_link_libraries(SumarClient ${catkin_LIBRARIES})\nadd_dependencies(SumarClient beginner_tutorials_gencpp)\n\nadd_executable(CCServer src/CCServer.cpp)\ntarget_link_libraries(CCServer ${catkin_LIBRARIES})\nadd_dependencies(CCServer beginner_tutorials_gencpp)\n", "<?xml version=\"1.0\"?>\n<package format=\"2\">\n  <name>beginner_tutorials</name>\n  <version>0.0.0</version>\n  <description>This package was created to follow the tutorial</description>\n\n  <maintainer email=\"aldo@todo.todo\">aldo</maintainer>\n\n  <license>TODO</license>\n\n  <buildtool_depend>catkin</buildtool_depend>\n  <build_depend>roscpp</build_depend>\n  <build_depend>rospy</build_depend>\n  <build_depend>std_msgs</build_depend>\n  <build_depend>turtlesim</build_depend>\n  <build_depend>tf</build_depend>\n  <build_export_depend>roscpp</build_export_depend>\n  <build_export_depend>rospy</build_export_depend>\n  <build_export_depend>std_msgs</build_export_depend>\n  <build_export_depend>tf</build_export_depend>\n  <build_depend>message_generation</build_depend>\n  <exec_depend>message_runtime</exec_depend>\n  <exec_depend>roscpp</exec_depend>\n  <exec_depend>rospy</exec_depend>\n  <exec_depend>std_msgs</exec_depend>\n  <exec_depend>tf</exec_depend>\n\n  <export>\n  </export>\n</package>\n"], "url": "https://answers.ros.org/question/335734/rosrosh-header-file-not-found/"},
{"title": "Obstacle avoidance and path planning with TurtleBot3 using ROS", "time": "2019-10-20 21:48:24 -0600", "post_content": [" ", " ", "Hello!\nI'm working on a project where I have 4 TurtleBot3 robots in a predefined working area, the goal is for one of the TB3 robot (the leader) to reach the indicated position inside of the working area (the position is set using a graphical interface), then the remaining TB3 robots are meant to reunite with the leader, using alternative paths, the robots have to avoid collisions with small obstacles randomly set in the working area as well, as this occurs with the real robots, a virtual simulation of their movement is to be carried out too. Could someone give me advice as to where I could start? Some resources I could consult? So far I've been reviewing the basic tutorials of ROS and some packages like TF and URDF-related, but since I need to learn to use a wide variety of packages, I really don't know where to start. Thank you very much!", "I'm using Kinetic Kame as my ROS version and Ubuntu 16.04 since it was the recommended configuration for using the TB3, I don't know if that is relevant, lol."], "answer": [], "url": "https://answers.ros.org/question/335775/obstacle-avoidance-and-path-planning-with-turtlebot3-using-ros/"},
{"title": "find homography between 2 cameras given tf", "time": "2019-08-26 16:03:06 -0600", "post_content": [" ", " ", "I have 2 cameras on my robot that are looking at the same scene. I have the rotation and translation between the 2 cameras and their intrinsic matrices. How can i project one camera view into other using homography ? ", "This isn't a ROS specific question, its an algorithmic computer vision question. I'd recommend consulting the openCV community or doing some reading on the math involved."], "answer": [], "url": "https://answers.ros.org/question/331662/find-homography-between-2-cameras-given-tf/"},
{"title": "ament_export_dependencies(Boost) not working?", "time": "2019-08-19 18:47:39 -0600", "post_content": [" ", " ", "I'm trying to use a library that depends on boost, and it doesn't seem like boost is properly being pulled in as a recursive dependency. How do I properly export the dependency on boost for downstream packages?", "My build fails with:", "From my library CMake:", "From the consuming package CMakeLists.txt:", "I can make the error go away by changing the consuming package to have the following, but this is very bad practice:"], "answer": [" ", " ", "The API docs of the ", "functions (", ") describes the requirements what you can pass to it.", "In your case of using ", " and requiring the component ", " a simple ", " call isn't sufficient. Since the API doesn't know about the ", " part it won't use it when downstream packages find your package and as a consequence they will fail to find the Boost thread symbols.", "You will need to create a custom CMake file which finds the right component of Boost and register that file as a ", "(see ", ").", "So what needs to go in the custom CMake file for Boost? What properties are being ignored by ament's dependency mechanism?", "Since ROS 2 tried to use newer C++ features instead of Boost where possible I don't have an example at hand. A similar case exporting TinyXML2 instead of Boost can be found here: ", " and ", "Thanks! I wrote up an issue here: ", "It would be nice to have a solution in ament rather than coding up a wrapper for each component-based CMake package. I definitely don't have the knowledge right now to make a PR myself."], "question_code": ["/usr/bin/ld: CMakeFiles/slam_karto.dir/src/slam_karto.cpp.o: undefined reference to symbol '_ZN5boost6detail23get_current_thread_dataEv'\n//usr/lib/x86_64-linux-gnu/libboost_thread.so.1.65.1: error adding symbols: DSO missing from command line\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake[2]: *** [slam_karto] Error 1\nmake[1]: *** [CMakeFiles/slam_karto.dir/all] Error 2\nmake: *** [all] Error 2\n---\nFailed   <<< slam_karto [ Exited with code 2 ]\n", "cmake_minimum_required(VERSION 3.5)\nproject(open_karto)\nfind_package(Boost REQUIRED COMPONENTS thread)\ninclude_directories(include)\nadd_library(karto SHARED src/Karto.cpp src/Mapper.cpp)\nament_target_dependencies(karto Boost)\nament_export_include_directories(include)\nament_export_libraries(karto)\nament_export_dependencies(Boost)\n", "ament_target_dependencies(slam_karto\n  open_karto\n  )\n", "find_package(Boost REQUIRED COMPONENTS thread)\nament_target_dependencies(slam_karto\n  open_karto\n  Boost\n  )\n"], "answer_code": ["ament_export_dependencies", "Boost", "thread", "find_package(Boost REQUIRED)", "COMPONENTS thread", "CONFIG_EXTRAS"], "url": "https://answers.ros.org/question/331089/ament_export_dependenciesboost-not-working/"},
{"title": "focal length of Ardrone front camera [closed]", "time": "2018-12-27 04:21:23 -0600", "post_content": [" ", " ", " ", " ", "Hello guys", "Referring to formula ", " where f_x can be obtained from camera intrinsic parameter, i need to find m_x (pixel per millimeter), i need to know the focal length (f) for Ardrone front camera having resolution of 640x360. ", "any help would be appreciated. ", "\nThanks", "This really isn't a question that's appropriate for this site because it's not ROS-related. Have you tried consulting the documentation?", "but do you know the answer /!"], "answer": [], "url": "https://answers.ros.org/question/311539/focal-length-of-ardrone-front-camera/"},
{"title": "How to create an action server for navigation?", "time": "2018-10-16 03:37:00 -0600", "post_content": [" ", " ", " I have created my own robot and already created a 2d map using gmapping with laser. The simple amcl localization is used in rviz (laser - particle filter) and my robot is able to localize in a created pmg map in rviz. Now, I want to send a goal coordinates for my robot for it to navigate within a map and I have created a new package and a new node like following this tutorial:\n ", "I can see that it waits for the server and when I launch this node, I get the move_base/goal topic but the client just awaits. How can I create this server?", "So you are stuck here I guess:", "Is this message showing all the time? Also do you have move_base running?", "You should look into the ", " and also the ", " to understand the basics of it first.", " Yes, it shows every 5 seconds. It is running.", " I have read that tutorial, in Action Server CPP file, they have included ", " in ", ". What will be equivalentvent to that in ", " ?", "You should continue investigating the tutorials it's explained (", "). For move_base that should be ", ": what sort of robot do you have? A differential drive platform?", "If so: unless you want to learn how to write your own action server for this, I would suggest to use one of the available diff drive packages or ", " plugins. That will save you quite some time.", " Yes, a mobile robot. I can see that the plugin is responsible for the movement and I believe it publishes to /cmd_vel topic. But I was looking for the navigation of my robot. I want to send a goal coordinates and let my robot to navigate to that point (in known map).", "The navigation stack is typically responsible for consuming nav goals and then transforming those into ", " msgs published on ", " topics that ultimately make the robot move.", "It's rare for someone to write their own ", " consumer."], "answer": [" ", " ", "I had the similar situation a while ago when the move_base action server was not coming up. The reason was that move_base and the node where the action client was created were in different namespaces. After they were put in the same namespace the problem was solved."], "question_code": ["  while(!ac.waitForServer(ros::Duration(5.0))){\n    ROS_INFO(\"Waiting for the move_base action server to come up\");\n  }\n", "chores::DoDishesGoalConstPtr& goal", "void execute", "move_base_msgs::MoveBaseAction", "move_base_msgs::MoveBaseGoalConstPtr& goal", "ros_control", "Twist", "cmd_vel", "MoveBaseGoal"], "url": "https://answers.ros.org/question/305903/how-to-create-an-action-server-for-navigation/"},
{"title": "joy_node using ~3gb of RAM", "time": "2018-07-24 10:32:40 -0600", "post_content": [" ", " ", "I haven't been able to find any other reports of this issue, but when running joy_node on my robot, it appears to have a memory leak, jumping in memory use every few seconds, until it consumes all of the available system memory. There was a similar issue reported here: ", ", but it appears to have been fixed. "], "answer": [" ", " ", "Whoops, set the rosconsole level to DEBUG, mistakenly attributed to joy_node"], "url": "https://answers.ros.org/question/298427/joy_node-using-3gb-of-ram/"},
{"title": "How accurate is the current read by /svh_controller/channel_currents?", "time": "2018-09-05 04:31:38 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone!", "I\u00b4m working with a real SVH 5-Finger hand from Schunk and I have performed a lot of tests regarding to the current. As it was pointed out before, there is no force feedback on the hand so the current is the only relevant variable to measure how much \"effort\" is applied to an object.", "I\u00b4m working on developing a grasp controller based on the current of the hand, therefore I\u00b4m using the topic /svh_controller/channel_currents provided by the driver. My question is wether this topic is reliable enough and if it contains an accurate measurement of the current consumption. Or in other words, should I use another method to obtain the current?", "Best regards,", "Charlie"], "answer": [" ", " ", "Hello Charlie,", "there is an error of about +- 10 mA.", "We compared the current reading of the topic to the actual current between the handcontrolcircuit and single motors to get this value."], "url": "https://answers.ros.org/question/302461/how-accurate-is-the-current-read-by-svh_controllerchannel_currents/"},
{"title": "ar_track_alvar publication rate too small", "time": "2018-03-26 10:02:53 -0600", "post_content": [" ", " ", " ", " ", "Hello!\nThese days I'm trying to use ar_track_alvar to track ar tags but the publication of ar_track_poses is too slow. ", "I use Ubuntu 16.04 and ros kinetic. My CPU is Intel I7 with 8G memory. When I use ar_track_alvar, it onlypublish /ar_pose_markers at poorly 0.2hz. ", "I also found that the problem is the rate at which the callback function is called. That function is called merely once every 5 secs. But I can't find anything wrong. I also checked the ", " loop, it tooks 5 secs per loop. However, every step in that loop is very quick. It shouldn't take that long to run each loop. I also checked that the expected rate per loop is 8 hz but actual rate is 0.2 hz. I did this by checking rate.expectedCycleTime() and rate.cycleTime().", "Here is the ", " and the ", " loop is at line 326.", "I have also tested it on Ubuntu 14.04 and ros indigo, the publication rate is 1hz. However my friend can use it with a publication rate of 8 hz on Ubuntu 14.04 and indigo. That's very odd.", "Thanks!", "I would check the publication of your image topic and the CPU usage of the ar_track_alvar node to get a sense for whether the slowness is combing from the input, CPU usage or something else within the code.", "Also of interest, how big are the images that you're trying to process?", "The image is raw image from the camera. I use ASUS xtion and the image topic is /camera/rgb/image_raw", "I checked the CPU usage, it seems that the usage of ar_track_alvar node is always around 2%, regardless of whether I turn off other CPU consuming process or not. What's more, how can I check the image size? I use the rgb image from kinect and I assume it is 640*480", "Well, that seems to rule out the CPU as a bottleneck. Have you tried checking the rate of the image topic? Try ", "Why did you delete your previous edit?"], "answer": [" ", " ", "I have fixed that error. The problem is the code, there is a ros::Duration(5.0) in my code. But when I checked the original code  it is right. Maybe I changed the code carelessly."], "question_code": ["while(ros::ok())", "while", "rostopic hz"], "url": "https://answers.ros.org/question/286578/ar_track_alvar-publication-rate-too-small/"},
{"title": "Rosbridge, trying to connect to service, but cannot be found", "time": "2018-04-23 07:25:25 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to connect to a service, but I get an InvalidServiceException, saying: beginner_tutorials/AddTwoInts does not exist. The connection itself seems to go OK, according to the terminal.", " When I consult rossrv, I see the service.\nSome googling got me to this:  ", " , but when I try to  ", "rosrun rosapi rosapi", "I get slapped with an error:", "[rosrun] Couldn't find executable named rosapi below /opt/ros/kinetic/sahre/rosapi", "[rosrun] Found the following, but they're either not files,", "[rosrun] or not executable:", "[rosrun]    /opt/ros/kinetic/share/rosapi", " I installed rosbridge according to\n ", "  and\n ", "So I ended up doing 2 installations, one of server and one of suite. I sourced it, and when I start rosbridge using: roslaunch rosbridge_server rosbridge_websocket.launch I see rosapi in the list of NODES: rosapi (rosapi/rosapi_node).", "My linux is ubuntu 16.04.", "Best,", "Update:", "I tried the command ", "rosrun rosapi rosapi_node", "That seemed to start rosapi, but I still get the same error. ", "Furthermore: publishing messages from my windows environment (C#) to the ubuntu DOES work :)"], "answer": [], "url": "https://answers.ros.org/question/289420/rosbridge-trying-to-connect-to-service-but-cannot-be-found/"},
{"title": "MeBo 2.0 on ROS?", "time": "2018-01-06 19:06:41 -0600", "post_content": [" ", " ", " ", " ", "Hello All, I\u2019m a beginner with ROS. I have it up and running just fine, and have completed the basic Turtle sim. I also have a book and some other literature on ROS, and have a pretty good idea on how ROS basically works.", "I have a robot toy called a ", "The robot has both and Android and iOS application. They way the robot is controlled is to connect to its WiFi, and then you are presented with a view through its camera with a soft button control overlay on the screen. You can go forward backwards, etc, and control the manipulator.", "There is a project out on the web that shows the MeBo web-API, but I can\u2019t seem to make the MeBo respond with HTTP-only commands (I think it\u2019s my syntax???). See for details: ", "My question is, is it possible to control the MeBo 2.0 from a laptop running ROS (Ubuntu 16.04LTS Kinetic) to control the robot with HTTP commands through WiFi, and is it possible to import the camera preview page from the MeBo\u2019s web page and use OpenCV to do basic navigation?", "The MeBo itself uses a Sonix Access Point camera board similar to the SkyViper GPS drone (less GPS of course). Although firmware is available for most of Rocket Toys products, I have yet to find the actual firmware source for the MeBo 2.0 on their github site ", "I would think this is possible, and was thinking of having the MeBo carry a ROS-enabled Raspberry Pi with Astro-Pi Sense Hat for a mobile solution. I know from reading some of the ROS tutorials, the MeBo would have to be defined with a URDF file, but I don\u2019t have much information on how I would go about adding a consumer toy robot like the MeBo 2.0", "Any help, thoughts or suggestion will be appreciated. As I mentioned I have a ROS beginner, and I\u2019m learning as much as I can on my own.", "Thank you!", "Edit: Thank you for your prompt reply!", "One note of interest is, that I've actually opened the MeBo 2.0 to see what is on the inside. The camera head has a camera and Sonix AP board with a speaker. That board communicates through a ribbon cable of sorts to the motor board in the base. The motor board has both a serial interface and a SW debug port on it. The drive train of the robot consists of a central motor for each pair of wheels... both left side and right side are each driven by an independent motor through a series of gears to give the MeBo 2.0 four wheel drive. The front axle of each drive train is internally connected to a simple wheel encoder. Feedback for the arm position consists of analog voltages going to the motor board.", "Although all of the sensors exist, there is no \"diagnostics web page ...", ": please don't post answers unless you are ", " your own question. For everything else, update your original question or use comments to interact with other posters."], "answer": [" ", " ", " For people googling for this in the future, I had help with some users on the Letsrobot.tv Website, to hack this robot, and our API is open source on this github  ", " \nit includes the Video API, and Controlls, arms, Claw, Wrist, and turning speed\nFeel free to download it! Lots of love! :) ", " ", " ", "Hello,", "This is a great toy, thanks for sharing it :)", "You have no luck, there seem to be very low to none hacker for this toy (the link you provide is not updated since a year)", "But you can be the first !", "For a project like this, I will go this way : ", "The link you provided look like a begining of this work, a .pcap file (packet capture between the app and the robot) that can be analysed to see which command are sent (", ")", "Describe the API :", "Build an API in Python or C++ :", "Python will be easier here, since you can export it to C++ if necessary", "Build a simple ROS driver :", "At this point you have a ROS driver that accept command and give access to camera.", "If you want to go further (optional but nice to have for learning) : ", "Build a URDF model of the robot : ", "You need to do it from scratch, and be very precise when you do it. Maybe ask the manufacturer for some help since hacking their toy is very good for the brand image.", "Build a MoveIt! plugin for the robot :", "Have fun :)"], "answer_details": [" ", " ", " ", " ", "Define the position of the computation module : ", "You can have an on-board raspberry, but since the toy can only communicate in WiFi, your raspberry will be on the robot communicating in WiFi with the robot. It can be good for a outdoor robot or totally autonomous robot. But you will have to manage the raspberry battery and position on the robot (pro tips : use a lot duct tape :) ). The main code will be on the raspberry, the ROS driver on the raspberry too. But since you can have only 1 WiFi connection on the raspberry (without an external WiFi dongle), you will not be able access the raspberry will the robot is running.", "You can also control the robot with a raspberry connected to your local router in ethernet, and controlling the robot remotely in WiFi. The main code will be on your computer, the ROS driver on your raspberry. You will be able to access your raspberry in ssh through the ethernet connection.", "record all data transfered from the smartphone app to the robot : \n", "Launch Wireshark or tool like this.", "Record all http command passing from the app to the robot", "analyse them to extract the commands", " ", "With Swagger.io for example or just a plain text file", "Cameron Lane, the author of the github you provide began to do it ", ", you can probably ask him to collaborate on his work)", "Expose the Python/C++ API with standard ROS message, service and/or action.", "You will be able to command the robot with standard message from a ROS node.", "To plan movement of the arm and the base", " ", " ", " ", " "], "url": "https://answers.ros.org/question/279030/mebo-20-on-ros/"},
{"title": "The new path planner(ftc_local_planner) can't work with hector_mapping together", "time": "2017-07-18 22:11:56 -0600", "post_content": [" ", " ", " ", " ", "Hi, all, this question is similiar to the question we have asked ", ". But we found new situations, so we change the subject and ask a new question here. ", "We are using hector mapping and move_base to implement an online mapping and path planning.  We changed the combination of global planner and local planner. ", "If the global planner is  ", ", \nthe local planner is ", ", the robot moves fine and builds the map, although the terminal outputs following warning.", "[ WARN] [1500425881.365911556,\n  793.010000000]: Control loop missed its desired rate of 4.0000Hz... the\n  loop actually took 0.5200 seconds", "[ INFO] [1500425881.365978957,\n  793.010000000]: Got new plan", "[ WARN] [1500425881.784682569,\n  793.430000000]: Control loop missed its desired rate of 4.0000Hz... the\n  loop actually took 0.4200 seconds ", "Then if the local planner is ", ",  the global planner is navfn or other methods (We tested a linear global planner programmed by us). The robot almost doesn't move(only with rotation in place sometimes), and the following warning outputs. We also decreased the update frequency of costmap and the control frequency, but it didn't work. ", "We also tried other slam package like ", ". The robot works fine. ", "Can anyone who is familiar with move_base give us advices?", "Thanks!", " For experiments in simulator(gazebo), the hardware is with Intel\u00ae Core\u2122 i5-6500 CPU @ 3.20GHz \u00d7 4 , memory is 15.6 GiB. We thinks such hardware is totally enough for running hector and ftc_local_planner. So it is so weird that the above warning information is output. As I am told from some questions in  ", " , this is usually because the CPU power or memory  is not enough for running such packages. ", "Hi,", ". Thank you for your advices! After by adding the time calculation into the source code in sereral different places. We found that the most time-consumed part is the function getXPose in ", ". ", " Since getXpose is often called by other functions, so the accumulative time is so much that blocks the execution. ", "However, when running with other mapping pacakges such as gmapping, karto, ", "\nWe can change the ...", "as I understand you are trying to simultaneously map and navigate. have you tried using ", " with a previously built map?", "Thank you! We didn't use amcl with a previously built map. In this case, a map server is enough. How the map is built is not important. So the mapping packages don't need to be launched.", "Yes, that is the idea. But I was actually wondering if your objective is indeed building a map while navigating?", "Our objective is not only building a map. The robot also have to finish other tasks such as cleaning room(with coverage path planning) like cleaner vacuum robot. So the tasks and map building are running at the same time. We also obtain the robot's pose from map building to guide the path planning.", "Have you tried ", "?", "No. Actually we also considerd ", " which uses their own move_base node. But we think even if we run them successfully, we can't use the specified planner, our problem is not solved either", "We think the key to solve this problem is: how to modify the move_base(or the planner)  or hector mapping  to make them work together successfully.", "Edit my answer"], "answer": [" ", " ", " ", " ", "Thanks for your measurements ", ". \nThis log calculation time is occurs of to slow tf. Hector_mapping and ftc_planner use often tf transformations. So tf is overwhelmed. \nI have minimized the calls of tf transformations and put this changes for the moment in an new branch \"fix_transformation\". If I have test the changes in detail I will merge it to the master branch.", "This could coused because the ftc_planner need to much time to calculate. \nCan you show your ftc_planner paramaters?", "And how big is your global map with your are build (Resolution)?", "Drives your robot fine, if you use ftc_planner on a already created global map (without hector mapping)?", "I cant reconstruct this problem. If I use the same parameter the planner works fine with hector mapping. ", "I have add a Debug output in the ftc_planner. Now you should see how long the plan calculation is. Mostly it should lover than 0.0005 seconds It is shown the calculation time in seconds.", "The calculation time of the ftc_planner is only dependet on the global plan lenght and its resolution. So what is your global planner?", "You can also activate the debug output at the hector map (in the launch file) to see the calculation time of it.", "My Output with same parameters and navfn in gazebo simulation:", "The calculation of the path is very simple (you can look in die driveForward Methode at the code). There is no couse because the calculation is so long. ", "But i have also used some times the turtlebot, and there is very slow in calculation because many\nbackground processes. But my turtlebot wasnt so slow.", "I have install the turtlebot packages and drives the turlebot in simulation. The calculation isnt so high like yours >0.01 seconds. I have run: Gazebo, RVIZ, Turtlebot amcl_demo.launch (with ftc_local_planner) and hector_mapping. I have deactivate \"pub_map_odom_transform\" at hector_mapping.\nAnd I have nicer parameters for FTCPlanner:", "Out global planner is ", ". We tested it mostly in simulated environment by gazebo . If we change this global planner to be navFn\uff0c the problem still exists.\nThanks!", "ok. I have no plan why it doesnt work with hector. For my personal interest can you check the Debug output of ftc_planner (the new git push before an hour), and how big the calculcation time is? (Enable Debug output with roslaunch rqt_logger_level rqt_logger_level)", "roslaunch rqt_top rqt_top could also be usefull.", "Thanks! You mentioned the amcl_demo.launch. Did you do your experiments with a known map built by hector mapping in advance?  We didn't use amcl. We launched  hector mapping and movebase at the same time. It's a navigation with online mapping. The robot's pose is obtained from mapping, not from amcl", "Because I cant reconstruct this problem. If you want use ftc_planner you must show where the calculation problem is. Thereforce add duration debug outputs in the driveToward in ftc_planner.cpp. Or you must use dwa_planner with hector (But with personal experience dwa drives very worse)", "We updated the question, please read it and give us advices, Thanks!"], "question_code": ["[ WARN] [1500431832.147879220, 6740.630000000]: Map update loop missed its desired rate of 1.0000Hz... the loop actually took 21.1200 seconds\n[ WARN] [1500431854.738732473, 6763.210000000]: Control loop missed its desired rate of 4.0000Hz... the loop actually took 22.5800 seconds\n[ WARN] [1500431854.738900263, 6763.210000000]: Map update loop missed its desired rate of 1.0000Hz... the loop actually took 21.5800 seconds\n"], "answer_code": ["[DEBUG] [1501069889.603913517, 2303.245000000]: Planning...\n[DEBUG] [1501069889.606579979, 2303.245000000]: Got Plan with 65 points!\n[DEBUG] [1501069889.606638318, 2303.245000000]: Generated a plan from the base_global_planner\n[DEBUG] [1501069889.606699296, 2303.245000000]: Planner thread is suspending\n[DEBUG] [1501069889.883098172, 2303.514000000]: Publishing feedback for goal, id: /move_base-1-2287.511000000, stamp: 2287.51\n[DEBUG] [1501069889.883148549, 2303.514000000]: Publishing feedback for goal with id: /move_base-1-2287.511000000 and stamp: 2287.51\n[DEBUG] [1501069889.883180475, 2303.514000000]: Got a new plan...swap pointers\n[DEBUG] [1501069889.883201015, 2303.514000000]: pointers swapped!\n[DEBUG] [1501069889.883239073, 2303.514000000]: FTCPlanner: Old Goal == new Goal.\n[DEBUG] [1501069889.883253688, 2303.514000000]: In controlling state.\n[DEBUG] [1501069889.883484852, 2303.515000000]: FTCPlanner: max_point: 37, distance: 0.256672, x_vel: 0.233333, rot_vel: 0.022912, angle: 0.017184\n[ INFO] [1501069889.883600382, 2303.515000000]: FTCPlanner: Calculation time: 0.001000 seconds\n[DEBUG] [1501069889.883617439, 2303.515000000]: Got a valid command from the local planner: 0.233, 0.000, 0.023\n[DEBUG] [1501069889.883634019, 2303.515000000]: Full control cycle time: 0.000611030\n", "FTCPlanner:\n  max_x_vel: 0.2\n  max_rotation_vel: 2.0\n  min_rotation_vel ..."], "url": "https://answers.ros.org/question/266759/the-new-path-plannerftc_local_planner-cant-work-with-hector_mapping-together/"},
{"title": "The recommended way to integrate ROS and GAZEBO to build a robot model", "time": "2017-10-13 08:16:22 -0600", "post_content": [" ", " ", "Hello, I know that there is a plenty of treads about this subjects but none of them really answered my doubts. But first, a little contextualization. ", "I am using ROS a while and currently I consider myself an intermediate ROS user. Recently I joint a project where I will be developing and implementing algorithms for an autonomous  airship. In the scope of the project, there is not an airship simulator integrated with ROS and since it would be a value tool to have I decided to develop one via GAZEBO. In GAZEBO I consider myself a beginner. ", "For now I studying a lot of documentations and tutorials and so I am feeling somewhat drowned by all this new information. So I decided to open this thread as an way to consult some expert in ROS and GAZEBO. Here goes what I want and my questions:", "My question is, since for me appears that everything comes from this, the robot model should be build upon the ", " or ", " format to I be able to achieve the detailed functionalities? Because, from what I read, if I use SDF I easily will be able to use the GAZEBO tools and plugins but, in the other hand, I will have difficult to interact with it via ROS. If I use URDF I will be able to interact with it via ROS but I may face some issues with GAZEBO or its plugins.  So, what should I work with?", "I know that this thread seems a little broad, but I just want an orientation to where to start. For example, If in a few days I  make a square floats in GAZEBO that I can move it via ROS it will be a great achievement!", "Not an answer, but your question reminded me of  ", ", one of the presentations at ROSCon'17. It's about underwater robotics, ..", ".. but a lot of it is probably applicable to your situation as well (similar dynamics?).", "The team of the presenter implemented a full control stack + model + gazebo plugins for their robot. Perhaps you can use that as inspiration for your own work.", "Be sure to check the ", " as well."], "answer": [" ", " ", "I'm by no means an expert, but so far I've found the ROS+Gazebo path to be pretty productive. All of the robot logic is in ROS, and the simulation is run in Gazebo. I was able to bring up my robot before I had settled on a hardware design, and Gazebo saves me tons of testing time as I add features.", "A few caveats:", "(1) the URDF-to-SDF conversion process is a bit tricky. In my workflow I found it helpful to check the intermediate SDF output to make sure I was using the gazebo tags correctly in my URDF file (I wasn't.)", "(2) The aerodynamics and hydrodynamics plugins are good starting points, but there's a lot that they don't do. There are some ", " ", ", but it also helped me to write a few simple plugins to get familiar with how Gazebo worked."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "A want to develop o model of a balloon (let, for now, forget about airship) which I can simulate via GAZEBO and I can \ninteract via ROS. With GAZEBO I expect to use all, or most of all, its  simulations tools. In special I want to be able to use the plugins Aerodynamics and Hydrodynamics. With ROS I expect to send control commands to the model and receive sensors information from it"], "url": "https://answers.ros.org/question/272977/the-recommended-way-to-integrate-ros-and-gazebo-to-build-a-robot-model/"},
{"title": "The hector_mapping and move_base can't work together ,but cartographer can.", "time": "2017-07-09 08:00:51 -0600", "post_content": [" ", " ", " ", " ", "Hi , all :", "I want to use hector_mapping and move_base to work together on the turtlebot which has a rplidar,but when I shart up move_base to get a path planning, I find move_base continuously print the alarm information, ", "which leads to the robot path planning is not smooth. The robot dosen't move. I have already reduced update_frequency, publish_frequency of  global_costmap and local_costmap.  I also decrease the controll frequence of planner in move base. However, move_base still continuously output the warning information. The robot dosen't move a little.", "PS:  when I only use hector_mapping build a map, everything is ok. who can tell me why move_base print this warn and how to solve this problem ? Thanks.", "\nI also tried the ", "  and ", " with movebase. They all worked fine. ", "Actually, we have monitored the usage of CPU and memory when running these SLAM packages. We have the order of consumption is ", ".", "Hopefully anyone give us any advices!", " For experiments in simulator(gazebo), the hardware is with Intel\u00ae Core\u2122 i5-6500 CPU @ 3.20GHz \u00d7 4 , memory is 15.6 GiB. We thinks such hardware is totally enough for running hector and ftc_local_planner. So it is so weird that the above warning information is output. As I am told from some questions in  ", " , this is usually because the CPU power or memory  is not enough for running such packages. ", "We found new situations and asked a new question here, please move to ", ". Thanks!"], "answer": [" ", " ", "Hello,in my opinion,the move_base package work in a different environment.Move_base need the amcl and odom information,but the hector don't approve that.It seems that move_base use a different map message,too.\n       And,how about your hector map?Did it build correctly?I run the hector map with some errors.", "hi, the mapping process is without move_base. After a local map( eg. a room) is built, the move_base node is used. We use such combination(we don't use amcl) to implement an online mapping and path planning. Thank you!", "We found new situation and asked a new question here, please move to ", " Thanks!"], "question_code": ["[ WARN] [1499649044.602318070, 109.050000000]: Control loop missed its desired rate of 3.0000Hz... the loop actually took 3.1000 seconds\n[ WARN] [1499649044.602556838, 109.050000000]: Map update loop missed its desired rate of 3.0000Hz... the loop actually took 2.7667 seconds\n"], "url": "https://answers.ros.org/question/265834/the-hector_mapping-and-move_base-cant-work-together-but-cartographer-can/"},
{"title": "Unconnected nodes: Hector_Slam and Navigation", "time": "2017-05-21 06:58:06 -0600", "post_content": [" ", " ", "I try to get the hector_slam (github.com/tu-darmstadt-ros-pkg/hector_slam.git /catkin) package running with the navigation stack (github.com/ros-planning/navigation.git /kinetic-devel). The hector_slam package runs well. A map is generated, localization works smooth. Trying to get the hector_slam running with the navigation stack leads me to troubles.", "I think that I'm doing something wrong with the configuration.", " tells me:", "The ", " looks like this:\n", "The ", " delivers:\n", "My configuration files for the navigation stack are as the following ones:", "1) The ", " file", "2) The ", ":", "3) The ", "4) The ", "5) The ", "What did I wrong here?", "Please add details what the \"troubles\" are", "Hi Humpelstilzchen,", "thank you for your comment.", "1) Sending goals is okay. But a navigation path is not calculated.\n2) While looking for the problem myself I \"activated\" roswtf. It said me that there are unconnected nodes. See the beginning of my question.", "Usually move_base outputs a lot of logging information when something is wrong. From the roswtf output: Yes, odom unconnected is usually a major problem. Doesn't your robot have odometry output?", "Hi again, I am sorry for the delayed answer.", "My robot provides odom published by the hector_slam. After your hint I took a look into the rosgraph. The topic 'scanmatch_odom' seems unconnected. I also do not see any chance to configure it somewhere in the navigation stack.", "ok, so no real odom, you could ", ". But using hector_mapping this is probably not what you want. Also I don't see odom used in your config. But I don't know hector_slam_launch)/launch/slam.launch. It doesn't seem to exist.", "I think you're probably right. hector_mapping delivers 'scanmatch_odom' topic, the navigation stack consumes 'odom' topic. Remapping could solve the problem. Why do you say, that hector_mapping is probably not what I want? Is there another (improved) way to go?", "When remapping the topic to '/odom' the navigation stack tells that 'odom' is found. That's great. But when I'm sending 2D Nav Goals (via Rviz) there is no (global /local) plan calculated and showed in Rviz.", "A remap is probably not what you want since hector is afaik optimized to work without odometry information (Correct me?). The odometry publisher of hector doesn't even seem to be documented."], "answer": [], "question_code": ["Found 1 warning(s).\nWarnings are things that may be just fine, but are sometimes at fault\n\nWARNING The following node subscriptions are unconnected:\n * /hector_mapping:\n   * /initialpose\n   * /syscommand\n   * /tf_static\n * /move_base:\n   * /move_base_simple/goal\n   * /tf_static\n   * /odom\n   * /move_base/cancel\n\n\nFound 1 error(s).\nERROR The following nodes should be connected but aren't:\n * /move_base->/move_base (/move_base/global_costmap/footprint)\n * /move_base->/move_base (/move_base/local_costmap/footprint)\n", "<launch>\n  <include file=\"$(find rplidar_ros)/launch/rplidar.launch\" />\n  <include file=\"$(find hector_slam_launch)/launch/slam.launch\" />\n  <!-- <include file=\"$(find amcl)/examples/amcl_diff.launch\" /> -->\n\n <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"$(find darkmole_nav)/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"$(find darkmole_nav)/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"$(find darkmole_nav)/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find darkmole_nav)/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find darkmole_nav)/base_local_planner_params.yaml\" command=\"load\" />\n  </node>\n</launch>\n", "TrajectoryPlannerROS:\n  max_vel_x: 0.45\n  min_vel_x: 0.1\n  max_vel_theta: 1.0\n  min_in_place_vel_theta: 0.4\n\n  acc_lim_theta: 3.2\n  acc_lim_x: 2.5\n  acc_lim_y: 2.5\n\n  holonomic_robot: false\n", "obstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[0.305, 0.278], [0.04, 0.193], [-0.04, 0.193], [-0.282, 0.178], [-0.282, -0.178], [-0.04, -0.193], [0.04, -0.193], [0.305, -0.278]]\n#robot_radius: ir_of_robot\ninflation_radius: 0.55\ntransform_tolerance: 1.5\nobservation_sources: laser_scan_sensor\nupdate_frequency: 1.0\nlaser_scan_sensor: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true}\n", "global_costmap:\n  global_frame: map\n  robot_base_frame: base_link\n# update_frequency: 5.0\n  static_map: true\n", "local_costmap:\n  global_frame: map\n  robot_base_frame: base_link\n  # update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: true\n  rolling_window: true\n  width: 6.0\n  height: 6.0\n  resolution: 0.05\n"], "url": "https://answers.ros.org/question/262213/unconnected-nodes-hector_slam-and-navigation/"},
{"title": "Self-drive car PhD studentship - ROS - Leeds UK - fully funded [closed]", "time": "2017-04-13 08:04:41 -0600", "post_content": [" ", " ", "May be of interest to members of this forum -- we have money available to pay someone to work with - and maybe on - ROS - around self-driving cars.    Applications from ROS forum members with strong track records of being helpful are particularly welcome.", "Fully-funded PhD available - University of Leeds, UK", "SELF-DRIVING CARS \u2013 robotics, machine vision, human interactions", "Comes with enhanced salary, training grant expenses, access to\nvehicles, data and software. (Funding is for UK/EU/EEA students only)", "Self-driving pods are small, electric, shared, autonomous person\ntransporter vehicles which may form a solution to \u201clast mile\u201d\ntransportation, with commuters travelling by train into a city then\ntransferring to a pod to reach their workplace. Such vehicles have\nrecently been used for demonstrations in a number of cities.", " We have previously worked with self-driving minibus-like vehicles in\nthe EU CityMobil2 project, including delivering road user detection\nsystems ( ", " ). We also led the InnovateUK\nIBEX2 autonomous agriculture vehicle project, delivering localisation,\nplanning and vision systems ( ", " ). ", "As part of newly funded projects in this area, there is currently full\nfunding available for a PhD position to extend the work in related\nareas including:", "1) Construction of new hardware and software systems for pod vehicles.\nUsing SLAM algorithms, lidar and machine vision sensors, mapping and\nplanning systems; which may also include mechanical and electronics\nwork on the vehicles themselves;", "2) Use of the pod to study live and recorded human-vehicle\ninteractions with pedestrians and other road users. Machine vision\nmethods for road user detection and novel scene analysis / event\ndetection and prediction algorithms to understand road user behaviour\nand potential safety threats, such as who is likely to pull out in\nfront of the vehicle. This work may include models from Psychology\nsuch as crowd behaviour and game theory, as well as agent-based\nmicro-simulation, and statistical / big-data analysis of results.", "ENTRY REQUIREMENTS:", "Required skills (to be evidenced by CV, references and interview):\n- Very strong programming and applied maths skills; (e.g. from\nindustry experience, portfolio, open source contributions, or academic\nqualifications)\n- Knowledge and experience of robotics, machine vision, and/or human\nbehaviour /game theory/ HCI interaction modelling (e.g. from academic\nqualifications, industry experience, portfolio, or open source\ncontributions)\n- Ability to work well in a technical team (e.g. from strong\nrecommendation letters, social networks, or documented open source\nhistory)", "Desirable/optional skills:\n- Experience of publishing academic or industrial papers, or\nopen-source projects\n- Linux, Python, C++, ROS, OpenCV, git, PCL, SQL, SGE/Hadoop/Spark\n- Understanding the behaviour of pedestrians or crowds on the road\n- Electronics, mechanics, vehicle maintenance, DIY, welding, Arduino,\nhardware hacking", " Further information about entry requirements can be found here:\n ", "HOW TO APPLY:", " Please send a CV and a short \u2018statement of motivation\u2019 to Dr Charles\nFox ( ", " ). Further information will then be provided.\nDr Fox is also available for informal consultation if you would like\nto find out more ...", " This is a question and answer site. Job postings can go here:  "], "answer": [], "url": "https://answers.ros.org/question/259179/self-drive-car-phd-studentship-ros-leeds-uk-fully-funded/"},
{"title": "Compress images into custom message", "time": "2017-02-14 18:36:48 -0600", "post_content": [" ", " ", "I want to build a frame filter to synchronize the image streams from an RGBD camera into one message. I am using a ", " to listen to the ", " and ", " streams and want to publish them in a single message (see blow).", "How do I compress/decompress the images to put them into the message? Is it possible to build the messages manually by compressing/decompressing the images with OpenCV? Can I utilize image_transport for that to avoid high memory consumption across several nodes?"], "answer": [], "question_code": ["ApproximateTime Synchronizer", "sensor_msgs/Image", "sensor_msgs/CameraInfo", "Header header\nsensor_msgs/CompressedImage rgb_image\nsensor_msgs/CompressedImage depth_image\nsensor_msgs/CameraInfo rgb_cam\nsensor_msgs/CameraInfo depth_cam\n"], "url": "https://answers.ros.org/question/254698/compress-images-into-custom-message/"},
{"title": "I cannot play a rosbag file for gmapping.", "time": "2017-01-10 00:02:38 -0600", "post_content": [" ", " ", " ", " ", "Hello.", " I'm installed ros-indigo in Ubuntu 14.04 (VMware).\nI tried \"How to Build a Map Using Logged Data\".\n ", " \nBut, I did not play a rosbag file (basic_localization_stage.bag) completely.\nGenerated map was a different map from \u201cbasic_localization_stage_ground_truth.png\u201d. ", "Below is commands and the error message:", "and, another tarminal;", " Others information (generated map, rqt_graph, ...)\n ", "I want to solve this problem.\nThanks in advance.", "Error messege\uff1a\n    [ WARN] [1483110424.590366037, 125.179980513]: Detected jump back in time. Clearing TF buffer.", " I tried same commands in Ubuntu 12.04 (hydro), but \uff54he same problem occurred.\nI consulted the following pages for the order of commands.\n ", "I do not know what is wrong."], "answer": [" ", " ", "I probably solved this problem myself.\nI think that the cause is because the bagfile is wrong.", "I got a rosbag file for gmapping from this site.", "However, This bagfile (basic_localization_stage.bag) might be different from the ground truth map image (basic_localization_stage_ground_truth.png).", "Therefore, I got a bagfile with the same name (basic_localization_stage.bag) from others.", "In this case, gmapping and rosbag worked well.\nI could get a generated map like a ground truth map."], "question_code": ["$ rosparam set use_sim_time true\n$ rosrun gmapping slam_gmapping scan:=base_scan\n", "$ rosbag play ./basic_localization_stage.bag --clock\n", "[ INFO] [1483110424.111356815, 124.700547870]: Laser is mounted upwards.\n -maxUrange 29.99 -maxUrange 29.99 -sigma     0.05 -kernelSize 1 -lstep 0.05 -lobsGain 3 -astep 0.05\n -srr 0.1 -srt 0.2 -str 0.1 -stt 0.2\n -linearUpdate 1 -angularUpdate 0.5 -resampleThreshold 0.5\n -xmin -100 -xmax 100 -ymin -100 -ymax 100 -delta 0.05 -particles 30\n[ INFO] [1483110424.158217525, 124.745581898]: Initialization complete\nupdate frame 0\nupdate ld=0 ad=0\nLaser Pose= -19.396 -8.33452 -1.67552\nm_count 0\nRegistering First Scan\n[ WARN] [1483110424.590366037, 125.179980513]: Detected jump back in time. Clearing TF buffer.\n\n...\n\n[ WARN] [1483110457.960022078, 158.549614954]: Detected jump back in time. Clearing TF buffer.\n"], "url": "https://answers.ros.org/question/251619/i-cannot-play-a-rosbag-file-for-gmapping/"},
{"title": "Cover an angle range from -210 to -150 degrees for a laser scan", "time": "2016-09-21 06:23:19 -0600", "post_content": [" ", " ", " ", " ", "I have a sensor mounted in the rear of my mobile base, facing backwards. It creates a pointcloud and I am trying to create a fake laserscan using pointcloud_to_laserscan package.", "The problem is that relative to base_footprint, I need the range from -210 until -150 degrees (the sensor has a horizontal field of view of 60 degrees, facing backwards) to be collapsed into a laserscan. Trying (transformed to rad of course)", "and visualizing the data in rviz, it seems that only -180 until -150 is taken into consideration. When I use", "It just works fine but I get a lot of useless points that cover areas that the sensor does not cover (thus getting the value Inf assigned).", "The less data consuming alternative would be to create a second base_footprint_rotated that will be rotated by 180 degrees relative to base_footprint (around z axis) and use it as reference for defining the region of interest for the laserscan as", "Is there any other way for overcoming this limitation?", "[..] angles in ROS are defined from -Pi until +Pi [..]", "This may be a limitation of some components, but I'd be very surprised if this is true ", ". There is no such limit. Angles (ambiguous) can be any value. Also: ROS standardised on quaternions, and this seems like something else.", "I would say a limitaton for the laserscan min and max angle definition.", "That makes more sense :). Would perhaps be nice to update your question?"], "answer": [" ", " ", " ", " ", "I think the customary way this is handled is that your URDF for the robot would include the sensor's pose, and the sensor's  tf would be published by the ", ". Then, when you're lidar (or other sensor) publishes it's point cloud, it uses that tf in the point cloud messages' headers. ", "If you don't have a URDF for the robot (or don't want to mess around with editing it) then you're idea of setting the ", " for the ", " node to a ", " is a good way to achieve what you're wanting to do but it is a hack because only ", " will know the correct orientation. Using the URDF is the correct way because all nodes will know the sensor points backwards. In other words, if you add a node in the future that needs that data, you'll have to do a similar hack again unless you edit the URDF.", "When I mentioned base_footprint_rotated it was indeed a new link in the robot's urdf. And the sensor also has a link in the urdf but I think it doesn't make sense to use the sensor's link as target_frame. The reason is that parameters like max_height and min_height become very cumbersome to define", "if they are not expressed relative to the floor (here base_footprint). They even might have no sense at all as z in the robot's frame is the normal height but if you consider the z axis in the camera's frame, it is the depth, and you don't want to change that.", "They shouldn't be cumbersome - min_height is just the distance from the sensor to the floor and max_height is the distance from the sensor to the top of the robot.(plus a little bit for clearance).", "so in the consideration of min and max heights, it is being internally converted to the \"floor\" frame? It is not clear to me in which coordinate system those min and max are defined.", "They are with respect to the sensor frame. As long as your sensor is mounted level and the sensor frame in the URDF uses the ", ", everything should just work, you could measure with a measuring tape.", "To be clear, your min number would be negative, the sensor is at 0, and the max number would be positive.", "so you mean they are defined in the base_footprint frame with an offset representing the z coordinate of the sensor in base_footprint frame (height means z coordinate as far as I know). But I think I will still use base_footprint as target and let TF do the conversion for me, just to avoid confusion", "After all you want to define the min and max heights of obstacles you want to detect relative to your mobile base's frame."], "question_code": ["min_angle: -210\nmax_angle: -150\n", "min_angle: -180\nmax_angle: +180\n", "min_angle: -30\nmax_angle: +30\ntarget_frame: base_footprint_rotated\n"], "answer_code": ["target_frame", "pointcloud_to_laserscan", "pointcloud_to_laserscan"], "url": "https://answers.ros.org/question/244152/cover-an-angle-range-from-210-to-150-degrees-for-a-laser-scan/"},
{"title": "Which single board computer should I chose?", "time": "2016-03-26 06:43:19 -0600", "post_content": [" ", " ", " ", " ", "Hi ros community", "I' m working with ros to make a robot which can go itself with image proccessing. I am using lots of hardware in this project and I used pcduino V3 until this time. But I want to change pcduino because I' m having some trouble with it.", "I looked lots of single board compuers on the internet and finally find 4 boards to chosing. But I cannot decide which board I should choice for best performance. Can anyone help me for it?", "I can give 100$ at most, so I find:", "Which board is the powerfull board from other boards or are there more powerpull board from these?(less $100)", "Thanks a lot..", "Take a look at the BeagleBone Black too. It costs $55 and you can install Ubuntu 14.04.x for ARMhf and ROS Indigo for ARMhf on it. It also has a ton of I/O pins.", "Thanks for your advice but beagle bone black' s performance is less than all off the boards in the list. So I shouldn' t prefer it.", "Do you run from a battery? If so you might want to ditch the Cortex A7 ones.", "I have to use battery because I am working with a mobile robot. But it causes power consumption problem if I choice like odroid xu4.", " I'm curious, why is that? Does the Cortex A7 consume a lot of power? Is that also true of the A8? Thanks!", "Its not that the A7 in particular consumes a lot of power, its actually pretty good. It is just that various benchmarks (including my own) seem to indicate a better performance/watt ratio for the A53 then the older A7/A8/A9."], "answer": [" ", " ", "If you want more powerful performance, you will usually need to provide more power, this is one of the trade-offs you need to consider. ", " we used x-u4 with turtlebots and were very pleased with their performance. We used  ", "  to step down the power from the kobuki base. ", "Thanks for your advice. I have one more question.", "I want to use 7 A/h Li-ion battery in my project. Is it enough for this board?", "Depends how long you hope it will run (and what voltage the battery is). The x-u4 only peak very briefly at more than 3A as they start up, most of the time they are in the 1-2A range, but that will also depend on what you are getting them to calculate. We used the eMMC memory option with them.", "Thanks again.", "If I can work more than 30 minutes without charging is enough for me. Some other additional hardware (like wireless) will work with it. And 7 A/h battery will feed only them. If not enough can you advice a battery?", " ", " ", "If you like to have an easy ros usage you should use a board with Ubuntu 14.04 support. With your spec I think the odroid xu4 is the best. But clock speed is not the only think. The Ram size is important to.", "The other thinks are Linux drivers for open cl and open gl es, if you need some kind of visualization of ros.", "The PI 3 should be nice to but there is no Ubuntu 14.04 support. You can use debian Jessie, but the environment setup is non that easy but possible.", "Thanks for your answer.", "The ram size in these boards are same and 2 GB so I don' t need to write it. I want to chioce odroid xu4 but it has a very big power problem. How can I give it 4A without adapter?", " ", " ", "I have looked at many boards and have benchmarked a few of them.\nBBB will not have the performance that you need.", "Qualcomm's dragonboard 410C is a good board.  75$ USD. ", " I installed Indigo on it using CHROOT\n( ", " ) ", "RPI3 Should do the job, GPU not as good as the DB but you probably will not use the GPU unless you write your own image processing algorithms using the GPU directly.", "Pine64 is another one to try.", "Basically I think an A53 process would be a good choice.  if you had a larger budget then the NVIDIA Jetson X1 would be ideal :)"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Banana Pi M3          ==>  1.8Ghz * 8 Cores / PowerVR SGX544MP1 / wireless / 8 Gb Emmc Flash /2 usb ports", "Orange Pi Plus 2      ==> 1.6Ghz * 4 Cores / Mali 400 Mp2            / wirekes / 16 Gb Emmc Flash / 4 usb ports", "Odroid XU4             ==> 2.0Ghz * 4 Cores + 1.4 Ghz * 4 Cores / Mali T 628 MP6 / no wireless / no flash / 3 usb ports", "Odroid C2               ==>  2.0Ghz * 4 Cores / Mali 450                   / no wireless / no flash           / 4 usb ports"], "url": "https://answers.ros.org/question/230198/which-single-board-computer-should-i-chose/"},
{"title": "How to stream CompressImage data as mjpeg", "time": "2016-03-17 23:44:27 -0600", "post_content": [" ", " ", "Are there any packages for streaming ", " data as mjpeg over http?", "I've found ", " and ", ", and while these support output as mjpeg, they only accept raw uncompressed image data as input.", "My project is running on a Raspberry Pi, so computing resources are limited. I'm using ", " to stream image data. It supports both raw and compressed output. However, the raw node consumes 180% CPU whereas the compressed node consumes...9%, so you can imagine why I'd like to use the compressed stream."], "answer": [" ", " ", " ", " ", " I believe web_video_server does support compressed images. Source code for the class here:  ", " ", " ", "how do I use in a web page? I can sometimes get the video tag to work but only with certain cameras and I am not sure why. What is the best source of documentation for web_video_server?"], "url": "https://answers.ros.org/question/229420/how-to-stream-compressimage-data-as-mjpeg/"},
{"title": "How to interface an Arduino with ROS?", "time": "2015-12-14 09:00:08 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to control a few Arduinos (Uno and Leonardo) from ROS. What's the \"right\" way to do this?", "I started with Rosserial, by following the simple ", ". It installed and compiled just fine, but much to my surprise, Rosserial simply doesn't work. First, the compiled hex file consumes almost 75% of the Arduino's memory...so I'd have almost nothing left for my application code. Second, it can't actually communicate with the host. Attempting to run ", " fails with the error:", "My PC is a fresh install of Ubuntu 14.04 with Indigo, so I'm sure it's not a version mismatch. Googling this error shows it's a bug in Rosserial related to it being unable to handle certain Arduino USB interfaces, which has gone unresolved for years and has several open bug reports filed for it.", "I spent six hours trying to figure this out, and got nowhere. So then I tried implementing the same \"blink\" functionality with a simple pure Arduino+Python interface using Pyserial....and got it to work in 10 minutes.", "Unfortunately, this has been my experience with ROS in general. It claims to make your life easier, but it has a huge learning curve and even after you read through all the docs and follow all the steps without error...it still doesn't work very well. Am I missing something here? If ROS can't even blink and LED, why should I rely on it for anything more complicated?", " hi,\nhave you rebuild and installed the ros_lib for arduino   ", "No, I tried that, but catkin_make through a ton of errors, so I just installed the ubuntu packages for indigo.", "is there a reason not to use jade?", "Yes, I'm using Ubuntu 14.04 and am not in a position to upgrade my entire computer just to tinker around with ROSSerial.", "Just an FYI: Jade is supported on Trusty (see ", "). There is no need to upgrade your OS."], "answer": [" ", " ", "Look into ", " . It uses a lot less memory on the arduino. ", " ", " ", "Try specifying the serial port using the ", " parameter:", "(I updated the wiki to reflect this several years ago, but it looks like someone reverted my change)", "That was one of the first things I tried. Neither ttyUSB0 nor ttyACM0 worked.", "Trying ports at random is not the correct strategy. Which port is associated with your arduino?"], "question_code": ["serial_node.py", "Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino\n"], "answer_code": ["~port", "rosrun rosserial_python serial_node.py _port:=/dev/ttyUSB0\n"], "url": "https://answers.ros.org/question/222498/how-to-interface-an-arduino-with-ros/"},
{"title": "Understanding TF, URDF and Static_transform_publisher", "time": "2015-06-30 02:45:41 -0600", "post_content": [" ", " ", "I Am struggling to understand where TF and URDF fit it. Also, while consulting the  tutorial on ", " I came across the static_transform_publisher which is confusing me as well.", "Firstly, I am trying to produce a 2d  map of an environment on ROS (indigo). I am planning on using the range_sensor_layer as a costmap_2d plugin to enable the use of Sonar sensors.", "I understand that a URDF places all the robot components in perspective, for example my sensors are placed in the front of the robot each 45 degrees apart, and a URDF allows me to \"place\" these sensors i.e Tell ROS how they are placed.", "Now where does TF come into play (I understand it is the transform from my robot base (base_link) and the so called laser scan), Do I need both TF and URDF and how do they link?", "Also with reference to the costmap_2d tutorial and the static_transform_publisher, it says that a static transform publisher tells the package the transformation from the /map and the base_link, what would this be? Is it the TF or an addition to the TF?"], "answer": [" ", " ", "In your use case, it's easy to get the two confused. But consider a multi-DOF arm. ", "The urdf describes all the things about the arm that don't change: how long the links are, where the joints are in reference to the link, how the joints can move. None of this depends on where the arm is in 3d space.", "tf is a library to keep track of where the arm is in 3d space. It gives you the relative position of from one frame to another. If you look up the tf from your base to the end effector, then move the shoulder joint of the arm, then look up the tf again, it will now reflect the new relative position of your end effector. The urdf hasn't changed, though, because the arm is still the same arm.", "Typically there will be a ", " node that looks at the urdf of your robot and a message of all its joint states. It then calculates where each link of your robot is and broadcasts that to the rest of the network.  ", "urdf is also used for calculating inverse kinematics.", "If the joints between the links of your robot can't move, then the tf is static.  Frequently, though, tf is dynamic. urdf is always static.", "I see, thank you my understanding is slowly coming together, it gets quite overwhelming haha. \nThank you for your response!"], "url": "https://answers.ros.org/question/212525/understanding-tf-urdf-and-static_transform_publisher/"},
{"title": "tf static_transform_publisher breaks laser", "time": "2014-11-12 04:33:30 -0600", "post_content": [" ", " ", " ", " ", "I am trying to use slam gmapping on a ", " with no success. One of the problems is that I have the laser topic working apparently fine (in RViz the points cloud changes as I move the robot), but when I run ", " from ", " to ", " it stop working (in RViz it looks frozen).", "I run it like this:", "Run robot's driver node", "Run odometry node: Basically consumes ", " and provides ", " (", "). I know there is a misconception here because the robot's driver node should provide ", " . I will fix, but I think the problem is not here.", "Run slam gmapping like this: ", "rosrun gmapping slam_gmapping tf_static:=tf", "At this point I get the following frames chain: ", " -> ", " -> ", " ", " and this is the ", ". ", "In RViz I check laser's points cloud by manually putting \"laser\" on fixed frame, and after added LaserScan display and I can see it and it get updated when I move the robot. ", "I also have this error on terminal running it: ", ".", "In some blog posts I've seen that laser frame is published through a static transform through", "rosrun tf static_transform_publisher 0 0 0 0 0 0 /base_link /laser 50.", "Then, in RViz the image freezes even trying clicking \"Reset\" button. On the LaserScan section, in \"Status\" it appears this error: ", ". ", "But also, in the terminal where I run RViz, this is warning message: ", ".  ", "At this point this is the ", " and ", ".", "Finally, when I kill the process of static_transform_publisher, it gets back to behaviour before.", "What could be happening here?"], "answer": [" ", " ", " ", " ", "Just a basic comprobation. Have you tried to launch the static_transform before launching the gmapping and check if the TF tree is correct? Just launch steps 1 (maybe 2 also) and 5 and see in rviz if the laser points move along with the robot when you put ", " as the fixed frame.", "Also, the way you launch gmapping seems strange to me. I don't know about the ", " package, but usually slam_gmapping from ", " package should be launched ", ". Also, slam_gmapping doesn't subscribe to ", " but to ", ", so no remapping should be necessary.", "Edit:", "I think that probably the problem is in the robot's driver that publishes the /laser topic. My guess is that something makes it to delay the publication of the laserScans and hence it's time stamp is too outdated (maybe some heavy processing, sleep, time locking...). That's why you see the lasers in rviz when the fixed frame is /laser (no TF required) but fails when you change the fixed frame to other frame (tries to use the TF to visualize it, but the laserScan's time stamp is too old).", "Also note that there is a huge delay between the last TF published by ", " and the ones and published by ", " and ", ", which point in the same direction. Are the driver and the odometry node running in separate computers than the static transform publisher and gmapping?", "I tried and I cannot see anything in RViz. It also shows this error message: \"Message removed because it is too old (frame=[/laser] stamp=[1415802689.161971000]).\nThe \"gaming\" package was a typo. I fixed it. Thanks.", "The time stamp observation is definitely causing this problem. They seem to be about an hour apart. Something is wrong there.", "The problem was a bug in the custom timestamp module for the robot driver node. It was 1 hour delayed. Once fixed everything worked fine.", " ", " ", "I would forget about gmapping for now and just make sure that the rest works fine. gmapping then usually works without problems. I suspect you have setup you TF chain wrong. You should have something like /odom -> /base_link -> /laser.\nFor each transform in this chain there must be one and only one publisher.", "First, don't start your odometry node and check what TF you have (e.g. ", " and/or check the fixed frames in rviz.). Someone should provide /base_link -> /laser and you should be able to select /base_link as a fixed frame in rviz without errors. This is either provided by your robot, e.g. by URDF + robot_state_publisher or your static_transform_publisher. There must be only one publisher.", "Second, if that works start your odometry node. This node must provide odometry TF, that is the /odom -> /base_link TF transform, not only the /odom topic. You should be able to select /odom as a fixed frame in TF. Everything should work fine and driving around you should see the laser being transformed correctly in rviz. If you set the laser display decay high in rviz, you should see an \"odometry map\" of laser scans.", "If all this works fine, gmapping should work fine. If not, fix the step that doesn't work without running anything else that interferes."], "question_details": [" ", " ", " ", " ", " ", " ", " ", "Publishes: ", " (", ") and ", " (", ")", "Subscribes: ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["static_transform_publisher", "/base_link", "/laser", "/scan", "/pose", "/command_velocity", "/pose", "/odom", "/odom", "/map", "/odom", "/base_link", "[ WARN] [1415365252.295826588]: MessageFilter [target=odom ]: Dropped 100.00% of messages so far. Please turn the [ros.gmapping.message_notifier] rosconsole logger to DEBUG for more information", "\"Transform [sender=unknown_publisher] Message removed because it is too old (frame=[/laser],stamp=[1415289470.551291000])\"", "[ WARN] [1415365456.608259629]: MessageFilter [target=map ]:   The majority of dropped messages were due to messages growing older than the TF cache time.  The last message's timestamp was: 1415361856,504979, and the last frame_id was: /odom"], "answer_code": ["base_link", "rosrun gmapping slam_gmapping", "tf_static", "tf", "/tf_out_relay", "/static_transform_publisher", "/slam_gmapping", "rosrun tf view_frames"], "url": "https://answers.ros.org/question/197269/tf-static_transform_publisher-breaks-laser/"},
{"title": "Error with octomap_server: \"process has died, exit code -6\" [closed]", "time": "2014-10-08 19:29:01 -0600", "post_content": [" ", " ", "I am trying to use ", " (", " file) for building a 3D map using point cloud data from velodyne. So i drive my robot around, record all sensor data for ", " to post process it on my desktop. I really want to play the rosag at slower rate so that I can get a better 3D map, but unfortunately the slower I run the rosbag more I get the following error (process has died, exit code -6):", "Once this error occurs, I have to restart ", " and then I loose the previously saved 3D map. If I play my ", " at more than 2x speed, then ", " doesn't fail but I don't get a good 3D map. Does someone have anything on why am I getting this error?", "Also, sometimes the following error pops up, I don't know what that means but it doesn't seem to effect octomap processing. Can someone throw some light on this too.", " means that your process failed to allocate memory. How much memory is the octomap server using before it crashes?", "I have a 4 core processor with 16GB RAM. Observing ", " for few runs, I observed that ", " process uses almost 100% of one core, and the memory usage grows up to 18.4% every time before it dies. Is there any way to allocate more space to the process, as I still have 60% of memory free.", "That's awfully close to 3GB. Are you running a 32-bit version of Ubuntu or a 32-bit version of octomap?", "Most of the memory consumed by octomap_server is actually for the different kinds of visualizations, not for the octomap itself. You can reduce the memory usage by not subscribing to the PointCloud and voxel visualizations (they thus won't be created).", "Instead, use the octomap_rviz_plugins package. It will directly create a visualization in RViz out of the serialized octomap.", ": I am using MarkerArray display in RViz on the topic \"occupied_cells_vis_array\", and changing it to display OccupancyGrid type message from octomap_rviz_plugin doesn't change anything. It still crashes.", "I migrated to a 64-bit platform now. Everything works fine. Apparently, octomap_server is using up to ~5GB RAM which makes sure that I should not run octomap server in real time on a robot."], "answer": [" ", " ", " ", " ", "Thanks ahendrix for leading me in the right direction. After you pointed out the problem with memory allocation, I read a little bit more about how memory allocation works with processes. I am running a 32-bit OS which limits to 2.75 - 3 GB address allocation memory, which is close to 18.4% of 16GB (= 2.94GB). So I'm definitely hitting memory limit. (Reference: ", ")", "Having the rosbag run at different speeds, and still failing at 18.4% memory usage every time makes sure that the above error is because I'm using a 32-bit machine. ", "I'll try running it on a 64-bit computer, and post the limitations (if any) of using octomap with 64 bit.", "Edit: Now that I'm on a 64-bit machine (with 32GB RAM), I tried mappusing octomap. The system monitoring stats show that octomap at its peak uses up toing an outdoor area of around 100mx20m (post processing on recorded data)  50% of memory (range- 35-50%) along multiple processes running on multiple processors (using at least 1 complete core). Now I am convinced that a 64-bit machine with 16+GB RAM is a minimum requirement for building octomap for large areas.", " You're probably actually hitting the 3GB/1GB user/kernel split described here:  ", "  , but the end result is the same: you need a 64-bit OS. "], "question_code": ["octomap_server", "octomap_mapping.launch", "octomap_server", "terminate called after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc\n", "octomap_server", "rosbag", "octomap_server", "[ERROR] [1412812984.342799452]: Error serializing OctoMap\n", "std::bad_alloc", "top", "octomap_server"], "url": "https://answers.ros.org/question/194543/error-with-octomap_server-process-has-died-exit-code-6/"},
{"title": "Minimum Hardware Requirement for ROS", "time": "2014-09-19 16:15:41 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm wondering what is the minimum hardware requirement for ROS? We are trying to build a rover with least power consumption. So we need to select the hardware which is energy efficient and can have ROS run on it."], "answer": [" ", " ", "This is a very tough question, because it depends on which software and which ROS nodes you want to run, and how much performance you need. The \"right\" answer for your application could be anything from a BeagleBone to an Intel i7-based desktop, depending on what you want to use it for.", "This question has been asked before on ROS Answers in various forms. You may want to review those questions and answers as well."], "url": "https://answers.ros.org/question/193078/minimum-hardware-requirement-for-ros/"},
{"title": "what is the purpose of zeroconf and avahi?", "time": "2014-03-09 06:03:46 -0600", "post_content": [" ", " ", "Recently, I discovered a process on my ubuntu that was eating away at both processor time and memory, 18% and 10% respectively.  ", " and I got rid of it along with ", ". It seems to me that there is no need for ", " which is a ", " derivative in itself. If this process caused issues, ", ", and eats up precious resources then why is it being utilized in ROS? All the way to Hydro, I see ", ". I read the discription provided by the ROS wiki, and checked Archlinux for ", " and ", ". I still don't see what purpose it serves because I don't see it being a dependency for anything other than ", ". ", "What purpose does zeroconf and avahi serve when the amount of resources it consumes is detrimental in a resource scarce scenario such as most robot systems, e.g. 7-DOF manipulators, vision-based path planning/navigation, etc?", "I am trying to understand what, why, and how of this package/stack. I feel like it should be deprecated."], "answer": [" ", " ", "Later versions of Ubuntu include support for Zeroconf, a technique that allows machines on the same subnet to reference each other using local hostnames instead of IP addresses. So you can use this method if your desktop computer and robot are connected to the same router on a home or office network\u2014a common scenario for both hobby and research robots.", "You can use zeroconf to communicate between two systems (say, robot and desktop) to ssh into using just their hostnames. ", "For eg. If you type $hostname and you get something like \"my_robot\" you can use this instead of the IP address to communicate between the two pcs. Instead of IP address of both PC just using \"my_robot.local\" will be helpful. "], "question_code": ["Avahi-daemon", "pulseaudio", "avahi-daemon", "zeroconf"], "url": "https://answers.ros.org/question/137576/what-is-the-purpose-of-zeroconf-and-avahi/"},
{"title": "rostwitter consumer key", "time": "2014-10-02 11:10:20 -0600", "post_content": [" ", " ", "Dear Ros users,\nI'm trying to user the hydro-rostwitter package (get using sudo apt-get install ros-hydro-rostwitter), but when running get_access_token.py, it asks me a consumer key (not a twitter login).", "But I wouldn't like to develop my own twitter app, I just would like to post a tweet from a rostopic :)\nThank you for your support!"], "answer": [" ", " ", "The rostwitter package use the python-twitter API (and for twitter this is considered as an app!). You have to make your personal registration to the twitter application manager to get all the security keys to put in the yaml configuration file. You have no choice ;-)", " ", " ", "I have rewrite rostwitter package and released as 1.0.45, this version should work only with deb dependency, please try them once it is available on repository."], "url": "https://answers.ros.org/question/194051/rostwitter-consumer-key/"},
{"title": "Getting rid of hddtemp prompt", "time": "2014-08-22 23:01:59 -0600", "post_content": [" ", " ", "Is there any option to get rid of the ", " prompt or automate the response?", "I followed the ", " and ran ", ", and the ", " prompt comes up. Is there a standard way to automatically answer no to this question so that I don't have to 'babysit' every computer that I install ROS on?", "Thanks,\nBart", "This isn't really ros-related; that prompt is coming from the hdtemp package. You may want to consult the dpkg and apt manuals or ask on a Debian or Ubuntu forum.", "That said, I remember being able to do this when I built the PR2 installer, but I don't remember the esoteric invocation that I needed to make it work properly, and I don't remember if it was something that I set in the preseed file or some other way."], "answer": [" ", " ", " ", " ", "The answer (ironically) is on the ", ". It is to do with the Ubuntu package installation. The command is:", "See the page for full context.", "I am in two minds. It's not a ROS problem in the sense that it's not a ROS package that is interrupting the automated setup process, on the other hand it is a ROS problem because it affects a ROS."], "question_code": ["hddtemp", "sudo apt-get install ros-hydro-desktop-full", "hddtemp"], "answer_code": ["echo \"hddtemp hddtemp/daemon boolean false\" | sudo debconf-set-selections"], "url": "https://answers.ros.org/question/191042/getting-rid-of-hddtemp-prompt/"},
{"title": "Where can I contact ROS developers for a collaboration project?", "time": "2013-12-24 13:52:24 -0600", "post_content": [" ", " ", "Hello", "I want to integrate an Adroid app to a platform based on ROS.  I use freelancer and have not found anyone with the skillset.  Is there anyone here who could point me in the right direction to locate a pool of people (or person) who can offer consultancy and development services for ROS on a time and materials basis?  ", "Many thanks in advance, Dataman. "], "answer": [" ", " ", "This is a good place to start:", "/"], "url": "https://answers.ros.org/question/112926/where-can-i-contact-ros-developers-for-a-collaboration-project/"},
{"title": "rosbag API C++: Message Header Stamp sec and nsec are always zero?", "time": "2014-01-27 15:07:23 -0600", "post_content": [" ", " ", " ", " ", "The \"sec\" and \"nsec\" values in the message header are always zero when recording using \"rosbag::write\".  I provide a valid Time for the second parameter, but that only seems to affect the message body timestamp and playback timing, but not the header.  Is that expected?", "I am converting some odometry and depth image information that was collected in Windows to a rosbag containing tf, depth_image, and camera_info messages (using rosbag API in C++)for consumption by the slam_gmapping node.", "The rosbag I create looks correct and it plays back OK (correct timing) but the map is not generated.  I looked at the code, and the slam_gmapping node is using a tf::MessageFilter.  When I turn on DEBUG messages, I see messages like this \"", "\".  The message queue fills up with messages with time ", ".   When I inspect my bag with rqt_bag the header.sec and header.nsec are always zero, so I thought maybe that is the issue.", "I have called \"rosparam set use_sim_time true\", but that does not help.  Why is the header stamp always zero, and is that what is causing the tf::MessageFilter to never synch?"], "answer": [" ", " ", "I figured out how to set the header. ", "\nIt is the geometry_msgs::TransformStamped that has a .header and .stamp... not the tf::tfMessage (which is a list of geometry_msgs)", "-G"], "question_code": ["MessageFilter [target=/odom]: Added message in frame camera_depth_frame at time 0.000", "0.0000"], "url": "https://answers.ros.org/question/123096/rosbag-api-c-message-header-stamp-sec-and-nsec-are-always-zero/"},
{"title": "Ubuntu 12.04 and Pandaboard -> fails to open ASUS xtion/ Driver issues for ARM", "time": "2012-08-26 08:41:28 -0600", "post_content": [" ", " ", " ", " ", "I'm using a Pandaboard which runs Ubuntu 12.04 and I try to use openni_launch. Therefore I need to install the OpenNI drivers. I used the tutorial ", " which is pretty good and helped me (and probably a lot more developers) to save some time. Unfortunately, I'm using an Asus xtion (because of the lag of enough usb ports and the fact of less energy consumption). The compiling seems to work well but when I'm trying to run the tutorials I get the following failure msgs:\n\"Open failed: Failed to open the USB device!\" \nBy executing the samples as sudo I get following output:\n\"Open failed: Device Protocol: Bad Parameter sent!\"", "I know that this is probably the wrong forum to ask this question but I know that the robotic community (especially the Raspberry pi owner) are interested to solve this error.", "Would be great get your thoughts about this issue.\nBest regards"], "answer": [" ", " ", "For kinect use, i needed to add a udev rule... perhaps you have to this too."], "url": "https://answers.ros.org/question/42315/ubuntu-1204-and-pandaboard-fails-to-open-asus-xtion-driver-issues-for-arm/"},
{"title": "TF on multiple robots gets crowded", "time": "2012-02-03 06:55:02 -0600", "post_content": [" ", " ", "I have been running experiments with a team of four KUKA youBots communicating via ROS over WiFi.  I have observed that the /tf topic is crowded with a lot of transforms not of interest to most of the TF consumers.  By the design of TF, all transforms are broadcast globally, and it's up to each ROS node to sort through them and find the ones it is interested in.  Are there any plans to implement partial TF sharing according to the hierarchy, perhaps by a publish/subscribe model---maybe hidden from users within the TF library?", "For example, suppose robot1 broadcasts the frame of each joint in its arm as well as the fingers, wheel orientations, etc.  These transforms are of interest to processes onboard robot1 and to rviz running on a workstation.  But robot2-4 do not care about most of these transforms.  They only want to know the basic position of the robot (i.e. /robot1/base_link) for collision avoidance purposes.", "How big an impact are these excess TF messages?  Let's say we have 2 ROS nodes on each robot that use the tf library.  There are 5 arm joints, 2 fingers, and 4 wheel positions = 11 frames that no other robot cares about.  There are 3 other robots.  Since it's a managed WiFi network, each message must go up to the AP and then down to the neighboring robots.  By default, all these frames are broadcast at 20 Hz.  So that is 2", "11", "20 = 2640 WiFi messages per second.  We have observed severe WiFi lag, which was significantly attenuated by turning off transmission of the arm joints and dropping broadcast rates on the rest.  However, this is not a desirable solution in the long run.", "I can imagine a robot-local TF bus on a topic, say /robot1/tf.  A TF server running on robot1 would maintain a list of transforms of interest to others.  Thus, it would send only /robot1/base_link to the topics /robot2/tf, robot3/tf, etc. and to the global /tf for rviz.  Thus, we could run high-speed mobile manipulation loops on robot1 while sending only 2", "20 = 160 WiFi messages per second, plus 11*20 = 220 WiFi messages destined for rviz (thus not essential to the experiment).", "Is anything like this in the works?  I realize that network topology is a complex topic, but the current solution appears inadequate for all but the simplest multi-robot installations.\nThank you!", "-ross"], "answer": [" ", " ", " ", " ", "tf data traveling overwireless is often too much for anything less than the simplest robots.  In general the multi robot case over wireless is a challenge and most of the focus for solutions for that is to support multiple masters.  There's a ", " if you want more information on that.  ", "With resepect to tf in particular there is a draft implementation of a new version of tf in the geometry experimental stack.  There is development time planned for it to land in Groovy.  There are many different aspects it seeks to improve including wireless operations.  The especially relevant infrastructure upgrade is that it has the ability to remotely ask for a query to be performed in a different node.  Thus trading off the bandwidth saved from continuously monitoring the /tf topic for the extra latency of a query over the network.  This is targeted at a common use case discovered using tf where a process would siimply be monitoring /tf for a specific state and then change based on the result, such as marking a goal reached.  This process could now not subscribe to the whole bandwidth of /tf and simply poll a shared tf server, this is only effective if there are more than one of these processes, which in the cases like the PR2 demos, there are many.  ", "Another approach which as been required for remote access to tf streams, especially for web interfaces is a store and forward approach sending a downsampled set of the tree over a separate topic.  The change_notifier in the tf package is an example implementation, where desired frames are noted with their desired frequency.  There are several other implementations of this floating around, if people see this and they could link to other implementations in the comments that would be great.  Unfortunately so far each application has had it's own specific requirements and tends to lead to very specific implementation details, which don't generalize to other use cases.  ", "Other challenges to come when dealing with multiple robots are the fact that frame_id names start colliding when data is transferred between robots.  The tf_prefix was an attempt to deal with this, however it puts too many requirements on the developer to fully implement the specification and we never got compliant code.  To deal with this when passing messages between robots with similar link names some form of frame_id remapping needs to be applied, both to tf data as well as all data with a frame_id embedded. ", "There's lots of improvements to make tf work better across multiple robots.  If you have suggestions and would like to start a discussion we can do so on the mailing list.  I expect we'll start a SIG for the new tf in the next planning cycle for Groovy.  ", " ", " ", "I believe this is a known weakness in the design of the current tf system.", "I think ", " may address some of these problems, but I'm not sure what the state of development on it is."], "url": "https://answers.ros.org/question/12877/tf-on-multiple-robots-gets-crowded/"},
{"title": "Hundreds of outbound connections to rosout, OS out of sockets [closed]", "time": "2012-02-19 22:37:47 -0600", "post_content": [" ", " ", " ", " ", "This is the output of ", " of one of the nodes running in my computer, all nodes have a similar output. ", "Applications that usually do not use many resources (like robot_monitor) are consuming 100% of the CPU.", "I'd love to be able to dig into this.  Previous reports have not been able to provide reproducible test cases.  Do you think you have something that is easy to reproduce?  Furthermore, can you reproduce this w/o rosbridge running?", "I'd second seeing if you can get this error sans rosbridge. Being so thread heavy and so severely abusive of rospy, it's a likely culprit. This is potentially related to the clients disconnecting without unsubscribing memory leak you spotted :). A fix for one may fix the other. I'll look into it. ", "I have not been able to determine the steps to reproduce  it with or without rosbridge, but a coworker had this same issue without running rosbridge at all. He detected it because robotmonitor was using 99% CPU on top and lsof showed that it had hundreds of CLOSE-WAIT connections to localhost. ", "Can you at least post a the log files from the robot monitor node?  Really can't help unless there's more to work with.", "I've added the log output of robot_monitor. I had some errors attaching it so I had to copy it to the question.", "Thanks, the log is helpful.  Can you try the patch here: ", "  . It's done against Fuerte, but I believe it will work against diamondback as well.   I do recommend moving to Electric, as this patch does not address the root (unknown) cause.", "We're currently migrating to electric, but I will try this in my diamondback installation and see if I stop getting this issue. The problem is that since it's so random it might be a long time before I'm sure it is not happening.", "I believe I've run into a similar issue which I was able to solve. It appears to be a symptom of multiple instances of rosout running and competing. They constantly restart in a race condition and thus create many rosout connections before they can be torn down."], "answer": [], "question_code": ["rosnode info", "$ rosnode info /robot_monitor_8970_1329732898963 \n--------------------------------------------------------------------------------\nNode [/robot_monitor_8970_1329732898963]\nPublications: \n * /rosout [rosgraph_msgs/Log]\n\nSubscriptions: \n * /clock [rosgraph_msgs/Clock]\n * /diagnostics_agg [diagnostic_msgs/DiagnosticArray]\n\nServices: None\n\n\ncontacting node http://denmark:40743/ ...\nPid: 8970\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /rosout\n    * to: /rosout ..."], "url": "https://answers.ros.org/question/27961/hundreds-of-outbound-connections-to-rosout-os-out-of-sockets/"},
{"title": "Kinect: how do I convert depth image to pointcloud?", "time": "2012-02-19 03:33:27 -0600", "post_content": [" ", " ", " ", " ", "I want to send kinect data wirelessly to a desktop computer. Point clouds consume a lot of bandwidth, so I want to send the depth images instead. But on my desktop computer, what is the easiest way to convert those depth images into point clouds? Do I have to make my own node and just grab the correct point cloud functions out of the kinect driver or has someone already done that?"], "answer": [" ", " ", "There also now appears to be a new package ", " in the image_pipeline stack for Fuerte that will take a Kinect uint16 image and transforms it into a point cloud.", "are there any tutorials on that package? I have been trying so hard to find a way to use that package on my rosbag data, but still got no luck. Really frustrated", " ", " ", " ", " ", "The ", " driver provides that functionality. It's broken up in nodelets - some are responsible for publishing the images, and some for processing them and assembling point clouds.", "Normally, you start one nodelet manager, and all nodelets together on the same machine. In your use case, you would have to start two nodelet managers - one on for each machine on the end of the WiFi connection. ", "I had set up some launch files for a similar purpose a while back. You can find them here: ", "/", "The openni_driver file is responsible for publishsing the image data, and the opneni_proc file is responsible for assembling it to 3D. They were set up for a single nodelet manager, so you would have to modify that.", "I found the documentation for the ", " but haven't had time yet to try them with your launch files ... thanks! ", " Unfortunately the URL  ", "  is broken ", " ", " ", "The ", " nodelet (which is in the ", " package) does exactly this. It was developed to allow easy recording of Kinect data to disk, but should adapt to your use case pretty easily."], "answer_code": ["openni_record_player", "semanticmodel"], "url": "https://answers.ros.org/question/27921/kinect-how-do-i-convert-depth-image-to-pointcloud/"},
{"title": "What is the difference between canonical_scan_matcher and polar_scan_matcher ?", "time": "2011-05-11 01:59:05 -0600", "post_content": [" ", " ", "My main question after having read quickly the ROS documentation is the difference beetween the canonical and the polar scan matching.", " I know there must be answers in the related documentations  :\n ", " ", "But, it'll help to have a synthesis of differences (I imagin it is about performance on axis, robustness, consumed cpu ...) on the scan_tools main page. Just some words for newbies like me in this domain :)"], "answer": [" ", " ", "Hi Willy,", "The canonical and polar scan matcher ROS packages are wrappers around two different scan matching algorithms. The ROS packages behave in the same way in terms of what the input and output is. This means you should be able to use them interchangeably.", "Since the algorithms are different, I recommend trying them both, and seeing which one gives you better performance. In our lab, we chose to go with the canonical scan matcher, although we got good results with both. Whichever one you choose, take a look at the parameters - some tweaks here and there can make a big difference.", "In terms of specific differences in the algorithms - as far as I remember, CSM uses a point-to-line metric and works in Cartesian space, while PSM uses a point-to-point metric and works in polar space. Beyond that, I'd suggest you refer to the papers", "Hope this helps"], "url": "https://answers.ros.org/question/9938/what-is-the-difference-between-canonical_scan_matcher-and-polar_scan_matcher/"},
{"title": "Physical setup for collecting tod_training data?", "time": "2011-03-23 02:48:10 -0600", "post_content": [" ", " ", "I wonder how to collect tod_training data? From a ", " I know that we should be able use", "Unfortunately, the tutorial about recording a training bag is empty. I wonder how to setup our environment. We need fiducial markers rigidly attached to a rotating table and in between these markers, we place an object, correct? Is it enough to create a setup as shown on the picture?", "(", ")", "As collection of training data can be time-consuming and needs hardware, I want to make sure we're on the right track before starting the attempt."], "answer": [" ", " ", " ", " ", "We need fiducial markers rigidly attached to a rotating table and in between these markers, we place an object, correct? ", "Yes. ", "Is it enough to create a setup as shown on the picture?", "Yes.", "Must the print size of the fiducial markers have a certain size?", "No. You can use different markers (different cells count, size, etc). But you should change fiducial configuration file (fiducial.yaml) for your markers. ", "Are there any crucial aspects I have to be aware of in my preparation?", "I think no. You should only write bag file with object rotation."], "question_details": [" ", " ", " ", " ", "Must the print size of the fiducial markers have a certain size?", "Are there any crucial aspects I have to be aware of in my preparation?"], "question_code": ["roslaunch tod_training record_openni.launch bag:=pwd/awesome.bag interval:=0.5\n"], "url": "https://answers.ros.org/question/9536/physical-setup-for-collecting-tod_training-data/"},
{"title": "wiimote cpu load [closed]", "time": "2011-05-08 22:11:00 -0600", "post_content": [" ", " ", " ", " ", "Hi all!", "I am experiencing a weird behavior in wiimote_node. When I launch it and subscribe to the /joy topic, the CPU load of the node goes up to 140%! I run the node in an Intel Core2 Duo, where each processor is 1.8 GHz. It seems that those CPU loads are not justified, but I saw in the code that the publishing rate is 100 Hz. Although I changed that frequency to 10 Hz the node keeps on consuming the same CPU. Does anybody have a clue about it?", "Thanks in advance!"], "answer": [" ", " ", " ", " ", "I checked into trunk a fix that reduced the CPU from 114% to 9% ", "r37137: ", "try it out and tell me how it goes.."], "url": "https://answers.ros.org/question/9921/wiimote-cpu-load/"},
{"title": "XnSensorServer CPU consumption", "time": "2011-06-03 00:15:51 -0600", "post_content": [" ", " ", " ", " ", "I notice that XnSensorServer is consistently consuming 43% of the CPU resource.  Is there any way of reducing this, and what code is associated with it?"], "answer": [" ", " ", " ", " ", "I notice that if I set depth_registration to false within openni_camera then CPU usage drops by 30% to a much more reasonable value.  Is depth_registration just registering the RGB image with the depth image?", " ", " ", "As far as I know, registration is done only for rgb point clouds. This might have changed since my last peek at the "], "url": "https://answers.ros.org/question/10151/xnsensorserver-cpu-consumption/"},
{"title": "topic tantrums", "time": "2011-06-10 19:53:12 -0600", "post_content": [" ", " ", "Imagine a scenario where there are 2 nodes subscribing to the same topic. And that, node B subscribes to the topic a few minutes after node A. Now, when node B starts subscribing to the topic, does it start receiving all the earlier messages that were consumed by 'A' since it started earlier or does it only receive the messages that are available on the topic from that moment in time. ", "And what is the size of any given topic? Does it hold messages forever or is there some kind of limit as to how many messages a topic can hold at any point in time.", "I apologize if my understanding of ROS is severely lacking, but I am still learning.", "Thank you."], "answer": [" ", " ", "Node B will only receive messages that are published after it has connected (with the execption of latching).", "There is a queue size for messages, that you can define, but it is not for a topic in general, but for the publisher/subscriber.", "These things come clear, when you look at what a topic is. Messages are not sent to a physical topic (i.e. some program) and stored there, where subscribers receive them from.\nA topic is merely a common name to publish/subscribe to, but nothing more. Each message stream is handled out between the publisher and subscriber directly, they just use the topic name to \"find each other\"."], "url": "https://answers.ros.org/question/10229/topic-tantrums/"},
{"title": "Where is ROSJava code?", "time": "2011-09-29 00:03:06 -0600", "post_content": [" ", " ", " ", " ", "Could someone please point me to the location where I can download the ROSJava packages to use for Java development in Eclipse?\nCould you also explain exactly what I need to download/build to be able to use it?\nUnbelievable how difficult and time consuming it is to obtain and use this software. Instructions on getting this set up is pitiful!"], "answer": [" ", " ", "Get a local copy of the rosjava repository with this command:", " ", " ", "Eventually found answer - Installed Mercurial. (Would have been nice to find this in some guide)."], "answer_code": ["hg clone https://code.google.com/p/rosjava/\n"], "url": "https://answers.ros.org/question/11360/where-is-rosjava-code/"},
{"title": "Where is C++ node implementation code?", "time": "2011-11-25 15:21:44 -0600", "post_content": [" ", " ", "Hi, I want to initialize a C# node.", "And I want to consult the implementation of C++ node, I found several files in /opt/ros/electric/stacks/ros_comm/clients/cpp/roscpp/src/libros may be useful.But I am not sure if these files are the right ones I am looking for.", "Thanks~"], "answer": [" ", " ", "You're really talking about ", " for C#. That page has a number of links to various documentation on the design of client libraries."], "url": "https://answers.ros.org/question/12097/where-is-c-node-implementation-code/"},
{"title": "turtlebot_gazebo demo questions", "time": "2012-02-22 08:06:58 -0600", "post_content": [" ", " ", " ", " ", "I have a few questions regarding the turtlebot_gazebo demo. ", "1  > Does the physics simulator gazebo limit the frame rate at 25 fps?", "2  > What happens when a node publishes messages to a topic faster than they are being \n     consumed? Do the messages wait in a queue to be consumed?", "3  > And is it possible to have a computation graph where nodes are written using \n     different client libraries?", "4  > Could you also please tell what is the meaning of the term RMS Error in gazebo?", "5  > How can a node consume and produce messages to the same topic? Is it some kind of closed feedback loop control for that particular node?", "Thanks and regards.", "Shouldn't this be five questions?", "I agree! At least put the common topic of all questions in the title, e.g. \"turtlebot_gazebo demo details\".", " Thanks for pointing that out. Title changed )"], "answer": [" ", " ", "1) The \"frame rate\" of Gazebo (the rate at which it's drawn to screen) is based on your computer's hardware. Gazebo runs two threads, a physics thread and a render thread. The physics thread's update rate can be set in the ", " file. ", "2) ROS queues all incoming messages until nodes are able to service those callbacks. This queue length is specified in the constructor of a subscriber. ", "3) If by \"different client libraries\" you mean, \"can you use C++ and Python for different nodes in the same graph?\", then the answer is yes. You may be some nodes in C++ and some in Python with no problem at all. ", "4) I will omit this for someone more with more Gazebo knowledge. ", "5) It is not recommended to have a node subscribe to and publish to the same topic. You should have two different topics for this. Since you cannot know which node publishes a certain message on a topic, you could cause a lot of confusion doing this. ", " ", " ", "4) RMS error displayed in gazebo (electric) is the root-mean-squared value of the change in Lagrange multipliers (\u0394\u03bb) as described ", ".  It's an approximate indicator of how much the constraint satisfaction step has converged under the quickstep algorithm.", "Im sorry but that just flew over my head. In any case, is this something that I have to be consciously  aware while using ROS and gazebo?"], "answer_code": [".world"], "url": "https://answers.ros.org/question/28208/turtlebot_gazebo-demo-questions/"},
{"title": "android_camera_tutorial consuming high memory (raw_image) slow image refresh", "time": "2012-04-06 17:23:20 -0600", "post_content": [" ", " ", "Hello, The android_camera tutorial memory consumption is too high. I just upgraded to new version of rosjava_android. I am using one mobile device as RosCameraPreview node which runs roscore and transmits images to another android device (motorola xoom) which subscribes to the message. Both seem to consume high memory and I can see GC constantly running. The frame changes approximately 30 seconds on the ImageTransport side \nI rarely see the message onFlyCompress. I think this version publishes /camera/raw_image/compressed, that may be the issue, how can I subscribe to /camera/image/compressed ? ", "The previous version I had was working very well. I recently upgraded to new version. Any ideas to improve the performance ? \nRegards\nGOPI", "06 23:10:27.667: D/dalvikvm(24445): GC_CONCURRENT freed 1226K, 53% free 3950K/8263K, external 2486K/2773K, paused 2ms+2ms\n04-06 23:10:27.977: D/dalvikvm(24445): GC_CONCURRENT freed 1226K, 53% free 3950K/8263K, external 2486K/2773K, paused 2ms+5ms\n04-06 23:05:12.377: D/dalvikvm(24445): GC_CONCURRENT freed 1226K, 49% free 3951K/7623K, ", "external 2486K/2773K, paused 3ms+2ms"], "answer": [" ", " ", "See ", "I have the same problem but with image_transport. I'm sorry but can't see any solution in your post, damonkohler", "EDIT: I found solution here: http://code.google.com/p/rosjava/issues/detail?id=141"], "url": "https://answers.ros.org/question/31405/android_camera_tutorial-consuming-high-memory-raw_image-slow-image-refresh/"},
{"title": "ROS offline installation", "time": "2012-05-07 11:11:52 -0600", "post_content": [" ", " ", "Hi there,", "I know that this question is already been asked but this threat didn't get me to the solution.\nI'm trying to install the ROS-Base version on an ASCTEC Pelican. Unfortunately, is there no way to connect the Pelican to the internet. I can checkout the ", " repository but how do I get the other Packages onto the machine. Would be great to get some help.", "Thanks & Cheers", "Please save everyone's time by providing links to related questions you already consulted.", "Also please be a little bit more specific about what \"other\" pachages you want to install.  And what architecture/platform you are targeting.", "I would like to use this solution ", "    which seemed pretty convenient at first but this is where the funny part starts... How do i get the apt-offline package on the non-network machine?"], "answer": [" ", " ", "Just go to ", "/ and search for apt-offline. There, you can download the Debian package for your architecture.", "I did that and its running on the non-network machine (to be precise the asctec pelican). But how do I set up the softwaresource ( sudo sh -c 'echo \"deb ", " maverick main\" > /etc/apt/sources.list.d/ros-latest.list' ) without an internet connection?", " ", " ", " ", " ", "This is the download-script which is generated from apt-offline on the Asctec Pelican. ", "Weird. Are you sure your internet connection on your online machine works? When I run ", " (with your list pasted into apt-offline.sig), this is what I get: ", " (don't worry about all the errors on Translation files, all essential files are okay)", "I tried a different router and it worked. Thank you for your help. "], "answer_code": ["'http://packages.ros.org/ros/ubuntu/dists/maverick/main/binary-i386/Packages.bz2' packages.ros.org_ros_ubuntu_dists_maverick_main_binary-i386_Packages 0 :\n'http://packages.ros.org/ros/ubuntu/dists/maverick/Release' packages.ros.org_ros_ubuntu_dists_maverick_Release 0 \n'http://packages.ros.org/ros/ubuntu/dists/maverick/Release.gpg' packages.ros.org_ros_ubuntu_dists_maverick_Release.gpg 0 \n'http://packages.ros.org/ros/ubuntu/dists/maverick/main/i18n/Translation-en.bz2' packages.ros.org_ros_ubuntu_dists_maverick_main_i18n_Translation-en 0 :\n'http://extras.ubuntu.com/ubuntu/dists/maverick/main/source/Sources.bz2' extras.ubuntu.com_ubuntu_dists_maverick_main_source_Sources 0 :\n'http://extras.ubuntu.com/ubuntu/dists/maverick/main/binary-i386/Packages.bz2' extras.ubuntu.com_ubuntu_dists_maverick_main_binary-i386_Packages 0 :\n'http://extras.ubuntu.com/ubuntu/dists/maverick/Release' extras.ubuntu.com_ubuntu_dists_maverick_Release 0 \n'http://extras.ubuntu.com/ubuntu/dists/maverick/Release.gpg' extras.ubuntu.com_ubuntu_dists_maverick_Release.gpg 0 \n'http://extras.ubuntu.com/ubuntu/dists/maverick/main/i18n/Translation-en.bz2' extras.ubuntu.com_ubuntu_dists_maverick_main_i18n_Translation-en 0 :\n'http://archive.canonical.com/ubuntu/dists/maverick/partner/binary-i386/Packages.bz2' archive.canonical.com_ubuntu_dists_maverick_partner_binary-i386_Packages 0 :\n'http://archive.canonical.com/ubuntu/dists/maverick/Release' archive.canonical.com_ubuntu_dists_maverick_Release 0 \n'http://archive.canonical.com/ubuntu/dists/maverick/Release.gpg' archive.canonical.com_ubuntu_dists_maverick_Release.gpg 0 \n'http://archive.canonical.com/ubuntu/dists/maverick/partner/i18n/Translation-en.bz2' archive.canonical.com_ubuntu_dists_maverick_partner_i18n_Translation-en 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/main/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_main_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/restricted/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_restricted_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/universe/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_universe_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/multiverse/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_multiverse_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/main/binary-i386/Packages.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_main_binary-i386_Packages 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/restricted/binary-i386/Packages.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_restricted_binary-i386_Packages 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/universe/binary-i386/Packages.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_universe_binary-i386_Packages 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/multiverse/binary-i386/Packages.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_multiverse_binary-i386_Packages 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/Release' ubuntu.media.mit.edu_ubuntu_dists_maverick_Release 0 \n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/Release.gpg' ubuntu.media.mit.edu_ubuntu_dists_maverick_Release.gpg 0 \n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/main/i18n/Translation-en.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_main_i18n_Translation-en 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/multiverse/i18n/Translation-en.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_multiverse_i18n_Translation-en 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/restricted/i18n/Translation-en.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_restricted_i18n_Translation-en 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick/universe/i18n/Translation-en.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick_universe_i18n_Translation-en 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick-updates/main/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick-updates_main_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick-updates/restricted/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick-updates_restricted_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick-updates/universe/source/Sources.bz2' ubuntu.media.mit.edu_ubuntu_dists_maverick-updates_universe_source_Sources 0 :\n'http://ubuntu.media.mit.edu/ubuntu/dists/maverick-updates/multiverse/source/Sources.bz2' ubuntu.media.mit ...", "apt-offline get apt-offline.sig"], "url": "https://answers.ros.org/question/33469/ros-offline-installation/"},
{"title": "RVIZ will not execute", "time": "2012-06-03 10:29:52 -0600", "post_content": [" ", " ", "I am trying to run a LiDar taken from the Neato XV-11 vacuum cleaner on ROS nad amd using RVIZ to visualize the data coming from the sensor. However, I ran into the problem that RVIZ was not receiving and MAP data to display. I consulted the site and As per the video tutorial under map, I found that I did not have any topics being displayed. I tried to enter the topic manually but not RVIZ will not initialize and I am receiving the following error.", "Kindly suggest what when wrong and what can be done regarding the same."], "answer": [" ", " ", "Seems to me you placed a space in ", " which is not valid. ", "Remove that space in ", " or ", " and start rviz.", "Previously:", " ", " ", "Thanx, but where do I change it?\nIt occured when i tried to manually change the topics in RVIZ, now even the RVIZ windo won't open. I tired looking in the config file in my rviz folder but could not find where the change was made by RVIZ. Kindly help.", "Open both mentioned files and search for your buggy string \"map (nav_msgs/OccupancyGrid)\" or parts of it. And use comments on answers :)", "Thanx. But I cannot open the file. I don't know where it is and I am having a hard time finding it. I tried to open the file through the cmd window but it keeps telling me a bunch of errors. I tried the open, edit and even the start command. None of them open the file that I want. Any suggestion?", "Umm.. the ", " in linux paths is used to represent the users home directory. type ", " and then ", " -- ", " the directory for the new rviz in fuerte is ", "Im sorry, I should have mentioned this earlier. \nI am working on the electric version of the ros and am currently working on ubantu.", "I found a file named GetMap.lisp", "but on top of the life the first line says auto generated and do not edit!"], "url": "https://answers.ros.org/question/35554/rviz-will-not-execute/"},
{"title": "How to align the joints of a manipulator with high accuracy on ROS", "time": "2012-06-30 22:29:01 -0600", "post_content": [" ", " ", " ", " ", "I like to know how we can align the joints of a robotic arm. Currently we can only do trial and error for correcting the origins of joints .Its a very time consuming task and cant use for complex robotic system. If ROS has a plugin or package to export the 3D models designed in popular softwares such as SolidWorks,Autocad,Blender,FreeCAD, it will be great tool for robotic model designers. Is any package existing for converting this 3D models into URDF or xacro? And which is the best tool for modelling Robots  in  Opensource community?  I think ROS developers have to give more importance to modelling tools."], "answer": [" ", " ", "This tool was developed to convert CAD models to URDF models semi-automatically. It makes use of the XML files exported by the SimMechanics Link . Mathworks, makers of SimMechanics, have developed plugins for a couple of leading CAD programs, including SolidWorks, ProEngineer and Inventor.", " ", "hope this give you a hint ", " ", " ", "Hi Cagatay\n       Thanks for your answer. But is it possible to export directly from solid-works or auto-cad model into URDF?  othervice we need to purchase simechanics. Also i was searching for some free CAD tools that support robot modelling and can export to URDF. I found Free-CAD will support robot modelling. I think its necessary to have a opensource model tool along with ROS. It will be really great.", "you may use blender it is a free modeling tool", "Not sure if this is what you looking for. It's not automatic, but it has all the steps. "], "url": "https://answers.ros.org/question/37588/how-to-align-the-joints-of-a-manipulator-with-high-accuracy-on-ros/"},
{"title": "How does ROS handle messages between 3 or more nodes across 2 machines?", "time": "2012-07-04 22:12:01 -0600", "post_content": [" ", " ", "Hey,", "I am working on a robot that creates a node on one computer to capture and transmit (through ros messages) an image (Node 1). The image is sent to two other nodes. One node is on a separate computer (connected via wi-fi) which also runs the rosmaster (Node 2). The third node is on the same computer as the image capture node (Node 3).", "My question is: How is the image message transmitted to each node. Does the image get sent to the computer with the rosmaster and redistributed to the listening nodes (Node 1->Master->Node 3 and Node 2)? Or is the messaging system more dynamic in that messages go directly to listening nodes(Node 1-> Node 2 and Node 1->Node 3)? The reason this matters is because of the delay involved in wireless transmission to the rosmaster and then back to the transmitting computer to be consumed by node 3. Also the processing overhead of sending and receiving extra messages is undesirable.", "Thank you,", "Tim "], "answer": [" ", " ", " ", " ", "In short: the master can be thought of as a coordinator; it does not route all traffic between nodes through itself, but helps nodes (at startup, and during execution) to find each other. Nodes then communicate directly, via either TCP or UDP (this can even be configured per subscriber).", "From the ", " and ", " wiki pages.", "The role of the Master is to enable individual ROS nodes to locate one another. Once these nodes have located each other they communicate with each other peer-to-peer.", "In your case, images will go directly from the publisher to the subscriber, after the master has provided them with enough information to be able to find each other.", " ", " ", "Messages are always transmitted directly to the subscribers. When subscribing to a topic, a socket is opened between each of the publishers of a topic and the subscriber. The core is only used as sort of a name service to find out which nodes are publishing topics, who is providing a service and it stores ROS parameters. For more information, have a look ", " and in ", "."], "url": "https://answers.ros.org/question/37965/how-does-ros-handle-messages-between-3-or-more-nodes-across-2-machines/"},
{"title": "Raspberry Pi optimization [closed]", "time": "2012-08-19 10:24:24 -0600", "post_content": [" ", " ", " ", " ", "Trying to get ROS working on Raspberry Pi.  Encountered problem here: ", "/", "Rather than leave well enough alone I found ", "typedef qreal\nTypedef for double on all platforms except for those using CPUs with ARM architectures. On ARM-based platforms, qreal is a typedef for float for performance reasons.", "That got me wondering how to optimize ROS for ARM platforms in general.  Is the solution mentioned in the first link the best approach for this specific type of problem?  Is there an automatic way to replace doubles with floats in areas where the extra precision is not needed, but the extra performance is; based on the target platform?  Would it be OK to define floats and doubles both equal to the same length for ARM targets?", "I'm a noob with ROS and Rpi and there will soon be huge numbers of us looking to combine the two in the most braindead fashions possible.  The sooner ARM optimizations get pushed upstream the less pain everyone will feel when the wave of new users breaks.", "I'm curious about this as well, especially with using a Raspberry Pi with the Turtlebot to save on power consumption and simplify charging."], "answer": [], "url": "https://answers.ros.org/question/41815/raspberry-pi-optimization/"},
{"title": "tf tree becomes randomly disconnected", "time": "2012-10-12 05:14:49 -0600", "post_content": [" ", " ", "I've noticed when running a bunch of nodes that random transforms in my tf tree seem to disappear. I have two instances of openni.launch running, along with a bunch of other fairly CPU intensive stuff running. What happens is that once I have everything running, one or more of the transforms coming from a static_transform_publisher disappears. If I kill some of the nodes, those transforms will come back. It seems clear that this is related to CPU usage (I only see it when all cores of my i7 2600K are at 90%+ utilization). ", "I'm of course trying to get my CPU usage down, but is there anything else I should try? Would running the static_transform_publishers with higher scheduler priority make a difference? They don't seem to be consuming much CPU as it is.", "What's the publish rate of your static publishers?", "It's about 10Hz", "Interesting... Do you think the trees get disconnected in the listener or because the transforms are not sent? Maybe the problem is that you might have quite a lot of sockets, one between each client and each sender. I would try to send all transforms with one publisher, maybe it improves things.", "The publishers are at 10Hz, but what does \"", "\" give?", ", interesting idea. It's hard to tell exactly what's going on without putting debug statements in the publishers. It wouldn't be too hard automate that with a tf publisher supernode that looks for static_transform_publishers, broadcasts their transforms and kills the individual publishers.", " I'm getting 90-100Hz from tf.", "There are actually already multi-static publishers that have been written. I put this one together a few months ago: ", " and I remember finding another one somewhere else after that. This is also something that should be fixed with tf2.", " I think ", " implemented something more awesome a few months ago, a static transform publisher that can publish more than one transform. I hope he can provide more infos..."], "answer": [" ", " ", " your multistatic publisher seems to be doing the trick. I'm also using your version of kinect_frames.launch (in my own fork of openni.launch so I can use it with 1.8.3)."], "question_code": ["rostopic hz /tf"], "url": "https://answers.ros.org/question/45720/tf-tree-becomes-randomly-disconnected/"},
{"title": "erratic simulator gazebo does not work anymore", "time": "2012-10-15 01:54:05 -0600", "post_content": [" ", " ", " ", " ", "I tried erratic simulation some months ago and it worked, but now it does not work anymore. If I launch \"roslaunch erratic_description erratic_empty_world.launch\", gazebo gui starts without the robot and it seems blocked on:", "waiting for service /gazebo/spawn_urdf_model", "The gzserver process consumes a lot of CPU (100%), but I do not know if it is related.\nI tried also on another computer with the same results.\nI have Ubuntu 12.04 64bit and fuerte.\nDoes someone have an advice to solve the problem?\nThanks.", "Update. I did another test. The ", " file is:", "</launch>", "I tried to run each node separately with 4 commands: ", ", ", ", ", ", ", ".\nand it worked. The general ", " file should do the same, but it does not work. Any suggestions?"], "answer": [" ", " ", "Just change the node name from node \"empty_world_server\" to \"gazebo\" and it should be able to find the /gazebo/spawn_urdf_model service and the rest of them.", "Thanks now it works!! But why some months ago did it work? Is it changed? I thought that \"name\" was a customizable field.", " ", " ", " ", " ", "This sounds like you try it on fuerte (and used it on electric the last time you tried). The Gazebo server (simulation) and client (visualization) have been split into two independent processes starting with fuerte, which means that old launch files might start up without a Gazebo GUI.\nStart the launch file and type", "afterwards. The Gazebo GUI should come up then.", "/edit: As mentioned below in the comments, the above wasn't the solution. Just tried it myself, and the robot indeed doesn't spawn with the spawner waiting for the spawn model service to appear. It works with the laser equipped erratic, however:", "or:", "For the latter, you have to start the gui manually as it appears it was forgotten in the launch file.", "Not sure why it doesn't work with the launch file in the original question, I suspect either\na) a subtle difference in launch files that I didn't catch or\nb) a timing problem during gazebo startup which leads to the spawn model service not getting called.", "/edit: As apparent from answer by arebgun, it was a) ;)", "Also the last time I used fuerte. The gazebo gui node is already into the launch file, in fact Gazebo GUI starts and it shows correctly the empty world, but not the robot.", "Sorry, apparently misread a part of your posting. Is there any output on the gazebo terminal? Can you spawn other models? (Plz comment here or edit your original post)"], "question_code": ["<launch>\n<param name=\"/use_sim_time\" value=\"true\" />\n\n<node name=\"empty_world_server\"\n      pkg=\"gazebo\"\n      type=\"gazebo\"\n      args=\"-u $(find gazebo_worlds)/worlds/empty.world\"\n      respawn=\"false\"\n      output=\"screen\"/>\n\n<node name=\"gazebo_gui\" pkg=\"gazebo\" type=\"gui\" respawn=\"false\" output=\"screen\"/>\n\n<include file=\"$(find erratic_description)/launch/erratic_mobile_base.launch\" />\n"], "answer_code": ["rosrun gazebo gui\n", "roslaunch erratic_description erratic_laser_wg_world.launch\n", "roslaunch erratic_description erratic_laser_empty_world.launch\n"], "url": "https://answers.ros.org/question/45883/erratic-simulator-gazebo-does-not-work-anymore/"},
{"title": "rosbag play  speed [closed]", "time": "2012-10-22 05:43:37 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi All \nI am using rosbag play --clock myrecod.bag to play file and subscribe these three topics:\n\"/camera/rgb/image_color\", \"/camera/depth/image\", \"/camera/depth/camera_info\"\nand I want to process each frame of these topics by fovis (", "/).my recording file is 20 fps and I have time-consuming calculation on each frame of data. There for I only able to subscribe 4 fps of my recordings.   ", "How can I slow down the play back speed of my recordings file  in order to get all frames?", "Thanks in Advance \nRamin "], "answer": [" ", " ", "Use the ", " or ", " parameter of ", ".", "Thank you Thomas \nI set rate value to 0.1 and now its working well and I am able to capture all the frames."], "answer_code": ["-r", "--rate"], "url": "https://answers.ros.org/question/46472/rosbag-play-speed/"},
{"title": "What's the most highest ampere when PR2 comsume the most when charging?", "time": "2012-08-07 00:00:07 -0600", "post_content": [" ", " ", " ", " ", "How to calculus ampere which PR2 need the highest?", "What action will cause PR2 consume the most power?", "Thank you~"], "answer": [" ", " ", "You can find some information here in paragraph 1.2.4.\n", " \nAnd here the current per joint:\n"], "url": "https://answers.ros.org/question/40734/whats-the-most-highest-ampere-when-pr2-comsume-the-most-when-charging/"},
{"title": "Cannot find my own packages", "time": "2012-11-16 05:48:59 -0600", "post_content": [" ", " ", "Dear ROS users,", "I don't have any experience with ROS but I want to learn to use it.\nI just installed it in Ubuntu 12.04 (fuerte), follow some steps in the tutorials, but I got stuck.", "In \"Creating a ROS Package (rosbuild)\" it says to create a new package:\n", "and check if ROS can find it.\n", "It can't. \nAnd I don't understand this:\n\"If this fails, it means ROS can't find your new package, which may be an issue with your ROS_PACKAGE_PATH. Please consult the installation instructions for setup from SVN or from binaries, depending how you installed ROS. If you've created or added a package that's outside of the existing package paths, you will need to amend your ROS_PACKAGE_PATH environment variable to include that new location. Try re-sourcing your setup.sh in your fuerte_workspace.\"", "First I didn't have a ROS_WORKSPACE. So I made some research and used \"export\" to do it. Now ", " is my workspace.\nAnd it's where I created my new package (not in ", " as it's said).", "Second, if it's an issue with ROS_PACKAGE_PATH, of course I created a package that's outside of the existing package paths, and ", " is out of it! Then how do I amend it?", "My Ubuntu is in a virtual machine. I deleted and created a new one to install ROS again and it is still the same. So, I concluded that this tutorial steps are not coherent with my system.", "Could anyone help me, please?\nNat0ne"], "answer": [" ", " ", "First off, I would suggest to set your workspace in somewhere ", ", and not in ", ". It is preferable to work in a directory where you only need standard user permissions.", "Here is an example how to set your workspace to ", ", and also make sure the workspace is part of the package path. I have this appended at the end of my ", " file.", "If you now create your packages in ", ", everything should work fine.", "Manually managing ROS_PACKAGE_PATH is not recommended. The best way is to use rosws. The relevant wiki page can be found ", ". ", " ", " ", "Thank you for your answer Ivan.", "I am sorry, last message I said I have the workspace in ", ", but I did not mention I had it in ", ", previously.", "Anyway, now I did like you say and I have:\nworkspace -> ", "\npackage path -> ", "I and it is the same problem persists.", "Am I doing something wrong?\nThank you in advance.", "Please read the ", ". Do ", " create answers for comments or discussion. This is not a forum. Instead, ", " your original post or use the comment functionality."], "answer_code": ["~/", "/opt", "~/ros", "~/.bashrc", "ROS_WORKSPACE=$HOME/ros\nexport ROS_WORKSPACE \n\nROS_PACKAGE_PATH=$ROS_WORKSPACE:$ROS_PACKAGE_PATH\nexport ROS_PACKAGE_PATH\n", "~/ros"], "url": "https://answers.ros.org/question/48484/cannot-find-my-own-packages/"},
{"title": "laser_scan_matcher crash", "time": "2012-11-07 06:22:59 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nWhen I use laser_scan_matcher everything works well for about 5-10 minutes, then the module crashes with the following output: ", "I searched for the log file in the suggested path, but I was not able to find it. Do you have any other suggestion to understand what's going on?", "Additional info:\nI don't know if it is related, but when I start the laser_scan_matcher I get the following warnings (but the module works correctly)", "Additional info about my system:\nI am using Ubuntu 12.04 with ROS distro fuerte. For the installation of laser_scan_matcher I followed the instructions reported here: ", "\nCurrently I'm using the following parameters:", "Could you give a little more information: ros version, os version, etc. Also, the which version of the scan matcher are you using? ", "Is the problem reproducible? Have you tried running laser_scan_matcher_node in a debugger (such as gdb) to see exactly where it dies?", "I got the following output:", "maybe a memory leak?", "confirmed memory leak (I see the consumed ram constantly increasing, >1Gb in few minutes)", "Any updates from the developers about this issue? Should I submit a ticket on github? Thank you"], "answer": [" ", " ", "Confirmed memory leak (introduced in a recent commit). Just fixed it."], "question_code": ["[laser_scan_matcher_node-1] process has died [pid 6944, exit code -11, cmd /opt/ros/fuerte/stacks/scan_tools/laser_scan_matcher/bin/laser_scan_matcher_node scan:=/my_robot_bridge/laser_out odom:=/my_robot_bridge/odometry_out __name:=laser_scan_matcher_node __log:=/home/user/.ros/log/57fa5eac-28cd-11e2-b8c6-00221522c9d6/laser_scan_matcher_node-1.log].\nlog file: /home/user/.ros/log/57fa5eac-28cd-11e2-b8c6-00221522c9d6/laser_scan_matcher_node-1*.log\n", "core service [/rosout] found\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[laser_scan_matcher_node-1]: started with pid [6944]\n[ INFO] [1352310705.912643992]: Starting LaserScanMatcher\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\n", "<node pkg=\"laser_scan_matcher\" type=\"laser_scan_matcher_node\" \nname=\"laser_scan_matcher_node\" output=\"screen\" args=\"scan:=/my_robot_bridge/laser_out\">\n<param name=\"fixed_frame\" value = \"odom\"/>\n<param name=\"base_frame\" value=\"base_link\"/>\n<param name=\"use_alpha_beta\" value=\"true\"/>\n<param name=\"publish_tf\" value=\"true\"/>\n<param name=\"max_iterations\" value=\"10\"/>\n<param name=\"use_imu\" value=\"false\"/>\n<param name=\"use_odom\" value=\"false\"/>\n</node>\n", "Program received signal SIGSEGV, Segmentation fault.\n0xb7a2ee6d in alloc_int_array (n=1080, def=-1)\nat /opt/ros/fuerte/stacks/scan_tools/csm/build/csm-git/sm/csm/laser_data.c:29\n29                      v[i] = def;\n"], "url": "https://answers.ros.org/question/47723/laser_scan_matcher-crash/"},
{"title": "trajectory filter server always fail for 10 DOF robot [closed]", "time": "2012-12-18 13:51:41 -0600", "post_content": [" ", " ", " ", " ", "dear all, i'm doing arm navigation for a 7DOF robot arm plus a 3DOF base. For the planning group containing only the 7DOF arm, both ompl and trajectory filter server work well. For the planning group containing the whole 10DOF, only the ompl works. the trajectory filter server always fails with error std::bad_alloc. It will also  consume 100% CPU and make the whole computer hault for a while. when i check this in detail, i find that for the 10DOF case, when the input trajectory contains less than 8 points, the trajectory filter server works. when the points number exceeds 10, it always fail. has anybody tried the filter server for high DOF robots?", "p.s. for the 7DOF case, even the input trajectory contains more than 100 points, the filter server works well. so i think my computer is not a problem (intel i5 CPU, 4G memory)", "i'm using Fuerte on Ubuntu12.04.", "For the trajectory filter server, i'm using unnormalize_trajectory+cubic_spline_short_cutter_smoother"], "answer": [], "url": "https://answers.ros.org/question/50753/trajectory-filter-server-always-fail-for-10-dof-robot/"},
{"title": "ROS on gumstix for on-board processing", "time": "2012-11-12 20:36:12 -0600", "post_content": [" ", " ", "I wonder how good a gumstix can run a ROS-based application using \"gumros\"(for instance). For example this ", " mentions that it has 512 MB of memory. I would like to know if this one is sufficient using gumros to build an application like doing pose estimation using openCV or similar. ", "What I see, one of my packages consumes about 100 MB on my laptop. But with gumstix, it still needs to install OS and install ROS etc, will it be capable? Any experience having this running in (almost)real-time or on-board(at least)?", "Thanks in advance."], "answer": [" ", " ", " ", " ", "http://www.willowgarage.com/blog/2012/12/11/exploring-ros-turtlecore", "It's been done.\nAlso, disk space shouldn't be an issue if you have an SD card.", " ", " ", "I spent some time a year and half ago working on a vision system using an Overo. I was using a pretty outdated ROS release (I think it was cturtle), but it was able to capture images using OpenCV API and transfer the files over wifi to a ground node. All that was done while ROS was doing flight control of the aerial vehicle, which was not small operation by any means. Strictly speaking, it wasn't real time control. When the system is loaded and the air space was busy with other wireless devices, I experienced ~300ms delay."], "url": "https://answers.ros.org/question/48173/ros-on-gumstix-for-on-board-processing/"},
{"title": "groovy source install is strange", "time": "2013-02-02 21:41:39 -0600", "post_content": [" ", " ", "All,\nI installed ROS at least 4 times from source on different devices over the last month. I still do not really understand the structure behind this procedure. Let's say I want to build the desktop package from source. I would have to initialize wstool using ", "\nThen I call rosdep to have all dependencies installed and call catkin_make after that. What I do not see is why OpenCV2 and PCL are downloaded into the src folder and compiled using catkin_make and are not installed with rosdep using prebuilt binaries. Especially when you build on embedded devices, those huge libraries consume a lot of time and space to compile. If there is a reason, please let me know", "Kai"], "answer": [" ", " ", "The source based installations are designed to support people running on platforms without prebuilt binaries available or who do not want to or cannot use the prebuilt binaries. ", "For example your use case on most embedded devices prebuilt binaries do not exist.  ", "If you don't want to build something because you have resolved the dependency externally, you can just remove it from your workspace.  You will have to make sure that the external installation of the package is in your CMAKE_PREFIX_PATH.", "that is what my assumption was from the beginning. I got only confused because on MacOS rosdep also installs dependencies from source. I really like that approach because it will result in only ROS packages to be placed in the src folder. Couldn't PCL and OpenCV be installed the same way?"], "url": "https://answers.ros.org/question/54089/groovy-source-install-is-strange/"},
{"title": "AMCL works intermitently?", "time": "2013-03-13 12:40:47 -0600", "post_content": [" ", " ", " ", " ", "So I've been trying to get AMCL to work on my ad-hoc network using the amcl_demo.launch. I've tried and can successfully run AMCL every time I attempt it with a wired connection. With the ad-hoc network though, it fails a majority of the time resulting in", "At first I thought this might be a bandwidth issue. But when I looked at the consumption it was ~1.5Mb/s for the peek. And when AMCL has successfully run on the ad-hoc network, there didn't seem to be much issues with sending it navigation goals and controlling it via teleoperation keyboard. I do remember there being lots of warnings and errors with the successful runs, but because it doesn't work that often, I can't replicate their messages to report them. What is possibly going wrong with my network or ROS, such that amcl_demo.launch works only on a rare occasion?", "Upon further investigation with tf_monitor, I've noticed that I get a large delay in /base_footprint to /odom until the map is received. The delay is on the order of 20s. I also noticed that there are delays in the nodes /robot_pose_ekf and /robot_state_publisher that are more consistent across the runtime that are around 5s and 1s, respectively.", "I also tried to pay attention to the CPU usage. At the beginning of running launch file, the CPU usage is around 90%, but once all the processes have begun, it tappers off.", "When I run gmapping_demo.launch, I don't have nearly as many problems.", "UPDATE: Just in retesting with a LAN connection, I've come across the same problem. It just doesn't occur that often. ", "Side Note: The TurtleBot does have an odd behavior not being able to move forward smoothly, whereas all other directions can move smoothly."], "answer": [" ", " ", "After talking to Bill Morris from iheartrobotics, I changed my setup a bit. He suggested that it may be a problem with node priorities or nodelet managers miscommunicating since I had roscore running on a different computer than the TurtleBot. Instead of having a master node that runs ROS, I made it so the TurtleBot laptop was running ROS itself. With that, it has consistently run without the costmap errors. For the most part it runs smoother with commands. Now rviz and viewing the map in real time is slow, but that's alright.", " ", " ", " ", " ", "Possible causes: ", "(1) if the ad-hoc network automatically obtains IP address, it may get another IP address which is different from the ROS_MASTER_URI that you previously set; ", "(2) perhaps the wireless signal is weak or unstable, so you need to place a wifi antenna nearby;", "(3) perhaps the slam map is not properly saved, so check the path of the map file.", "(1) is not the issue, cause the IP addresses are static and set by me. (3) doesn't seem that likely since I can use the same map with a wired connection. For (2), the wireless does seem to be stable enough for me to do nearly everything else. I've even been able to work with rviz for other tasks.", "(a) Are you using TurtleBot or what robot? (b) When you bringup the camera do you also need to try like 10 times before success (other times would be No devices detected... waiting for connection)? (c) gmapping_demo runs normally with wireless, just acml does not work?", "I am using a TurtleBot. Bringing up the camera with 3dsensors.launch goes smoothly. And both gmapping and amcl bring up the camera smoothly as well. amcl is the only one that works intermittently.", "I see... hmmm... I don't know..."], "question_code": ["[  INFO] [#]: Subscribed to Topics: scan\n[  INFO] [#]: Requesting the map...\n\n[  INFO] [#]: Still waiting on map...\n\n[  INFO] [#]: Still waiting on map...\n\n[  WARN] [#]: You have set map parameters, but also requested to use the static map. Your parameters will be overwritten by those given by the map server\n[  INFO] [#]: Received a 544 x 512 map at 0.050000 m/pix\n\n[  WARN] [#]: Waiting on transform from /base_link to /map to become available before running costmap, tf error:\n[  WARN] [#]: Waiting on transform from /base_link to /map to become available before running costmap, tf error:\n[  WARN] [#]: Waiting on transform from /base_link to /map to become available before running costmap, tf error:\n"], "url": "https://answers.ros.org/question/58054/amcl-works-intermitently/"},
{"title": "Showing data obtained from images over the image", "time": "2013-05-09 07:02:17 -0600", "post_content": [" ", " ", "Hi!", "Consider this situation: I have a camera node publishing images, and a node that processes the image to get some information (e.g., features). I want to show the processed information over the original image.", "For this, a simple solution is to have an image publisher that assembles publishes a debug image when it has any subscribers.", "The problem arrives when I want to debug a third node that further processes the information (e.g. to identify groups of features, or tracking). This node subscribes only to the processed information, it does not receive an image. If I change it to receive images, it will be unsynchronized.", "Is there a good and simple way to show information over the original image in this third node?", "I know of two ways to do this but neither satisfies me:", "Having the second node publish two topics: One simply containing the processed information as before, and another with a custom message containing both the processed information and the original image. If the third node has any subscribers in the debug topic, it will subscribe to the second topic instead. This way it has the right image to use for debug.", "Having yet another node consuming images and information, synchronizing it and displaying. This is quite an overkill and I have to change the information message to contain the original seq number or timestamp.", "Thanks for any suggestions!"], "answer": [" ", " ", " provide a good way to aggregate information. You can conveniently combine the markers with the original image display. "], "url": "https://answers.ros.org/question/62460/showing-data-obtained-from-images-over-the-image/"},
{"title": "win_ros Quaternion conversions", "time": "2013-05-09 09:51:27 -0600", "post_content": [" ", " ", " ", " ", "Hi, i'm fairly new to ROS and i got stuck on the following problem. I have a setup where i have a node running on Ubuntu that controls a robot and reports its position. The position is encoded into Quaternion as follows:", "Now, i want to consume that information on windows. I've set it all up and everything works meaning that i receive the robot's position and all but win_ros doesn't seem to have tr:: package so i can't extract the 'theta' angle. Can anyone confirm that tr:: has not been ported to windows? And if so what's the proper way of extracting robot's orientation from Quaternion?", "Edit:", "I've done the conversion myself a while back but forgot to post it here. Even though it's pretty straight forward here is the code that works well for me (i just took it straight out of ", " code and massaged it a little).", "That's useful to have floating around for people, thanks Eugene."], "answer": [" ", " ", "According to the wiki, ", " and ", " packages are part of the ", " stack.  I don't know if the ", " wiki is up-to-date, but its ", " list does not include the ", " stack.", "As you've probably already seen, the \"normal\" way in ROS is to use the ", " functions:", "But, if you can't build the ", " package in win_ros, you can always copy-paste the math from the relevant tf functions:", "1) build a rotation matrix from quaternion data (see ", ")", "2) convert rotation matrix to Euler Angles (see ", ")", "Alternatively, you could use the transform functions built into ", ", which should be available on Windows.", " ", " ", "Thanks for the reply. I just wanted to check that i'm not missing anything before i code the conversion myself. I thought that maybe somebody had to deal with this already. But i guess not too many people are using win_ros.", "tf and rqt are probably the next stacks needing porting for winros now that the build environment and basic comms are working. Porting isn't too hard and I'd be happy to help anyone step through the process if they want to try.", "Hmmm, that's a thought. I have some backlog of stuff i need to do but once i get some free time perhaps i'll contact you to get some guidance on how to port them. "], "question_code": ["tf::poseTFToMsg(tf::Transform(tf::createQuaternionFromYaw(theta), tf::Vector3(x, y, z)), position.pose.pose);\n", "tf::", "static void convertQuaternion2Matrix(const geometry_msgs::Quaternion& q, double m[3][3])\n{\n    double d = q.x*q.x + q.y*q.y + q.z*q.z + q.w*q.w;\n    if(d == 0.) \n        return;\n    double s = 2./d;\n    double xs = q.x * s,   ys = q.y * s,   zs = q.z * s;\n    double wx = q.w * xs,  wy = q.w * ys,  wz = q.w * zs;\n    double xx = q.x * xs,  xy = q.x * ys,  xz = q.x * zs;\n    double yy = q.y * ys,  yz = q.y * zs,  zz = q.z * zs;\n    //row1\n    m[0][0] = 1.0 - (yy + zz);\n    m[0][1] = xy - wz;\n    m[0][2] = xz + wy;\n    //row2\n    m[1][0] = xy + wz;\n    m[1][1] = 1.0 - (xx + zz);\n    m[1][2] = yz - wx;\n    //row3\n    m[2][0] = xz - wy;\n    m[2][1] = yz + wx;\n    m[2][2] = 1.0 - (xx + yy);\n}\n\nstatic double tfAcos(double x) \n{ \n    if (x < -1.)\n        x = -1.;\n    if (x > 1.)\n        x = 1.;\n    return acos(x);\n}\n\nstatic double tfAsin(double x) \n{ \n    if (x < -1.)\n        x = -1;\n    if (x > 1)\n        x = 1;\n    return asin(x);\n}\n\nstatic void getEulerYPR(const geometry_msgs::Quaternion& q, double& yaw, double& pitch, double& roll, unsigned int solution_number = 1)\n{\n    double m[3][3];\n    convertQuaternion2Matrix(q, m);\n    struct Euler\n    {\n        double yaw;\n        double pitch;\n        double roll;\n    };\n\n    Euler euler_out;\n    Euler euler_out2; //second solution\n    //get the pointer to the raw data\n\n    // Check that pitch is not at a singularity\n    // Check that pitch is not at a singularity\n    if (fabs(m[2][0]) >= 1)\n    {\n        euler_out.yaw = 0;\n        euler_out2.yaw = 0;\n\n        // From difference of angles formula\n        double delta = atan2(m[2][1],m[2][2]);\n        if (m[2][0] < 0)  //gimbal locked down\n        {\n            euler_out.pitch = M_PI / 2.0;\n            euler_out2.pitch = M_PI / 2.0;\n            euler_out.roll = delta;\n            euler_out2.roll = delta;\n        }\n        else // gimbal locked up\n        {\n            euler_out.pitch = -M_PI / 2.0;\n            euler_out2.pitch = -M_PI / 2.0;\n            euler_out.roll = delta;\n            euler_out2.roll = delta;\n        }\n    }\n    else\n    {\n        euler_out.pitch = - tfAsin(m[2][0]);\n        euler_out2.pitch = M_PI - euler_out.pitch;\n\n        euler_out.roll = atan2 ..."], "answer_code": ["tf", "tf_conversion", "win_ros", "geometry", "tf::", "tf::Matrix3x3(q).getRPY(R,P,Y)\n", "tf"], "url": "https://answers.ros.org/question/62483/win_ros-quaternion-conversions/"},
{"title": "Throttle publish rate based on topic queue size and fill level", "time": "2013-06-06 04:09:48 -0600", "post_content": [" ", " ", "I've seen questions asked about how to change the subscriber's message receive rate, and filters seem to be the solution to this problem. However, this assumes that it is ok if the subscriber does not receive every published message.", "My question is similar: is it possible for a publishing node to query the fill level of the topic queue to determine if there is space left? Generally speaking, is there a way to throttle an upstream publisher node's publish rate based consumption rates further down the pipeline? The key here is that I cannot afford to have any messages dropped. ", "The scenario: an upstream node publishes messages at a high rate and a downstream node needs to process EVERY message published. I know I could simply increase the pub/sub queue sizes, but this solution is not ideal for extended tests.", "Ideally, I would like to check how much space is left in the queue before publishing, but as far as I can tell, there's no way to do that.", "Any suggestions would be greatly appreciate!"], "answer": [" ", " ", " ", " ", "The functionality that you're describing doesn't really work with the publish/subscribe model, where the publishers by definition just push out data and don't care about how many subscribers they have, or what those subscribers are doing with the published data.", "It would probably be a better fit for the ", " model, where the subscriber (or client) requests data from the publisher (or server) when it is ready to process it. "], "url": "https://answers.ros.org/question/64436/throttle-publish-rate-based-on-topic-queue-size-and-fill-level/"},
{"title": "Rviz is stop - need some help", "time": "2014-02-16 03:48:54 -0600", "post_content": [" ", " ", " ", " ", "Hello all!", "I have Ubuntu 12.04, Groovy. I launch openni_launch and rviz\nI have a problem with Rviz. When I add pointcloud2, rviz is stop and update one per two second. but i haven't any errors.", "What is wrong? maybe it is tf? ", "Also I have warn message: ", " and I found this topic    ", " ", "\nmaybe  improper timestamps relevant for my case? but I can't check it.", "This TF error in openni.launch file? ", "I am use intel core i7.", "What is the CPU usage on your computer while rviz is running?", "I am use intel core i7.", "It looks like rviz is consuming most of a core. Do you have a dedicated graphics card, or are you using software OpenGL rendering?", "I am use OpenGL.", "Doing a minor editing to your question just to push it to the top of the queue is bad form.", "You still haven't stated if you have a dedicated graphics card or not.", "I am sorry for my push.\nNo. I haven't a dedicated graphics card."], "answer": [" ", " ", " ", " ", "This sounds like either there isn't enough network bandwidth, or your computer is too slow to display point clouds.", "Running the system monitor application (GUI) or top (command line) should show the system CPU usage, which will give you some hints about where the problem is.", "Rendering point clouds is pretty graphics-intensive. You may be able to squeeze a little more performance out of your system by switching the rendering style.", "I suspect the tf issues you're seeing are due to buffering and lag issues caused by how long it takes for your computer to render a single point cloud.", "How I can change rendering style?", "http://wiki.ros.org/rviz/DisplayTypes/PointCloud"], "question_code": ["[ WARN] [1392574116.780493741]: TF exception:\nLookup would require extrapolation into the future.  Requested time 1392574116.741091059 but the latest data is at time 1392574116.679388999, when looking up transform from frame [/camera_depth_optical_frame] to frame [/camera_rgb_optical_frame]\n", "PID USER        PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND            \n19715 max       20   0 2085m 551m  63m R   82 18.3   1:04.19 rviz               \n19475 max       20   0 1543m  65m  22m S   74  2.2   0:44.53 nodelet\n"], "url": "https://answers.ros.org/question/129601/rviz-is-stop-need-some-help/"},
{"title": "map_saver doesn't return from \"Waiting for the map\"", "time": "2011-11-15 14:41:08 -0600", "post_content": [" ", " ", " ", " ", "I'm doing ", "'s tutorial ", " and can't generate a map file. Actually I succeeded only first 2 times. Since then ", " always says:", "and the process ", " keeps consuming around 95%.", "When I succeeded:", "I used ", " to move turtlebot.", "Env) Ubuntu 10.04, "], "answer": [" ", " ", "it seems to me that your nodes are not able to communicate with each other.\nyou could try running rxgraph next to the two nodes and see if the actually share the same map topic.", "if not, make sure that", "quick question: are both nodes running on the same machine?", "Raph", " ", " ", "This is the answer to my own map saving problem that led to the \"waiting for the map\" hanging problem (which I could only ctrl-z kill-9 kill, ctrl-c couldn't stop it).  My OccupancyGrid map was actually on /map2d, so I needed to argument remap:", "The proper output is this:", " ", " ", " ", " ", "Though this can't be the permanent solution, I found after rebooting OS saving map works fine. But still the issue occasionally occurs. At least at this moment there could be some tips and do I just understand that the issue happens? Hope this won't happen when I try very large room...", "And I just realized myself that some of my question I asked here were solved by just rebooting OS. I know rebooting is often an ultimate solution for software development and from next time I'll try that before asking question."], "answer_details": ["a roscore is still running", "both nodes can see the same master", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["turtlebot", "map_saver", "turtlebot@turtlebot-1:~$ rosrun map_server map_saver -f /tmp/my_map\n[ INFO] [1321409343.272498323]: Waiting for the map\n^C         <---- I manually killed \n^CTerminated\nturtlebot@turtlebot-1:~$\n", "map_server", "turtlebot@turtlebot-1:/tmp$ rosrun map_server map_saver -f /tmp/my_map\n[ INFO] [1321405355.532849715]: Waiting for the map\n[ INFO] [1321405355.807251282]: Received a 480 X 544 map @ 0.050 m/pix\n[ INFO] [1321405355.812693593]: Writing map occupancy data to /tmp/my_map.pgm\n[ INFO] [1321405355.851996657]: Writing map occupancy data to /tmp/my_map.yaml\n[ INFO] [1321405355.853300442]: Done\n", "teleop", "ROS/turtlebot electric"], "answer_code": ["rosrun map_server map_saver -f map /map:=/map2d\n", "[ INFO] [1393526083.353818074]: Waiting for the map\n[ INFO] [1393526083.936695077, 256.619000000]: Received a 480 X 480 map @ 0.050 m/pix\n[ INFO] [1393526083.936841755, 256.619000000]: Writing map occupancy data to map.pgm\n[ INFO] [1393526083.946531480, 256.620000000]: Writing map occupancy data to map.yaml\n[ INFO] [1393526083.946678716, 256.620000000]: Done\n"], "url": "https://answers.ros.org/question/11957/map_saver-doesnt-return-from-waiting-for-the-map/"},
{"title": "terminate & launch other node from code", "time": "2011-12-07 10:30:19 -0600", "post_content": [" ", " ", " ", " ", "Is there a ROS way to terminate and launch another node from one node's code?", "For launching example in python, it's possible by doing something like this w/o using ROS:", "Any way to do the same in an official way in ROS? ", "How about terminating as well? If a node run as \"respawn = true\" receives \"shutdown\" msg/service request then shuts down itself? Better idea?", "Update) I thought I'd better describe a situation when I need this...There is a node that gets to consume higher CPU in a process of time (eventually CPU load reaches 12.0 or above although it's not supposed to do so). And I found that restarting makes it stay calmer and still functions. Besides debugging that node itself, it would be nicer if I can just work around the issue for now by restarting the node.", "Update-2) Found ", " and ", " might be good as well for launching from ", " (I'm sure the argument for ", " is ", " instead of ", " as in ", ")."], "answer": [" ", " ", "Yes, of course, you can start a node from another node using your favorite process management API.\nIn ROS, you also have ", " which will provide this kind of service.", "However, it seems to me that what you want to achieve does not really fit well with the ROS framework. When using component oriented frameworks, the general idea is to start all the services at the beginning (using a roslaunch file for instance) and stating nodes relations and configurations using the ROS parameters.\nThen no nodes should be killed before the whole system is shutdown.", "Then monitoring tools and/or interactive tools should just be launched from outside, using rosrun.", "can you tell me how it will be done in rosjava? i mean which api in rosjava is used to run another node?", " please don't use comment section to ask a whole new question. Open a new one.", " ", " ", " ", " ", "We've encountered this problem as well. On our robot, we have ROS set to autoboot on power-up. At this point, it boots into a \"minimal\" state in which only a few nodes are running until we receive a connection to an external network. At that point, it will launch the rest of the ROS nodes. Why we do this isn't really important, but here's HOW we do it... Just a warning, it's fairly complex. ", "Our whole autoboot/control system is based on a Bash/C++ interface. You are able to pipe the output of a C++ program to a Bash script and read the output. To do this, your C++ program must simply output to stdout and Bash can read it. Here's a quick example of what I mean:", "control.cpp", "control.bash", "In the above example, \"/home/robot/scripts/control\" represents the location of the c++ executable on the filesystem. The reason it's wrapped in a Bash function is because Bash doesn't allow you to pipe an executable directly in to the function. I.E. \"", "\" is not allowed. That's why the executable is wrapped in the Bash function cpp_func.", "This allows us to execute any functions we want (start, kill, restart, etc.) on any node we want. As long as you expect an output in your Bash script, you can handle any ROS commandline processes from that one control.bash script. Just a note, you cannot send commands back to the c++ program using this method. ", "I can't understand \"/home/robot/scripts/control\", is this the path where \"control.cpp\" locates or some other path?", " ", " ", "I just had the same problem and was stunned to see how difficult it was to find a simple example for this basic task.\nSo I added a minimal code example to the roslaunch wiki page:", "Is there a way not just to start a single node, instead start an whole launch file?", " it looks like there might be api entries for that but they aren't currently implemented:  ", "Though you can start a launch file bypassing the api ", "- but it blocks and doesn't have the stop/is_alive methods."], "question_code": ["import os\nos.system('roslaunch turtlebot_navigation_ours amcl_customized.launch')\n", "roslaunch.scriptapi", "python", "ROSLaunch.launch", "roslaunch.core.Node", "roslaunch.Node"], "answer_code": ["#include <iostream>\n#include <string>\n#include <unistd.h>\n\nusing namespace std;\n\nint main(int argc, char** argv)\n{\n    while(1)\n    {\n      if(guiNotConnected())\n      {\n        cout << \"Connected\" << endl;\n      }\n      sleep(1);\n    }\n}\n", "#!/bin/bash\n\nfunction cpp_func {\n\n    /home/robot/scripts/control\n\n}\nwhile read line; do\n    if [ \"$line\" == \"Connected\" ]; then\n        roslaunch robot_launch rest_of_ros.launch\n    fi\ndone < <(cpp_func)\n", "done << /home/robot/scripts/control", "import roslaunch; args = ['roslaunch', 'my_package', 'foo.launch']; roslaunch.main(args)"], "url": "https://answers.ros.org/question/12260/terminate-launch-other-node-from-code/"},
{"title": "Integration of energy framework with ROS", "time": "2014-04-11 12:33:43 -0600", "post_content": [" ", " ", "I am currently working on a software framework to monitor the energy consumption of robots or any embedded device. I wanted to build my framework in a similar manner as ROS.", "At the moment, I have created a user API that does the following (front end):    ", "This framework allows users to profile their code for energy consumption.  (I have also created the back-end but is not relevant to the question I want to ask, so I am not including its details here.)", "Is it possible to integrate my framework with ROS? For example, I would like to profile devices attached to a central machine running ROS. Is it possible to set checkpoints and sample them within a node process, along with the rest of the ROS API? If yes, may I know how I would go about doing this?", "Thank you very much."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "a user can create a probe (a data structure used to collect data from an energy source) and associate it with a device attached to the embedded board.  ", "set a checkpoint with respect to any probe that is already created; once set, energy monitoring of the corresponding device begins.", "A user can 'sampleCheckpoint' at which point the latest energy reading collected from the corresponding device is returned.  ", "Finally, a user can also delete checkpoints and probes.  "], "url": "https://answers.ros.org/question/152146/integration-of-energy-framework-with-ros/"},
{"title": "ROS internals - topics storage and network comms?", "time": "2014-04-11 02:52:34 -0600", "post_content": [" ", " ", " ", " ", "I am currently working on a software framework to monitor the energy consumption of robots or any embedded device. I wanted to build my framework in a similar manner as ROS.", "At the moment, I have created a user API that does the following (front end):    ", "This framework allows users to profile their code for energy consumption.  ", "The backend: ", "\nThe architecture of the system is built in a producer-consumer fashion (similar to ROS), where energy sources are the producers and any software using the API are consumers. There is a central authority in between that stores checkpoints and probe related data in an sqlite database. And all communications between producers and consumers happen via this central authority over unix sockets.", "I would also like to integrate this framework with ROS at some point. In order to do that, I believe I need to create a ROS package for the framework?", "Thank you.", "Some nitpicking: ROS (the middleware) is built upon publish-subscribe and a flavour of RPC (or implicit and explicit invocation). While similar, a plain producer-consumer system is not the same.", "Thanks for pointing that out."], "answer": [" ", " ", " ", " ", "To answer your questions:", "1 . Does ROS also use UNIX sockets to do all its network communications?", "No, it uses TCP/IP for nodes (to cross process boundaries), and pointer-passing in nodelets (essentially nodes mapped on threads).", "2 . Does ROS store all the topics & subscribers associated with each topic in a database of some sort or just in memory?", "Plain ROS (ie: no extensions providing additional transports) uses in-memory buffering for messages, and a special node (the ", ") as a central index to provide topic->node mapping services. AFAIK that mapping is stored in memory, no databases (if you mean (R)DBMS) involved. Node->node communication is also direct, so the master is only involved in the initial setup of the connection.", "Edit: see also the ", " wiki page.", "Thanks for your reply. I've just read that nodes send messages to each other via topics. So is a topic a 'name (char *) + memory buffer (user definable datatype)'  data-structure? And, does the master keep a track of all the topics in a linked-list data-structure? Thanks."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "a user can create a probe (a data structure used to collect data from an energy source) and associate it with a device attached to the embedded board.  ", "set a checkpoint with respect to any probe that is already created; once set, energy monitoring of the corresponding device begins.", "A user can 'sampleCheckpoint' at which point the latest energy reading collected from the corresponding device is returned.  ", "Finally, a user can also delete checkpoints and probes.  ", "Does ROS also use UNIX sockets to do all its network communications?", "Does ROS store all the topics & subscribers associated with each topic in a database of some sort or just in memory?"], "answer_code": ["master"], "url": "https://answers.ros.org/question/151946/ros-internals-topics-storage-and-network-comms/"},
{"title": "Are there packages or libraries about sensors,for example laser,to accomplish the simulation?", "time": "2014-04-20 17:33:57 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone! I'm trying to describe my question clearly. Now I'm working on a project  based on ros to develop a graphical robot  platform that can simulate behavior of a robot,such as obstacle avoidance,navigation and so on.First I want to solve the simulate question about sensors,because we know that sensors are necessary to avoid obstacle.Are there packages or libraries about sensors,for example laser,which can be useful for me to accomplish the simulation?Or similarly,if you understand what I need,are there any useful links?                                       PS:I have no  physical devices,just want to simulate the behavior.", "But ROS is in fact consumer of device data, not provider. You want to simulate something, using ROS?"], "answer": [" ", " ", " Take a look at Gazebo ( ", " ), which is an open-source simulator that has ROS integration and plugins for at least simulating laser range finders, cameras, and depth cameras. "], "url": "https://answers.ros.org/question/155436/are-there-packages-or-libraries-about-sensorsfor-example-laserto-accomplish-the-simulation/"},
{"title": "Does anyone have a catkinized version of the hector_navigation package?", "time": "2014-08-01 04:44:46 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I was trying to catkinize the hector_navigation package myself but it is quite time consuming. If anyone has even catkinized a couple of packages, particularly the hector_exploration_planner, please do post the link here, it would help immensely.", "Thank You."], "answer": [" ", " ", "There\u00b4s ", " that we have not tried yet (but should) and ", " that both provide catkinized versions. If you try them, let me know if things worked :)", "Thanks for the answer, Stefan! \nI managed to get the forked link to work after solving a few compilation errors, it appears to be running perfectly now!"], "url": "https://answers.ros.org/question/188565/does-anyone-have-a-catkinized-version-of-the-hector_navigation-package/"},
{"title": "Any simple simulators suitable for multi-UAV applications?", "time": "2014-09-14 20:10:11 -0600", "post_content": [" ", " ", "Hi there,", "We are going to do some research on multi-UAV coordination. And we've been told that Gazebo is too complicated and too computationally consuming, making it an overkill, while Stage is not good at UAV applications. Of course, we would be happy to know if there are any other opinions.", "To make our question more clear, basic requirements for the simulator are listed below:", "Able to simulate multiple UAVs\n(better with example models for\ntypical UAVs provided);", "Able to simulate ground vehicles (as\n    nonholonomic systems);", "Typical sensor models (LiDAR,\n    sonars, GPS, IMU, ..., having\n    cameras would be a plus);", "Fancy graphics is not necessary,\n    since we are looking for a\n    light-weight solution.", "Any suggestions will be appreciated.", "BTW, we know that there is a simple simulator called STDR. Any comment on this? Is it possible to use it as a base and add features when necessary?", "What about MORSE? Or V-REP? Both have integration to ROS and can do 'kinematical' simulation (no CPU intensive physics computations)."], "answer": [], "url": "https://answers.ros.org/question/192683/any-simple-simulators-suitable-for-multi-uav-applications/"},
{"title": "The images from Bumblebee 2 is noisy", "time": "2014-11-23 13:55:15 -0600", "post_content": [" ", " ", " Hi,\n   I capture an image from Bumblebee 2 (mono8 from mono 16 raw data).\n   It looks like that:  ", " \n   It looks very noisy. Are they supposed to noisy like that ? Or have to adjust some parameters?\n   Thanks. ", "I haven't used the bumblebee before, but that looks sort of like a raw bayer-pattern image. Have you tried passing it through a debayering node/nodelet?", "I tried to do debayering. But it returns error. My understanding is that mono image does not need to do debayer? Am I right? \nI do err = dc1394_deinterlace_stereo_frames(frame, &decodeFrame, DC1394_STEREO_METHOD_INTERLACED);\nit returns success.", "I'm not familiar with the API you're referring to. You may want to consult the manufacturer's docs or wait for someone more knowledgeable to come along.", "Also... is this actually related to ROS?"], "answer": [" ", " ", " is right, the first image is a bayer pattern, and the other image posted looks as if the data stream was not correctly gathered or that different color channels are shown in grayscale.", " We implemented a driver for a Bumblebee 2 camera some time ago. We are still using and maintaing it. I recommend you that you run it and see what happens. The node is available at  ", "  . ", "Run the stereo_camera.launch together with an stereo_image_proc node and subscribe with an image_viewer. If the problem persists I would contact PointGrey support.", " ", " ", " It is strange that the camera has 4 different exposure at the same time.\n ", " \nThe picture is from the raw data.  ", "Are you sure this isn't 4 channels from a color camera?", "Yes. I set the dc1394video_mode_t mode = DC1394_VIDEO_MODE_1024x768_MONO16. The raw data is Mono16. I separate it too two mono 8 image. If it is color image, the 3 channels will look very different.", "Please do only post answers as answers. For comments post comments and for additional information regarding your question update your question."], "url": "https://answers.ros.org/question/198053/the-images-from-bumblebee-2-is-noisy/"},
{"title": "ROS indigo rviz OGRE mesh wrong lighting", "time": "2014-11-20 01:51:18 -0600", "post_content": [" ", " ", " ", " ", "Hi All,", "i got another issue with the ROS rviz plugin while moving from hydro to indigo version. I wrote a rviz-plugin which loads and display a ogre mesh (a vehicle to be more exact). In ROS hydro everything is the more or the less fine, the mesh is drawn as it should and also the lighting is correct. ", "If i change nothing in the plugin and only change the ROS version to indigo, some really strange things happen:", "What i already tried / did:", "What seems to work:", "Did someone of you encounter the same problems drawing OGRE Meshes in ROS-indigo-rviz?", "Mesh in ROS Hydro:", "Mesh in ROS Indigo:", "Fixed your image links: links to html pages won't work.", "Thank you for fixing my post again!", ": have you considered reporting this to the ", "? It seems like you have already narrowed things down pretty well.", ": i added a new issue to the RViz bugtracker and added a link to this post, hopefully this ok!", "Yes, that is ok. You could perhaps summarise the contents of your question here in the issue on github, so the issue stands on its own.", "Done. Don't take me wrong but this process seams to be very time consuming. Thank you for your help.", "Which process? The ROS Answers forum is moderated and frequented by volunteers (in addition to OSRF employees), it is not a dedicated support forum :).", "And for reporting bugs -- as you essentially did -- it is always a good idea to directly go to the github repository of the relevant package."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "The lighting of the mesh seams to be broken, nearly all surfaces are drawn black. ", "The mesh seams to be drawn twice. I tried to add a GLSL shader program which adds a simple phong-shading to some surfaces / materials. Despite of that the model and view matrix will not get passed correctly to the shader (i used the auto params) i got some heavy z-fighting on surfaces which definitely are not modeled double sided.", "Double checked normals, they are ok.", "Viewed the model with the Ogre Max Viewer, which also uses OGRE Version 1.8.1 (same as the ROS indigo) - Lighting is correct!", "Wrote a simple test program which not uses ROS. - Lighting is correct!", "Updated mesh with the Update Utility of OGRE.", "Tested another mesh - Same issue!", "Programmatically changed Face Culling, Light, various material values of the mesh, use own GLSL shader (see above) Nothing changed. ( Set ambient light in scene-manager, add additional light to scene ... ) ", "Exported a simple cube from Blender (start scene cube) 2.69.0 with blender2ogre-0.6.0 and tested this mesh in both ROS versions. Same here, Cube is black in ROS indigo and white (correct lighting) in ROS hydro. Both with identical rviz plugin.", "Cloned newest rviz version (indigo-devel-branch) to ROS-hydro version so rviz 1.11.3 will run under ROS-hydro, everything works well! I also tested the old rviz version 1.10.18 (hydro-devel-branch) under ROS-indigo, lighting still broken. ", "Emissive materials seems to work in ROS indigo. The cube will be red if emissive set to 1 0 0 1 in ogre-material file."], "url": "https://answers.ros.org/question/197839/ros-indigo-rviz-ogre-mesh-wrong-lighting/"},
{"title": "Commanding simple paths / curves to move_base", "time": "2014-12-19 20:40:03 -0600", "post_content": [" ", " ", "Hi", "I want to measure the power consumption of my robot when it moves on some simple paths.", "\nFor example I would like to command ", ". Is there any way to save and modify  certain paths? \nWhat I tried so far:", "Thanks for your help.", "have you found the solution to this?"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Just giving goals to the robot leads to a very unspecific behavior which is not easily repeatable. ", "Also simply using ", " is too low level for this."], "url": "https://answers.ros.org/question/199865/commanding-simple-paths-curves-to-move_base/"},
{"title": "custom messages under rosjava indigo", "time": "2015-01-08 09:07:56 -0600", "post_content": [" ", " ", "I am working with ros indigo on Xubuntu (Ubuntu 14.04.1 LTS) ", "I want to create some custom messages for consumption in both Matlab and C++\nI'm working on the assumption that in order to do this I need rosjava to generate a jar file from my custom message package.", "I've followed the Indigo tutorial for installing rosjava from source. I then started working through the Hydro tutorials only to find that these seem no longer applicable. In the end I simply copied the custom messages package from my catkin_ws/src into my rosjava/src and tried a catkin_make. This seems to have worked but given the complexity of this process under Hydro and can't help feeling I might be missing something. "], "answer": [" ", " ", "After further testing we have satisfied ourselves that the process under Indigo is now just a simple catkin_make."], "url": "https://answers.ros.org/question/200610/custom-messages-under-rosjava-indigo/"},
{"title": "Raspian / armhf package repository plans?", "time": "2015-01-21 16:03:13 -0600", "post_content": [" ", " ", " ", " ", "Are there any plans for building and providing a standard repository for armhf packages compatible with the likes of Rasbian?", "Raspberry Pi seem like a great gateway drug for getting into ROS given how cheap, relatively powerful and well supported platform it is.  Seem like a very sensible/obvious first choice when upgrading a robot to include a computer capable of running a full blown OS.  The rather convoluted, shaky and time consuming process of building the environment and trying to get different packages is pretty intimidating if your not already a ROS veteran and a competent  linux buff. I can claim neither.", "I have come across a willow garage effort by Paul Mathieu in mid 2013 for groovy (including buildfarm instructions) which looks great but I don't see any indication that it was ever taken further or updated since.", "Having prowled through the ROS tutorials (via ROS in a VM) and being thoroughly impressed by it's capabilities I only imagine the satisfaction of being able to start operating ROS on a fresh $40 RPi with in minutes of powering it on for the first time.", "I myself am one of those amateur robot builder trying to upgrade from a arduino based arbotix include a RPi and I know it has all the tools I need but getting them to work remains a pretty convoluted process still.", "Spend the extra $10-15 for a better board with an ARMv7 processor. You'll get something significantly more powerful and better supported by Ubuntu and ROS. I have no plans to support the Raspberry Pi.", "Raspberry Pi 2 is being announced these days with a quad 900Mhz ARMv7 and 1GB RAM so maybe I will have the entire cake and eat it in the end. I'll be looking for your packages when I get my hands on one :)"], "answer": [" ", " ", "The current build farm does not build ARM packages on a regular base.", " built ARMhf packages some time ago and they are being imported into the official repositories in the next days (?). But they are already several months old.", " The upcoming build farm will be able to build ARMhf at the same time as the other architectures. If you would like to try these packages (which are still in a temporary apt repository) you can get them from  ", "Hopefully the new build farm can replace the current one (for Indigo and Jade) in the near future.", "Note that these packages (old and new) won't work on the Raspberry Pi; it's a ARMv6 instruction set, while all of the builds are done for the ARMv7 instruction set.", "Will they work on the new Raspberry Pi 2 B? I am playing around with one now but there was some problem following the Raspbian instructions. I suppose the Raspbian instructions when refrencing binary debs will give you the wrong kind (for ARMv6).", " The UbuntuARM debs probably won't work on Raspbian. They might work if you install Ubuntu. See also:  "], "url": "https://answers.ros.org/question/201469/raspian-armhf-package-repository-plans/"},
{"title": "Trouble compiling rviz on Raspberry Pi 2", "time": "2015-03-06 08:44:28 -0600", "post_content": [" ", " ", " ", " ", "Hello,\nI've been following the ", " and am failing at step 2.3 (building the catkitn workspace). The process fails at step 178 of 185, building rviz. It gets to 2% of that process and then hangs the Pi, which continues to consume 1/4A at 5V but sits there for hours and hours with no progress. The machine also becomes unresponsive to pings, so I think it's totally hosed at that point. Has anyone seen anything like this? Swapping hardware doesn't seem to make a difference. Thanks in advance.", "Here's the last screen of data before everything goes down:", "This may not solve your problem, but  try the build with make -j2 instead of -j4.  This will run two sets of jobs concurrently rather than four.  The RPi2 doesnt have enough memory for 4 jobs and will thrash quite a bit and then die on large builds but will bld MUCH faster with two.", "This helps, but doesn't completely build. Now, I get to 94% on rviz's build, which is probably worth opening another question for. For the original question, though, this seems to be the answer: make must be invoked with -j2, so the wiki should be modified accordingly.", " Ok; I got rviz to make using the patch suggested in the following question:  ", "  . I'll update the wiki accordingly if I get catkin_make_isolated --install to complete. ", "This worked and the wiki has been updated. I made an answer below that has both of these steps, but I don't have enough \"karma\" to mark it as correct."], "answer": [" ", " ", "First, a patch needs to be applied to rviz as described ", ". Then, as ", " suggested, running make -j2 from .../ros_catkin_ws/build_isolated/rviz will allow the build to succeed; once this is complete, catkin_make_isolated will proceed as normal. I have updated the wiki accordingly, though someone will probably want to check my formatting. Thanks to everyone for all the help! (You can probably run catkin_make_isolated with ", " after patching, but I didn't test that.)", "when i failed, i running sudo make -j2 from .../ros_catkin_ws/build_isolated/rviz, but i got this \"-- Configuring incomplete, errors occurred!\nMakefile:1030: recipe for target 'cmake_check_build_system' failed\nmake: ", " [cmake_check_build_system] Error 1\n\",please help me.i am a new.", "I fix it by building rviz alone(remove all the packages from .../ros_catkin_ws/src excpet rviz), then run catkin_make_isolated with --make -j2 -l2.", " ", " ", " Unlike the original Raspberry Pi, the Raspberry Pi 2 is a proper ARMv7 processor. Install Ubuntu and use  ", "  instead. ", "That was my original intent. However, Snappy Ubuntu Core is the only supported Ubuntu distribution for the Raspberry Pi 2, and it does not support the apt-get package management system, which is called for in the armhf directions. There are unofficial 14.04 installs, but I'd rather not go that way.", " ", " ", " I'm using ROS INDIGO at my PI2 for some days with Ubuntu 14.04 LTS now...\nTry this:  ", " ", "what installation guide you use top get ROS. Was it the standard ubuntu or a source?", "I'm using binary installation", "Right - you're using an unsupported community build of Ubuntu. It does not have the guaranteed security updates that you'd get if you were using the supported LTS version. Not sure this is the best solution to the problem, which is an inability to make using the official scripts.", "thanks for the help", " Sometimes active communities provide better support than \"official\" installation paths.", " I don't mean any offense. Point well-received; after all, what is this if not an extremely dedicated and active community. My fear stems from having burned from such efforts in the past, especially when it's a matter of security, as in the install of an OS. That said, I'll give it a try!", " ", " ", "I have gotten as far as you, I came to the conclusion that I should just install comm packages and wait until someone else does the dirty work."], "question_code": ["<== Finished processing package [177 of 185]: 'robot_state_publisher'\n\n==> Processing catkin package: 'rviz'\n==> Building with env: '/opt/ros/indigo/env.sh'\nMakefile exists, skipping explicit cmake invocation...\n==> make cmake_check_build_system in '/home/sshamlian/ros_catkin_ws/build_isolated/rviz'\n==> make -j4 -l4 in '/home/sshamlian/ros_catkin_ws/build_isolated/rviz'\n[  0%] [  1%] Building CXX object src/rviz/CMakeFiles/rviz.dir/visualization_frame.cpp.o\nBuilding CXX object src/rviz/CMakeFiles/rviz.dir/visualization_manager.cpp.o\n[  2%] [  2%] Building CXX object src/rviz/CMakeFiles/rviz.dir/view_manager.cpp.o\nBuilding CXX object src/rviz/CMakeFiles/rviz.dir/visualizer_app.cpp.o\n"], "answer_code": ["--make-args -j2"], "url": "https://answers.ros.org/question/204422/trouble-compiling-rviz-on-raspberry-pi-2/"},
{"title": "Query for robotic movement", "time": "2015-02-15 08:27:50 -0600", "post_content": [" ", " ", "Dear sir,\n  I have a query on robotic movement could u please help me out in solving it ??", "Query is :-", "A robot has to define to pick and place 2 different kind of balls in the shelf which is linear in nature and when picking up it has to follow the rule as left & right side of the ball should not be of same kind\nand the robot has to give solution as how many balls are there of each kind and while placing it has to utilize the complete set of available space then only it can say you have no space to place the ball.!", "I will be highly thankful to you if u could give some good solution on this!", "Waiting for your reply!Hope to listen from you ASAP", " This question seems inappropriate. Please consult\n "], "answer": [], "url": "https://answers.ros.org/question/203155/query-for-robotic-movement/"},
{"title": "Fitting line to pointcloud data", "time": "2015-01-08 18:19:40 -0600", "post_content": [" ", " ", "Hello Everyone,\nI am trying to fit a line to a point-cloud data received from a 2D lidar. ", "I am using random sample consus from pcl library to get this done. The first picture when the LIDAR is aligned with the wall, the line fits well. But when lidar is yawed the line deviates by certain degrees. ", " ", "Here is the code which does it:"], "answer": [" ", " ", "It works now!!. I was visualizing it wrongly. The trick is to take the rotation coefficients and convert it into quaternions, which is what RVIZ understands. ", "Here is the modified code ", " which does that:", " ", "hey, nice work. ", "did you open the source code on the github or the whole code opened yet?", " ", " ", "The line coefficients are x, y, z, dx, dy, dz, not pose."], "question_code": [" ros::Publisher marker_pub;\nros::Publisher pub;\nros::Publisher pub1;\nvisualization_msgs::Marker marker;\nvisualization_msgs::Marker text_1;\n#define PI 3.14159265\nusing namespace std; \nfloat angle=0.0;\nstd::ostringstream tostr;\n#include <sstream>    \nvoid cloud_cb (const pcl::PCLPointCloud2ConstPtr& input)\n{\n        pcl::PCLPointCloud2::Ptr cloud_filtered (new pcl::PCLPointCloud2);\n    pcl::PassThrough<pcl::PCLPointCloud2> pass_through_x;\n        pcl::PassThrough<pcl::PCLPointCloud2> pass_through_y;\n\n        pass_through_x.setInputCloud(input);\n    // set fieldname we want to filter over\n    pass_through_x.setFilterFieldName (\"x\");\n    // set range for selected field to 1.0 - 1.5 meters\n    pass_through_x.setFilterLimits (0.0,1.0);\n    // do filtering\n    pass_through_x.filter (*cloud_filtered);\n\n    pass_through_y.setInputCloud (cloud_filtered);\n    // set fieldname we want to filter over\n    pass_through_y.setFilterFieldName (\"y\");\n    // set range for selected field to 1.0 - 1.5 meters\n    pass_through_y.setFilterLimits (0.0, 2.0);\n    // do filtering\n    pass_through_y.filter (*cloud_filtered); \n\n        pub.publish(*cloud_filtered);\n    pcl::PointCloud< pcl::PointXYZ> cloud;\n        pcl::fromPCLPointCloud2(*cloud_filtered, cloud);\n        pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);\n    pcl::PointIndices::Ptr inliers (new pcl::PointIndices);\n    // Create the segmentation object\n    pcl::SACSegmentation<pcl::PointXYZ> seg;\n    // Optional\n    seg.setOptimizeCoefficients (true);\n    // Mandatory\n    seg.setModelType (pcl::SACMODEL_LINE);\n    seg.setMethodType (pcl::SAC_RANSAC);\n    seg.setDistanceThreshold (0.006);\n    seg.setInputCloud (cloud.makeShared ());\n    seg.segment (*inliers, *coefficients);\n\n    if (inliers->indices.size () == 0)\n    {\n    PCL_ERROR (\"Could not estimate a LINE model for the given dataset.\");\n    }\n        angle=2*asin(coefficients->values[4])*180.0/PI;//formulae for quarternions.\n    //std::cerr <<\"deviation from the wall:\"<<angle<<endl;\n\n    tostr << angle; \n    text_1.ns = \"vehicle_orientation\";\n    text_1.header.frame_id = \"laser\";\n    text_1.header.stamp = ros::Time::now();\n    text_1.type = visualization_msgs::Marker::TEXT_VIEW_FACING;\n        text_1.action = visualization_msgs::Marker::ADD;\n        text_1.pose.position.x =0.0;// coefficients->values[0];\n        text_1.pose.position.y =-0.3;// coefficients->values[1];\n        text_1.pose.position.z =0.0;// coefficients->values[2];\n        text_1.pose.orientation.x = 0.0;\n        text_1.pose.orientation.y = 0.0;\n        text_1.pose.orientation.z = 0.0;\n        text_1.pose.orientation.w = 1.0;\n        text_1.id = 1;\n        text_1.text=tostr.str();\n        tostr.str(\"\");\n\n        text_1.scale.z = 0.12;\n        text_1.color.r = 0.0f;\n        text_1.color.g = 1.0f;\n        text_1.color.b = 0.0f;\n        text_1.color.a = 1.0f;\n        text_1.lifetime = ros::Duration();\n            marker_pub.publish(text_1);         \n\n              marker.ns = \"vehicle_orientation\";\n             marker.header.frame_id = \"laser\";\n             marker.header.stamp = ros::Time::now();\n             marker.type = visualization_msgs::Marker::ARROW;\n            marker.action = visualization_msgs::Marker::ADD;\n        marker.pose.position.x = coefficients->values[0];\n        marker.pose.position.y = coefficients->values[1];\n        marker.pose.position.z = coefficients->values[2];\n        marker.pose.orientation.x = coefficients->values[3];\n        marker.pose.orientation.y = coefficients->values[4];\n        marker.pose.orientation.z = coefficients->values[5];\n        marker.pose.orientation.w = 0.0;\n        marker.id = 0;\n        marker.scale.x = 0.6;\n        marker.scale.y = 0.01;\n        marker.scale.z = 0.01;\n        // Set the color -- be sure to set alpha to something ..."], "answer_code": ["        tf::Vector3 axis_vector(coefficients->values[3], coefficients->values[4], coefficients->values[5]);\n        tf::Vector3 up_vector(1.0, 0.0, 0.0);\n        tf::Vector3 right_vector = axis_vector.cross(up_vector);//\n        right_vector.normalized();//\n        tf::Quaternion q(right_vector, -1.0*acos(axis_vector.dot(up_vector)));//\n        q.normalize();//\n        geometry_msgs::Quaternion line_orientation;\n        tf::quaternionTFToMsg(q, line_orientation);\n"], "url": "https://answers.ros.org/question/200635/fitting-line-to-pointcloud-data/"},
{"title": "High CPU usage with listener/talker on RPi", "time": "2015-04-03 14:28:33 -0600", "post_content": [" ", " ", "Hi,", "I am using a Raspberry Pi running Debian with Indigo distro installed on it. I was surprised verifying that the simple listener/talker pair from the tutorials was consuming about 40-55% (20-30% each node) when the talker was running at 100Hz. The only modification made on the source was on the talker side to change the frequency. I would expect much less CPU load... Has anyone found similar issues? Does anyone have any idea why this happens? Could it be an issue on ros-comm package on RPi? Any clue?", "Thanks in advance for your help."], "answer": [], "url": "https://answers.ros.org/question/206610/high-cpu-usage-with-listenertalker-on-rpi/"},
{"title": "UR5/UR10 typical use cases", "time": "2015-02-25 03:48:06 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "We are currently reviewing the UR5 arm specifications to make sure it can cope with all our requirements and later invest and buy one arm. One criterion on which we focus is the possibility for the arm to follow a trajectory planned on an external PC. Here is our desired setup:", "We have a ROS external PC", "The planned trajectory is sampled on the ROS PC", "As far as I know the communication between the ROS PC and the UR controller is rather limited in terms of frequency (50Hz for UR script and 120Hz for the C-API). This limits the complexity and speed of the trajectory the ROS PC can specify to the UR controller - as you can only send 50 samples per second (complex and quick trajectories may require more samples than this).", "In order to get a better understanding of the applications and use cases possible, I have a few questions:", "Are there people around here who have the same setup?", "What kind of application and use case do you use this setup for?", "Are you somehow limited in what you do, by the communication frequency?", "Do you use UR script (ur_bringup) or the C-API (ur_c_api_bringup)?", "Do you have the UR controller interpolate between the trajectory samples you provide?", "--- EDIT ---", "What is meant by \"", "\" relates to the 2 control modes usually possible when streaming desired configuration samples to a robot controller periodically (a sample = position + velocity + acceleration):", "Either the controller interpolates between the samples, so these samples are seen as the 2 extremities of a new (sub)trajectory", "This mode allows to specify consecutive samples which are rather far away (as they are interpolated)", "But the mode is not compatible with all applications as the samples may themselves come from a higher level planned trajectory (and you do not really want a \"black box\" interpolation in this case)", "Or the UR controller uses an incoming sample \"as is\" and straightforwardly as a new set point directly provided to the low-level control of the arm. The low-level joint PID (or whatever type of low-level control is used) makes sure the set point is reached as fast as possible", "This requires the contiguous samples to be close in space", "This mode allows an external industrial PC to generate its own trajectory and have it executed by the UR5 arm. This is the mode I am interested in", "Now \"", "\"  means we do not want behavior 1: what we look for is behavior 2.", "Kind regards,", "Antoine.\u200b", "Just a quick comment: \"we want no interpolation between samples\": perhaps you should clarify this a bit, interpolation where? The UR controller will obviously interpolate at some level (unless you can generate trajectories at the ctrlrs interpolation step perhaps).", "Hi, I have just updated my post accordingly. Thanks.", "Even your option 2 will result in the controller interpolating your TrajPts if you don't sync with the controller interpolation step (and use some supported method of submitting points to the ctrlrs motion controller). Only the c-api driver does that right now, afaik.", "As to \"120Hz for the C-API\": that is a fundamental limit of the UR controller: its internal control frequency is 125Hz. The network comm between ROS & the C-API driver running ", " does not impose that limit.", "Hello gvdhoorn, I am curious on how you got to know that the C-API synchronizes the provided samples with the samples consumed by the controller. Also, do you know more about how this mechanism is achieved?", "That statement was based on experience with other (industrial) robot control system internals. As to the C-API and 'syncing': I haven't heard that from UR themselves, if that's what you mean to ask. Comments in the C-API headers lead me to conclude that. There is always interp., even in c-api."], "answer": [" ", " ", " ", " ", "1 . You mean the low-level joint control (the PID or so) runs at 125Hz? That does not sound like much...", "From section 1.7.2 - ", ", in chapter 1 - ", " in the ", " (version 1.8, page 4):", "The robot must be controlled a frequency of 125 Hz, or in other words, it must be told what to do every 0.008 second (each 0.008 second period is called a frame). To achieve this, each thread is given a \u201cphysical\u201d (or robot) time slice of 0.008 seconds to use, and all threads in a runnable state is then scheduled in a round robin1 fashion.", "This obviously does not mean that the joints themselves are controlled at 125Hz (they each have a local (micro) controller), but I do believe this text states that the ", " daemon itself generates joint setpoints at that rate (or at least: the URScript environment seems to use that time granularity when it comes to motion commands).", "2 . For the interpolation, do you mean that if samples are not synched, the UR box modifies the samples' value to match it with the value it \"should have\" at the time the sample is accounted for?", "Most (industrial) robots have something called an ", ", it is often the time between two successive iterations of the high(er) level motion control loop. For a UR, this is apparently run at 125 Hz. IIUC you'll get 1-on-1 execution of your TrajPts if you supply the controller with new points (in time and space) at every iteration (spaced out for the 8 ms period).", "Edit: the C-API headers document the 125 Hz frequency as well:", "#define ROBOT_CONTROLLER_FREQUENCY 125 /*  Hz  */", "Edit2: you might also like to read ", " by Morten Lind, Lars Tingelstad, and Johannes Schrimpf. It contains some more information on the lower level systems in the UR controller.", "This gives me a lot to chew on, thanks!", "Well, it's not really an answer to your question, more of a comment (that didn't fit in the comment box). I hope you get some responses to your other points.", "This is indeed not the final answer but it gives food for thoughts ...waiting for the actual answer ;)"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "This external PC generates trajectories for the arm", " ", " (via Ethernet): we want ", " ", " ", "You mean the low-level joint control (the PID or so) runs at 125Hz? That does not sound like much...", "For the interpolation, do you mean that if samples are not synched, the UR box modifies the samples' value to match it with the value it \"should have\" at the time the sample is accounted for?"], "answer_code": ["script_manual_en.pdf", "URControl"], "url": "https://answers.ros.org/question/203763/ur5ur10-typical-use-cases/"},
{"title": "odometry messages to C++ node silently dropped", "time": "2015-03-23 23:42:28 -0600", "post_content": [" ", " ", " ", " ", "I'm sure this is something silly, but I'm stumped and would really appreciate some help.", "I was writing a node that needs to consume odometry messages, and I just couldn't get it to actually consume the messages.  (By that I mean that I can ", " a message and a ", " will show that it has been sent, but my subscriber callback method isn't called.)  I have trimmed my code down to a small ROS package and posted it on GitHub ( ", " ), but I will also post it below. ", "The problem seems to be related to ", " messages somehow.  If I modify the code to consume ", " messages instead, then it works.  ", "QuinticControl.h:", "QuinticControl.cpp:", "main.cpp:", "This is my ", " command:", "EDIT 1", "\nThis is the output of ", ":", "Here is the version using ", " that works:", "QuinticControl.h:", "QuinticControl.cpp:", "And the ", " command:", "When that is run, the \"updateOdometry() called\" message is written to the console.  You can see that I have only modified the callback method signature and the ", ".  Unless there's a problem with the way I'm using ", "?", "I have verified the works-with-strings/doesn't-work-with-odometry-messages on a second computer running Indigo.  Unfortunately, I don't have easy access to a second computer running Hydro.", "can you verify using ", " your node really subscribes to the correct topic ", " ?", "Good question.  I have edited my question, adding that and other information.  Thank you!", "Are you sure your message is not malformed via the YAML file?", "I don't think ", " will publish a malformed message, and I am able to ", " and ", " the message (using Indigo) without errors or warnings.  I have also confirmed again that all of this, including the C++ code, works with Hydro.", "Did you copy you yaml file from your hydro PC? Maybe the definition of the nav_msgs::Odometry changed between both versions??", "I did.  I created it manually there, and used the error messages from ", " to tell me when it was malformed.  It seems like the message is okay though given that ", " on Indigo has no problem sending or receiving it.", "rostopic does plot you the message even if it is outdated! Your really have to check the md5sum for being sure the message can be received by your subscriber.", "I believe that the MD5 sum would only matter if it was a bag file.  But ", " returns ", " on both Indigo and Hydro, so it doesn't look like the message format has changed anyways."], "answer": [" ", " ", "I used the rqt plugin \"Message Publisher\" to send ", " messages and they are received by your node perfectly fine.", "Looking at your yaml file it looks like it is incomplete. I used the rqt plugin to publish messages and \"record\" them with ", ". The resulting file works also fine when replayed with ", ".", "See my yaml file which contains the header information not available in yours:"], "question_code": ["rostopic pub ...", "rostopic echo ...", "nav_msgs/Odometry", "std_msgs/String", "#include \"nav_msgs/Odometry.h\"\n#include \"ros/ros.h\"\n\nnamespace aav_control {\n  class QuinticControl {\n  public:\n    void updateOdometry(const nav_msgs::Odometry::ConstPtr &odometry);\n  };\n}\n", "#include \"QuinticControl.h\"\n\nusing namespace aav_control;\n\nvoid QuinticControl::updateOdometry(const nav_msgs::Odometry::ConstPtr &odometry) {\n  fprintf(stderr, \"updateOdometry() called\\n\");\n}\n", "#include \"QuinticControl.h\"\n#include \"ros/ros.h\"\n\nint main(int argc, char **argv) {\n  ros::init(argc, argv, \"aav_control2\");\n  ros::NodeHandle node;\n  aav_control::QuinticControl control;\n  ros::Subscriber subscriber = node.subscribe(\n      \"odometry/filtered\",\n      1000,\n      &aav_control::QuinticControl::updateOdometry,\n      &control\n    );\n  ros::spin();\n  return 0;\n}\n", "rostopic", "rostopic pub /odometry/filtered nav_msgs/Odometry -f src/aav_control2/odom_.1_.5.yaml\n", "rosnode", "$ rosnode info /aav_control2 \n--------------------------------------------------------------------------------\nNode [/aav_control2]\nPublications: \n * /rosout [rosgraph_msgs/Log]\n\nSubscriptions: \n * /odometry/filtered [unknown type]\n\nServices: \n * /aav_control2/get_loggers\n * /aav_control2/set_logger_level\n\n\ncontacting node http://turbo:36683/ ...\nPid: 7592\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n", "std_msgs::String", "#include \"std_msgs/String.h\"\n#include \"ros/ros.h\"\n\nnamespace aav_control {\n  class QuinticControl {\n  public:\n    void updateOdometry(const std_msgs::String::ConstPtr &odometry);\n  };\n}\n", "#include \"QuinticControl.h\"\n\nusing namespace aav_control;\n\nvoid QuinticControl::updateOdometry(const std_msgs::String::ConstPtr &odometry) {\n  fprintf(stderr, \"updateOdometry() called\\n\");\n}\n", "rostopic", "rostopic pub /odometry/filtered std_msgs/String 'hello'\n", "#include", "rostopic", "rosnode info", "/odometry/filtered", "rostopic", "rostopic pub", "rostopic echo", "rostopic", "rostopic", "rosmsg md5 nav_msgs/Odometry", "cd5e73d190d741a2f92e81eda573aca7"], "answer_code": ["nav_msgs::Odometry", "rostopic echo", "rostopic pub", "header: \n  seq: 1\n  stamp: \n    secs: 0\n    nsecs: 0\n  frame_id: ''\nchild_frame_id: child_frame\npose: \n  pose: \n    position: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n    orientation: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n      w: 0.0\n  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\ntwist: \n  twist: \n    linear: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n    angular: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"], "url": "https://answers.ros.org/question/205741/odometry-messages-to-c-node-silently-dropped/"},
{"title": "How can I make a robot joints and links reset completely in gazebo from python?", "time": "2015-05-01 22:28:47 -0600", "post_content": [" ", " ", "I want to make an experiment in gazebo using an evolutive learning algorithm to find walking gaits in an hexapod robot. For this, I've built an hexapod consisting in a body and six legs controlled by three revolute joints to emulate servo motors. The experiment will consist in the following stages:", "Do develop this, I am programming in python to send the moving commands. I generate a list of the walking patterns I want to test and then I iterate on that list to try the patterns. After that, I measure the advance in x making a service call to", "And here comes the problem. To reset the model, I make a service call to ", "(I have also tried /gazebo/reset_world)", "and my robot model goes back to position <0,0,0> and rotation <0,0,0>. The problem is that after I make this service call, the joints states and the links are not reseted and this causes sometimes the model to go up fliying since the legs intersect with the floor model and the simulator processes this badly.", "I wanted to ask if there is some way to make a call to gazebo to reset the model's pose including its joints states so the robot can go back exactly to the state it was spawned in for all of its joints and links.", "By now, I'll reset joints positions by calling their respective topics before reseting the simulation, but I find this solution dirty and more time consuming than reseting the whole model with a single call (Time is important, since I have to make lots of iterations).", "Thanks in advance to help me with my first question in the forum :)", "Did you find any solution to this? I'm having similar problems...", "Did you find a way to solve this problem ?"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Spawn the hexapod model in Gazebo\n", "Generate a population of randomly generated gaits (lists of joint states) to send to the hexapod", "Pick a gait from the population and send each set of joint states in sequence to the respective topics in the hexapod", "Measure how much the hexapod advances for the given gait", "Use the gaits with the biggest advance achieved to build the next population", "Go back to step 2 until the algorithm reaches a limit of iterations and finds a gait that makes the hexapod walk properly"], "question_code": ["/gazebo/get_model_state\n", "/gazebo/reset_simulation\n"], "url": "https://answers.ros.org/question/208343/how-can-i-make-a-robot-joints-and-links-reset-completely-in-gazebo-from-python/"},
{"title": "RFID in localization ? [closed]", "time": "2015-05-19 10:12:48 -0600", "post_content": [" ", " ", "Hi folks,", "Does someone tested the improvement of localization of a mobile robot like turtlebot using RFID technology ?", "The AMCL method uses the kinect or laser sensor to localize the robot and I think that may be it's interesting to add RFID tags to improve the localization !", "Waiting for your answers.", "Best Regards.", "There's already lots of published research about this. I suggest you consult your local university library."], "answer": [], "url": "https://answers.ros.org/question/209508/rfid-in-localization/"},
{"title": "How to use a ROS package", "time": "2015-05-22 14:06:28 -0600", "post_content": [" ", " ", "I am very new to ROS, so here's a very general question.", "Suppose I have installed a ROS package that I now want to use. In order to use it, should I build my own ROS package, which depends on this package, and then run a node from within my package? Or should I just create a normal C++ project, with a CMakeLists.txt file which tells the project where to find the necessary header and library files? Or should I just create a standalone ROS node (can a node live outside a package...?)?", "Thanks!", "What do you mean exactly by \"using a ROS package\"?. Do you need to consume the C++ API provided by the package or do you need to communicate with a ROS node provided by a package through topics/services?"], "answer": [], "url": "https://answers.ros.org/question/209786/how-to-use-a-ros-package/"},
{"title": "Realtime PCL computation with Kinect", "time": "2015-06-05 05:14:57 -0600", "post_content": [" ", " ", "I am working on a realtime lightweight obstacle detection package. This package will help a mobile robot to sense obstacles.\nI use Kinect to get a point cloud of a scene.", "My algorithm is running in 0.2s\nThe conversion with pcl::fromROSMsg requires 0.060s\nThe PassThrough filter requires 0.1 s\nAnd some other filters.", "So the first two functions consumes a lot but looks quite simple to me.", "Is there any way to downsample the Kinect pointcloud with cheaper function? By setting some freenect parameters for example... Or do you have any advice to help me mqke it faster?", "I would like to go below 0.1s to have a good reactivity when the robot is moving", "Thank you", "I found people talking about way to downsample the cloud. It looks to not easy at all.\nFirst way is to downsample the cloud in the callback, manipulating the point cloud is CPU expensive as it contains 5 millions points.\nAn other way is to tweak the freenect/OpenNI driver to publish less points", " Here is the link  "], "answer": [], "url": "https://answers.ros.org/question/210646/realtime-pcl-computation-with-kinect/"},
{"title": "Getting Started with ROS", "time": "2015-07-17 17:55:14 -0600", "post_content": [" ", " ", " ", " ", "I've built a couple of small robots using the Arduino for 2 servo control in addition to a distance sensor.  I've also gone through the ROS tutorial using VM VirtualBox to get Ubuntu to run ROS.  The reason I want to develop using ROS is given the extensive libraries that exist for ROS. However, I've read that in order to run ROS reasonably, the Raspberry PI or BeagleBone may run a bit slow if one is using multiple processor-intensive libraries like machine computer vision.  Is there a microcontroller that is able to run intensive libraries or do you really need to use at minimum a laptop for a robot that needs a number of libraries like computer vision, learning, motor control, path planning, etc.?", "Also, I'd like to know what level of difficulty exists in trying to run libraries like computer vision, machine learning, etc.  Would I be better off 1) first building robots with the Raspberry PI to get more used to interfacing motor controllers and servos with the PI and to also get more experience with Ubuntu (I'm familiar with it enough to follow the tutorial and build an image)?  I'm pretty familiar with C++ but not Python.  Or, would you recommend 2) diving into ROS and trying to build simple robots with either the Raspberry PI or a VirtualBox?  ", "Edit: The TK1 looks very intriguing - what sounds like very high performance and very low power consumption.  Would the TK1 be used only for computer vision or as a standalone microcontroller?  Or, would you recommend adding the TK1 to another microcontroller that would handle other processing?  If you recommend adding the TK1 to another microcontroller, which one would you recommend and how would you join the 2 together to communicate?", "Also, are there any rules of thumb for how much processing and RAM various libraries typically use - like computer vision, path planning, learning, etc.? ", ": I've moved your answer (which was more a comment) to your OP. Here on ROS Answers we typically only use answers to answer the question, not to post follow-up questions, or comment on other answers. Not a big deal, but would be nice to keep in mind."], "answer": [" ", " ", "Hi", "We are using a raspberry pi for our robot for running ROS, but the motor control stuff we are executing externally on a second microcontroller and controlling it as slave over SPI because of the realtime aspects of motor control, otherwise you would need to add realtime kernel patches and control the motor directly from the raspberry pie. When looking at its quite weak performance, I would definitely not recommend this. And as you already guessed the raspberry pie is not really good for machine computer vision.", "I guess there are some ARMs of manufacturers like used in mobile phones, but the probability to get them for your project, are quite low, well if you do not offer a perspective for selling a lot of chips with your product.", "Doing both, machine vision algorithms together with motor control on the raspberrypi ... well ... even with really optimized software ... I guess it will not work out. ", "And to your question about recommendation... \nGetting ROS run on the rasperrypi is also not so nice then just installing it in a VM. Also it takes a long time to startup nodes on the rasperrypi. Some code also does not work on ARM ...", "I would recommend starting to use functions of ROS in your VM or better directly on your computer because of RViz, and Gazebo (if you want to simulate robots).", "Regards,", "Christian", "so you think the custom robots like turtle bot or jakal or any others.... do you think they put motor control in a separate mcu than the ros one? ", "btw, do they use raspberry pi or their own hardware? I didn't see a source for designing custom hardware to run ubuntu and ros on the internet.", " ", " ", "You can check out the Nvidia Jetson development kit:", " "], "url": "https://answers.ros.org/question/214026/getting-started-with-ros/"},
{"title": "depthimage_to_laserscan outputs laserscan upside down", "time": "2015-09-02 14:25:18 -0600", "post_content": [" ", " ", "Hi everyone, Im having trouble with depthimage_to_laserscan, its essentially upside down, if i try rotate the TF by 180 degrees the map is offset from the laserscan? is there anyway to flip the depthimage_to_laserscan? ", "Did you try to flip your depth image?", "How is that done?", "Use OpenCV function ", " :  ", " See this post for details  ", "I know there is a way with OpenCV but this would consume much more processing power flipping each depth image?", "This will not affect performance at all if you are working on a desktop or on a laptop.", "The other way to do image processing is by means of ", " but I did not find a method for image flipping. Or you can try ", " package  ", "  but it does the same opencv operations. ", "The depth camera is on a Raspberry Pi.I will try that, thanks for the help."], "answer": [], "question_code": ["flip", "image_transport", "image_rotate"], "url": "https://answers.ros.org/question/216983/depthimage_to_laserscan-outputs-laserscan-upside-down/"},
{"title": "Read the current of the schunk lwa4p", "time": "2015-10-06 07:52:53 -0600", "post_content": [" ", " ", "Hi!,", "For my project we will want to control the pg70 gripper of schunk using the current it is consuming. I know that using the schunk motion software (", ") pag 61 you could be able to read it through the CANOpen, however does anyone knows if that is already implemented in the schunk_robot ros package or anywhere else?"], "answer": [], "url": "https://answers.ros.org/question/218709/read-the-current-of-the-schunk-lwa4p/"},
{"title": "what should be my recommended hardware architecture?", "time": "2015-11-30 12:02:26 -0600", "post_content": [" ", " ", "I'm essentially trying to build a robot platform (similar to Turtle) from scratch. part of the reason is that Turtle started many years ago, and after that, hardware technology have progressed a lot (multiple powerful ARM chips appeared , etc).  my understanding is that Turtle basically uses a small desktop computer with a miniATX motherboard, sort of. ", "at first I tried to do all the processing on an Arduino, but then found that Arduino (at least UNO) is way too simple, doesn't have a competent OS, hence possibly can't run the full C++ code in which ROS is written in. \nArduino TRE does run Linux, and is a small form-factor and consumes just 5v power supply to be very light. but I am still not sure if it's powerful enough to run complex algorithms such as video feature extraction and then SLAM.", "another attempt is to run an Arduino board solely for the purpose of interacting with sensor data collection, and sending out actuator commands, and then communicate all the in/out data with another computer . the latter may be an Android phone, or a small desktop. but using a desktop requires me to provide a big power supply (how is the Turtle getting its power?)", "Thanks\nYang"], "answer": [], "url": "https://answers.ros.org/question/221765/what-should-be-my-recommended-hardware-architecture/"},
{"title": "import workcell/ geometry from ABB RobotStudio into MoveIt/ RViz", "time": "2016-01-18 13:57:37 -0600", "post_content": [" ", " ", " ", " ", "I am working with ABB RobotStudio since a longer time and currently started working with MoveIt/ RViz. Importing and arranging geometry, especially the whole work cell into RViz is very time consuming.", "Has someone experiences with both RobotStudio and RViz and has some tips/ a tool chain to quickly import/ arrange geometry from RobotStudio to RViz?", "Edit: I am looking for something similar to ", "."], "answer": [], "url": "https://answers.ros.org/question/224405/import-workcell-geometry-from-abb-robotstudio-into-moveit-rviz/"},
{"title": "Making a 2D costmap from a .world", "time": "2016-02-03 14:05:13 -0600", "post_content": [" ", " ", "I have a .world file with some walls and other simple obstacles in it. I need a 2D costmap in order to use move_base and plan paths for my quadrotors. In the past I have been able to do this by using a SLAM package (gmapping) and flying my quadrotor all around the world to generate a good map. This is incredibly time consuming. I was wondering if there are any people out there much smarter than me who have developed a way of going straight from a .world file, giving it a specific height (eg. if there are mountains, only count them as obstacles if they are above 10 ft since I only fly above 10 ft), and generating a 2D costmap. A 3D would be pretty sweet as well. If anyone has any ideas please let me know. Thanks!", "Did you find an efficient way to automatically generate a 2D cost map given a .world file?"], "answer": [], "url": "https://answers.ros.org/question/225800/making-a-2d-costmap-from-a-world/"},
{"title": "Why do most ROS image nodes lack support for CompressedImage?", "time": "2016-03-18 13:02:41 -0600", "post_content": [" ", " ", "I'm trying to setup a simple mjpeg stream from a webcam, and I'm finding most of the common image processing packages provide no support for CompressedImage. Why is this?", "For example, neither the deprecated mjpeg_server nor recommended web_video_server nodes supported compression as input, causing them to consume massive amounts of CPU. Even basic transformation nodes like image_rotate only supports the uncompressed Image type.", "I'm finding I have to write a lot of ROS nodes from scratch...just to process Jpegs. Am I missing something? Why does the ROS community eschew efficient compression? I get there's some minor loss in quality with compression, but I'm seeing a 10-fold difference in CPU consumption between the compressed and uncompressed image streams. On mobile platforms with limited resources, that's a huge problem.", "Do those packages use the image_transport library? If they do, they should support compression through ", "Have you tried requesting compressed transport by setting the ", " parameter to ", " for each node in question?"], "answer": [" ", " ", "For mjpeg_server and web_video_server you are right, here using a compressed input can help reducing CPU usage as the nodes would no longer have to compress the stream internally before streaming to the browser.", " However, in general this is not the case for nodes doing image processing. For example the transformation node internally needs the raw image data in order for performing the image transformation (E. g. see  ", " ). I e. if you would require it to use compressed streams and input and output it would have to decompress at the input and compress again at the output --> likely much higher CPU usage for a slightly reduced data traffic. And it comes worth: Think of an image processing pipeline --> each processing step element would have to decompress at beginning and compress when the image leaves. Dramatic CPU usage and moveover the image quality does not decrease ones but step by step decreases while passing through the chain. ", "Hence, most of the times it is better to use the uncompressed stream for processing chains and only compress the image ones, directly before passing it through the network.", "I was thinking about CPU optimization and came to the same conlusion as you. However, i don't understand why a simple usage of image_rotate is so ineficient in terms of CPU."], "question_code": ["~image_transport", "compressed"], "url": "https://answers.ros.org/question/229515/why-do-most-ros-image-nodes-lack-support-for-compressedimage/"},
{"title": "What is the features of Flexgui-Industrial for Ros?", "time": "2016-04-05 23:30:19 -0600", "post_content": [" ", " ", "I want to know Flexgui is open source or not and What are all the features we can use in the Flexgui.", " The license listed here:  ", "  is the Apache License, which is generally friendly to business users. DISCLAIMER: I am not a lawyer. As usual with open-source licenses, consult a lawyer before using any open-source software for business purposes. "], "answer": [" ", " ", "It is on Github so obviously (partly) open source...\nAnd ", " away from the Github repository, you get all the features available for different versions and prices.", "Thank you for your support. The link you sent is really helpful . I need some more assistance from you. I hope you will help us in it.\nBy using open source version, \n\u2022   Can I create, edit and save the program?\n\u2022   How the program editor environment in scripting which is indicated as open source?", "check this ", ". Apache license is explained with plain English.", " ", " ", "As ", " and ", " already told you, yes, parts of FlexGUI are open-source, but not everything. The parts that are can be found on the ros-industrial github organisation.", "You might be interested in a ", " that was given by one of the developers at the ROS-Industrial Community Meeting last February."], "url": "https://answers.ros.org/question/231133/what-is-the-features-of-flexgui-industrial-for-ros/"},
{"title": "Roslaunch blocking message publication", "time": "2016-03-16 00:59:07 -0600", "post_content": [" ", " ", "I have two simple Python nodes, a publisher outputting data at /mypub/data and a subscriber consuming that data.", "When I run them using rosrun, everything works perfectly, but when I launch each of them using roslaunch, the subscriber never receives any topics being output by the publisher.", "Logging inside the publisher shows messages are being published and ", " shows my message type has been generated, but when my publisher is running and publishing, ", " shows nothing and appears to hang.", "My publisher's launch file is trivial:", "And the node seems to start up cleanly with the output:", "If I start it instead with rosrun, like:", "it again seems to start cleanly, with output:", "and then ", " successfully shows the messages.", "What's the difference between these two calls? Why is roslaunch preventing messages from being visible outside the node?", "Do you see your topic with ", " ? Is your node doing anything other than publishing?"], "answer": [" ", " ", "The problem was that my node was initializing itself with the name \"MyPub\", whereas in the launch file I was using \"mypub\". I didn't realize the XML name overrode the node's internal name or that it was case sensitive. After I made them the same, everything started working."], "question_code": ["rosmsg list|grep -i mypackage", "rostopic echo /mypub/data", "<launch>\n   <node name=\"mypub\" pkg=\"mypackage\" type=\"mypub_node.py\" output=\"screen\">\n   </node>\n</launch>\n", "started roslaunch server http://localhost:60558/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro: indigo\n * /rosversion: 1.11.16\n\nNODES\n  /\n    mypub (mypackage/mypub_node.py)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[mypub-1]: started with pid [12609]\n[DEBUG] [WallTime: 1458106329.532694] init_node, name[/mypub], pid[12609]\n[DEBUG] [WallTime: 1458106329.535395] binding to 0.0.0.0 0\n[DEBUG] [WallTime: 1458106329.537606] bound to 0.0.0.0 49259\n[DEBUG] [WallTime: 1458106329.540770] ... service URL is rosrpc://localhost:49259\n[DEBUG] [WallTime: 1458106329.542789] [/mypub/get_loggers]: new Service instance\n[DEBUG] [WallTime: 1458106329.554825] ... service URL is rosrpc://localhost:49259\n[DEBUG] [WallTime: 1458106329.556568] [/mypub/set_logger_level]: new Service instance\n  self.packet_pub = rospy.Publisher('~data', msgs.Data)\n[DEBUG] [WallTime: 1458106329.639749] ... service URL is rosrpc://localhost:49259\n", "rosrun mypackage mypub_node.py\n", "[DEBUG] [WallTime: 1458107532.209104] init_node, name[/mypub], pid[13349]\n[DEBUG] [WallTime: 1458107532.211273] binding to 0.0.0.0 0\n[DEBUG] [WallTime: 1458107532.212995] bound to 0.0.0.0 37078\n[DEBUG] [WallTime: 1458107532.215700] ... service URL is rosrpc://localhost:37078\n[DEBUG] [WallTime: 1458107532.217320] [/mypub/get_loggers]: new Service instance\n[DEBUG] [WallTime: 1458107532.228779] ... service URL is rosrpc://localhost:37078\n[DEBUG] [WallTime: 1458107532.230445] [/mypub/set_logger_level]: new Service instance\n[DEBUG] [WallTime: 1458107532.320980] ... service URL is rosrpc://localhost:37078\n[DEBUG] [WallTime: 1458107532.323380] [/mypub/packet_write]: new Service instance\n", "rostopic echo /mypub/data", "rostopic list"], "url": "https://answers.ros.org/question/229189/roslaunch-blocking-message-publication/"},
{"title": "Alternative to diagnostic_aggregator and robot_monitor?", "time": "2016-03-28 02:25:37 -0600", "post_content": [" ", " ", " ", " ", "Firstly, my first post, hello everyone!", "I have been working with /diagnostics topic recently. Mostly for urg_node and mavros. I was wondering if there is any reasonable, maintained alternative to diagnostic_aggregator and robot_monitor? Basically I need a very simile GUI or terminal app that will consume /diagnostics topic, check names/keys/values against provided template and display overall status information. I know about rqt_runtime_monitor but it doesn't quite do what I need, it just displays whatever comes in.", "Thanks\nMarcin", "Don't those nodes provide exactly that? What are their shortcomings that necessitates an alternative?", "rqt_robot_monitor does exactly that"], "answer": [], "url": "https://answers.ros.org/question/230297/alternative-to-diagnostic_aggregator-and-robot_monitor/"},
{"title": "Python 3 in ROS indigo", "time": "2015-05-18 07:21:57 -0600", "post_content": [" ", " ", "I have a script in python 3, it reads data from a serial port, do some processing and return some integer and float values. This script manipulates bytes in a way that it will be time consuming for me to adapt it to python2. Is there a way to run python3 script in ROS Indigo? I dont use any ros package in python, only external libraries like pyserial.", "I try to install python3-pkg, but the package remove many(if not all) of my ros packages and do not install any substitutes.", "Thanks,", "Iuro Nascimento"], "answer": [" ", " ", " ", " ", "I don't know why the apt installation of rospkg removes existing packages, but the same happened to me. What works, actually, is to create a virtual machine with python3 interpreter (", ", for example) and under it run ", ").", "(additionally, you'd have to run ", ", in order to run python3 scripts. I can confirm this approach works for basic ROS functionality - establishing nodes, publishing and subscribing to topics etc.However, there are packages that don't work with python3. In order to combine both - scripts that have to use python3 and those that have to use python2, I define in the shebang the interpreter to use, e.g. ", ")", "The problem is that many ROS Python packages install different versions under the same file name, so the ", " package has to \"Conflict\" with the corresponding ", ".", "I followed these steps but couldn't make it work. I posted my question ", ". Could you please write down these steps from the beginning? Do I just need to run these two commands ", " and under ", " run ", "?", " ", " ", "I had the same Problem with Python 3 . The easiest way is to use Python 2.7.  instead, or if your really need 3 use virtualenv as mentioned above. I migrated my .py files to pyhon2 and there were nearly no changes necessary. Additionaly i was able to use other packages, wich are still only aviable for Python 2. "], "answer_code": ["python3-foo-bar", "python-foo-bar", "virtualenv -p /usr/bin/python3 myenv", "myenv", "pip install rospkg"], "url": "https://answers.ros.org/question/209417/python-3-in-ros-indigo/"},
{"title": "Unreliable subscribers based on launch order", "time": "2016-04-23 13:46:46 -0600", "post_content": [" ", " ", "How do you ensure your node subscribes to a topic? I'm finding launch order effects if a node is able to successfully subscribe.", "For example, say node A publishes /A/topic, and node B consumes it.", "I found that if I launch A, then B...B never receives any messages from A even though I can see A's messages with ", ". If I kill A and restart it, or launch it after B, then B receives the messages.", "Is this a bug in ROS or am I misunderstanding how ROS messages work? I would expect a node to subscribe and receive messages on a topic regardless of when it was launched relative to the publisher. As long as both nodes are running, the launch order shouldn't matter.", "The docs say launch order shouldn't matter, but this doesn't seem to be the case. Why is this?", "This even seems to effect ", ". If you call it after the publisher, it can receive messages, but if you kill and restart the publisher, ", " shows nothing until it too is restarted.", "I've found this is causing some very obscure and difficult to diagnose bugs. The only workaround I've found is to kill every single node in my system and restart...which seems hacky and inefficient.", "Do you have a dedicated terminal window running a ROS core, or are you relying on roslaunch to bring up a core?", "I'm using roslaunch."], "answer": [" ", " ", "When the ROS core goes down, any running nodes with topic subscribers or publishers will have those topic connections terminated.  They also will not automatically reconnect when a new core comes up, which is probably why you have to restart to the get the topics working again.", "Therefore, I think if you always leave a terminal window/tab running a ROS core, your problems will go away.  Another benefit of this is that any parameters that are set will continue to exist after stopping the launch file."], "question_code": ["rostopic echo /A/topic", "rostopic echo", "rostopic echo"], "url": "https://answers.ros.org/question/232702/unreliable-subscribers-based-on-launch-order/"},
{"title": "Database or bag with costmap/occupancy grid", "time": "2016-05-06 04:03:36 -0600", "post_content": [" ", " ", "Hello,\ndo you know a database or a collection of bag files with costmap/occupancy grid?\nI need to test some algorithms on different maps, and it's too expensive and time consuming to build my own maps.\nI searched a lot and I wasn't able to find it.\nThanks"], "answer": [" ", " ", "Try the Husky Nav stack tutorials -- they are extremely well documented and should have everything you need including SLAM and CostMap2D."], "url": "https://answers.ros.org/question/233766/database-or-bag-with-costmapoccupancy-grid/"},
{"title": "Subscribing /map consumes 4gb of memory!?!?!", "time": "2016-05-06 11:32:32 -0600", "post_content": [" ", " ", " ", " ", "I have created a python node that just subscribes /map from gmapping. Callback function does nothing, only returns null. By starting this node I can see an increase of around 4gb in my ram. I know each /map message have around 100mb.", "Can anyone tell me if this is normal behaviour? ", "[EDIT]:", "I also have realized that running ", " will consume even more memory ending up crashing the system...", "[EDIT 2]:", "I\u2019ve tried to decrease map resolution from 0.01 to 0.025 and now it works fine.", "Thanks", "Could you please provide the source code of your node?", "Please provide the code. There can be many reasons for this. A very straight forward one would be you storing each map message in your memory, growing it very fast.", "What is the input source (into gmapping) frequency? I.e. your laser scanner, etc."], "answer": [], "question_code": ["import rospy\nfrom nav_msgs.msg import OccupancyGrid\n\ndef callback(data):\n    return None\n\nif __name__ == '__main__':\n    rospy.init_node('test_node')\n    rospy.Subscriber(\"/map\", OccupancyGrid, callback)\n    rospy.spin()\n", "rostopic echo /map"], "url": "https://answers.ros.org/question/233789/subscribing-map-consumes-4gb-of-memory/"},
{"title": "How to get the battery status of a robot in gazebo?", "time": "2016-06-14 04:03:45 -0600", "post_content": [" ", " ", "I have 2 Husky robots in Gazebo world moving around. I need to get their battery status like how much battery are they consuming in different tasks? I am using ROS-Indigo with Gazebo-2.2.2 on Ubuntu 14.04.", "I tried with PR2 robot as well from  ", " , but I am not able to understand how to use these. Also, I got PR2 dashboard running but it show \"Battery : stale\"."], "answer": [], "url": "https://answers.ros.org/question/236888/how-to-get-the-battery-status-of-a-robot-in-gazebo/"},
{"title": "ROS Concept", "time": "2016-08-07 21:45:01 -0600", "post_content": [" ", " ", "I refer to this ", "Under \"topics\", It says that", "In general, publishers and subscribers are ", ". The idea is to ", " the production of information from its consumption. ", "BUT it also says that", "Nodes ", "; the Master only provides lookup information, much like a DNS server. Nodes that subscribe to a topic will request connections from nodes that publish that topic, and will establish that connection over an agreed upon connection protocol.", "I don't understand. If publisher -subscribers not aware of each other existence, how come they connect to each other directly. So confused.", "Doesn't it say that the idea is to decouple the production of info and its consumption? Connecting node to node directly doesn't sound like decoupling though.", "Please explain, and elaborate.Thanks"], "answer": [" ", " ", "Basically,", "The publishers will publish their messages in the topics (a Topic can be seen as a Facebook Page)", "The subscribers (Facebook Page Followers) can read the status of the publishers by reading information directly from the topics (so they connect to Facebook Pages of the publisher directly and read the status)", "If the topic is shutdown, the subscribers can't read the messages (in other words if the publisher desactivate his Facebook Page, you will not be able read the messages, even if the Facebook Server which is the ROS Master is still running).", "But, when you connect to the Facebook Page of the publisher, he don't know that you're reading his status. And we assume that you can not write a comment. So, There's not direct communication between you (the subscriber) and the publisher, because the communication in one-way only. ", "In other words, the publisher writes messages in the topic, the subscriber reads the messages from the topic. And no one cares about the location of the other or his address. ", "But all of this is just an abstraction: in the low level communication of the machine, the nodes need to know all the network information the nodes to communicate between them. But, the subscribers don't want to bother themselves with the IP adresses and ports, and that's why, they contact the ROS Master which will redirect the messages to the correct node.", " ", " ", "I think this is a duplicate of "], "url": "https://answers.ros.org/question/241251/ros-concept/"},
{"title": "How can I use the teleoperation with iRobot Roomba 645 and ROS Indigo?", "time": "2016-09-04 19:18:10 -0600", "post_content": [" ", " ", " ", " ", "Hi!!!", " I'm new in ROS and I want to build a TurtleBot using iRobot 645 and Kinect with ROS Indigo. I've followed the tutorial  ", "  to do the teleoperation to begin, I've changed the hardware parameters but I get the next error: ", " The robot is power on, and the UST-TTL is working because I've used it to move the robot via serial communication using the iRobot Create 2 Open Interface (OI) Specification based on the iRobot Roomba 600 commands. Also I've tried to use this:  ", "  without results. ", "I'd thank you a lot if you could help me.", "Greetings!!!", "More Information:", "I've trying to connect ROS with this robot executing the next commands:", "Then it appears the error. By that I tried the create_autonomy driver. I followed the instructions to install it and when I try to execute it with this commands:", "The next message appears:", "I've using a PL2303 USB to Serial Converter and an Adafruit FTDI Serial TTL-232 USB Cable. I've tried with a baud rate of 19200 and 115200 and nothing. I want to create a map (SLAM) with this robot using ROS. I'm very new in this and I'd thank you a lot if you could tell me step by step how can I do it.", "Thanks and greetings!!!", "What were the specific issues with ", "?", "The command ", " assumes that you have created the workspace in your home directory.", "Try ", " (the square brackets signify optional parameters and should not be there literally, eg, ", " not ", ")", "Thank you by your answer Jacob!!!", "I tried your suggestion but the same error appears:", "[create2.launch] is neither a launch file in package [ca_driver] nor is [ca_driver] a launch file name", "That error means that you have either not properly sourced the workspace where you built the driver, or the driver was never successfully built. I would go through all the steps in ", " again and make sure you get no errors or suspicious warnings throughout the process.", "Thanks again!!!", "You're right. I solved the problem with the next commands:", "But now, I do not how to do SLAM using Gmapping (for example) with this driver and with a Kinect", "Greets", "I think turtlebot uses the package ", " to convert the Kinect data to a message consumable by ", ". I would read about these packages and if you still have trouble open a new question.", "Thank you!!!", "I'll read them.", "Greetings", "Hello!!!", "Excuse me Jacob, but I have some doubts about the create package:", "Thanks"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Yes, the workspace was created in my home directory.", "How do you obtain the odometry?", "How can I open the models of the ca_description carpet in Gazebo or in Rviz?"], "question_code": ["Failed to contact device with error: [Error reading from SCI port. No data.] Please check that the Create is powered on and the connector is plugged into Create\n", " export TURTLEBOT_BASE=create\n export TURTLEBOT_STACKS=circles\n export TURTLEBOT_SERIAL_PORT=/dev/ttyUSB0\n roslaunch turtlebot_bringup minimal.launch\n", " source ~/create_ws/devel/setup.bash\n roslaunch ca_driver create_2.launch [desc:=true] [publish_tf:=true]\n", "[create_2.launch] is neither a launch file in package [ca_driver] nor is [ca_driver] a launch file name\nThe traceback for the exception was written to the log file.\n", "source ~/create_ws/devel/setup.bash", "roslaunch ca_driver create_2.launch", "desc:=true", "[desc:=true]", "cd ~/create_ws/\nsource ~/create_ws/devel/setup.bash\ncatkin_make --source ~/create_ws/src/create_autonomy/\n"], "url": "https://answers.ros.org/question/243085/how-can-i-use-the-teleoperation-with-irobot-roomba-645-and-ros-indigo/"},
{"title": "How to profile message activity?", "time": "2016-10-13 09:49:29 -0600", "post_content": [" ", " ", "Are there any tools for profiling or collecting statistics on the types of message activity per node?", "I'm trying to troubleshoot some nodes consuming excessive amounts of CPU, and to help diagnose the problem, I'd like to get a histogram of the types of messages they publish over a given time range.", "e.g. If I find something like:", "then I'll immediately know it's publish rate for IMU messages is too high.", "Obviously, I could do this manually by checking every single topic, but it's be nice to have a tool to automate this, and I don't want to reinvent the wheel."], "answer": [" ", " ", " ", " ", "Since _ROS Indigo_ there is builtin support for topic statistics in ROS. Based on ", ", these are the metrics that are collected:", "To enable this feature, the ", " parameter needs to be set to ", ".", "There are some tools that enable higher level analysis, visualisation and diagnostics based on topic statistics:", "There is also a tool called ", " that collects fine-grained statistics about hosts, processes and network sockets that are part of a running ROS computation graph independent of the builtin data from ROS topic statistics. (", " I am the author of ", " and the documentation is far from perfect at the moment).", "When you say set \"/enable_statistics\" to true, on which node? All of them?", "The ", " link explains how to set the ", " parameter. It can also be done in a launch file similar to ", ".", " ", " ", "If you create a bag file then do ", " it will show you how many messages occur on each topic."], "answer_details": ["Period of messages by all publishers (average, maximum, standard deviation)", "Age of messages, based on header timestamp (average, maximum, standard deviation)", "Number of dropped messages", "Traffic volume (Bytes)", " (Visualisation of topic statistics)", " (Profiler and Visualisation based on topic statistics)", " (Visualisation and Fault Detection/Mitigation based on topic statistics and a custom process and host level monitor)", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["topic                     messages/min\n/mynode/encoders          23\n/mynode/button            1\n/mynode/imu               239423847887\n/mynode/gps               56\n"], "answer_code": ["/enable_statistics", "true", "/enable_statistics", "/use_sim_time", "rosbag info"], "url": "https://answers.ros.org/question/245636/how-to-profile-message-activity/"},
{"title": "How to echo to multiple topics at once", "time": "2016-10-13 09:42:24 -0600", "post_content": [" ", " ", "Is it possible to use rostopic to echo multiple topics at once?", "I have a node that's consuming a lot of CPU, and I'm not sure why. It publishes a dozen topics, so to help debug, I tried to do:", "but of course, rostopic told me the topic \"/mynode/*\" doesn't exist."], "answer": [" ", " ", "You can open multiple terminal windows/tabs and use ", " in each one, or you spawn multiple copies of rostopic, all echoing to the console.", "But you might find it more useful to use ", " or ", " to see the publish rate or bandwidth for some of these topics. It may be more likely that your main loop is publishing faster than you need. Are you using a ", " to publish at regular intervals, as in ", "? (For the C++ code counterpart ", ".)"], "question_code": ["rostopic echo /mynode/*\n"], "answer_code": ["rostopic echo", "rostopic echo /nodename/topic1 & rostopic echo /nodename/topic2 & rostopic echo /nodename/topic3 & ...\n", "rostopic hz", "rostopic bw", "rospy.Rate"], "url": "https://answers.ros.org/question/245635/how-to-echo-to-multiple-topics-at-once/"},
{"title": "Consume a service from a Master node", "time": "2016-10-21 12:17:45 -0600", "post_content": [" ", " ", " ", " ", "Hello!", "I want to consume a service running in a computer from another computer in the same network. Is this possible?", "I have set up ROS_MASTER_URI correctly on both, I know its correct since I was able to do the talker/listener example from the tutorials. ", "If I run roscore and the service from the basic tutorial:", "and this on the client computer:", "I get nothing. ", "Does anybody know what could be wrong?", "Thanks in advance.", "EDIT: One runs the server and one runs the client, I do not have the server running on both computers.", "You should only need one copy of the server node; why are you running it on both computers?", "Just to clarify: ", " asks why you run ", " on both your server and your client. You write: \"I get nothing\", but that makes sense, as the servers will just sit there, waiting for a client to connect, which you don't start.", "Im sorry, I typed in both the same script, but one of those should be running a client. I edited my post to address this. Thanks.", "you need to specify the input arguments : the two numbers that you're going to add, when calling the client."], "answer": [], "question_code": ["rosrun beginner_tutorials add_two_ints_server.py\n", "rosrun beginner_tutorials add_two_ints_client.py\n", "add_two_ints_server.py"], "url": "https://answers.ros.org/question/246184/consume-a-service-from-a-master-node/"},
{"title": "Node not running at constant frequency", "time": "2016-11-15 16:44:58 -0600", "post_content": [" ", " ", "Hi there :)", " I've got a little problem running ROS on two machines and I don't really know how to find out what could cause the problem:\nSetup: I'm running nodes on two machines at different but constant frequencies (e.g. IMU at 50 Hz, controller at 10 Hz, ...). Most of the time it works perfectly well but just sometimes it seems that all nodes stop running for short instants (see plot of IMU measurements here:  ", " ). You can see in the plot that it's running well until about 10.5 seconds, then slower and then there's a pretty long break of no data being published/received.\nI'm suspecting two problems: ", "Have you ever observed a similar problem or do you know what could cause it? And if not - which tools or methods would you suggest to find the problem?", "Any help will be much appreciated, thank you very much in advance!", "Best,\nMax", "Are you having multiple callbacks (either subscriber/service callback) in the single-threaded node? Also, check for any ", " in your callback, it might block the main thread and prevent other callback to be triggered", "Any details on how the two nodes are connected to one another? e.g. Wifi is known to create lags.", "I do have multiple callbacks in almost all nodes (7 nodes and up to 5 callbacks) but there are no sleep or other time consuming functions in the callbacks.\nThe nodes are connected by ssh over Wifi so this might be the reason. I also found out that the machines are properly synchronized.", "Assuming the wifi is indeed the problem: Do you know any tricks to make the connection more reliable? Both in-ROS (maybe decrease number of topics/messages) and 'outside' of ROS?", "I had the problem that bad reception with a lot of traffic ended in low throughput (Low Bit Rate in iwconfig).\nFor my robot a bigger antenna helped.", "Provided your application allows for it (can cope with lost msgs) and your msgs aren't too large (smaller than a datagram) you could try and see whether ", " works better. See ", " for info.", "Personally I never understood the transport hints. Does someone really need to patch all nodes for UDP usage? I'd wish for a global setting...", "Btw we are indeed not sure yet if Wifi is the problem. I'ld recommend to check if problems also occur with Ethernet."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Wrong synchronization of the machines (I'm using chrony but I can't guarantee that everything is set up perfectly right)", "Too high CPU workload which does not allow all nodes to be executed (which would be weird since it's running fine at other times)"], "question_code": ["sleep", "UDPROS"], "url": "https://answers.ros.org/question/248196/node-not-running-at-constant-frequency/"},
{"title": "ros_canopen with lwa4d initialization fails", "time": "2016-12-23 01:23:39 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I am quite new to ROS. My intention is to use ROS_CANopen package to connect with my robot (Maxon EPOS2 motor), but I got the failure during initializing the ", ". ", "Here is my hardware configuration:", "When initializing...", "the failure as follows.", "after run roslaunch schunk_lwa4b robot.launch, the log is shown as follows", "Note: I already set the txqueuelen 20.", "From error code showed that 1003sub0, is that possible Epos studio was generated incorrect DCF ?  Does someone meet this failure before?", " \nI modified some configuration in epos studio and generated a new DCF. the results are different as follows.\nWhen initializing rosservice, the failure as follows.", "after run roslaunch schunk_lwa4b robot.launch, the log is shown as follows.", "Your new DCF specifies a ParameterValue (0x00) for 2303sub3, but your controller does not like it.\n", " sends SDOs for all non-default parameters at start-up. Please consult the EPOS manual to find out in which cases 2303sub3 (and 1003) can be written.", "I commented some objectdict in my DCF file and run rosservice init. I still observed one error, 60c1sub1, which is not listed in EPOS manual. and I also consulted Drive operation modes, it's shown Interpolated Position. but EPOS manual doesn't mention 60C1 item. Could you plz advise for this issue?", "log and details listed on above ", " , thanks for your time and the reply", "Maxon uses 0x20C1 instead of 0x60C1, not sure why..\nThere is not config option for the object to use, so you have to subclass the 402 layer to make it work.\nOr just use another mode..\n(Please ask a new question if you really want to subclass)", "You probably want to use Maxon-specific Postion Mode (-1).", " Or  ", "Did you mean I should not comment some objectdict in my DCF file eg. 2303sub3 (analog position setpoint notation index) or each of ", " should be included and make it work? I will ask a new question, thanks for the reply.", "For my understanding, epos_hardware only supports USB and rs232. My intention is to use CANopen to communicate with my robot. Is ros_canopen package the only one solution for my case? If yes, I will try to let it work on my robot. Thank you for the reply.", "Maxon does not support standard IP mode, it uses setpoint (pos, vel & time) with 0x20C1.", "You could implement this mode (difficult) or you could implement mode -1 (looks almost like IP mode).", "If you do not need full trajectories, you can uses PP mode. Or you can use a PID controller with Pv mode."], "answer": [" ", " ", "abort1003#0, reason: Invalid value for parameter (download only).", "ros_canopen tries to reset the error counter on initialization.\nThere might be a little glitch because the code just checks if the object is available, but does not check if it's writable.\nIt seems like it is marked writable in your EDS/DCF.\nCan you upload it somewhere? gist would be best.", "sudo ip link set can0 txqueuelen 20", "Don't do this if you have a single axis!\nThis is just required (and recommended) if you  have more than 4 nodes on one CAN bus.", "Thank you for your reply,  should I just upload my DCF, or it's better to upload the entire lwa4d project which I modified some part of configuration?", "Why should I set txqueuelen to 20 if I have more than 4 nodes on one CANbus ? If I don't set one, it will be more possible to lose packages ? thanks for your reply", "I set txqueuelen to 10 as a single axis config, it still failed to initial package.", " sends synchronous PDOs in bursts, so the socketcan queue might overrun and the driver stops working.\n( ", " ) "], "question_details": [" ", " ", " ", " ", " ", " ", "A USB-to-CAN compact is used to communicate between my laptop and the robot. The Linux driver called IXXAT socketcan driver. ", "The robot uses Maxon EPOS2 with CANopen. DCF file is generated from EPOS studio."], "question_code": ["rosservice call /arm/driver/init\n", "success: False\nmessage: /home/cj/catkin_ws/src/ros_canopen/canopen_master/src/sdo.cpp(429): Throw in function void canopen::SDOClient::transmitAndWait(const canopen::ObjectDict::Entry&, const canopen::String&, canopen::String*)\nDynamic exception type: boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<canopen::TimeoutException> >\nstd::exception::what: SDO\n[canopen::tag_objectdict_key*] = 1003sub0\n", "auto-starting new master\nprocess[master]: started with pid [1962]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to a2e27e66-c8d0-11e6-ba1f-005056c00001\nprocess[rosout-1]: started with pid [1998]\nstarted core service [/rosout]\nprocess[arm/robot_state_publisher-2]: started with pid [2013]\nprocess[joint_state_publisher-3]: started with pid [2015]\nprocess[arm/driver-4]: started with pid [2023]\nprocess[arm/arm_controller_spawner-5]: started with pid [2034]\nprocess[topic_transf-6]: started with pid [2055]\n[ INFO] [1482470911.684654700]: Using fixed control period: 0.010000000\n[ INFO] [1482470936.775348397]: Initializing XXX\n[ INFO] [1482470936.775647194]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1482470936.775941459]: Current state: 2 device error: system:0 internal_error: 0 (OK)\nabort1003#0, reason: Invalid value for parameter (download only).\nCould not process message\n[ INFO] [1482470936.838776231]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1482470936.838828278]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n", "sudo ip link set can0 txqueuelen 20\n", "success: False\nmessage: /home/cj/catkin_ws/src/ros_canopen/canopen_master/src/sdo.cpp(429): Throw in function void canopen::SDOClient::transmitAndWait(const canopen::ObjectDict::Entry&, const canopen::String&, canopen::String*)\nDynamic exception type: boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<canopen::TimeoutException> >\nstd::exception::what: SDO\n[canopen::tag_objectdict_key*] = 2303sub3\n", "setting /run_id to 6adddc60-cc2a-11e6-bacb-005056c00001\nprocess[rosout-1]: started with pid [23228]\nstarted core service [/rosout]\nprocess[arm/robot_state_publisher-2]: started with pid [23247]\nprocess[joint_state_publisher-3]: started with pid [23248]\nprocess[arm/driver-4]: started with pid [23249]\nprocess[arm/arm_controller_spawner-5]: started with pid [23257]\nprocess[topic_transf-6]: started with pid [23262]\n[ INFO] [1482839325.074128952]: Using fixed control period: 0.010000000\n[ INFO] [1482839327.892443220]: Initializing XXX\n[ INFO] [1482839327.892948706]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1482839327.893326862]: Current state: 2 device error: system:0 internal_error: 0 (OK)\nabort2303#3, reason: Data cannot be transferred or stored to ...", "ros_canopen"], "answer_code": ["ros_canopen"], "url": "https://answers.ros.org/question/250653/ros_canopen-with-lwa4d-initialization-fails/"},
{"title": "How to get/set a \"system state\" in ROS?", "time": "2017-01-11 18:17:18 -0600", "post_content": [" ", " ", "I'm porting a system to ROS that will be composed of several nodes linked by topics in a chain-like structure, in a way that each node produces some data to be consumed by the callback of the next node.", "The problem I'm facing is that some of the nodes set variables that represent \"system states\" that influence the behavior of other nodes later in the chain (not the next imediate one). I'm wondering what is the best way to implement such thing on ROS. ", "I though about using the parameter server with getParam() and setParam() for that, but I read in the ", " that:", "As it is not designed for high-performance, it is best used for static, non-binary data such as configuration parameters.", "So I'm not sure if I should use it for these state variables. I also read about getCachedParam(), but I think the same doubt about the performance applies. I also saw ", ", but I don't know if it was developed for this kind of scenario. ", "Could anyone tell me what would be the best way to implement this in ROS?"], "answer": [" ", " ", " For reference, there's also some documentation of common patterns in ROS:  ", "Depending on what you're trying to achieve, and what the timing requirements are, parameters, topics, services, or a single process may all be valid choices.", "If each node in the pipeline is responsible for part of the system state, it could publish that state on a latched topic, and your other nodes could subscribe to that topic. ROS doesn't guarantee the timing of message delivery, so if your state needs to be set before the data reaches the next node in the pipeline, this probably isn't the right choice.", "If your state follows the data, you could simply include it in the message that is passed from one node to the next. Downside here is that your state isn't persistent outside of the nodes.", "If you do have synchronization requirements and your data is small, you could use the parameter server, and call setParam immediately before you publish, and then getParam at the beginning of the next callback. Since these are blocking, you're guaranteed that the new state is pushed to the parameter server before the next node requests it.", "If you have synchronization requirements and your shared data is large, or if more than one node can set the same shared variables, you could implement a separate node to hold your shared state, and each node could update that shared state either by publishing (fast but not guaranteed timing) or by service call (blocking and guaranteed update); and then retrieve the shared state either by subscribing to a shared state topic, or by making a service call to request the state. Writing your own node to manage the shared state also gives you built-in type safety and more control over update semantics and atomicity.\n(this is basically just implementing your own parameter server, with custom data types and less load on the ROS master)", "Finally, if your state is very large (100's Megabytes) or you have tight deadlines on how quickly it needs to be transferred to other components, it may be better to pull all of the pieces of your pipeline in a single process (you can still use ROS pub/sub within a process), and communicate your shared state in a more traditional way, with shared memory and mutexs to prevent concurrent updates."], "url": "https://answers.ros.org/question/251773/how-to-getset-a-system-state-in-ros/"},
{"title": "Sending a message containing messages", "time": "2017-02-01 07:58:25 -0600", "post_content": [" ", " ", "My current rqt_graph looks as follows ", " ", " ", "\nAll the nodes on the left send the same message sensor.msg. ", "\nIs there a way to send a message from FB1 which is an encapsulation of all the messages from the nodes on the left? ", "\nLike a message containing messages *struct of type sensors.msg which is sent by FB1 to the nodes on the right.     ", "Why do you need such encapsulation? Why don't you subscribe the nodes on the right to the topics they consume from the left?", "The node in the middle is on the processor, it receives the messages, processes them and forwards the actions to the nodes on the right. I have limited memory and hence need to find a way to encapsulate the messages"], "answer": [], "url": "https://answers.ros.org/question/253498/sending-a-message-containing-messages/"},
{"title": "UDP Packet Abstraction", "time": "2017-03-15 05:21:54 -0600", "post_content": [" ", " ", " ", " ", "I have an OXTS xNav550 and have downloaded the oxford_gps_eth package for it.", "It doesn't quite do what I want (only publishes once there is a gps lock and doesn't provide all the packet data).  Therefore I want to modify it, so am looking at the code.", "I was considering writing (or finding) a generic UDP ROS node.  i.e. opens a port (optionally on a specific ethernet device and/or IP address) and publishes all incoming packets on a ROS topic.\nI would then write a new OXTS node (based on the Oxford source code - maybe).  This would subscribe to the UDP topic.", "This would have the advantage that the raw UDP packets could be logged into a ROS bag.  Until every bit in the packet is published in device specific topics, this would mean prevention of loss of data which may, in future analysis, be useful.  It would also mean that the device specific node could be developed away from the sensor by replaying a bag.", "The disadvantage would be that there would a slight overhead in effectively retransmitting an incoming UDP node.", "So, any thoughts?", "Is this [..] 'the ROS way of doing things'?", "I'm not an authority, but in general I try to design my ROS nodes in such a way that they only pub/sub meaningful msgs that don't need any implementation details of other nodes to be interpretable. A transparent bridge (which your UDP node ..", ".. sort of is) tends to ", " domain concepts from outside the ROS nodegraph into your application. That will reduce re-usability of nodes, but that could be perfectly acceptable. A pkg that does this is the ", " driver. A set of nodelets ..", ".. form a cooperating set where one decodes raw traffic from the sensor, uses an intermediary representation (a ROS msg) to publish the result and a subscriber then consumes that and produces 'meaningful' ROS msgs from it (PointClouds).", "If you want to do this, be sure to use nodelets."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Is this design pattern consistent\nwith 'the ROS way of doing things'?", "Does the generic UDP node already\nexist?", "Any other reason why I\nshouldn't do any of this?"], "url": "https://answers.ros.org/question/257045/udp-packet-abstraction/"},
{"title": "Tuning down PointCloud2 data for smaller computation devices", "time": "2017-03-21 05:31:18 -0600", "post_content": [" ", " ", "Hello,", "I have installed ROS navigation on a Jetson TK 1 microprocessor and I am using Kinect for /scan sensory using laser_to_scan node.", "I have added the scan topic inside costmap_common_params.yaml", "It computes the AMCL and executes tway points pretty well but now I would like to add the point cloud topic as well so that it avoids obstacles in the way using the local costmap obstacle avoidance.", "Unfortunately by doing this I have slowed down the path planning drastically. ", "Sending a goal pose from RVIZ makes the ros navigation about 25 seconds to send the first plan and even worse, since it doesnt send the next plan in time, the path planning fails altogether.", "It seems that the CPU on the Tegra might not be managing all the computation required from the pointcloud topic so I was wondering if there were ways to tune the data down or even take only a chunk from it so that move_base can compute with less resource consumption.", "Thanks in advance."], "answer": [" ", " ", " ", " ", "What I do for small computation devices is to set the Kinect driver to QQVGA mode (160x120). The default mode is VGA (640x480 pixels). It makes a huge difference to process only 19.200 points per frame (QQVGA) instead of 307.200 points per frame (VGA) ", "If you are using openni2_launch you can set the parameter ", " (11 is QQVGA@30fps) either in your launch file or using ", ".", "Although the ideal solution would be to modify costmap_2d package to process the pointcloud using the GPU in TK1 instead of the CPU", "Thanks for your great answer, I see that in fact openni has image_mode which can be reduced. A couple of questions please: do you also reduce depth_mode res? And do these modifications affect your /scan performance?", "Yes, I do reduce the depth_mode res as well. I always use the depth sensor as a complement of a real laser range finder but I would expect these modifications to affect the laser_to_scan /scan performance, although not critically"], "question_code": ["observation_sources:  scan\nscan:\ndata_type: LaserScan\ntopic: kinect_scan\n", "bump:\ndata_type: PointCloud2\ntopic: camera/depth/points\n"], "url": "https://answers.ros.org/question/257499/tuning-down-pointcloud2-data-for-smaller-computation-devices/"},
{"title": "gps time synchronization with pps?", "time": "2017-03-21 16:56:07 -0600", "post_content": [" ", " ", " ", " ", "I'd like to receive a GPS time message and be able to convert any ros time stamp into GPS time for consumption by other systems that have GPS receivers but aren't synchronized to my ros system clock.  ", "It sounds like I would also want to make use of a gps receiver that sends a pulse when it sends the message with the timestamp, so that latency with receiving the serial message can be eliminated, and the timestamp could be made good to within a few milliseconds.", "Is there software available for this, and perhaps recommended hardware for receiving the sync pulse?  I can imagine having a microcontroller that receives the sync and the gps message while doing something ntp like to figure out what the ros time conversion is for the host computer (or eliminate the mcu and connect the pulse to audio input and low latency audio drivers?), but naturally would prefer to use something available rather than make this from scratch.", "If you have a serial port and a GPS with PPS and RS-232 outputs, you can use gpsd and chrony to synchronize your system clock to GPS time.", "It looks like there is a USB solution (I'd like it to work with a modern laptop) with more latency but still good enough for me, the pps is routed to a usb signal that gets polled 1024Hz, the problem then is actually locating one of these for purchase, that answer probably lies in another forum.."], "answer": [], "url": "https://answers.ros.org/question/257567/gps-time-synchronization-with-pps/"},
{"title": "Get sensor data using Crazyflie ROS Package", "time": "2017-04-10 03:49:34 -0600", "post_content": [" ", " ", "Hello everyone,", "I'm new to ROS, I just learnt the basic but I have never coded ROS before. However, I need to get/store sensor data from Crazyflie and publish it on the screen for my project. Can anyone give me some steps I need to do or some references with similar code I can consult to achieve my purpose? If it's possible, the simple or detailed it is, the better.", "Thank you!"], "answer": [" ", " ", " ", " ", "Welcome to the ROS Community! Searching around the ROS wiki will give you the answer most of the times!", "I found ", " while looking for relevant stuff. Assuming you have Crazyflie 1.0 (or 2.0 with stock firmware) this package promises to work out of the box for you! Unfortunately I do not have a Crazyflie to test it, but seems like a good start even if it is not exactly what you need. ", "For a quick example, if you just need all the imu data that is being published to be stored in a file, you could just open a terminal (while the driver of the Crazyflie is running), and execute ", " while replacing topic_name with your preferred topic (/imu) and filename.txt with your preferred file.", "Good luck with your project!"], "answer_code": ["rostopic echo /topic_name >> filename.txt"], "url": "https://answers.ros.org/question/258873/get-sensor-data-using-crazyflie-ros-package/"},
{"title": "What tools are available for persistent storage in ROS?", "time": "2017-05-10 16:46:56 -0600", "post_content": [" ", " ", " ", " ", "I have a small amount of information (e.g., camera position and orientation) that I would like to persist through roscore shutdowns and restarts.  There are potentially multiple providers and multiple consumers of this information so it seems like a latched ROS topic would be appropriate -- the only difference is that I would like to latch to disk instead of just latching in memory.  Are there any tools that accomplish this task or something like it?", "I see that ", " exists, but setting up a MongoDB just for a few hundred bytes of information seems like ultra overkill.  warehouse_ros seems geared more toward storing very large amounts of information."], "answer": [" ", " ", " ", " ", " I decided there was apparently no generalized/robust way to persist information using the ROS infrastructure, so I wrote something myself:  ", "   That package allows the user to read from and write to ROS topics and this node will take care of ensuring that the topic is populated with the message from last session next time the system is started up. ", "EDIT: This actually doesn't work (well) because of the way ROS topics are architected.  If a topic represented a series of messages that supersede each other (the way I assumed it would work since that seems like the vast majority of use cases, and is the way it works within a single node), this would work great.  Instead, a topic just represents a set of instantaneous messages with no assumption of precedence or behavior -- sometimes, as in the case of /tf_static, messages on a topic represent the accumulation of information instead of a sequence of information.  If there are two nodes who each have a latched message where the type doesn't include a timestamped header, how a subscriber should interpret the message pair they'll receive upon subscription is completely undefined.", "I'm a fan of sending a pull request to somewhere upstream for such a useful and generic feature. Maybe ", "?", "I'm glad you think it's useful and generic :)  If people end up finding it useful as a separate package, I might consider a pull request in the future, but not a whole lot of time available right now."], "answer_code": ["ros_comm"], "url": "https://answers.ros.org/question/261486/what-tools-are-available-for-persistent-storage-in-ros/"},
{"title": "Example codes for sending velocity commands to Iris+ over mavlink", "time": "2017-06-01 18:31:47 -0600", "post_content": [" ", " ", "Hello,", "I'm trying to make a 3DR Iris+ fly itself -- to follow an object. My setup is ", "1) Odroid XU4 running trusty, OpenCV 3.0, ROS jade, \n2) Have a sample node that is running on the Odorid and can consume ROS image message, over cv_bridge, from a camera. What this sample ROS node does is basically analyzing images from a camera to make the Iris follow an object. The output of this sample node is the location of the object.", "I'd like to have a ROS node that consumes the outputs from the sample node and send velocity commands to PX4 at the Iris+. I've been googling for a while to get any information, e.g., codes or tutorials, related to what I want to do, but it wasn't successful. So, it'd be great if any of you could give pointers where I can look for or some advices.", "Thanks"], "answer": [], "url": "https://answers.ros.org/question/263033/example-codes-for-sending-velocity-commands-to-iris-over-mavlink/"},
{"title": "Simple nodes taking up 100% CPU [closed]", "time": "2015-08-26 06:39:28 -0600", "post_content": [" ", " ", " ", " ", "I have a bunch of nodes running on a robot and sometimes (not sure after how much runtime) very simple nodes such as topic_tools/relay or robot_pose_publisher/robot_pose_publisher take up 100% CPU. A restart of the system brings it back to the normal state (less than 1% CPU for the same process).\nI attached a gdb session to the relay node while it was consuming 100% and every time I interrupted it it was at ", " (as expected, same as when it is at normal CPU usage).", "The system does not appear to be on a heavy load when this happens, memory usage is fine and the number of open file descriptors seems reasonable.", "Any idea what else I could check?", "EDIT:", "A little more info about the system:", "EDIT2:", "Apparently this has something to do with wifi. We have robots at specific locations where we observe this behavior and when we disable the wifi interfaces, the system load goes down again. The nodes that take up CPU don't transmit or receive anything over wifi.", "The relay is passing LaserScan messages at 15 Hz. It looks like it gets stuck at 100% for a long time.\nThe error is very random. At some point in time one or multiple random C++ nodes grab 100% CPU, both standard ROS nodes and custom (very simple) ones.", "Will it make any difference if you reduce the message queue length?", " This seems similar to  ", "That's possible. Thanks!"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Are you pushing many messages through the relay? 2. Is that shortly 100% before going back to 1% or does it get stuck at 100%?"], "question_code": ["CallbackQueue::callAvailable()", "$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.3 LTS\"\n$ uname -a\nLinux beta4 3.13.0-61-generic #100-Ubuntu SMP Wed Jul 29 11:21:34 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n$ dpkg -s ros-indigo-roscpp | grep Version\nVersion: 1.11.13-0trusty-20150522-1157-+0000\n"], "url": "https://answers.ros.org/question/216567/simple-nodes-taking-up-100-cpu/"},
{"title": "reduce roscore startup time", "time": "2014-06-30 02:21:08 -0600", "post_content": [" ", " ", "Hello,", "I'm running ros on an embedded system (2xCortexA9@900MHz). It takes about 13 seconds till roscore is ready.\nThe problem is, that I need a running system with roscore and some small nodes after about 12 seconds. The bootup time of the operating system (~3sec) is included, so roscore must be started in 9 sec. or better.", "I used the python cProfile module and runsnake but I didn't find the \"one\" time consuming element.\nAre there any ideas how to reduce the startup time of roscore? Is it possible to \"compile\" roscore (binary without python)?", "Thx!"], "answer": [" ", " ", " ", " ", "I believe that there's an option to roslaunch that skips the disk space check; that may be able to shave a few seconds off of the startup time, particularly if there isn't a user to see the results.", "UPDATE:", "The command-line option to roslaunch to skip log size checks is ", "Thanks a lot for your great advice! May I know if there is any tips for finding the function of disk space checking in the source code of ROS? :)", "I've updated my answer with the option in question. If you're still curious about how it works, I'd recommend that you start at the roslaunch wiki page and follow the source code link to the source code."], "answer_code": ["--skip-log-check"], "url": "https://answers.ros.org/question/179200/reduce-roscore-startup-time/"},
{"title": "Our robot will be  equipped with lidars, sterio camera, SLAM algorithm, machine vision and similar loads. Please suggest us the computer specification which will be ideal and optimal?", "time": "2017-06-12 05:33:13 -0600", "post_content": [" ", " ", "We are team of 4 engineers trying to develop autonomous robot system which can navigate autonomously in farms and other agricultural environment. We are kicking off the project next month and We are looking for ideal computer system specification. ", "We are purchasing stuffs for the development and we are looking for recommendations for hardware requirement. ", "Is this mean to be prototyping? Do you have any constrains about weight, power consumption?", "Weight constrain is not there as for now. Power consumption can be taken care off by using larger batteries if needed.", "But it will be helpful if you can give insights in either situation for us to evaluate further.", "regards", "If so, I would consider multiple small PCs (like Inter NUC or Gigabyte Brix) as a start, because the computational power depends on many factors (number of sensors, your algo, speed of the vehicle, etc.).", "ROS is also best choice to use distributed systems (multiple PCs working together), so you can have 1 PC to handle motion planning + localization, 1 for machine-vision for example. This also enable \"plug-and-play\" if you wanna try different machine vision algo with same motion planning capability.", "Yes, I was thinking of something in those lines. I will dig in more into inter NUC and gigabyte Brix.", "For the software development purpose or simulation work before actual robot testing, do you think we will need high computional requirements?", "What I mean is, I will be distributing the vision, navigation and robotic hand work in the team and I am in process of purchasing the hardware for them. I want to buy the optimal hardwares."], "answer": [], "url": "https://answers.ros.org/question/263682/our-robot-will-be-equipped-with-lidars-sterio-camera-slam-algorithm-machine-vision-and-similar-loads-please-suggest-us-the-computer-specification/"},
{"title": "Optionally disable ros publisher buffering", "time": "2017-06-20 05:07:15 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "We are using ROS as a basis for our robotics/AI project. It's been great so far with all the tools and code it provides, however today I ran into a small (big?) problem.", "A small conceptual explanation of how our system is designed and how it works. We have some input on a socket port which is basically visual data. We have a ros node that reads this port out, does some processing, and publishes the data ready for consumption on a topic. This happens at 60Hz. We have another node that consumes this visual data and sends instructions out a serial port, the control node. This also happens at 60Hz. So far so good; this is standard ROS stuff and besides some minor caveats we've had no problems using this model.", "Now here comes my problem. Some buffering takes place in the visual node when publishing the processed data. So at 60Hz we receive visual data from the socket, and at 60Hz we call ", ". What sometimes happens is that new data from the socket arrives before the previously published data is actually sent to the control node. I know this for sure because sometimes this happens in the control node (function calls derived from print statements throughout my program):", "So what happens here is because of the buffering in the visual node (or in the TCP of the subscriber? I've been looking at the ROS source code a lot lately and I've seen buffers in literally every class I look at) the control node sometimes runs without a new visual update, even though that visual update was given to the publisher in the visual node. So even though both nodes run at 60Hz (I've confirmed this with ", "; the throughput of visual messages and control messages is almost 60Hz), the control nodes effectively runs at 30Hz, because every odd cycle it has to reuse \"stale\" data.", "There are a few possible solutions I've found:", "Change line 215 on [link] from true to false", "I'm confused here: according to ", " and the page you link, ", " is already ", ".", "Whoops, that's on me. Of course I meant to write true. Changing it now.", "And a question: have you looked into the ", "? Especially ", "?", "I assumed tcpNoDelay is true always and would only be false if you want to test your application under dire circumstances. Is this not the case?", "As can be seen on ", ", I suppose I'm right? Please correct me if I'm wrong.", "And a high-level comment: I think what you are seeing is the classic event-based vs polling (or periodic) system clash. ie: on the one hand you have an async comms pattern (pub-sub) and on the other a sync, periodic 'control system'. Marrying those is always a bit of a challenge.", "I assumed tcpNoDelay is true always and would only be false if you want to test your application under dire circumstances. Is this not the case?", "Afaik, ", " is not enabled by default. That is also not what the code shows. The function arg has a ", " default, but that is something else.", "About the high-level comment: I agree. With the information I have now after almost a year of development I'm not sure if I would pick ROS's publisher/subscriber system again. At the very least I would go with nodelets (at least for the critical stuff. For debugging infrastructure pub/sub is great)."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "\"Start spinning...\"", "\"Ros spinning complete\"", "\"Control finished.\"", "\"Start spinning...\"", "\"Ros spinning complete\"", "\"Control finished.\"", "Set the queue of the publisher in the visual node to one. That ensures that everytime a message is being sent it's guaranteed to be the most up-to-date. The buffering delay is somehow still in place however, so control still runs at 30Hz. (I tested this by only running the control function when new data arrived. It's output was 30 Hz even though the visual update frequency was 60Hz.)", "Produce more messages. I guess this can work, but there are 2 limitations\n", "The visual input really runs strictly at 60Hz. I can't make it produce more data", "Buffering is still in place. So even if I could crank the visual data up to 100Hz, then my control node will still run at a percentage of that. That's bad because it makes performance unpredictable: for some topics, if ..."], "question_code": ["publisher.publish(processedVisualData);", "ros::spinOnce()", "visualUpdateCallback();", "runControl()", "ros::spinOnce()", "runControl()", "rostopic hz", "immediate_write", "false", "tcpNoDelay", "tcpNoDelay", "true"], "url": "https://answers.ros.org/question/264207/optionally-disable-ros-publisher-buffering/"},
{"title": "Issues with ament CmakeLists", "time": "2017-09-11 09:00:59 -0600", "post_content": [" ", " ", " ", " ", "Hello All,", "I just started out with ROS 2.0. I have the latest build. I am unsure if I am doing something stupid or missing out something. However, I can\u2019t build these simple files. If I should have posted somewhere else kindly let me know.", "The command used for building:", "Error thrown:  ", "This is my test_node.cpp", "This is my test.cpp", "This is my header:", "CMakeLists:", " Please do not post pictures of text. Please post the text. Also, please do not cross post questions. We see both sites in our notifications already.  ", "Also, it's fine (for ROS 2.0) to post questions here, or on discourse.", " I don't see the error in your cmake, but it is a link error. I would suggest starting with our examples and going from there:  ", "  Also, you're using  ", " and an include guard, which is redundant.", " I have made the changes, apologies for posting the picture and cross posting.", " \nHi,arunavanag,\n  I have a question to consult.How to achieve launch service or topic in ROS2 TEST?\n  I'm writing TEST for the ROS2's project.I learned to launch service or topic via the .test file in ROS,but there is no .test file in ROS2 TEST.What do I need to do to achieve.\nThanks!", " your question is not related to this thread. Please create a separate question for it rather then commenting on this already long finished one."], "answer": [" ", " ", "There are multiple problems from a first look at the CMake snippet:", "Thanks ", "This is where i got confused. Still learning ament.\nYou can't pass a target name like test_lib to ament_target_dependencies(). You need `target_link_libraries(test_node test_lib) for that.", "Is this answer still valid? Specifically regarding passing target name to ", "? It seems like that is the way it is done in ", "The first argument is a target, and all arguments after must be names of packages that have been found. ", ". See the function documentation here: "], "answer_details": ["Your ", " line contains ", " but that should be ", ".", "You can't pass a target name like ", " to ", ". You need `target_link_libraries(test_node test_lib) for that.", "The install rule for the headers is wrong. Either remove the trailing slash from the ", " argument or change the ", " to ", ".", " ", " ", " ", " "], "question_code": ["ament build --symlink-install --only-package testpackage", "Process package 'testpackage' with context:\n--------------------------------------------------------------------------------\n source_space => /home/artc/ros2_ws/src/ros2/testpackage\n  build_space => /home/artc/ros2_ws/build/testpackage\ninstall_space => /home/artc/ros2_ws/install\n   make_flags => -j4, -l4\n  build_tests => False\n--------------------------------------------------------------------------------\n+++ Building 'testpackage'\n==> '. /home/artc/ros2_ws/build/testpackage/cmake__build.sh && /usr/bin/make cmake_check_build_system' in '/home/artc/ros2_ws/build/testpackage'\n==> '. /home/artc/ros2_ws/build/testpackage/cmake__build.sh && /usr/bin/make -j4 -l4' in '/home/artc/ros2_ws/build/testpackage'\n[ 50%] Built target test_lib\n[ 75%] Linking CXX executable test_node\nCMakeFiles/test_node.dir/src/test_node.cpp.o: In function `main':\ntest_node.cpp:(.text+0x161): undefined reference to `testRos2::testRos2()'\ntest_node.cpp:(.text+0x174): undefined reference to `testRos2::initialize(std::shared_ptr<rclcpp::node::Node>&)'\ntest_node.cpp:(.text+0x1b0): undefined reference to `testRos2::~testRos2()'\ntest_node.cpp:(.text+0x1ef): undefined reference to `testRos2::~testRos2()'\ncollect2: error: ld returned 1 exit status\nCMakeFiles/test_node.dir/build.make:123: recipe for target 'test_node' failed\nmake[2]: *** [test_node] Error 1\nCMakeFiles/Makefile2:136: recipe for target 'CMakeFiles/test_node.dir/all' failed\nmake[1]: *** [CMakeFiles/test_node.dir/all] Error 2\nMakefile:127: recipe for target 'all' failed\nmake: *** [all] Error 2\n\n<== Command '. /home/artc/ros2_ws/build/testpackage/cmake__build.sh && /usr/bin/make -j4 -l4' failed in '/home/artc/ros2_ws/build/testpackage' with exit code '2'\n<== Command '. /home/artc/ros2_ws/build/testpackage/cmake__build.sh && /usr/bin/make -j4 -l4' failed in '/home/artc/ros2_ws/build/testpackage' with exit code '2'\n", "#include <testpackage/testpackage.h>\n int main(int argc, char * argv[]){\n rclcpp::init(argc, argv);\n auto node = rclcpp::node::Node::make_shared(\"test\");\n testRos2 test_object;\n test_object.initialize(node);\n rclcpp::spin(node);\n return 0;\n}\n", "#include <testpackage/testpackage.h>\n testRos2::testRos2(){    }\n testRos2::~testRos2(){  }\n void testRos2::initialize(std::shared_ptr<rclcpp::node::Node>& n){\n}\n", "#pragma once\n#ifndef TESTPACKAGE\n#define TESTPACKAGE\n\n#include <iostream>\n#include <string>\n\n#include \"rclcpp/rclcpp.hpp\"\n#include \"std_msgs/msg/string.hpp\"\n\nusing namespace std;\n\nclass testRos2{\n\npublic:\n testRos2();\n~testRos2();\n void initialize(std::shared_ptr<rclcpp::node::Node>&);\n };\n#endif\n", "project(testpackage)\n\nif(NOT WIN32)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -Wall -Wextra\")\nendif()\n\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(std_msgs REQUIRED)\n\ninclude_directories(include \n            ${rclcpp_INCLUDE_DIRS}\n            ${rmw_implementation_INCLUDE_DIRS}\n            ${std_msgs_INCLUDE_DIRS})\n\nadd_library(test_lib\n  src/test.cpp\n )\ntarget_link_libraries(test_lib ${rclcpp_LIBRARIES} ${rmw_implementation_LIBRARIES} ${std_msgs})\n\nadd_executable(test_node src/test_node.cpp)\nament_target_dependencies(test_node test_lib rclcpp std_msgs)\n\ninstall(\n DIRECTORY include/${PROJECT_NAME}/\n DESTINATION include\n)\n\n install(\n  TARGETS test_lib test_node \n ARCHIVE DESTINATION lib\n LIBRARY DESTINATION lib\n RUNTIME DESTINATION bin\n)\n\nament_package()\n", "#pragma once"], "answer_code": ["target_link_libraries()", "${std_msgs}", "${std_msgs_LIBRARIES}", "test_lib", "ament_target_dependencies()", "DIRECTORY", "DESTINATION", "include/${PROJECT_NAME}", "ament_target_dependencies()", "ament_target_dependencies(my_target pkg1 pkg2 ...)"], "url": "https://answers.ros.org/question/270676/issues-with-ament-cmakelists/"},
{"title": "Nodes are takin 100% cpu on any computer", "time": "2017-09-14 15:36:46 -0600", "post_content": [" ", " ", "Hi,", "I am using couple callback queues in my nodes with asyncSpinner. It does not matter on which computer I'm placing it (Odroid XU4, NUC) it does consume 100% CPU. Can anyone point o what I'm missing in these?"], "answer": [" ", " ", " ", " ", " does not have any delays or locks, so calling it repeatedly will use 100% of your CPU time (it's effectively busy-waiting for incoming messages).", "Since you're not doing anything else in that loop, you can replace that with ", ".", "If you want to add other things to that loop, you could add a ", " object to call ", " at a fixed rate and sleep for the remainder of the loop.", "Cool, thanks, will try it now!"], "question_code": ["#include \"ros/ros.h\"\n#include \"ros/spinner.h\"\n#include \"ros/callback_queue.h\"\n#include \"sensor_msgs/LaserScan.h\"\n#include \"mavros_msgs/State.h\"\n#include \"std_msgs/Bool.h\"\n\n\nclass SubscribeAndPublish\n{\n  private :\n\n    ros::NodeHandle nh_1;\n    ros::NodeHandle nh_2;\n\n    // Subscribers and publishers\n    ros::Subscriber sub_range;\n    ros::Subscriber sub_system_state;\n    ros::Publisher pub_block_state;\n\n    bool is_armed; // parameter check that vehicle is armed\n    bool on_hold = false; // parameter, stating that hold sequence initiated\n    std::string flight_mode;\n    float range;\n    float range_threshold = 1.2;\n    float range_min = 0.011;\n    double time_start; // parameter for checking, when hold sequence initiated\n    double time_end = 0.0; // parameter for checking, when hold sequence ended\n    uint16_t range_count = 0;\n    uint16_t last_obstacle_before = 0;\n    bool last_on_hold = false;\n    bool avoidance_active = false;\n\n  public :\n    SubscribeAndPublish(ros::NodeHandle &nh_simple, ros::NodeHandle &nh_state);\n    void system_state_callback(const mavros_msgs::State::ConstPtr& state_msg);\n    void range_callback(const sensor_msgs::LaserScan::ConstPtr& range_msg);\n    bool run();\n};\n\n\n\nSubscribeAndPublish::SubscribeAndPublish(ros::NodeHandle &nh_simple, ros::NodeHandle &nh_state)\n{\n    nh_1 = nh_simple;\n    nh_2 = nh_state;\n\n    sub_system_state = nh_2.subscribe(\"mavros/state\", 2, &SubscribeAndPublish::system_state_callback, this);\n    pub_block_state = nh_1.advertise< std_msgs::Bool >(\"/block_state\", 10);\n    sub_range = nh_1.subscribe(\"/scan\", 10, &SubscribeAndPublish::range_callback, this);\n}\n\n\n\n// Callback Handling \nvoid SubscribeAndPublish::range_callback(const sensor_msgs::LaserScan::ConstPtr& range_msg)\n{\n  std_msgs::Bool msg_block;\n\n  avoidance_active = ((is_armed) && ((flight_mode == \"LEARNING\") || (flight_mode == \"AUTO\") || (flight_mode == \"GUIDED\") )); \n\n  //ROS_INFO(\"Got mode of: %s\", flight_mode.c_str());\n\n  for (unsigned int i = 0; i < range_msg->ranges.size(); i++) {\n\n    range = range_msg->ranges[i];\n\n    if (std::isinf(range_msg->ranges[i])) {\n      continue;\n    }\n\n    if (!avoidance_active) {\n      last_obstacle_before = 1000;\n    } else {\n      if (range > range_threshold || range < range_min) {\n          last_obstacle_before++;\n      } else {\n        last_obstacle_before = 0;\n        range_count ++;\n      }\n    }\n\n    if (last_obstacle_before %100 == 0) {\n      //ROS_INFO(\"Last %d\", last_obstacle_before);  \n      //ROS_INFO(\"Avoidance_active %d\", avoidance_active);  \n    }\n\n    last_on_hold = on_hold;\n\n    on_hold = last_obstacle_before < 250 and avoidance_active;\n\n    if (last_obstacle_before > 15000) {\n        last_obstacle_before = 1000;\n\n        if (!ros::ok()) {\n            return;\n        }\n\n        if (avoidance_active) {\n          //ROS_INFO(\"RELEASE NMESSAGE 1500\");\n          on_hold = false;\n\n          msg_block.data = false;\n          pub_block_state.publish(msg_block);\n        }\n    }\n\n    if (on_hold != last_on_hold) {\n\n      if (!(ros::ok())) {\n        return;\n      }\n\n      if (on_hold) {\n        //ROS_INFO(\"ON HOLD NMESSAGE\");\n        on_hold = true;\n\n        msg_block.data = true;\n        pub_block_state.publish(msg_block);\n      } else {\n        //ROS_INFO(\"RELEASE MESSAGE\");\n        on_hold = false;\n\n        msg_block.data = false;\n        pub_block_state.publish(msg_block);\n      }\n    }\n  }\n  return;\n}\n\n\n\nvoid SubscribeAndPublish::system_state_callback(const mavros_msgs::State::ConstPtr& state_msg)\n{\n  is_armed = state_msg->armed;\n  flight_mode = state_msg->mode.c_str();\n\n  //ROS_INFO(\"(state callback) Got mode of: %s\", flight_mode.c_str());\n  //ROS_INFO(\"Got arming state of: %d\", is_armed;\n}\n\n\n\nbool SubscribeAndPublish::run()\n{\n    while((nh_1.ok()) && (nh_2.ok()))\n    {\n        ros::spinOnce();\n    }\n\n    return true;\n}\n\n\n\n\nint main(int argc, char **argv)\n{\n\n  ros::init(argc, argv, \"listener\");\n\n  ROS_INFO(\"Starting MIO node\");\n\n  usleep(5000000);\n  ros::NodeHandle nh;\n  ros::CallbackQueue my_callback_queue0;\n  ros::AsyncSpinner spinner0(1, &my_callback_queue0);\n  nh.setCallbackQueue(&my_callback_queue0);\n  spinner0.start();\n\n  usleep(5000000);\n  ros::NodeHandle nh_state;\n  ros::CallbackQueue my_callback_queue1;\n  ros::AsyncSpinner spinner1(1, &my_callback_queue1);\n  nh_state.setCallbackQueue(&my_callback_queue1);\n  spinner1.start();\n\n\n  SubscribeAndPublish mysupernode(nh, nh_state);\n  mysupernode.run();\n\n  return 0;\n}\n"], "answer_code": ["ros::spinOnce();", "ros::spin()", "ros::Rate()", "ros::spinOnce()"], "url": "https://answers.ros.org/question/270910/nodes-are-takin-100-cpu-on-any-computer/"},
{"title": "Connecting multiple S300 Laserscanners (RS422 devices) to one PC [closed]", "time": "2017-11-22 02:38:23 -0600", "post_content": [" ", " ", "Hi,\nwe'd like to build a robot with 360 FOV and plan to use two S300 Laserscanners.", "My question is now, how do I connect multiple devices with RS422 Output to one Computer that might have just one (or even no) RS422 input? \n", " says on page 7 that I need two connections, but some of you guys even use hardware like a NUC that doesn't even have one of those inputs.", "I saw a solution where a SPS with serial input was used and then connected via Ethernet to the Navigation PC. But same question here, how do I set up two devices?", "Thank you!\nStephan", "This is not a ROS question, so here isn't the right place to ask. My recommendation would be to contact the manufacturer (or consult the datasheet for the transmission rates)."], "answer": [" ", " ", "Well,\nI didn't think of those tiny RS422 <> USB Converters before and I guess, ", " will do it. (?)\nIs the speed of 921.6 Kbps enough to run two Laserscanners at high performance?", "Best, Stephan"], "url": "https://answers.ros.org/question/276452/connecting-multiple-s300-laserscanners-rs422-devices-to-one-pc/"},
{"title": "ROS for CRS A460 robot", "time": "2017-11-10 13:43:52 -0600", "post_content": [" ", " ", "Hi, I'm new here and would like to use ROS as a wrapper for RAPL2 (proprietary CRS language) to control the CRS A460 robot. Can this be done? If so, where should I look to get help making the wrapper? Thanks ", "I don't know of any ready-to-reuse components, but ", " was a question about a similar robot. Don't know whether that ever resulted in anything usable though.", "Are there any instructions or tutorials for writing a wrapper? e.g.", "would translate to ", "move[,2]", "in RAPL", "No, I don't believe there are tutorials for that specifically.", "Writing a robot driver for a robot arm (not a mobile base) typically involves broadcasting ", " msgs and consuming ", " action goals. Not ", "s (but it could be done)."], "answer": [], "question_code": ["rostopic pub -1 /turtle1/cmd_vel geometry_msgs/Twist -- '[2.0, 0.0, 0.0]' '[0.0, 0.0, 0.0]'\n", "JointState", "FollowJointTrajectory", "Twist"], "url": "https://answers.ros.org/question/275584/ros-for-crs-a460-robot/"},
{"title": "rospack find beginner_tutorials  failing", "time": "2012-09-22 21:38:16 -0600", "post_content": [" ", " ", " ", " ", "After doing a successful full desktop install of ROS Fuerte on Ubuntu Precise 64-bit, ( overlay too)\nI did:", "then:", "then:\nTHIS IS THE PROBLEM:", "My .bashrc file in my home directory has the following source lines where I replaced one source line with another as prescribed in the installation instructions.... and executed . ~/.bashrc ", "The tutorial instructions say:", "If this fails, it means ROS can't find your new package, which may be an issue with your ROS_PACKAGE_PATH. Please consult the installation instructions for setup from SVN or from binaries, depending how you installed ROS. If you've created or added a package that's outside of the existing package paths, you will need to amend your ROS_PACKAGE_PATH environment variable to include that new location. Try re-sourcing your setup.sh in your fuerte_workspace.", "I didn't make any effort to \" create or add a package that's outside of the existing package paths\".", "I don't quite understand how", "\"Please consult the installation instructions for setup from SVN or from binaries, depending how you installed ROS.\" helps me since I don't understand what's really happening here.", "It seems that maybe the profile command should have listed my /home/sam/fuerte_workspace/ path?!....but I'm not sure.", "Please advise.", "Re-opened the question because no answer has been accepted yet. If you see a correct answer, please accept it."], "answer": [" ", " ", " ", " ", "I found out what the real cause of my problems was!\nThe ROS Fuerte release makes it possible for users not to mess with ROS_PACKAGE_PATH by supplying a \"sandbox\" directory in the  fuerte workspace...i.e. -  ~/fuerte_workspace/sandbox\nAt the URL ", " section 1.2. Creating a New ROS Package, there is an instruction saying: ", "\nNow go into the ~/fuerte_workspace/sandbox directory:\n$ cd ~/fuerte_workspace/sandbox", "It then says (and here is where the problem occurs): \n", "The \"alternatively\" part led me to believe I could also arrive at the ~/fuerte_workspace/sandbox by simply doing $ roscd, but this is not true. One must still explicitly perform $ cd ~/fuerte_workspace/sandbox to complete the tutorial. Otherwise doing $ roscreate-pkg beginner_tutorials std_msgs rospy roscpp will create the beginner_tutorials directory outside thee sandbox directory. If that is what one really wants to do sometime in the future, then the ROS_PACKAGE_PATH environment variable must be modified as stated above, but to complete the tutorials, modifying ROS_PACKAGE_PATH environment variable is unnecessary. ", "Thank you for all your help! Greatly appreciated!", " ", " ", " ", " ", "If you are adding an extra path you have to maintain source line as it is and add ", " to your ~/.bashrc file and do ", " on a new shell. Now ", " then create the package beginner_tutorials using the same command. Everything else should work fine now!", "If you are not adding the ", " and calling ", " it would take you to the ", " (default directory) and you will end up creating a package there.", "Hope this helps!", "You ask: \"You are not supposed to replace source /opt/ros/fuerte/setup.bash in your ~/.bashrc. Where is it mentioned that way?\"..........at ", ", in section 1. Creating a new overlay\\", "Section 1 says: It is very common to replace the line source /opt/ros/fuerte/setup.bash to source the setup.bash in ~/fuerte_workspace.\nThe overlay now includes all packages that were installed in /opt/ros/fuerte.", "Ok. I did not update myself wrt. fuerte. I thought things would be similar as in electric. Sorry for that! But did the thing with ROS_PACKAGE_PATH work ? "], "question_code": ["$ roscd\n$ roscreate-pkg beginner_tutorials std_msgs rospy roscpp    (this part worked)\n", "sam@Mecha:~/fuerte_workspace$ rospack profile\nFull tree crawl took 0.022642 seconds.\nDirectories marked with (*) contain no manifest.  You may\nwant to delete these directories.\nTo get just of list of directories without manifests,\nre-run the profile with --zombie-only\n-------------------------------------------------------------\n0.011132   /opt/ros/fuerte/stacks\n0.010529   /opt/ros/fuerte/share\n0.004067 * /opt/ros/fuerte/share/common-lisp\n0.003998 * /opt/ros/fuerte/share/common-lisp/ros\n0.002150 * /opt/ros/fuerte/share/swig\n0.002037 * /opt/ros/fuerte/share/swig/1.3.29\n0.001495 * /opt/ros/fuerte/share/doc\n0.001454 * /opt/ros/fuerte/share/doc/pcl-1.5\n0.001429 * /opt/ros/fuerte/share/doc/pcl-1.5/tutorials\n0.001376   /opt/ros/fuerte/stacks/navigation\n0.001299 * /opt/ros/fuerte/share/doc/pcl-1.5/tutorials/sources\n0.000902   /opt/ros/fuerte/stacks/robot_model\n0.000612   /opt/ros/fuerte/stacks/geometry_experimental\n0.000593   /opt/ros/fuerte/stacks/image_pipeline\n0.000474 * /opt/ros/fuerte/share/catkin\n0.000450   /opt/ros/fuerte/stacks/geometry\n0.000411   /opt/ros/fuerte/stacks/diagnostics\n0.000398   /opt/ros/fuerte/share/ros\n0.000387   /opt/ros/fuerte/stacks/vision_opencv\n0.000371 * /opt/ros/fuerte/share/catkin/cmake\n", "sam@Mecha:~/fuerte_workspace$ rospack find beginner_tutorials \n[rospack] Error: stack/package beginner_tutorials not found\n", "# source /opt/ros/fuerte/setup.bash  <---this is commented out\nsource /home/sam/fuerte_workspace/setup.bash\n"], "answer_code": ["export ROS_PACKAGE_PATH=/home/sam/fuerte_workspace:opt/ros/fuerte/share:/opt/ros/fuerte/stacks", ". ~/.bashrc", "cd ~/fuerte_workspace", "ROS_PACKAGE_PATH", "roscd", "opt/ros/fuerte/stacks"], "url": "https://answers.ros.org/question/44401/rospack-find-beginner_tutorials-failing/"},
{"title": "Raspberry Pi3 + controller or BeagleBone or other for ROS?", "time": "2017-11-30 00:17:14 -0600", "post_content": [" ", " ", "I'm ROS beginner. I've finish tutorial on beginner level and understand concept of node, topic, message, service, parameter also some test ultrasonic publish between PC and Arduino by Rosserial. So I start to have some picture in mind to build a system.", "My robot project is hobby. \nI plan begin ROS with 2-wheel robot with sensor for line follower. I know simple line follower no need ROS but it for learning into ROS publish/subscribe/service create package or etc. Also it interesting begin. ", "Next step is to add more sensor and more task like camera to do like line-track or avoid obstacle. Path-planing. Next is to do navigation/localization/map/path-planing/moveit so UMI together with LIDAR, Kinect will be add. \nI'd like to satisfy with 2D-map or so some cool thing like SLAM which is I don't know how to code before met ROS!\nSo this is my roughly goal to do from now. ", "I need connect like 5-9 IR-sensor for link-track, motor drive with maybe encoder. Next is webcam, maybe ultrasonic. Next is UMI, LIADR, Kinect.\nSo I need processor and controller. My initial thinking maybe Raspberry together wit Arduino. Or 1-BeagleBone. Or SBC with controller. Problem is: which one to choose or consider? I don't want to buy all of them just to test which one is good, which one is bad.", "But since my test for Arduino Uno (2k-sram) wiht 1-ultrasonic take already 60% sram, and when I use sensor_msgs, memory go to 70% and out of synce already. Clearly simply Uno or Leonaro not suit job here. Anyway I love how easy this platform offer.", "So should I go for Mega (8k-sram) or Arduino Due (32bit arm cortex-m3 with 96kb-sram) ? \nOr consider go for other platform like custom my AVR/PIC 16-32bit? Just for increase more memory.\nOr go to other Arduino base controller board like OpenCR? Also there is Cortex-M controller board to consider like STM32 Nucleo with plenty of IO and RAM", "I also see BeagleBone have a lot peripheral and IO. What is con if I choose BeagleBone instead of RaspberryPi + Arduino/uC? I know BeagleBone is also SBC which is not for realtime application. \nIs the realtime will be problem here? for example pwm to drive motor will be timing problem?", "So briefly here is my question", "What combination should I use?\n1. Raspberry Pi3 is enough for these task?", "Raspberry Pi3 + Arduino/Arduino base/OpenCR/AVR/PIC/STM32Nucleo  or  just BeagleBone ?", "SBC (UDOO, Odroid, etc) + Arduino/Arduino base/OpenCR/AVR/PIC/STM32Nucleo? ", "I can't really choose way to go or it cost too much and time consume. \nCan you share idea/help?", "Raspberry Pi3 is enough for these task?", "Raspberry Pi3 + Arduino/Arduino base/OpenCR/AVR/PIC/STM32Nucleo  or  just BeagleBone ?", "sorry typing layout seem strange", "Arduino Uno  SRAM 2048 or 2K, simple Hello World already take 1358 or 66%. If I try more code it take 70% above then it unable to communicate with rosserial. But now I try STM32F469 Discovery Board have SRAM 320K with Hello World take for 1.3K-1.5K I've plenty room to play.. \nHope see more comment.."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "SBC ex. UDOO, Odroid etc + Arduino/Arduino base/OpenCR/AVR/PIC/STM32Nucleo?"], "url": "https://answers.ros.org/question/276943/raspberry-pi3-controller-or-beaglebone-or-other-for-ros/"},
{"title": "tf::transformBroadcaster shows error of Lookup into the past, best practice for setting up broadcaster and listener", "time": "2018-02-18 16:08:41 -0600", "post_content": [" ", " ", " ", " ", "Follow up with my previous question ", ", currently I have another problem with tf broadcaster with same error of lookup into the past. I am really confused now and I hope that you can help me understand a proper way to set up tf broadcaster/listener. \nI followed an example here ", " and fixed my listener.\nHere is the code:", "I have no idea why I can't send my tf broadcast. I believe it is proper to stamp my broadcast with ros::Time::now() since it requires sometime to process all my stuff. \nI also tried to stamp my broadcaster with the time stamp of the incoming message but the error still happened. ", "Obviously, I have missed something about tf here. I understand that:", "a tf listener requires some time to cache up all the transforms, so in callback, the first few calls should just return;", "if we can look up transform, it means that the tf tree is set up properly, then a broadcaster can broadcast anytime with correct transformations between frame_ids. It is logical to stamp this broadcaster with a current time.", "So my question is :", "1) Could you show me what I misunderstand and/or miss here?", "2) Is there a proper way set up the tf to avoid this situation? Example for my class, I put every publishers and subscribers in class constructor, and all callbacks are for ...", "Have you gone through the ", "? You may have issues with the synchronization between clocks.", "Yes, I did. I did set up ntp so that the robot's NUC is always synced with the current time. I recorded my data with rosbag and work with it on my laptop. I did set use_sim_time true before playing my bag and launching my node.", "We're going to need some more info here: what does \"keeps dying\" mean? A ", ", some other error? As this is CPP, I would try to get a GDB backtrace.", "Also: any reason to not make ", " a member variable?", "Finally: you're ", "ing the ", " in ", ", but then just printing the msg. If ", " throws an exception, continuing with the rest of the callback is pointless, so you should perhaps add a ", " there or something similar.", "Finally2:", "I believe it is proper to stamp my broadcast with ros::Time::now() since it requires sometime to process all my stuff", "that depends: if the frame you attempt to broadcast is the result of processing the incoming cloud -- which was captured at a specific time, then reusing the stamp ..", ".. from the cloud that was the input of your process makes more sense to me: how are 'downstream' consumers ever going to be able to correlate your TF frame with the state of the world (ie: at the time that the cloud was captured)? If ", " takes a (hypothetical) 5 mins, ..", ".. ", " will be broadcast based on 5 min old data. Without the stamp from the cloud that was processed, that will be impossible to detect.", "Thank you for your detailed answer. This is a very long answer so I can't put it in a comment.", "exactly. So ", " your original question would have been appropriate.", "I've merged your answer into your original question."], "answer": [" ", " ", "re: your gdb trace: you'll need to (re)build your workspace with CMAKE_BUILD_TYPE set to Debug", "I was able to catch a segfault in <...process my stuff...> which seems to cause a segfault in a broadcaster object. ", "I am still curious about my error though: if I build my package in Release mode, the segfault caused by my algorithm wouldn't crash my node without asking a broadcaster to broadcast my result. It would only crash when I ask a broadcaster to broadcast the result. ", "But this is a different question. Thank you for your help ", ". Please close this question."], "question_code": ["#include <...my stuffs..>\nclass cloudHandler{\npublic:\n    cloudHandler():\n    {\n        main_sub = nh.subscribe(\"pico_flexx/points\",1,&cloudHandler::mainCB,this);  \n        rail_plane_pub = nh.advertise<sensor_msgs::PointCloud2>(\"rail_plane\",1);\n        fit_rails_sub = nh.subscribe(\"rail_plane\",1,&cloudHandler::fit_railsCB,this);   \n    }\n    void mainCB(const sensor_msgs::PointCloud2ConstPtr& input){ \n\n        <...my stuff...>     \n    }\n    void rail_plane(pcl::PointCloud<pcl::PointXYZ>::Ptr input, pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_projected){\n        <...my stff...>\n    }\n\n    void fit_railsCB(const sensor_msgs::PointCloud2ConstPtr& input_from_camera_frame){              \n        try{\n            cam_bl.waitForTransform(\"base_link\",input_from_camera_frame->header.frame_id,input_from_camera_frame->header.stamp,ros::Duration(2));\n        }\n        catch(tf::TransformException &ex){\n            ROS_WARN(\"%s\",ex.what());\n        };\n\n        sensor_msgs::PointCloud2 input_in_bl; \n    pcl_ros::transformPointCloud(\"base_link\",*input_from_camera_frame,input_in_bl,cam_bl); // this is ok to transform now\n\n    <...processing my stuff...>\n\n    /*Now I tried to published my result*/\n    tf::Vector3 rail_origin(p.x,p.y,p.z);\n    static tf::TransformBroadcaster rail_br;\n    tf::Transform rail_tf_;\n    rail_tf_.setOrigin(rail_origin);\n    rail_tf_.setRotation(q);\n    rail_br.sendTransform(tf::StampedTransform(rail_tf_,ros::Time::now(),\"base_link\",\"rail\")); /*Can't send broadcast here, the node keeps dieing and restarting which cause a broken tf*/\n\n    geometry_msgs::PoseStamped r_p2;\n\n    r_p2.header.stamp = ros::Time::now();\n    r_p2.header.frame_id = \"rail\";\n\n    r_p2.pose.position.x = p.x;\n    r_p2.pose.position.y = p.y;\n    r_p2.pose.position.z = p.z;\n\n    r_p2.pose.orientation = tf::createQuaternionMsgFromYaw(yaw_rail);\n\n    rail_pose_pub2.publish(r_p2);/*Can't even publish this message*/\n  }\nprivate:\n  tf::TransformListener cam_bl;\n  ros::NodeHandle nh;\n  ros::Subscriber main_sub, fit_rails_sub;\n  ros::Publisher  rail_pose_pub;  \n};\nint main(int argc, char **argv){\n    ros::init(argc, argv, \"pico_rails_node\");\n    cloudHandler handler;\n    ros::spin();\n    return 0;\n}\n", "[ERROR] [1518989417.344710428, 1518732057.962978755]: Lookup would require extrapolation into the past.  Requested time 1518732055.923351000 but the earliest data is at time 1518732056.005952182, when looking up transform from frame [pico_flexx_optical_frame] to frame [base_link]\n", "SEGFAULT", "rail_br", "catch(..)", "tf::TransformException", "fit_railsCB(..)", "waitForTransform(..)", "return", "processing my stuff", "rail_tf_"], "url": "https://answers.ros.org/question/283054/tftransformbroadcaster-shows-error-of-lookup-into-the-past-best-practice-for-setting-up-broadcaster-and-listener/"},
{"title": "no .project  file created", "time": "2018-03-19 01:55:28 -0600", "post_content": [" ", " ", "above is the error is eclipse. i am using ros kinetic, ubuntu 16.04 with catkin_make build . I have followed so many tutorials but right now i am following ", " tutorisl to configure eclipse with ros but after i enter command ", "\n.prject file is not created inside the build diretory or any diretory. so what should i do to use eclipse ide for ros develipment in ros kinetic. cas without ide it is really time consuming and confusing to develop a ros project efficiently. "], "answer": [], "question_code": [" The project description file (.project) for 'catkin_tools_prebuild@catkin_tools_prebuild' is missing.\n", "catkin_make --force-cmake -G\"Eclipse CDT4 - Unix Makefiles\" -DCMAKE_BUILD_TYPE=Debug"], "url": "https://answers.ros.org/question/285793/no-project-file-created/"},
{"title": "How to dynamically launch and kill nodes?", "time": "2018-03-13 08:31:32 -0600", "post_content": [" ", " ", "Is there any best practice and/or guides surrounding how to dynamically launch and kill nodes?", "I'm building a small Raspberry Pi based robot, and since it only has 1GB of memory and limited CPU, I don't want every possibly node I use to be loaded, because even a node that's \"standing by\" consumes about 2-3% of memory.", "For example, say I want to start a procedure that looks for QR codes in a video stream. If I don't normally do this, it doesn't make sense to have a QR tracking node running at all times, so I'll want to dynamically launch the node as needed, and then kill it when the procedure is complete. What's the best way to do this using rospy?"], "answer": [" ", " ", " ", " ", "For killing a node when you finish processing: ", "For starting a node only when it is required, I would suggest using ", ".", "Sorry, edited the link."], "answer_code": ["Page Not Found"], "url": "https://answers.ros.org/question/285269/how-to-dynamically-launch-and-kill-nodes/"},
{"title": "Image related nodes eating up majority of cpu until system becomes unresponsive?", "time": "2016-04-26 08:27:56 -0600", "post_content": [" ", " ", "Has anyone seen issues with nodes consuming a great deal of cpu (e.g. 400% plus), making gnome terminal turn gray and unresponsive, and either requiring ctrl-c killing the roslaunch containing the offending node (which sometimes works, usually taking 10 seconds or more for the nodes to come down), or killing gnome-terminal which Ubuntu itself sometimes suggests but all the open terminals have to be quit, or ctrl-alt-fX or sshing in from another computer to run ", " and ", " because the entire window manager is so unresponsive?", "It has been hard to track this down because system and ros tools become unresponsive as well- I could have a second machine try to set the first as ros master, or run the ros master on the second to begin with so that it can more likely be reached.", "What I have seen is that Image related nodes seem to be at the top of ", "- rqt_image_view more than a few times, image_proc some other times.  I've deliberately throttled frame rates to 5 hz or so to avoid legitimate high cpu usage- but maybe if these nodes had large queue sizes and got behind, then suddenly wanted to process a bunch of old images they would slow way down?  ", "Should they be spanning multiple cpu cores?  My own nodes usually don't, and when I've done something wrong the damage is usually limited to a single core going to 100% which means tons of cpu left over for the OS to allow me to see that and stop them, and then fix them.", "I'd like to limit all nodes to a certain percentage of cpu- ", " came up in searches, it looks like there would have to be a script that gets a list of processes then limits them after they have started.", "I think this started happening with Ubuntu 14.04 and Jade.  I believe I've seen other non-ros instances coincident with upgrading to 14.04 of something happening to cause gnome-terminal to turn gray which was initially very surprising, but was intermittent enough I didn't invest time in tracking it down.  This is on a fast desktop with 8 real cores (which is seen as 16 in linux), and an i7 class laptop (both around a year old so not the latest intel architecture).", "I am having latest architecture, 8th gen and even I am having same problem. Suddenly while working my CPU usage on single core turns 100% and system crash. I am using Image_view and image_proc.", "i5 8600k 6 Physical cores and GTX 1080.", "I am only detecting different whycon markers and nothing else. Usually cpu usage remains in range 15-20 but after sometime it increases to 100% on single core and system crash. ", "I have already over-clocked my PC to 4.5 Ghz.", "Please repost this as a comment, as this is not an answer to ", "'s question."], "answer": [], "question_code": ["top", "kill", "top", "cpulimit"], "url": "https://answers.ros.org/question/232919/image-related-nodes-eating-up-majority-of-cpu-until-system-becomes-unresponsive/"},
{"title": "How can i read messages from a ROSbag file ?", "time": "2018-04-11 10:20:22 -0600", "post_content": [" ", " ", "I have a bag file in which some are custom messages whereas others are string messages. How can i display the contents of the topics which have message of type strings (without converting it to a .csv file) ?", "for example, From the attached image, how can i read the messages of the topic ", "I do have a question for you, how do you convert a rosbag file into a csv file?\nAnyway, why don't you use the ", " to play your bag file, and then in another terminal you just see the messages passing into the topic with ", "?", "Hi to convert a bag file to .csv format, use ", "\nAnd to the second part, I only have a bag file and when i play it, unfortunately no new nodes are created and no topics as well. That is why i cannot echo any topic.", "Personally, whenever I use rosbag I start all nodes manually, and then play the bad file to publish on the wanted topics. I don't think rosbag starts nodes.", " please don't use an image to display text. Images are not searchable and people cannot copy and paste the text from the image. See  ", " how do you start all the nodes manually when you receive a bag file from someone else ?", "I am confused: ", ": which nodes are you talking about? ", " publishes msgs ", " the original nodes were running. ", " does not need the original nodes. If you're referring to \"the rest\" of the application (ie: consumers of msgs), that would be something else.", " yes, that is correct. I wanted to reply to ", " 's previous comment and then realised that ", " do not requires original nodes. \nQuestion : that means just by playing a bagfile i cannot access the messages. am i right ?", "You can, as long as the system on which you are playing the bag has the necessary msgs. ", " should be able to show msg content though, just as it would with 'normal' publications."], "answer": [" ", " ", "You can use Rosbag API to access messages using a simple script ", "Can you please update your question with a minimum working example?", " is it possible that i can execute this script in the terminal or do i need to specifically put this in the bagfiles directory and then execute it ?", " ill try and do that and next time will take care about the image with text.", "Sorry for the late response. I hope your problem has been resolved. and to answer your question yes you can execute it like a normal script"], "question_code": ["/pnc/relative_carstatus", "rosbag play", "rostopic echo /pnc/relative_carstatus", "rostopic echo /topicname -b bagFileName.bag -p > file.csv", "rosbag", "rosbag", "rosbag", "rostopic echo"], "url": "https://answers.ros.org/question/288319/how-can-i-read-messages-from-a-rosbag-file/"},
{"title": "ROS Segmentation fault (core dumped)", "time": "2018-04-04 09:26:12 -0600", "post_content": [" ", " ", " ", " ", "I've been developing a face detection app with ROS to run on a Drone. The face detection code is the code that can be find on google and works fine but when I mixed it with ROS, I've been struggling with an error: segmentation fault (core dumped). I've tried to find error like using the wrong type of variables but unsuccessfully. Here is the code:", "The program is crashing everytime my face goes to the left or right of the windows. I marked where it's crashing. No ERRO MSG are appearing in the terminal.", "Hope you can help me and could make it clzar this time! Thanks!", "A ", " is a C/C++ thing, while you show Python code. It's still possible to run into ", "s, but it's likely that would be where Python is interacting with 'native' (ie: compiled) code. It would probably help if you could identify where that is, or use ", " to find that spot.", "Note also: you have an infinite ", " loop in your program, which precludes the line ", " from ever executing. That is not necessarily a problem, but something to be aware of.", "Finally: please check the indentation of your code. The formatting seems a little off.", "^ Instead of ", ", I will typically use ", "(alternatively, instead of ", " -> ", ":)", "Makes exiting rospy nodes a bit cleaner.", "Hi! Thanks for the answers, gvdhoorn what do I need to identify to help you out?\nFor the identation, I think it's because of the code block from the text.", "gvdhoorn what do I need to identify to help you out?", "it's not me per se, it's everyone that might be able to help you. Without an idea of where the ", " occurs, it's probably going to be hard to help you.", "First: copy the ", " error msg into your question.", "For the identation, I think it's because of the code block from the text.", "probably, but that doesn't mean you can't fix it. Just make sure to keep a minimum of 4 spaces of indentation, as that is what Askbot uses to markup verbatim text.", "And do what ", " writes. ", " is almost an anti-pattern in an event based system, especially in event consumers.", "Thanks for the feeback, I changed the ", ", add ", "* and just put the part that I think that matters."], "answer": [" ", " ", "I finally make it works, what I did was:", "Thanks for the answers and the help!"], "answer_details": ["Instantiate the publisher and    init_node inside the function that     was called by the main", "Now my function main just call the function that recognize the face ", "Delete  rospy.spin()", "Create a new .py to work with the publishing, the one  that was crashing is being used to  just sent to the new one the value   where the drone must go.", " ", " ", " ", " "], "question_code": ["    while not rospy.is_shutdown():\n        # Capture frame-by-frame\n        ret, frame = video_capture.read()\n        frame = imutils.resize(frame, width=600)\n\n        #convert the frame (of the webcam) to gray)\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        mask = cv2.erode(gray, None, iterations=2)\n        mask = cv2.dilate(mask, None, iterations=2)\n\n        # detecting the faces\n        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n\n        # Draw a rectangle around the faces\n        for (x, y, w, h) in faces:\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            x_imagem = x + (w/2)\n            y_imagem = y + (h/2)\n\n            cv2.circle(frame, (x+(w/2), y+(h/2)), 5, (0,0,255), -1)\n            #600 x 450;\n\n            cmd_vel_msg = Twist()\n            if(x_imagem > 200 and x_imagem < 400):\n                rospy.loginfo(\"CENTER\")\n\n            elif(x_imagem < 200):  #WHERE IT CRASH\n                rospy.loginfo(\"LEFT\")\n                cmd_vel_msg.linear.x = 0.0\n                cmd_vel_msg.linear.y = -0.3\n                cmd_vel_msg.linear.z = 0.0\n                pub_cmd_vel.publish(cmd_vel_msg)\n\n            elif(x_imagem > 400): #WHERE IT CRASH\n                rospy.loginfo(\"RIGHT\")\n                cmd_vel_msg.linear.x = 0.0\n                cmd_vel_msg.linear.y = 0.3\n                cmd_vel_msg.linear.z = 0.0\n                pub_cmd_vel.publish(cmd_vel_msg)\n\n         # Display the resulting frame\n\n        cv2.imshow('Video', frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n# When everything is done, release the capture\n    video_capture.release()\n    cv2.destroyAllWindows()\n", "SEGFAULT", "SEGFAULT", "gdb", "while", "rospy.spin()", "while True:", "while not rospy.is_shutdown():", "while bool_condition:", "while not rospy.is_shutdown() and bool_condition", "SEGFAULT", "while True"], "url": "https://answers.ros.org/question/287500/ros-segmentation-fault-core-dumped/"},
{"title": "Obstacle detection/avoidance and deep/extreme online learning", "time": "2018-04-02 22:49:51 -0600", "post_content": [" ", " ", "For our research project, we would like to perform collision detection and obstacle avoidance with Machine Vision. I would like to use deep learning or maybe even better extreme online learning techniques. I found out that extreme online learning is fast and might be better when not so much data is available to label.", "Also I have done some research and find out that labeling data would be a time-consuming process for our application as we might need to do it manually. So please can someone suggest some platforms and tools where to start with obstacle avoidance /detection and deep learning/extreme online learning w to hit the ground? I would like to use monocular camera and point clouds and ROS for this project ", "Would appreciate your help Thanks"], "answer": [], "url": "https://answers.ros.org/question/287328/obstacle-detectionavoidance-and-deepextreme-online-learning/"},
{"title": "New message format for compressed pointcloud2?", "time": "2018-07-24 01:27:44 -0600", "post_content": [" ", " ", "I'm trying to subscribe the pointcloud message (~10MByte per frame) from a 3D camera, but find the DDS transmission consumed lots of bandwidth, and the latency is unacceptable under ROS2. ", "Considering pointcloud is more and more important for robot usages and actually it has big potential to be compressed, I'd like to check if it's possible to add a new message format in  ", ", something like CompressedPointCloud2 for the data compressed by pcl. Thus the transmission effectiveness will be improved a lot.", "Is there any other better idea?", "Please include more information in your question. For example what programming language are you experiencing this with (C++ or Python), what version of ROS 2 you are using, etc.", "  also, can you in addition to what Dirk asked, provide e.g. a bag with the PointCloud2 data so we can benchmark?", ", ", ", I'm using C++ and the ROS2 version is Bouncy. I can provide a PCD file with 1 frame 640x480 pointcloud2 data inside, but the file size is too big to share here which is about 12M. A manipulate PCD file should also work to reproduce this issue.", " Related github issue:  "], "answer": [" ", " ", "There is a Discourse thread regarding this issue:", "In short: The performance on large messages like PointCloud2 seems to be much worse in ROS2 than in ROS1. Since transferring large point clouds is a common use case, this is unacceptable, and I expect this to be fixed at some point. This is the core issue behind your problem, so it should be addressed first.", "That being said, it sounds like a great idea to offer point cloud compression! However, before a message is added to the core ROS message packages like ", ", there should be at least one (but better several) implementations that use the proposed message for a longer time. So the first step would be for you to create that message in your own code and provide an implementation. If this is successful and there is wide-spread interest, one can begin the process of adding the message to ", ". If you want to go down this road, let me suggest Google Draco for point cloud compression:", "I haven't used it myself yet, but it looks extremely intriguing.", "That's really informative! Glad to know there is patch to fix the big-size data transmission issue in ", ".\nIt's fair to have a reference implementation for compressed pointcloud msgs. I'll look at both pcl Octree and draco solutions.", "Hmm... but if you use an octree to downsample a point cloud, the result will be a regular point cloud with fewer points, not a lossless compressed pointcloud. So we wouldn't need a new message type. Am I missing something?", "you probably are right, let me do further investigation on kinds of solutions, and back to you.", "I have implemented a ", " package for transport of PointCloud2 messages, which uses plugin interface ala image_transport. In the README, you can also find links to other repositories, which contain but are not limited to: ", "Any help in further development of the project is welcome and appreciated. ", "Hope that this helps."], "answer_details": [" ", " ", " ", " ", "plugin for compression using Google Draco", "template for implementing your custom plugins", "tutorial on how to use the package"], "answer_code": ["sensor_msgs", "sensor_msgs"], "url": "https://answers.ros.org/question/298352/new-message-format-for-compressed-pointcloud2/"},
{"title": "Navigation stack not working properly when running remotely", "time": "2018-08-17 08:58:34 -0600", "post_content": [" ", " ", " ", " ", "Hi comunity!", "When I execute \"move_base\" node in the local machine (the robot) I do not get any errors or warnings. But, when I run that node remotely (from my PC), after getting the message \"odom received!\", I get persistently the same warnings:", "In both cases the configuration files used are the same. My PC and the robot clocks are synchronized using ntp.\nAny idea what can be happening? Would increasing the tolerance be a good idea for solving the problem? Thanks in advance.", "Pd.: The reason why I want to execute it remotely is because the CPU consumption of the robot is over 100% and I believe that is the reason why when setting a navigation goal it gets lost...And it says: \"Control loop missed its desired rate of 10.0000hz navigation... the loop actually took X.XXX seconds\". Amcl works fine, so I do not think localization is the problem.", "What is your network latency? Did you measure network utilization? e.g. with wifi the latency can get really high (more then 200ms)", "Latency is not high, I ping robot from my PC and mostly I get a latency of 1.5-5 ms and sometimes 30-50 ms. I can \"solve\" this problem by increasing \"transform_tolerance\" parameter in the \"costmap_common_params.yaml\" file from 0.2 to 0.5 but then I get the warning:", "...: \"", ".\". I can \"solve\" this increasing \"expected_update_rate\" from 0.2 to 1. After that I almost do not get this two warnings, but when setting a navigation goal robot only rotates."], "answer": [], "question_code": ["[ WARN] [1534427628.512533003]: Costmap2DROS transform timeout. Current time: 1534427628.5125, global_pose stamp: 1534427628.2315, tolerance: 0.2000\nCould not get robot pose, cancelling reconfiguration\n"], "url": "https://answers.ros.org/question/300859/navigation-stack-not-working-properly-when-running-remotely/"},
{"title": "How to get the serialized message size/length in ROS2", "time": "2018-09-21 16:50:03 -0600", "post_content": [" ", " ", " In ROS1, given a generic ROS message type, you can call ros::serialization::serializationLength() ( ", " ) to get the size of the message in bytes that will get sent over the network on a call to publish. How can I do that same in ROS2? ", "I am trying to collect the amount of bytes that gets sent out by the ROS 1 - 2 bridge for topics so I know how much network bandwidth is consumed by each topic.", "The bridge uses FastRTPS.", "Any help is appreciated."], "answer": [" ", " ", " ", " ", "There's no function to do this for you right now.", "One thing that is different in ROS 2 is that there's no notion of a \"Serializer\" class that handles serialization for all configurations, instead serialization is handled by the middleware through an abstraction layer. This is nice because it allows us to use different serialization formats if we want or need to do so, but due to the abstraction layer you don't actually know how big it will be on the wire without asking the middleware implementation.", "We already have functions for asking the current middle implementation to do serializing and deserializing of messages:", "So specifically for your use case you could try serializing messages yourself, checking the resulting size of the serialized message, and then publishing it as normal", "The serialized type is in ", ", but it is actually a typedef in to a general purpose data structure in our C utility package called ", ":", "The ", " data structure has the size of the serialized result though:", "Doing it this way might actually even be preferred, because determining what the serialized size would be might take almost as long as just serializing it and you'll be serializing it anyways (most likely).", "There is, however, no way to ask the middleware right now to \"predict\" the serialized size of a message given the ROS data structure, e.g. an instance of ", ", without first serializing it.\nIt is probably possible to implement such a function, but we'd need to add it to our middleware interface.", "There is also no C++ class that wraps up these related functions or C++ versions of the functions for that matter, but it's something that we'd be interested in having and if you're interested in helping with that I'd suggest starting a discussion on an issue in the ", " repository to have an equivalent to the ", " class from ", ".", "In a related subject, we do have these traits for each message:", "Among them is the ", " trait and the ", " trait. If the ", " is true then there is a single size for the message in memory and on the wire (I'm pretty sure about the wire part but not 100%). And if ", " is true then there is a fixed upper bound on the size of the message in memory and/or on the wire.", "But again, there's not yet a function to actually calculate the fixed size or bounded size for messages."], "answer_code": ["rmw", "rcutils", "rcutils", "std_msgs::msg::String", "rclcpp", "Serializer<T>", "roscpp", "is_fixed_size", "is_bounded_size", "is_fixed_size", "is_bounded_size"], "url": "https://answers.ros.org/question/303992/how-to-get-the-serialized-message-sizelength-in-ros2/"},
{"title": "Robot drives arm into table on its way to target pose", "time": "2018-08-19 16:26:08 -0600", "post_content": [" ", " ", " ", " ", "Greetings,", "Apologies for newbie question.  Is there a command that moves \"straight\" to a target set of joint angles, where \"straight\" means the end effector moves in a roughly straight line?", "Background:", "I use my Kinova MICO 4DOF arm for art projects.  I drive it with shell commands.  It currently plays a decent game of chess, using a DGT sensory board.  I'm an end user who programs in bash, consumed by art projects that leave little time for mastering robotics.", "I've recorded joint angles for all robot arm poses of interest.  I can get current joint positions with:", "rostopic echo -c /m1n4s200_driver/out/joint_angles", "I subtract these from the target-position angles, and drive the arm like this:", "rosrun kinova_demo joints_action_client.py -v -r m1n4s200 degree -- $M1 $M2 $M3 $M4", "This works fine much of the time.  However, all joints seem to move at the same speed independently, toward their individual goals.  So the end effector follows a ponderous curvy path.  Depending on where the joints are, the robot will often try to drive straight through the table surface (presumably hoping to emerge somewhere else en route to the target angles).", "Is there a command that will move the end effector straight, or avoid the table surface?  I'm using joint angles, not Cartesian positions.", "I'm using kinova-ros, as described here:", "Everything I know comes from this document -- it's a pretty spare tutorial for me.", "Thanks,\nKevin", "PS. My software versions:"], "answer": [" ", " ", "You need to plan a path for the end effector to follow and then control the joint motors at varying speeds over time to move the end effector along this path. Planning can involve making sure the robot doesn't collide with itself, making sure any part of the robot doesn't try to drive through the table, varying the speeds of joints to give smooth motion of the end effector, and many other factors. Sounds difficult, right? It is! That's why we all use ", " to control our manipulators.", "Fortunately, it looks like the Kinova Robotics ROS software ", " as of version 1.20.", "MoveIt is a very complex piece of software, but the simple things like what you want to do are relatively straightforward. You do need to learn a little about how it works, though. There are ", " available. I recommend you start by going through that Kinova Robotics page to make sure it's all working, and then move on to the tutorials provided by MoveIt. The ", " could be a good place to begin. If you want a simple API to do simple tasks, then ", " is the one for you. That tutorial will take you through the basics of telling the end effector to go somewhere. In particular, pay attention to the bit about adding objects to the planning scene. This is how you tell MoveIt that you have a table and chess pieces so that it doesn't try to drive through the table or throw chess pieces around the room like an angry toddler.", "The tutorials on the C++ API give a lot more detail on the things that are possible with MoveIt. The majority of them are possible with the Python API as well so don't feel like you have to learn C++ just to use MoveIt.", "If ultimately you want to program it all using bash, then I recommend you make a simple Python script that sends an end effector goal to the robot using MoveIt and executes that motion before returning. Then you can do the rest through bash.", "I'd just like to add that the OP's goal of moving the end effector I'm a straight line to a goal is actually a very challenging task and in some cases impossible where that route passes through a singularity. Hence the requirement to use a path planning algorithm to achieve this goal.", "Thank you very much for the clear explanation.  I got 90% of the way there with rosrun shell commands, but for the last 10%, looks like I should learn MoveIt and Python and 3D models.  At least I know the path!  Thanks for your patience @gvdhorn, much obliged.", "You won't need to learn too much about 3D models to use MoveIt. The robot comes with its own 3D model, and for representing things in the environment you can get away with using boxes."], "question_code": ["% rosversion -d\nkinetic\n\n% lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.4 LTS\nRelease:        16.04\nCodename:       xenial\n"], "url": "https://answers.ros.org/question/300965/robot-drives-arm-into-table-on-its-way-to-target-pose/"},
{"title": "Best practices for optimized data transmission", "time": "2018-09-13 16:31:43 -0600", "post_content": [" ", " ", " ", " ", "Hi, I would like to ask you which are the best practices for creating optimized ROS projects.", "I'm using ROS2 bouncy, but I think the issues applies to ROS1 as well.\nI have some nodes which publish sensor data and many other nodes which require only the data coming from specific sensors.\nThere are different threads, some nodes are on the same thread. All sensors data are published more or less at the same frequency.", "One node provides data A and B.\nThen other nodes want to access this data through subscriptions or client requests.\nSome nodes require only A, some nodes only B and some nodes both data. ", "How should this data be provided by the first node?", "How should the functions use the subscription/client API ?\nNote that they are all non method functions (i.e. they are not part of a class)", "A process requires once in a while a continuous stream of sensors data for a relatively short amount of time.", "What's the best ways for accessing this data?", "Thank you a lot", "Have you read this ", " ?\nThen to answer some of your questions :", "First question : if Data A and B are related you can create a unique message, if not two publisher is fine", "If both msgs are timestamped, then using a ", " approach might be nicer. Correlating data based on time is the whole point of ", ".", " I agree but from his question it seems it's more about dealing with the content of the data rather than synchronizing them", ": it seems like the question comes down to: \"if I have two consumers that ", " both need the same data, but not always, should I put all data in a single msg or not?\"", "Using the timestamp we can say: no, use separate msgs and synchronise them when the content of both is needed.", " Yes you're right it's better with your analogy", "Thank you for all your ideas!\nYes, the first question can be reformulated as ", " said.\nUsing a ", " looks like the most elegant solution. Is it also the most efficient one?"], "answer": [], "question_details": [" ", " ", " ", " ", "create two separate publishers/services (one for A and one for B)", "create a message which contains both A and B, to provide it through a unique publisher/service (even if some recipients will only use part of the message)", "each function has its own node and subscribers/clients (all global variables) initialized at the first function call", "there is only one node common to all functions, each function has its own subscribers/clients", "there is only one node with a subscriber/client for each available data", "creating a subscriber at startup\nand spinning only when the data is  required. ", "creating a subscriber\n    every time data are required and\n    then letting it go out of scope to\n    unsubscribe", "use a service", "First question : if Data A and B are related you can create a unique message, if not two publisher is fine (or a service depending of how you want to provide the data)", "Second question : for a continous stream, use topics. Don't create a new subscriber each time because if the sensors are publishing the data you just have to decide when you want to use it or not. You can have a callback treating the data only when required (by setting a flag or something)", "Last question : i don't really get it, you can have a subscriber for each data which calls the right function. If you can't have 2 functions running at the same time maybe consider using threads to allow it."], "question_code": ["    void f1()\n    {\n        auto dataA = ... coming from subscription/client to dataA ...\n        cout << dataA <<endl;\n    }\n\n    void f2()\n    {\n        auto dataB = ... coming from subscription/client to dataB ...\n        cout << dataB <<endl;\n    }\n\n    void f3()\n    {\n        auto dataA = ... coming from subscription/client to dataA ...\n        auto dataB = ... coming from subscription/client to dataB ...\n        cout << dataA<< \" \" << dataB <<endl;\n    }\n", "    void f_loop()\n    {\n        while(1){\n\n             sleep(5 minutes);\n             start_time = time;\n             while (time <= start_time + 15 seconds){\n\n                   auto response1 = ... client 1 request ...\n                   cout << response1->data <<endl;\n      } } }\n", "TimeSynchronizer", "header.stamp", "TimeSynchronizer"], "url": "https://answers.ros.org/question/303337/best-practices-for-optimized-data-transmission/"},
{"title": "web_video_server VP8 stream support [closed]", "time": "2016-03-08 10:08:33 -0600", "post_content": [" ", " ", " ", " ", "How do I consume the VP8 stream. I can't find any documentation on the form of the stream. Is there a good source of documentation for this tools?", "Thanks in advice!", "Did you ever figure it out? There's still absolutely no information on how to do this,", "There's a newer question ", " that describes a question better. Closing."], "answer": [], "url": "https://answers.ros.org/question/228532/web_video_server-vp8-stream-support/"},
{"title": "Custom image message type", "time": "2018-09-12 02:21:20 -0600", "post_content": [" ", " ", " ", " ", "I'm writing to enquire about possibility of ROS supporting or creating a custom sensor message type for video stream (images).", "Our product, based on the FPGA, generates the rosbag file and logs one video stream in one file. It uses the VideoLink protocol, which is packet based, so all events on the parallel interface need to be packetized. The ROS message data here holds a data block.", "The internal structure of a data block consists of a series of data packets each reflecting one line of the video data, or other data arriving via video link. Short data packets (without actual image) are here to store specific events that are natural to the video link and need to be stored in order to allow all features to be present during playback in a later moment. Long data packets contain one line of a video frame, but may also contain a metadata info block coming over the video link, as sent from camera.", "Our goal is to be able to access the video (or images) from the recorded bag files using CvBridge or similar library which supports OpenCV image file format. In a current configuration, ROS environment doesn't recognize this type of message. Is there a posibility to include some plug-in conversion function or a different ROS library which will convert our format to some useable ROS message type.\nThank you for your consideration!", "Edit: These sensors are commonly used in automotive, and they're able to send multiple expositions. We would like to propose to the comminity to create a format for these messages so they can be widely used. Here's a short presentation:", "Related / history: ", ".", "If the goal is to be able to pass this video to all types of ROS consumers then converting your datastream into ", " would be the way to go. The \"specific events\" would probably need custom messages.", "Conversion to ", " would be up to you I believe, I don't know ..", ".. of a library available that takes in videolink packets and converts it to camera frames. The msg structures for the events can be created using standard functionality.", "As soon as you're using standard infrastructure, ", " and other ROS nodes should be able to work with your data.", "If your ..", ".. goal is to ", " rosbags (ie: ", " files) to store your original data stream (so packet-per-packet) then please clarify. It could still be done (ie: create a custom msg for a videolink packet and store those), but there would be no compatibility with ", " in that case.", "Hi, thank you for your answer.To clarify, we're using Omnivision OV2775 and Sony IMX290 image sensors and images (video) is taken in HDR.Those sensors are commonly used in automotive. So, what we want is to store our original data in .bag and than be able to convert them into OpenCV compatible type", "If you want to store data in its original format, then you'll have to create custom msgs that can encode that format.", "Conversion to a ROS ", " compatible format (I assume that is what you meant) would have to be done afterwards, by a tool you write yourself. Input would be ..", ".. the ", " with the custom msgs, and output could/would be a ", " with ", " (in any of the encodings supported).", "Conversion could of course also happen live, and then a single back could contain both the original data as well as the converted data.", "The problem is that we don't record our data using ROS, just store our data in .bag files using rosbag file format respectively. So, creating custom messages and converting them by writing our own tool would be inconvenient."], "answer": [], "question_code": ["sensor_msgs/Image", "sensor_msgs/Image", "rosbag", ".bag", "cv_bridge", "cv_bridge", ".bag", ".bag", "sensor_msgs/Image"], "url": "https://answers.ros.org/question/303127/custom-image-message-type/"},
{"title": "Windows 10 + ROS kinetic with docker + Deep learning Nvidia", "time": "2018-10-31 05:27:44 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm using ROS Kinetic with docker under Ubuntu 16.04, and nvidia-docker as well to use gpu.", "Under windows 10, I know that a beta version of ROS 2 is working. But ROS 2 needs some improvements in tools for us to use it, so we want to use ROS 1.", "We are able to run under windows 10 a docker with ROS Kinetic in it (I didn't test it, but it seems to work and you can add a X11 server to get graphics). But there is no nvidia-docker in Windows 10, and it's not planned by Nvidia.", "Currently we work under Ubuntu, we communicate to the robot through a serial communication and we take images from cameras in ROS topics ?", "My questions are these ones: if I use the deep learning part directly under windows 10, is-it easy to communicate with ROS under docker ? The computer will be directly on a robot, so does using such a configuration will be disadvantageous ?", "Hope that my question is not completely silly but I need to explore this solution because some people in our team prefer using windows.", "Thanks for reply", "Edit: Hi gvdhoorn,", "Thanks for your reply.", "You're right. It's needed to know what is the bandwidth needed. Basically, we need to get from ROS 2 color images (right+left) at 2K/30Hz, using 2 USB3.0 links.", "But after thinking about your question, the biggest problem I have is how to capture images in docker under windows from USB3.0 and how to communicate from docker to a COM ports. And if this is possible, what are the performances. ", "Maybe these questions are more docker and windows related than ROS, even if I want to use ROS in a docker. But if someone has some ideas, it will be great for me !", "Have a nice day.", " Update: maybe I found the answer: it seems not possible to communicate with USB directly from a docker container under windows 10. One work-around is to use USBIP, but you will have delays ( ", " ). This answer was made in 2017, so maybe some progress has be done about this since then. ", "I believe useful answers are going to depend on what it is that you actually want to \"communicate\" between your Windows programs and your ROS application. Images? Point clouds? Other sensor data? Who is producing, who is consuming?", "If you can add a few sentences about that, that would be good.", "Perhaps the experimental ROS Melodic port that Microsoft announced can help here. That would seem to reduce the need for Docker, although it will depend on the exact packages that you're trying to use.", "See ", " for the announcement.", "Hi gvdhoom,", "Thanks for your answer. I saw the presentation in ROSCon. ", "I will try it.", "Can you please not post answers unless you are ", " your own question?", "Thanks"], "answer": [" ", " ", "Maybe these questions are more docker and windows related than ROS, even if I want to use ROS in a docker.", "I\u2019d recommend reflecting on the core dependencies and requirements for your application, and how they relate to Windows 10. If you want to use Docker with GPU hardware acceleration, graphical interface passthrough, mounting of external device interface, and software defined networking, I\u2019d suspect opting for any major Linux distro as a host OS could resolve most of that functionality, reducing the number of hurals just out of the gate so you can focus on the robotic aspects of the project. This is part in due to that the state of linux containers is more mature on linux.", "That being said, as gvdhoorn has pointed out, MS has invested efforts in releasing ROS1 for Windows. I haven't used it yet, but if your more comfortable doing software development in windows, this could be a nice avenue for writing native ROS1 code. I\u2019d argue that if you really needed windows support for your application, and are just starting with ROS, then going with ROS2 to target the window deployment would be the better long term investment; using a ros1 bridge in the interim as you migrate older code.", "As for using Docker VMs on windows, I\u2019ve found the complication of the virtual machine it introduces difficult; with bridge networks constraining, trying to share file systems a pain, and the added performance overhead unwelcome."], "url": "https://answers.ros.org/question/307292/windows-10-ros-kinetic-with-docker-deep-learning-nvidia/"},
{"title": "How are we supposed to use the vp8 stream option of web_video_server", "time": "2018-11-10 14:12:10 -0600", "post_content": [" ", " ", " ", " ", "The docs on the wiki mentions a vp8 stream option, but I can't figure out how to consume it. I tried putting it as a source on a video tag but there's an extreme amount of lag which makes it unusable. I assume I'm doing something wrong, but I can't know since it's not really documented. Even a simple example would be great.", " In general please provide a concrete example that you've tried. Ref.  ", "I edited my question with an example, but it's as I already said in it. I tried putting it directly in a video tag and it didn't work."], "answer": [], "question_code": ["<video autoplay>\n <source src=\"http://localhost:8888/stream?topic=mytopic&type=vp8\" type=\"video/webm\">\n</video>\n"], "url": "https://answers.ros.org/question/308127/how-are-we-supposed-to-use-the-vp8-stream-option-of-web_video_server/"},
{"title": "ROS2 and network usage", "time": "2019-01-24 04:24:55 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm using ROS2 Bouncy and Crystal and I noticed a very strange behavior.", "I'm running several nodes on the same machine (my laptop).\nThe nodes are standard talkers and listeners.\nThe DDS implementation is FastRTPS.\nMy laptop is connected to the internet through an ethernet connection and wi-fi", "When I start several nodes (one immediately after the other), it's common to see the ethernet connection disconnecting.\nThis happens more frequently when the number of nodes that I run is high (10 15 nodes).", " EDIT:\nlinked to this github issue  ", "EDIT:", "When the ethernet gets disconnected I observe the following logs:", "e1000e: enp0s31f6 NIC Link is Down", "Do you have any hint about what could be causing this?", "it's common to see the ethernet connection disconnecting. ", "is it really disconnecting, or can you just not get any other traffic to be passed through the connection? Those are different things.", "It's really disconnecting, I get the popup message from Ubuntu showing it.", "Does ", " show something related at that time? ", " lines?", "Updated with ", " output and partial ", "What sort of switch / router do you have this connected to?", "Simple(r) consumer routers can crash when they are bombarded with too much traffic.", "Not saying this is the cause, but something to look at.", "My laptop is connected to the internet through an ethernet connection and wi-fi", "This is a bad idea and causes a lot of problems. Use one link only and your system will run stable.\nDesktops are not prepared as routers do. 2 default routes via 2 interfaces will cause a lot of pain.", "BTW: how handles DDS dual homed hosts ???", "As long as one interface does not have a default route things should be fine. And even then, if the metric is different for the two routes, one will almost never be used.", "It doesn't make much sense to me such a setup, that I agree with."], "answer": [], "question_code": ["dmesg", "syslog", "Jan 24 11:26:15 asoragna kernel: [6536.084261] e1000e: enp0s31f6 NIC Link is Down\nJan 24 11:26:17 asoragna ntpd[3727]:Deleting interface #25 enp0s31f6,10.102.1.49#123, interface stats: received=0, sent=5, dropped=0,active_time=72 secs\nJan 24 11:26:17 asoragna ntpd[3727]: Deleting interface #26 enp0s31f6, fe80::a49b:ede8:e675:83c7%2#123, interface stats: received=0, sent=0, dropped=0, active_time=72 secs\nJan 24 11:26:19 asoragna NetworkManager[785]: <info>  [1548329179.3922] device (enp0s31f6): link disconnected (calling deferred action)\nJan 24 11:26:19 asoragna NetworkManager[785]: <info>  [1548329179.3928] device (enp0s31f6): state change: activated -> unavailable (reason 'carrier-changed') [100 20 40]\nJan 24 11:26:19 asoragna NetworkManager[785]: <info>  [1548329179.4094] dhcp4 (enp0s31f6): canceled DHCP transaction, DHCP client pid 21312\nJan 24 11:26:19 asoragna NetworkManager[785]: <info>  [1548329179.4095] dhcp4 (enp0s31f6): state changed bound -> done\nJan 24 11:26:19 asoragna avahi-daemon[772]: Withdrawing address record for 10.102.1.49 on enp0s31f6.\nJan 24 11:26:19 asoragna avahi-daemon[772]: Leaving mDNS multicast group on interface enp0s31f6.IPv4 with address 10.102.1.49.\nJan 24 11:26:19 asoragna avahi-daemon[772]: Interface enp0s31f6.IPv4 no longer relevant for mDNS.\nJan 24 11:26:19 asoragna NetworkManager[785]: <info>  [1548329179.4164] manager: NetworkManager state is now CONNECTED_LOCAL\nJan 24 11:26:19 asoragna avahi-daemon[772]: Withdrawing address record for fe80::a49b:ede8:e675:83c7 on enp0s31f6.\nJan 24 11:26:19 asoragna avahi-daemon[772]: Leaving mDNS multicast group on interface enp0s31f6.IPv6 with address fe80::a49b:ede8:e675:83c7.\nJan 24 11:26:19 asoragna avahi-daemon[772]: Interface enp0s31f6.IPv6 no longer relevant for mDNS.\n", "dmesg", "syslog", "dmesg", "syslog"], "url": "https://answers.ros.org/question/313628/ros2-and-network-usage/"},
{"title": "Is it possible to make a bag file or a mapping file from a cad or bim file?", "time": "2019-03-05 13:10:44 -0600", "post_content": [" ", " ", "I have a revit CAD file and an AUTOCAD file that I would like to convert to a bag file for my robot to consume, so that when i get there I would have a head start navigating a relatively \"unknown\" space.", "So basically a floor plan of a building, that I would like to navigate autonomously. (also add it to the rviz sim)", "Is this possible?", "Thanks \nKeith", "I'm not aware of any tools to do this directly. A long shot, but: Gazebo supports DEM terrain models, and uses GDAL to load supported formats. If there is a way to transform your data in one of the supported formats, that could work.", "If all you're after is a 2D floor plan, then you could simply ..", ".. convert it to a ", " or any of the other formats supported by the navigation stack.", "also add it to the rviz sim", "please note: RViz is not a simulator, but a visualisation tool.", "do you know of any api that would convert to a pgm file?", "Convert from what? ", " is a graphics file format and fi Gimp can save and edit those files.", "But my comments were just me brainstorming, not necessarily giving you actionable advice."], "answer": [], "question_code": [".pgm", ".pgm"], "url": "https://answers.ros.org/question/317591/is-it-possible-to-make-a-bag-file-or-a-mapping-file-from-a-cad-or-bim-file/"},
{"title": "Rviz crashes on \"ROS time moved backwards\"", "time": "2019-03-06 03:50:45 -0600", "post_content": [" ", " ", "Hi all,\nI am using ubuntu 14.04 with ros indigo.\nI am running rosbag play with --clock --loop, viewing it on rviz with pointcloud scenes I have recorded previously.\nI subscribe to the message output with another node, running some segmentation scripts I wrote.\nSo, I used to be able to loop through the bagfile, changing parameters on the fly. Every time it looped I would get a warning (actually I think it was en error message) but everything would have kept on going. I'm not sure when it started or what I did to make it so, but now every time I get to the end of the recording (or exit the rosbag and run it again) then I get the same message but Rviz crashes. It consumed a lot of time for me.", "Does anyone know how to make it so that Rviz does not crash on time moving backwards?", "Thanks in advance, Steve"], "answer": [], "url": "https://answers.ros.org/question/317679/rviz-crashes-on-ros-time-moved-backwards/"},
{"title": "ROS2 serious memory leak when using timer?", "time": "2019-04-11 02:39:21 -0600", "post_content": [" ", " ", "I have created almost the simplest ROS node possible in python with a single time doing nothing - code below. When I run it, memory consumption continuously increases and ultimately the my memory is used up. Is there something I am missing, or is this a known or unknown issue? Trying memory profiler (mprof) shows that in 4 min, memory consumption is increased by at least 10MB."], "answer": [" ", " ", " The existing ticket:  "], "question_code": ["import sys\nimport rclpy\nimport time\n\nfrom rclpy.node import Node\n\ndef main(args=None):\n    rclpy.init(args=args)\nnode = Node('leak_test')\n\ndef timer_callback():\n    time.sleep(0.005)\n    #print(\"Timer Callback\")\n\ntimer = node.create_timer(0.01, timer_callback)\n\ntry:\n    rclpy.spin(node)\nexcept KeyboardInterrupt:\n    print(\"Shutdown initiated\")\n\nnode.destroy_timer(timer)\nnode.destroy_node()\nrclpy.shutdown()\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n"], "url": "https://answers.ros.org/question/320851/ros2-serious-memory-leak-when-using-timer/"},
{"title": "Cannot resize buffers in rosserial, Arduino runs out of RAM", "time": "2019-04-08 09:43:57 -0600", "post_content": [" ", " ", "I am using ROS kinetic on Ubuntu 16.04. The Adruino used is an ", " plus an ", ". The Arduino sketch includes ros_lib and ", ".", "What I am trying to do with the Arduino is to use it to drive a linear actuator according to (", ") instructions sent by other ROS nodes from the computer, as well as publish the position (also ", ") of the motor from its encoder.", "My sketch takes around 73% of the Arduino's RAM, leaving less than 600 bytes free after compiling. When I run a toy program, without the motor shield, there is enough memory for the Arduino to function properly but, with it, the controller stops responding very few milliseconds after the actuator starts running (and the positions start being published). I tested the sketch independently from the ROS library and it worked normally, taking instructions and printing the positions through a serial monitor.", "The default buffer lengths for the Arduino UNO are 280 bytes each, which is probably trying to eat up more RAM than the controller has left. What I have been trying to do to fix this was resizing the publisher and  subscriber buffers in ", " arduino library as well as in the ", " ROS package (I did remake it with ", "). In both cases I changed every default buffer length value to 64 and every default or maximum number of publishers/subscribers to 6 (that last number should not be the problem, though). However, when running ", ", the node connects to the Arduino and prints:", "which indicates to me (along with the Arduino crashing after the actuator starts moving) that the buffers are still unchanged.", "The files I changed in ", " are: ", " and ", "Files changed in ", ": ", "Here is my Arduino sketch, with some simplifications and commentings that I tried using to make it lighter (please don't judge ;p). I am including the whole sketch because the minimal working example does work, since its RAM consumption is low enough to allow ", "'s buffers to be allocated."], "answer": [], "question_code": ["std_msgs::Int16", "std_msgs::Int16", "ros_lib", "rosserial_arduino", "catkin_make", "rosrun rosserial_arduino serial_node.py _port:=/dev/ttyACM0", "[INFO] [1554732556.538001]: Note: publish buffer size is 280 bytes\n[INFO] [1554732556.538715]: Setup publisher on /Actuator_Position [std_msgs/Int16]\n[INFO] [1554732556.554360]: Note: subscribe buffer size is 280 bytes\n[INFO] [1554732556.554954]: Setup subscriber on /Actuator_Control [std_msgs/Int16]\n", "ros_lib", "ros.h", "ros/node_handle.h", "rosserial", "rosserial_arduino/src/ros.h", "ros_lib", "/*\n* Controlling the linear actuator:\n* this node is subscribed to a ROS topic named \"/Actuator_Control\", of type\n* `std_msgs::Int16' and will control the position of the linear actuator accor-\n* ding to the Hall sensor units in the actuator.\n* The instructions sent to this node should be formatted as follows:\n* 2-byte instruction ([INS][DIR][RES][VAL])\n*   bit[15:14], INSruction: 0b00 for GO-TO\n*                           0b01 for STEP\n*                           0b10 for FORCE\n*                           0b11 for RESET\n*   bit[13],    DIRection:  0b0 for FORWARD\n*                           0b1 for BACKWARD\n*   bit[12],    REServed:   Unused\n*   bit[11:0],  VALue:      0x0000 - 0x0FFF step size, absolute position or ..."], "url": "https://answers.ros.org/question/320581/cannot-resize-buffers-in-rosserial-arduino-runs-out-of-ram/"},
{"title": "Equip Arduino based lawn mower with ROS", "time": "2019-05-13 05:57:03 -0600", "post_content": [" ", " ", "Hi there,", "I build a lawn mower based on Arduino few years ago (Ardumower). To bring it even further, I plan to re-invent it using ROS.", "My main target is to build a robust path planning and to get rid of random crossing my lawn for hours. Currently, everything is handeld by Arduino code and I wonder how to start best. Please don't hate me for asking how to begin. I read through lots of tutorials but I'm still lost how to set up best.", "Robot is a 2 wheel robot (like Turtlebot i assume) equipped with:\n- 3x sonar\n- bumper sensor\n- perimeter wire (like a fence)\n- IMU\n- Odometry\n- monitoring power consumption of motors (to detect high lawn and overload)\n- Raspberry PI 3 (or Zero) with PI camera (currently for FPV view and manual remote control)\n- Bluetooth", "My Idea is to keep Arduino in place but to use it as a basic controller for the robot. It should note ROS about any problem (Bumper hit, Sonar trigger, overload, perimeter crossed etc). ROS should than decide, what to be done next (reverse x cm and turn to left in a given angle etc.) Arduino will always stop robot in any event and wait for instructions coming from ROS. For communication between ROs and Arduino I plan to use ros_serial.", "Is this the right way to go? Should I attach sensors like IMU, Odometry etc directly to RPI or is it ok to keep them to Arduino and send only data to ROS?\nWhich version should I start? It has to run on RPI 3, maybe on zero, so I plan to use Kinetic. Is this still valid?", "Sorry for this stupid questions, you provide so many information here, too much for me to get the right track to start."], "answer": [], "url": "https://answers.ros.org/question/323055/equip-arduino-based-lawn-mower-with-ros/"},
{"title": "Which lidar can I build for turtlebot in gazebo when using ros kinetic?", "time": "2019-04-19 00:44:25 -0600", "post_content": [" ", " ", "Please tell me, what I need to do to add a lidar for turtlebot in turlebot_world gazebo when I used ros kinetic?\nI tried rplidar but I did'nt see that turtlebot had a lidar on it in turtlebot_world gazebo. Please help me!\nThanks for your helps! ", "This sounds similar to ", ".", "Thanks for your helps. But I only used Turtlebot 2 and I searched many documents but I did'nt find how to build. I tried to make a random lidar placed on turtlebot 2 but could not do it. I have also consulted a lot. Please show me which lidar is suitable to be placed on turtlebot 2 in gazebo while using ros kinetic?Any link refer to how to do it. Thank you very much!"], "answer": [], "url": "https://answers.ros.org/question/321443/which-lidar-can-i-build-for-turtlebot-in-gazebo-when-using-ros-kinetic/"},
{"title": "move_base dies when I publish range data to the range_sensor_layer", "time": "2019-05-03 14:08:23 -0600", "post_content": [" ", " ", " ", " ", "I am using a turtlebot3_burger with kinetic and Ubuntu 18.04.2 LTS with melodic on my laptop. I connected two sonic sensors (HC-SR04) to the raspberry and wrote a driver to publish the range data. I can map the space and then use the navigation stack to navigate; however, once I turn on these sonic sensors they kill the move_base. I have published them to an alternative topic to see if the hardware was the problem, and they don't interfere with move_base until I publish the range data to the topics I have listed for the plugin. I have tried to figure out why and I'm just not seeing my error. Can anyone help?", "error message (although the log file doesn't actually exist when I try to find it):", "the log file doesn't exist:", "costmap_common_params_burger.yaml", "global_costmap_params.yaml", "local_costmap_params.yaml", "range data publishing driver", "Just an observation, but this doesn't seem right:", "Note how ", ".", "I was under the impression that the navigation stack dealt/deleted data outside of the ranges since: ", " is set.", "That could be, but right now you're publishing messages that are malformed. That would be something to fix regardless of who your consumers are (or what they are doing with the messages).", "Good point. Thank you.", " Have you solved your problem? I have a very similar problem. My range data is between min and max range. But when I publish on range_sensor topic, the move_base dies.", "@VictorHirozawa Not yet. Let me know if you figure anything out. I'd love to solve this.", " For sure. I will let you know if I solve this issue."], "answer": [], "question_code": ["[move_base-4] process has died [pid 17199, exit code 127, cmd /opt/ros/melodic/lib/move_base/move_base\n[WARN] [1556908330.052460]: Failed to get param: timeout expired                                     \u2502 cmd_vel:=/cmd_vel \nodom:=odom __name:=move_base __log:=/home/tyrel/.ros/log/b4efff2c-6dd1-11e9-8231-a0\n[INFO] [1556908330.056455]: Setup TF on Odometry [odom]                                              \u2502c5890be7a3/move_base-4.log].\n[INFO] [1556908330.060261]: Setup TF on IMU [imu_link]                                               \u2502log file: /home/tyrel/.ros/log/b4efff2c-6dd1-11e9-8231-a0c5890be7a3/move_base-4*.log\n", ":~/.ros/log/b4efff2c-6dd1-11e9-8231-a0c5890be7a3$ ls\namcl-3-stdout.log  map_server-2-stdout.log  master.log  robot_state_publisher-1-stdout.log  roslaunch-urithiru-17109.log  \nroslaunch-urithiru-17159.log  rosout-1-stdout.log  rosout.log  rviz-5-stdout.log\n", "1 obstacle_range: 3.0\n2 raytrace_range: 3.5\n3\n4 footprint: [[-0.105, -0.105], [-0.105, 0.105], [0.041, 0.105], [0.041, -0.105]]\n5 #robot_radius: 0.105\n6 \n7 inflation_radius: 1.0\n8 cost_scaling_factor: 3.0\n9 \n10 max_obstacle_height: 0.6\n11 min_obstacle_height: 0.0\n12 \n13 obstacle_layer:\n14    observation_sources: scan\n15    scan:\n16       data_type: LaserScan\n17       topic: scan\n18       marking: true\n19       clearing: true\n20       expected_update_rate: 0.0\n21       observation_persistence: 0.0\n22 \n23 range_sensor_layer:\n24    ns: /sensors/sonar_sensor\n25    topics: [\"/range_data/front_bumper\",\"/range_data/back_bumper\"]\n26    no_readings_timeout: 0.0\n27    clear_threshold: 1.0\n28    mark_threshold: 8.0\n29    clear_on_max_reading: true\n", "1 global_costmap:\n2   global_frame: map\n3   robot_base_frame: base_footprint\n4   map_type: costmap\n5   static_map: true\n6   rolling_window: false\n7   resolution: 0.1\n8   update_frequency: 2.0\n9   publish_frequency: 2.0\n10   transform_tolerance: 3.0\n11 \n12   plugins:\n13     - {name: static_layer, type: 'costmap_2d::StaticLayer'}\n14     - {name: inflation_layer, type: 'costmap_2d::InflationLayer'}\n", "1 local_costmap:\n2   global_frame: odom\n3   robot_base_frame: base_footprint\n4 \n5   update_frequency: 2.0\n6   publish_frequency: 2.0\n7   transform_tolerance: 3.0\n8 \n9   static_map: false\n10   rolling_window: true\n11   width: 3\n12   height: 3\n13   resolution: 0.05\n14 \n15   plugins:\n16   - {name: range_sensor_layer,   type: \"range_sensor_layer::RangeSensorLayer\"}\n17   - {name: obstacle_layer,  type: \"costmap_2d::ObstacleLayer\"}\n18   - {name: inflation_layer, type: 'costmap_2d::InflationLayer'}\n", "sonic_pub = rospy.Publisher('/range_data/%s_bumper' % name, Range, queue_size=50)\ndata = Range()\n\ndata.radiation_type = 0 #ultrasound\ndata.field_of_view = 0.5 \ndata.min_range = 0.01\ndata.max_range = 1.0 \n\nr = rospy.Rate(1)\n\ntry:\n    while not rospy.is_shutdown():\n        GPIO.output(t_pin, True)\n        time.sleep(0.00001)\n        GPIO.output(t_pin, False)\n        while GPIO.input(e_pin ...", "min_range: 0.00999999977648\nmax_range: 1.0\nrange: 2.23068761826\n", "max_range < range", "clear_on_max_reading: true"], "url": "https://answers.ros.org/question/322442/move_base-dies-when-i-publish-range-data-to-the-range_sensor_layer/"},
{"title": "what is the meaning of tags in csv files from autoware semantic maps", "time": "2019-05-23 03:46:02 -0600", "post_content": [" ", " ", "Hi guys, I started working with autoware because I want to use the semantic map for my research in indoor robotics, however I cant find information relate to it, how it is built and the tags that are used, for example for the lane.csv the columns have names like lno, lanecfgfg, etc. How do I know what these tags mean and how the car uses it? ", "Is it there a document where I can find the architecture of how the semantic maps are being created and consumed?", "I appreciate your help, thank you very much. "], "answer": [" ", " ", " Hi!\nDo you mean vector map??\nIt is internal format of a map company in Japan.(https://www.aisantec.co.jp/english/)\nWe also think using internal format is very very bad in terms of usability of Autoware.\nSo, we are talking about changing to other open map format such as OpenDrive,Lanelet.\n( ", " ) ", "thank you for your answer", " ", " ", "Hi, ", "If by semantic maps you mean the vector map format, unfortunately Vector Maps is a proprietary map format owned by a third party company. Documentation is not available. ", " Autoware Release 1.12 ( ", " ) will include map converters to open source formats such as Lanelets2 (OSM format). ", "In general they contain information describing  road layout, lane rules (changing, branching, merging) and traffic signal information, which is used to verify vehicle routes. ", "thank you for the information I will give a look"], "url": "https://answers.ros.org/question/323841/what-is-the-meaning-of-tags-in-csv-files-from-autoware-semantic-maps/"},
{"title": "mavros, sitl, gazebo not working topics", "time": "2019-06-07 04:44:39 -0600", "post_content": [" ", " ", " ", " ", "I need some help. I'm doing the offboard tutorial explained in ", ". I have followed all the steps and finally when I launch the node no topic publishes anything. I have installed and deleted the packages 6 times, I have tried to install from source or binary. I have consulted many forums and I can not know what is the problem I have and because my drone does not take off. Please, I need someone's help because I'm desperate. ", "I have uploaded all my configuration of both ros, catkin workspace and PX4/Firmware in my github repository because I don't find the problem ", "I explain the most important steps I have taken below: ", "For \"make px4_sitl gazebo\" work fine and Iris in Gazebo simulator is armed and take off correctly.", "After that:", "And finally, I launch the scripts:", "Please check your formatting next time. Askbot does not support markdown, so backticks do not work."], "answer": [" ", " ", "As shown in Rqt_Graph there is /offb is being published I am not able to get the error correctly.", "I will suggest you to use shell scripts to launch the Firmware It will definitely work."], "question_code": ["cd ~/src/Firmware\nmake px4_sitl_default gazebo/\nsource Tools/setup_gazebo.bash $(pwd) $(pwd)/build/px4_sitl_default\nexport ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:$(pwd)\nexport ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:$(pwd)/Tools/sitl_gazebo\nroslaunch px4 posix_sitl.launch\n", "sergio@sergio-ing:~/catkin_ws$ ./launch-offb.sh \nGAZEBO_PLUGIN_PATH :/home/sergio/src/Firmware/build/posix_sitl_default/build_gazebo\nGAZEBO_MODEL_PATH :/home/sergio/src/Firmware/Tools/sitl_gazebo/models\nLD_LIBRARY_PATH /home/sergio/catkin_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu:/usr/local/cuda-9.0/lib64:/home/sergio/src/Firmware/build/posix_sitl_default/build_gazebo\n... logging to /home/sergio/.ros/log/08d278c6-8903-11e9-bdb6-309c23913dfd/roslaunch-sergio-ing-7231.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://localhost:36531/\n\nSUMMARY\n========\n\nCLEAR PARAMETERS\n * /mavros/\n\nPARAMETERS\n * /mavros/cmd/use_comp_id_system_control: False\n * /mavros/conn/heartbeat_rate: 1.0\n * /mavros/conn/system_time_rate: 1.0\n * /mavros/conn/timeout: 10.0\n * /mavros/conn/timesync_rate: 10.0\n * /mavros/distance_sensor/hrlv_ez4_pub/field_of_view: 0.0\n * /mavros/distance_sensor/hrlv_ez4_pub/frame_id: hrlv_ez4_sonar\n * /mavros/distance_sensor/hrlv_ez4_pub/id: 0\n * /mavros/distance_sensor/hrlv_ez4_pub/orientation: PITCH_270\n * /mavros/distance_sensor/hrlv_ez4_pub/send_tf: True\n * /mavros/distance_sensor/hrlv_ez4_pub/sensor_position/x: 0.0\n * /mavros/distance_sensor/hrlv_ez4_pub/sensor_position/y: 0.0\n * /mavros/distance_sensor/hrlv_ez4_pub/sensor_position/z: -0.1\n * /mavros/distance_sensor/laser_1_sub/id: 3\n * /mavros/distance_sensor/laser_1_sub/orientation: PITCH_270\n * /mavros/distance_sensor/laser_1_sub/subscriber: True\n * /mavros/distance_sensor/lidarlite_pub/field_of_view: 0.0\n * /mavros/distance_sensor/lidarlite_pub/frame_id: lidarlite_laser\n * /mavros/distance_sensor/lidarlite_pub/id: 1\n * /mavros/distance_sensor/lidarlite_pub/orientation: PITCH_270\n * /mavros/distance_sensor/lidarlite_pub/send_tf: True\n * /mavros/distance_sensor/lidarlite_pub/sensor_position/x: 0.0\n * /mavros/distance_sensor/lidarlite_pub/sensor_position/y: 0.0\n * /mavros/distance_sensor/lidarlite_pub/sensor_position/z: -0.1\n * /mavros/distance_sensor/sonar_1_sub/id: 2\n * /mavros/distance_sensor/sonar_1_sub/orientation: PITCH_270\n * /mavros/distance_sensor/sonar_1_sub/subscriber: True\n * /mavros/fake_gps/eph: 2.0\n * /mavros/fake_gps/epv: 2.0\n * /mavros/fake_gps/fix_type: 3\n * /mavros/fake_gps/geo_origin/alt: 408.0\n * /mavros/fake_gps/geo_origin/lat: 47.3667\n * /mavros/fake_gps/geo_origin/lon: 8.55\n * /mavros/fake_gps/gps_rate: 5.0\n * /mavros/fake_gps/mocap_transform: True\n * /mavros/fake_gps/satellites_visible: 5\n * /mavros/fake_gps/tf/child_frame_id: fix\n * /mavros/fake_gps/tf/frame_id: map\n * /mavros/fake_gps/tf/listen: False\n * /mavros/fake_gps/tf/rate_limit: 10.0\n * /mavros/fake_gps/tf/send: False\n * /mavros/fake_gps/use_mocap: True\n * /mavros/fake_gps/use_vision: False ..."], "url": "https://answers.ros.org/question/324954/mavros-sitl-gazebo-not-working-topics/"},
{"title": "Install on shared HPC cluster as normal user without using OS repos", "time": "2019-06-19 22:27:27 -0600", "post_content": [" ", " ", "Hi,", "One of our users has asked for us to install a package (FlightGoggles) that depends on ROS. However, I am struggling to get ROS working.", "The base OS image for our system is a _read-only_, minimal CentOS 6 install. All centrally-installed software packages that we've installed for users live in our ", " directory, where they're separate and versioned (for example -- the Python installation that I've been using is at ", "). We use environment modules to manage access to these -- e.g. ", " makes the needed changes to the environment such that that Python install is usable.", "I've so far managed to get rosdep to install in my own directory. I needed to patch one file to change the hard-coded(!) ", " to something that is writable, and then ", " and ", " worked properly. However, I cannot get the Catkin workspace to build.", "I'm following the instructions at ", ", and can download the source packages, but attempting to install them fails:", "This is not an answer, hence a comment, but I believe this is exactly what ", " was created for. It's a container technology stack that is specifically targeting HPC and cluster environments with security setups that are incompatible with how those kinds of environments are typically setup.", "I would definitely recommend you look at that.", "You're student would create an image on his own machine, upload it (in some way) to the cluster and then use it for his experiments without ever needing any write access to anything or super user permissions.", "It's Docker compatible (as in: can consume Docker images) and the newer versions support things like access to compute accelerators, GPUs and other resources a cluster might have.", "(and I've linked you to the old documentation on purpose as I find it slightly more beginner friendly. You'll want to go to the main ...", "Thanks. Yes, Singularity was my next go to. I was just hoping there was a way to do a native install as well; we put a lot of effort into optimising the packages in the central store, so it seems a waste to not make use of them.", "Which packages?", "ROS is a CBSE framework. There are literally thousands of packages. Would it still scale to treat that as you do regular, stand-alone pieces of software?", "From a maintenance perspective I would not want to have to deal with all of that. Using containers seems like an efficient way to scale this without losing control over your infrastructure.", "I don't need ROS per se on its own, I just need it to install the real program the user is after (FlightGoggles). I couldn't find a normal autoconf or CMake way of building that, only a rosinstall script.", "Thing is: I don't believe you can treat ROS applications the same as you'd do with other, more monolithic software.", "FlightGoggles itself already consists of 6 packages. Those pkgs directly depend on 3 others, and all of that combined depends on 49 other pkgs (", ").", "All of those packages are essentially stand-alone programs or libraries, all of which come with ", " or ", "s that you could, theoretically, ", " and/or ", ".", "I don't know of anyone doing that manually though, as it doesn't scale with typical ROS applications.", "Would you still refer to those N packages as \"the real program\"? You cannot install just FlightGoggles. It needs all of those other packages as well.", "Packaging all of that in a container -- which contains a ROS workspace and has been setup following the build setup and deployment ...", "Hmm, yes, that's sounding like a better idea at this stage. Thanks!"], "answer": [], "question_code": ["/apps", "/apps/python3/3.6.7", "module load python3/3.6.7", "/etc/ros", "rosdep init", "rosdep update", "$ rosinstall_generator desktop --rosdistro kinetic --deps --wet-only --tar > kinetic-desktop-wet.rosinstall\n<success>\n$ wstool init -j8 src kinetic-desktop-wet.rosinstall\n<success>\n$ rosdep install --from-paths src --ignore-src --rosdistro kinetic -y\nrosdep detected OS: [centos] aliasing it to: [rhel]\nERROR: the following packages/stacks could not have their rosdep keys resolved\nto system dependencies:\nrospack: No definition of [python-coverage] for OS [rhel]\nrqt_logger_level: No definition of [python-rospkg] for OS [rhel]\nroslz4: No definition of [lz4] for OS [rhel]\nrqt_robot_monitor: No definition of [python-rospkg] for OS [rhel]\nrqt_bag: No definition of [python-rospkg] for OS [rhel]\nwebkit_dependency: No definition of [python-qt5-bindings-webkit] for OS [rhel]\nroscreate: No definition of [python-rospkg] for OS [rhel]\nroslaunch: No definition of [python-rospkg] for OS [rhel]\nqt_gui_py_common: No definition of [python-rospkg] for OS [rhel]\nactionlib: No definition of [python-wxtools] for OS [rhel]\nrqt_publisher: No definition of [python-rospkg] for OS [rhel]\npython_qt_binding: No definition of [python-qt5-bindings] for OS [rhel]\ncatkin: No definition of [python-nose] for OS [rhel]\nurdf: No definition of [liburdfdom-headers-dev] for OS [rhel]\nqt_gui: No definition of [tango-icon-theme] for OS [rhel]\ncollada_parser: No definition of [collada-dom] for OS [rhel]\nrqt_pose_view: No definition of [python-rospkg] for OS [rhel]\nrqt_top: No definition of [python-psutil] for OS [rhel]\nrosclean: No definition of [python-rospkg] for OS [rhel]\nrqt_console: No definition of [python-rospkg] for OS [rhel]\ngeometric_shapes: No definition of [libqhull] for OS [rhel]\nrosgraph: No definition of [python-rospkg] for OS [rhel]\nrqt_robot_steering: No definition of [python-rospkg] for OS [rhel]\nrqt_runtime_monitor: No definition of [python-rospkg] for OS [rhel]\nroslisp: No definition of [sbcl] for OS [rhel]\nresource_retriever: No definition of [python-rospkg] for OS [rhel]\nrqt_web: No definition of [python-rospkg] for OS [rhel]\nrviz: No definition of [liburdfdom-headers-dev] for OS [rhel]\ngl_dependency: No definition of [python-qt5-bindings-gl] for OS [rhel]\nkdl_parser: No definition of [liburdfdom-headers-dev] for OS [rhel]\nrqt_dep: No definition of [python-rospkg] for OS [rhel]\nrqt_topic: No definition of [python-rospkg] for OS [rhel]\nrqt_service_caller: No definition of [python-rospkg] for OS [rhel]\nrobot_state_publisher: No definition of [liburdfdom-headers-dev ...", "rospack depends", "CMakeLists.txt", "Makefile", "mkdir build && cd build && cmake ..", "make && .."], "url": "https://answers.ros.org/question/326323/install-on-shared-hpc-cluster-as-normal-user-without-using-os-repos/"},
{"title": "Remote node running on/off", "time": "2019-07-02 04:09:35 -0600", "post_content": [" ", " ", "Hi, ", "I am running a node on a remote machine but the node continuously goes on and off.", "I am using a dedicated wired network and ROS_IP and ROS_MASTER_URI are specified for both machines in the env files. ROS Kinetic is being used on Ubuntu 16.04 for both machines. ", "The setup is very simple: from machine-A I run a roslaunch file including just the node to be executed on remote machine-B. The remote node cannot be any simpler, it just publishes a fixed size array of float data to a topic at a specified rate. The communication between machine-A and machine-B seems to be working fine (ping and ssh work on both directions). The remote node starts to run but then it suddenly stops (for about ~10s), then it runs again (for about ~1s), then it stops... and so on. ", "This on/off behaviour is reflected running rqt_gui: it shows that during the stop-period the CPU % consumed by the node is 0, while it goes to ~5% at the running-period. ", "If the node is launched locally there's no problem with it, so I guess it is not a problem of how the node is implemented (actually, I have tried with other nodes and the same thing happens). Also, the problem persists regardless the specified publishing rate (I have checked for 1Hz and 500Hz and the behaviour is the same). ", "Any ideas on where the problem might come from and what is actually going on?? Please, do not hesitate if more info is needed to solve this. ", "Thanks,", "Ignacio"], "answer": [], "url": "https://answers.ros.org/question/327468/remote-node-running-onoff/"},
{"title": "How to automatically unpause a simulation when sim time is frozen?", "time": "2019-06-18 02:10:09 -0600", "post_content": [" ", " ", " ", " ", "I'm attempting  to synchronize two Gazebo simulations by listening to their clocks and pausing the faster simulation until the slower simulation catches up.", "Each of the simulation are run on a different ROS core and different Gazebo master. The Multimaster FKIE package takes care of the communication between them. The ", " topic is ignored by the Multimaster FKIE topic, so that the ", " topic messages from both simulations don't get merged.", "For each of the simulations there is a topic which publishes their clock, but with distinct name -- ", " and  ", " for the \"slave\" and \"master\" simulations respectively.  Those topics are being shared across both ROS cores. I tried remapping the ", " topic but that didn't work. I'm not sure why.", "The \"slave\" simulation is the slower one, so I run this synchronization node inside master ROS core. The Sincronization node reads the slave and master clocks, pauses master simulation when it gets ahead of the slave simulation and is supposed to unpause it when the slave simulation catches up. But that does not happen. I think the problem is that the synchronization node consumes simulation time, which is not running when the simulation is paused and therefor the node is not running and can't send the service call to unpause the simulation. ", "How can I resolve this issue?", "EDIT", "The first thing I tried was to write a world gazebo_ros plugin that runs on the master world and reads the ", " topic and on every slave_clock message desides whether to pause or unpause the simulation using the ", " method. ", "The slave clock were taken from the message and the master clock obtained with ", " method.", "The simulation paused as expected, but it didn't unpause. I suspect for the same problem. The ", " topic was not active because sim time was freezed and no message was being published.", "I don't know what approach you had in mind exactly. This is how I interpreted it.", "EDIT 2", "Now that I was forced to think about my plugin again, I have found a method in the Gazebo API that returns the world's simulation time ", ". Using this method the plugin does not wait for the message that is not comming and the plugin is able to unpause the simulation.", "This approach works. Thank you.", "This is not the plugin approach I suggested in ", ", right?", " see question update.", "I believe the suggestion in ", " was to ", " the simulation by the delta-t coming from an external source.", "That would seem to be the opposite from what you are doing now. You seem to have:", "the suggested plugin however would do:", "However, your approach is one solution that seems to work for your use-case.", "It would be great if you could make your code available and post a description of your approach (essentially your two edits) as an answer to your own question."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "pause (default situation)", "unpause if there is a message", "upon reception of message: run simulation for "], "question_code": ["/clock", "/clock", "/slave_clock", "/master_clock", "/clock", "/slave_clock", "physics::pause_world(this->world, true)", "ros::topic::waitForMessage<rosgraph_msgs::Clock>(\"/clock\",ros::Duration(0.1))", "/clock", "physics::world::GetSimTime()", "delta_t = clock_now - clock_previous"], "url": "https://answers.ros.org/question/326082/how-to-automatically-unpause-a-simulation-when-sim-time-is-frozen/"},
{"title": "PID motor speed control, integrating motor control feedback to loop [closed]", "time": "2019-07-04 09:54:32 -0600", "post_content": [" ", " ", "Hello,", "I have implemented a pid controller for controlling speed of a N20 motor with gearbox and encoders. \nThe motor drivers I use also allow me to measure and monitor the current used by motors.", "I wonder if I can introduce this current feedback into PID loop, and make a variation of PID algorithm.", "Like when the robot is stationary and motors have just received a command, the current consumption peaks, until the dead zone has passed. I wonder this kind of data be used for better control, gain scheduling, etc.", "Best regards,\nC.A.", "Can you please clarify how this is related to ROS? This sounds like something you should be asking on a more general site like ", " or even a more suitable forum dealing with controls.", "I'm closing this for now, but you should still be able to edit the question. If/when you clarify and this does appear to be a ROS question, we can re-open."], "answer": [], "question_code": ["robotics.stackexchange.com"], "url": "https://answers.ros.org/question/327699/pid-motor-speed-control-integrating-motor-control-feedback-to-loop/"},
{"title": "[ROS-SLAM-Cartographer] Various questions and need support", "time": "2019-07-13 04:26:27 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone, I'm experimenting 2D SLAM using cartographer-ros and had various random questions to which I failed to find an answer. I'm very grateful if anyone can support me with them! Here is my setup:", "Here are my questions:", "Thanks so much for your support!", "Update:", "POSE_GRAPH.optimize_every_n_nodes = 1", "TRAJECTORY_BUILDER_2D.submaps.num_range_data = default (untouched)\n", "I don't know about the rest, but:", "Do I need to sync the timestamp for all published/broadcasted data?", "yes. Always yes.", "It's impossible to relate sensor data (ie: messages) without having a common or synced clock.", "Think about a rotating robot with some sensor on it. A sensor processing node will try to use TF to lookup the exact pose of the robot at the time the sensor data was captured. Imagine a 30 seconds delta-t between the producer and the consumer. How is the consumer ever going to retrieve the ", " pose of the robot if it's not time-synced?", "Tks gvdhoorn!\nThat was also my initial thought when I first know about timestamp in ROS. However, will it cause any problem for the cartographer pkg to run? For example, the latest topic as well as tf odom has timestamp too far in the past while the scan data might be up to 3s newer? This question pop up when I got the validate result from cartographer_rosbag_validate tool. It warns about a huge time gap between received data which is ~3s different!", "Unsynced clocks with multiple hosts in a single ROS nodegraph can cause all sorts of strange problems.", "I would definitely first fix that. Then start looking at other things."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "A turtle robot using arduino for driving motors & getting IMU data", "Attached on it is a Jetson TX2 which running on Ubuntu 16.04 LTS + ROS Kinetic. It collect data from arduino as well as from rpLIDAR A1 from USB port. roscore is running on this one.", "A PC that wirelessly connect to the robot for running heavier tasks like SLAM and navigation. It has Ubuntu 18.04 LTS + ros melodic installed", "Do I need to sync the timestamp for all published/broadcasted data? For example: the arduino publishes wheel speed data to the base controller node on Jetson, from then, it calculates and publish/broadcast odometry data for cartographer node. Do I have to set the timestamp of odom topic to match the timestamp that is published by arduino? Or I can use ros::time::now() for the published odom topic? Same goes for the tf broadcasting.", "My robot only work in 2D environment, so would the ros-localization-wiki ", " be any useful? Or I can directly calculate and publish/broadcast necessary data (like /odom) according to navigation tutorial?", "My current result of SLAM using cartographer has a problem but I don't know which one is the culprit behind it. The local su", "bmap is good but whenever it is added to the global map, it get a slight shift (rotate to be concrete) - see below. Can anyone provide me a suggestion which parameters I should tune in order to get the loop closure to be correct? Could it be because of a drifting in my IMU sensor reading or odom calculation?", "The tuning guide from cartographer doc lack of visual illustration to describe what is a good tuning for local SLAM when global SLAM disabled. Can anyone here share your sample picture or something like for this part of cartographer tuning process?"], "url": "https://answers.ros.org/question/328452/ros-slam-cartographer-various-questions-and-need-support/"},
{"title": "UKF vs EKF performance", "time": "2019-08-27 14:58:07 -0600", "post_content": [" ", " ", "I hear alot about how UKF is better than EKF at the cost of CPU usage. How much are we talking about here in terms of the robot_localization ekf and ukf nodes?", "What metrics are you looking for? These are really well documented algorithms used across many industries. I'd recommend consulting research and benchmark papers. This isn't specific to robot localization.", ":", "at the cost of CPU usage", "so it would appear the metric OP is interested in is \"CPU usage\".", "For most localization methods I would expect these things to be use-case specific. It is probably true that you can rank methods from heavy on the CPU to lightweight, but depending on the size of your map, amount of landmarks/features your sampling methods etc. the differences between methods will change. I don't think you'll find an easy answer to this question (besides collecting your own empirical results). If you don't want to perform real life testing or simulation then at least provide us with more information about your use-case. This way people that have experimented with these methods before using similar setups may be able to provide you with some rough estimates.", "For most localization methods I would expect these things to be use-case specific. It is probably true that you can rank methods from heavy on the CPU to lightweight, but depending on the size of your map, amount of landmarks/features your sampling methods etc. the differences between methods will change. ", "Slightly off-topic, but: big-O analysis and similar methods can definitely be used to express the (runtime) complexity of algorithms, without having to use benchmarking. It would allow to compare algorithms based on their input sizes (ie: \"size of your map, amount of landmarks/features your sampling methods etc\") and say something about which would outperform which.", "Whether that exists for these specific algorithms is something else.", "This is true but I don't think the difference between UKF and EKF is of that magnitude (I could be wrong here), if one would be O(n) and the other O(n^2) for instance, then yes of course you could get a pretty good idea based on the size of your input."], "answer": [], "url": "https://answers.ros.org/question/331776/ukf-vs-ekf-performance/"},
{"title": "Cannot Launch Universal Robot in RViz", "time": "2019-11-11 08:04:48 -0600", "post_content": [" ", " ", "I am trying to launch a UR10 in RViz, and when I launch this command:", "I get the following error message:", "Please show ", " commands you're using and tell us how you've installed things / have setup your workspace.", "Be precise. Without precise information we cannot help you.", "And pedantic (but perhaps also informative):", "Cannot Launch Universal Robot in RViz", "please understand that RViz is not a simulator or offline programming tool. You don't \"load X robot into RViz\". The only thing RViz does is visualise data streams. Those visualisations are affected by values of specific parameters. ", " is one such parameter, which typically contains an textual description of the kinematic (and some dynamic) parameters of a robot model.", "The error message you show seems to indicate that you haven't populated the ", " parameter, so RViz cannot find it and therefore cannot visualise your robot model for you.", "But it's very likely that you're not following the required startup sequence for all of this, hence the question to tell us about which commands you are using, and in which order.", "I apologize for not providing this information. My goal is to \"visualize\" a UR robot doing a pick and place operation. I am assuming from your last post that it is best to do this in Gazebo then? If I am supposed to use Gazebo, what would be the best way to approach simulating a UR doing pick and place operations? I am relatively new to ROS so I apologize for the stupid questions!", "To answer your question, I was able to get it working. My question is, I am getting an error under \"Global Status: Warn\" in the Fixed Frame, I am getting \"No tf data. Actual error: Fixed Frame [world] does not exist\". I was able to move the robot around initially, but now the globes off the flange of the robot are not displaying. Please see the link here for a screenshot of the issue: ", "Do you only want to visualise it (ie: kinematic play-out), or also simulate the dynamics (ie: interact with the environment, \"really\" pick something up)?", "If the former: RViz could be used. If the latter: use a proper simulator (such as Gazebo, but V-REP or Webots would also be options).", "If using a simulator: consider the simulator as a stand-in for real hw. Simulator plugins publish sensor data (ie: robot and world state), your code consumes that and publishes actuation messages. Simulator consumes those and 'makes things move'.", "The latter. So if I am using gazebo, what is the best way to approach my task? I have tried launching gazebo but I can't seem to control the objects? Is it just the C++ files and you run everything in the gazebo environment?"], "answer": [], "question_code": ["roslaunch ur10_moveit_config ur10_moveit_planning_execution.launch sim:=true\n", "[ERROR] [1573248244.342930024]: Robot model parameter not found! Did you remap 'robot_description'?\n[ERROR] [1573248244.362488739]: Robot model not loaded\n[ERROR] [1573248244.376630752]: Planning scene not configured\n", "robot_description", "robot_description"], "url": "https://answers.ros.org/question/337327/cannot-launch-universal-robot-in-rviz/"},
{"title": "How do I deploy a rosdep mirror?", "time": "2019-11-20 12:09:16 -0600", "post_content": [" ", " ", " ", " ", "Github is starting to throttle my requests to ", " . How do I deploy a mirror of such a package index and tell ", " to consume it?", "Edit: responding the obvious question; it's probably in the neighborhood of 5000 invocations daily from my employer's office. I've already taken steps eliminate the ones I make during sundry container initialization, but there are some that I can't eliminate --- notably the one issued by ", " when I bloom my projects.", "Just to get some sense of scale here: how many package (re)builds are you running? Or how many packages are you building per time-unit?", "I'm not aware of ", " using any mirror or cache for ", " specifically, so I'm wondering what it would take for Github to start throttling you.", "Wow, that's a lot of calls. Can you confirm which operations are problematic, is it the rosdep updates that are failing? Those should not be necessary to call that often as the rosdep database doesn't change that often. Most rosdistro operations are designed to use the cache. ", "It would be possible to add an enhancement to rosdep to use a github token to do authenticated fetching with a much higher limit. And bloom could also be enhanced to do the same as well as pass the credentials through.", "For reference ", " implies that this is governed by the unauthenticated API rate limit of 60/hour, Authenticating will get you up to 5000/hour. ", "it's probably in the neighborhood of 5000 invocations daily from my employer's office", "can you say anything about what you're actually doing that requires that many ", "s?", "there are some that I can't eliminate --- notably the one issued by ", " when I bloom my projects.", "5000 ", "s per day (?).", "There's about 50 invocations to ", " daily. I've been able to eliminate all other invocations to ", ". Now, I ", " as ", " during the construction of a build environment and then copy ", " to ", " when I wish to build catkin projects."], "answer": [" ", " ", " ", " ", "As a direct answer to your question you can setup your own rosdep sources. See the documentation here: ", "But I have to agree with ", " that we have used this a lot and have not experienced throttling from github on our CI setups. How often are you calling rosdep update?", "I put together a quick Dockerfile to demo how to use a local copy: ", "You can host the clone of the rosdistro anywhere you want. it's convenient for this demo to be on the filesystem.", "I can certainly replace the default list...", "...with something that references the same gbpdistro and yaml content under ", " URIs instead of ", " URIs. But when I try that trick with the ", "...", "...it errors out looking for non-existent files on my host. Any ideas what's happening there?", "What are you doing to change the DEFAULT_INDEX_URL? What are you running? And what are the errors?", "Here's a ", "demonstrating what I mean: ", "The error produced is as follows:", "I saw your hastebin posting this morning, but now it' won't load on a heroku error ", "Application error\nAn error occurred in the application and your page could not be served. If you are the application owner, check your logs for details. You can do this from the Heroku CLI with the command\nheroku logs --tail", "Shucks. Try this one: ", "I updated my answer with an example of how to use rosdep using a local copy of the rosdistro in a Dockerfile", "That will certainly work for my purposes --- thanks very much.", " ", " ", "As a counterpart, I present my maximally sneaky and invasive implementation:"], "question_code": ["rosdep", "bloom-release", "build.ros.org", "ros/rosdistro", "rosdep update", "bloom-release", "bloom-release", "bloom-release", "rosdep update", "rosdep update", "root", "~root/.ros", "~luser"], "answer_code": ["FROM ros\n# pull a copy of the rosdistro\nRUN mkdir -p /cache && cd /cache && git clone https://github.com/ros/rosdistro.git -b master --depth 1\n# Update it to self reference\nRUN sed -i s#https://raw.githubusercontent.com/ros/rosdistro/master/#file:///cache/rosdistro/# /cache/rosdistro/rosdep/sources.list.d//20-default.list\n# Tell the local environment to use the local instance of rosdistro\nENV ROSDISTRO_INDEX_URL=\"file:///cache/rosdistro/index-v4.yaml\"\n# Tell rosdep where to look for sources\nENV ROSDEP_SOURCE_PATH=/cache/rosdistro/rosdep/sources.list.d\n# Clear system sources for clarity. They will be overridden anyway, but to show for this demo they're not being used.\nRUN rm -rf /etc/ros/rosdep\n\n# Show rosdep working using local resources\nCMD rosdep update\n", "https://github.com/ros/rosdistro/blob/master/rosdep/sources.list.d/20-default.list\n", "file:///", "https://", "DEFAULT_INDEX_URL", "Dockerfile", "    ERROR: error loading sources list:\n    <urlopen error <urlopen error [Errno 2] No such file or directory: '/var/lib/rosdep/crystal/distribution.yaml'>(file:///var/lib/rosdep/crystal/distribution.yaml)>\n", "# frozen-rosdep.dockerfile\nFROM ros\nENV ROSDISTRO_INDEX_URL=\"file:///etc/ros/index-v4.yaml\"\nRUN set -eu \\\n  && apt -y update \\\n  && apt -y install curl \\\n  && curl -fsSL https://github.com/ros/rosdistro/archive/master.tar.gz \\\n       | tar -C /etc/ros --strip-components 1 -xzf- \\\n  && sed -i s,https://raw.githubusercontent.com/ros/rosdistro/master,file:///etc/ros,g \\\n       /etc/ros/rosdep/sources.list.d/20-default.list \\\n  && rosdep update --verbose \\\n  && true\n"], "url": "https://answers.ros.org/question/338122/how-do-i-deploy-a-rosdep-mirror/"},
{"title": "Timestamps of static transforms", "time": "2019-11-29 09:12:05 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to run RTAB-Map on my laptop connected to PAL Robotics TIAGo platform. The robot is the ROS master and my laptop consumes the topics from the robot (sensors, poses etc) and runs RTAB-Map. My TF tree looks like the following:", "The relevant part is only related to the first frames, e.g.: from \"base_footprint\" to \"base_link\". Notice that the most recent transform has a timestamp of 0.000, despite the fact that the transforms are actually being broadcast regularly, as I can see from ", "The transform from \"base_footprint\" to \"base_link\" is a static one. The zeroed timestamps are probably one of the problems preventing RTAB-Map from running correctly in my setup, since sometimes it spits out \"TF_OLD_DATA\" warnings.", "Why does static transforms have zeroed timestamps? How to fix it? ", "Related questions:", " - mentions that zeroed timestamps are expected with TF2, but I didn't understand how he solved it (if at all).", " - mentions \"use_sim_time\", so it's probably related to simulation, which is not my case.", " - mentions similar problem to mine but it's not clear if it's been solved.", "First of: Thanks for actually looking around here and describing the problem, as well as linking to related questions! something you don't find here a lot!", "Coming to your problem:\nI doubt that the static transforms are actually responsible for the problem you are seeing. This is basically the outcome of [1] and [3] cited by you as well.", "Could you please copy'n'paste the full warning you get in your question (please edit it). Also, please more clearly describe which node you are running and maybe show the launch and config file. Thank you.", "Maybe there's time difference between the computers? You should try running ", " server on one of them and use the other as client."], "answer": [], "question_code": ["rosrun tf tf_monitor base_footprint base_link\n"], "url": "https://answers.ros.org/question/338862/timestamps-of-static-transforms/"},
{"title": "Finding/making power cable to get power out of Kobuki", "time": "2019-11-23 09:22:45 -0600", "post_content": [" ", " ", "I have a variant of TurtleBot2 and I'm attempting to get power out of the Kobuki base, so that I can power a small external LCD screen like ", " Based on my very limited electronics knowledge, I believe the 12V/1.5A output from Kobuki should have sufficient power for such a small monitor (consumes 6W on 12V, therefore draws only 0.5A).", "My problem is I don't know where I can buy the power cable to connect between Kobuki and the monitor. There doesn't seem to be one available, and according to another post, I might need to make my own cable: \n", "However I'm an electronics newbie and don't know how to do it.", "Based on the answer on the post above, I understand that I need to order a \"housing connector\" (DigiKey part number WM1783-ND ", ") and need the \"crimp pins\" to put in  and some kind of \"crimp tool\" to stick them together, but I don't know how to do that, and don't know how to connect it with a standard DC male plug so that it become a power cable I can use.", "Any help is appreciated thank you!"], "answer": [], "url": "https://answers.ros.org/question/338404/findingmaking-power-cable-to-get-power-out-of-kobuki/"},
{"title": "RTABMap ICP odometry CPU usage is too high", "time": "2019-12-11 14:25:55 -0600", "post_content": [" ", " ", " ", " ", "I am using RTABMap ICP odometry with the default parameters. I am using Intel Core i5 8th generation and sometimes the node consumes almost 100%! Why is that? And is there something I can do?", "EDIT:\nNot default parameters but these (I think ", " is not used anyway as ", " is false):"], "answer": [" ", " ", "What kind of lidar are you using? With ", " to 0, ", " points will be used for scan matching. I would set it between 0.05 (indoor mapping with 2d lidar), 0.2 (indoor mapping with 3d lidar) and 0.5 (autonomous car with 3D lidar) depending on the lidar and the application. Note also that ", " computes as fast as possible, if the lidar rate is 60 Hz, odometry will try to go as fast as 60 Hz, taking 100% of one CPU core if it cannot process all them at same frame rate.", " is it possible to provide IMU as when turning the odometry gets lost", "It is possible to provide an orientation guess with IMU before doing ICP (see ", " on ", "). However, if there are not enough matches between scan, the resulting transform could still be rejected."], "question_code": ["scan_normal_k", "PointToPlane", "<param name=\"scan_normal_k\"   type=\"int\" value=\"10\"/>   \n\n        <param name=\"Icp/PointToPlane\"  type=\"string\" value=\"false\"/>\n        <param name=\"Icp/VoxelSize\"     type=\"string\" value=\"0\"/>\n        <param name=\"Icp/PM\"            type=\"string\" value=\"true\"/>\n        <param name=\"Icp/PMOutlierRatio\" type=\"string\" value=\"0.65\"/>\n        <param name=\"Odom/GuessMotion\"  type=\"string\" value=\"true\"/>\n        <param name=\"Odom/ResetCountdown\"  type=\"string\" value=\"1\"/>\n"], "answer_code": ["Icp/VoxelSize", "icp_odometry", "guess_frame_id"], "url": "https://answers.ros.org/question/339741/rtabmap-icp-odometry-cpu-usage-is-too-high/"},
{"title": "How do I make MoveIt KDLKinematics consume the robot's current velocity?", "time": "2020-01-10 20:35:07 -0600", "post_content": [" ", " ", "I have a setup using JointStateHandles to report position and velocity to MoveIt, which is using the KDLKinematics solver. I am starting a new solve while the robot is already moving, so the joint velocity is reporting non-zero. I would like the new trajectory to incorporate the velocity state into its solve, but every new trajectory has all the joints starting at velocity 0.", "How can I make the solve incorporate the robot's current velocity into its plan?", "Thanks.", "Afaik this is not supported, as MoveIt (or really: OMPL) does not take any dynamics into account, but is purely a position based planner.", "KDL itself is only used for solving FK and IK. None of the dynamics parts of KDL are used IIRC.", "Adding timing information is done in a later step, with the ", " plugins. But those assume they've been handed a complete trajectory (eg: from 0-state to 0-state).", "You'd have to check whether they already do, but by changing the parameterisation to accept an initial state vector you could achieve what you are asking. But again, it's not supported out-of-the-box.", "PS: an alternative may be to not do this and instead rely on the ", " functionality provided by ", "s ", ".", "thanks for the helpful response. the joint trajectory looks promising, but won't that result in dips in the velocity due to the new trajectory starting from 0 velocity?", "I believe the page I linked to about trajectory replacement discusses this.", "it does. it seems that the answer is yes, it will result in dips. I'm also having some trouble getting trajectory replacement to work in the way I want.", "I have MoveIt repeatedly planning to the same goal and feeding it to the controller. I've manually adjusted the ", " for all of the points in the replacement trajectory so that any points with positive time are forward from where the robot will be at the time it receives the new trajectory. However, the robot is jerking backwards each time it receives a new trajectory. I'm not sure if I'm doing something wrong or the jerkiness is to be expected.", "Thanks.", "If you've created your trajectories properly there should be no jerk, but it also depends on how you're interfacing with your robot controller and which requirements that places on input signals (ie: does it do any filtering, or does it require jerk-limited trajectories fi).", "We've successfully used trajectory replacement for system with a 20 Hz update rate. I don't remember any issues with \"jerking backwards\", but we did not always use MoveIt.", "Part of the jerk is due to the sudden drop in velocity between the two trajectories. Since the initial velocity does not match what the robot is doing, there will be a brief moment in time when it transitions from the old path to the new path where the new path is behind the old path, since it thinks it's starting from a stop. This is why I was hoping the planner could consume the initial velocity. Do you know of any planners that would be able to start planning with a robot that's already in movement? I'm not sure what the proper terminology is to search for such a thing. Thanks."], "answer": [], "question_code": ["ros_control", "joint_trajectory_controller", "time_from_start"], "url": "https://answers.ros.org/question/341503/how-do-i-make-moveit-kdlkinematics-consume-the-robots-current-velocity/"},
{"title": "Using roscpp in existing app on Android/iOS", "time": "2019-11-21 12:29:55 -0600", "post_content": [" ", " ", " ", " ", "Hi there.\nI have an existing app, or rather a shared library, that I compile for mobile platforms (currently Android, but iOS will come very soon too). It is a C++ library that have heavy dependencies (\u201cheavy\u201d in terms of cross-compiling and amount of code).\nNow, I need to publish (and consume) some ROS topics. I was looking at ", " and it seems to me that I should use roscpp for my project.\nHowever, after following through all the ", " I\u2019m still confused on how I would incorporate roscpp into my existing code.\nI would expect to have a set of .so or .a libraries compiled for the target platform (btw, I use arm64 while docker container compiles for arm7) and a headers folder to include in my source code.\nMy question is \u2013 how can I use roscpp in my existing C++ android library? I am not using Android studio, I use ndk-build directly.\nI noticed there\u2019re Application.mk and Android.mk files in \u201croscpp_android_ndk\u201d folder, is that shall be used ultimately?", "Could you elaborate a bit more on your hardware setup? Are you running ROS on a robot and you want to read /publish topics from it from Android/iOS? Or you want to actually run ROS nodes on Android/iOS?", "ROS runs on multple ", " machines. My mobile client needs to publish specific topics that will be consumed by a ROS node. Mobile client will also need to subscribe for few topics.\nI have a c++ library that I NDK-compile and incorporate in mobile application in a third-party framework (not a Java/Swift app)."], "answer": [" ", " ", "I was able to successfully compile roscpp for Android using this ", ". It is a fork of the repo from the ", " but most recent one (dated April 2019).\nOnce libraries are compiled, I added this ", " to glue them into a static library and use it in my project:", "More information can be found ", ".", " ", " ", "you may use the\n", "and you have only json and some web sicket tcp oder udp connection und your done", "is this the de-facto way to go in the cases like mine? we used ros bridge C# implementation in previous project for Unity app, this time wanted to go bare C++ with no WebSocket intermediary...", "It is the easy way to get ros connection. With lowest dependecys. You can use a TCP connection without the websockets. Und you Code ist easy Portable from kinetic to melodic.", " ", " ", "You can use the rosjava API even with otherwise fully native code by using the JNI.", "If you really don't want to use Java then I have some hints here (I hope I have time in the future to write that down properly). Be warned tough that it took me many weeks of fighting various build systems, fixing code and hunting down the right flags!", "All you actually need to communicate is roscpp and its dependencies. Basically you start with ", " and go from there. Throw out everything you don't really really need. Some libraries that ROS uses are cumbersome to cross-compile but easily stripped out by removing a few lines.", "I had successs with Boost 1.69 using ", " (1.70 didn't work) and a current cmake (3.15.5 for me). ", "I have a folder where I ", " all third party dependencies into (like boost or eigen). Everything else is in a normal catkin workspace using the following config:", " is a list of paths where all the third party libraries are installed to, as well as the catkin install folder. Every .so that's ever needed is in there basically so other packages can find it.", "I spent a long time getting a working python 2 for Android. I got python itself running, but haven't managed to get pip and python libraries to cross-compile. You actually don't need any python just for roscpp tough ;)", "I haven't gotten image_transport working, because that relies on some python modules. So I have to transmit uncompressed images which quickly saturates even a gigabit link!", "Some ROS packages didn't compile out of the box and needed a few small fixes.", "As for linking everything into our Android application: I'm using Android's cmake (with Android Studio 3.5.1 and NDK 20.0.5594570) which makes integration of a catkin package quite easy actually: If you've set your ", " properly in the Android project you can just use ", ".\nI don't have any experience using the old ndk-build based system and I warmly recommend cmake instead ;)", "But basically: If you use an install layout in your catkin workspace (", ") you will get an ", " folder that contains all the .so and header files that you can just link to your code.", "Another option might be to use ", " and compile everything you need on the device itself.", "Good luck!"], "answer_code": ["Android.mk", "LOCAL_PATH := $(call my-dir)\n\nstlibs := xmlrpcpp Bullet3Geometry  boost_stacktrace_basic  diagnostic_aggregator  pcl_recognition  orocos-bfl  yaml-cpp  bz2  boost_math_tr1  charset  amcl_sensors  base_local_planner  theoraenc  vorbisfile  orocos-kdl  opencv_imgcodecs3  opencv_xobjdetect3  opencv_videoio3  SDLmain  tf2  boost_signals  image_publisher  SDL_image  image_proc  opencv_reg3  xml2  camera_calibration_parsers  move_base  boost_container  joint_state_listener  boost_context  bondcpp  boost_math_c99f  camera_info_manager  opencv_calib3d3  boost_math_c99l  navfn  tinyxml2  pcl_io_ply  boost_iostreams  opencv_xfeatures2d3  opencv_stereo3  urdfdom_world  boost_thread  eigen_conversions  boost_program_options  roslib  boost_coroutine  pcl_common  opencv_xphoto3  PocoNet  boost_timer  Bullet3Dynamics  opencv_ml3  boost_contract  ogg  opencv_plot3  collada-dom2.4-dp  tf  rosbag_storage  opencv_rgbd3  boost_type_erasure  interactive_markers  boost_log_setup  tinyxml  boost_atomic  flann_cpp_s-gd  pcl_search  laser_geometry  boost_random  pcl_ros_surface   boost_date_time  opencv_structured_light3  urdf  theora  opencv_optflow3  params  qhullcpp  uuid  pcl_surface  map_server_image_loader  rosconsole_backend_interface  urdfdom_model  LinearMath  tf2_ros  Bullet3OpenCL_clew  vorbisenc  pcl_features  pluginlib_tutorials  tf_conversions  opencv_fuzzy3  pcl_registration  opencv_saliency3  boost_test_exec_monitor  theoradec  boost_stacktrace_noop   opencv_img_hash3  opencv_ccalib3  boost_system  PocoUtild  opencv_tracking3  opencv_superres3  opencv_core3  lz4  opencv_surface_matching3  pointcloud_filters  roscpp_serialization  opencv_phase_unwrapping3  compressed_image_transport  compressed_depth_image_transport  move_slow_and_clear  PocoXML  assimp  pcl_kdtree  PocoJSON  opencv_aruco3  cpp_common console_bridge rosconsole_bridge pcl_ros_filters  opencv_ximgproc3  pcl_io  opencv_bgsegm3  boost_exception  pcl_sample_consensus  layers  Bullet3Collision  BulletCollision  robot_state_publisher_solver  opencv_imgproc3  depth_image_proc  rosbag  pcl_filters  stereo_image_proc  octomap  pcl_segmentation  opencv_video3  pcl_stereo  rosconsole_android  boost_math_c99  kdl_conversions  boost_prg_exec_monitor  opencv_dnn3  opencv_line_descriptor3  image_transport_plugins  amcl_map  opencv_objdetect3  pcl_octree  polled_camera  boost_math_tr1l  boost_math_tr1f  voxel_grid  flann_cpp_s  qhullstatic_r  actionlib  boost_wave  PocoUtil  opencv_bioinspired3  image_geometry  theora_image_transport  opencv_text3  kdl_parser  urdfdom_sensor  Bullet3Common  pcl_ros_tf  opencv_highgui3  costmap_2d  opencv_dpm3  Bullet2FileLoader  carrot_planner  nodeletlib  BulletSoftBody  pcl_keypoints  pcl_ros_segmentation  curl  opencv_features2d3  increment  mean  PocoXMLd  boost_log  cv_bridge  roscpp  rotate_recovery  opencv_photo3  SDL  pcl_ros_features  clear_costmap_recovery  opencv_datasets3  rospack  random_numbers  boost_graph  BulletDynamics  iconv  image_rotate  dynamic_reconfigure_config_init_mutex  image_transport  opencv_shape3  octomath  amcl_pf  opencv_flann3  nodelet_math  PocoJSONd  pcl_ros_io  median  rostime  boost_regex  trajectory_planner_ros  message_filters  opencv_videostab3  pcl_ml  PocoFoundationd  global_planner  roslz4  resource_retriever  boost_wserialization  rosconsole  pluginlib  boost_unit_test_framework  opencv_face3  octomap_ros  PocoFoundation  transfer_function  qhullstatic  laser_scan_filters  opencv_stitching3  class_loader  vorbis  urdfdom_model_state  boost_filesystem  geometric_shapes  boost_chrono  boost_serialization  PocoNetd  dwa_local_planner  topic_tools\n\ndefine include_shlib\n$(eval include $$(CLEAR_VARS))\n$(eval LOCAL_MODULE := $(1))\n$(eval LOCAL_SRC_FILES := $$(LOCAL_PATH)/../lib/lib$(1).so)\n$(eval include $$(PREBUILT_SHARED_LIBRARY))\nendef\ndefine include_stlib\n$(eval include $$(CLEAR_VARS))\n$(eval LOCAL_MODULE := $(1))\n$(eval LOCAL_SRC_FILES := ../lib/lib$(1).a)\n$(eval include $$(PREBUILT_STATIC_LIBRARY))\nendef\n\n$(foreach stlib,$(stlibs),$(eval $(call include_stlib,$(stlib))))\n\ninclude $(CLEAR_VARS)\nLOCAL_MODULE    := roscpp_android_ndk\nLOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/../include\nLOCAL_EXPORT_CPPFLAGS := -fexceptions -frtti\nLOCAL_CPP_FEATURES := exceptions\nLOCAL_EXPORT_LDLIBS := $(foreach l,$(stlibs),-l$(l)) -L$(LOCAL_PATH)/../lib\nLOCAL_EXPORT_LDLIBS += -L$(LOCAL_PATH)/../share/OpenCV-3.3.1-dev/3rdparty/lib -ltegra_hal\nLOCAL_STATIC_LIBRARIES := $(stlibs) c++_static\n\ninclude $(BUILD_STATIC_LIBRARY)\n", "rosinstall_generator roscpp --rosdistro melodic --deps", "make install", "catkin config --install --cmake-args \\\n    -DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n    -DANDROID_ABI=arm64-v8a \\\n    -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake \\\n    -DANDROID_TOOLCHAIN=clang \\\n    -DANDROID_NDK=$ANDROID_NDK_HOME \\\n    -DANDROID_NATIVE_API_LEVEL=26 \\\n    -DCMAKE_FIND_ROOT_PATH=<a whole bunch of paths here> \\\n    -DANDROID_STL=c++_shared \\\n    -DBoost_DEBUG=ON \\\n    -DBoost_COMPILER=-clang \\\n    -DBoost_ARCHITECTURE=-a64 \\\n    -DBUILD_SHARED_LIBS=ON \\\n    -DBoost_USE_STATIC_LIBS=ON\n", "CMAKE_FIND_ROOT_PATH", "CMAKE_FIND_ROOT_PATH", "find_package(roscpp)", "catkin config --install", "install"], "url": "https://answers.ros.org/question/338249/using-roscpp-in-existing-app-on-androidios/"},
{"title": "Good practices to publish markers", "time": "2020-01-08 10:22:06 -0600", "post_content": [" ", " ", "Hello", "I would like to know if there are any good practices to publish markers so that rviz consumes as little GPU as possible."], "answer": [], "url": "https://answers.ros.org/question/341255/good-practices-to-publish-markers/"},
{"title": "Improving odometry from RTAB mapping", "time": "2019-04-05 01:51:48 -0600", "post_content": [" ", " ", "I'm using real-time appearance-based mapping  (", ") to perform SLAM with a first-generation Kinect RGBD sensor, and this is working fairly well. However, the odometry is generated by rtabmap wholly through visual information, and so it updates somewhat slowly, and is at risk of losing its fix when a featureless wall fills the view.", "With this in mind, I'd like to supplement the visual odometry with the (likely highly noisy) motor encode+steering angle information I have on hand, as well as, perhaps, IMU data from either the Kinect's on-board accelerometer (no gyro, but I can get linear acceleration components with the ", " package), or the Adafruit 9DoF board.", "It seems that the ", " package can do odometry fusion from multiple sources. However, how can I do this such that rtabmap make use of the extra information, rather than just down stream odometry consumers, like my path planner (", ")? I would like, for instance, for the continued wheel and IMU information to allow us to track position through loss of visual odometry, as in the white-wall situation.", "I did find ", ", which is very close to the what I'm asking about, though it looks like the question wasn't fully answered there.", "My rtabmap launch file is ", ". Would the approach be something like the following?", "You can read more about this project ", ", including a video which can be seen directly ", "."], "answer": [" ", " ", " ", " ", "It appears that this is possible by ", "There may also be some tf frames that need adjusting.", "I gather this from the provided ", " file,\nas well as ", " ", " forum threads.", "I'll edit this answer once I've actually tried this approach.", " Well, this approach did work, but a major difficulty I had was tuning the covariance on my IMU measurements, to be fused with ", ", and also specifying just what types of information should be fused. In particular, I never really  decided whether I should include accelerometer data or not, or whether I should just let it be used for orientation computation (via gravity vector subtraction).", "I tried some wonky things, like allowing a settling time on startup to read the magnitude of the accelerator vector, and then scaling it to be properly g (which it wasn't naturally).", "Additionally, I fused wheel odometry, which initially seemed to help a lot with smoothness of localization, but I think eventually caused drift because my turning angle wasn't, in reality, what I reported it as.", "I now have replaced my Kinect device with an Intel RealSense R435 and T265, and I'm using the localization stream that the black-box T265 provides directly as an odometry source, with visual odometry in RTAB-map turned off completely. This works quite well. I haven't yet brought back the wheel odometry, since I expect it still to cause drift unless I fix that steering-bias problem. The realsense ROS packages provide their own way to ingest external odometry information (such as these wheel/steering measurements), so this would be a complete replacement for ", ".", "Hi, tsbertalan. Could you kindly share how you are using T265? I mean, use it as odometry directly or filter its output before using. Have you experienced any pose drift or abnormal velocity? And, do you have any plan to fuse the VO of T265 with the wheel odometry? Thanks", "Did you managed to fuse the robot wheels odometry into T265? That can be done both using t265 config file or robot_localisation node, but I couldn't manage to get either of them working yet. Thank you!", " See ", " and ", " . I\u2019m using it directly, as in the realsense tutorials. As I mentioned in my answer above, there is the possibility of adding wheel odometry to the realsense (they do the fusion for you, apparently, and robot_localization is not needed), but I think that would cause problems that I\u2019d additionally need to solve. As far as drift goes, the biggest problem I\u2019ve seen is with a lack of loop closure at the dozens-of-meters range due to z misalignment. However, this might be fixable by rtabmap settings (lack of recognition of previously recorded scenery), rather than the fault of the realsense odometry.", "Yes, this should be possible, but, as I mentioned in my own answer, I have some issues with my wheel odometry that might be specific to my built hardware which I\u2019d need to fix first, and I don\u2019t have time to work on these right now,"], "answer_details": ["remapping the output of the visual odometry from ", " to e.g. ", ", ", "reading this in to ", " along with other data sources for fusion, ", "republishing from there to e.g. ", ", and then", "telling the ", " node to use that instead of ", ".", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Run rtabmap's visual odometry, but publish it to an itermediate topic (or, a different tf frame name).", "Use ", " to fuse the visual odometry with my other information sources.", "Publish the fused odometry to the topic that rtabmap expects (or to the tf frame name that it expects)."], "question_code": ["rtabmap_ros", "kinect_aux", "robot_localization", "teb_local_planner", "robot_localization"], "answer_code": ["odom", "/vo", "robot_localization", "/odometry/filtered", "rtabmap", "odom", "robot_localization", "robot_localization"], "url": "https://answers.ros.org/question/320379/improving-odometry-from-rtab-mapping/"},
{"title": "Efficient way to send a new large pointcloud (100.000+ Points)", "time": "2020-01-21 15:06:03 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I have pointers to arrays for x, y, z and intensity data. Now I want to create a Pointcloud message from it but I'm missing the correct idea to this efficiently. I have ~100.000 Points @~30Hz+ that need to be sent in an appropriate time.\nRight now I'm creating a point with x,y,z data and push it into my point cloud in each iteration (100.000+...) which is quite time-consuming.\nI would be really happy if you could point me in the right direction on how to do this with the least amount of computation time. Is there a way to handle it without copying any data?", "Thanks for your help! :) ", "Here is a simplified version of the code I'm using:", "My solution was to do it this way:"], "answer": [" ", " ", "Where are you trying to publish it to?", "One option that will help with reducing the copies and serialization is if you use intraprocess communication by means of Nodelets vs nodes. ", " Its a slightly different API but everything should be pretty straight forward. When you load multiple nodelets into a nodelet manager, they will communicate over intraprocess communication (ei shared ptr) and not serialize and deserialize the messages. ", "I'd also recommend making use of the type masquerading to reduce even further the number of copies you make from conversions. ", "Side note: ROS2 will do this faster and better in concept. I might recommend if you're throwing around that much information, you may want to consider moving to Eloquent.", "Thanks for your answer, interesting point! i'll have a look into that. But I think a major problem is my way to create the actual Pointcloud. Do you have any advice there?", "What is x, y, z? It seems they are just vectors of values, which is all the PointXYZ is as well.", "Exactly these are arrays containing my point cloud data. Do you have any suggestions on how to do this operation more efficient? :)", "Where did the arrays come from? Would it be possible to populate the pointcloud directly rather than these arrays?", "It is actually a callback to another process and I get these pointers back. Changing that will require some serious rework."], "question_code": ["typedef pcl::PointCloud<pcl::PointXYZ> PointCloud;\nPointCloud::Ptr cloud(new PointCloud);\n    for(int i = 0; i < (size = 100000) ; i++)\n    {\n        pcl::PointXYZ point;\n        basic_points.x = x[i];\n        basic_points.y = y[i];\n        basic_points.z = z[i];\n        new_pointcloud->points.push_back(point);\n    }\ncloud->points = new_pointcloud->points;\n", "typedef pcl::PointCloud<pcl::PointXYZ> PointCloud; \nPointCloud::Ptr cloud(new PointCloud);\n    for(int i = 0; i < (size = 100000) ; i++)\n    {\n        cloud->points[i].x = x[i];\n        cloud->points[i].y = y[i];\n        cloud->points[i].z = z[i];\n    }\n"], "answer_code": ["  typedef pcl::PointCloud<pcl::PointXYZ> PointCloud;\n  PointCloud::Ptr cloud(new PointCloud);\n        for(int i = 0; i < (size = 100000) ; i++)\n        {\n            pcl::PointXYZ point;\n            basic_points.x = x[i];\n            basic_points.y = y[i];\n            basic_points.z = z[i];\n            new_pointcloud->points.push_back(point);\n        }\n cloud->points = new_pointcloud->points;\n"], "url": "https://answers.ros.org/question/342222/efficient-way-to-send-a-new-large-pointcloud-100000-points/"},
{"title": "Limit on concurrent connections between nodes?", "time": "2020-01-21 04:02:16 -0600", "post_content": [" ", " ", " ", " ", ":", "In the original question I observed a perceived limit on action client-server connections. It turns out that this limit affected any type of connections (topic, service, action) between nodes.", ":", "I have a network of several hundred of nodes (let's call them server nodes), each implementing an action server. Then there are a handful of client nodes, where each node can potentially open up an action client to communicate with each of the server nodes.", "In tests I have observed that there appears to be a specific limit on concurrent connections between action client and action server, which appears to be at 196 such connections, as the 197th one consistently fails. In roscpp I can initialize the 197th action client, but ", " will block indefinitely. When sending a goal anyway, the server does not receive it. This occurs with both the ", " and ", ".", "I am interested in the limits to ROS' scalability (in terms of number of nodes and connections). Why is it that action connections appear to be hard-capped at 196 and what other such limits exist?", "Would you not be running into a limit of (the) Linux (network stack)? On 'consumer' Linux distributions (or those configured with a consumer 'profile'), settings like max nr of sockets, file descriptors, etc are typically set relatively conservatively.", "You don't tell us which OS you're using, but it would probably be a good idea to check those.", "I'm not claiming there are no further limits caused by how ROS is designed and architected, but the OS is a good source of (artificial) limits that you should first investigate.", "Finally:", "I have a network of several hundred of nodes [..]", "for your sort of question I would suggest to be specific. How many exactly?", "Thank you for your response. You seem to be correct in that I'm dealing with some OS side limitations rather than something directly related to ROS. Actions, topics and services all seem to use up the same 'resource' (whatever that may be, I have to investigate yet). I'm running Kubuntu Bionic.", "To answer your questions regarding the number of nodes: It depends on the scenario, but let's say 800 server nodes. With 800 nodes, the following combinations of connections reached the limit:", "I would first try increasing maximum nr of file descriptors for your user (or the user you're running these experiments with). ", " should show you the current limits.", "Thank you, that seems to indeed have been the problem. The original limit was set to 1024 user-side; I knocked it up quite a bit and am now unable to reproduce the connection limit."], "answer": [" ", " ", "Based on ", "'s comment, the solution was indeed to increase the file descriptor limit, which was set to 1024 by default:", " (arbitrary choice)", "Please note that -- of course -- you are now just ", " the point at which you'll again run into this same limit.", "It may be that other limitations will get in the way first (memory fi), but it's good to realise this.", "And pedantic (but seeing as you seem to be investigating this): the question title as-is suggests a cause, it's not a description of the observation. The question text itself is slightly better, but it still suggests a cause: \"Why is it that action connections appear to be hard-capped\". They weren't, the only thing that happened was:", "In tests I have observed that there appears to be a specific limit on concurrent connections between action client and action server, which appears to be at 196 such connections, as the 197th one consistently fails. ", "this is already a better description of an observation, but it still says ..."], "question_code": ["waitForServer()", "ActionServer", "SimpleActionServer", "actions    topics    services\n156        200       156\n176        100       176\n194        10        100\n", "ulimit -a"], "answer_code": ["ulimit -n 32768"], "url": "https://answers.ros.org/question/342164/limit-on-concurrent-connections-between-nodes/"},
{"title": "/etc/network/interfaces configuration for urg node", "time": "2015-06-17 00:36:54 -0600", "post_content": [" ", " ", "Hi guys. I got problem with the network when im using urg_node for Hokuyo Laser Scan.", "After running roscore, I start up urg_node with:", "However im getting an error that states:", "I looked this up at Google and many of them said to configure the IP address to 192.168.0.15.", "So I edit /etc/network/interfaces with:", "and replace the text with:", "I tested the urg_node already and it's running.", "However, another problem came up. I could not connect to internet. My wifi speed stucked at 1mb/s (using ubuntu now). ", "I came out with another solution by editing the interfaces file to:", "In this case, I can change to use eth0 (able to connect to WIFI but cannot run urg_node) and eth0:0 (cannot connect to WIFI but able to run urg_node) by using this two command:", "                 (Set eth0:0)", "             (Return back to eth0)", "I was wondering whether there is a way I can combine these two function together, that is able to connect to WIFI and run urg_node.", "Thanks for any help!", "Hi i have the exact same issue here. I tried your method but it did not help. I'm using a UST 20LX. Any help? Thanks! :)", "Hi there! Its been quite some time back since I did this project. However, I hope I can help you. I am using UST-20LX too. First of all, you can actually connect the laser to your computer through wired network. If im not wrong, you cant use internet too.", "In order for you to connect to internet, you need to disconnect the wired network and connect to wireless. Thats a bit time consuming so I decided to do the above method. Hokuyo UST-20LX static IP is 192.168.0.10 while my WiFi connection is 192.168.0.10x. Therefore it clashes.", "Thats why I need to create a new static ip of 192.168.0.15, leaving aside the wifi connection of 192.168.0.10x. May I know your internet connection IP?", "Hi thanks for replying! My IP address is 192.168.0.100 for WiFi and 192.168.0.15 for the UST", "If you follow this:\nauto eth0\niface eth0 inet dhcp", "auto eth0:0\niface eth0:0 inet static\naddress 192.168.0.15\nnetmask 255.255.255.0", "there should be no problem.", "After setting this network interface, you need to type ", " to use laser and ", " to use WiFi.", "Or is it that you are facing other errors?", "Sorry i tried ur method and i got this: Cannot find device \"eth0:0\" Failed to bring up eth0:0. i realised my eth port is eth3 so i changed it accordingly and i still get the same results"], "answer": [" ", " ", "What is the IP address range for your Wifi network?", "I suspect you have overlapping IP address ranges for your Hokuyo and your Wifi network.", "Hi there! I checked my Wifi's connection information and the IPv4 is 192.168.0.111. My Hokuyo's IP is 192.168.0.10.", "Yes, your IP address ranges overlap (both are 192.168.0.X). If you can, try changing the IP address of your Hokuyo and the IP of the eth0:0 interface that you use to connect to it. Maybe use 192.168.7.10 and 192.168.7.15 ?", "Wow i can try that! Thanks for your help!", "Hi psprox96. i am also facing same problem. My wifi address is 172.16.x.x and eth0 address is 192.168.x.x but its not working. Could you tell me where am I going wrong?", " ", " ", "Reviving this just because this is still something people need to figure out, & I wrestled with this too.", "The connection out for my robot is different than the IP range for the network connection to the Hokuyo, so I needed to edit /etc/network/interfaces to have an ", " and an ", " interface. I also needed to keep in mind that the metrics for both interfaces needed to be such that ", " has a lower metric than ", ". ", " is the link to the ubuntu manpage for that file. If you want to view the metric info for your interfaces, then ", " will tell you that."], "question_code": ["rosrun urg_node urg_node _ip_address:=\"192.168.0.10\"\n", "[ERROR] [1434516842.963885543]: Error connecting to Hokuyo: Could not open network Hokuyo:\n192.168.0.10:10940\ncould not open ethernet port.\n", "sudo nano /etc/network/interfaces\n", "auto eth0:0\niface eth0:0 inet static\naddress 192.168.0.15\nnetmask 255.255.255.0\n", "auto eth0\niface eth0 inet dhcp\n\nauto eth0:0\niface eth0:0 inet static\naddress 192.168.0.15\nnetmask 255.255.255.0\n", "sudo ifup eth0:0", "sudo ifdown eth0:0"], "answer_code": ["eth0", "eth1", "eth0", "eth1", "route -n"], "url": "https://answers.ros.org/question/211508/etcnetworkinterfaces-configuration-for-urg-node/"},
{"title": "How to prevent intel realsense2_camera driver to publish unnecessary topics", "time": "2020-02-12 07:23:02 -0600", "post_content": [" ", " ", "Hi all,", "My question is very simple. ", "I followed this ", " to use an Intel D435i with ROS. When I execute this command:", "I obtain as a result a huge list of topics published by this nodelet:", "I'm pretty sure that the huge number of topics to be published cause this nodelet to consume many resources. As a result I can see (with \"rostopic hz\") that the frequency of the ", " is ~12Hz while it should be 30Hz. Plus, I really don't need all those topics.", "Is there a way not to publish all \"compressed\" image topics and save some computational resources?", "Thanks.", "Just an observation: as you can see ", ", the node(let) actually doesn't publish or convert data if there are no subscribers.", "The linked lines are just one example. Search for ", " in the code to see other places where it checks.", "Great suggestion. Same strategy is adopted for imu and pose callback. But how can I know if also the \"compressed\" streams are not actually being published?", "They are not, unless there's a subscriber."], "answer": [], "question_code": ["roslaunch realsense2_camera rs_camera.launch\n", "/camera/aligned_depth_to_color/camera_info\n/camera/aligned_depth_to_color/image_raw\n/camera/aligned_depth_to_color/image_raw/compressed\n/camera/aligned_depth_to_color/image_raw/compressed/parameter_descriptions\n/camera/aligned_depth_to_color/image_raw/compressed/parameter_updates\n/camera/aligned_depth_to_color/image_raw/compressedDepth\n/camera/aligned_depth_to_color/image_raw/compressedDepth/parameter_descriptions\n/camera/aligned_depth_to_color/image_raw/compressedDepth/parameter_updates\n/camera/aligned_depth_to_color/image_raw/theora\n/camera/aligned_depth_to_color/image_raw/theora/parameter_descriptions\n/camera/aligned_depth_to_color/image_raw/theora/parameter_updates\n/camera/aligned_depth_to_infra1/camera_info\n/camera/aligned_depth_to_infra1/image_raw\n/camera/aligned_depth_to_infra1/image_raw/compressed\n/camera/aligned_depth_to_infra1/image_raw/compressed/parameter_descriptions\n/camera/aligned_depth_to_infra1/image_raw/compressed/parameter_updates\n/camera/aligned_depth_to_infra1/image_raw/compressedDepth\n/camera/aligned_depth_to_infra1/image_raw/compressedDepth/parameter_descriptions\n/camera/aligned_depth_to_infra1/image_raw/compressedDepth/parameter_updates\n/camera/aligned_depth_to_infra1/image_raw/theora\n/camera/aligned_depth_to_infra1/image_raw/theora/parameter_descriptions\n/camera/aligned_depth_to_infra1/image_raw/theora/parameter_updates\n/camera/color/camera_info\n/camera/color/image_raw\n/camera/color/image_raw/compressed\n/camera/color/image_raw/compressed/parameter_descriptions\n/camera/color/image_raw/compressed/parameter_updates\n/camera/color/image_raw/compressedDepth\n/camera/color/image_raw/compressedDepth/parameter_descriptions\n/camera/color/image_raw/compressedDepth/parameter_updates\n/camera/color/image_raw/theora\n/camera/color/image_raw/theora/parameter_descriptions\n/camera/color/image_raw/theora/parameter_updates\n/camera/depth/camera_info\n/camera/depth/color/points\n/camera/depth/image_rect_raw\n/camera/depth/image_rect_raw/compressed\n/camera/depth/image_rect_raw/compressed/parameter_descriptions\n/camera/depth/image_rect_raw/compressed/parameter_updates\n/camera/depth/image_rect_raw/compressedDepth\n/camera/depth/image_rect_raw/compressedDepth/parameter_descriptions\n/camera/depth/image_rect_raw/compressedDepth/parameter_updates\n/camera/depth/image_rect_raw/theora\n/camera/depth/image_rect_raw/theora/parameter_descriptions\n/camera/depth/image_rect_raw/theora/parameter_updates\n/camera/extrinsics/depth_to_color\n/camera/extrinsics/depth_to_infra1\n/camera/infra1/camera_info\n/camera/infra1/image_rect_raw\n/camera/infra1/image_rect_raw/compressed\n/camera/infra1/image_rect_raw/compressed/parameter_descriptions\n/camera/infra1/image_rect_raw/compressed/parameter_updates\n/camera/infra1/image_rect_raw/compressedDepth\n/camera/infra1/image_rect_raw/compressedDepth/parameter_descriptions\n/camera/infra1/image_rect_raw/compressedDepth/parameter_updates\n/camera/infra1/image_rect_raw/theora\n/camera/infra1/image_rect_raw/theora/parameter_descriptions\n/camera/infra1/image_rect_raw/theora/parameter_updates\n/camera/pointcloud/parameter_descriptions\n/camera/pointcloud/parameter_updates\n/camera/realsense2_camera_manager/bond\n/camera/rgb_camera/auto_exposure_roi/parameter_descriptions\n/camera/rgb_camera/auto_exposure_roi/parameter_updates\n/camera/rgb_camera/parameter_descriptions\n/camera/rgb_camera/parameter_updates\n/camera/stereo_module/auto_exposure_roi/parameter_descriptions\n/camera/stereo_module/auto_exposure_roi/parameter_updates\n/camera/stereo_module/parameter_descriptions\n/camera/stereo_module/parameter_updates\n", "/camera/aligned_depth_to_color/image_raw", "getNumSubscribers"], "url": "https://answers.ros.org/question/343922/how-to-prevent-intel-realsense2_camera-driver-to-publish-unnecessary-topics/"},
{"title": "Using sensor_msgs for control?", "time": "2020-01-09 14:39:02 -0600", "post_content": [" ", " ", "I assume, based on the name, that there is a convention of only using sensor_msgs for feedback from sensors.", "There are times, though, when it would be nice to set a sensor value as a goal or target to use for control. For example, JointState could be reported as feedback from a set of encoders or stepper motors, but it might also be used to command new controller target positions or velocities.", "Would it be considered bad form to use a sensor_msg for a control topic? There would be a name mismatch, but it might be nice to have the goal data in exactly the same form as the feedback data.", "Using control_msgs for control topics would be a better fit, but as control_msgs and sensor_msgs are in separate repositories, they seem to diverge in places in the latest releases or at least have that potential.", "Would it be better to just use a sensor_msg for a control topic and not worry about the name mismatch or would it be better to work with the maintainers of the control_msgs repository to make sure it more closely follows sensor_msgs? Or could it make sense to add a message category to common_interfaces that mirrors sensor_msgs but is used for goal or target topics rather than feedback topics?"], "answer": [" ", " ", " ", " ", "This is my personal opinion. It's not based on any theoretical foundation or necessarily best practice, as agreed upon by \"the ROS community\".", "Would it be considered bad form to use a sensor_msg for a control topic? ", "You already mention it yourself a couple of times: ", " contains messages that are meant to be used to encode data from sensors. Not for control signals. The \"name mismatch\" is actually a very good indication that something is not entirely correct with what you're proposing.", "In the ", " example, this is what the comments in the message state (from ", "):", "This is a message that holds data to describe the state of a set of [..] joints. ", "And that's just the first line. The word \"state\" appears 5 more times in the 10-ish lines of comments. And not as ", " state, but as current (or historical) state.", "Using ", " for control purposes would also mean that you lose the ability to tell whether some nodes are supposed to be able to talk to each other. Again with your ", " example, I could suddenly feed the ", "s coming out of your controller to the ", ".  This node (from ", "):", "[..] allows you to publish the state of a robot to tf. [..]", "So again: the current state, not the desired state. Consumers of TF frames would now not know whether TFs represent future or current state (in other words: the semantics of ", " messages now also become ambiguous).", "These are good examples I believe of what it means to \"violate the semantics\" of messages: both producers and consumers have certain expectations about message form and meaning. With your proposal, the form (syntax) is kept, but the meaning (semantics) are changed. This complicates life for both producers and consumers, as they cannot rely on what they 'know' about these messages, and you lose the ability to unambiguously interpret a message on its own, without requiring additional knowledge (ie: whether a ", " encodes desired or current state).", "Personally I don't think that's a good idea, as I believe one of the strengths of ROS is exactly the standardised semantics of messages (and services, and actions). If you start mucking with that, things will go wrong.", "PS: you could of course use different topics for messages which you are abusing. But that leads again to the idea that messages should be interpretable on their own: without knowing which topic it is that a message arrived on ", " somehow knowing what the meaning is of that topic name (something which you cannot realistically implement in a node), you wouldn't be able to tell which encodes current and which desired state.", "Yes, very good points and I agree. The only reason I considered abusing the message names is the issue of control messages being intimately related to sensor messages in a feedback loop. Right now control messages and sensor messages are in separate packages and repositories and do not seem to be totally compatible. Perhaps that is just due to ROS 2 controller packages not being completely worked out yet.", "Do you think it is better to help work on control messages to make them compatible with sensor messages or do you think it is more appropriate to create some new category like goal messages or target messages that are just copies of sensor messages but document that they are desired state instead of current state? Or might it be better to just have one set of messages with a name like feedback messages that could be used either for current ...", "Perhaps it would be good if you could first explain ", " exactly your issues are with ", ".", "Well, for example, control_msgs seems to only apply to joint kinematics. If you wanted to say set a temperature target or fluid pressure as a goal in a temperature controller or fluid pressure controller, would control_msgs be an appropriate place to add those messages? ", "Plus in the latest versions of control_msgs, the JointJog message was removed, which was most similar to the JointState sensor message, but still seem to imply setting desired kinematic deltas rather than a target value. This particular case is easy to change if the maintainers of control_msgs agree, but some changes might be more debatable. ", "I just think there is a class of controllers that could use messages that are pretty much identical to sensor_msgs to use as control targets and I am wondering where people think is the most appropriate place to store those messages. Having them in the same package or repository as sensor_msgs ...", "In general we tend not to focus on generating standard messages until there's a class of devices upon which to generalize the interface. As a first pass I would recommend creating an application specific message package that would have your specific requirements. ", "As an example using temperature, say you wanted to write a ROS based thermostat. You'd create ", ". And commonly for a thermostat you'd want a few fields beyond the raw pressure, and the header and variance no longer have real meaning in the control sense. I'd define something like this to start.", "As you can see the control message is very different from the sensor message for this application. There's also lots ...", "Thank you, that is great advice. I will just create custom messages and packages for now and someday perhaps open a REP to discuss moving them to standard messages if they are refined enough and generally useful. There are some environments where using standard messages is significantly easier than using custom messages, though, which was making me tempted to abuse standard messages rather than simply create new ones. For example, using ROS with Matlab on Windows only requires a Matlab toolbox when using standard messages, but requires installing Python, CMake, and a compiler to use custom messages. Not that big of a deal and it is pretty trivial using Linux, but enough of a pain on Windows to bias me towards standard messages. Perhaps there are better ways that I could be managing and distributing pre-generated or compiled custom messages rather than regenerating them over and over on each machine.", "For example, using ROS with Matlab on Windows only requires a Matlab toolbox when using standard messages, but requires installing Python, CMake, and a compiler to use custom messages.", "strictly speaking, this is not a problem of ", " or ", ", but of Matlab's ROS support. It should also be solved there (ie: Mathworks should make it easier to use custom messages with their software).", "(Ab)using ", " like you suggested would at best be a work-around for this particular issue in this case.", "Yes I agree and I would like to avoid work-arounds and use best practices whenever possible to best maintain long term stability and compatibility with as many packages and people as possible. I guess I am just asking these sorts of questions in an attempt to fully, if slowly, understand what the community considers best practices in these sorts of situations. At some point I would be curious to learn more about the grand future plans that all of you have for standard messages and ros_controls and device drivers and the like. I imagine many people would prefer to keep standard messages to a bare minimum, but it might also be nice to have good standards that minimize custom messages help maintain compatibility across the larger ROS ecosystem."], "answer_code": ["sensor_msgs", "JointState", "sensor_msgs", "JointState", "JointState", "robot_state_publisher", "JointState", "control_msgs", "thermostat_msgs", "ThermostatSetPoint.msg", "int32 OFF=0\nint32 HEAT=1\nint32 COOL=2\nint32 HEAT_OR_COOL3\n\nint32 control_mode\n\nfloat64 target_temperature # Degress Celsious\nfloat64 control_deadband # Degrees Celsius error within which not to trigger the furnace.\n", "control_msgs", "sensor_msgs", "sensor_msgs"], "url": "https://answers.ros.org/question/341369/using-sensor_msgs-for-control/"},
{"title": "How to roslaunch cwd=node other project?", "time": "2019-12-14 13:33:23 -0600", "post_content": [" ", " ", "I know about the ", " attribute ", ":", "cwd=\"ROS_HOME|node\"(optional)", "If 'node', the working directory of the node will be set to the same directory as the node's executable.", "-- ", "If I want to start my node in the same directory as _another_ node's executable, how might I do that?", "In brief, I want an easier way of locating assets belonging to other projects. The simplest trick I know is to ", " a wrapper script wherein I sniff the location of another named project:", "In brief, I want an easier way of locating assets belonging to other projects.", "From your example it's unclear why you cannot use a regular ", " roslaunch ", ". Please clarify.", "Because", "gives", "I may not have been clear. I was specifically asking about this:", "In brief, I want an easier way of locating assets belonging to other projects.", "If I understand this correctly, this is typically done using something like this (just an example):", "What's unclear to me is what you really want to do.", "You're asking a question about your selected solution, but it's unclear whether it is an actual solution for what you really want to do (ie; xy-problem).", "Starting one node in another node's executable directory _is_ what I \"really want to do\". I can specialize the problem to fit what ", " can do, but then I lose the abstraction of launching nodes in arbitrary project binary directories."], "answer": [" ", " ", " ", " ", "Starting one node in another node's executable directory _is_ what I \"really want to do\"", "with the current implementation of ", " this is not supported.", "So the answer to this would be: you can't.", "then I lose the abstraction of launching nodes in arbitrary project binary directories.", "This is also not really supported. It can work, but it is not recommended to depend on this.", "Accepted practice would be to use absolute paths to resources (ie: those returned by the ", " subst arg) and/or pass ROS args to your nodes in the ", " section of the ", " that launches them.", "Location transparency cannot be guaranteed if you start making assumptions about what the ", " is of nodes. This is exactly why using ", " and passing the result of that to your node and/or using ROS parameters (populated with the output of ", " again) is considered 'better'.", "The bash script you show is afaict unneeded: you could add this to the ", " element in your ", " file:", "that should result in the same path being passed to your node. And ", " will complain (ie: abort the launch) if ", " cannot be found.", "But I believe passing absolute paths and/or using ROS params would be more robust (see previous comment).", "Point taken; thank you for the explanation.", "And this will sound really pedantic, which is not my intention, so apologies in advance, but:", "Starting one node in another node's executable directory _is_ what I \"really want to do\"", "from your other comments it does not seem to be what you really want to do.", "From this:", "In brief, I want an easier way of locating assets belonging to other projects.", "and", "then I lose the abstraction of launching nodes in arbitrary project binary directories.", "it sounds like you'd like to use paths relative to the ", " of a node and access resources through those paths. Would it be incorrect to assume that you have packages providing configuration files, or other application specific files that you'd like reusable nodes (in other packages) to load through those paths?", "That assumption would indeed be incorrect. This is going to sound equally pedantic: configuration files, by ", ", live under ", ". It's unclear to me how a catkin project might contain such a file without heavily overriding its installation prefix.", "No, the files I'm interesting in consuming between projects would be things to support the execution of an executable in a way that (if done for every executable) would be duplicative to the point of impracticality. E.g.: linker scripts, ", " shared objects, ", "ed shared objects.", "configuration files, by ", ", live under ", "perhaps in a \"regular\" Linux system, but certainly not when using the normal ROS (1) work and development workflow.", "Configuration files (ie: ", ", ", " and similar files) would be hosted by ROS packages and located using ", ". This would again be to maintain location independence.", "No, the files I'm interesting in consuming between projects would be things to support the execution of an executable in a way that (if done for every executable) would be duplicative to the point of impracticality. E.g.: linker scripts, ", " shared objects, ", "ed shared objects.", "The first is a type of files I've not seen consumed by ROS nodes very often (if at all). The latter two examples seem like files which should be either on the relevant search path(s) already or placed there using other means.", "You're the only one to ...", ".. seem to be one that diverges from (what is typically seen as) accepted development practices in ROS 1 environments (but of course I'm not pretending to have seen them all, nor am I claiming that what you're doing is wrong in any way).", "Re-reading your comment: especially the linker scripts sound like things which are not used at runtime. ", " is a runtime tool. Setting up library search paths could also be considered a runtime activity (or perhaps deployment or configuration), but in any case: what you're describing doesn't sound like what ", " would typically be used for. That may explain why you are having difficulty finding approaches."], "question_code": ["roslaunch", "cwd", "roslaunch", "#!/bin/sh\nif ! other_pkg_dir=\"$(rospack find other_pkg)\"; then\n    echo \"oh noes!\"\n    exit 1\nfi\nmy_package --other-package-dir=\"${other_pkg_dir}\" $@\n", "$(find ..)", "cwd=$(find other_pkg)\n", "Invalid <node> tag: cwd must be one of 'ROS_HOME', 'node'.\n", "<arg name=\"resource_in_other_pkg\" value=\"$(find other_pkg)/path/in/other/pkg/to/resource\" />\n", "roslaunch"], "answer_code": ["roslaunch", "find", "args", "node", "CWD", "find", "find", "node", ".launch", "args=\"--other-package-dir='$(find other_pkg)'\"\n", "roslaunch", "other_pkg", "CWD", "/etc/", "LD_PRELOAD", "dlopen", "/etc/", ".yaml", ".rviz", "find", "LD_PRELOAD", "dlopen", "roslaunch", "roslaunch"], "url": "https://answers.ros.org/question/339946/how-to-roslaunch-cwdnode-other-project/"},
{"title": "Why my publisher node stops when subscriber node start?", "time": "2019-06-11 22:34:04 -0600", "post_content": [" ", " ", " ", " ", "Hi all, ", "I currently working on creating a GPR system based on Walabot sensor. The data that I want to publish is the raw signal from Walabot which consist of two-row array of amplitude and timeaxis value with 4096 columns for each row. Hence, using float32multiarray i was hoping that i could send the array over to subscriber node for signal processing. I followed the normal procedure of roscore -> publisher -> subscriber. ", "As soon as I turn on subscriber, my publisher immediately stops. The frustrating part is both terminal of publisher and subscriber didn't print out want error that has happened which leads to more confusion.", "I have tested the way is publish and subscribe to Float32MultiArray message type using simple array. The way I did work but when I do the complicated node is does not work. I included my simple publisher and subscriber that use Float32MultiArray as message type.", "Here is ", "Hope anyone could help in this. Thank you!", "This is my publisher node.", "This is my subscriber node:", "My simple testing of using Float32MultiArray message in Publisher and Subscriber\nPublisher:", "Subscriber:", "The publisher also stop when I use rostopic echo", "What is a \"GPR system?\"", "ground penetrating radar system", "what is the contents of ", "? ", " is not a simple msg, it needs all of its fields properly setup for it to work. Refer to the ", ". Only setting ", " is probably not going to work.", "Please also note: usage of msgs from ", " is discouraged. It would be better to use a more semantically meaningful message type (a ", " could carry ", ", not just a scan of a GPR sensor).", "Thank you for your reply.", "targets is a arrray with 2 rows * 4096 columns of data with raw signal amplitude and time (nanoseconds).", "I just notice that when I add another row to my simple publisher, the same problem happens. It is probably because the way I setting the publisher and I also don't really know how to properly subscribe to it either. I have tried the documentation before this but I don't know how to set the parameter as it has parameters inside parameters. ", "Yeah I'm kinda new in using ros and a terrible programmer. I just wanted to publish a 2*4096 array. I thought Float32MultiArray would do the trick since there isn't other message type that could carry array. Float32MultiArray seems doesn't have many documentation on it in the internet which makes it even harder for me to properly set it up.", "There are a few Q&As that deal with ", " messages. Most are C++ though, so you'll have to adapt a bit. See ", " for one such question.", "You must initialise the various fields properly. Otherwise it probably won't work.", "Yeah that's the problem. I'm pretty weak in C++. I looking for a python alternative. While I have the chance, can I ask regarding the question that you just suggested. Since I'm just gonna post 2*4096 array of amplitude value and time, do I need to define the stride? What is stride anyway? I think I getting the idea but I can't wrapped my head around the 'stride' and this code here. Is it necessary to go through row by row and col by col?"], "answer": [" ", " ", "The cause to my problem is that my publishing message doesn't have the same row of array as my subscriber's. Hence, the node stop. To solve this, experts advised my to make custom message to carry the message that I want. I created msg file to carry two array that I want and it works. I would advise the readers to do the same to avoid from being frustrated in using Float32MultiArray like me. Just do custom message. Don't think too much about it. Goodluck!", "I would advise the readers to do the same to avoid from being frustrated in using Float32MultiArray like me. ", "That was not why we suggested you use a custom message.", "Just do custom message. Don't think too much about it.", "In fact: future readers: please do think about it.", "Designing messages is not to be taken lightly, as it can significantly affect the consumers of your datastreams."], "question_code": ["#!/usr/bin/env python\nfrom __future__ import print_function  # WalabotAPI works on both Python 2 an 3.\nfrom sys import platform\nfrom os import system\nfrom imp import load_source\nfrom os.path import join\nimport rospy\nfrom std_msgs.msg import Float32MultiArray\n\n\nmodulePath = join('/usr', 'share', 'walabot', 'python', 'WalabotAPI.py')\nwlbt = load_source('WalabotAPI', modulePath)\n\nwlbt.Init()\n\ndef DataCollect():\n    # wlbt.SetArenaX - input parameters\n    xArenaMin, xArenaMax, xArenaRes = -10, 10, 0.5\n    # wlbt.SetArenaY - input parameters\n    yArenaMin, yArenaMax, yArenaRes = -10, 10, 0.5\n    # wlbt.SetArenaZ - input parameters\n    zArenaMin, zArenaMax, zArenaRes = 5, 11, 0.5\n\n    # Initializes walabot lib\n    wlbt.SetSettingsFolder()\n\n    # Establish a connection between the Walabot and the computer\n    wlbt.ConnectAny()\n\n    # Set sensor profile\n    wlbt.SetProfile(wlbt.PROF_SHORT_RANGE_IMAGING)\n    # Set arena by Cartesian coordinates, with arena resolution\n    wlbt.SetArenaX(xArenaMin, xArenaMax, xArenaRes)\n    wlbt.SetArenaY(yArenaMin, yArenaMax, yArenaRes)\n    wlbt.SetArenaZ(zArenaMin, zArenaMax, zArenaRes)\n    wlbt.SetThreshold(50)\n\n    # Set filtering to none\n    wlbt.SetDynamicImageFilter(wlbt.FILTER_TYPE_NONE)\n\n    pair = wlbt.GetAntennaPairs()\n\n    # Start the Walabot device\n    wlbt.Start()\n\n    pub = rospy.Publisher('rawSignal', Float32MultiArray, queue_size=1000)\n    rospy.init_node('walabotRawSignal', anonymous=False)\n\n\n    while not rospy.is_shutdown():\n        rospy.sleep(2.0)\n        try:\n            wlbt.Trigger()\n            targets = wlbt.GetSignal((pair[4]))\n            rawSignalArray = Float32MultiArray(data=targets)\n            pub.publish(rawSignalArray)\n            rospy.loginfo(rawSignalArray)\n        except rospy.ROSInterruptException:\n            pass\n\n    wlbt.Stop()\n    wlbt.Disconnect()\n    print(\"Terminate successfully\")\n\nif __name__ == '__main__':\n    DataCollect()\n", "#!/usr/bin/env python\nimport rospy\nfrom std_msgs.msg import Float32MultiArray\n\ndef callback(data):\n    rospy.loginfo(data.data)\n\ndef processRawSignal():\n    rospy.init_node('processRawSignal', anonymous=True)\n    rospy.Subscriber(\"rawSignal\", Float32MultiArray, callback)\n    rospy.spin()\n\nif __name__ == '__main__':\n    processRawSignal()\n", "#!/usr/bin/env python\n\nimport rospy\nfrom std_msgs.msg import Float32MultiArray\n\ndef talker():\n    pub = rospy.Publisher('chatter', Float32MultiArray, queue_size=1000)\n    rospy.init_node('talker', anonymous=True)\n    rate = rospy.Rate(10) # 10hz\n    while not rospy.is_shutdown():\n        hello_str = [1,2,3,4,5]\n        array = Float32MultiArray(data=hello_str)\n        rospy.loginfo(array)\n        pub.publish(array)\n        rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        talker()\n    except rospy.ROSInterruptException:\n        pass\n", "#!/usr/bin/env python\nimport ...", "rawSignalArray = Float32MultiArray(data=targets)\n", "targets", "Float32MultiArray", "data", "std_mgs", "Float32MultiArray", "*MultiArray", "dat.layout.dim[0].stride = H*W;\ndat.layout.dim[1].stride = W;\ndat.layout.data_offset = 0;\nstd::vector<int> vec(W*H, 0);\nfor (int i=0; i<H; i++)\n    for (int j=0; j<W; j++)\n        vec[i*W + j] = array[i][j];\ndat.data = vec;\n"], "url": "https://answers.ros.org/question/325416/why-my-publisher-node-stops-when-subscriber-node-start/"},
{"title": "How do you use two lidars that are placed at a diagonal(scan area of 270 degrees)?", "time": "2019-04-26 04:02:20 -0600", "post_content": [" ", " ", "\nNow,I have a AGV. I want to use the lidars' data.\nTwo lidars whose scan area of 270 degrees are placed at a diagonal.How to combine two data into one data?\nOr,in the applications,the two data are used separately?"], "answer": [" ", " ", "This very much depends on the application. It is fairly easy to convert both LIDAR's data to a single pointcloud by first separately converting them, transforming them into a common frame and the combining them. This would largely work correctly for consumers that are only interested in the scan endpoints of both sensors, but raytracing a path from sensor origin to endpoints (as many grid mapping approaches do) would no be possible anymore.", "See ", " for an implementation."], "url": "https://answers.ros.org/question/321969/how-do-you-use-two-lidars-that-are-placed-at-a-diagonalscan-area-of-270-degrees/"},
{"title": "Is it possible to delete a static TFs", "time": "2019-01-03 04:23:29 -0600", "post_content": [" ", " ", " ", " ", "Hello ros community,\nI am using static TFs for the Transformation between Robot Flansch and TCP. Now i wonder if i am able to get rid of ones sent static tfs (broadcaster = tf2_ros.StaticTransformBroadcaster()). Atm I am just able to send a new static TF for a new TCP. But the \u201cold\u201d static TF is then ofc still in the \u201cTF Broadcaster\u201d and visible in Rviz.", "I thought, I ones red something about it, but now I cannot find a solution in the documentation.", "best wishes and a happy new Year\nmgangl", "If you stop publishing a TF then it should be graphically shown to fade out in RVIZ after a few seconds. It may remain in the list because RVIZ remembers it was published in the past, but if a TF hasn't been published for a 60 seconds or more it realistically no longer exists.", "you are right for using the \"TransformBroadcaster()\", \nbut i am talking about the \"StaticTransformBroadcaster()\". The idea is to publish a TF only once, since it is STATIC."], "answer": [" ", " ", "You could republish the static tf with the same child_frame_id but with a new parent that detaches it from your tf tree- the parent should have no static or dynamic transforms to any other frame in your scene.  This will produce exceptions in lookups that try to use it which need to be handled appropriately, rviz will have some complaints (e.g. ", ") about it but anything hanging off of that child frame should disappear.  I just now tested this with rviz tf, not robot model or Markers but they should disappear also.", " ", " ", "This will make more sense if I explain how the TF system works because even static transforms need broadcasting at regular intervals.", "ROS components that are consumers of TF information use a TF listener, this listener is created not knowing any transforms. It listens to the TF topic and over time receives TF messages which allow it to fill up a buffer of TF information so it can solve TF queries. So if a TF isn't being regularly broadcast, even if it's static, then consumers of TF information will not know that TF exists. This is the reason why you specify a frequency when creating a static transform publisher node.", "As such there is not concept of 'deleting' a transform. But if it's no longer published eventually it will no longer be present in the TF buffers of all registered listener objects, at this point it effectively no longer exists.", "Hope this makes sense.", "[..] because even static transforms need broadcasting at regular intervals.", "Static TF works slightly different: under the hood, ", " publishers are used for static TF broadcasters, removing the need for period republishing of the same transform, which was the goal of TF2 and static TFs.", "hmm, but if the listener is created before the static TF got broadcasted  it is in the buffer of the listener. I guess i have to test it, but in this Scenario the static TF will stay for that listener for ever.", "I also guess, that rviz has just an build in listener.", "@gvdhoorn\u00a0 thanks for clarifying. Sry for the question again. I just want to be sure. Is there a way to delete static tfs? i guess not.", "\"deleting a TF\" is a strange question, as it implies that they are stored somewhere and then retrieved. It doesn't work that way. As Peter already explained, consumers receive broadcasted transforms and cache them locally. Any queries inside those consumers are performed against that local cache. ..", ".. If a consumer has never heard of a transform, queries for it will fail.", "There is no \"delete\" operation defined for TF publications. Simply not broadcasting them means that information about them will go stale, and eventually queries will return errors as timestamps will have diverged too ..", ".. much from when the last information about a transform was received.", "Latching publishers make this all a bit more difficult, as late joiners will always receive the last message published on those topics, even if it is (very) old. So with static TF, the publisher should not be around any more ..", ".. for a transform to \"not exist\" any more (but note again: that is a strange thing to say). ", " the last msg broadcast by a producer of transforms must not include any information about a transform any longer. In that case consumers will not receive it (and late joiners will never have seen it).", "Ok thanks. Nevertheless, i think a functionality to rename or delete STATIC TFs would be a nice to have. Ty for your time."], "answer_code": ["No transform from [foo] to frame [bar]"], "url": "https://answers.ros.org/question/311864/is-it-possible-to-delete-a-static-tfs/"},
{"title": "Creating an array/list of existing message type without defining custom message type?", "time": "2019-02-04 16:16:01 -0600", "post_content": [" ", " ", " ", " ", "Is there a way to create an array of an existing message type without defining another message type?", "As a use-case, say you have many ", " files, each with a username. You compile them into a summary bag file and would like all unique usernames in a ", " topic for convenience. Is there any way to accomplish something like this without a custom message type?", "It seems that an array of an existing message is such a simple extension that I'd figure such a mechanism exists. My current approach was to create a new package, ", " with:", "Is this expected or am I overlooking something? I do note the following from the ", ":", "...it's usually \"better\" (in the sense of making the code easier to understand, etc.) when developers use or create non-generic message types", "Perhaps the intention is to force more project-specific message creation rather than relying on built-in, canned examples. This use-case might be somewhat fringe where I truly am looking for a \"throwaway\"; all I care about is getting a list of very basic values into the output ", " vs. something like sensor data specific to a more complicated process."], "answer": [" ", " ", "You already cite the correct wiki page about the purpose of ", ".", "For integer/float values, there are the ", " messages, which could suite your needs for numbers. For strings, you either have to create a new custom message, or wriggle your way through something like adding a seperator between all users concatenated into a single string...", "In short: There is no way to create a message type to be sent over ROS or written to the bag file without properly creating a ", " file. This is required for creation of the language specific representations, such that you can ", " or ", " them.", "[..] such that you can ", " or ", " them.", "I would go one step further: being able to ", " or ", " them is an implementation detail. The main point of having to be explicit about message types (ie: having to create them) is to make sure both syntax and semantics are ..", ".. properly encoded/stored somewhere.", ": how would we do that without having a message type defined?", ": agreed!", "@gvdhoom: the question was prompted by the fact that a ", " msg corresponding to some ", " can be used as a ", ". Is a ", " a \"custom message type\" in your mind? My question asks if one must use a custom message to consume or provide an array of an existing message.", "It seems that answer is \"Yes,\" so now I know. For this particular scenario it seemed \"heavy\" to do this when I really just want a list of strings or floats. Still, it seems this is the way, so now I know for sure.", "No: ", " is not a custom message.", "The message ", " a field named ", " that is an arbitrary length array (ie: a list) would be a custom message.", "Pedantic perhaps, but two very different things.", "re: heavy: thing is, a list of ", "s is ", " semantically meaningful. Unless ..", ".. you give the message a semantically meaningful name. ", " is what you should create a custom message.", "Besides, it's technically impossible to send, receive or store message data without an associated message type, so even if you see it as a lot of overhead, there wouldn't be any other way.", "Thanks for the reasoning. This is indeed a narrow use case. For the purpose of storing metadata, the field name doesn't matter to me as it's named by the topic (e.g. ", ") I assign. Again, pretty fringe use case."], "question_code": [".bag", "metadata", "users = get_unique_users() # a function returns a list of strings: [user1, user2, ..., userN]\noutbag = rosbag.Bag('/path/to/file.bag', 'w')\noutbag.write('/metadata/unique_users', std_msgs.msg.String(users))\n", "std_msgs_array", "$ rosmsg show std_msgs_array/StringArray \nstring[] data\n\n$ rosmsg show std_msgs_array/Float64Array \nfloat64[] data\n", ".bag"], "answer_code": ["std_msgs", "MultiArray", ".msg", "#include", "import", "#include", "import", "#include", "import", "foo", "Foo.msg", "foo[]", "foo[]", "foo[]", "foo", "float", "/metadata/foo"], "url": "https://answers.ros.org/question/314702/creating-an-arraylist-of-existing-message-type-without-defining-custom-message-type/"},
{"title": "Fetch robot, joint_state topic sometimes contains all joints, sometimes doesn't", "time": "2018-05-16 13:52:37 -0600", "post_content": [" ", " ", "I am programming with the Fetch robot (the physical one, not Gazebo simulation).", "I use my laptop with the following settings:", "Also the ROS variables:", "I have the following ROS topics:", "So far so good. Now I want to check the joint angles. What I often do here is echo the corresponding ROS topic. Here I call ", " repeatedly. I press ENTER on my keyboard, then I count 4-5 seconds in my head, then press ENTER again, then count 4-5 seconds, press ENTER again, and so forth. Check out the output after running this several times:", "Notice that in two of the above cases, all the links of the Fetch are present.", "No, they're not. ", " the gripper joints are there, or the others.", "I don't have a Fetch, but the gripper probably uses a separate ", " publisher, which only publishes for the gripper. The other joints ..", ".. are published by another ", " publisher.", "That is all perfectly valid and supported. What I don't understand though is why Fetch would configure their robot that way. Typically different publishers would be placed in separate namespaces and ", " would be configured ..", ".. with the ", " parameter to subscribe to all topics, coalesce the msgs into a single ", " msg and publish that on ", ".", "You'd have to check the documentation and / or ask Fetch as to why that is not the case here.", "Sorry, you are are right, I did not read carefully. Yeah it's ", " the gripper or the joints. Yeah let me look over at the Fetch github repository for ROS (their actual docs don't explain this topic)", " I found an explanation somewhat  ", "  embarrassed that I didn't see this earlier. The wiki says that each message may only contain a subset of the joints. I see. ", "I would be very surprised if there was no documentation at all about this topic. I'd really recommend you ask them. If you have one of their robots, you should be entitled to some support, no?", "Yes, all customers are entitled to support, and we're usually busy with the commercial side of things so ROS answers aren't the best way to get help.", "I know this is documented somewhere more officially - I'll try to get a link."], "answer": [" ", " ", "As gvdhoorn mentioned, I made a mistake here, either the gripper joints are there, or the other joints are there.", "This is expected behavior. ", ".", "Luckily for us, the robot continuously\n  publishes the current joint angles to\n  the /joint_states topic. However, each\n  message might only contain a subset of\n  the joints. This works because\n  multiple nodes can publish the state\n  of the subset of joints they are\n  responsible for. This is how\n  /joint_states works on the real robot.\n  However, in simulation, all of the\n  joint states are published by Gazebo.", "Because no single message on the\n  /joint_states topic will necessarily\n  tell us about all the joints of the\n  robot, we must listen to multiple\n  messages over time and accumulate the\n  full state of the robot. In practice,\n  the joint states are published very\n  quickly, so we will not have to wait\n  long.", "Note that the wiki you link to is not in any way 'official'. It's just a page on a wiki for a course. I would still recommend to ask Fetch about this.", "we must listen to multiple messages over time and accumulate the full state of the robot", "as I wrote, there are standard tools that can do ..", ".. this for you, no need to implement it in all consumers. But to be able to use those tools, things have to be configured in a certain way. That is where Fetch comes in, as they must have had a reason to configure your robot the way it is.", "I have contacted Fetch support directly. Thank you for the suggestion.", "If/when you get a response, it would be great if you could update your answer.", "I believe the reason the gripper state publishes separately from the rest of the robots states is because the gripper is modular. Other grippers can be purchased from Schunk, Shadow, Robotiq and used on the Fetch, it has the same gripper mount as other commercially available robots.", "I agree this needs to be documented somewhere more clearly.", "Thanks ", "!"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Macbook Pro Late 2013 edition", "Ubuntu 14.04 installed as sole operating system", "ROS Indigo", "Plus the ", "e."], "question_code": ["<fetch>~/FETCH_CORE/fetch_core$ env | grep ROS\nROS_ROOT=/opt/ros/indigo/share/ros\nROS_PACKAGE_PATH=/opt/ros/indigo/share:/opt/ros/indigo/stacks\nROS_MASTER_URI=http://fetch59.local:11311\nROSLISP_PACKAGE_DIRECTORIES=\nROS_DISTRO=indigo\nROS_IP=10.0.0.121\nROS_HOME=/home/daniel/.ros\nROS_ETC_DIR=/opt/ros/indigo/etc/ros\n", "<fetch>~/FETCH_CORE/fetch_core$ rostopic list\n/arm_controller/cartesian_twist/command\n/arm_controller/follow_joint_trajectory/cancel\n/arm_controller/follow_joint_trajectory/feedback\n/arm_controller/follow_joint_trajectory/goal\n/arm_controller/follow_joint_trajectory/result\n/arm_controller/follow_joint_trajectory/status\n/arm_with_torso_controller/follow_joint_trajectory/cancel\n/arm_with_torso_controller/follow_joint_trajectory/feedback\n/arm_with_torso_controller/follow_joint_trajectory/goal\n/arm_with_torso_controller/follow_joint_trajectory/result\n/arm_with_torso_controller/follow_joint_trajectory/status\n/base_controller/command\n/base_scan\n/base_scan_no_self_filter\n/base_scan_raw\n/base_scan_tagged\n/battery_state\n/charge_lockout/cancel\n/charge_lockout/feedback\n/charge_lockout/goal\n/charge_lockout/result\n/charge_lockout/status\n/cmd_vel\n/cmd_vel_mux/selected\n/diagnostics\n/diagnostics_agg\n/diagnostics_toplevel_state\n/dock/result\n/enable_software_runstop\n/graft/state\n/gripper/gyro_offset\n/gripper/imu\n/gripper/imu_raw\n/gripper_controller/gripper_action/cancel\n/gripper_controller/gripper_action/feedback\n/gripper_controller/gripper_action/goal\n/gripper_controller/gripper_action/result\n/gripper_controller/gripper_action/status\n/gripper_controller/led_action/cancel\n/gripper_controller/led_action/feedback\n/gripper_controller/led_action/goal\n/gripper_controller/led_action/result\n/gripper_controller/led_action/status\n/gripper_state\n/head_camera/crop_decimate/parameter_descriptions\n/head_camera/crop_decimate/parameter_updates\n/head_camera/depth/camera_info\n/head_camera/depth/image\n/head_camera/depth/image_raw\n/head_camera/depth/image_rect\n/head_camera/depth/image_rect_raw\n/head_camera/depth/points\n/head_camera/depth_downsample/camera_info\n/head_camera/depth_downsample/image_raw\n/head_camera/depth_downsample/points\n/head_camera/depth_rectify_depth/parameter_descriptions\n/head_camera/depth_rectify_depth/parameter_updates\n/head_camera/depth_registered/camera_info\n/head_camera/depth_registered/hw_registered/image_rect\n/head_camera/depth_registered/hw_registered/image_rect_raw\n/head_camera/depth_registered/image\n/head_camera/depth_registered/image_raw\n/head_camera/depth_registered/points\n/head_camera/depth_registered_rectify_depth/parameter_descriptions\n/head_camera/depth_registered_rectify_depth/parameter_updates\n/head_camera/driver/parameter_descriptions\n/head_camera/driver/parameter_updates\n/head_camera/head_camera_nodelet_manager/bond\n/head_camera/ir/camera_info\n/head_camera/ir/image\n/head_camera/projector/camera_info\n/head_camera/rgb/camera_info\n/head_camera/rgb/image_raw\n/head_camera/rgb/image_rect_color\n/head_camera/rgb_rectify_color/parameter_descriptions\n/head_camera/rgb_rectify_color/parameter_updates\n/head_controller/follow_joint_trajectory/cancel\n/head_controller/follow_joint_trajectory/feedback\n/head_controller/follow_joint_trajectory/goal\n/head_controller/follow_joint_trajectory/result\n/head_controller/follow_joint_trajectory/status\n/head_controller/point_head/cancel\n/head_controller/point_head/feedback\n/head_controller/point_head/goal\n/head_controller/point_head/result\n/head_controller/point_head/status\n/imu\n/imu1/gyro_offset\n/imu1/imu\n/imu1/imu_raw\n/imu2/gyro_offset\n/imu2/imu\n/imu2/imu_raw\n/joint_states\n/joy\n/laser_self_filter/cancel\n/laser_self_filter/feedback\n/laser_self_filter/goal\n/laser_self_filter/result\n/laser_self_filter/status\n/odom\n/odom_combined\n/query_controller_states/cancel\n/query_controller_states/feedback\n/query_controller_states/goal\n/query_controller_states/result\n/query_controller_states/status\n/robot_state\n/robotsound\n/rosout\n/rosout_agg\n/sick_tim551_2050001/parameter_descriptions\n/sick_tim551_2050001/parameter_updates\n/software_runstop_enabled\n/sound_play/cancel\n/sound_play/feedback\n/sound_play/goal\n/sound_play/result\n/sound_play/status\n/teleop/cmd_vel\n/tf\n/tf_static\n/torso_controller/follow_joint_trajectory/cancel\n/torso_controller/follow_joint_trajectory/feedback\n/torso_controller/follow_joint_trajectory/goal\n/torso_controller/follow_joint_trajectory/result\n/torso_controller/follow_joint_trajectory/status\n", "rostopic echo -n 1 /joint_states", "<fetch>~/FETCH_CORE/fetch_core$ rostopic echo -n 1 /joint_states\nheader: \n  seq: 63846\n  stamp: \n    secs: 1526495986\n    nsecs: 155393416\n  frame_id: ''\nname: ['l_gripper_finger_joint', 'r_gripper_finger_joint']\nposition: [0.004585660994052887, 0.004585660994052887]\nvelocity: [-4.410743713378906e-06 ...", "JointState", "JointState", "joint_state_publisher", "source_list", "JointState", "/joint_states"], "url": "https://answers.ros.org/question/291420/fetch-robot-joint_state-topic-sometimes-contains-all-joints-sometimes-doesnt/"},
{"title": "Loading robot_description dependent on rosparams", "time": "2017-11-29 15:01:12 -0600", "post_content": [" ", " ", " ", " ", "There are a few dimensions of my robot model that are subject to change at runtime, and so I would like to use the param server to set and get those values. It was easy enough to set the values, but now I am unsure as to the best way to pass them to xacro when uploading the robot description:", "Am I right that there isn't a way to query a parameter from the server within a launch file? Otherwise I would do something like:", "<param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(arg model) my_var:=$(rosparam get my_var)\"> ", "I can instead query the parameters within a python script, but how do I invoke xacro from within that script? Should I use ", "? ", " ", "Any suggestions welcome. Thanks!", "Btw: after seeing your own answer, the bit about \"a few dimensions of my robot model that are subject to change at runtime\" was rather confusing. \"At runtime\" typically means \"at any point in time after a program has been started\". That would not seem to be what your Python script allows."], "answer": [" ", " ", "I ended up solving this problem by running xacro in a subprocess from within python. ", "Here's the launch file: ", "And the python:", "As long as you understand that nodes are only started ", " parameters have been set on the server, ", " that there is no guaranteed order in which nodes are started.", "This means that nodes that need ", " might try loading it before your Python script is run.", "Using ", " (the library) directly is possible by calling ", " yourself. It directly parses ", ", which you can override / append to just before you call the function.", "I have encountered that issue and will decide whether or not being able to set the xacro arguments from the param server is worth whatever amount of work it will take to make that happen. I like the idea of your last edit though (using a ", " within the ", " tag in the launch file).", "Thanks for the syntax help on calling ", ". I'm not very familiar with Python.", " ", " ", " ", " ", "I'm pretty sure (but not entirely) it's not possible to use parameters while ", " is setting parameters (", " is just another parameter), but you could perhaps pass the 'dynamic dimensions' to ", " as args right when you ask it to convert the xacro to urdf for you.", "See ", ":", "If with \"a few dimensions of my robot model that are subject to change at runtime\" you really mean: they need to change while my application is running, that is a different problem, and not something that is approachable with a universally accepted or supported solution (", " is essentially immutable, as almost all consumers are not written such that they consider the possibility of the parameter ever changing).", "Edit:", "The problem with the ", " tag in the code you posted is that I want to query the values to pass to xacro.py from the param server.", "Well then I believe my first sentence applies:", "I'm pretty sure (but not entirely) it's not possible to use parameters while ", " is setting parameters (", " is just another parameter), [..]", "There are roughly two phases ", " goes through:", "If I understand you correctly you're trying to do this in phase 1, which afaik is not possible.", "You ", " use all the ", "s that have been set in the launch file (and all the parents) at the point where you are setting the ", " (ie: the ", ").", "I remember some questions here on ROS Answers that have asked similar things, so perhaps a Google search (", ") could turn something up.", "Edit2: after seeing your own answer.", "What you could do perhaps is use ", " with a ", " which uses your Python script. That would ensure it's executed before nodes get started. You would just need to pass the script the parameter yaml that you already have. And instead of using ", ", you'd directly load it (with ", ").", "I don't think I was clear enough in my post. See my update (though I'm not sure how to get the XML to render properly as code...). The problem with the ", " tag in the code you posted is that I want to query the values to pass to xacro.py from the param server.", "Well then I guess we agree that bullet point 1 won't work. As far as bullet 2, I was hoping there was some C++ or Python API for calling xacro rather running the shell command and capturing stdout (bullet point 3). That's what I ended up doing, so I'll write that up as an answer."], "answer_details": [" ", " ", " ", " ", "evaluating all launch files and setting the resulting values for the parameters on the parameter server", "starting all the nodes (potentially based on those parameter values)", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Presumably python would allow me to go the \"hacky\" route and just run the xacro command in a shell and get the result from stdout?"], "question_code": ["xacro.main(['arg1', 'arg2', ...])", "main()"], "answer_code": ["<?xml version=\"1.0\"?>\n<launch>\n  <rosparam command=\"load\" file=\"$(find my_robot_bringup)/launch/dynamic_dimensions.yaml\"/>\n  <arg name=\"namespace\" default=\"\"/>\n  <node name=\"my_robot_description_uploader\" pkg=\"my_robot_description\" type=\"load_robot_description.py\" output=\"screen\" respawn=\"false\">\n    <rosparam subst_value=\"True\">\n      namespace: \"$(arg namespace)\"\n      model_filepath: \"$(find my_robot_description)/urdf/my_robot.urdf.xacro\"\n    </rosparam>\n  </node>\n</launch>\n", "#!/usr/bin/env python\nimport sys\nimport rospy\nimport xacro\nimport subprocess\n\ndef load_robot_description():\n    my_robot_dynamic_dimension_1 = rospy.get_param(\n        '/my_robot_dynamic_dimension_1')\n    namespace = rospy.get_param('~namespace')\n    model_filepath = rospy.get_param('~model_filepath')\n\n    try:\n        command_string = \"rosrun xacro xacro --inorder {} my_robot_dynamic_dimension_1:={}\".format(model_filepath, my_robot_dynamic_dimension_1)\n        robot_description = subprocess.check_output(\n            command_string, shell=True, stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as process_error:\n        rospy.logfatal('Failed to run xacro command with error: \\n%s', process_error.output)\n        sys.exit(1)\n\n    rospy.set_param(namespace + \"/robot_description\", robot_description)\n\nif __name__ == '__main__':\n    try:\n        rospy.init_node('load_robot_description', anonymous=True)\n        load_robot_description()\n    except rospy.ROSInterruptException:\n        pass\n", "robot_description", "xacro", "sys.argv", "command", "param", "main()", "roslaunch", "robot_description", "xacro", "<param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(arg model) myvar:=true\" />\n", "robot_description", "param", "roslaunch", "robot_description", "roslaunch", "arg", "robot_description", "$(arg model)", "site:answers.ros.org", "param", "command", "rospy.get_param(..)", "yaml.load(..)", "param"], "url": "https://answers.ros.org/question/276918/loading-robot_description-dependent-on-rosparams/"},
{"title": "Changing End effector in realtime in moveit", "time": "2017-09-15 04:25:19 -0600", "post_content": [" ", " ", "Hello all,\n  I am using UR5 robot with some custom end effector which obviously works good.Now my requirement is I need to change end effector in run time while move group is running which means URDF which is loaded as robot_description on ros param server is to be changed to other URDF which consist of my second gripper.Also I need to change semantic description on ros param server.Anybody has worked in such scnerio or can help me regarding this?", "Hello, I'm exactly in the same situation, did you manage to find a solution for this ?"], "answer": [" ", " ", " ", " ", "Yup.I found a workaround to solve this issue not a complete solution though.Following are the steps.", "1.Load robot_description to rosparam server with urdf consisting of 1st end effector.", "2.Whenever you want to switch to new end effector replace robot_descritption on rosparam server by reading data from URDF of new end effector with complete robot and also replace robot_description_semantic on rosparam server by new srdf file corrosponding to your new URDF.", "3.Now I am doing all this replacing through my cpp node and then using system() function to kill move_group node,robot_description node and rviz_node.(Remember to set respawn=true for all these nodes in your corresponding moveit package launch files).", "4.All the killed nodes will respawn now and will load my newly uploaded robot_description and robot_description_semantic from rosparam server and in rviz you will be able to visualize your new end effector.", "Since moveit move_group node spawns up moveit_planning_scene service once at the start and it loads robot_description only once at start we need to respawn all the nodes to take up our new robot URDF and its corresponding semantic description.Hope this helps you!", "Thank you, well played for this solution, I will try to implement this for my needs !", "A note of caution: this isn't really a solution, but more of a work-around, and, as you're only restarting ", " of the consumers of those parameters/artefacts, you run the risk of ending up with a 'desynced' ROS node graph, in which some participants are using one version of ", " ..", ".. and related parameters, and others another. In controlled environments this probably won't be a big problem, but I wanted to note it (for future readers) in any case.", "The real problem here is the immutability of ", ", and the fact that none of its current consumers are set up ..", ".. to deal with changes to that parameter.", "Also note: there are multiple ways to parse urdfs (raw xml, ", ", custom parsers, etc), and many nodes even use a combination of those different ways at the same time. This all makes \"changing EEFs in 'real-time'\" a rather complex piece of ..", ".. functionality to implement. Much more complex than a lot of people assume.", "Finally: some alternative work-arounds:", ".. MoveIt-specific)", "2 . run two ", " instances in parallel. Namespaced and each with their own ", " and related parameters.", " wrote:", "but move_group loads robot_description only at start was my main concern", "as it should: ", " is a parameter, and those are only intended to be read in a node's initialisation phase.", "That is why having ", " being a parameter is such a limitation."], "answer_details": [" ", " ", " ", " ", "if the EEF doesn't require any control (based on URDF, that is): use MoveIt's ", " infrastructure for your gripper. Those can be attached and detached at runtime without the earlier mentioned complications (although it will be .."], "answer_code": ["robot_description", "robot_description", "urdf_dom", "move_group", "robot_description", "robot_description", "robot_description"], "url": "https://answers.ros.org/question/270954/changing-end-effector-in-realtime-in-moveit/"},
{"title": "ROS using C++ 11 & How to Use With Catkin?", "time": "2015-08-31 23:20:33 -0600", "post_content": [" ", " ", "Is ROS compiled using c++ 11 with catkin? I suspect not since I am getting an error trying to use {} initializer for a vector, i.e. c++98...must be initialized...", "For at least my builds where in the catkin files would I add the '-std=c++11' option?", "I am using Eclipse but I think this is controlled via the cmake files, not a project property. ", "I poked around and found that adding the \"-std-c++11\" in the file flags.make worked. But I suspect it will be overwritten the next time I add a file to the CMakeLists.txt. Just wanted to mention this with the warning. The file was overwritten with the changes to the C*.txt file.", "Both answers provided worked. I flagged the one by Julius as 'correct' since it was more succinct. I added the line to just below the ## Build ## box, just fyi. Appreciated also the link to ROS/C++11 points."], "answer": [" ", " ", " ", " ", "All the current distributions of ROS are compiled using C++03.", "You can compile your own packages using c++11 by adding the following line to the CMakeLists.txt of your project:", "Note that it is recomended to have checks for c++11 features at configure time and provide equivalent functionality with extra compiler features. This is only required if you are planning to release the ROS packages that you develop. If you are only planning to use your packages yourself then you can just c++11 functionality.\nYou can read more about ROS and C++11 here:\n", "Thanks for the answer. I am at the moment only developing for myself. It will be awhile before I would attempt adding to ROS itself.", "Worked like a charm for me too. Thanks!", "Is there a way to add this flag for an individual file and not the entire project?", "You could try SET_SOURCE_FILES_PROPERTIES( foo.cpp PROPERTIES COMPILE_FLAGS -std=c++11). I have, however, not tried this and I am not sure how this will affect ABI compatibility in your executable.", " ", " ", "You have to add", "to the CMakeLists.txt of your package.", " Also consult this:  ", "  Not sure if ROS is compiled to be ABI compatible between C++ versions. ", "Thanks for the answer."], "answer_code": ["add_compile_options(-std=c++11)\n", "set(CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\")\n"], "url": "https://answers.ros.org/question/216842/ros-using-c-11-how-to-use-with-catkin/"},
{"title": "NodeHandle effect on advertise", "time": "2013-03-18 04:12:52 -0600", "post_content": [" ", " ", " ", " ", "I'm working on a node with several callback queues, which I'm handling through separate NodeHandles, in order to process time critical callbacks separately from longer running callbacks. I think I understand the consequences this has on the subscription side , but I don't quite see the implications on publishers.", "The documentation I've been able to find hits at internal roscpp threads handling the actual publishing of the messages, but it is unclear on how this is done and on the effects of having multiple NodeHandles.", "Aside of the obvious namespace managing performed by the different NodeHandles, how does using one NodeHandle or another to advertise a subscriber affect the publishing of the messages? Does it matter at all which of the NodeHandles I use for advertising a publisher given they all lay in the same namespace?"], "answer": [" ", " ", " Multiple Nodehandles will not give you multiple callback queues. Nodehandles are just \"handles\" to a global singleton that manages the ROS publish/subscribe interface. For what you're trying to do, you should check out this article on the ROS wiki:  ", " , specifically the section called  ", ".", "In this situation, I usually use a \"thin\" callback that just pushes the message onto a work queue and a standard producer/consumer threading model to do the long-running work. If your callback uses the message pointer signature and your work queue is also just a queue of shared pointers, you can push the message to the work queue without copying any data."], "url": "https://answers.ros.org/question/58427/nodehandle-effect-on-advertise/"},
{"title": "Why cannot we communicate with nodes and receive a reply using topics?", "time": "2017-02-25 11:01:23 -0600", "post_content": [" ", " ", " ", " ", "I'm reading ", " ", " and ", ". \n", "In Chapter#2, Pg#36, under the heading 'Services', it is written that:", "\"When you need to communicate with\n  nodes and receive a reply, you cannot\n  do it with topics; you need to do it\n  with services\"", "Why is this not possible? Is this not possible that Node1 publishes a topic that Node2 subscribes and then Node2 publishes a topic that Node1 subscribes?", "(Please reply from a beginners' perspective)", "Why not just use a ", "? This is exactly what services do.", " I can use but that's not the question. I am asking the explanation of that statement."], "answer": [" ", " ", "Like the others have said, you ", " use topics to send information from Node1 to Node2 and then reply along that topic from Node2 back to Node1 But, topics are many-to-many communication channels so there could be many different nodes publishing along that topic and sending replies along that topic as well. Also, topics only carry one data type (message) along them so that means that the message would have to include the communication and response both ways.", "For example, let's say that you want to send a message saying \"hi\" from Node1 to Node2, then a response saying \"hello\" from Node2 back to Node1. If there's only Node1 and Node2, this isn't such a big deal because it's only a string and that same message type can be used to communicate back and forth. This simple message can have a simple ", ":", "Now, if there's also Node3, Node4, Node5, ...,NodeN then you're going to have issues because they're all using the same topic to greet each other. How is each node supposed to know where the greeting came from and what its destination is? You're going to have to increase the complexity of the message:", "This is already problematic because you're potentially tripling the bandwidth of your topic and adding the burden of filtering the messages to make sure that the nodes don't get unwanted greetings. Not to mention that ", ".", "This example is trivial, but imagine that you have several robots running at the same time and you need to send a command telling only one of the robots where to go. This will require you to send the command and also include some sort of identifier.", "Again, you have the problem with needing to keep the channel open and having to send the same message back and forth (request and response). What if you're send images? Images consume a large amount of bandwidth (even compressed) and now you're going to have to send an image back and forth. If you were using a service this would be much easier. The service is always available, but not necessarily consuming the band width plus there's an entire request/response cycle. A service client sends a request (take a picture, go over there, etc.) and the server sends a response (acknowledgement, a picture, etc). The entire message isn't sent, first the request, then the response such as", "The top portion, ", ", is the request and the bottom portion, ", ", is the response. (This is a ", " by the way.)", "So, you can use nodes to send information along with responses, but it can be done better depending on your situation and what you're trying to accomplish. Also, as ", " mentioned, there's also ", "Thank you for the explanation", "No problem, glad to help.", " ", " ", "This is because of how Ros is developed to work... Ros communication is intended to offer a decoupled communication ", "this means node 1 can publish info about topic  X and node2 can subscribe to topic x", "if node2 doenst exist in the graph, then there is no problem, node 1 can still live and publish topicX, this means node1 is just publishing for fun since noone will hear his info...", "if node1 doenst exist in the graph, then there is no problem, node 2 can still live and subcribe to topicX, this means node 2 is ther waiting for a node that gives it information it needs....", "on the other hand, IF YOU NEED TO TALK TO A NODE AND GET FEEDBACK from it, then you need to implement a service,", "something similiar to RPC(remote procedure calls) see image below:", "Thank you for the explanation", " ", " ", " ", " ", "You are right that it is perfectly possible to do this with topics. The best argument for this is that the actionlib completely relies on topics for communication. For every action, a set of topics (e.g. for goal, feedback and result) are created and handled internally so that the user can concentrate on the fun part and does not have to care about the internals. ", "The book does only talk about actions in the move_base example on p. 269ff, ('action' is not even listed in the index!) which is imho a red flag for a book about ROS. So you are right and the part you quoted is simply wrong.", "Took too long to post my answer.", "I think the quote isn't necessarily wrong, it's a bit too vague and relies on previous knowledge on the part of the reader to be understood correctly. There is definitely a difference between pub-sub and services, and I think that is what the quote refers to.", " Thank you!", " ", " ", "tl;dr: yes, that would certainly be possible.", "The long version basically still remains \"yes, that is definitely possible\", but also has to do with semantics (publish-subscribe is a very different sort of communication pattern from request-reply).", "If you're interested, I could expand a little, but for now see ", " for some background.", "Yes please whenever you could do. Please also expand on why ", ".", "Just in case you forgot, I'm still waiting for you to expand on that!"], "answer_code": ["string greeting\n", "string source\nstring destination\nstring greeting\n", "PoseStamped destination\nstring name\nbool acknowledgement\n", "Image image\n---\nbool acknowledgement\n", "Image image", "bool acknowledgement"], "url": "https://answers.ros.org/question/255599/why-cannot-we-communicate-with-nodes-and-receive-a-reply-using-topics/"},
{"title": "Increase thread count of Asyncspinner? (Multi-threading)", "time": "2016-03-08 16:41:42 -0600", "post_content": [" ", " ", "Hey guys,", "I have a ", " which offers a service to ", ".\nOnce called, the service creates a new C++ object from ", ".\nThis Robot object now sets up a few topics mainly for controlling the robot (e.g. ", ") in its contructor.", "Now the problem is, when I create the object in my ", " service callback, my whole ", " blocks. I already used AsyncSpinner for multi-threaded calls already (e.g. the topic listeners in ", "), but here is the problem: ", "The number of robots is changing, which means that I don't know for how many threads I have to set up AsyncSpinner. \nThe API does not offer adding a new thread, so what would you suggest to do? I could stop the spinner, create a new one with more threads, but that would cancel all the stuff currently done by the robots.\nI could also have the Robot object be created in the a main of it's class file and simply create a new node. But that seems even more messy than what I already do.", "I use jade. Please help me."], "answer": [" ", " ", "Threads that don't have anything to do don't consume resources (well, the overhead is probably manageable). Is it an idea to set the nr of threads to some upper bound (say: 20)? Not really a solution (\"640k is enough for everybody\"), but would probably work well enough in practice."], "question_code": ["world_node", "register_new_robot", "Robot.cpp", "goToPose", "world_node", "world_node", "Robot"], "url": "https://answers.ros.org/question/228551/increase-thread-count-of-asyncspinner-multi-threading/"},
{"title": "Reasons for slow response to actions, services, messages?", "time": "2013-09-07 20:52:10 -0600", "post_content": [" ", " ", " ", " ", "Hi guys!", "I'm trying to debug a tool, which responds to service calls and action goals with a lag of 10 to 20 seconds depending on the load of the machine (50% - 90%). I determined this time lag using the debug output, which shows when the action server receives the action goal.", "What could be the reasons for this significant time lag?", "In my use case - pick & place using MoveIt - the node running the action servers and service providers is causing most of the load. So my first guess is, that something is blocking the respective callbacks.", "Is this just a matter of not enough horse power? Which parts of the code could cause this blocking/slowing down?", "Interesting is that actions, services, messages of nodes running in parallel are processed fine, i.e. there is only a small time difference between sending and receiving action goals, responding to services, receiving messages.", "Thanks for your help!", "Additional info using the tools recommended in ", " 's answer:", "As I mentioned above, the load is usually around 50 ~ 70% (desktop) / 50% ~ 90% (robot) (using top). iftop is an interesting tool! Shows me that the local traffic (lo interface) goes up to 2Gb/s, when starting to process point clouds.", "Which process is generating the load? Is it the roscore or one / multiple of the started nodes?", "The rosmaster load is neglectable. The main load comes from MoveIt's move_group node (more details added in the question). And it's only that node's topics, services and actions, which are processed extremely slow. The other nodes run fine, what is probably because the CPU is not fully used."], "answer": [" ", " ", " ", " ", "More than an answer, this may serve as a first diagnostics step.", "How loaded is your system when this happens?. I recommend a first sweep with the following tools:", " to check ", " and ", ". ", " (may require sudo) to query the ", ", eg. ", " for loopback only.", "It might also be good to check the ", ", eg. ", ". ESTABLISHED, CONNECTED correspond to sockets currently in use, while TIME_WAIT, CLOSE_WAIT are pending to close. Pay special attention to the latter, as large counts here can indicate lots of short-lived sockets, which usually occur in ROS environments when you frequently query the master (non-persistent service calls or parameter reads). Many socket opening/closing operations will increase your system CPU load (shown in top under Cpu(s) .... sy).", " From the updated question details.", "Could you post for completeness the CPU load and network traffic values with pointcloud perception disabled?.", "It seems that the pointcloud messages are taking up a lot of bandwidth, and (de)serializing + processing them (coordinate system change, self-filtering, object detection, etc.) is in turn consuming significant CPU resourecs (maxing out a core, leaving no room to the scheduler to process all incoming messages).", "What kind of pointcloud input are you feeding ", "?. If it's the raw input from a Kinect-like RGBD sensor, that might indeed prove prohibitive. Preprocessing the pointcloud might help. These are some indicative numbers I took some time ago:", "Finally, if you need point clouds at discrete time instances (as opposed to a continuous stream), gate pointcloud traffic through an on-demand snapshotter.", "Thanks for listing these helpful tools. I added the result of them to my question. To me CPU load looks OK as well as the sockets. The network traffic looks high, but then I have no idea what is \"high\" and \"low\" regarding the traffic on the lo interface. Do you have any experience with it?", "Extra info added. Your suggestions about preprocessing the point cloud is a good idea. I wonder however, if there are other ways to improve this situation. There is still 10% of the computation power unused. Also, there are multiple cores available. Can't that be used?", " ", " ", " ", " ", " Just found this interesting Q&A:  ", "  Could multiple callback queues be useful in the current case?  "], "answer_details": ["Original cloud contains 200k-300k valid points.", "Crop to a bounding volume of interest (~1 order of magnitude less points)", "Downsample with octomap (additional ~2 orders of magnitude reduction)", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "top\n", "with point cloud processing: ~40% idle, load average: ~2.5 (quad-core CPU: i5-2500 CPU @ 3.30GHz), move_group process at ~130%", "w/o pcp: ~90% idle, load average: ~0.5", "iftop\n", "with pc: ~2Gb/s", "w/o pcp: ~5Mb/s", "sockets\n", "When idling few (~5) sockets show up every few seconds (5 - 10). For each motion plan action a few more sockets pop up."], "answer_code": ["top", "iftop", "sudo iftop -i lo", "netstat | grep ESTABLISHED | wc -l", "move_group"], "url": "https://answers.ros.org/question/77161/reasons-for-slow-response-to-actions-services-messages/"},
{"title": "How do you implement GPS using robot_localization", "time": "2015-07-01 12:36:13 -0600", "post_content": [" ", " ", " ", " ", "I'm a bit new to ROS and I'm unsure how to implement GPS into the project I'm working on. So far I know that I can use robot_localization to get GPS but how would that be implemented ? ", "Edit 1:", "Thanks for the reply,", "Yes I have, I've been working on it for a couple of days and it's starting to make sense now. I'm still struggling on some stuff such as how to use the nmea drivers. In the robot_localization tutorials you make a launch file but the nmea wiki it is possible to just use rosrun to get things up and running. Do you first use rosrun to make the connection with the hardware and then launch the launch file to start getting all of the data? Another thing is that I will not be using any odometry data for my localization project. What files do I need to edit so that the code won't be calling for any odometry data?", "thanks"], "answer": [" ", " ", " ", " ", "Have you seen ", " page? If you have and something isn't clear, please let me know and I'll update it.", "EDIT: You might benefit by reading up a little more on ", ". One of the points of ", " is to save you from having to use ", " and entering a ton of parameters by hand, e.g., ", "Instead, you just create a launch file that kicks off the node and passes in all those parameters. Then you can just launch your launch file instead, e.g.,", "The state estimation nodes in ", " do not require an odometry source. ", " converts GPS data to an odometry ", " so that ", " or ", " can consume it. However, if you want to fuse GPS data, you ", " need an IMU with an earth-referenced absolute heading."], "answer_code": ["roslaunch", "rosrun", "rosrun amazing_package amazing_node param_a:=100 param_b:=true param_c:=-999 param_d:=false\n", "roslaunch amazing_package amazing_launch_file.launch\n", "robot_localization", "navsat_transform_node", "ekf_localization_node", "ukf_localization_node"], "url": "https://answers.ros.org/question/212702/how-do-you-implement-gps-using-robot_localization/"},
{"title": "rosserial arduino: tf::broadcaster contradicts using publisher", "time": "2015-02-02 03:39:20 -0600", "post_content": [" ", " ", "Hello together,", "I am running rosserial on the arduino UNO with the \"rosserial_server\" packages node \"serial_node\" under ubuntu 14.04. The setup works in general, I just can not solve the following problem:", "When using a tf::broadcaster and another publisherof arbitrary type at the same time, the arduino must run into some error. At least the serial_node either never gets anything back from the arduino or gives the error output: ", "[WARN] [WallTime: 1422869167.806875] Serial Port read returned short (expected 1 bytes, received 0 instead).\n  [WARN] [WallTime: 1422869167.807257] Serial Port read failure: ", "I prepared a minimal example:", "It is enough to comment in the message definition \"std_msgs::Int32 iMsg;\" to produce the error. I can not see how this is related."], "answer": [" ", " ", "Turns out this one and several more \"random\" errors (combinations of unrelated variables and lines of codes would not work together but seperately) was just related to the ros.h taking up a lot of memory.", "Hard to guess though, make sure your program is using the right arduino, so for the Arduino UNO use:", "Have a look at:   ", " for more infos about the limited memory.", "To debug these \"weird\" errors there is really just the possibility to comment in and out memory consuming features of your code ", "Also check ou", " for tuning suggestions."], "question_code": ["#include <ros.h>\n#include <std_msgs/Int32.h>\n#include <ros/time.h>\n#include <tf/tf.h>\n#include <math.h>\n#include <tf/transform_broadcaster.h>\n\n// Configuration hardware\nconst long BAUD_RATE = 115200;\nint PUBLISH_HZ =10;\n\n//time of last tf send\nlong int lastSendMillis=0;\n\n\n//ros variables\nconst char base_link[] = \"/base_link\";\nconst char odom[] = \"/odom\";\nros::NodeHandle nh;\ngeometry_msgs::TransformStamped t;\ntf::TransformBroadcaster broadcaster;\n\n//std_msgs::Int32 iMsg;\n//ros::Publisher intChatter(\"timeDifs\", &iMsg);\n\nvoid setup()\n{\n  t.header.frame_id = base_link;\n  t.child_frame_id = odom;\n\n  nh.getHardware()->setBaud(BAUD_RATE);\n  nh.initNode();\n\n  broadcaster.init(nh);\n\n // nh.advertise(intChatter);\n}\n\nvoid loop() {\n\n  int timeDiff=(millis() - lastSendMillis);\n  if (timeDiff >= 1000/PUBLISH_HZ) {\n    // send it now!\n    lastSendMillis=millis();\n\n   t.transform.translation.x = 1;\n   t.transform.translation.y = 1;\n   t.transform.rotation = tf::createQuaternionFromYaw(M_PI / 2.);\n   t.header.stamp = nh.now();\n\n   broadcaster.sendTransform(t);\n\n  // iMsg.data=timeDiff;\n  // intChatter.publish(&iMsg);\n\n   nh.spinOnce();\n  }\n}\n"], "answer_code": ["#define __AVR_ATmega8__\n"], "url": "https://answers.ros.org/question/202215/rosserial-arduino-tfbroadcaster-contradicts-using-publisher/"},
{"title": "Interfacing between ros control and dynamixel controllers", "time": "2015-01-06 12:37:21 -0600", "post_content": [" ", " ", " I am trying to use the steered wheel base controller plugin with ros control and dynamixel controllers to control a robot having four steered wheels(servos used are dynamixel MX 28). I am able to load the plugin given in the following\nlink:  ", " . \nNow that I have to read and write the states and commands of dynamixel servos, I believe that I have to follow the tutorial in this link:  ", " At this stage I am wondering if I can set the values in the topics published by dynamixel controllers to the variables given in the above tutorial directly, or should I be using the dynamixel API's to sets these values. I found the former method to be doubtful as the following link:  ", "  says that \"One major feature that is missing in ros_control is a proper realtime-friendly dataflow interface\".  "], "answer": [" ", " ", " ", " ", " As Adolfo already explained, you can make a ros_control wrapper that talks to the dynamixel drivers via topics (at which point real-time guarantees do not hold anymore). We recently did this with a 3D printed dynamixel arm and it works surprisingly well. Code is available online:  ", "The packages are not polished and prepared for public consumption, but they should give you some ideas of how you can approach the problem regardless. Note we for instance also have a fake 6th joint on the arm to be able to use standard KDL IK.", " ", " ", "If you use topics to send commands to actuators, you will not have real-time behavior, but then again your application might not have real-time constraints. ros_control can be deployed in real-time systems, but the implementation of your robot hardware abstraction and the controllers you use must be real-time friendly. You must also use an operating system that can deliver real-time guarantees."], "answer_details": ["\ncontains the launch files for the\ndynamixel driver (and some obsolete\ncode for joint state publishing that\nyou can ignore I think)", "\ncontains the ros_control hardware\ninterface. This fakes the read() and\nwrite() method functionality by\ncommunicating with the dynamixel\ndriver topics", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/200468/interfacing-between-ros-control-and-dynamixel-controllers/"},
{"title": "binarydeb with CGAL", "time": "2015-01-14 02:12:20 -0600", "post_content": [" ", " ", "I have a problem building a package using the CGAL libray on the buildfram.", "The prerelease test is sucessful on both my local computer and Jenkins but I get the following error when trying to build the binary deb:", "CMake Error at> /usr/lib/CGAL/CGAL_SetupFlags.cmake:65", "(message):   RelWithDebInfo is not a\n  valid build type: only Release or Debug is allowed", "The inclusion of CGAL in my CMakeLists.txt looks like this:", "find_package(CGAL REQUIRED COMPONENTS Core)", "set(CGAL_DONT_OVERRIDE_CMAKE_FLAGS TRUE CACHE BOOL \"Don't override flags\")", "include(${CGAL_USE_FILE})", " The package name is crossing_detector. Its source is in  ", " . ", "How can I deal with this problem?"], "answer": [" ", " ", "You should be able to reproduce this locally by using:", "Once you can do that, you can work out a solution locally before you release a new version to the farm.", "My only thought about actually fixing this is to try setting the CGAL_DONT_OVERRIDE_BUILD_FLAGS variable _before_ you call find_package(CGAL).", "Thanks for the answer, unfortunately setting CGAL_DONT_OVERRIDE_BUILD_FLAGS before find_package didn't help.\nWhat I did is a workaround:\n", " before and ", " after.\nI don't like to play with the build type in CMakeLists.txt.", "I agree that setting the build type in your CMakeLists is not a good solution. You may want to consult the CGAL documentation or contact the CGAL developers, since it's their build files which are rejecting a standard CMake build type."], "answer_code": ["catkin_make -DCMAKE_BUILD_TYPE=RelWithDebInfo\n", "set(CMAKE_BUILD_TYPE \"Release\")", "set(CMAKE_BUILD_TYPE \"RelWithDebInfo\")"], "url": "https://answers.ros.org/question/200929/binarydeb-with-cgal/"},
{"title": "ROS/gazebo consulting service", "time": "2014-09-30 16:40:44 -0600", "post_content": [" ", " ", "Hi all,", "I was wondering if anyone is aware of a ROS/gazebo consulting service currently in place. I had a look at the ", " and it looks like my initial best candidate for some gazebo support service (hiDof) is at rest as the webpage seems to indicate. Meanwhile I recently received a poll from ClearPath about a support service they are aiming at putting in place - not sure where they are at...", "Anyone has an idea of who could best provide gazebo support?", "Thanks,", "Antoine."], "answer": [" ", " ", " ", " ", "I don't think we want to see business activities on this forum that much, so I keep my answer brief; We have an experience for a consultancy work for ", ", ", ". Please contact ", ".", " ", " ", "Hi Antoine,", " I'm the Client Success Coordinator at Clearpath Robotics. We do offer consulting services already. The poll was merely to solidify our offering, and make sure we are offering the most value to the community. If you'd still like to discuss your project, please consider contacting us by sending an email to info @  ", " , or submit an inquiry  ", ".", "Hope that helps!", "Cheers,\nPaul"], "answer_code": ["Gazebo", "drcsim"], "url": "https://answers.ros.org/question/193868/rosgazebo-consulting-service/"},
{"title": "marking parameter costmap_2d", "time": "2014-06-25 11:36:03 -0600", "post_content": [" ", " ", "Hi all,", "I have a very brief question regarding the ", " parameter in the ", " tag. To be more precise, let's suppose that I set ", " for my laser_scan in the obstacle_layer, then my local costmap will be no longer updated with the laser readings, but what about the motion planner in ", "? Will move_base take into account those readings to plan locally or is it going to ignore them (so basically setting ", " will be equivalent to remove the  obstacle_layer)?", "Thanks and regards,\nFederico"], "answer": [" ", " ", "Costmaps, planners, etc are all fairly decoupled, the planners don't need to know how to update a costmap, they just consume the data from a fully updated costmap. Thus, if your sensor is not marking the costmap, the readings won't be used used by the planners of move_base."], "question_code": ["marking", "marking: false", "move_base", "marking: false"], "url": "https://answers.ros.org/question/174993/marking-parameter-costmap_2d/"},
{"title": "ros subscribe to the latest available message", "time": "2014-03-18 23:58:21 -0600", "post_content": [" ", " ", " ", " ", "I am using rospy but I don't mind getting a solution in roscpp.", "I have a publisher that publish message at a rate of 60Hz, and let say my subscriber is capable of handling messages at 6Hz. I am planning to sample the messages from the subscriber side and take the last available message from the publisher for subscription. Setting the queue_size to one, I can possibly sample the message. However, the new message that I am processing is turn out to be the one available just after the subscriber node finish processing the previous topic.", "To clarify more, let say the publisher publish message at time t \nt = [0, 1, 2, 3, ..., 9, 10, 11, ...] sec\nAssuming the first subscription to occur at 0.5 Sec\nWhat I'm getting is the next subscription on the message topic occurring at time t = 11sec, what I want is to use the the message at time t=10Sec.", "I hope I explain myself well.", "There seems to be some confusion of terms. The publisher advertises a topic (once) and the subscriber subscribers to the topic (once). After that, what occurs repeatedly is the publisher publishing messages (not topcis) and the subscriber recieving/processing messages.", "Having said that, I'm not sure I understand your question exactly. Do you want to process always the latest (newest) received message in your 6Hz subscriber? I.e on average every 10th message would be used? Providing some code of your subscriber and publisher could help.", "Thanks ", ", I have tries to put things in properly.", "I would like to add that both, publisher and subscriber have a queue size. I assume both have to be at 1 for getting the behaviour you are looking for...", " is it really possible to set queue_size to one in rospy.Publisher or do I have to use the other arguments?", " why do you say that? What happens when only subscriber has queue size 1?", "Advertise of publisher has also arg queue_size. C++ signature:  template <class m=\"\">\n    Publisher advertise(const std::string& topic, uint32_t queue_size, bool latch = false); not familiar with python. Doc of queue size says:Maximum number of outgoing messages to be queued for delivery to subscribers", "I assume - have not tested that - that the publisher also uses this as FIFO queue if the subscriber is not able to subscribe at full rate. I. e. if subscriber queue size is 1 and publisher queue 10 you would still always get the \"10 messages ago\" message if publisher queue is full..."], "answer": [" ", " ", "+1 to ", " for his clarification about topic/message. Please review these terms.", "I would spontaneously suggest a threaded solution. You do all your time consuming work in a separate thread and in your subscriber callback just pick the messages you want, discard all the others and provide the worker thread the correct messages.", " thanks", " ", " ", "If I understand correctly, setting the queue_size of the subscriber to 1 would solve your problem.", "No. It won't.\n"], "url": "https://answers.ros.org/question/141676/ros-subscribe-to-the-latest-available-message/"},
{"title": "What driver should I use for my USB camera?", "time": "2011-02-16 02:24:24 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I see people using lots of different ROS packages to work with their USB webcams. What are the options?"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Most recent consumer webcams are compatible with the USB video class (UVC) device specification. ROS has several drivers that support these cameras as well as some non-UVC-compliant ones.\n<style type=\"text/css\">\ntable#drivers tbody td { padding: 0.5em 0.25em 1em 0; border-bottom: 1px solid #ccc; }\ntable#drivers code { font-size: 90%; }</style>", "\nAlso see ", ".", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "The best link for comparing all the ROS usb camera options ", "Why I am getting karma - when I am answering someone questions???!!!!!", "What exactly are you asking here? Why are you not getting karma for your answers? Why are you losing karma when being downvoted? Why are you actually receiving karma for answering a question and getting upvotes?", "Don't you think the way you answering my question is a bit offensive!!! I just friendly asked a question while I gave a short answer compared to the previous answer with the correct link to help somebody else I got minus sign \" - \" (- means MINUS in my comments above if you didn't understand).", "Please calm down. Your original comment was unclear. There is no way to know why people voted your answer down, but don't take it personally."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "What cameras does the driver support?", "Which features does it have?", "What's the license?", "Where can I get it?"], "answer_code": ["camera_node", "sender", "sender", "uvc_cam", "camera_node", "stereo_node", "uvc_cam"], "url": "https://answers.ros.org/question/9089/what-driver-should-i-use-for-my-usb-camera/"},
{"title": "sensor_msgs/Image encoding conversion", "time": "2013-06-19 03:31:06 -0600", "post_content": [" ", " ", "Hi,\nI'm trying to publish the ros image message encoded as \"rgba8\" from the original image, which has encoding \"bgr8\". ", "What would be a proper way to do that? ", "I'm thinking of doing it in the following steps:", "This seems a bit tedious... any suggestions??", "Thanks!!"], "answer": [" ", " ", "Have a look ", ", the tutorial are plenty here ", ", select the appropriate ROS distro while at it.", " ", " ", "Thanks for pointing that out!", "I tried to get the most out of that tutorial to do the conversion, and I just figured out the following:", "convert the ros image (bgr8) to cvImage in cv_bridge by specifying the wanted encoding (rgba8).", "convert this cvImage to ros msg, and publish.", "When I run this script, it seems to consume quite a bit of cpu. I assume that during the first conversion, it needs lots of operations. ", "I just wonder if there would be a more efficient way of doing it?", "i also have to publish a cvMat image in ROS...can you please tell me how can i convert the cvMat to ROS msg image?", "Were you able to get any other conversions to work? I'm trying the same thing with RGB8 > Mono16, with no luck: ", "Edit: My stream was only coming in as RGB8. This was the default in opencv, changing that fixed it."], "question_details": [" ", " ", " ", " ", " ", " ", "convert ros image to cvImage", "split the channels", "merge the image according to \"rgba\" encoding (assuming a = 255)", "convert new cvImage to ros image"], "answer_code": ["cv_bridge", "encoding specified as mono16, but image has incompatible type 8UC3"], "url": "https://answers.ros.org/question/65517/sensor_msgsimage-encoding-conversion/"},
{"title": "openni_launch + fuerte + ubuntu 12.04: No image received", "time": "2012-08-27 01:00:41 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I just installed openni_camera and openni_launch stacks. When I launch the driver and rviz using the following commands:", "in rviz, I see a warning telling me that it didn't receive any images. I try to echo the messages sent by ", ", but no messages are published on that topic.", "I need to mention that I get some number of messages from ", " topic. The interesting thing here is that only a number of messages are published by that topic. It stops after a random number of messages.", "There are a few similar problems like mine. Following the proposed answers, I suspect that my drivers might be faulty. But reinstalling them didn't solve the problem.", "Here is a  ", "\nIn this run, I only got 74 point cloud messages. It just stopped at that point. I also didn't get any image messages.", "I deeply appreciate any help.", ": ", "I must also add that when I run the following command:", " and ", ", I can see the camera working for only a few(5-10) seconds, and again it hangs.", " ", "I get lots of errors and warnings but they are said to be harmless, for example", "Exception AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored", "[ERROR] [1346137170.283120890]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressed/set_parameters]", "[ WARN] [1346137171.480522722]: Using default parameters for RGB camera calibration.", "In several questions on Ros Answers, I ran into the problems with the same errors and warning. They all say that ignoring these errors wouldn't do any harm. ", "I paste the full output ", " I cannot comment on the posts, that's why I will post this edit.\nAs pgorczak says, I don't believe that my problem is performance related, I use a powerful computer. In addition to that, even if I don't start rviz or image_view and just echo, let's say, /camera/rgb/image_raw, I don't see any messages on that topic. I believe openni just freezes. ", "I will keep trying to see if I can get it to work. And, thanks a lot pgorczak, I am looking forward to your update.", "Did you check for any suspicious log messages? Try running ", " and ", " before the driver's launch file and see if anything shows up when it hangs.", "I updated the question. Please check edit2.", " were you able to ever solve your problem? I think I am having the same issue. Are you using Ubuntu 12.04 64-bit? Are you using the openni-dev unstable and ps-engine avin2 packages found ", "?", "Yeah, somehow... Please take a look at this reply ", ". In my case, both problems are related"], "answer": [" ", " ", " ", " ", "The first exception is not harmful as you can see ", ".", "When I am using ", ", I also get messages like:", "Still, everything works just fine (image_view and rviz) so those shouldn't be the issue.", "I think running ", " image_view like in your first edit and still getting the freeze pretty much rules out a performance issue--at least concerning rviz. I think (I'm not sure though) the point cloud only gets computed when you subscribe to the points topic which would mean there is pretty much nothing going on computationally when you just display an image with image_view.", "You could also try a different Kinect device or make sure your Kinect works e.g. on Windows with the OpenNI examples to make sure there is no problem with the device itself.", "And my last question would be: What is your version of ROS and your operating system (version)? I'm using ubuntu 12.04 with fuerte and it worked with the OpenNI-packages I got with ", ".", "You could try the driver from openni_camera_deprecated just to see if it gives you different output or some hints on what's going on.", ":", " ", " ", "This post solved my problem about the double advertising", " ", "hope it helps", " ", " ", "Hi,", "I read through the description of your problem and it reminded me of a similar situation I was stuck up in earlier. The solution i found was something to do with the frames settings in your rviz.", "You need to check the fixed frame and the target frame that you have chosen in rviz. Your screenshot did not show the fixed frame. However, you can look at the dropdown box for other options of fixed frame and chose the one in which you would be able to see the rgb image output. You may do a trial and error to find the right one. Usually the fixed frame has to be something like", "/camera/rgb_optical_frame or /camera/base_link", "If you exactly know the various transforms published, you can directly type in the transform relevant to rgb frames in the fixed frame option.", "As far as hanging of your system is concerned, remember that rviz and image topics always consume an ample amount of memory in order to run. Thus, it may get frozen sometimes when it runs out of memory. Always try to use a computer with a high configuration for rviz operations. Besides, you may want to run rviz with the --sync option, i.e.", "rosrun rviz rviz --sync", "This could help you to some extent as it would add necessary delays to synchronize the data flow using buffers.", "Good luck.", "Regards."], "question_code": ["roscore", "rxconsole"], "answer_code": ["roslaunch openni_launch openni.launch", "[ERROR] [1346317018.377804268]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressedDepth/set_parameters]\n[ERROR] [1346317018.384694527]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressed/set_parameters]\n[ERROR] [1346317018.413763705]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/theora/set_parameters]\n(...)\n[ WARN] [1346317020.499849507]: Camera calibration file (...)/rgb_A00364A06244047A.yaml not found.\n[ WARN] [1346317020.500131130]: Using default parameters for RGB camera calibration.\n[ WARN] [1346317020.500289570]: Camera calibration file (...)/depth_A00364A06244047A.yaml not found.\n[ WARN] [1346317020.500426152]: Using default parameters for IR camera calibration.\n", "sudo apt-get install ros-fuerte-openni-camera", "openni_camera/OpenNINodelet", "<launch> \n    <node pkg=\"nodelet\" type=\"nodelet\" name=\"openni_manager\" output=\"screen\" respawn=\"true\" args=\"manager\"/>\n    <node pkg=\"nodelet\" type=\"nodelet\" name=\"openni_launch\" args=\"load openni_camera/OpenNINodelet openni_manager\" respawn=\"true\">\n        <!--<param name=\"rgb_frame_id\" value=\"camera_rgb_optical_frame\" />-->\n        <!--<param name=\"depth_frame_id\" value=\"camera_depth_optical_frame\" />-->\n        <param name=\"depth_registration\" value=\"true\" />\n        <param name=\"image_mode\" value=\"VGA_30Hz\" />\n        <param name=\"depth_mode\" value=\"VGA_30Hz\" />\n        <param name=\"debayering\" value=\"EdgeAwareWeighted\" />\n        <param name=\"depth_time_offset\" value=\"-0.055\" /> <!-- Taken from TurtleBot's kinect.launch -->\n        <param name=\"image_time_offset\" value=\"0\" />\n    </node>\n</launch>\n"], "url": "https://answers.ros.org/question/42347/openni_launch-fuerte-ubuntu-1204-no-image-received/"},
{"title": "Reading the sequence number from a transform", "time": "2012-08-27 12:25:55 -0600", "post_content": [" ", " ", " ", " ", "Hi folks!", "I'd like to be able to use the sequence number from a transform that I am getting from a motion capture vrpn. I use this piece of code to read the transform:", "and am attempting to read the sequence via:", "But, naturally, the transform does not include that header. How can I read get both the transform and it's associated sequence number?", "Thanks,\nParker"], "answer": [" ", " ", "As far as I know, it is not possible to get the sequence number due to the ", ". Since there can be multiple producers of tf info, data can arrive out of order and may not be synchronized. It is the consumer's responsibility to check for the required tf using the timestamp, hence the lookupTransform with associated time (ros::Time(0) for example to get the latest tf)", "If it is absolutely necessary to have a sequence number, it is probably best to go back to the producer (in your case, whoever broadcasts /optitrak->/quad) and publish that transform with stamp. But I'm wondering about the necessity of this, since by your tf lookup, you know it is the latest tf, and can just be seq+1 by default.", "AFAIK tf even interpolates between frame, so there might not be any single message behind a tf request."], "question_code": ["tf_listener.lookupTransform(\"/optitrak\", \"/quad\",  ros::Time(0), quad);\n", " seq= quad.header.seq;\n"], "url": "https://answers.ros.org/question/42420/reading-the-sequence-number-from-a-transform/"},
{"title": "Control architecture design - help!", "time": "2013-01-16 08:27:51 -0600", "post_content": [" ", " ", "Dear all,", "I am having a control architecture design decision to take and I was thinking some of you guys have probably already faced the same kind of problems. So here follows the description.", "We are currently designing a mobile robot + mounted arm with multiple controlled degrees of freedom and sensors. In the initial configuration there are 10 positioning motors to control and we are considering modifications were this number would scale up. Sensors include an IMU (gyros + accelerometers + magneto) + kinect (for SLAM) + motor encoders.", "I am considering the following architecture:", "I know my framework needs to be scalable to account for more motors, more sensors, more PCs (eg. for external mocap). I was wondering whether there were ", " (I know no brainer solutions do not exist but maybe there are typical ones that are often used). I have looked at papers related to robots architecture (eg. HRP2\u2026), most often they describe the high level control architecture but I have yet to find information on how to have the low level communicate with the high level and in a scalable way. Did I miss something?", "My main problem is to decide how to have the different RTx communicate with PC1. I am not sure which technology of BUS/netword to use to connect the fast RT machines ensuring the motor control with PC1. I have considered TCP/IP, CAN and UART. There may be other solutions, I do not know about:", "Is TCP/IP really a no go because of its non-deterministic characteristics? It is so easy to use\u2026", "At the moment no solution really seems obvious to me. And as I can find no serious robot exemple using a specific reliable and scalable solution, I do not feel confident to make a choice. Anyone has a clear view on this ...", "Another bus to consider is (EtherCat)[", "]."], "answer": [" ", " ", "Welcome arennuit!", "Nice first question! However, since it is not really ROS-related, you might get more and maybe better answers at general Robotics forums/Q&As, such as ", ". Anyway, here are my thoughts:", "You already did the important step of splitting your system up into RT and non-RT areas. Now, the question remains how to connect them. To answer this, you need to figure out whether or not you need a hard real-time connection between both.", "For example, if you plan to do joint trajectory interpolations and send each position/velocity command at hight speed to the motors, you might want to have a deterministic/low jitter connection to your motor control boards.", "I'm not sure, what requirements you have: You stated, you want to determine (deduct) desired motor positions and speeds on the PC, but you also plan to run those control loop(s) at only 30hz. So, in case you don't do something like a fine-grained joint trajectory computation on the PC, I wouldn't expect you to need a real-time connection between your PCs and RTs. Hence, TCP/IP could be well suited for your system (as you stated, cheap and simple).", "If however at some point you have the need for it, you can look into CAN[1] and Ethercat controllers for your PC. Be aware, that those controllers are often quite expensive. CAN has the advantage, that especially the devices/slaves (not the PC controller) are much cheaper than Ethercat, which on the other hand is more powerful (e.g. much higher bandwidth). Furthermore, you will need a RT system for your PC, such as Xenomai, in order to be able to run the fast control loops, which need to communicate with your RTs, in the real-time space. Setting up such a system and writing correct programs for it is also more difficult and time-consuming, than writing programs for normal user space.", "[1] I don't think CAN is so exotic in the Robotic world. Companies such as ", " and ", " are working with it. Also, we recently starting experimenting with it. ", "I hope, I could help you a bit!\nKeep us updated how your robot will look like in the end! :-)", "Hello bit-pirate, your answer was very clear and very helpful. It definitely opens doors. Thanks a lot!", "I'm happy, I could help. If you find your question answered, please mark it accordingly. Thx!"], "question_code": ["1. RTx: Positioning motors control\n    - Achieved by a RT system (either Raspeberry Pis running an RTOS such as Xenomai or bare metal microcontrollers)\n        -> let us call these machines RTx, with x=1,2,3\u2026 depending on the number of microcontrollers\n    - Each of them controlling 2 or 3 motors with related encoders (not sure about the technology of the motors yet)\n    - Fast loop: 200Hz\n\n2. PC1: SLAM computation + IMU computation + fusion (SLAM + IMU + mocap) + high level logic (decide the robot\u2019s task and deduct the motors desired position and speed)\n    - Compute on a powerful machine (vanilla linux, no RT)\n        -> Let us call this machine PC1\n    - Onboard robot\n    - Using ROS \n    - Slow loop: 30Hz\n", "1. TCP/IP: not deterministic but easy to put in place. Is non determinism a real issue (as it will only be used at at slow speed 30Hz anyways)?\n2. CAN: slow, very reliable, targeted to cars ( have seen there are some exemples using CAN with robots but it looked exotic)\n3. UART: if I had only had one RT machine for motor control I would have considered UART but I guess this port does not scale well with many RTx\n"], "url": "https://answers.ros.org/question/52501/control-architecture-design-help/"},
{"title": "move_base/base_local_planner maximum velocity", "time": "2011-11-23 02:59:09 -0600", "post_content": [" ", " ", "I'm currently working on a large outdoor robot that requires high speeds to operate effectively. As such, I've tried to increase the command velocity output by move_base and base_local_planner, but upon increasing the max_vel_x parameter, I see a cap in the cmd_vel message output by move_base. My current configuration can be found ", ". ", "I have the max_vel_x parameter set to 10.0 m/s, but the maximum cmd_vel output by move_base is ~2.75m/s. I've observed this both in a simulated environment and on the actual robot. ", "I assumed that the forward simulation time might have something to do with it. I figured, \"if the robot can't simulate far enough into the future, it may not allow itself to move faster than it can predict.\" I tested this theory by setting the sim_time parameter from 1.7 to 3.0. What I noticed was the exact opposite of what I predicted. With a higher sim_time, the maximum output cmd_vel was ~1.8m/s (lower than the previous). A sim_time of 5.0 resulted in a maximum cmd_vel of ~1m/s. ", "My question is: what are the factors that play into the calculation of the cmd_vel message output by base_local_planner, and is there any way to make it move faster? We're looking to achieve ~10m/s. "], "answer": [" ", " ", "OK, I finally took some time to check this out. After playing around a bit I was able to get a robot, in an empty environment in simulation, to achieve very, very, high velocities. I'm not sure that the navigation stack is an awesome way to control an outdoor robot at high speeds, but I can at least give an explanation of why you were likely having trouble:", "The local planner doesn't consume the entire global path for planning, instead it is given a region of the path that fits inside of a local window that based on the size of the local costmap. By default, the local costmap is centered at the robot and sized at 6m x 6m. This means that the goal point for the local planner is never going to be more about 3 meters away from the robot. As such, the cost function will actually limit the speeds the robot chooses to be slower because its planning to a point that is only a couple of meters away. Furthermore, as you increase the maximum velocity for the local planner, you're also increasing the size of the steps between simulated velocities. For example, 10 steps between 0 and 1 m/s would give 0.1 m/s increments, but 10 steps between 0 and 10 m/s would give 1.0m/s increments. As such, you could end up with slower velocities chosen for the 10 m/s max  than the 1 m/s max because its at a coarser resolution. One solution here would bet to increase vx_samples, but this has performance implications as the planner will simulate a lot more trajectories.", "I ended up changing my local map size to something absurdly large, it should  be noted that this will also have some performance implications, and ran the local planner again with a high velocity limit. Lo and behold, the robot achieved its max velocity. To hit max velocity even faster, I upped the acceleration limits a bit, reduced the pdist_scale parameter to zero to allow the robot to just pursue its goal point, and switched to Trajectory Rollout from DWA since it consideres accelerations over the entire sim_time for the velocities it commands as opposed to DWA which just considers acceleration over the first simulation step.", "I'm not sure this will make for the most reliable or robust planner for your application, but at least it explains what was happenning and I hope it helps.", " ", " ", "The base_local_planner computes the maximum velocity for a given time step as:", "\nmax_vel_x_iteration = min(max_user_vel_x, current_vx_from_odom + acc_limit_x * sim_period) ", "and", "\nmax_vel_x_iteration = min(max_user_vel_x, current_vx_from_odom + acc_limit_x * sim_time) for ", "As such, there shouldn't be a limit on the maximum velocity the base can achieve. Also, if you were running the planner in an environment that has obstacles, the longer sim time could result in more trajectories being in collision, especially those simulated with higher velocities. The base_local_planner is, admittedly, a bit simplistic in how it decides whether a trajectory is valid. It just checks if it collides with an obstacle at any point... even if that point is far enough away for the robot to stop before the collision. A couple of questions:", "1) Have you tried running your test in an environment without obstacles? That should, for testing purposes, eliminate the worry that some trajectories are being detected as in-collision.", "2) Are you sure that the planner is receiving odometry information from your base and that this information is correct? This could be another reason that the base doesn't achieve max velocity as it takes the current velocity of the robot and acceleration limits into account."], "url": "https://answers.ros.org/question/12066/move_basebase_local_planner-maximum-velocity/"},
{"title": "rosbridge CPU usage", "time": "2012-01-28 07:57:51 -0600", "post_content": [" ", " ", "I've noticed on two of the development environments that I've been using that rosbridge has a tendency to consume a large amount of the CPU's resources. One dev environment is a VirtualBox VM, and the other is a ", ", both running Ubuntu 11.04 with ros-electric-brown-remotelab version 0.0.15-s1326515606. In each case establishing a connection to rosbridge through my web browser is enough to produce this behavior. Have others seen this? Are there suggested minimum machine requirements for rosbridge? If this is a common problem, is there anything that I can do to reduce the machine load?"], "answer": [" ", " ", "Victor,", "Thanks so much for the bug-fix and for cross-posting this to our Google Code issue tracker ( ", " ). I've just incorporated this fix into our SVN version ( svn co ", " rosbridge ) and it will be available in the Electric and Diamondback binaries as soon as Willow refreshes their distribution (the packages will be labeled 0.0.19).", "What your fix basically does is make the loop sensitive to the HZ parameter. Since the default HZ in your version is so low (200Hz) CPU usage drops dramatically. However, at the moment most of our users are expecting performance to favor throughput. I've thus changed the default HZ to 100000 (equivalent to 100 kHz) coincident with applying your fix. This will mean that if you want to cut down on CPU usage, you'll need to specify your own HZ as the default will do quite a bit of spinning.", "One other thing I should mention is that the 100% usage is a bit misleading and you should always try to run rosbridge alongside your planned load. While select operations will dominate a CPU on an \"empty\" processor according to top, they are still blocking (as called in rosbridge) and present yield opportunities. The end result is that while rosbridge will take 100% of a CPU with no other load, it will not in general effect the performance of other applications very much (if at all). For example, when you run Gazebo (a CPU-bound process) on the same core, rosbridge will only take up ~1% of the proc according to top.", "Thanks again for the patch!", "_Trevor", " ", " ", " ", " ", "I'm having the same issue, with the latest publicly available version of rosbridge accessing using TCP Sockets.", "Messing with the code and with some profilers, I've seen that there's a loop in the serveForever function in ws.py file that is what's consuming most of the cpu time.", "What I've seen is that in the loop there are two calls to the ", " method. These calls have a timeout which is configurable using the --hz parameter. But in my case this parameter changed nothing.", "The thing is, that these selects were returning immediately, because my socket was available for output (the outputReady list contained a connection to my socket). \nTherefore the loop spinned forever without control or sleeps.", "My solution (though it might have many side effects, i've not tested it extensively) is to replaced the ", " parameter in the first select, and the ", " parameter in the second select, with an empty list.", "What this does is that the first select doesn't end immediately, and waits for its timeout, which reduces the CPU usage to 3% on my computer. Still a lot, but I guess you can play with the hz parameter to reduce it even more."], "url": "https://answers.ros.org/question/12798/rosbridge-cpu-usage/"},
{"title": "Create land and sky in rviz?", "time": "2011-11-27 15:43:57 -0600", "post_content": [" ", " ", "Hey Everyone,", "Is there a way that anyone knows of that would allow me to make the rviz environment look like the land and the sky (instead of the standard grid and background color)?", "Thanks a lot!"], "answer": [" ", " ", " ", " ", "Export your environment as a collada file and then write a node which streams it as a marker (see ", ").", "The problem is that it will consume ressources to send regularly the position.\nIf you want to avoid that, you could write a dedicated rviz plugin.", "edit:", "Follow this tutorial to see a simple example:\n", "then take a look at the Marker message definition:\n", "\nYou can see that using the MESH_RESOURCE type, a model can be displayed in rviz.\nSee also ", "Change the tutorial code to send a mesh instead by filling the appropriate fields in the message definition. To start you can use whatever model is available on your machine.", "Once the model is streamed correctly, you can use a 3d modeler of your choice to create the objects you need using a format that rviz understands (i.e. collada for instance)."], "url": "https://answers.ros.org/question/12111/create-land-and-sky-in-rviz/"},
{"title": "Is it possible to avoid busy-waiting in a spinner? [closed]", "time": "2011-04-10 13:20:59 -0600", "post_content": [" ", " ", "I've read the ", " page, and have written the following spinner:", "If there are no callbacks in the queue, the \"while\" loop will busy-wait, using a lot of CPU time. I could not find a CallbackQueue equivalent of select(). Is it possible to implement such a thing? For example, is it possible to call select() on the sockets over which topic messages and service calls are received (if sockets are used for that purpose). I can reduce the frequency with which callAvailable() is called by creating a ros::Rate object and calling sleep() on it, but I'd rather avoid busy waiting at all if possible."], "answer": [" ", " ", "As I understand the documentation, your loop above does not cause busy-waiting and should not at all consume a lot of CPU time. callAvailable will block and wait for an event to arrive until timeout expires. Internally, callAvailable performs a timed_wait on a condition variable that gets triggered whenever a new callback becomes available."], "question_code": ["void SteeringCtrlr::callCallbacks()\n{\n  static const double timeout = 0.01;\n  while (node_->ok())\n    cb_queue_.callAvailable(ros::WallDuration(timeout));\n}\n"], "url": "https://answers.ros.org/question/9688/is-it-possible-to-avoid-busy-waiting-in-a-spinner/"},
{"title": "No disparity image with rotating detergent", "time": "2011-05-16 17:08:10 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am following the ", " tutorial and receiving some errors.", "At the end, all three image viewers (left, right, disparity) have blank images. How do I get the bag file to play correctly and which commands should I use to get the viewers to work properly?", "Using the tutorial, the steps I did after downloading the rotating_detergent.bag file were:", "The output of 'rosbag info' looked the same as that shown in the tutorial.", "Next, I did", "and the parameters were identical. After that I did", "and the three image viewers open with blank images. In the terminal where 'rosbag play' is running there are several error messages with contents", "Can anyone else run that tutorial and see images displayed correctly? Any hints on why this might not work or how to fix the checksums? ", "Thanks,\nMaxi"], "answer": [" ", " ", " ", " ", "This implies that the executable that produced the message and the executable that consumed the message were compiled with different versions of ROS (or different versions of the ", " message at least)", "In this case the executable that stuffed the images into the bag file was probably compile with cturtle and you are probably using the diamondback version of the image viewer.", " ", " As tfoote pointed out, you can migrate old bag files using migration scripts shown here: ", "Are you in fact running diamondback btw? "], "question_code": ["rosbag reindex rotating_detergent.bag\nrosbag info rotating_detergent.bag\n", "ROS_NAMESPACE=narrow_stereo_textured rosrun stereo_image_proc stereo_image_proc\nrosrun dynamic_reconfigure reconfigure_gui\n", "rosrun image_view stereo_view stereo:=narrow_stereo_textured image:=image_rect\nrosbag play rotating_detergent.bag -r 0.1\n", "[ERROR] [1305607385.588706440]: Client [/narrow_stereo_textured/stereo_image_proc] wants topic /narrow_stereo_textured/left/camera_info to have datatype/md5sum [sensor_msgs/CameraInfo/c9a58c1b0b154e0e6da7578cb991d214], but our version has [sensor_msgs/CameraInfo/1b5cf7f984c229b6141ceb3a955aa18f]. Dropping connection.\n[ERROR] [1305607385.589029589]: Client [/narrow_stereo_textured/stereo_image_proc] wants topic /narrow_stereo_textured/right/camera_info to have datatype/md5sum [sensor_msgs/CameraInfo/c9a58c1b0b154e0e6da7578cb991d214], but our version has [sensor_msgs/CameraInfo\"/1b5cf7f984c229b6141ceb3a955aa18f]. Dropping connection.\n"], "answer_code": ["CameraInfo"], "url": "https://answers.ros.org/question/9972/no-disparity-image-with-rotating-detergent/"},
{"title": "amcl subscriber queue", "time": "2011-05-19 13:06:23 -0600", "post_content": [" ", " ", " ", " ", "I am currently working with the AMCL node and I noticed a spring effect when the robot moves at high speed, I mean that there is a kind of delay in the transformation provided by AMCL node. I initially assumed that my PC was not fast enough, then I tried with a core-i7 \nbut the situation has not improved as hoped. So I took some time to analyze the code and I want to ask you something about it: ", "at lines 345 + 346 of the amcl_node  (", " @ \nline 345 + 346) there are the definitions of the laser subscribers and filters, both with a \"queue_size\" of 100 messages. In this case, if the laser node publishing frequency is 25hz, this queue will contain a long \"history\" (4 secs) and if the PC is not fast enough to process \nevery single beam in this queue, this could be the reason of the \"spring effect\". ", "I tried to change the value of the queue, I set it to 1 because I think AMCL should take the last laser scan to compute the new pose (not a pose in the past, even if the published transform is future dated). ", "With this change the spring effect is no longer present, but I would like to understand why the authors of the code put a queue lenght like that, maybe there's something that I have not yet considered. ", "Thank you! \nAugusto "], "answer": [" ", " ", "I put the queue length of 100 in there, not for any particularly good reason, but because it was the sort of default that I was using everywhere at the time.  I agree that is could be lowered, but I don't think that it should be causing the behavior that you're seeing.", "Fundamentally, ", " is clocked by incoming laser data: it only does work on receipt of a new scan.  When a scan is received, the robot's travel distance is checked, and if it's large enough (compared to ", " and ", "), then a filter update is performed; otherwise the scan is discarded.  Under normal circumstances, the great majority of laser scans are simply discarded, with minimal computational cost, and the incoming message queue doesn't fill up.", "If your robot is moving very fast and/or you have set the ", " parameters very low, then I suppose that ", " could get busy, processing more than the usual number of laser scans.  In that situation, and if laser scans are also coming in a high rate, then I could see how the incoming message queue might fill up.  But I'd be a little surprised.", "Is the ", " node consuming 100% of the CPU?", "Can you provide a bag with representative data that I can use to reproduce the problem?"], "answer_code": ["amcl", "~update_min_d", "~update_min_a", "~update_min_*", "amcl", "amcl"], "url": "https://answers.ros.org/question/10003/amcl-subscriber-queue/"},
{"title": "Advice ROS Single Board Computer - (Kinect, +other sensors)", "time": "2011-07-23 08:07:24 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have a robotic platform that I wish to put ROS and the Kinect sensor on.", "I would like some advice on the hardware for the robotic platform I am building. My current robot platform has blackfin processors which do not have USB interfaces. I need a board that will be powerful enough to process the Kinect data, and be able to accept all the camera, sensors, etc that I already have. I need ports for I2C, UART, additional USB for webcam etc.", "I looked at the Beagleboard, and it was suggested it would not be powerful enough to handle the Kinect.", "It was suggested to me that a mini-itx board would be a good replacement. ", "How would I integrate the i2C, motor controller, and other components into this type of board? Is there some kind of I2C and various connector expansion pack I can get for this?", "Is there another signle board computer that will work well?\nAny advice is appreciated."], "answer": [" ", " ", "You have two ways of integrating the I/O interfaces:", "Look for an embedded board that supports all your requirements.", "Buy a embedded board with good performance and use a microcontroller or similar (PC104) for additional interfaces.", "I think option 2) will be much simpler. Especially I2C is not necessarily available and if you want to process Kinect data you don't want to compromise on power. The arduino is a comparably easy way to get I2C and depending on the data rate you can get additional UARTs.", "When you look into embedded boards in comparison to a standard board, you will usually find that you get things like multiple UARTs which can be really nice and is fading out in consumer products.", "We have a NANO-PV-D510A and a WAFER-945GSE board and both work well, although they not up 2 date on power.", "I think, the ", " is used quite often.", " ", " ", "Thanks for the response. I have been looking at various boards and really am not sure what will work with ROS and Kinect well. Were you using the boards you mentioned with a robotic application and the Kinect sensor?", " ", " ", "Take a look here:\n"], "url": "https://answers.ros.org/question/10701/advice-ros-single-board-computer-kinect-other-sensors/"},
{"title": "care-o-bot spins in place with move command", "time": "2011-10-05 13:09:47 -0600", "post_content": [" ", " ", " ", " ", "Does anyone know why the care-o-bot will sit in place and rotate about the z axis with some move commands.  For example:", "self.sss.move(\"base\",\"order\");", "From a startup state, the above command in a python script will cause the bot to spin in place.  I'm running ubuntu 10.4 with electric."], "answer": [" ", " ", "What kind of navigation are you using? I assume you are using \"roslaunch cob_2dnav 2dnav.launch\", which is the ROS navigation. Sounds like the robot in not properly localized. Please check the localization in rviz.", " ", " ", "Gazebo is a simulator (generates data) and rviz is a visualization (consumes/visualizes data)."], "url": "https://answers.ros.org/question/11434/care-o-bot-spins-in-place-with-move-command/"},
{"title": "Processing of point clouds generated by ROS", "time": "2011-10-06 09:34:27 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI'm working with ROS on object identification. ", "1] After ROS generates point cloud from live video, where are the point clouds stored and in what file format? how do we convert a file to PCD format?", "2] How do we seperate points for the objects to be identified from the point cloud of the whole frame? Is there a filtering process for that? ", "3]How many seconds of live video is enough for ROS to generate appropriate pointcloud for object identification? ", "Thanks.. any help is appreciated!"], "answer": [" ", " ", " ", " ", "1) think of ROS as a messaging infrastructure, like a phone company. It lets you listen to (subscribe to) messages that are being sent right now. The messages aren't \"saved\" anywhere, much like how words spoken on the phone aren't saved in the copper wire that transmits them. If you have a program that collects and saves messages they can be stored in any file format you like though. The ROS messages all have a formats, of course.  ", "If you want to know the details of a particular message you can use the command ", "In particular, for point cloud messages, you might want to look at", "If you just want to listen to a topic with this type of message and have the result saved as pcd files you can use the following command (directly in terminal). Supposing that you have the topic /camera/rgb/points where pointclouds are being published:", "use ctrl-c to abort when you have enough messages as they tend to consume a fair bit of disk-space in the long run.", "This will save points published on the topic /camera/rgb/points into the present working directory as kinect_rgb_xxxxxxxxxxxxx.xxxxxxxxx.pcd where the x's are time signatures generated by ROS. For more advanced programs, look at pointclouds.org", "If you're unsure about what topics are currently available, do ", "2) There are several algorithms and methods for what you describe (also known as object segmentation) none of which I could properly fit into a reply here. You'll probably find hundreds of papers on the subject at IEEEXplore or similar databases. pointclouds.org describes one way of doing it.", "3) Object identification, depending on the application/context can be done with one single frame. It's not so much a matter of how many views you have of the object as what you are extracting from the views and comparing them to. Again, much research is done on this subject.", "BTW. live video from a regular camera won't give you point clouds, the message type would most likely be sensor_msgs/Image. see the output of:"], "answer_code": ["rosmsg show <message type>\n", "rosmsg show sensor_msgs/PointCloud2\n", "rosrun pcl_ros pointcloud_to_pcd input:=/camera/rgb/points _prefix:=kinect_rgb_\n", "rostopic list\n", "rosmsg show sensor_msgs/Image\n"], "url": "https://answers.ros.org/question/11449/processing-of-point-clouds-generated-by-ros/"},
{"title": "How to make use of the nvidia gpu on ASUS 1215N for rviz and gazebo", "time": "2011-09-23 12:09:17 -0600", "post_content": [" ", " ", "How to make use of the nvidia gpu on ASUS 1215N for rviz and gazebo.", "The default Intel gpu and driver does not support GLX Direct Rendering, so rviz and gazebo will not run."], "answer": [" ", " ", "Please note this list of instructions is constructed off my memory, and it will almost certainly semi-permanently mess up rendering on your laptop, please only attempt it at your own risk!", "Steps to \"working\" nvidia gpu in Natty on ASUS 1215N:", "Finally, to start rviz:", "How this works:  By default, all rendering to the screen are done through the i915 kernel module.  Ironhide tunnels an invisible rendering buffer for the nvidia gpu to the screen using VirtualGL.  For details, please consult the ironhide site.", "Potential trouble:  If something goes wrong, and you find yourself having to \"type in the dark\" because screen rendering has been compromised, boot into rescue mode, remove or rename ", ", make sure you have the ", " kernel module installed:", "and reboot.", "Please let me know if you make any attempts to replicate this process, so I can refine and make corrections to the above instructions.", "Lastly, for reference, turtlebot gazebo simulation runs about 1.4Xreal-time on the asus laptop if sensor generation is not activated.  Once you subscribe to the kinect point clouds, simulation drops to about 0.15Xreal-time.", " ", " ", "With my Asus 1215N that came with my turtlebot...", "I had trouble with Ironhide; instead, I ended up using Bumblee Project at ", "\">", " . With it, I get ~250 fps from optirun glxgears (and it requires no configuration; just download, install, reboot).", "Note: do not confuse \"Bumblee Project\" with just plain \"Bumblebee,\" which is a parent of both Bumblebee Project and Ironhide."], "answer_details": ["Upgrade to Natty", "Blacklist nouveau, fbcon, vga16fb in ", " by adding following lines:", "Make a backup of ", " (sometimes installing nvidia proprietary driver overwrites ", ", breaking glx for the intel driver).  If that happens, restore it with the copy that you've backed up, or reinstall xserver-xorg-core.", "Follow instructions on the ", " and install Ironhide.  This automatically pulls in nvidia-current.  Make sure this process does not create ", ", if that happens, rename it to something else.  Otherwise, you'll get a blank screen when you boot up your laptop again.", "Reboot your laptop.", "now try ", ", and you should see ", ".  (Also, glxgears gets about 60 fps with i915, but ~170 fps with nvidia (", ").", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["/etc/modprobe.d/blacklist.d", "blacklist nouveau\nblacklist fbcon\nblacklist vga16fb\n", "/usr/lib/xorg/modules/extensions/libglx.so", "libglx.so", "/etc/X11/xorg.conf", "Direct Rendering: Yes", "roscd rviz\noptirun -f ./bin/rviz\n", "/etc/X11/xorg.conf", "i915", "sudo apt-get install --reinstall xserver-xorg-video-intel xserver-xorg-core\n"], "url": "https://answers.ros.org/question/11297/how-to-make-use-of-the-nvidia-gpu-on-asus-1215n-for-rviz-and-gazebo/"},
{"title": "Using GPL licensed packages in a commercial product", "time": "2011-12-12 00:54:36 -0600", "post_content": [" ", " ", "Hi,\nUsing a GPL licensed package in a commercial product should not be a problem, right? It ought not force us to release all our own software in the product under the GPL? As long as we're not linking directly towards the package, but rather just send or receive messages from it, that is.", "If I understand it correctly, if we use a secret message format that we don't explain to anyone we could get into trouble. But now it's just a Scan message. It should be fine, shouldn't it?", "We will still have to distribute the source code of the GPL'ed package of course, if requested too.", "Regards,\nRasmus Ahlberg"], "answer": [" ", " ", "Too long for a comment to Dimitri's answer, so separate answer.", "IANAL, but I disagree with the interpretation that requiring a GPL'd node is \"linking against\" it. Statically or dynamically linking against a GPL component will get you one, complete binary file that contains that GPL component, hence the license applies to all of that binary. You should be careful about the manifest.xml, since you may automatically be linking against things if your ROS node has a dependency on the GPL'd package and it exports a library. That is one reason to use separate message packages.", "If you are not directly linking against the GPL'd library, then your software should not need to be licensed as GPL. Just because you \"require\" a piece of GPL software in order to function, as long as the GPL component is standalone, I do not believe your software also needs to be GPL. Since nodes communicate via TCP and live in completely separate processes (and hence are separate binaries), I do not think the GPL can cross the process boundary and \"infect\" other pieces of software on the system. For example, much of ROS \"depends\" on the Linux kernel to function (which is GPL'd) but, because we don't link directly against the kernel and live in user-space, we don't need to worry about the kernel's licensing terms, since ROS is not a \"derivative work\" of the Linux kernel.", "Really, my understanding is that if component FOO never includes any of the copyrighted material of BAR (either through headers or linking), then the license for BAR ", " affect FOO. Again, IANAL, so obviously you should consult an actual intellectual property lawyer (maybe the FSF has a list/could point you to one knowledgeable about the GPL) before making any business decisions.", " ", " ", " ", " ", "A question like this is extremely complex. Linking with GPL'd licenses is still regularly under debate. You really need to talk to a lawyer or someone that specializes in these areas. ", "That being said, here is my take on the issue and some concerns that you must think about. The GPL states that \"Linking ABC statically or dynamically with other modules is making a combined work based on ABC. Thus, the terms and conditions of the GNU General Public License cover the whole combination.\" That being said, although you may not be changing the GPL'd package in any way, if your package requires that package to exist in order to function, I would say that's linking against. If the GPL'd package is included in your manifest.xml, then it's pretty clear that it's required for operation. ", "Even without requiring the package directly, if your program requires that the GPL'd node be running, one could argue that you're \"linking against\" it. The ROS node system obfuscates the linking process, and hence you would need to speak to a professional. ", "ROS's BSD license does not carry these concerns.  ", ": The Free Software Foundation made the following two statements regarding programs that operate using pipes, sockets, RPC (like ROS), etc.", "\"By contrast, pipes, sockets and command-line arguments are communication mechanisms normally used between two separate programs. So when they are used for communication, the modules normally are separate programs. But if the semantics of the communication are intimate enough, exchanging complex internal data structures, that too could be a basis to consider the two parts as combined into a larger program.\" ", "and", "\"The glue script (rpc and rpc messages, sockets, pipes, etc.) would ultimately create a single work, derived from both the original scripts, and you would need to follow the terms of all those licenses to create it. Combining the first script with the second this way would violate its exception-free GPL.\"", "The Free Software Foundation is very active in protecting the rights of GPL'd code, so it's best not to make any assumptions at all in this matter. While there are merits to both sides of the argument, the one thing we all agree on is that there's someone that knows the rules better than we do. You should find that person and talk to them, just to be safe. "], "url": "https://answers.ros.org/question/12313/using-gpl-licensed-packages-in-a-commercial-product/"},
{"title": "When should I send geometric information over a topic as opposed to TF?", "time": "2011-11-07 11:39:44 -0600", "post_content": [" ", " ", " ", " ", "Please help in writing up a ROS best practice."], "answer": [" ", " ", "Some cases when tf is less suitable:", "Added: ", " ", " ", "In my experience, I find tf useful for real-time, single-estimation data; this breaks down when you want one to apply a correction to a geometric relationship in the past (like when a loop closure occurs during SLAM, and you want to use this new information to update the entire robot's path history), or when you have a couple things estimating the same transform (like odometry and laser scan matching, or two robots looking at the same thing). There are some ways to make tf do what you want in these cases, but they don't exhibit the same simplicity and elegance that tf shows for the real-time, single-estimation case.", "Multi-robot tf over wifi, as others mentioned, won't work like you want it to out of the box. But a fairly lightweight fix is to use multiple /tf topics, say /robot_model_bandwidth_hog/tf and /i_wanna_communicate_these_few_transforms_to_other_robots/tf", "Added: ", " ", " ", "Suitable for:\nRobot joints, links, Sensor coordinate frames, and any other coordinate system that is going to be used as reference.", "Not suitable for:\nVery noisy data.", "Also consider:\nUsing a topic rather than tf means you can more easily record / replay the data with rosbag/rxbag, as well as transfer the data between multiple robots.", " ", " ", "If you for example just want to monitor the estimated position of an autonomous robot, using tf can mean significant overhead. Tf can easily consume bandwidth in the order of hundreds of kB/s, which can choke up connections in constrained scenarios (like RoboCup, where you have thousands of people/hundreds of teams/1-2 dozen leagues competing for Wi-Fi). An optional lower update rate tf topic for displaying the robot model in rviz for example could make sense for such situations.", "Added: "], "answer_details": ["The coordinate frames naturally form a graph rather than a tree (e.g., a slam graph).", "There is frequent reparenting of frames (e.g., a robot which is localized w.r.t. a changing local frame rather than a global one)", "The transformations include additional information such as uncertainty estimates not present in the TF message type.", "For efficiency, in situations where there are a large number of frames, most of which are only of interest to a few subscribers.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/11837/when-should-i-send-geometric-information-over-a-topic-as-opposed-to-tf/"},
{"title": "How to use tf in more than one robot?", "time": "2012-01-19 00:39:52 -0600", "post_content": [" ", " ", "Hi!", "I'm trying to do coperative things in between turtlebots. The thing is that first I need both robots in the same core, otherwise they can not interact. But when I put them under the same core I don't know how to solve the tf.", "I've tried to push one tf to turtlebot1/tf and the other to turtlebot2/tf, but then I can not see the tf on rviz. Is it correct to do that or do I need a tf which contains all the robots? If i need this how can I start having a look at that to do it?", "Thanks!"], "answer": [" ", " ", "try setting the tf_prefix rosparam in your launch file.", "as a word of caution: having multiple robots on a single master consumes a lot of network bandwidth (especially if they share /tf!), you may run into seemingly strange issues as the result of dropped packets. If you try and scale beyond two robots without throttled topics over a standard 802.11g wifi connection you will definitely run into bandwidth constraints and may be forced to run a single master per robot and foreign relay between them."], "url": "https://answers.ros.org/question/12688/how-to-use-tf-in-more-than-one-robot/"},
{"title": "org.ros.rosjava Jar Messages files Do Not update", "time": "2011-12-20 19:23:56 -0600", "post_content": [" ", " ", "Hi all,\nMy message jar files at at /.ros/rosjava/lib/org.ros.rosjava.my_msgs-0.0.0.jar. I have the problem that when I change the message (add one more field) and rosmake the message. It is compiled but the jar files are not updated. I even removed the org.ros.rosjava.my_msgs-0.0.0.jar from /.ros/rosjava/lib/ even then it does not generate new jar file for my_msgs.\nDid anyone have the same problem, how can I fix it??\nRegards"], "answer": [" ", " ", "This is a known issue. See ", "As a workaround, the easiest (although time consuming) thing to do is ", " and then rebuild. It is also possible to selectively remove everything from ", " (i.e. use grep and find) that has to do with you package and then rebuild which is faster."], "answer_code": ["rm -rf ~/.ros/rosjava", "~/.ros/rosjava"], "url": "https://answers.ros.org/question/12415/orgrosrosjava-jar-messages-files-do-not-update/"},
{"title": "Navigation stack tutorial: move_base crashes: bad_alloc thrown, gdb output attached", "time": "2012-02-06 02:10:42 -0600", "post_content": [" ", " ", "Hi,", "This is a question that is related to ", " ", "It is a follow-up in the sense that in the previous question I proposed to use AMCL and a pre-made map as in the ROS tutorials. In the previous question, I was not using a pre-made map. I was using Gmapping and making the map as I went along. This time I am running AMCL, pretty similar to the tutorial described in: ", "While following this tutorial, the move_base node dies even before I send a navigation goal. Upon searching ROS-Answers for similar questions, I came across ", "where the solution was investigated by running gdb and running a back trace. I have attempted the same, and the results of the back trace are attached. ", "After a few seconds, the terminal where I run the navigation launch file crashes. Here is the output of that terminal along with the gdb backtrace : ", "I hope this information is sufficient. Please ask me anything else that you might require. ", "Cheers!"], "answer": [" ", " ", "I would run ", " while you try to run this program to see how much of your system memory you are consuming. You're using a 4000x4000 pixel map, so this could easily use the majority of your RAM. Maybe I read it wrong, but it looked like you loaded this map more than once in the backtrace. This could explain your problem. ", "Thanks for your answer Dimitri, it was indeed because the default parameters for gmapping, create a map that starts with 200X200 m. Ofcourse it is possible to change the start size of the map using gmapping parameters (xmin, ymax etc.). A small starting map size worked. Thanks!"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Run Motor drivers", "Run Laser drivers", "Run relevant transforms", "Run pre-made map through map server. ", " ", "Run Navigation : ", " , ", " , ", " , ", " , ", " .", "Here are the tf frames: ", "Here is the rxgraph: ", "Here is the rxgraph with all topics: "], "answer_code": ["top"], "url": "https://answers.ros.org/question/12904/navigation-stack-tutorial-move_base-crashes-bad_alloc-thrown-gdb-output-attached/"},
{"title": "Typical Methods of Building Large 3D Maps?", "time": "2012-03-22 06:28:26 -0600", "post_content": [" ", " ", " ", " ", "I'd like to build a 3D map of our lab (a number of rooms and hallways and cubicles, etc) using a Kinect and/or Turtlebot.", "I've looked into a number of methods (right now I'm looking at possibly using ", "), but I'd like to know if there are any typical methods people use.", "(Related side-note: I tried using a Turtlebot with ", " to build a 2D map of one section of the office, and it came out horribly. It also refused to work properly with any sort of loop closures involving hallways, I think largely because of the Turtlebot's horrible odometry.)", "Please ask the TurtleBot problem as a separate question with any diagnostic information you have. Also, you may want to check that the gyro is calibrated and that you are using the latest turtlebot code. The calibration code has been improved here ", "Also this belongs in a seperate question but this may also help with tuning the parameters for gmapping. ", "I mostly included that just to note other similar things I've tried, but thanks for the info -- it looks very helpful for if I decide to try that again. :)"], "answer": [" ", " ", "This is an area of open research, with several research groups working in slightly different directions. So, I am not sure there is a consensus on what is the best approach.", " is a nice resource the provides code for some 2D and 3D SLAM systems.", "Also, if you are looking for papers you may want to look at the archives for the RGB-D Workshop at RSS. ", "The biggest problems I see with large scale mapping is managing the massive amount of data coming from the sensor and limiting the growth of the map over time without discarding critical information. One solution is to simply wait for Moore's law to provide faster processors.", "As odd as it might sound, I'm actually semi working alongside (kinda complicated) one of those research groups. ;) Part of why I'm asking is I wanted to try to see things from the user/consumer side instead of the developer side. Thanks for the response, that's helpful information!", "Something like SLAM6D ", " seems pretty useful for surveyors and architects, even in the state it is in now.", "Robots which could benefit the most, such as quadrotors with 6DoF are often very cpu limited which makes 3DSLAM even more difficult to do in real-time.", "Also, for those who are interested, Dr. N\u00fcchter has a book that I can recommend that covers many of the fundamentals of 3D mapping. \nAndreas N\u00fcchter. 3D Robotic Mapping. Springer Tracts in Advanced Robotics (STAR), ISBN 978-3540898832, 210 pages, Springer Verlag", " ", " ", " ", " ", "You can use RGBDSLAM for a large scale map (in the terms you mentioned), but it's not out-of-the-box-one-shot-mapping, but carefully-moving-the-camera/robot-mapping, most possibly with several attempts and some parameter tuning."], "url": "https://answers.ros.org/question/30254/typical-methods-of-building-large-3d-maps/"},
{"title": "rosbridge (Websocket protocol update ?)", "time": "2012-04-10 05:45:30 -0600", "post_content": [" ", " ", "Hello everybody,\nDid someone plan to update this package according the last websocket protocol ?", "Sincerely,", "Beno\u00eet"], "answer": [" ", " ", " ", " ", "Are you sure you're running the latest version? A quick sanity check for me testing ", " webpage against ", " service works correctly under Firefox 11 for Windows, Firefox 11 for Fedora, Chrome 18 for Windows, and Chrome 18 for Fedora:", " ", " ", "Hello,\nIn fact I have developed an application like yours. My goal is to have several widgets in the clients side (javascript). For example one widget for a basic display of a topic in ROS. A second one for some graph using flot. An other one which could display  the roll,pitch,yaw ...\nIt's also using websockets and it is multithreaded.\nFor now, i display messages from ros each time a new message arrives (using threading.Event())\nOne of the problem of my application is the CPU consumption", "Last week, i discovered your application which looks really incredible. Moreover you can write messages in ros.That's why i will update your node with the last websocket protocol and compared the CPU consumption. \nI 'll give you some feedback soon", "Beno\u00eet", "I'm glad rosbridge could be of some use to you. One thing you should definitely check out if CPU usage is of a concern to you is the hz rosparam. For example, rosparam set /brown/rosbridge/hz 200  will greatly reduce usage compared to the default.", " ", " ", "I implemented the last webscoket protocol for the handshake function. It works because my socket status is connected :).\nWhen I launch run rosbridge node , I have \"2 concurrent connection\"", "I register a handler for a specific topic and subscribe to this topic following\n", " and ", "Apparently it works because I dont catch any error but I don't receive message from this topic in my javascript client.\nFinally, after my main function ended, I get an error in the javascript file (from firebug)", "The connection to ws://127.0.0.1:9090/ was interrupted while the page was loading.", "this.socket = new WebSocket(url);", "And appears in Rosbridge terminal :", "closed 9", "I can't join my javascript code because my Karma is too low. Let me know if you need it", "I'm having a bit of trouble following. Does rosbridge seem to have compatibility problems with FF 11? ", "Yes, rosbridge doesn't work with FF11 because the websocket protocol has been updated in this version of FF", " ", " ", "That's the (not necessarily specifically scheduled) plan. Do you know of any release browser implementing the new draft? That's usually when we make time for the update.", " ", " ", " ", " ", "Sorry Tjay i didn't answer to your question. For example, Firefox V11.0 uses the last websocket protocol. Moreover, MozWebsocket is deprecated"], "url": "https://answers.ros.org/question/31568/rosbridge-websocket-protocol-update/"},
{"title": "Querying topic types in C++", "time": "2012-03-13 19:03:59 -0600", "post_content": [" ", " ", " ", " ", "I there a way in C++ to query if a topic exists and if so what message type it uses?", "I am trying to write a generic visualization tool and would like to subscribe to a topic using a callback matching the topic's type.", "Alternatively a method that accepts all messages but allows me to learn their type in the callback would also work."], "answer": [" ", " ", "The ", " should be able to do that. (getPublishedTopics)", " ", " ", "Take a look at shape_shifter in the topic_tools package.\n", "You can subscribe to any topic and when a message has been received consult the message type."], "url": "https://answers.ros.org/question/29660/querying-topic-types-in-c/"},
{"title": "Error installing ROS on MacBook Pro OS X 10.6.7", "time": "2011-07-02 09:29:32 -0600", "post_content": [" ", " ", " ", " ", "Hello i am trying to install ROS on a MacBook pro running OS X 10.6.7 and i am getting this error!\nAny help please?"], "answer": [" ", " ", " ", " ", "It seems it cannot locate you boost installation, which is you installed from macports should be at:", "Your install seems to search in ", " directories. Can u make sure these environment variables are set", "The run the install again after verifying that you have boost installed correctly. For further instructions, please consult this ", " ", " ", "This thread is a bit old, but for others - I had a similar issue albeit with homebrew, and this worked for me:", "export ROS_BOOST_ROOT=/usr/local/Cellar/boost/1.48.0", " ", " ", "You are missing boost. Do you have boost installed?"], "question_code": ["Bootstrapping ROS build\nDetected ros_comm bootstrapping it too.\n[ rosmake ] Packages requested are: ['ros', 'ros_comm']                         \n[ rosmake ] Logging to directory/Users/Diamadis/.ros/rosmake/rosmake_output-20110703-002019\n[ rosmake ] Expanded args ['ros', 'ros_comm'] to:\n['rosunit', 'rospack', 'rosmake', 'rosemacs', 'rosdep', 'roscreate', 'rosclean', 'rosboost_cfg', 'rosbash', 'test_rospack', 'test_rosmake', 'test_roslib', 'test_rosdep', 'test_roscreate', 'roslib', 'roslang', 'rosbuild', 'mk', 'xmlrpcpp', 'roswtf', 'rostime', 'message_filters', 'cpp_common', 'topic_tools', 'rostopic', 'rostest', 'rosservice', 'rosrecord', 'rosparam', 'rosout', 'rosnode', 'rosmsg', 'rosmaster', 'roslaunch', 'rosgraph', 'rosconsole', 'rosbagmigration', 'rosbag', 'test_topic_tools', 'test_roswtf', 'test_rostopic', 'test_rostime', 'test_rostest', 'test_rosservice', 'test_rospy', 'test_rosparam', 'test_rosnode', 'test_rosmsg', 'test_rosmaster', 'test_roslib_comm', 'test_roslaunch', 'test_rosgraph', 'test_roscpp_serialization_perf', 'test_roscpp_serialization', 'test_roscpp', 'test_rosbag', 'test_ros', 'test_crosspackage', 'perf_roscpp', 'roscore_migration_rules', 'std_srvs', 'std_msgs', 'rosgraph_msgs', 'rospy', 'roslisp', 'roscpp_traits', 'roscpp_serialization', 'roscpp']\n[ rosmake ] Generating Install Script using rosdep then executing. This may take a minute, you will be prompted for permissions. . .\nrosdep executing this script:\n{{{\nset -o errexit\n#No packages to install\n}}}\n[ rosmake ] rosdep successfully installed all system dependencies               \n[ rosmake ] Starting >>> tools/rospack                                          \n[ rosmake ] Finished <<< tools/rospack                                          \n[rosmake-0] Starting >>> roslib [ make ]                                        \n[rosmake-1] Starting >>> rosemacs [ make ]                                      \n[rosmake-1] Finished <<< rosemacs  No Makefile in package rosemacs              \n[rosmake-1] Starting >>> rosboost_cfg [ make ]                                  \n[rosmake-1] Finished <<< rosboost_cfg  No Makefile in package rosboost_cfg      \n[rosmake-1] Starting >>> rosbash [ make ]                                       \n[rosmake-1] Finished <<< rosbash  No Makefile in package rosbash                \n[rosmake-1] Starting >>> rosbuild [ make ]                                      \n[rosmake-1] Finished <<< rosbuild  No Makefile in package rosbuild              \n[rosmake-1] Starting >>> roslang [ make ]                                       \n[rosmake-1] Finished <<< roslang  No Makefile in package roslang                \n[rosmake-1] Starting >>> mk [ make ]                                            \n[rosmake-1] Finished <<< mk  No Makefile in package mk                          \n[rosmake-1] Starting >>> xmlrpcpp [ make ]                                      \n[ rosmake ] All 20 linesoslib: 15.2 sec ] [ xmlrpc... [ 2 Active 7/68 Complete ]\n{-------------------------------------------------------------------------------\n  mkdir -p bin\n  cd build && cmake -Wdev -DCMAKE_TOOLCHAIN_FILE=`rospack find rosbuild`/rostoolchain.cmake  ..\n  [rosbuild] Building package roslib\n  [rosbuild] Including /Users/Diamadis/ros/ros_comm/clients/roslisp/cmake/roslisp.cmake\n  [rosbuild] Including /Users/Diamadis/ros/ros_comm/clients/rospy/cmake/rospy.cmake\n  [rosbuild] Including /Users/Diamadis/ros/ros_comm/clients/cpp/roscpp/cmake/roscpp.cmake\n  Traceback (most recent call last):\n    File \"/Users/Diamadis/ros/ros/bin/rosboost-cfg\", line 35, in <module>\n      rosboost_cfg.main()\n    File \"/Users/Diamadis/ros/ros/tools/rosboost_cfg/src/rosboost_cfg/rosboost_cfg.py\", line 328, in main\n      raise BoostError(\"Cannot find boost in any of %s\"%search_paths(options.sysroot))\n  rosboost_cfg.rosboost_cfg.BoostError: \"Cannot find boost in any of [('/usr', True), ('/usr/local', True)]\"\n  CMake Error at /Users/Diamadis/ros/ros/core/rosbuild/public.cmake:848 (message):\n    rosboost-cfg --include_dirs failed\n  Call Stack (most recent call first):\n    CMakeLists.txt:5 (rosbuild_add_boost_directories)\n\n\n  -- Configuring incomplete, errors occurred!\n-------------------------------------------------------------------------------}\n[ rosmake ] Output from build of package roslib written to:\n[ rosmake ]    /Users/Diamadis/.ros/rosmake/rosmake_output-20110703-002019/roslib/build_output.log\n[rosmake-0] Finished <<< roslib [FAIL] [ 15.26 seconds ]                        \n[ rosmake ] Halting due to failure in package roslib. \n[ rosmake ] Waiting for other threads to complete.\n[rosmake-1] Finished <<< xmlrpcpp [PASS] [ 81.85 seconds ]                      \n[ rosmake ] Results:                                                            \n[ rosmake ] Built 9 packages with 1 failures.                                   \n[ rosmake ] Summary output to directory                                         \n[ rosmake ] /Users/Diamadis/.ros/rosmake/rosmake_output-20110703-002019         \nTraceback (most recent call last):\n  File \"/usr/local/bin/rosinstall\", line 5, in <module>\n    pkg_resources.run_script('rosinstall==0.5.16', 'rosinstall')\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib ..."], "answer_code": ["/opt/local/include/boost/\n", "/usr/...", "export CPATH=/opt/local/include\nexport LIBRARY_PATH=/opt/local/lib\nexport DYLD_FALLBACK_LIBRARY_PATH=$DYLD_FALLBACK_LIBRARY_PATH:/opt/local/lib\nexport PATH=/opt/local/bin:$PATH\n"], "url": "https://answers.ros.org/question/10467/error-installing-ros-on-macbook-pro-os-x-1067/"},
{"title": "rospy horrible performance with sim_time", "time": "2012-03-09 04:18:34 -0600", "post_content": [" ", " ", "It is widely known that python isn't the best language when it comes to performance.\nBut I've detected that mixing rospy and a simulation environment leads to awful performance.", "Here's a quick example, run the following commands with and without gazebo running (or switching /use_sim_time from false to true while the simulator is running): ", "roscd rospy_tutorials/001_talker_listener/\n./listener.py", "If you run a tool like top, you'll see that this listener node (which is doing nothing but waiting for the /chatter topic) is taking up around 10% of cpu time, depending on your computer.\nBut with /use_sim_time = false it uses 0% cpu.", "Things get worse if you start an ActionServer, cpu usage with sim time is 15% and cpu usage without it is still 0%.", "My guess is that it is because of listening to the /time topic, but it still seems to be that this performance is unacceptable for an application that is doing nothing.", "Is there any way to improve this situation? Are there any plans for the future?"], "answer": [" ", " ", "The app isn't actually \"doing nothing.\" It's having to follow the simulator's time, which will take some CPU.", "On my Macbook Pro, 2.2Ghz, I can get the same CPU numbers if the simulator is publishing time at 1Khz, i.e. that is the load of deserializing 1000 clock messages a second.  The clock rate is a function of your simulation step size and the speed of the simulation.  You may, for example, to consider a smaller step size if you don't need that level of clock fidelity.", "There are some tweaks that rospy can use to lower the CPU usage, but these would be small improvements.  The fact is, running nodes in simulated time will add CPU load.", "I understand that the application must listen to the time topic, what I meant to point out is that even taking this into account, if you start an Action Server, the cpu consumption with use_sim_time = true increases to 15% but with use_sim_time=false it remains at 0%. "], "url": "https://answers.ros.org/question/29425/rospy-horrible-performance-with-sim_time/"},
{"title": "Using rosserial for a ATmega168/Arduino based motorcontroller", "time": "2012-03-01 08:51:52 -0600", "post_content": [" ", " ", " ", " ", "I once again tried to use rosserial for a Atmega168 based microcontroller (", "). As described in the manual of the microcontroller, one should set the Arduino IDE to use the \"Nano with ATmega168\" setting. Flashing for example the Arduino \"blink\" example works fine, but I can't get the controller to work with rosserial. I installed the current rosserial version as described in the ", " tutorial. I then also confirmed that the rosserial installation works in principle by testing with a Arduino UNO, which works fine.", "Flashing rosserial \"HelloWorld\" example on the ATmega168 based microcontroller with the Arduino IDE, I get the following:", "Starting the rosserial python node, this is then the output:", "The subscriber example shows the same symptoms (flashing looks fine, but rosserial never connects). They look a bit like the sketch uses too much memory, but rosserial now reduces the num of publishers/subscribers for ATmega168 and I confirmed via a #error in ros.h that it indeed uses the nodehandle with the lower subscriber number. Any suggestions are welcome.", "/Update:\nTo gain some further insights, I started with the Arduino \"Blink\" example and added rosserial stuff step by step to see when problems start. This is what I got:", "When I comment in the nh.spinOnce() call, the light stops blinking. My understanding is that normally, this call should not block when there is no rosserial connection, or is this a misconception? A connection using the rosserial_python client does not work, as described further above.", "Update 2:\nFor testing serial communication, I use the following code:"], "answer": [" ", " ", "So, I've also been playing around with similar problems trying to send an Imu message (about 320 bytes) using a 328 (set in rosserial for 280 bytes) and I couldn't get it to work.", "This is what I have learning by experimenting with that problem and also by comparing a 328 nano with a 168 nano.", "First it is important to note that adjusting memory needs to be done from both sides, i.e.  setting the buffers too low will also be a problem because then the initialization messages won't fit.\nFor the HelloWorld example I needed to set a minimum size of 79 (this will increase if the topic name is longer).", "I tried to increase the buffer to get to the working area for sending strings in the helloworld example and found out that on the 168 \n", " is the highest setting that works.\nSo, depending on what you want to send this might be sufficient although the memory limit is quite harsh.", "I tried to play around the same way on the 328 Nano for the Imu message, but did not succeed. Interestingly enough it didn't even work on a Mega that is setup to 512 by default. There were no error messages, but also nothing happened.", "Digging through the code, these sizes really are just the size of the used buffer. Serial data is sent byte-wise in a for loop. Judging from that, if 280 for in- and output buffer is OK, so should 320 for one and 240 for the other - regarding memory-consumption.\nThis is something that I do not understand, yet. It would be interesting to see if someone knows the reasons for these limits.", "Thanks a lot for your suggestions, I'll try them out later. Regarding your long messages: From looking at the Arduino HardwareSerial.cpp code (which seems to be used also when running rosserial) RX_BUFFER_SIZE is set to 128 by default. Have you tried playing around with that?", "It works with the <ArduinoHardware, 2, 2, 80, 105> setting you proposed. Thanks again!", "Thanks, I didn't look at the arduino libraries yet. I can now report that playing around with the serial buffer size in hardwareserial.cpp also doesn't work.", " ", " ", "I just installed ROS for the first time and connected to an Arduino 2560. The first couple of times I tried it I got the exact same result. I restarted and tried a few more times and with no changes it finally started working. Curious myself. ", "Once it did work I got a message after the 57600 baud comment that I need to start the publisher. ", " ", " ", "Yes, rosserial is still very much a work in progress. I have yet to get it working on ", " yet.", " ", " ", "Have you tried a smaller baud rate?", "Yes, but to no avail (tried 9600). I'll edit my OP with some new insights."], "question_code": ["Binary sketch size: 8528 bytes (of a 14336 byte maximum)\n", "stefan@SKdell:~/rosext/rosserial/rosserial_python/nodes$ rosrun rosserial_python serial_node.py /dev/ttyUSB0\n[INFO] [WallTime: 1330634524.911609] ROS Serial Python Node\n[INFO] [WallTime: 1330634524.913857] Connected on /dev/ttyUSB0 at 57600 baud\n[ERROR] [WallTime: 1330634539.916433] Lost sync with device, restarting...\n[ERROR] [WallTime: 1330634554.926519] Lost sync with device, restarting...\n[ERROR] [WallTime: 1330634569.928740] Lost sync with device, restarting...\n[ERROR] [WallTime: 1330634584.938269] Lost sync with device, restarting...\n[ERROR] [WallTime: 1330634599.943257] Lost sync with device, restarting...\n[ERROR] [WallTime: 1330634614.953234] Lost sync with device, restarting...\n", "#include <ros.h>\n#include <std_msgs/Empty.h>\n\nros::NodeHandle  nh;\n\nvoid messageCb( const std_msgs::Empty& toggle_msg){\n  digitalWrite(13, HIGH-digitalRead(13));   // blink the led\n}\n\nros::Subscriber<std_msgs::Empty> sub(\"toggle_led\", &messageCb );\n\nvoid setup() {                \n  // initialize the digital pin as an output.\n  // Pin 13 has an LED connected on most Arduino boards:\n  pinMode(13, OUTPUT);     \n  nh.initNode();\n  nh.subscribe(sub);\n}\n\nvoid loop() {\n//  nh.spinOnce();  // <-- As soon as this is commented in, blinking stops\n  digitalWrite(13, HIGH);   // set the LED on\n  delay(1000);              // wait for a second\n  digitalWrite(13, LOW);    // set the LED off\n  delay(1000);              // wait for a second\n}\n", "#include <ros.h>\n#include <std_msgs/Empty.h>\n\nros::NodeHandle  nh;\n\nvoid messageCb( const std_msgs::Empty& toggle_msg){\n  digitalWrite(13, HIGH-digitalRead(13));   // blink the led\n}\n\nros::Subscriber<std_msgs::Empty> sub(\"toggle_led\", &messageCb );\n\nvoid setup() {                \n  pinMode(13, OUTPUT);     \n  nh.initNode();\n}\n\nvoid loop() {\n  //nh.spinOnce();\n\n  if (Serial.available() > 0) {\n      // get ..."], "answer_code": ["ros::NodeHandle_<ArduinoHardware, 2, 2, 80, 105> nh;"], "url": "https://answers.ros.org/question/28890/using-rosserial-for-a-atmega168arduino-based-motorcontroller/"},
{"title": "rosbridge image (uri-data)", "time": "2012-05-21 05:20:14 -0600", "post_content": [" ", " ", "Hello,\nI try to deal with a topic which type is sensor_msgs/Image. I want to display those messages in a webpage using rosbridge. As it's written ", "\nthe json obejct contain an uri field.\nI created a canva and then drawed an image object containing this \"uri field value\" as source.\nI succeeded to connect to the topic because my handler is activated but i cant display the picture. Somebody already did something similar?", "Benoit"], "answer": [" ", " ", "Have you considered trying to use mjpeg_sever to stream images to your web page? This package takes advantage of browser optimizations by streaming ROS images in binary form. Take a look at the following wiki page which explains how to get it up and running:", "I second this recommendation for general in-page image use. Treating images special has been deprecated (and thanks to you removed from the documentation). If you really want to still handle images \"purely\" with JS and rosbridge, you should look at wsview.js in the scripts directory.", "Hello, \nTjay, I looked your script. Just to clear my mind, your img argument in the display function is the json_object received by a rosbridge handler?\nI found a code handling the mono8 encoding; \n/trunk/experimental/ROSDojo", "The object the display method expects is indeed the \"raw\" JSON object. Also, make sure to take a look at the rgb8 version. It's in the scripts directory of rosbridge itself.", "OK It works !!!! I will try the mpeg_server because this solution is quite power consuming. (50-60% of the CPU if i ask for a topic at 200Hz).  ", "Oh yeah. It's pretty terrible. Again, I recommend mjpeg_server unless you can get away with a tiny low framerate stream. I've had good luck with a 160x120 stream at 8HZ that I use the wsview.js script to upsize. It'll still hog your CPU more than a bit though.", "My picture are 7752*480 published at 30 Hz. Both strategy works and own quite high CPU consuming.(50 %)\nThanks you for your advices :)"], "url": "https://answers.ros.org/question/34462/rosbridge-image-uri-data/"},
{"title": "gazebo-1 has died; rosmake hector_quadrotor error", "time": "2012-05-23 12:06:37 -0600", "post_content": [" ", " ", "Hi everyone! I'm fresh from ROScon and am trying to impiment hector_quadrotor on 12.04 with fuerte. I am running into a problem with the first demo. What I've done:", "1.I took down every part of the stack to make sure that I didn't miss anything.", "2.I then attempted to build the stack"], "answer": [" ", " ", " ", " ", "Hi Parker, we did not make the switch to fuerte yet and are working on setting everything up so both distributions are properly supported. The main/most time consuming problem is the fact that all plugins for quadrotor simulation have to be changed to use the new Gazebo 1.0 plugin API. I'll update this posting with a approximate time estimate of fuerte compatibilty when I talked to all people involved later today.\nThe bottomline is: You have to use electric for the moment for quadrotor simulation, this should work fine.", "/edit: Unfortunately, it will probably be a week till we get to work on porting the stack to fuerte.", "Thank you for the quick response! Unfortunately that means I'll have to get rid of 12.04 also....", "You're right, that's of course quite a 'non-optimal' situation. We have two deadlines coming up next week, so it will take some more time till we get around to updating the stack.", "roscpp was messing in manifest.xml for quad_rotor teleop (just trying to pitch in... this package was why I upgraded to fuerte.. )", " ", " ", "Hello, Hope I am in correct post. I need to control a quadrotor in Gazebo. Is posible to use the new hector_quadrotor without ROS? I mean only with gazebo 1.x.x.\nthanks!\nFranco"], "question_code": [" svn co http://tu-darmstadt-ros-pkg.googlecode.com/svn/trunk\n", "parcon@PC-MBA:~/ros_workspace/trunk$ rosmake hector_quadrotor\n[ rosmake ] rosmake starting...                                                                                             \n[ rosmake ] Packages requested are: ['hector_quadrotor']                                                                    \n[ rosmake ] Logging to directory /home/parcon/.ros/rosmake/rosmake_output-20120523-155917                                   \n[ rosmake ] Expanded args ['hector_quadrotor'] to:\n['hector_quadrotor_gazebo', 'hector_quadrotor_teleop', 'hector_quadrotor_gazebo_plugins', 'hector_quadrotor_demo', 'hector_quadrotor_urdf']\n[rosmake-0] Starting >>> roslang [ make ]                                                                                   \n[rosmake-1] Starting >>> common_rosdeps [ make ]                                                                            \n[rosmake-2] Starting >>> std_msgs [ make ]                                                                                  \n[rosmake-1] Finished <<< common_rosdeps ROS_NOBUILD in package common_rosdeps                                               \n[rosmake-1] Starting >>> ogre [ make ]                                                                                      \n[rosmake-3] Starting >>> geometry_msgs [ make ]                                                                             \n[rosmake-0] Finished <<< roslang ROS_NOBUILD in package roslang\n No Makefile in package roslang                             \n[rosmake-0] Starting >>> roscpp [ make ]                                                                                    \n[rosmake-2] Finished <<< std_msgs ROS_NOBUILD in package std_msgs\n No Makefile in package std_msgs                          \n[rosmake-2] Starting >>> roslib [ make ]                                                                                    \n[rosmake-3] Finished <<< geometry_msgs ROS_NOBUILD in package geometry_msgs\n No Makefile in package geometry_msgs           \n[rosmake-1] Finished <<< ogre ROS_NOBUILD in package ogre                                                                   \n[rosmake-3] Starting >>> sensor_msgs [ make ]                                                                               \n[rosmake-2] Finished <<< roslib ROS_NOBUILD in package roslib[ sensor_msgs: 0.0 sec ] [ bullet... [ 4 Active 5/67 Complete ]\n No Makefile in package roslib                                \n[rosmake-1] Starting >>> bullet [ make ]                                                                                    \n[rosmake-2] Starting >>> colladadom [ make ]                                                                                \n[rosmake-0] Finished <<< roscpp ROS_NOBUILD in package roscpp\n No Makefile in package roscpp                                \n[rosmake-0] Starting >>> urdf_interface [ make ]                                                                            \n[rosmake-1] Finished <<< bullet ROS_NOBUILD in package bullet                                                               \n[rosmake-1] Starting >>> std_srvs [ make ]                                                                                  \n[rosmake-3] Finished <<< sensor_msgs ROS_NOBUILD in package sensor_msgs\n No Makefile in package sensor_msgs                 \n[rosmake-2] Finished <<< colladadom ROS_NOBUILD in package colladadom                                                       \n[rosmake-2] Starting >>> rosconsole [ make ]                                                                                \n[rosmake-3] Starting >>> gazebo_msgs [ make ]                                                                               \n[rosmake-0] Finished <<< urdf_interface ROS_NOBUILD in package urdf_interface                                               \n[rosmake-0] Starting >>> urdf_parser [ make ]                                                                               \n[rosmake-1] Finished <<< std_srvs ROS_NOBUILD in package std_srvs\n No Makefile in package std_srvs                          \n[rosmake-1] Starting >>> collada_parser [ make ]                                                                            \n[rosmake-3] Finished <<< gazebo_msgs ROS_NOBUILD in package gazebo_msgs                                                     \n[rosmake-2] Finished <<< rosconsole ROS_NOBUILD in package rosconsole\n No Makefile in package rosconsole                    \n[rosmake-2] Starting >>> angles [ make ]                                                                                    \n[rosmake-3] Starting >>> rospy [ make ]                                                                                     \n[rosmake-0] Finished <<< urdf_parser ROS_NOBUILD in package urdf_parser                                                     \n[rosmake-1] Finished <<< collada_parser ROS_NOBUILD in package collada_parser                                               \n[rosmake-1] Starting >>> rostest [ make ]                                                                                   \n[rosmake-2] Finished <<< angles ROS_NOBUILD in package angles                                                               \n[rosmake-3] Finished <<< rospy ROS_NOBUILD in package rospy\n No Makefile in package rospy                                   \n[rosmake-0] Starting >>> urdf [ make ]                                                                                      \n[rosmake-2] Starting >>> roswtf [ make ]                                                                                    \n[rosmake-1] Finished <<< rostest ROS_NOBUILD in package rostest\n No Makefile in package rostest                             \n[rosmake-3] Starting >>> message_filters [ make ]                                                                           \n[rosmake-1] Starting >>> rosgraph_msgs [ make ]                                                                             \n[rosmake-3] Finished <<< message_filters ROS_NOBUILD in package message_filters\n No Makefile in package message_filters     \n[rosmake-3] Starting >>> rosservice [ make ]                                                                                \n[rosmake-2] Finished <<< roswtf ROS_NOBUILD in package roswtf\n No Makefile in package roswtf                                \n[rosmake-1] Finished <<< rosgraph_msgs ROS_NOBUILD in package rosgraph_msgs\n No Makefile in package rosgraph_msgs           \n[rosmake-2] Starting >>> tf [ make ]                                                                                        \n[rosmake-1] Starting >>> rosbuild [ make ]                                                                                  \n[rosmake-0] Finished <<< urdf ROS_NOBUILD in package urdf                                                                   \n[rosmake-1] Finished <<< rosbuild ROS_NOBUILD in package rosbuild\n No Makefile in package rosbuild                          \n[rosmake-1] Starting >>> protobuf [ make ]                                                                                  \n[rosmake-0] Starting >>> nav_msgs [ make ]                                                                                  \n[rosmake-3] Finished <<< rosservice ROS_NOBUILD in package rosservice\n No Makefile in package rosservice                    \n[rosmake-3] Starting >>> dynamic_reconfigure [ make ]                                                                       \n[rosmake-2] Finished <<< tf ROS_NOBUILD in package tf                                                                       \n[rosmake-0] Finished <<< nav_msgs ROS_NOBUILD in package nav_msgs\n No Makefile in package nav_msgs                          \n[rosmake-2] Starting >>> mk [ make ]                                                                                        \n[rosmake-3] Finished ..."], "url": "https://answers.ros.org/question/34704/gazebo-1-has-died-rosmake-hector_quadrotor-error/"},
{"title": "Problems expected if logs deleted during process?", "time": "2012-07-02 07:00:45 -0600", "post_content": [" ", " ", " ", " ", "What would be the expected outcome of deleting logs while a process is running?", "Specifically which of the following categories would that fall under : ) ?", "Our logs are growing too big because of a bug in diamondback where roscpp_internal does not respect log levels. As a work around we have a script that deletes particular log files ever 30 seconds or so. We've also seen a crash (assert/-6) that we've yet to diagnose closely following our log workaround change, and we currently suspect our log deletions as the cause.", "Maybe it's a silly question but, could you upgrade to Electric or Fuerte to fix that bug? :)", "Unfortunately the system where this is occurring is a production machine which is locked to diamondback for the time being.", "Not silly, but I think we do still need to be able to build, test and release Diamondback fixes.", "If you need help with debugging this assertion, could you please try to to send more information about what happens? (a backtrace would be a good start)", "Thanks for the offer. Because of the nature of the situation (remote machine not internet connected) we're going treat-first-diagnose-later. I'll update if we find anything interesting."], "answer": [" ", " ", " ", " ", "Actually what you try to do just will not help. If you get a look at ", " where ", " is the process id of your ROS node, you will be able to see the currently opened files and in particular the log files you thought were deleted.", "Don't forget that file descriptors are a mechanism of reference counting on a resource, so as long as rosconsole maintains the file opened, the file will, in fact, ", " be deleted.", "This explains why what you do is perfectly safe on Linux :)", "So maybe this assertion is just due to you consuming the whole disk space and making the process crash or something? Monitor your disk space, you will see it decreasing until your process crashes, the file descriptor gets released and the kernel cleans the space keps by the previous log files...", "So I think it would be better to try to really fix the error instead of trying to workaround it if possible ;)", " is right, that is how POSIX filesystems work. The space will not be recovered until all references to the inode are gone.", "Duly noted regarding file deletions while the fd is still held. ", " ", " ", "I would expect it to work OK in terms of system stability. ", "But, the status of future log messages from that run might be undefined (e.g. they might get lost).", "What do you expect? What actually happens?", "Well we had a crash(assert/-6) in a system where we had been deleting the logs. We don't have enough info to do a postmortem, but the current assumption is that it was caused by our log deletion. I'll add some of this info to the original question.", "Seems plausible. Maybe construct a small test case that logs continuously, while some other process periodically deletes the log?", "Hopefully we'll get a chance to do some more tests, but it may instead end up being a treat-first-diagnose-later sort of situation do to time/budget constraints."], "question_details": [" ", " ", " ", " ", "Fine", "Undefined", "Very Bad"], "answer_code": ["/proc/XXX/fd", "XXX"], "url": "https://answers.ros.org/question/37717/problems-expected-if-logs-deleted-during-process/"},
{"title": "When to use param and rosparam on launch file?", "time": "2012-07-04 03:06:03 -0600", "post_content": [" ", " ", "For param example:", "For rosparam example:", "What's their difference?", "What's the case of using them?", "Thank you~"], "answer": [" ", " ", "I believe that the main difference is that ", " may be used to set a single command on the ROS parameter server, while ", " can be used to evaluate groups of parameters.", "So, in the cases above, ", " is setting a ", " parameter, and that parameter is an entire URDF file.  The ", " tag is reading in the ", " from within the launch file.", "The value set by the ", " tag may only be a ", ", or ", ", which may be set through the xml attribute ", ", or by reading in from a text file, bin file, or the output of a command line command.", "The value set by the ", " tag is most commonly a batch of related parameters, read in from a YAML file (or from the output of ", ").  You can think of it as a programmatic way to access the functionality of ", " from a launch file.  A good example of this in practice is the ", " related nodes, where there are many, many parameters to be set, it's less cumbersome to just use the YAML format.", "For more information on all of these topics, I would consult the following:", "This information has been added here: ", " (feel free to edit)"], "question_code": [" <launch>\n   <!-- send table urdf to param server -->\n   <param name=\"table_description\" command=\"$(find      xacro)/xacro.py $(find gazebo_worlds)/objects/table.urdf.xacro\" />\n\n   <!-- push table_description to factory and spawn robot in      gazebo -->\n   <node name=\"spawn_table\" pkg=\"gazebo\" type=\"spawn_model\"      args=\"-urdf -param table_description -z 0.01 -model table_model\"      respawn=\"false\" output=\"screen\" />\n </launch>\n", " <launch>\n   <node name=\"fake_localizaton\" type=\"fake_localization\"      pkg=\"fake_localization\">\n     <rosparam>\n       odom_frame_id: odom_combined\n       base_frame_id: base_footprint\n     </rosparam>\n   </node>\n </launch>\n"], "answer_code": ["<param>", "<rosparam>", "<param>", "<rosparam>", "<param>", "string, int, bool", "double", "value", "<rosparam>", "move_base"], "url": "https://answers.ros.org/question/37916/when-to-use-param-and-rosparam-on-launch-file/"},
{"title": "kinect's the world coordinate", "time": "2012-07-12 03:48:44 -0600", "post_content": [" ", " ", "kinect 3D point has the world coordinate (x,y,z).", "where is the the origin of the world coordinates(0,0,0)?", "I don't understanding your comment. Could you teach me again in detail ?"], "answer": [" ", " ", " ", " ", "The ", " in the ", " header names the relevant ", " frame of reference.", "ROS handles transforms between coordinate frames using the ", " package. That component is complex, not easily summarized here. Please consult the ", " and the ", " for more information."], "answer_code": ["frame_id", "PointCloud2", "tf"], "url": "https://answers.ros.org/question/38646/kinects-the-world-coordinate/"},
{"title": "Use a GPL package in a proprietary licensed software", "time": "2012-09-05 03:30:01 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "if I need to include nodes from a GPL licensed package (e.g. ar_pose) in my application's launch file, can my software have a proprietary license and be commercialized? I'm not linking ar_pose to my own code, I'm just subscribing to its published topics. And if I make some changes to ar_pose, I would just have to publish ar_pose with my modifications, right?", "Thanks in advance!"], "answer": [" ", " ", "IANAL, but from my understanding it's not that simple and seems to be pretty open for interpretation (which is why I try to stay away from GPL code as much as possible).  ", "The official FAQ is incredibly vague:\n", "The two programs have to remain well separated for one to use a GPL license and one to use a proprietary one.  From my understanding this is why connecting to a GPL web server from a proprietary browser is ok (they are distinctly separate programs and the communications between them is common and generic).  I don't think however you can write a wrapper for a GPL library that just translates the function calls into TCP messages and communicate with it from a proprietary program.  They've essentially become one system at that point even though they don't actually link against one another.  I'm not sure where ROS falls on that spectrum.  ", "There was another good discussion on the topic here:\n", "/\nand a good, but equally vague/consult your lawyer for a final answer explanation here:\n", "Given the thread you linked, it sound to me like a \"it depends on why the node is there\".", " ", " ", " ", " ", "IANAL, but I think normally these issues need to be dealt with on a case-by-case basis, and it's always better play on the safe side. It's also important to remember that if there's a chance that someone will take the time to sue you for violating copyright, it also means they should care enough to answer any licensing questions you may have.", "I imagine ar_pose's license restrictions come from ARToolKit itself, and on their ", ", they summarize the licensing terms by saying:", "i.e. your software incorporating or linking to ARToolKit must also be open-source software, licensed under the GPL.", "I think the above means that you cannot sell a product that uses the GPL-licensed ARToolKit, or at very least that they might pursue litigation if you do. However, there's good news, because they also have professional and commercial licenses:", "In response to demand from toolkit users, the holders of the copyright on much of the ARToolKit version 1.0 - 2.x code have elected to make this code and other substantially advanced ARToolKit and AR code available under proprietary licenses for professional and commercial use by persons for whom the GPL license is not ideal. These license arrangements are managed by ", ", Inc., Seattle, WA, USA. A variety of license types are available at reasonable cost. Please contact ARToolworks, Inc. directly for information on products, license terms, and pricing.", "If ARToolKit is the only GPL restriction in that package, then I would suggest:", "If there are other GPL components, then I would suggest looking for similar solutions.", " ", " ", " ", " ", "This is not a direct answer to your question, but a possible improvement and workaround.", "I have used both ", " (based on ", ") and ", " based on ", " for a project. In my experience the latter one is far better than the former in any terms. Porting your code to use ", " will likely take about a couple of hours. The stability, robustness and pose estimation accuracy of ", " library is not the only benefit you'll earn.  The ROS driver is BSD licensed and the original library is LGPL* which will make it more appropriate to be used in a commercial product.", "Thanks Mani, that seems to solve my problem!", "You are welcome. I am glad that helped.", " ", " ", "I am not a licensing expert in any way, but to my best knowledge the use case you describe is covered by the GPL. You would need to supply source code for you modified GPL code for free, but TCP/IP communications (ROS) with proprietary software should be OK.", "I'd be interested in answers that could confirm/refute this in a better verified way.", " ", " ", " ", " ", "Thanks all for the help! I guess I will have to contact the guys from ccny and ARToolkit directly for that then.", "Just for reference, ", " says this:\n\"  The ROS packaging and communication system allows for fine-grained licensing. Because nodes communicate via ROS messages, code from multiple nodes is not linked together. Thus the package provides a kind of \"license boundary.\"   \""], "answer_details": [" ", " ", " ", " ", "Contact the maintainers of the CCNY ROS packages to privately release a version of their ROS interfaces under a BSD or commercial license.", "Contact ", " to negotiate a commercial license.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["ar_pose", "ar_toolkit", "ar_track_alvar", "alvar", "ar_track_alvar", "alvar"], "url": "https://answers.ros.org/question/43061/use-a-gpl-package-in-a-proprietary-licensed-software/"},
{"title": "Exceptions not being caught using rosbridge version 1.0", "time": "2012-10-05 04:15:37 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have been developing an HTML5/Javascript GUI using version 1.0 of ", " and most functions are working perfectly.  However, I just noticed that my Javascript is not catching rosbridge exceptions where I assume they should occur.  For example, if I follow ", " and run:", "where neither host foobar nor the port 1234 is valid, I don't see the \"Failed to connect\" message.  Instead I see the \"Connection successful\" message.  Similar exceptions such as trying to connect to a non-existent service are not being caught.  Is this a Javascript problem or a rosbridge problem?", "Thanks!", "\npatrick"], "answer": [" ", " ", " ", " ", "I know this is not the answer you are expecting, but I recently migrated from rosbridge 1.0 to rosbridge 2.0 and I'm extremely pleased with the results.", "Rosbridge 2.0 is much more robust, consumes way less resources and has many other features. ", "If you already have a working websocket implementation, the migration shouldn't be hard at all.", "Thanks ", ".  Migrating to version 2.0 is probably the best answer so I'll give it a try.  I'll wait to see if anyone has a fix for version 1.0 before checking off your answer.", ": I'm finding the rosbridge 2.0 documentation very sparse and not sufficient for me to figure out how to translate my rosbridge 1.0 code to 2.0.  Do you know of any 2.0 examples out there that use the Javascript client in an HTML application?", "No, I am sorry. I implemented a client using Qt in C++. But I believe all you need is a websocket client and on top of that implement the protocol described in this document: ", " Did you eventually migrate to Rosbridge 2.0? Can you show the process of your migration?"], "question_code": ["var connection = null;\ntry {\n    connection = new ros.Connection(\"ws://foobar:1234\");\n} catch (err) {\n    log('Failed to connect to rosbridge!');\n    return;\n}\nlog('Connection successful!');\n"], "url": "https://answers.ros.org/question/45182/exceptions-not-being-caught-using-rosbridge-version-10/"},
{"title": "Can I create a new parameter for video_mode of camera1394", "time": "2012-12-05 05:55:13 -0600", "post_content": [" ", " ", "Currently, I have my firefly mv usb camera working with camera1394 package. The problem is that the maximum resolution of my camera is 1328x1048, but there is no parameter for 1328x1048 in video_mode. I am wondering can I create or modify a mode to enable 1328x1048 resolution works?\nThanks in advance."], "answer": [" ", " ", " ", " ", "The IIDC 1394 standard defines the available modes. Each camera provides some subset of those standard settings. So, no: you can't define your own video mode.", "However, the standard also provides Format 7, which allows the camera to provide additional non-standard resolutions. Please consult your device-specific specifications to see if it provides a Format 7 option that supports the full resolution. Most do, often using Format 7 Mode 0.", ":", "Because Format 7 behavior is not specified by the standard, individual devices vary widely. Most cameras I have tested display full device resolution, generally when using Format 7 Mode 0.", "The driver provides ", ". You can experiment with them, but the default values normally provide the behavior you want. You can also try Format 7 Mode 1, Mode 2, etc.", "To experiment with those parameters while the driver is running, use the ", ". Unfortunately, many cameras behave badly in Format 7 mode when given unexpected parameter values. If the device hangs while you are experimenting with those parameters, you may need to ", " to get it working again.", "If that does not work, run coriander on the device. See what parameters it supports. Then set the equivalent values in camera1394.", "Hi joq, I remember Format 7 Mode 0 works for my camera in the morning. But the problem is that the resolution still keeps low. How can I define the resolution of Format 7 Mode 0 in camera1394 package?", "Thanks, I got it works.", "Good. For the record: what modes and parameters did it take?", "I just use format7_mode0 and change the encoding method to raw8."], "url": "https://answers.ros.org/question/49794/can-i-create-a-new-parameter-for-video_mode-of-camera1394/"},
{"title": "Create Semantic Maps using RGBDSLAM + OctoMaps?", "time": "2012-07-01 02:52:17 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone!", "I'm interested in using RGBDSLAM and OctoMaps with the Kinect camera to create 3D maps. Using generated 3D maps and combining it with object recognition code I've written using OpenCV, I would like to create semantic maps of the environment. For clarity, when I say semantic maps, I just mean having maps are able to highlight or pinpoint recognized objects in the environment.", "I hope to do this in 'real-time' - it would be nice if the final product was able to update at a rate of at least 5Hz on my dual-core laptop. ", "I'm new to ROS/ RGBDSLAM/ OctoMaps so I was wondering whether anyone can provide feedback or suggestions? Is there anything I should watch out for? Are there any easier ways of going about this? Is this in fact possible!?", "As far as I can tell from my research, this is the best way to go about this. ", "I'm also open to suggestion for 2D maps as well, although would prefer 3D at this stage.", "Thanks!!!", "I have also posted a similar but slightly more generalized question to this one at "], "answer": [" ", " ", "Regarding the integration of semantic information, here's my thoughts:", "The easiest way (maybe not the most efficient though) to do this, is should be to have your recognition software subscribe to the rgb image and point-cloud, do recognition, recolor the ", ", e.g. red points for class 1, green for class 2, black points for unclassified. then send the rgb image and the recolored cloud to the topics rgbdslam listens to (you can modify them via parameters). Then the point cloud colors will be integrated into the map and can be seen as labels.\nTo make things efficient, you would integrate your software into rgbdslam and call it from the callback methods in openni_listener.cpp (e.g. kinect_callback, noCloudCallback).", "A cleaner way is to use a point cloud type that contains semantic information. Therefore you would need to redefine the point cloud type in rgbdslam (src/parameter_server.cpp) to one that contains your semantic information. Note that, if you omit the color (i.e. the point.rgb field), there will be errors in compiling glviewer.cpp. These should be easily solvable though.", "Then you need to adapt the octomap server to use a voxel leaf that stores semantic information. This has been done, but I don't know how.", " ", " ", "Using Multiresolution Surfel Maps for RGB-D-SLAM \nSee code.google.com/p/mrsmap\nand Random Forests for Object-class segmentation of individual views, Nenad Biresev fused this semantic information to create 3D semantic maps.", "See: \nJ\u00f6rg St\u00fcckler, Nenad Biresev, and Sven Behnke:\n Semantic Mapping Using Object-Class Segmentation of RGB-D Images\n In Proceedings of IEEE/RSJ International Conference on Robots and Systems (IROS), Vilamoura, Portugal, October 2012.\nSee www.ais.uni-bonn.de/papers/IROS_2012_Semantic_Mapping.pdf", " ", " ", "5hz on a dual core laptop might be hard to get. You'll need a GPU, and use SIFTGPU features otherwise detection & extraction of SURF will slow you down too much (~2hz). ORB might be an alternative, but I found them to be less accurate. With two cores, you will definitely need to reduce loop closure search (the ..._candidates settings, see parameter_server.cpp).", "Setting the openni camera driver to QVGA and lower FPS (use dynamic reconfigure) will reduce the cpu load of the driver.", "Producing Octomaps from online-mapping consumes a lot of RAM, because you need to store the clouds. Reducing resolution will also help here.", "Hi Felix, you've highlight some good points on improving the operation of RGBDSLAM but I was more interested in whether it is possible to highlight some pointcloud data which shows an object which has been recognised. Can the RGBDSLAM code be easily modified to do this if it can't already? Cheers."], "url": "https://answers.ros.org/question/37595/create-semantic-maps-using-rgbdslam-octomaps/"},
{"title": "Specs To Run RGBD-6D-SLAM?", "time": "2011-03-09 20:04:04 -0600", "post_content": [" ", " ", " ", " ", "I've successfully installed the packages, but when I try executing it, my computer lags/freezes when trying to run the code. The computer I've ran it on has a 2.4ghz dual core processor and 2gb of memory. ", "For those who have had success running the tool smoothly, what were the specs of the computer that you ran it on? Specifically the amount of memory and CPU speed?"], "answer": [" ", " ", " ", " ", "2 GB is a bit short, I guess. 10 minutes sounds extreme though. There's a debug message (actually a WARN-level message) in the publish-method, so before sending the whole model, start up rxconsole and check whether you get messages like \"Sending out a cloud with id X on topic XXX\" during sending of the model. Do you get these?", "Memory-wise, the best you can do is to restart rviz before sending the whole model, so the memory assigned to the individually sent point clouds is definitely freed. You could also try to record the whole with \"rosbag record /rgbdslam/batch_clouds /tf\", so you need no rviz in memory, and when you play it back later on you don't need rgbdslam to be running.", "I will soon (April?) release an improved version that, among other improvements, uses less memory.", " ", " ", "The spec you state should suffice. I ran it on a quad-core 2Ghz with 4GB. The memory consumption is about linear with the runtime, but you should be fine for several minutes.", "Do you get any results, or does it freeze/lag (which one?) directly?", "Felix", " ", " ", "Have the same problem, (2.4ghz dual core2 processor and 2gb of memory), get to see the stitched 3D in rviz, but cannot send whole model... never finishes. How can I extract the information? where does it get saved? ", " ", " ", " ", " ", "rgbdslam never freezes (lags very little). However, when I start recording data, rviz lags to the point where I can't navigate through it and rviz essentially freezes. ", "After I try to save the world model in rgbdslam, it seems to take a very long time to save the data. I've ever seen it complete the save, and I've waited for upwards to 10+ minutes.", "Am I not waiting long enough for it to save? I purposely pause within a few seconds after starting the recording in hopes to collect a small bit of data which should save in a reasonable amount of time.", "Edit: I apologize, but the computer I'm using actually has 2gb of memory (not 4 like originally stated).", " ", " ", " ", " ", "There is now also the ", " available,\nwhich performs dense RGB-D-SLAM on a CPU. This ", " provides the details."], "url": "https://answers.ros.org/question/9364/specs-to-run-rgbd-6d-slam/"},
{"title": "Unmet dependencies for Groovy on Ubuntu 12.10", "time": "2012-12-31 21:02:54 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I have a problem with Groovy install on Ubuntu 12.10. When I type ", "I get :", "Please make sure you have done an apt-get update recently.  And then try to narrow down exactly which package is failng.  Apt is bad in that it just tells you something is failing, not which specific package is failing.  ", "Thanks, but I did the apt-get update. What is a good way to do so ?"], "answer": [" ", " ", "I did install it using aptitude, by installing / downgrading the dependencies one by one.", "could you share more details on the use of aptitude? is it a time consuming work? should I be better downgrade my Linux to Precise? Thank you!", "You don't need to downgrade you Ubuntu to precise. 13.04 should work fine. \nAptitude would give you more details on the required packages (and their versions). So you can install the package if it is missing, or downgrade to the required version by typing:  ", " ", " ", "You should try ", ". That will force all packages to be updated and might ask you to remove packages which dependencies are not met anymore.", " ", " ", " ", " ", "Make sure you have the additional repositories for ", " and ", ". Here is the relevant part from my ", " (I can't post links yet due to insufficient karma):", "A simple", "should install everything.", " ", " ", " ", " ", "I had the same problem with a fresh installed system. ", "Tracing many unmet dependencies I ended up installing ", " (and friends).", "After that the install of ros-groovy-desktop is running without problems.", "Have not tried -full, but I trust that will work also."], "question_code": ["sudo apt-get install ros-groovy-desktop-full\n", "Some packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n ros-groovy-desktop-full : Depends: ros-groovy-ros-tutorials (= 0.3.9-0quantal-20121230-1232-+0000) but it is not going to be installed\n                           Depends: ros-groovy-nodelet-core (= 1.7.13-0quantal-20121230-1911-+0000) but it is not going to be installed\n                           Depends: ros-groovy-navigation-tutorials (= 0.1.1-s1356945732~quantal) but it is not going to be installed\n                           Depends: ros-groovy-geometry-tutorials (= 0.1.3-s1356894174~quantal) but it is not going to be installed\n                           Depends: ros-groovy-orocos-kinematics-dynamics (= 0.2.3-s1356865696~quantal) but it is not going to be installed\n                           Depends: ros-groovy-filters (= 1.6.0-s1356894312~quantal) but it is not going to be installed\n                           Depends: ros-groovy-slam-gmapping (= 1.2.7-s1356944956~quantal) but it is not going to be installed\n                           Depends: ros-groovy-ros (= 1.9.39-0quantal-20121230-1040-+0000) but it is not going to be installed\n                           Depends: ros-groovy-qt-gui-core (= 0.2.9-0quantal-20121230-1144-+0000) but it is not going to be installed\n                           Depends: ros-groovy-vision-opencv (= 1.9.14-0quantal-20121230-2323-+0000) but it is not going to be installed\n                           Depends: ros-groovy-perception-pcl (= 1.0.24-0quantal-20121230-1955-+0000) but it is not going to be installed\n                           Depends: ros-groovy-rqt-common-plugins (= 0.2.7-0quantal-20121231-2005-+0000) but it is not going to be installed\n                           Depends: ros-groovy-actionlib (= 1.9.11-0quantal-20121230-1206-+0000) but it is not going to be installed\n                           Depends: ros-groovy-pluginlib (= 1.9.17-0quantal-20121230-1110-+0000) but it is not going to be installed\n                           Depends: ros-groovy-image-pipeline (= 1.9.11-0quantal-20121230-2354-+0000) but it is not going to be installed\n                           Depends: ros-groovy-bfl (= 0.1.0-s1356865809~quantal) but it is not going to be installed\n                           Depends: ros-groovy-stage (= 1.6.7-s1356894745~quantal) but it is not going to be installed\n                           Depends: ros-groovy-rqt-robot-plugins (= 0.2.7-0quantal-20121231-2000-+0000) but it is not going to be installed\n                           Depends: ros-groovy-image-common (= 1.9.22-0quantal-20121230-1342-+0000) but it is not going to be installed\n                           Depends: ros-groovy-bond-core (= 1.7.9-0quantal-20121230-1142-+0000) but it is not going to be installed\n                           Depends: ros-groovy-common-tutorials (= 0.2.3-s1356896830~quantal) but it is not going to be installed\n                           Depends: ros-groovy-ros-comm (= 1.9.39-0quantal-20121230-1801-+0000) but it is not going to be installed\n                           Depends: ros-groovy-physics-ode (= 1.8.0-s1356864560~quantal) but it is not going to be installed\n                           Depends: ros-groovy-driver-common (= 1.6.4-0quantal-20121230-1912-+0000) but it is not going to be installed\n                           Depends: ros-groovy-executive-smach (= 1.2.0-s1356894288~quantal) but it is not going to be installed\n                           Depends: ros-groovy-common-msgs (= 1.9.11-0quantal-20121225-1907-+0000) but it is not going to be installed\n                           Depends: ros-groovy-dynamic-reconfigure (= 1.5.29-0quantal-20121230-1449-+0000) but it is not going to be installed\n                           Depends: ros-groovy-image-transport-plugins (= 1.8.15-0quantal-20121230-2344-+0000) but it is not going to be installed\n                           Depends: ros-groovy-xacro (= 1.6.1-s1356894606~quantal) but it is not going to be installed\n                           Depends: ros-groovy-robot-model (= 1.9.32-0quantal-20121230-1900-+0000) but it is not going to be installed\n                           Depends: ros-groovy-bullet (= 2.80-s1356864740 ..."], "answer_code": ["sudo aptitude install  pkg=version.", "sudo apt-get update && sudo apt-get dist-upgrade", "universe", "multiverse", "/etc/apt/sources.list", "deb http://yourUbuntuDebServer quantal main restricted universe multiverse\ndeb-src http://yourUbuntuDebServer quantal main restricted universe multiverse\n", "apt-get update\napt-get install ros-groovy-desktop-full\n"], "url": "https://answers.ros.org/question/51272/unmet-dependencies-for-groovy-on-ubuntu-1210/"},
{"title": "Producer-consumer problem in ROS", "time": "2013-01-23 22:08:47 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone,", "I have a newbie problem here. I am trying to implement a ROS Architecture which is similar to a producer-consumer problem. I have one node which is publishing images and a group of nodes that would take the image and do some processing in it. The behaviour that I want to implement is the following: If a consumer node takes one image, I do not want the others to receive that same image. Using the default image_transport publisher, I suppose that all the subscribers will receive the same image at a time. How can I avoid this?", "Thank you in advance "], "answer": [" ", " ", "I think using a service for this might be an easy solution. The producer node would not publish, but queue images and provide a service ", " that returns the next image or fails when empty.", "Consumer nodes call this service whenever they are ready to process the next image.", " ", " ", "You are right that all topic listeners will receive all images.\nYou will probably need to coordinate the consumers in a centralized way. So you'll need a centralized instance which makes sure each consumer knows which images he has to ignore. There are several ways in which such a centralized coordination can be achieved, depending on your exact setup and needs. E.g. is the number of consumers known beforehand and fixed, or does it change? Do you assume all consumers have the same processing time per image, or is that variable as well? Do you need the consumers to produce an ordered output of produced images, or are their work producets independent of each other?", "Depending on such factors, your scheduling strategy needs to be adapted.", "Note that you are free to create new topics for that purpose, and you can republish images if that's useful for you. So a central node could listen to the main topic and distribute images over individual topics. Or consumer nodes could be chained with topics, where each consumer either consumes the image, or publishes it to the next. Not saying either is a good solution for you, just making sure you see these possibilities as well."], "answer_code": ["consume_image"], "url": "https://answers.ros.org/question/53250/producer-consumer-problem-in-ros/"},
{"title": "hector_slam, read sensors over Ethernet?", "time": "2013-02-25 01:43:41 -0600", "post_content": [" ", " ", "I'm wondering if it is possible to read the Hokuyo and IMU data via Ethernet rather than through the USB ports?", "My current desire is to have the sensors attached to a raspberry pi/other interface and transmit the sensor data via wifi to the hector_slam host computer.", "I appreciate any thoughts here."], "answer": [" ", " ", "Sure, you just need to get them into an ethernet interface from somewhere. You obviously can't connect the USB ports to an ethernet cable (unless the sensor provides data via ethernet like the newest Hokuyo or some Sicks).", "The usual option would to use be mini-computers like a gumstix or your rapberry pi and get the laser data via USB from there to transmit it via ethernet. I'm not sure if you mean to transfer raw binary data via wifi and collect it there, which would require extra software.", "The easiest would be to run ROS on the embedded system and just run the ROS drivers from there. Then you can run everything like you would normally do.", " ", " ", " ", " ", "At a summer school last year we did exactly that, having a Raspberry Pi with a Hokuyo UTM-30LX connecting via WiFi to a computer that's hector_mapping. Worked surprisingly well (of course only as one has reliable wireless connectivity). Interestingly, the hokuyo_node took 50% CPU consumption on the Pi ;)"], "url": "https://answers.ros.org/question/56210/hector_slam-read-sensors-over-ethernet/"},
{"title": "command line arguments in launch file", "time": "2013-02-20 05:29:32 -0600", "post_content": [" ", " ", "Hi,", "I am using pcd_to_pointcloud from perception_pcl and want to remap the output using a launch file.\nThe problem is that it requires a pcd file as a command line argument which I do not get to work using a launch file. Is there an easy way or should I issue a pull request for a modified node which can also read a pcd file path from a parameter.", "Kai"], "answer": [" ", " ", "Did you try it? This shouldn't be a problem in general. Pass the .pcd as ", " in the ", " tag and do the remapping within a ", " tag in the ", ".", "If it doesn't work, something is wrong. The ROS args should be consumed by ", " and leave the .pcd as a param.", "I did try it out, but it did not work. I just tried again and it worked. It turns out I did not specify the full path for the pcd file. Thanks a lot"], "answer_code": ["args", "<node>", "<remap>", "<node>", "ros::init"], "url": "https://answers.ros.org/question/55793/command-line-arguments-in-launch-file/"},
{"title": "InteractiveMarker hook into mouseover/hover?", "time": "2013-02-24 13:30:55 -0600", "post_content": [" ", " ", " ", " ", "Is it possible to execute a callback for an interactive marker on mouseover? I can capture and click event and use it just fine, but I'd like to be able to capture a mouseover (or hover) event as well. The markers already highlight on mouseover, so I'm hoping it isn't difficult. Does anyone know how to do this? I'm on Ubuntu 12.04 with ROS Groovy."], "answer": [" ", " ", "That's not supported. What do you want to do with this?", "I'm using Rviz to make a GUI that allows people to hand label point cloud data. I take a point cloud and build a PCL octree, then, for each leaf in the octree, I draw an interactive marker. Right now I have written a panel for the user to select a label, and what I would like is for the user...", "...to be able to \"paint\" the markers with the selected label by just running the mouse cursor over the markers containing the point cloud points to be labelled. As it stands, I can click the markers individually, but that's time consuming. Would you have any suggestions?", "Maybe this (undocumented) tutorial gets closer to what you want: ", "Otherwise, you might want to look into writing an RViz plugin that provides it's own labeling tool that is similar to the 'Selection' tool.", "Ah, that looks like it might be promising. Thanks!"], "url": "https://answers.ros.org/question/56160/interactivemarker-hook-into-mouseoverhover/"},
{"title": "Which is the better ros distro for ubuntu 11.10", "time": "2013-03-05 13:41:45 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi, Which is the better ros distro (groovy/electric/fuerte) for ubuntu 11.10 for a biginner?"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "As Groovy is the latest release, i would go for that one under Ubuntu 11.10\nThe reasons are because many tools have been upgraded or updated.", "Please note that Groovy is the latest release, Fuerte is the previous one (and my second choice as it is a relatively stable and well-documented version) and electric is the oldest one you mentioned.", "For further info consult this link:\n", "+1 If you are just starting a project, it makes sense to use the most recent ROS distro that is supported on your platform, in this case Groovy. Note however, that Hydro will ", " support Ubuntu 11.10. You might want to upgrade to 12.04, Precise, instead."], "url": "https://answers.ros.org/question/57214/which-is-the-better-ros-distro-for-ubuntu-1110/"},
{"title": "Errors while installing ros-groovy on beablebone with ubuntu 12.10", "time": "2013-03-18 12:24:08 -0600", "post_content": [" ", " ", " ", " ", "I am trying hard to install ROS groovy from the source on a beaglebone board which has installed the Ubuntu 12.10 by following the the instructions on this page: http:// ros.org/wiki/groovy/Installation/Source", "Everything was working well until it got some errors with building the catkin Packages as cited below. I would therefore be grateful to receive some suggestions on how you propose to solve this problem.", "Regards,\nb4llb0t", "Partial output of the terminal (the complete terminal output is available at http:// pastebin.com/LbTPaXYU):", "...."], "answer": [" ", " ", " ", " ", "From looking at the full output, it looks like most packages downloaded successfully, and a few failed.", "Try running with -j1 instead of -j8; I suspect that your system can't handle the load of running 8 simultaneous downloads.", "You may also want to consult the ", " and ", " instructions, since your system is ARM, and most of the tweaks required to install on Raspberry Pi also apply to other ARM systems.", "I don't think what you suggested before was the main issue since I got the following output with ifconfig and ping http://pastebin.com/iFXHqdXP . In addition, if it helps, the complete terminal output is available at http://pastebin.com/LbTPaXYU", "The complete output is quite helpful; thanks! It looks like your internet connection is working, and only a few packages failed; probably due to high load somewhere.", "Thank you very much for the extra information. Apropos, it worked very well with $ wstool init src -j1 packages http:// packages.ros.org/web/rosinstall/generate/raw/groovy/ros_comm  "], "question_code": ["$ wstool init src -j8 packages http:// packages.ros.org/web/rosinstall/generate/raw/groovy/ros_comm\n", "ERROR [vcstools] Tarball download unpack failed: <urlopen error [Errno -2] Name or service not known>[/vcstools]\nERROR [vcstools] Tarball download unpack failed: <urlopen error [Errno -2] Name or service not known>[/vcstools]\nERROR [vcstools] Tarball download unpack failed: <urlopen error [Errno -2] Name or service not known>[/vcstools]\nException caught during install: Error processing 'rosconsole' : [rosconsole] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/rosconsole/1.9.41.tar.gz version ros_comm-release-release-rosconsole-1.9.41 into /home/ubuntu/ros_catkin_ws/src/rosconsole failed.\nError processing 'catkin' : [catkin] Checkout of https:// github.com/ros-gbp/catkin-release/archive/release/catkin/0.5.63.tar.gz version catkin-release-release-catkin-0.5.63 into /home/ubuntu/ros_catkin_ws/src/catkin failed.\nError processing 'genmsg' : [genmsg] Checkout of https:// github.com/ros-gbp/genmsg-release/archive/release/genmsg/0.4.17.tar.gz version genmsg-release-release-genmsg-0.4.17 into /home/ubuntu/ros_catkin_ws/src/genmsg failed.\nError processing 'rosbash' : [rosbash] Checkout of https:// github.com/ros-gbp/ros-release/archive/release/rosbash/1.9.42.tar.gz version ros-release-release-rosbash-1.9.42 into /home/ubuntu/ros_catkin_ws/src/rosbash failed.\nError processing 'roslib' : [roslib] Checkout of https:// github.com/ros-gbp/ros-release/archive/release/roslib/1.9.42.tar.gz version ros-release-release-roslib-1.9.42 into /home/ubuntu/ros_catkin_ws/src/roslib failed.\nError processing 'roscreate' : [roscreate] Checkout of https:// github.com/ros-gbp/ros-release/archive/release/roscreate/1.9.42.tar.gz version ros-release-release-roscreate-1.9.42 into /home/ubuntu/ros_catkin_ws/src/roscreate failed.\nError processing 'roslang' : [roslang] Checkout of https:// github.com/ros-gbp/ros-release/archive/release/roslang/1.9.42.tar.gz version ros-release-release-roslang-1.9.42 into /home/ubuntu/ros_catkin_ws/src/roslang failed.\nError processing 'roscpp' : [roscpp] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/roscpp/1.9.41.tar.gz version ros_comm-release-release-roscpp-1.9.41 into /home/ubuntu/ros_catkin_ws/src/roscpp failed.\nError processing 'topic_tools' : [topic_tools] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/topic_tools/1.9.41.tar.gz version ros_comm-release-release-topic_tools-1.9.41 into /home/ubuntu/ros_catkin_ws/src/topic_tools failed.\nError processing 'rostest' : [rostest] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/rostest/1.9.41.tar.gz version ros_comm-release-release-rostest-1.9.41 into /home/ubuntu/ros_catkin_ws/src/rostest failed.\nError processing 'rosbag' : [rosbag] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/rosbag/1.9.41.tar.gz version ros_comm-release-release-rosbag-1.9.41 into /home/ubuntu/ros_catkin_ws/src/rosbag failed.\nError processing 'rosmsg' : [rosmsg] Checkout of https:// github.com/ros-gbp/ros_comm-release/archive/release/rosmsg ..."], "url": "https://answers.ros.org/question/58478/errors-while-installing-ros-groovy-on-beablebone-with-ubuntu-1210/"},
{"title": "can not save map with octomap_server / octomap_saver from rgbdslam ROS Fuerte", "time": "2012-10-01 03:21:36 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I have seen on this ROS Answer ", " that someone already had trouble saving maps from rgbd slam using octomap. I have exactly the same problem, I click on \"Graph\" -> Send Model (ctrl+M) inside the rgbdslam window, the model and the map seem to be sent correctly (30 out of 30 nodes, for example), but when I run \"rosrun octomap_server octomap_saver mymap.bt\" there is only 1 node saved to the map file (so empty map in octovis).", "Let me explain my setup : rgbdslam from the wiki svn, adapted with many efforts to be used in fuerte and built with rosmake. I can capture a scene (by pressing space), but no map can be saved with octomap_saver as said above.", "I have also set up the octomap_mapping.launch file accordingly (fixed frame \"openni_camera\" and \"cloud_in\" remapped to \"/rgbdslam/batch_clouds\").", "I am also using the experimental branch of octomap. Here are the steps that I did to build it in ROS Fuerte:", "downloaded mapping_msgs from svn (ros wiki)\ndownloaded point_cloud_perception from svn (ros wiki)\ndownloaded geometric_shapes_msgs from svn (ros wiki)\ndownloaded octomap_mapping-experimental from svn ", "Error : /home/micmac/fuerte_workspace/octomap_mapping-experimental/octovis/src/octovis/TrajectoryDrawer.cpp:63:5: erreur: \u2018GLUquadricObj\u2019 was not declared in this scope", "Error: /home/micmac/fuerte_workspace/octomap_mapping-experimental/octomap_ros/src/conversions.cpp:45:114: erreur: explicit instantiation shall not use \u2018inline\u2019 specifier [-fpermissive]\nError: /home/micmac/fuerte_workspace/octomap_mapping-experimental/octomap_ros/src/conversions.cpp:46:125: erreur: explicit instantiation shall not use \u2018inline\u2019 specifier [-fpermissive]", "Then rosmake octomap_mapping-experimental : Built 39 packages with 0 failures", "So both RGBDSLAM and octomap_mapping-experimental seem to run fine, but I can't save any map using \"rosrun octomap_server octomap_saver mymap.bt\"", "I have also tried to publish a static transform from /rgbdslam/batch_clouds to octomap_server by adding this line to octomap_mapping.launch:", "<node pkg=\"tf\" type=\"static_transform_publisher\" respawn=\"false\" name=\"octomap_tf_conversion\" args=\"0 0 0 0 0 0 /rgbdslam/batch_clouds /octomap_server 10\" output=\"screen\"/>", "I must also precise that I am using the Asus Xtion Pro live cam, so the topics may be a little different here and there (camera_rgb_optical_frame instead of openni_rgb_optical_frame), but I also tried all the required changed without success.", "Would you please have any advice to give me on how to save a map from rgbdslam with octomap_mapping ? I've been spending monthes on this without success so far...", "Do you really need the experimental branch of octomap_mapping? I think it's pretty outdated and not maintained by anyone...", "Well I sticked to what is said in the wiki. Furthermore, I have read somewhere (can not remember url sorry) that the experimental branch includes some transforms or other required stuff in order for the communication between rgbdslam and octomap server to work.", "Could you update from the repository and see whether it works now?"], "answer": [" ", " ", " ", " ", "**2nd Edit: The latest release of my ", " package includes the ability to compute the octomap internally. You can save it directly from the gui or via ros service call. See the wiki or the readme file for instructions.", "Edit: I found a bug in rgbdslam, where the tf frames of the old openni_camera driver are hardwired in the code. Because of this, octomap_server will not find a transformation between /map and the point cloud frame when using openni_launch. I'll try to release a fix soon.", "This sounds like your settings for the interplay between rgbdslam and octomap_server are incorrect. The fixed frame should not be openni_camera, but whatever you set as parameter \"fixed_frame_name\" for rgbdslam.", "rxgraph lets you easily see, whether the rgbdslam and octomap-server node are communicating on the same topic.", "The pointclouds streamed to the octomap server need much bandwidth and much processing by the octomap server. Make sure your computer is capable of keeping up or adapt rgbdslam's parameter \"send_clouds_rate\".", "Using the Xtion Pro is fine, you only need to adapt the input topics of rgbdslam, which seems to work as you see the cloud correctly in rgbdslam's gui.", "The transform publisher you use makes no sense. The frame id set in /rgbdslam/batch_clouds is the fixed_frame_name as mentioned above. Make sure you set that same frame name for the octomap_server (see rgbdslam/launch/octomap_server.launch for an example).", "I have also corrected that in the code itself, but it didn't help. I have not tried yet with your new repository though... I will try it as soon as I can, thanks for updating the code :)", " ", " ", "Thanks for replying early,", "I have set the fixed frame name in octomap_server launch file to /map, because it is the fixed frame used in rgbdslam (I could see it in the settings using the gui).", "And I also changed the \"send_cloud_rate\" parameter in rgbdslam.launch to 3 (as in the example) but it didn't work either (1 node written only, as earlier). So I even tried to lower it to 1, but same thing. My cpu is a core i7 930, I have 6Gb ram, and no other cpu or ram consuming processes. I guess it's ok ?", "Furthermore I saw that there are two different launch files for the octomap_server:\none is present in the experimental branch launch directory and is named octomap_mapping.launch, and the other is the one that you quoted \"rgbdslam/launch/octomap_server.launch\". I saw that there are two additional parameters in that one:", "I tried to roslaunch the octomap_server using this launch file (I copied it in the launch directory of the experimental branch), and checked that the fixed frame as well as the cloud_in parameters were set correctly (\"/map\" and \"/rgbdslam/batch_clouds\").", "I am launching rgbdslam first, capture a scene (then stop with space), then I launch the octomap_server, and send the model and map using the gui (I can see the sending speed being more slow when changing the send cloud rate parameter).", "But no success either (1 node only written to the .bt map file using \"rosrun octomap_server octomap_saver mymap.bt\")", "So it seems that I did set the parameters correctly this time (frame names, and send rate) but it still doesn't work.", "Any other advice would be much appreciated. I must also say that I am working under ubuntu 12.04 with fuerte, so in the same time I am doing a separate install of ubuntu 11.10 with ros electric, because they are the ones used in the wiki.", "Hopping it will work better with those. Thanks again for your answer.", "I also see in the octomap_server wiki (ros.org/wiki/octomap_server) : \n\" Downprojected 2D occupancy map from the 3D map. Be sure to remap this topic if you have another 2D map server running. New / changed in octomap_mapping\". Do I have to do a remap in the launch file ?", "No, rgbdslam is no 2d map server. Your cpu and ram are definitely not the problem. To me everything you describe seems correct. 1 Node in the bt means, the octomap_server received nothing. Do you have set rgbdslam's \"store_pointclouds\" parameter set to true?", "If the above parameter doesn't help, check the output of rgbdslam using rostopic echo /rgbdslam/batch_clouds. This should output tremendous amounts of data. You can also use \"bw\" instead of \"echo\" to just look at the bandwidth. One cloud is about 10MB, so with send_cloud_rate=5 it should be 50MB/s.", "I have set \"store_pointclouds\" to true, but it didn't help. When I run the rostopic echo command, I see the list of points being sent separated by commas. With the bw command, I see that the average bw starts with full speed (related to the send_cloud_rate parameter), but immediately drops slowly.", "I have tried with the ros-fuerte-octomap* packages as well as the experimental branch of octomap, with the same result. I have also checked that no disturbing packages are left in the $ROS_PACKAGE_PATH pathes, and closed then reopened all terminals between changing from branches.", "Is there a way to see or use the map inside rviz or ROS for navigation instead of saving it ? maybe I could do otherwise if the only problem is saving the map to a file ? Here is the result from rxgraph : ", "When I run \"rostopic list\" I can see a /projected_map topic, but no /map topic. I run this command when everything is launched (rgbdslam, octomap_server, openni_launch).", "Ok, let's first find out how many clouds are sent. Before you send out the model from rgbdslam execute \"rostopic echo /rgbdslam/batch_clouds > /tmp/sentclouds\". Then check how many clouds are inside e.g. by \"grep header /tmp/sentclouds|wc\". This should be the amount of frames you recorded."], "question_code": ["> Edited /home/micmac/fuerte_workspace/octomap_mapping-experimental/octovis/src/octovis/TrajectoryDrawer.h to add #include <GL/glu>\n\n> and added GL and GLU to target_link_libraries to octomap_mapping-experimental/octovis/CMakeLists.txt\n", "> Edited /home/micmac/fuerte_workspace/octomap_mapping-experimental/octomap_ros/src/conversions.cpp to remove 'inline' specifier on both lines 45 and 46\n"], "answer_code": ["<param name=\"filter_ground\" value=\"false\" /><!-- who knows where the floor is? -->\n<param name=\"base_frame_id\" value=\"/camera_rgb_optical_frame\" /> <!--needs to be set, even if not used-->\n"], "url": "https://answers.ros.org/question/44852/can-not-save-map-with-octomap_server-octomap_saver-from-rgbdslam-ros-fuerte/"},
{"title": "roscore reduce network bandwidth", "time": "2013-02-07 12:00:58 -0600", "post_content": [" ", " ", "I just got warned about overloading my local network if I placed a bandwidth heavy sensor on one machine and the roscore on a different computer. What I don't understand is exactly how roscore coordinates data transfer. ", "For example, the roscore node is on machine A, and the sensor is on machine B. If all of the processes that need access to the data rich sensor remain on machine B, will  that still require heavy network bandwidth? In other words, does all data get passed through the roscore on machine A, thereby overloading the network? or does it actually coordinate direct tcp sockets between processes so that I could intelligently separate my processes to prevent network overload?", "Thank You!", "I warned you about running two bandwidth heavy nodes on different computers. The roscore itself is not the problem. :)"], "answer": [" ", " ", "Only subscribing to a topic is consuming bandwidth. The communication from/to ", " are negligible. So yes, if you run your two nodes on the same computer while having ", " on a second one you will be fine.", "See ", ", section 6.1 for a diagram explaining this.", " ", " ", "Well, that's theoretically correct but since a node is forced to connect to rosout and to clock topics:", "Generally those topics run where roscore runs.", "clock topic is only used in simulation and rosout logging can be totally disabled by defining the appropriate macros at compile-time (not recommended unless you really have little bandwidth)."], "answer_details": [" ", " ", " ", " ", "topic: /rosout\n", "to: /rosout", "direction: outbound", "transport: TCPROS", "topic: /clock", "to: /gazebo (http://darpaws5:45435/)", "direction: inbound", "transport: TCPROS", " ", " ", " ", " "], "answer_code": ["roscore", "roscore"], "url": "https://answers.ros.org/question/54696/roscore-reduce-network-bandwidth/"},
{"title": "Install a new package", "time": "2013-03-14 02:27:27 -0600", "post_content": [" ", " ", "Hi!", "I use ROS electric and I like to install a new package, for example the navigation package, so that I can use the map_server from this package.", "Is there something to adhere when I install this?", "Because I install it, but when I try to run the launch file, I get the message that the map_server node could not be found.", "Thank you for help."], "answer": [" ", " ", " ", " ", "Hello.\nAs you noticed, first you need to install it from its repository.", "Anyway it is already installed by default.\nThen you should install the tutorials for that package:", "then go to that directory, and execute a node:", "In another terminal roscd again to that package and:", "If you want to make that package by yourself, you should go to your workspace directory, and create a package, then create the two .cpp files, modify the CMakeLists.txt and rosmake.", "Hope this helps.", "Please consult these links for further information:\n", " "], "answer_code": ["sudo apt-get install ros-%YOUR_ROS_DISTRO%-navigation\n", "sudo apt-get install ros-%YOUR_ROS_DISTRO%-navigation-tutorials\n", "roscd navigation_tutorials\nrosrun robot_setup_tf_tutorial tf_broadcaster\n", "rosrun robot_setup_tf_tutorial tf_listener\n"], "url": "https://answers.ros.org/question/58125/install-a-new-package/"},
{"title": "Image-pipeline groovy unmet dependencies", "time": "2013-07-16 18:45:28 -0600", "post_content": [" ", " ", " ", " ", "Hello, \nI am trying to install ros-groovy-image-pipeline (sudo apt-get install ros-groovy-image-pipeline) on my beaglebone running ubuntu precise. when i try to install image-pipeline i get the following unmet dependencies. \n", "Any help would be appreciated..\nThank You,\nNav", "It may be missing temporarily due to repository updates. Try it again?", "I tried last night, but got the same unmet dependency ros-groovy-depth-image-proc. I am thinking about downloading source code and compiling it...is this a good idea? Or am i  missing something fundamental with the error above?", "Building from source is more error-prone. If Ubuntu package installation is not working, you probably have something fundamentally wrong, yet probably easy to fix. Do the basics first: make sure your APT sources are correct, and run `sudo apt-get update`.", "thanks, I am sure i tried apt-get update when i initially had the problem, but i will try again tonight...", "Maybe go back and work through the Ubuntu install instructions one step at a time: ", "Thanks for replying, Joq.I followed the instructions for ubuntu for ARM, and ...no luck, still same error. I downloaded source code for image_pipeline and tried rosmake and rosdep install depth_image_proc src. its complaining its unable to get sudo apt-get ros-groovy-pcl-ros..i wonder if this is the underlying issue.", "I assumed you were using the main Ubuntu repository for i386 and amd64. ARM is a different issue. That repo is experimental and not complete, you might contact ", " about it."], "answer": [" ", " ", "There is no compiled version of PCL for ARM; you'll have to build it yourself.", "Building PCL from source is tricky and incredibly time-consuming; you may want to seek guidance from other ARM users who have done the same thing. At the very least, make sure the parallelism level is turned down (look for -j options to make and/or ros parallel build cmake variables). This will help limit the amount of memory used during the compile process.", "This has been a problem for some months because the older versions of the PCL sourcedebs didn't build on Precise for ARM. I think it's been fixed since then, but my farm is out of date after the rosdistro update and needs some serious work before it will build debs again.", "I am trying to compile the source code for pcl, its complaining about ros-groovy-flann. i dont know how much more time i want to invest in this. I think i might focus on opencv purely for my stereo vision needs. \nAppreciate the reply and help :)", "There is no compiled build of flann for ARM; you'll have to install that from source as well.  In general, \"complaining about\" is not specific enough. \"complaining that XYZ cannot be found\" of \"is not installable\" is more helpful, and the full command output is even more helpful.", "Yeah, there is no compiled version for flann. I am unable to find src for flann, is it part of another package?"], "url": "https://answers.ros.org/question/67509/image-pipeline-groovy-unmet-dependencies/"},
{"title": "Can I access the absolute or parent namespace of a node from within a launch file?", "time": "2011-11-27 04:36:21 -0600", "post_content": [" ", " ", "Hi!", "I'm wondering if there is a way to determine a node's absolute or parent namespace from within a launch file at run-time.  The reason why I would like to be able to do this is as follows.", "First, I use many nested launch files, which contain groups with their own namespaces.  For this to work, my launch files need to avoid explicitly setting absolute namespaces, which maximises launch file reuse.  (Note that reading absolute or parent namespaces at run-time, on the other hand, would not reduce reuse, so it's not a problem.)", "Second, my nodes always read and write parameters and topics within their own private namespaces.  This allows intuitive grouping of parameters and topics, and since all nodes always do the same thing it's easy to keep track of things.  But most importantly, this avoids polluting the namespace in which the node was started; in particular, this allows multiple nodes to publish identically named topics for the same robot or namespace (without requiring me to look for and remap all potential conflicts).", "For example, imagine we have a robot named Marvin, with its own group/namespace \"marvin\".  Now image that marvin uses a robot base controller node called \"base_controller\", and an IMU-based position estimation node called \"imu\".  The odometry topic produced by the base_driver might be \"/marvin/base_controller/odom\", and the IMU node might also produce an \"odometry\" estimate \"/marvin/imu/odom\".  Without the private namespace we'd have to rely on remapping to achieve unique topic names like \"base_controller_odom\" and \"imu_odom\", which doesn't scale and gets very messy (what about the second IMU?).  Namespaces seem to be a perfect way to avoid this.", "The problem arises, of course, when I want to connect two nodes to the same topic.  Since both publishers and subscribers will interpret all names as private by default, I can only pass topic names between them if I use absolute paths, or paths which are relative to a common parent node.", "For example, if a navigation node, which uses its own \"/marvin/navigation/\" namespace by default, wants to read from Marvin's base controller's odometry topic \"/marvin/base_controller/odom\", I either need to tell the navigation node to read \"/marvin/base_controller/odom\", or I need to tell both to use something like \"/marvin/odom\".  In either case, the desired topic needs to be specified as a global name, or it needs to use a common parent, in order to override the private namespaces used by both nodes.", "...", "Solution 1.  For now I solve this in what I guess is the standard way, by not using private namespaces for topics.  I still use the node's name internally to put everything in the private namespace by default anyway, but I can then remap topics relative to the node's parent namespace in the launch file.  But this causes a bit more work for each node, and I really like the idea of every node using its own namespace ..."], "answer": [" ", " ", "The recommended solution is to define parameters in each node's private namespace, but define topics in the ", ". While parameters are typically specific to each node, topics are shared between multiple nodes. Publishers and subscribers   generally do not need to know who provides or consumes the data, the topic name is sufficient.", "So, in your example, ", " should provide ", " for Marvin. Each node will access it using the relative topic name, \"odom\". This is automatically be resolved in the appropriate namespace without any special handling in the nodes or launch files.", "I do think that OP had a point. Imagine two sub-modules, with their own launch files, that I want to include in separate namespaces into a another module launch file, especially in simulation. Remapping between these sub-namespaces is impossible right now, since the absolute names are not known.", "Thanks for your belated support. ;)  Your example is the type of thing I was getting at.  Launch file reuse and topic remapping could be much more powerful than they are.", "If you are convinced you need this feature, please open an enhancement request for roslaunch. You can link it to this page to avoid repeating everything.", " might be related.", " ", " ", " ", " ", "A current hacked solution we came up with: Keep track of the global namespace by always passing down a ", " to every included launch file, and include a ", ". For example, with a simple two-subnode case, being called from an upper launch file.", "The upper level launch:", "And then in a lower level launch:", "In this way, you are always using the global-remap method, which always works, and you can reference the global namespace without actually knowing it a priori. This allows you to build up launch files with multi-level includes, and \"cleanly\" remap at the appropriate points. The best place to remap then is always at the lowest namespace where two topics are common, and no higher."], "answer_code": ["/marvin/odom", "nav_msgs/Odometry", "<arg name=\"ns\" ...>", "<arg name=\"ns\" default=\"\"/>", "<launch>\n  <arg name=\"ns\" default=\"\"/>\n  <remap from=\"$(arg ns)/node1/chatter\" to=\"$(arg ns)/node2/chatter\"/>\n  <include file=\"talk_sub.launch\" ns=\"node1\">\n    <arg name=\"ns\" value=\"$(arg ns)/node1\"/>\n  </include>\n  <include file=\"listen_sub.launch\" ns=\"node2\">\n    <arg name=\"ns\" value=\"$(arg ns)/node2\"/>\n  </include>\n</launch>\n", "<launch>\n  <arg name=\"ns\" default=\"\"/>\n  <node name=\"talker\" pkg=\"play\" type=\"talker.py\"/>\n</launch>\n"], "url": "https://answers.ros.org/question/12105/can-i-access-the-absolute-or-parent-namespace-of-a-node-from-within-a-launch-file/"},
{"title": "how to use ethzasl_icp_mapping with velodyne?", "time": "2013-08-28 23:53:42 -0600", "post_content": [" ", " ", " ", " ", "Hi all,\nI am using a .pcap file to playback velodyne HDL 64E_2 data using the velodyne ros package. I would like to obtain some odometry data using the velodyne and the ethzasl_icp_mapping package seems to be a good start. I am able to register pointclouds using a kinect, but with the velodyne I have no output on /point_map. My launch file is as below. I am not using any filters or icp config file and just want to try out the package with default parameters. Is there anything wrong here?", "If there are any other techniques that can do laser odometry, kindly suggest"], "answer": [" ", " ", " ", " ", "I have not tried that combination. Several questions and comments:", "Are you using a recent version of the driver? It has fixes for the 64E S2. Are you running Hydro or Groovy?", "You should verify the speed of your device. I've seen examples at 300 RPM, but not 200 RPM, I don't think that is an option. Velodyne provides a procedure for configuring the speed, consult the manual if you need or want to change it.", "Using ", " rather than nodes would probably reduce your CPU overhead noticeably.", "You should use the correct angles calibration for your device, ", " is for an older model than your S2. The ", " script will convert the ", " provided by Velodyne with your device into the YAML format used by the velodyne_pointcloud package.", "The cloud node publishes data in the \"/velodyne\" frame to the ", " topic, ", " ", ". Use ", " or ", " to verify what is actually being published. Also, ", " will tell how often messages are actually sent.", "The driver does ", " publish a transform from \"/velodyne\" to \"/odom\". How could it know that? You do need to provide one yourself.", "Since your device is moving, something needs to transform \"/velodyne\" points into the \"/odom\" frame. Normally that would be done via the ", " instead of the cloud node. It transforms each packet as it's received, to minimise \"smearing\" of data as the device revolves. Unfortunately, that presumes you have a separate odometry source, publishing that transform for the conversion.", "I am not sure what to recommend for the circular odometry dependency: the driver needs odometry for transforming data into a fixed \"/odom\" frame, but that's what you are trying to generate. If the vehicle is moving slowly enough and the device is rotating quickly enough, you may be able to get away with assuming the point cloud generated for each rotation is \"instantaneous\". ", "I am open to suggestions for how to change the driver to better support your use case. Perhaps we could write a different transform node and nodelet that would better suit your requirements. It is not clear to me how that would work, however.", "I would expect other rotating LIDARs to exhibit similar problems. I would appreciate suggestions from people with experience doing similar things with those devices. Heading changes tend to cause more trouble than forward motion.", "Hi, first of all, thanks a lot for the work on the driver. I do have issues running some pcap files (e.g. ", "), but thats off topic. Yes, I normally run it at 900 rpm and it looks fine. I get errors on \"unable to open calibration file\", the 64e_utexas is the only working one i have. I am using the hydro package in ubuntu 13.04.", "Thanks for the suggestion on the rostopic. Thats probably what is wrong. Will try it out. I think the driver as is, serves its purpose; and no feature enhancement is required.", "I hope you're right. I am interested in knowing the results of your work.", "The latest Hydro version 1.1.1 provides a `params/64e_s2.1-sztaki.yaml` configuration file. That S2.1 is more similar to your S2 than our old 64E from 2007. I would try that, instead.", "The manual says your model supports spin rates from 300 to 1200 RPM. That higher, 20Hz, speed should help minimize rotational \"smearing\" for your application.", "Will post any updates here. Regarding the calibration file, I cannot load 64e_s2.1-sztaki.yaml. I will verify if I doing it correct 2moro.", "The problem was with the wrong ros topic as you suggested. It should just be /velodyne_points. Thank you.", " ", " ", "We didn't try yet the node ethzasl_icp_mapper with a Velodyne 64 (and never used pcap format) but we have some appropriate configuration files for the Velodyne 32. You can have a look at the folder ./launch/artor for some examples or directly \n", ".", "Some of the results can be seen here:", "Once you confirm that your topic names are correctly links, I'm foreseeing some other challenges at the registration level:", "The default configuration of the node might have some problem due to the concentric circle generated by the 64 beams. I really suggest you to start with the config files in ./launch/artor", "The point cloud that you will give to the topic \"cloud_in\" should optimally be from an assembled point cloud of 360 degrees. If you want to do it well, you would need an estimate of the platform motion. For a quick implementation, you can assume that the platform is static during one Velodyne revolution.", "The bandwidth use by that sensor is very large. Don't expect real-time capability at the beginning so you should replay your data at a slow rate to see what is going on.", "I hope that will put you on the good track!", "Very cool! Was your work done using the ROS Velodyne driver?", "I used the same parameters and filters used for artor, but with a 64E_2 laser. The program ran, but I did not see any associations or map built. This could be due to vehicle speed or my overlap parameters. Here is my data log with a \"no sensor noise or normals found\" warning. ", ": the link you provide doesn't seem to work.\n\n\n@ joq: I'm only supporting the platform for the 3D mapping but I think they use their custom version of the driver that is floating around the lab since some years. I will direct them to your node so they can have a look.", "It works: ", ". Unfortunately this page adds a (/p) to the link. So plz remove it if this is the case.", "Thanks to all for your support. I can get odometry with artor configuration with no problem. Due to the speed of the velodyne (1200rpm) and speed of the vehicle(10kmph), the map is lost, Nevertheless I was able to get fairly good odometry information. This was tested on a quadcore i7. I guess for higher speeds, a more powerful pc is required.", "I am using groovy  and velodyne 32E for ethzasl_icp_mapper, but it keep on showing this error\nERROR: cannot launch node of type [point_cloud_converter/point_cloud_converter]: point_cloud_converter\nartor file in launch folder i.e. artor_mapper.launch mentions this <remap from=\"cloud_in\" to=\"/velodyne/assembled_cloud2\"/> which really doesn't exist.", "Yes, the launch file that you are using is specific for a platform that we have in our lab and not intended to be used out-of-the-box. I suggest you to copy the launch file and adapt what your need.", "[follow up] In its current configuration, Artor produces PointCloud msgs and not PointCloud2, which explains the extra node point_cloud_converter. You will also need to remap cloud_in to your own topic.", " ", " ", "Hi,\nI have used ethzasl_icp_mapping with tilting laser-scanner. But I have not worked with velodyne. Do the velodyne nodes you are launching provide a ", " from the \"/sensor_frame\" to \"/odom\" frame? ", "In your case \"/odom\" frame and \"/sensor_frame\" are both velodyne. ", "The node ethzasl_icp_mapping requires the transform \"/sensor_frame\" to \"/odom\" frame. ", "It basically requires some initial guesses for odometry. The package provides uses this guesses and provides corrected odometry using ICP as far as I know.", "Hi, I do not have any odometry. I also tried giving a zero tf from /velodyne to /odom, but dint help. The openni example in the stack works without any odometry or tf though", "Hey, is odometry input (wheel, GPS, IMU) really necessary for node ethzasl_icp_mapping to work? I tried the node with velodyne 32E pcap file without any odometry input. But it couldn't output direction or move forward for more than 5m. After which it stopped at a point and kept adding points at the same place."], "question_code": ["  <node pkg=\"velodyne_driver\" type=\"velodyne_node\" name=\"velodyne_driver\">\n    <param name=\"model\" value=\"64E_S2\"/>\n    <param name=\"pcap\" value=\"/home/hope/Downloads/velodynepcap/S227_target.pcap\"/>\n    <param name=\"read_once\" value=\"false\"/>\n    <param name=\"read_fast\" value=\"false\"/>\n    <param name=\"repeat_delay\" value=\"10000.0\"/>\n    <param name=\"rpm\" value=\"200.0\"/>\n    <param name=\"frame_id\" value=\"velodyne\"/>\n</node>\n\n<node pkg=\"velodyne_pointcloud\" type=\"cloud_node\" name=\"cloud_node\">\n    <param name=\"calibration\" value=\"$(find velodyne_pointcloud)/params/64e_utexas.yaml\"/>\n    <param name=\"min_range\" value=\"3.0\"/>\n    <param name=\"max_range\" value=\"130.0\"/>\n</node>\n\n<node name=\"mapper\" type=\"mapper\" pkg=\"ethzasl_icp_mapper\" output=\"screen\" >\n    <remap from=\"cloud_in\" to=\"/velodyne/velodyne_points\" />\n    <param name=\"subscribe_scan\" value=\"false\"/>\n    <param name=\"subscribe_cloud\" value=\"true\"/>\n    <param name=\"odom_frame\" value=\"/velodyne\" />\n    <param name=\"map_frame\" value=\"/map\" />\n    <param name=\"useConstMotionModel\" value=\"false\" />\n    <param name=\"localizing\" value=\"true\" />\n    <param name=\"mapping\" value=\"true\" />\n    <param name=\"minOverlap\" value=\"0.8\" />\n    <param name=\"maxOverlapToMerge\" value=\"0.9\" />\n    <param name=\"tfRefreshPeriod\" value=\"0.01\" />\n    <param name=\"vtkFinalMapName\" value=\"finalMap.vtk\" />\n    <param name=\"useROSLogger\" value=\"true\" />\n    <param name=\"minReadingPointCount\" value=\"1000\" />\n    <param name=\"minMapPointCount\" value=\"10000\" />\n    <param name=\"inputQueueSize\" value=\"1\" /> \n</node>\n"], "answer_code": ["params/64e_utexas.yaml", "db.xml", "rostopic list", "rosgraph", "rostopic hz"], "url": "https://answers.ros.org/question/75433/how-to-use-ethzasl_icp_mapping-with-velodyne/"},
{"title": "when pids values cause errors that crash system [closed]", "time": "2013-09-17 07:51:15 -0600", "post_content": [" ", " ", " ", " ", "When PID values are changes via rviz  dynamic plugin to values that are too large both rviz and gazebo ros plugin crash. The model attemps to jump out of bounds ...but error should be handled more gracefully , yes?", "for ros gazebo plugin:", "and for rviz move-base", "What values are you sending? Can you provide instructions on how to reproduce?", "added configuration.....just increase pid to some large number. Was trying to get cmdvel up past unit value of 1 inorder to move."], "answer": [" ", " ", "There is no way a PID instance can determine if a set of gains will make a specific robot setup unstable or not, as the outcome depends on many external factors like mechanism dynamics (inertia, friction, interactions between bodies) and the simulation timestep. Your best bet is to start with a reasonable approximation of your robot dynamics, and then start tuning your gains, from lower to higher values. I recognize that the process can be frustrating and time-consuming at times, especially when simulation-specific factors come into play.", "This I understand quite well....its that if the system crashes as I try different pid setting then it makes convergence almost impossible in a testing environment.", "I get your question now. The unstable simulation is propagating nonsense values that trigger an assertion in rviz code. I agree that a more graceful error handling would be desirable. Can you get a backtrace of the crash?.", "I can reproduce the crash...but I need to know what information..logs, dumps,...etc you need and how to generate them if debug setting must be set. Not sure what you mean by backtrace , trace report?", "Build rviz with debug symbols and run it with a debugger like gdb, trigger the crash and get a backtrace (bt), which will give you the call stack when the crash happened. It will hopefully contain useful information.", "I set up my launch file to invoke rqt and rviz independently. So not to use the plugin and specified the Debug on the rviz node.\nIn the rviz package in my catkin src I created a rosconfig.cmake file and place in it  the line  set(ROS_BUILD_TYPE Debug) \nWhen I run my launch file the dbg window opens but it says no debug symbols found in catkin/devel...etc.\n\nIs this not the correct way to compile with debug? The compile has warnings but no errors.", "This maybe become moot topic as I am unable to replicate error. I have made many changes to my nav config files and even my tf and cmd publisher. Sorry. But I am still would like to know how to set debug in compile for future reference. If I run into it again I will be ready.", "I would like to see a better choice list for closing items of this nature. I will close under too localized.", "If using catkin, set the standard cmake variable CMAKE_BUILD_TYPE; if using rosbuild set ROS_BUILD_TYPE, as you mention above. Default build types containing debug symbols are: Debug or RelWithDebugInfo"], "question_code": ["[ INFO] [1379439029.301559081, 1.676000000]: Loaded gazebo_ros_control.\ngzclient: /build/buildd/ogre-1.7.4+dfsg1/OgreMain/include/OgreAxisAlignedBox.h:252: void Ogre::AxisAlignedBox::setExtents(const Ogre::Vector3&, const Ogre::Vector3&): Assertion `(min.x <= max.x && min.y <= max.y && min.z <= max.z) && \"The minimum corner of the box must be less than or equal to maximum corner\"' failed.\nAborted (core dumped)\n", "python: /build/buildd/ogre-1.7.4+dfsg1/OgreMain/include/OgreAxisAlignedBox.h:252: void Ogre::AxisAlignedBox::setExtents(const Ogre::Vector3&, const Ogre::Vector3&): Assertion `(min.x <= max.x && min.y <= max.y && min.z <= max.z) && \"The minimum corner of the box must be less than or equal to maximum corner\"' failed.\n[rqt_gui-1] process has died [pid 7495, exit code -6, cmd /opt/ros/hydro/lib/rqt_gui/rqt_gui __name:=rqt_gui __log:=/home/viki/.ros/log/c890f9ee-1fbe-11e3-807c-28cfe95d78b1/rqt_gui-1.log].\nlog file: /home/viki/.ros/log/c890f9ee-1fbe-11e3-807c-28cfe95d78b1/rqt_gui-1*.log\nmove_base: /usr/include/pcl-1.7/pcl/conversions.h:247: void pcl::toPCLPointCloud2(const pcl::PointCloud<PointT>&, pcl::PCLPointCloud2&) [with PointT = base_local_planner::MapGridCostPoint]: Assertion `cloud.points.size () == cloud.width * cloud.height' failed.\n[move_base-7] process has died [pid 7590, exit code -6, cmd /opt/ros/hydro/lib/move_base/move_base __name:=move_base __log:=/home/viki/.ros/log/c890f9ee-1fbe-11e3-807c-28cfe95d78b1/move_base-7.log].\nlog file: /home/viki/.ros/log/c890f9ee-1fbe-11e3-807c-28cfe95d78b1/move_base-7*.log\n\n\nTrajectoryPlannerROS:\n  # for details see: <a href=\"http://www.ros.org/wiki/base_local_planner\">http://www.ros.org/wiki/base_local_planner</a>\n  max_vel_x: 5\n  max_trans_vel: 5\n  min_vel_x: 3\n  min_trans_vel: 3\n  max_rotational_vel: 1.0   # 0.1 rad/sec = 5.7 degree/sec\n  min_in_place_rotational_vel: 0.1\n\n  acc_lim_th: 2.5\n  acc_lim_x: 2.5\n  acc_lim_y: 0\n\nControl yaml\n\nrrbot:\n  # Publish all joint states -----------------------------------\n  joint_state_controller:s\n    type: joint_state_controller/JointStateController\n    publish_rate: 15\n\n  # Position Controllers ---------------------------------------\n  joint1_position_controller:\n    type: effort_controllers/JointVelocityController\n    joint: joint1\n    pid: {p: 5, i: 5, d: 5}\n  joint2_position_controller:\n    type: effort_controllers/JointVelocityController\n    joint: joint2\n    pid: {p: 5, i: 5, d: 5}\n\nIn urdf:\n\n  <transmission name=\"tran1\">\n    <type>transmission_interface/SimpleTransmission</type>\n    <joint name=\"joint1\"/>\n    <actuator name=\"motor1\">\n      <hardwareInterface>EffortJointInterface</hardwareInterface>\n      <mechanicalReduction>1</mechanicalReduction>\n    </actuator>\n  </transmission>\n\n  <transmission name=\"tran2\">\n    <type>transmission_interface/SimpleTransmission</type>\n    <joint name=\"joint2\"/>\n    <actuator name=\"motor2\">\n      <hardwareInterface>EffortJointInterface</hardwareInterface>\n      <mechanicalReduction>1</mechanicalReduction>\n    </actuator>\n  </transmission>\n\nFor Gazebo in model.sdf\n\n    <plugin name=\"ros_control\" filename=\"libgazebo_ros_control.so\">\n      <robotNamespace>/rrbot</robotNamespace>\n      <robotSimType>gazebo_ros_control/DefaultRobotHWSim</robotSimType>\n    </plugin>\n"], "url": "https://answers.ros.org/question/80211/when-pids-values-cause-errors-that-crash-system/"},
{"title": "robot_state_publisher : process has died every time when start up the PR2 robot", "time": "2013-09-26 04:23:42 -0600", "post_content": [" ", " ", "On the PR2 robot, every time the robot is booted, we have to roslaunch /etc/ros/robot.launch to start up the essential nodes.", "However, the robot_state_publisher node died every time when I roslaunch /etc/ros/robot.launch. Here's the output message:", "I've done some basic testings:", "1.Suppose that there are some problems in URDF. ", "Since the robot_state_publisher need the robot URDF parameter in the parameter server and build up the TF tree. I guess if there are some problems in the URDF that cause the process to die every time. Therefore I test the uncalibrated URDF and the original URDF generated from the xacro, but The process still died every time it was launched.", "2.Suppose that there are some problems in the robot_state_publisher executable file. I first upload the robot_state_publisher executable file in my computer, which works fine when running gazebo simulation, onto PR2. But the process still died. Then I try to build the package from source, but I still get the same problem.", "3.I then run the node singly with required parameters such as the URDF and repeat the procedure above, but the problem is still there. But all is well when I run it on my own computer. The problem only happen on the PR2 computer.", "What might the problem be? Thanks for any advise."], "answer": [" ", " ", "After I update the packages in PR2, everything's fine!", "For more info about upgrade PR2 software, one can consult this website:", " I meet the seem like question,could u give me some sugestions\n "], "question_code": ["[head_traj_controller/point_head_action-41] process has died [pid 13035, exit code -11, cmd /opt/ros/groovy/stacks/pr2_controllers/pr2_head_action/bin/pr2_head_action /diagnostics:=/diagnostics /diagnostics_agg:=/diagnostics_agg __name:=point_head_action __log:=/home/pr2admin/.ros/log/ac1de78a-26ae-11e3-9f32-001517ebc33d/head_traj_controller-point_head_action-41.log].\nlog file: /home/pr2admin/.ros/log/ac1de78a-26ae-11e3-9f32-001517ebc33d/head_traj_controller-point_head_action-41*.log\n"], "url": "https://answers.ros.org/question/83621/robot_state_publisher-process-has-died-every-time-when-start-up-the-pr2-robot/"},
{"title": "Maps, is it better to publish on a topic or use a service and have other nodes request a map?", "time": "2013-08-30 08:38:38 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "So I have a question regarding the publishing and subscribing to maps.", "I have a simple mapping node which creates an nav_msgs/OccupancyGrid map of a robots environment built using LIDAR data. I currently have it written so that it both publishes the map on a ros topic and provides a service which clients call to get a map. As of now, only one other node I am running subscribes to the map topic (other than rviz of course for visualization, but this won't be used when the robot is actually running, except for debugging purposes).", "The node that uses the map is a dstarlite global navigator, which I have written so that it can either subscribe to the map topic or call the service to get the map when needed (which, due to the nature of the code making it run slower than the rest of the running nodes, is far less often than the map is published). ", "My question is: which would be better to use the client-service setup or the publish-subscribe setup? ", "By better I mean more efficient/less expensive.", "In addition I should note that right now, in the development stage of writing the mapping node, I also have it performing several other tasks in addition to publishing inflated obstacle maps. These tasks are to generate/publish both laser scan messages from the existing map to account for the limited angular range (which optimally would occur at a higher frequency) and to generate/publish a laser scan message with an Ego-kinematic space filter applied. If it turns out that the client/service setup is more efficient, then I was simply going to create another client node to also perform these tasks.", "Thank you in advance for your help, I apologize for my long-windedness and if I have not exactly followed proper post formats, this is my first post on here and I'm still learning!", "This is a good post.", "I personal guess (and as it's just a guess I add a comment instead of an answer) is that publish/subscribe must be less expensive, as services imply both request and response, but the difference must be minimal.", "But I think the important point is whether the client needs to know if all went ok and/or must wait for the map to be ready. Those things are provided by a service. Publish/subscribe is a looser relationship, where the publisher don't mind at all about how their messages are used.", "Hope I'm not telling bullshit.... Please someone correct me of I'm wrong!", "The one benefit I have noticed is that since everything is still in the development phase, and I haven't yet been able to perfectly sync all of my nodes for publishing/subscribing, this eliminates the need to check for \"fresh\" data since it returns false if a response was not received."], "answer": [" ", " ", "If you have multiple other nodes that need map data on a irregular, relatively low rate, on-demand basis, it likely is a good idea to use service calls. This way, bandwidth and CPU (for serialization) are only consumed when needed and (depending on the actual scenario) map data can be more up-to-date than when using the last published map data.", "I think it\u00b4 s pretty hard to state a general rule, as there are multiple factors playing a role here:", "A single service\ncall has higher delay and consumed\nbandwidth when compared to a single\npublication on a topic, but using\nservice calls can mean that only\nsingle queries are required as\nopposed to a \"stream\" on a topic.", "Service clients know when a service\ncall fails, while a topic callback\nnot getting triggered will generally\nhappen silently.", "See also this ", " for a general discussion of the matter.", "Also, if you publish map data for debugging, it makes sense to put all work related to that in a block that checks if anyone is actually subscribing:", "This way, there\u00b4 s only any real work done when there is at least one subscriber (for example rviz).", "One advantage of topics is that the map is published, whenever it changes. If your software needs to react on that, service calls won't work as they are triggered by the client.", "Very true, I\u00b4m pretty sure there are a few more nuances I have missed :)"], "answer_details": ["Service calls for example can\u00b4t be\nrecorded with rosbag, which can be a\ndisadvantage when trying to\nunderstand interaction in a complex\nsystem of nodes.", " ", " ", " ", " "], "answer_code": ["if (map_publisher_.getNumSubscribers() > 0){\n\n  nav_msgs::OccupancyGrid grid_map;\n\n  //Do work/fill grid_map here\n\n  map_publisher_.publish(grid_map);\n}\n"], "url": "https://answers.ros.org/question/75766/maps-is-it-better-to-publish-on-a-topic-or-use-a-service-and-have-other-nodes-request-a-map/"},
{"title": "ROS callbacks, threads and spinning", "time": "2013-01-22 06:33:33 -0600", "post_content": [" ", " ", " ", " ", "Hello,\nI have read some tutorials (such as ", ") but I still have some doubts about ROS and callbacks:", "Let's say that:", "My questions are:", "The reason why I have several threads is for parallel execution of some stuff related to the callbacks, but also to have different control loop speeds: i.e. one thread will run at 10hz while other one will run at 30hz.", "Thanks for your time!", "EDIT: detail explanation on my case:", "thread A: subscribes to laser, callback gets data from laser, thread process the data to detect legs", "thread B: subscribes to kinect, callback gets data from kinect, thread process the data to detect humans", "thread C: subscribes to a path-publisher (own work), callback gets points, thread computes a route and publish cmd_vel ", "etc...", "This seems like a complicated use case, and I want to be sure you don't have an XY problem. Can you describe what you're trying to accomplish (the problem itself, that is) in more detail?", "Detailled information published, thanks for your time!", "Cool, thanks for the update. How interdependent are A, B, and C here? If the coupling is fairly loose, this looks to me like three nodes, not three threads. If it's fairly tight, that's a different story.", "If you are just worried about message copying overhead, another option is to user nodelets, see: ", "Each thread writes its results in shared memory. These results are read by the main process, which is the robot's state machine", "I have read about nodelets in the past, but as I understand it requires the creation of several nodes first (extra work). Moreover, as I understand they are indicated for high throughput nodes that need to communicate, which I do not think it is my case (depends on what is high throughput)"], "answer": [" ", " ", " ", " ", "Briefly: It won't work as you expect and you probably do not need such a complex way of designing your node.", "When it comes to communication in roscpp, two kind of objects are handling callbacks:", "A spinner is an object that has the ability to call the callbacks contained in a callback queue. A callback queue is the object in which each subscriber will add an element each time a message is received by resolving which kind of message should call which callbacks (and with which arguments).", "Regarding the spinners, there is currently three implementations available in roscpp:", "These objects ", " be instantiated manually in advanced ROS nodes but to avoid verbose initialization, an ", " is provided through functions in the ROS namespace. Aka ", ", ", " and so on. This API rely on a default callback queue implemented as a singleton which is accessible through the ", " function.", "So basically when you call the ", " function, a single-thread spinner is created and its spin method is called once using the default callback queue (see ", " from roscpp).", "And to finish, when you create a ", ", you pass it a ", " and each ", " has an associated callbackqueue that default to the global one but which can be overridden using the ", "/", " methods.", "If you take a look at ", " you will see there is a mutex that make the SingleThreader thread-safe ", " discouraging you to use it in this case (line 48).", "Conclusion: what you do is safe ", " will trigger regularly ROS error messages as there is chances that several instances of ", " will be executed in parallel.", "Considering your applications, I think your callbacks are just not written as they should.\n", ".\nIn most of the cases, a callback is just feeding ROS data to your own classes which is usually as simple as copying data (and maybe converting them).\nIt will help you ensuring that your callbacks are thread-safe (if you want to convert your node to a nodelet for instance, one day) and avoid making \"ros::Spin\" blocking for a long time, even in the case you are using the single-threaded spinner.", "Typically, if you want to do time-consuming computations such as \"leg detection\", the callbacks are definitively ...", "Exactly the answer I was looking for.\nJust for the record I did not to get any ros error msgs (spinner.cpp line 48) about what I was doing, therefore It was unclear for me to notice if I was doing something strange or not. Moreover, I thank you for this clear explanation on ros callbacks and spins", "+1 for the clear and interesting answer. I am facing a similar problem. I don't get the \"main thread\" part. should it be while(ros::ok()) { leg_detector.process(latest_scan); ros::spinOnce(); } ? I think there should be at least 2 threads: one for the callbacks and one for the heavy work...", "Yes, this is what I suggest. The question is: what benefit will you have if create two threads? Your leg detector is anyway ", " to process all the data. 99% of the cases you are just fine letting them being thrown away.", "This is an excellent answer! It is even clearer than the tutorials in some places. Brilliant! Thank you!", "I have similar situation. I want subscribe scan data and image_rect data from xtion. so I already use message_fillters. but it seem not working. I think is that sync problem maybe. I ask ", ".", "Work perfectly here! thank you!", "How can we add this to the ", " wiki page? This would be very helpful to have and would clear up unnecessary confusion when visiting that page for the first time.", " ", " ", "I'd suggest you change topology.", "Why not having a single thread manage all the ROS messaging and then dispatching relevant information to the relevant processing thread?", "As to say:", "You should already be faced with the problem of how to pass data from your actual threads A and B to C (motion planner needs to know about legs and humans from A and B right?), so adding to this doesn't really over-complicate it.", "I suggest you have a look at the ", ".\nIt will allow you to easily shuffle messages back and forth from your threads, and make the program a little easier to both code and maintain."], "answer_details": [": the default one, takes the messages contained in a callback queue and process the callbacks one by one while blocking the execution of the thread that called it.", ": spawns a couple of threads (configurable) that will execute callbacks in parallel when messages are received but blocks the execution of the thread that called it.", ": spawns a couple of threads (configurable) that will execute callbacks in parallel while ", " blocking the thread that called it. The start/stop method allows to control when the callbacks start being processed and when it should stop.", " ", " ", " ", " ", "Thread A: ros spin, subscribing, callbacks, dispatcher", "Thread B: laser, gets data from relevant dispatcher in thread A", "Thread C: kinect, gets data from relevant dispatcher in thread A", "Thread D: motion planner, gets data from all of the above and publishes cmd_vel", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", "in my program I have several threads: A, B and C", "each thread A,B and C subcribes to a different topic and has declares its own ros::NodeHandle and ros::subscriber", "each thread has its own main loop where ros::spinOnce() is called and some other stuff is done, such as control loop with ros::Rate loop_rate(Hz) ", "when ros::spinOnce() is called in each thread, which callbacks are executed?", "is there a problem with this configuration? if so, how should it be done?"], "answer_code": ["ros::spin()", "ros::spinOnce()", "ros::getGlobalCallbackQueue()", "ros::spinOnce()", "Subscriber", "NodeHandle", "NodeHandle", "getCallbackQueue", "setCallbackQueue", "ros::SpinOnce"], "url": "https://answers.ros.org/question/53055/ros-callbacks-threads-and-spinning/"},
{"title": "amcl / Laser scan matcher frequency", "time": "2014-01-01 18:05:50 -0600", "post_content": [" ", " ", " ", " ", "Hi guys I have a question here.", "This is a normal tf frames of amcl.\n", "This is another where I have added extra node into it.\n", "My question is, does the frequency of amcl and laser_scan_matcher affect the performance?\nIf yes, how do I manipulate the frequency?\nIf no, why?"], "answer": [" ", " ", "AMCL publishing to tf at >1 kHz certainly is not normal behavior and doesn\u00b4t make a lot of sense. Can you update your question with information on how you make it publish so fast? Such a high tf rate does not necessarily break anything, but it will cause higher CPU consumption due to the many messages that are sent around (and subscribed) on the /tf topic. This higher CPU consumption could lead to problems when maxing out the machine running the system.", "+ it doesn't gain any algorithmic performance as it only makes sense to update on new data (unless that arrives with 1kHz)"], "url": "https://answers.ros.org/question/114321/amcl-laser-scan-matcher-frequency/"},
{"title": "building ROS on arch linux", "time": "2012-11-21 20:47:45 -0600", "post_content": [" ", " ", " ", " ", "Greetings! I'm trying to build ros on arch linux. I followed this instruction\nros.org/wiki/fuerte/Installation/Arch\nwith some trouble along:", "1)", "Hunk #1 FAILED at 31.", "This error was repeated for all entries in the patch file.", "2)\nAt the final step of ", "the making started, but so far I encountered following errors:", "And in another package", "/home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp:179:47: error: \u2018CV_YUV2GRAY_UYVY\u2019 was not declared in this scope", "/home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp:180:46: error: \u2018CV_YUV2RGB_UYVY\u2019 was not declared in this scope", "/home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp:181:46: error: \u2018CV_YUV2BGR_UYVY\u2019 was not declared in this scope", "/home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp:182:47: error: \u2018CV_YUV2RGBA_UYVY\u2019 was not declared in this scope", "/home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp:183:47: error: \u2018CV_YUV2BGRA_UYVY\u2019 was not declared in this scope", "What could cause it? Is it any easier to build previous versions of ros?", "thanks Lorenz, I didn't want to install a different distribution for the purpose of running a single package, but building under arch is beginning to appear infeasible."], "answer": [" ", " ", "Building ROS on Arch is definitely not trivial and the install instructions are probably outdated. The patches you were trying to apply are 8 months old.", "I didn't build more than the ROS underlay recently in Arch and there are basically mainly three things you have to do:", "Get the script to fix the python shebang lines from ", " and run it on your underlay.", "Get rid of rx. I couldn't manage to compile it at the moment because of some weird swig error I don't have time to investigate right now.", "Use the following cmake call: ", ".", "Open ros_comm/tools/rosbag/src/recorder.cpp and go to line 439. Change ", " to ", " to make it compile with the most recent boost version.", "This should allow you to build at least the middleware and the basic build system and command line tools. For higher-level stacks, you will have to fix a lot of code yourself which is a pretty time consuming process.", "An alternative solution in case you need to stick with Arch and cannot switch to Ubuntu is to install a Ubuntu chroot environment in which you can install ROS using apt-get. Just google for debootstrap in case you want to try that.", " ", " ", "if anyone (like me) found this question looking for the answer to this error:", "CV_YUV2GRAY_UYVY\u2019 was not declared in this scope", "You will need to install opencv from source, the prebuilt binary is too old", "Thanks for the tip!\nI'm battling away with getting hydro on a raspberry pi with opencv. It's a struggle, but pretty fun so far!. Next up Moveit!", " ", " ", " ", " ", "If you plan on moving to Groovy (resp. Hydro), there are ", " for that. It's a work in progress, but you can have your setup with rviz installed quite easily now. You just need to follow the method presented in the ", " (resp. ", "):", "(or the equivalent command for your AUR package manager)", "The PKGBUILD scripts used can be found on ", ".", "ok, thanks! Since then I installed ubuntu for sole reason of running ROS", "I used to run ROS in a Virtualbox Ubuntu VM, but I went back to Arch Linux recently. The only problem I have for the moment is with PCL and stacks that depend on it, but it should be fixed soon enough.", "FYI, Hydro can also be installed with AUR packages. It works quite well so far ;-)", "Good news, thanks!"], "question_code": [" patch -p0 < ros.patch\n", "\npatching file simulator_gazebo/gazebo/Makefile.gazebo.tarball", " ", " ", "1 out of 1 hunk FAILED -- saving rejects to file \nsimulator_gazebo/gazebo/Makefile.gazebo.tarball.rej\npatching file geometry/tf/include/tf/message_filter.h\nReversed (or previously applied) patch detected!  Assume -R? [n] \nApply anyway? [n] \nSkipping patch.\n", "rosmake -a\n", "\n[ 25%] Building CXX object CMakeFiles/camera_calibration_parsers.dir/src/parse_yml.o\n  /home/aash29/ros/image_common/camera_calibration_parsers/src/parse_yml.cpp:3:27: fatal error: yaml-cpp/yaml.h: No such file or directory\n  compilation terminated.\n", "\n[100%] Building CXX object CMakeFiles/cv_bridge.dir/src/cv_bridge.o\n  /home/aash29/ros/vision_opencv/cv_bridge/src/cv_bridge.cpp: In function \u2018std::map<std::pair<cv_bridge::format, cv_bridge::format=\"\">, std::vector<int> > cv_bridge::getConversionCodes()\u2019:", " ", " ", " ", " ", " ", " ", "make[3]: ", " [CMakeFiles/cv_bridge.dir/all] Error 2\n"], "answer_code": ["cmake .. -DPYTHON_EXECUTABLE=/usr/bin/python2 -DCMAKE_INSTALL_PREFIX=/opt/ros/fuerte", "boost::TIME_UTC", "boost::TIME_UTC_", "yaourt --noconfirm ros-groovy\nor\nyaourt --noconfirm ros-hydro\n"], "url": "https://answers.ros.org/question/48800/building-ros-on-arch-linux/"},
{"title": "Single board PC for ROS", "time": "2012-10-15 17:44:36 -0600", "post_content": [" ", " ", " ", " ", "Hey", "Which Single board computers are recommended/better suited for running ROS (Fuerte and Ubunutu 12.04 or other recommendation)? ", "This has been asked a few times before, but not recently as far as I could find anwyays, sorry if double up though.", "My application will be running with a Hokuyo URG LIDAR (", "/) and using an Arduino to do all the motor/servo controls.", "All low level control such as SLAM will done on board so will need to be capable of processing everything. ", "It will also be sending and receiving high level information from a main computer so will need to be network capable as well, we do currently have a Wireless modem on the robot itself so the SBC does not need built in wireless. ", "Cheers for any advice ", "Henry"], "answer": [" ", " ", "From my point of view, one of the best ways to find out which SBC to use is to take a look at what other people have published regarding their experiments with ROS. As an example, in ", " wiki page, there are some references to published works that include technical details about a working SBC based flying robot doing Visual SLAM and navigation. ", "From my own experience, you need a SBC with least a 1.0Ghz modern CPU + 2GB of RAM. If power consumption is not a concern, I would recommend Intel Core CPUs over Atoms.", "hey cheers", "Yea been trying to find SBC that other people have used, so many of them though haha", "Useful recommendation information, will keep that in mind", "hey cheers", "Yea been trying to find SBC that other people have used, so many of them though haha", " ", " ", "It sounds like you mainly want to process point cloud data?  therefore, perhaps a better question is \"how much PC do i need to run PCL?\". You could ask directly on the PCL (Point Cloud Library) mailing list?", " ", " ", " From my own experience, you need a SBC with least a 1.0Ghz modern CPU + 2GB of RAM. If power consumption is not a concern, I would recommend Intel Core CPUs over Atoms.\nSome good ones here  ", " . Never tried though so am not the voice of experience ...h ", " ", " ", " ", " ", "Some good ones here ", ". Never tried though so am not the voice of experience ... "], "url": "https://answers.ros.org/question/45950/single-board-pc-for-ros/"},
{"title": "Node Latency Questions", "time": "2014-03-16 12:19:12 -0600", "post_content": [" ", " ", "Hello everyone. I'm trying to understand the different latency issues that occur when using topics to stream data between nodes. ", "The default method is using TCPROS, which from how it was explained to me converts the data into XML data before it gets transmitted, and then has to be parsed back by the receiving nodes. Even if they're on the same processor. I'm going to conduct some of my own experiments to determine the latency, but since my setup is in a VM it might skew the results. So I'm wondering how bad the latency using this method is.", "From some further reading it looks like UDPROS is faster at transmitting the data. I don't really understand why though. While looking around trying to learn more I also came across the ETHAZSL library", "If I want to eliminate this issue altogether I could just make everything exist as nodelets under a single node, as they then just share memory. However there are some nodes that will need to exist in fully separate nodes (possibly over a network).", "So in summary I'd like to understand how each of these methods works on the backend, and what some good approaches are to reducing latency in the system. How bad is the latency using TCPIP vs UDP?", "Thanks!"], "answer": [" ", " ", "In general, ROS doesn't make any guarantees about latency, but in my experience it's generally quite low (<1ms locally) for TCPROS and UDPROS. Over a network, the biggest factor behind latency is the network itself.", "Both transports use a serialized binary format - not XML. That said, there is still some latency from the serialization process, but the designers took great pains to minimize that latency.", "In addition to trying to reduce the latency in the transport layer, you should spend a little time trying to understand how much latency you application can tolerate, and trying to measure the existing latency; these will tell you when you're reached \"good enough\" so that you can stop optimizing.", "Thanks for the info. Is there anywhere where I can get more technical documentation on the format that is used?", " ", " ", "For reference, we closed 1kHz loop over TCP with ", " between 2 computers on a fast local network in the cloud.  Here's ", " I am referring to.", "Was it a gigabit network? That's what I'm aiming to have on the vehicle I'm working on.", "It had a 10GB backbone.  I remember the actual time consumed by the network transport layer altogether was around ~0.25ms, given the nodes needed to do ~0.65ms of computing work.", "That sounds pretty reasonable. If I may ask what sort of computing work was being done? For my application there's going to be a bunch of data aggregation, and navigation.\n\nAlso, in your experience sort of latency I can expect locally between nodes?"], "answer_code": ["TCPNODELAY"], "url": "https://answers.ros.org/question/140651/node-latency-questions/"},
{"title": "set-up for real-time control in ROS", "time": "2013-04-19 06:04:32 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello,", "I am new to real-time control in ROS. When I looked up it in this wiki, integrating ROS and OROCOS and using Xenomai seems to be a good solution for it. I don't have a high level understanding of the advantage of using them. If you could share your insight into it with me, I would really appreciate it.", "I can enumerate several possible set-ups for real-time control in ROS although some of these may not make sense.", "We may scarcely be able to expect any good real-time performance on (1). How/why bad is it?\nWould be there any improvement in using (2) over (1)? \nHow would (3) be different to (4) in real-time performance? \nDo you have any other recommended set-up? \nDose it need to be broken down into some subdivided set-ups?", "Thank you very much in advance."], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "You can only get good realtime performance with option 4. The OROCOS RTT toolkit provides realtime behaviour but only on a kernel with a realtime extension. The other options can also give good performance but it will vary a lot. \nSee also this: ", "/ ", "A good example is how the realtime controllers of the PR2 work. ROS runs non realtime while the controller manager (which uses the RRT toolkit I think) is realtime. ", "\nFrom:", "Thank you very much for your answer. I'd also like to know the advantage of using OROCOS. If both a ROS node and a OROCOS component can be real-time priority threads, what makes OROCOS on Xenomai better perform than ROS on Xenomai?", "/ -- That mailing list has a lot of discussion about using ROS in a real-time system.", " ", " ", "In our research group we have used OROCOS for several years, and have hired some of the OROCOS developers as consultants to help us tune the real-time performance of our system.  I would recommend starting with the easiest to set up system (ROS on a vanilla kernel, components written in Python), and make the following upgrades ONLY as necessary after MEASURING your current performance, and ONLY to the parts of your system that NEED it:", "Never make any assumptions about where your bottleneck is; ALWAYS measure at every stage of development.  Rules of thumb like \"avoid dynamic memory allocation\" and \"only use low-level systems programming languages like C/C++/Fortran\" come secondary to this.  ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi, I've mainly used option 2. Theoretically options 2 and 4 should give you the same performance. The benefit of OROCOS is that it makes the integration easier, and makes it possible to switch between different real-time Kernels. OROCOS also provides many other tools to help develop real-time applications, such as methods for real-time modules to communicate with each other.", "So your list should include options 5,6, etc. for other real-time Kernels.", "e.g. Option 5, ROS on Preempt patched Ubuntu (this is what the PR2 uses)"], "answer_details": [" ", " ", " ", " ", "Make sure you have your components/nodes that need it running with real-time priority", "Upgrade your kernel to one with the PREEMPT_RT patches applied", "Dedicate a fraction of the cores on your CPU (if multicore) to your real-time processes, make sure the non-critical processes are launched on the other cores.", "Port components to C++ if delays within the components are the bottleneck  ", "Port components to Orocos if delays of message transmission between components is the bottleneck", "Install Xenomai kernel and recompile against it's API's  (at this point you are pretty deep into black magic territory; the only people using this are either operating obscenely tight control loops, or implementing safety-critical systems)", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "ROS on Ubuntu", "ROS on Xenomai patched Ubuntu", "ROS+OROCOS on Ubuntu", "ROS+OROCOS on Xenomai patched Ubuntu"], "url": "https://answers.ros.org/question/61162/set-up-for-real-time-control-in-ros/"},
{"title": "Gmapping outputs wrong map in tutorials", "time": "2014-05-25 22:01:49 -0600", "post_content": [" ", " ", " I followed the tutorials at the page of hector slam\nbut the final map seems not right, you can find my map at   ", "Are there any other parameters I need to set?", "I also use the bag \"Team_Hector_MappingBox_RoboCup_2011_Rescue_Arena.bag\" in gmapping following its tutorials ", "Also got the wrong map. ", "What should I do to solve this problem?\nWhat's more, how to generate a map simultaneously without a bag file?I just have a notebook and a hokuyo laser scanner(ulg-04lx), can any one give me a tutorial?"], "answer": [" ", " ", "The example bagfile we provide won\u00b4t work with gmapping as I don\u00b4t provide odometry data while walking ;) , which gmapping requires. Please edit your post with details such as", "operating system (e.g.    Ubuntu\n12.04 64 Bit or so)", "information on how you installed  hector_slam (from .debs or compiled\nyourself)", "I suspect that for some reason things run too slow, so the mapping process can\u00b4t keep up with the incoming data. This can happen when accidentally building code without optimization using catkin (see further info ", ") and/or when running the system on a very slow CPU.", "My notebok is Thinkpad X201 with Ubuntu 13.04.\nI installed hector_slam just as the tutorials(sudo apt-get install ros-hydro-hector-slam)\nWhen I run the bag, I checked out the CPU status, it consumes about 9% of CPU. What can I do in the next step?\nAnd what should I do if I want to generate a map without a bag file, just with my notebook and laser scanner. \nThank you for your reply!"], "answer_details": ["computer type ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/169576/gmapping-outputs-wrong-map-in-tutorials/"},
{"title": "Odometry with optical mouse?", "time": "2014-07-01 07:47:37 -0600", "post_content": [" ", " ", "Hi guys,", "I am currently looking at using odometry with the position given by an optical mouse.", "As I couldn't find any package/node driver that would allow me to have access to the position of the mouse, I was wondering if anyone had already looked into that and could give some feedback.", "Thanks in advance for the replies", "Very good approach ! Thinking outside the box at its best !", "can't really take any credit for that, many people have already successfully done what I'm trying to do :), just wondering if anyone has done with ROS", "Since we are thinking outside of the box here, you could use two mouse-based sensors: one on the robot, close to the floor, and another on your wheel's surface (mechanics permitting) , so you could measure skidding, implement cliff-avoidance, and some other good stuff.", "I am definitely interested in any idea that may help the odometry work better, although I do not see where you want to put the second sensor. Could you maybe draw a quick sketch or just elaborate a bit might be enough for me to understand?", "I think he means to fix the second mouse above one of the wheels, with very small distance between the mouse's sensor and the wheels surface (wheel has to be flat in this case)", "Hi, did you finally happen to work on this type of Odom? any good resources?"], "answer": [" ", " ", " ", " ", "Just adding the sketch as requested:", "@Mehidi: Your description is right. Indeed, that was what I meant.", "One comment: if the sensor on the base is too far from the floor, it may not work, or you will need lenses to focus/compensate the distance. ", "This also reduces the hight of your base and the possibility of going over obstacles.", "But it is cheap, and tends to be precise: optical mouses can work with resolution of typically 300 dpi.", "Quick explanation of sketch: gray wheel: free wheel. Black wheel (yellow core) drive wheels. \"Robot\" is mounted on a triangular base. ", "Ok, so let's think about the possibilities on what kind of data you can gather from this.", "The concept is simple physics: distance over time.", "2) If the linear speed of the wheel and\n    the linear speed detected by the\n    base sensor are the same, you robot\n    is moving. ", "3) If the linear speed of\n    the wheel is greater than the linear\n    speed of the \"floor\", your drive\n    wheel is skidding.  ", "4) If the linear\n    speed of the floor is greater than\n    the linear speed of the wheel, your\n    robot is being dragged or going down\n    a very slippery path.  ", "5) if the\n    base sensor detects movement in the\n    X and Y axes, your robot is either\n    making a curve, or spinning. You can\n    calculate angular speed of the\n    curve, if you think you need it. ", "6) If you detect that the wheel speed\n    is fluctuating, your robot may be on\n    some sort of low cohesion soil, like\n    pebbles.", "For tracking the wheels encoders seem more suited although they work the old fashioned style.", "With regard to odometry from mice: If I remember correctly from experiments a few year ago your comment about the distance is indeed a problem. Unless you have a very flat surface like a table top (i.e. the same a mouse works on) it won't work.", "Thanks for the sketch ccapriotti. So if the sensor on the wheel tells you that the wheel is rotating but the one at the back tells you that the robot isn't moving then you can tell that you're robot is skidding. Is that about right or can you actually get more out of your sensors?", "I mean except for the position from the back sensor and the rotation direction of the wheel from the other sensor", "Thanks ccapriotti for the explanation, you kept it nice and simple, I got it now. I strongly recommend the link to the first paper that Stefan Kohlbrecher gave, great practical solutions.", "2nd link actually", " ", " ", " ", " ", " If you put other lenses on the sensors you can actually get some distance between it and the surface. There\u00b4s been very interesting research on using mouse sensors for obstacle avoidance in UAVs as well as using multiple sensors on ground vehicles for highly accurate odometry. See for example  ", "  or  ", "  . ", " ", " ", "You can check this ", "and this ", "The Python code to get mouse coordinates on the screen is really short and you can easily integrate it into your project. In that case you maybe need to calibrate by yourself the ratio between Pixels/Meters by doing some tests. If the mouse gets stuck at the border of the screen you could also implement something to automatically take the mouse to the opposite border such that it could continue moving.", "If you are more into hacking hardware and getting directly odometry data without  plugging the mouse to a computer then check this :", "Thanks for the ideas Mehdi. My first approach was to get the real world coordinates of the mouse from the position of the cursor on the screen. But you can not use this method if you are ever to use more than one of these sensors, or at least I do not see how you could make it work.", "I am actually using the rosserial_arduino package with an arduino Uno to read the data out of the sensor. Seems to works well enough, though I haven't put it on an actual robot yet.", "Try it then , and keep us updated.", " ", " ", "What operating system will the mouse driver run on?  In Linux, reading mouse data is a simple file read: ", "You can just create a ROS node that reads the mouse file data into the I/O event data structure and then your code can just look at the type/code values from the event data structure to tell which direction and how far the mouse was moved and publish that information into an ROS Topic for other ROS nodes to consume.  ", "... and can tell speed too !", "Thanks for the the link, I will definitely look into that. Although I am focusing more on solutions including 'hacking' by connecting to the output of the sensor. It seems to be the way to go as I am probably going to buy off the shelves sensors in the future (as opposed to buying an actual mouse)."], "answer_details": ["If the wheel sensor reads\nmovement, you can actually calculate\nthe linear speed of the wheel. ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/181668/odometry-with-optical-mouse/"},
{"title": "Could not find a connection between 'ee_link' and 'world' because they are not part of the same tree.Tf has two or more unconnected trees.", "time": "2014-08-08 06:22:30 -0600", "post_content": [" ", " ", " ", " ", "Dear all,", "Willing to avoid the use of ", " (as I find the loop it implies does not look clean), I used ", " so as to create a callback on tf changed. Here is my code:", "This code returns me the following error (I used it against the ", ") before actually displaying a real position message 10 or 20 seconds after my node started:", "[ERROR] [1407495099.316372953, 56679.089000000]: \"world\" passed to lookupTransform argument source_frame does not exist. ", "[ERROR] [1407495100.333225598, 56680.089000000]: Could not find a connection between 'ee_link' and 'world' because they are not part of the same tree.Tf has two or more unconnected trees.", "[ERROR] [1407495100.333225598, 56680.089000000]: Could not find a connection between 'ee_link' and 'world' because they are not part of the same tree.Tf has two or more unconnected trees.", "... (10 - 20 seconds)", "[ INFO] [1407495286.595620292, 56859.293000000]: EE pos.x: [-0.642255]", "[ INFO] [1407495286.595733987, 56859.293000000]: EE pos.x: [-0.642255]", "[ INFO] [1407495286.595796002, 56859.293000000]: EE pos.x: [-0.642255]", "...", "I can understand a few error messages while waiting for the buffers to fill up but in my case I have to wait for 10 or 20 seconds before I get the real position message displaying relevant data. This looked weird to me so I created a hacky code shown below and which ", ", this code looks like this: ", "Now everything works fine and I get the expected position message without the 10-20 ...", "Your additional question doesn't really make sense. In your second example you are reading messages from a normal subscriber! This works perfectly fine. You are not forced to use a tf listener, it is just way more convenient to use.", "Well, when you use the message directly it's your job to figure out what the actual transform is. Some translation for some transform is 0 here. Which one is also contained in the message. That is what a transform listener does for you.", ": when I read from the subscriber (e.g. msg->transforms[0].transform.translation.x) I actually get 0 (not -0.642255 as I fakely wrote in code sample 2) I believe this is another problem though (for which I have no solution right now).", "This might just be another transform unless you are sure that there is just one transform in the whole system."], "answer": [" ", " ", " ", " ", "if you are having issues with transforms only appearing after a certain time. The usual problem is with your publishers not the listener. You should use view_frames and tf_monitor to debug your transforms sources. ", "  I'll also note that it's uncommon to want to take callbacks for every tf update. The best approach for holding data until it's available is to use the ", "Subsidiary Question: tf messages are just ", ", you can subscribe to them manually. However each message is only an incremental update which the listener aggregates and computes the net transform. You are looking up the first transform in a list of published transforms without checking the name so it's quite possible that the first transform in the list of transforms has value of 0 for the x translation component. ", " I recommend that you go through the full set of tf tutorials to understand how tf works better.  ", "Edit answering additional questions:", "Interpolation is actually more accurate than taking data from the closest timestamp as long as you can make the assumption of continuous values and a publish frequency which is above the natural bandwidth of the physical system. And you make the assumption that publishing data for every timestep is much to high bandwidth. ", "Interpolation is necessary because tf is aggregating information from several asyncronous sources across a distributed network. The standard models for using tf are: ", "You're first implementation is a partial implementation of the tf::MessageFilter however it does not queue data for later callbacks because you have a hard coded query. Not only that you are simply querying at time 0 so it is going to start reporting as fast as possible when the transform becomes available. But as you mention it does not solve you lag of 20 seconds. ", "Your second example is not querying the data that you want and does not demonstrate that the data is available. It demonstrates that there is some message coming over the tf topic. you do not even ...", "Something is still odd with the posted problem as both variants seem to only trigger the same functionality all the time either by the change handler or by the message callback.", ": I have cleaned my initial post, I believe things are clearer now, especially the way coordinate X is accessed (let me know). I admit things were a bit misleading in my initial post.", ": sure I went trough al the tutorials before posting ;) Now I refined my question for extra details on when to use which approach for tf data consumption... The update is marked ", ". Also what do you mean by ", "?", " The data you are accessing likely does not represent the information that you are trying to plot. Please go over the linked tutorials so you can understand the tf data model. The value msg->transforms[0].transform.translation.x represents the x value of the first transform published by whatever publisher last published which is likely not the computed transform you are looking for.", "Yep, I understand that: the value I put is actually a fake (I did not run the updated code sample 2) because what I am mainly interested in is a feedback on the ", " which I mention in my last edit :)", "Ok so I understand with tf it is common to work with data which is not sampled synchronously thanks to interpolation. To consume data you can have a loop timed independently of any data produced or you can use a callback on a data source using tf::MessageFilter. Now the tutorials and doxygen show...", "...a data source external to tf but what if you want this callback to be triggered when a frame inside tf updates? Do you have to default back to using ", "?"], "answer_details": ["Timing based - you want to know the latest available information during you update loop. To do this simply query with Time(0). Or if you're on a specific schedule you can use a specific time query. ", "Data source driven - you have received some data and want to transform it into a different coordinate frame. If the data is in the buffer already you can simply lookup at the timestamp of the data. Doing interpolation to evaluate as accurately as possible each link of the chain to form the transform. \n", "In simple cases you can simply wait for the transform to become available (waitForTransform) (useful for simple scripts which are single purpose and can block.) ", "If you are in a data intensive and latency intensive application you can buffer and hold the data until the required transform is available.  ", "this is what the tf::MessageFilter does for you using the onUpdate callbacks internally", " ", " ", " ", " "], "question_code": ["#include \"ros/ros.h\"\n#include <tf/transform_listener.h>\n\n\n////////////////////////////////////////////////////////////////////////////////\n// Callback when tf is updated.\nstruct MyTfCommunicator\n{\npublic:\n    void connectOnTfChanged()\n    {\n        m_tf_connection = m_tf_listener.addTransformsChangedListener(boost::bind(&MyTfCommunicator::onTfChanged, this));\n    }\n    void disconnectOnTfChanged()\n    {\n        if (m_tf_connection.connected())\n            m_tf_listener.removeTransformsChangedListener(m_tf_connection);\n    }\n\nprotected:\n    void onTfChanged()\n    {\n        try\n        {\n            // Read ee_link's position.\n            tf::StampedTransform transform;\n            m_tf_listener.lookupTransform(\"/ee_link\", \"/world\", ros::Time(0), transform);\n\n            // Read desired position.\n\n            // Compute the error.\n            ROS_INFO(\"EE pos.x: [%f]\", transform.getOrigin().x());\n        }\n        catch (tf::TransformException &ex)\n        {\n            ROS_ERROR(\"%s\", ex.what());\n            ros::Duration(1.0).sleep();\n        }\n    }\n\n    tf::TransformListener m_tf_listener;\n    boost::signals::connection m_tf_connection;\n};\n\n\n////////////////////////////////////////////////////////////////////////////////\n// This node computes the effector's positional and rotational errors and publishes them.\nint main(int argc, char **argv)\n{\n    // Pass argc, arv to init() to allow cmd line remapping.\n    ros::init(argc, argv, \"lpc_error\");\n\n    // Handle to access ROS.\n    ros::NodeHandle n;\n\n    // Setup on tf update callback and spin.\n    MyTfCommunicator tfCommunicator;\n\n    tfCommunicator.connectOnTfChanged();\n    ros::spin();\n    tfCommunicator.disconnectOnTfChanged();\n\n    // Exit.\n    return 0;\n}\n", "#include \"ros/ros.h\"\n#include \"tf2_msgs/TFMessage.h\"\n\n\nvoid onTfChanged(const tf2_msgs::TFMessage::ConstPtr& msg)\n{\n    // Compute the error.\n    ROS_INFO(\"EE pos.x: [%f]\", msg->transforms[0].transform.translation.x);\n}\n\n\n////////////////////////////////////////////////////////////////////////////////\n// This node computes the effector's positional and rotational errors and publishes them.\nint main(int argc, char **argv)\n{\n    // Pass argc, arv to init() to allow cmd line remapping.\n    ros::init(argc, argv, \"lpc_error\");\n\n    // Handle to access ROS.\n    ros::NodeHandle n;\n\n    // Setup on tf update callback and spin.\n    ros::Subscriber sub = n.subscribe(\"tf\", 3, onTfChanged);\n\n    ros::spin();\n\n    // Exit.\n    return 0;\n}\n"], "url": "https://answers.ros.org/question/189469/could-not-find-a-connection-between-ee_link-and-world-because-they-are-not-part-of-the-same-treetf-has-two-or-more-unconnected-trees/"},
{"title": "Stage Tutorial Error: Laser Model Type Not Found", "time": "2013-03-22 05:01:41 -0600", "post_content": [" ", " ", "Ubuntu 1210\nNvidia GeForce GT640M\nGroovy", "Followed Tutorial in:\nhttp://www.ros.org/wiki/stage/Tutorials/IntroductiontoStageControllers", "Successfully downloaded: http://mobotica.googlecode.com/files/stage_controllers.tar.gz \nSuccessfully extracted\nSuccessfully found by rospack find", "Filesystem structure for stage_controller looks fine\nBut when I try to do: rosrun stage stageros /home/vmrguser/rosGWS/sandbox/stage_controllers/world/roomba-wander.world ", "Or any other world configuration, I get the following behavior:\n- Stage simulation window opens and then closes\n- Console says: \nerr: Model type laser not found in model typetable (/tmp/buildd/ros-groovy-stage-1.6.7/debian/ros-groovy-stage/opt/ros/groovy/stacks/stage/build/stage/libstage/world.cc CreateModel)", "Not sure why this happens", "Hi, Have you solved the problem for using stage simulations, i have exactly the same problem, I am getting the following message ", "err: Model type laser not found in model typetable (/tmp/buildd/ros-groovy-stage-1.6.7/debian/ros-groovy-stage/opt/ros/groovy/stacks/stage/build/stage/libstage/world.cc"], "answer": [" ", " ", " ", " ", "Groovy has Stage 4, which no longer provides the ", " device simulation. It has been replaced by a generic ", ". ", "It looks like the ROS stage wiki page documentation is out of date.", "Old world files need to be ported to the new interface.", "how can one port an old world file to the new interface?", "You will have to consult the Stage documentation for that. ", " ", " ", "have a look at this q/a"], "url": "https://answers.ros.org/question/58948/stage-tutorial-error-laser-model-type-not-found/"},
{"title": "bash problems on pr2, robot command not found", "time": "2014-09-24 10:42:37 -0600", "post_content": [" ", " ", " ", " ", "Hey guys, I was having some issues with the kinect on the pr2 and so I had to uninstall and reinstall openni. I now get the kinect working (roslaunch / ros commands in general work), but somehow I think I messed up the bash configuration on the Pr2 machine. Now all commands robot start/stop, robot users etc, are not found.", "I tried source again /opt/ros/groovy/setup.bash but nothing seems to work.", "Also, right when I ssh into the pr2 machine AND when I type /bin/bash, I get the following:", "-bash: /usr/bin/check-ssh-keys: No such file or directory\nSourcing /etc/ros/setup.bash\n-bash: /etc/ros/setup.bash: No such file or directory ", "Do you guys know any workaround for this issue?"], "answer": [" ", " ", "You didn't just mess up bash. It sounds like you uninstalled most of the system management utilities that are required on the PR2. Did the package manager ask you something like:", "(You should never answer yes to that prompt without consulting a PR2 support representative first).", "You should be able to get back to mostly working by reinstalling the pr2-core-groovy and pr2-network packages:", "You should probably follow that up by running a systemcheck to make sure things are functioning properly.", " ", " ", " ", " ", "Yeah it was my fault. I got everything working with above commands, but now joystick doesn't work:", "I have the robot commands now but because of the previous issue, joystick doesn't work and some stuff is missing on diagnostics. for example:", "I tried to reinstall joystick drivers then, and I got the following:", "So I did apt-get -f install and I think it installed livudev-dev and everything.", "So I tried installing again the drivers and all went well:", "But  diagnostic still says joystick is stale and Joystick driver is missing so I cant control the Robot ...", "there are some weird ...", "Have you tried running ", " as it suggests?", "Given that you uninstalled and had to reinstall the joystick drivers, you probably need to restart the ps3joy service: ", "I tried all above, No work.", "I then tried apt-get install --reinstall ros-groovy-joystick-drivers\nand no work. I tried reinstall pr2-core-groovy and network, stilll no work. i dont what else to do.", "You should probably contact PR2 support.", "Yeah, I just installed pr2-* and ros-groovy-pr2-* and now my diagnosis is all green. Thanks a lot Hendrix!", "That's almost certainly a bad idea, because it pulls in many packages, and may even try to install conflicting packages, or packages that shouldn't be installed on a PR2. It would be much better to look at the set of packages that weren't installed, and only install them if they're necessary.", "This is why I suggest that you contact PR2 support; they understand which packages are necessary and can walk you through the debugging process."], "answer_code": ["You are about to do something potentially harmful.\nTo continue type in the phrase 'Yes, do as I say!'\n", "sudo apt-get install pr2-core-groovy pr2-network\n", "Power system: error\nIBPS 0 to 3 are missing, smart battery all missing. the only ok is the power board 1056.\n    More examples:\nDevices: error\nJoystick: Stale\nJoysting driver status: missing\n", "$ sudo apt-get install ros-groovy-joystick-drivers\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nros-groovy-joystick-drivers is already the newest version.\nYou might want to run 'apt-get -f install' to correct these:\nThe following packages have unmet dependencies:\n ros-groovy-oculus-sdk : Depends: libudev-dev but it is not going to be installed\nE: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).\n", "$ sudo apt-get install ros-groovy-joystick-*\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'ros-groovy-joystick-drivers' for regex 'ros-groovy-joystick-*'\nros-groovy-joystick-drivers is already the newest version.\nThe following packages were automatically installed and are no longer required:\n  config-package-dev libpcl-surface-1.7-dev\n  ros-hydro-opencv2 syslinux dnsmasq-base\n  libnet-ssleay-perl ros-hydro-rosbag ros-hydro-cv-bridge\n  chrony libgraphicsmagick3 ckermit pr2-grant\n  libpcl-surface-1.7 libpcl-features-1.7-dev\n  libcxsparse2.2.3 octave3.2 ros-hydro-roscpp\n  ros-hydro-rosgraph libfile-copy-recursive-perl\n  ros-hydro-image-proc ros-hydro-roscpp-serialization\n  ros-hydro-smclib ros-hydro-roslaunch libglpk0 libarpack2\n  ros-hydro-rospack libpcl-outofcore-1.7\n  libpcl-recognition-1.7 ros-hydro-message-runtime\n  pr2-ctr350 libcholmod1.7.1 libxml-parser-perl liburi-perl\n  unionfs-fuse ros-hydro-message-filters ros-hydro-roslib\n  libpcl-geometry-1.7-dev libhtml-parser-perl\n  ros-hydro-rosnode update-inetd\n  libpcl-registration-1.7-dev pr2-sendhwlog libv8-3.7.12.22\n  libhttp-daemon-perl ros-hydro-rosmsg rlinetd\n  ros-hydro-tf2-msgs ros-hydro-rosout\n  ros-hydro-camera-info-manager screen ros-hydro-pcl-msgs\n  libfont-afm-perl pr2-netconsole libhttp-negotiate-perl\n  libfile-listing-perl libhtml-form-perl\n  libpcl-features-1.7 libfftw3-3 syslinux-common\n  libvtk5.8-qt4 python-urlgrabber bluez-hcidump\n  ros-hydro-nodelet ifplugd ros-hydro-roswtf\n  ros-hydro-actionlib libaprutil1-ldap apache2-mpm-prefork\n  cdbs libpcl-tracking-1.7 netperf\n  ros-hydro-camera-calibration-parsers apache2-utils\n  ros-hydro-topic-tools libpcl-tracking-1.7-dev\n  libccolamd2.7.1 ros-hydro-cpp-common\n  libpcl-search-1.7-dev apache2 tcsh libhtml-tree-perl\n  ros-hydro-gencpp intltool libencode-locale-perl\n  libhttp-date-perl ros-hydro-actionlib-msgs\n  libmailtools-perl snmpd ros-hydro-diagnostic-msgs\n  pr2-chrony liblwp-protocol-https-perl apache2.2-common\n  ros-hydro-rostest libnetfilter-conntrack3 timelimit snmp\n  libpcl-keypoints-1.7 pr2-sysros libpcl-apps-1.7\n  ros-hydro-tf2-ros ros-hydro-rostime libparpack2\n  ros-hydro-rosgraph-msgs libhttp-cookies-perl\n  libpcl-keypoints-1.7-dev libhttp-message-perl\n  ros-hydro-geometry-msgs python-svn\n  ros-hydro-message-generation ros-hydro-rosservice\n  smartmontools pr2-ckill python-scour ros-hydro-genmsg\n  libaprutil1-dbd-sqlite3 libflann1 ros-hydro-class-loader\n  libapache2-mod-python libcolamd2.7.1 ros-hydro-rosclean\n  nodejs libnet-http-perl apache2.2-bin ros-hydro-xmlrpcpp\n  ros-hydro-rosmaster libgraphicsmagick++3 octave3.2-common\n  ros-hydro-rosunit libpcl-octree-1.7-dev libcap-dev\n  ros-hydro-genlisp libpcl-people-1.7 nfs-kernel-server\n  libhtml-format-perl ros-hydro-image-geometry libev4\n  libpcl-common-1.7-dev pr2-wrt610n ros-hydro-catkin\n  ros-hydro-rosbag-storage ros-hydro-bondcpp\n  libpcl-search-1.7 libpcl-sample-consensus-1.7-dev\n  libpcl-common-1.7 ros-hydro-roscpp-traits pr2-repo\n  pr2-netboot dnsmasq libdaemon0 libsocket6-perl libc-ares2\n  ros-hydro-tf zsh ros-hydro-bond executable-selector\n  pr2-stress libpcl-filters-1.7 pr2-repo-pr2\n  ros-hydro-image-transport ros-hydro-dynamic-reconfigure\n  libpcl-kdtree-1.7-dev dh-translations libhtml-tagset-perl\n  libpcl-filters-1.7-dev ros-repo ros-hydro-rospy libkms1\n  libwww-perl ros-hydro-nodelet-topic-tools fuse-utils\n  libvtk5-qt4-dev ros-hydro-rosbuild ros-hydro-tf2-py\n  libflann-dev ros-hydro-rosconsole ros-hydro-std-msgs\n  libpcl-registration-1.7 ros-hydro-genpy ros-hydro-tf2\n  libio-socket-ssl-perl libqrupdate1 libpcl-1.7-doc distcc\n  libpcl-sample-consensus-1.7 libwww-robotrules-perl\n  liblwp-mediatypes-perl libpcl-segmentation-1.7\n  libpcl-octree-1.7 ros-hydro-rosparam\n  ros-hydro-console-bridge ros-hydro-sensor-msgs\n  libpcl-kdtree-1.7 ros-hydro-rostopic\n  ros-hydro-diagnostic-updater texinfo pr2-systemcheck\n  libio-socket-inet6-perl atftpd\n  libpcl-segmentation-1.7-dev pr2-bios-bmc-images\n  ros-hydro-pluginlib\nUse 'apt-get autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 252 not upgraded.\n", "apt-get -f install", "sudo service ps3joy restart"], "url": "https://answers.ros.org/question/193393/bash-problems-on-pr2-robot-command-not-found/"},
{"title": "RGB and depth image to point cloud data", "time": "2014-10-10 11:35:29 -0600", "post_content": [" ", " ", " ", " ", "My System : Ubuntu 12.04, ROS Fuerte, Python", "Goal: Want to combine rgb and depth image to point cloud data. I am using depth_image_proc/point_cloud_xyzrgb nodelet to achieve this. Just to test it, I used the kinect topics depth_registered/image_rect, rgb/image_rect_color, and rgb/camera_info and it works (I can visualize it in RViZ). ", "Now, the issue is, I subscribe the kinect topic rgb/image_rect_color and use Python OpenCV to change some pixel colors and publish it back as a new topic. When I use depth_image_proc/point_cloud_xyzrgb with this new topic and try to get a point cloud from the new rgb image and original depth image, it does not output anything. I can view the changed image from the new topic in RViZ but the point cloud is not generated.", "To test it further with a simpler problem, I just subscribe to the rgb/image_rect_color, do no processing at all, and publish the same image with a different topic name. Still, it does not work. Is this a time synchronization issue. Any pointers will be helpful. ", "Thanks\n-gilmour"], "answer": [" ", " ", " ", " ", "It could definitely be a time synchronization issue, since depth_image_proc uses an approximate time synchronizer with a queue size of 5 (Default). It could still work, assuming you're:", "The issue with the queue is that if you take to long to publish your modified message, the associated depth image may have already been overwritten on the queue. With a queue size of 5, and an fps of 30, that would only take a fraction of a second. Try increasing the queue size on depth_image_proc to something significantly larger and see if that helps.", "To process your images significantly faster, try developing your node in C++ as a nodelet, and load the nodelet into the kinect's nodelet manager. That way, your node will be running in the same process as both the producer and the consumer of your image data, which should allow you to bypass the expensive ", " process normally incurred for node-to-node messaging.", "Thanks a lot for the pointers. There were two issues : even though I was not explicitly modifying the time-stamp, when I use CV-bridge, it overwrites the original time stamp values. So, I had to manually assign them again. And increasing the queue size helped too."], "answer_details": ["modifying and publishing very quickly", "not modifying the timestamp of the original message", " ", " ", " ", " "], "url": "https://answers.ros.org/question/194731/rgb-and-depth-image-to-point-cloud-data/"},
{"title": "how to send a message?", "time": "2014-10-30 09:16:42 -0600", "post_content": [" ", " ", " ", " ", "Hello, I'm Furuike.", "I made a program that control turtlebot.\nBut the program does not work although its publications, subscriptions, services and connections are the same as keyop (package : kobuki_keyop    file name : keyop.launch).", "\nCould you tell me what is the problem with my program?", "This is my program. ", "And this is the information. ", "Can you post the code without any <br> symbols? Also what exactly does not work?", "if you do:", "do you get something on your console?", "I'm sorry. I didn't notice that it start a new line without br if I choose preformatted text.", "It return the well formatted reaction if I push two times when I do rostopic echo."], "answer": [" ", " ", "Your code isn't well-formatted, so it's pretty difficult to read and understand, but my best guess is that you aren't doing the keyboard I/O properly. I suspect there's nothing wrong with the ROS parts of your code.", "You can test if you have the I/O wrong by trying to use the ", " command to quit. If you push ", " by itself, I suspect nothing will happen. If you push ", " 2-3 times and then push enter, I suspect your program will exit.", "The trouble here is that, by default, terminal input is line-buffered and \"cooked\", meaning that your program doesn't get any input until the user has entered an entire line and pushed enter, and that the terminal will strip or reinterpret nonprinting characters such as arrow keys and the control key.", "There are lots of good resources for understanding non-blocking terminal input. You can either grab your favorite systems programming book, or consult Google and StackOverflow. ", " seems to do a good job of describing the basics.", "And don't forget to have your program put your terminal back the way was on exit. The terminal device is shared between all process that run in that shell, and programs like ssh will fail in strange and unusual ways if you leave the terminal in non-blocking mode. (The ", " will also put it back to normal).", "You're right. The program exit if I push q 2-3 times and then push enter. ", "\nI'll study about the terminal input. Thank you for your help. I will post the result after I rewrite the program.", "Special thanks to you!!\nI corrected the part you pointed out (and the code of the arrow key) and finally succeeded."], "question_code": [" #include <ros/ros.h> \n #include <geometry_msgs/Twist.h> \n #include <kobuki_msgs/MotorPower.h>\n #define  UP            0x48    /* \u2191 */ \n #define  DOWN          0x50    /* \u2193 */\n #define  LEFT          0x4D    /* \u2192 */ \n #define  RIGHT         0x4B    /* \u2190 */\nint main(int argc, char** argv){ \n  ros::init(argc, argv, \"key\"); \n  ros::NodeHandle n1; \n  ros::NodeHandle n2; \n  ros::Publisher velocity_pub = n1.advertise<geometry_msgs::Twist>(\"/mobile_base/commands/velocity\", 3);; \n  ros::Publisher motor_pub = n2.advertise<kobuki_msgs::MotorPower>(\"/mobile_base/commands/motor_power\", 1); \n  int i; \n  while (ros::ok()) \n    { \n      geometry_msgs::Twist msg_velocity; \n      kobuki_msgs::MotorPower msg_motor; \n      char ss_velocity; \n      int ss_motor; \n      if(getchar()=='e'){\n    ss_motor = 1;\n    msg_motor.state = ss_motor; \n    motor_pub.publish(msg_motor); \n      } \n      if(getchar()=='d'){ \n    ss_motor = 0; \n    msg_motor.state = ss_motor; \n    msg_velocity.linear.x=0; \n    msg_velocity.angular.x=0; \n    msg_velocity.linear.y=0; \n    msg_velocity.angular.y=0; \n    msg_velocity.linear.z=0; \n    msg_velocity.angular.z=0; \n    motor_pub.publish(msg_motor); \n    velocity_pub.publish(msg_velocity); \n      }  \n      if(ss_motor == 1){ \n    switch(getchar()){ \n    case UP :  msg_velocity.linear.x += 0.05;  break; \n    case DOWN : msg_velocity.linear.x -= 0.05; break; \n    case RIGHT : msg_velocity.angular.z += 0.33; break; \n    case LEFT : msg_velocity.angular.z -= 0.33; break; \n    } \n    velocity_pub.publish(msg_velocity); \n      } \n      if(getchar()=='q'){ \n    return 0; \n      } \n      ros::spinOnce(); \n    } \n  return 0; \n}\n", "Node [/key] \nPublications: \n * /rosout [rosgraph_msgs/Log]\n * /mobile_base/commands/velocity [geometry_msgs/Twist] \n * /mobile_base/commands/motor_power [kobuki_msgs/MotorPower] \nSubscriptions:  \n * /clock [rosgraph_msgs/Clock] \nServices:  \n * /key/set_logger_level \n * /key/get_loggers \ncontacting node http://IMI-T400s:60686/ ... \nPid: 16016 \nConnections: \n * topic: /rosout \n    * to: /rosout \n    * direction: outbound \n    * transport: TCPROS \n * topic: /mobile_base/commands/velocity \n    * to: /gazebo \n    * direction: outbound \n    * transport: TCPROS \n * topic: /mobile_base/commands/motor_power\n    * to: /gazebo \n    * direction: outbound \n    * transport: TCPROS \n * topic: /clock \n    * to: /gazebo (http://IMI-T400s:48512/) \n    * direction: inbound \n    * transport: TCPROS\n", "rostopic echo /mobile_base/commands/velocity\nrostopic echo /mobile_base/commands/motor_power\n"], "answer_code": ["q", "q", "q", "stty sane"], "url": "https://answers.ros.org/question/196319/how-to-send-a-message/"},
{"title": "How to get the Pixy camera working on ROS ?", "time": "2014-05-15 07:33:07 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to get the Pixy camera working with ROS (Hydro) on Ubuntu 12.04 x64.", "If you haven't heard about the Pixy yet, it's a camera with embedded vision processing that does mainly fast blob tracking. ", "It is provided with a software and a GUI written in C++ and Qt, called PixyMon. So I thought it could be great to convert it to a ROS package ! \nOf course I could connect the Pixy to an Arduino and get some data from a serial link, but I'd like to use the direct USB link to the Pixy, no arduino, and have all the capability of PixyMon within ROS.\nPixyMon allows you to interact with the Pixy, change its parameters (lightness, contrast...), get the camera image, detected blobs position/size, record new colors...so I'd like to have a ROS wrapper for all of this.", "So far I can build and link and execute PixyMon as a ROS package, it detects my Pixy but cannot connect to it, PixyMon terminal shows :", "and the terminal from where I execute the node shows :", "And they all repeat. A click on Parameters button give a segmentation fault and crash.\nSo somewhere something is not using the right libraries, or not the right Qt version, or the different parts of the project are not linked properly I suspect.", "So I basically created an empty Qt package, and paste PixyMon (that means common and host folders) inside src folder.\nMy package.xml contains this for Qt project :", "My CMakeLists.xml is as follow (I removed all comments) :", "I can build and execute the normal Pixymon project provided for the Pixy and it workd well, it is building using Qt5 with the .pro file. But it seems ...", "This looks sort of reasonable. Without hacking on it a lot, it's hard to say for sure. I suspect most of your build process is fine, or you would have symbol errors at link time or at startup. Have you tried running your node in gdb? Does it have a debug mode that you can enable?", "Thank you for your reply. Yes I agree the build process should be ok. I tried to run the node in gdb but it doesn't give me more information. Actually, all the error written in the terminal are written by qDebug calls. The error \"libusb_bulk_write -7\" cause the problem, I'll try to trace it back."], "answer": [" ", " ", "I too am very interested and have approached this in a 'hack' sort of way as my 1st cut but hope to implement a true ROS node with Pixy as my next step.  I am not suggesting this is clean, I am only putting it forth as an unclean alternative that some may benefit from till a clean solution is devised.", "My solution is that I have hacked the linux (Raspberry PI) hello code to stuff it's results to shared memory segment.  My current ROS node then inspects this shared memory with both processes only accessing the shared memory segment using a system V semaphore so it is safe for the producer (hello code) and consumer (ROS node).  My ROS node then inspects memory and passes what it finds over via a ROS topic to my main node which is the end consumer of the Pixy object recognition data.   All this then means the pixy.h and pixydef.h includes and a definition of the named shared memory segment of my own .h file are all the ROS node need see as it compiles.  This works but I agree is a bit of a 'hack' if you will as I have to start the pixy converted 'hello' process outside of my roslaunch config file.", "Just a thought and maybe it will be a reasonable short term solution for others in the mean time as we strive to get the code to operate as a true ROS node which of course is the 'right' way to deal with this issue.", " ", " ", "Hi,  I've taken the recently released libpixyusb from Charmed Labs' pixy repository & made a ROS node around it.  At the time of this writing, it only supports USB streaming of plain signiture & color-code block data.  It will also subscribe to a servo command topic and move the servos for pan-tilt... though I don't have any demo for that (yet).  ", " to my source code.  I welcome any feedback and/or collaboration from interested users.  I've already created a to-do list on the github page of features I'd like to add.  Please feel free to add suggestions there.", " ", " ", "You might check out libpixyusb, which makes direct communictation with Pixy over USB fairly easy ---", " ", " ", "I'm not sure if you're running Windows or Linux.  Linux is well-tested.  Windows isn't so much (for libpixyusb). "], "question_code": ["Pixy detected.\nerror: Unable to connect to device.\n", "libusb_bulk_write -7\nlibusb_bulk_write -7\ninterpreter finished\ndestroying interpreter...\ndone\nlibusb_bulk_write -7\nlibusb_bulk_write -7\n", "<buildtool_depend>catkin</buildtool_depend>\n<build_depend>qt_build</build_depend>\n<build_depend>roscpp</build_depend>\n<build_depend>libqt4-dev</build_depend>\n<run_depend>qt_build</run_depend>\n<run_depend>roscpp</run_depend>\n<run_depend>libqt4-dev</run_depend>\n", "cmake_minimum_required(VERSION 2.8.9)\nproject(pixymon)\n\nfind_package(catkin REQUIRED COMPONENTS qt_build roscpp)\nfind_package(Qt4 COMPONENTS QtCore QtGui QtWidgets)\ninclude_directories(\n    ${catkin_INCLUDE_DIRS} \n    src/common \n    src/host/pixymon\n    /usr/include/libusb-1.0)\nlink_directories(\n    src/common \n    src/host/pixymon)\ncatkin_package()\n\nfile(GLOB QT_FORMS RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} src/host/pixymon/*.ui)\nfile(GLOB QT_RESOURCES RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} src/host/pixymon/*.qrc)\nfile(GLOB_RECURSE QT_MOC RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} FOLLOW_SYMLINKS src/host/pixymon/*.h)\nQT4_ADD_RESOURCES(QT_RESOURCES_CPP ${QT_RESOURCES})\nQT4_WRAP_UI(QT_FORMS_HPP ${QT_FORMS})\nQT4_WRAP_CPP(QT_MOC_HPP ${QT_MOC})\n\nfile(GLOB_RECURSE QT_SOURCES RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} FOLLOW_SYMLINKS src/common/*.cpp)\nfile(GLOB_RECURSE QT_SOURCES RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} FOLLOW_SYMLINKS src/host/pixymon/*.cpp)\n\nadd_executable(pixymon ${QT_SOURCES} ${QT_RESOURCES_CPP} ${QT_FORMS_HPP} ${QT_MOC_HPP})\nadd_library(chirp src/common/chirp.cpp)\nadd_library(qqueue src/common/qqueue.cpp)\nadd_library(blob src/common/blob.cpp)\nadd_library(blobs src/common/blobs.cpp)\ntarget_link_libraries(blobs blob)\ntarget_link_libraries(pixymon ${QT_LIBRARIES} ${catkin_LIBRARIES} chirp qqueue blobs /usr/lib/x86_64-linux-gnu/libusb-1.0.so)\ninstall(TARGETS pixymon RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION})\n"], "url": "https://answers.ros.org/question/165396/how-to-get-the-pixy-camera-working-on-ros/"},
{"title": "How do you calibrate the extrinsic parameters?", "time": "2014-10-28 03:28:48 -0600", "post_content": [" ", " ", "I was trying to calibrate my camera mounted on the robot base so that I can get the transform from base_link to camera_link. i was looking for efficient and fast ways to do this and I found the following ", " which was useful because I could fix my pattern on the robot base directly and use a mirror to make it visible to the camera.", "After doing some tests with that, the results were totally wrong. I also tested with a webcam mounted on my pc and I couldn't get good results. ", "How do the ROS community proceed with the extrinsic calibration of a monocular camera?"], "answer": [" ", " ", " ", " ", "You might be interested in the ", " package from the ROS-Industrial community. Also: ", ".", "It can be a bit involved to get it working at the moment, but that is being worked on. If you find you need help with anything, don't hesitate to send a mail to the ROS-Industrial ", " (or ask here on ROS Answers of course).", "That doesn't work with melodic.", "You're commenting on a 5 year old Q&A. I'm not surprised that pkg may not work on Melodic.", " ", " ", "Hello,", "I posted a similar question  a while ago, and not having found a good solution, i coded one myself (unaware of ros industrial package).\nMy solution needs a way to recover a frame through the camera, like ar_pose, plus a marker that is attached to the end effector (no need to know where, only that does not move during calibration).\nI checked it with Asus Xtion + ar_kinect\nthe solution gives the camera to base frame transformation (plus the position of marker w.r.t. the robot link where the marker is attached, a data that is normally discarded)", "see ", "Hello ", "I've been using your tool to calibrate a static camera with respect to robot coordinates.\nCould you help me out with some resources to understand the math behind optimization process to minimize the error", "I have been consistently getting some translation error in one of the axes", "No offense but this has quite poor documentation, it's quite time consuming to get anything working.", " ", " ", "Hello, ", "I use this GUI based ", " for intrinsic and extrinsic camera parameters. I've used it a lot and have always had good results. I've only used it for pre-processing, I've never used it during real-time applications. ", "I really hope they wrote that in a cross platform way", "The code was written for OpenCV 2.1 and was tested mostly on win32 (Windows XP, Windows Vista, Windows 7) platform. System Requirements: .Net Framework 3.5 SP1", "Perhaps Wine or Mono could help here, but apparently no guarantees."], "url": "https://answers.ros.org/question/196096/how-do-you-calibrate-the-extrinsic-parameters/"},
{"title": "PCL Header to Sensor_msg convertion problem", "time": "2015-01-21 13:42:12 -0600", "post_content": [" ", " ", " ", " ", "Guys,", "I have this code snippet (I wrote it):", "Where cloud_msg is defined as:", "But after I run the code I get this error:", "Is there a library I am missing? Or a new format?\nThis code was running in Fuerte, and now I am trying to make it run in Indigo."], "answer": [" ", " ", " ", " ", "There were incompatible changes to the way ROS uses PCL between Groovy and Hydro.", "Please consult the ", " for details.", ": since Groovy there is a new ", " package that may help with your problem. The ", ".", "I still can't find the way to convert sensor_msgs::ImageConstPtr& to pcl::PointCloud<pcl::pointxyz>\nEvery function I sue (such as pcl::toPCL) always gives me an error that there is not matching function."], "question_code": ["#include \"depth_image_proc.h\"\n#include <ros/ros.h>\n#include <image_transport/image_transport.h>\n#include <pcl_ros/point_cloud.h>\n#include <pcl/point_types.h>\n#include <sensor_msgs/image_encodings.h>\n#include <image_geometry/pinhole_camera_model.h>\n#include <sensor_msgs/PointCloud2.h>\n\ngeneratePCL::generatePCL(const sensor_msgs::ImageConstPtr& depth_msg,          \n        const sensor_msgs::CameraInfoConstPtr& info_m)                         \n{\n    cloud_msg.header = depth_msg->header;                                      \n    cloud_msg.height = depth_msg->height;                                      \n    cloud_msg.width  = depth_msg->width;                                       \n    cloud_msg.is_dense = false;\n    cloud_msg.points.resize(cloud_msg.height * cloud_msg.width);                                                 \n    convert(depth_msg);\n}\n", "pcl::PointCloud<pcl::PointXYZ> cloud_msg;\n", "depth_image_proc.cpp: In constructor \u2018generatePCL::generatePCL(const ImageConstPtr&, const CameraInfoConstPtr&)\u2019:\ndepth_image_proc.cpp:16:19: error: no match for \u2018operator=\u2019 (operand types are \u2018pcl::PCLHeader\u2019 and \u2018const _header_type {aka const std_msgs::Header_<std::allocator<void> >}\u2019)\n  cloud_msg.header = depth_msg->header;\n                   ^\n"], "url": "https://answers.ros.org/question/201465/pcl-header-to-sensor_msg-convertion-problem/"},
{"title": "Using Kinect for both navigation and user tracking", "time": "2013-05-16 05:44:58 -0600", "post_content": [" ", " ", " ", " ", "Hi there!", "I'm trying to use one Kinect for two tasks: navigation and user tracking. At first firing navigation simultaneously with user_tracking doesn't work, and I couldn't find any information if anyone tried that?", "Navigation and tracking work if I run them separately.", "Any help would be appreciated!", "da-na", "Please be more specific as to \"doesn't work\". Also, what user_tracking package are you using?", "I am working on the same type of project.  What packages are you using?", "I am using ", "for the Kinect, ", " for skeleton tracking, and ", " for Pioneer3DX.", "I have not yet started on the navigation piece yet, but I figure I would start with ", ".  I figure the 'goal location' could subscribe to the tf topic of the skeleton with an offset.  I'll keep you posted with updates and you will see my questions soon."], "answer": [" ", " ", " ", " ", "ROS nodes are a publisher/subscriber system - the publishing nodes (should) operate normally no matter how many subscribers are consuming the data downstream. Are you pushing the limits of your hardware's processing capability? ", "Check ", " output to see the CPU load. An easy way to reduce processing load with Kinect or other RGBD cameras is to reduce the resolution using ", ".", "How are you using the Kinect output in navigation? Instead of using the pointcloud as a datasource, try feeding it through ", " or use the depth image via ", ", both options will reduce the CPU load significantly.", "Please add launch files and debug output to your question.", " ", " ", "I have gotten my robot to be able to navigate and user track.  I am utilizing ROS Hydro on two Ubuntu 12.04 computers.  One is the base computer.  The other is on my Pioneer3-DX with the Microsoft Kinect.  In order to do navigation and user-tracking, I run a launch file to start the following nodes: ", ", ", ", ", ", ", ", ", ", and ", ".  ", "I then worked through the ", " and ", ", to learn to set up the appropriate navigation parameter files and how to use gmapping.  I then used the following ", " file to start navigation and this ", " to control the robot.  ", "The robot is able to navigate while tracking users.  ", "  I am not sure what problems you are having.    As mentioned before, please post your launch files and any errors you have.  I will try to help you if I can.", " ", " ", "does it work? i have the same problem", "please don't post me-too answers. constructive additions can be added as comments, otherwise please upvote to display your interest and keep the board less cluttered."], "answer_code": ["top", "dynamic_reconfigure", "pointcloud_to_laserscan", "depthimage_to_laserscan", "p2os_driver", "p2os_enableMotors", "p2os_urdf", "openni_launch", "openni_tracker", "depthimage_to_laserscan", "move_base.launch", "<launch>\n\n  <!-- Start gmapping -->\n    <node  pkg=\"gmapping\" name=\"slam_gmapping\" type=\"slam_gmapping\" output=\"screen\" />\n\n  <!-- Start navigation -->\n    <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n        <rosparam command=\"load\" file=\"$(find follow_me_2dnav)/launch/costmap_common_params_p3dx.yaml\" ns=\"global_costmap\"/>\n        <rosparam command=\"load\" file=\"$(find follow_me_2dnav)/launch/costmap_common_params_p3dx.yaml\" ns=\"local_costmap\"/>\n        <rosparam command=\"load\" file=\"$(find follow_me_2dnav)/launch/local_costmap_params.yaml\"/>\n        <rosparam command=\"load\" file=\"$(find follow_me_2dnav)/launch/global_costmap_params.yaml\"/>\n        <rosparam command=\"load\" file=\"$(find follow_me_2dnav)/launch/base_local_planner_params.yaml\"/>\n    </node>\n</launch>\n"], "url": "https://answers.ros.org/question/62896/using-kinect-for-both-navigation-and-user-tracking/"},
{"title": "Package 'sbcl' has no installation candidate", "time": "2015-02-06 11:08:56 -0600", "post_content": [" ", " ", "I am trying to install ROS hydro bare bones on a Samsung Chromebook that is in developer mode. This means that I am using an ARM form of Ubuntu (with the xfce desktop environment, if that matters). All of my packages thus far have downloaded with the \"armhf\" suffix, which I gather is preferable to \"armel\".", "Using the official installation from source instructions for ROS hydro (", "), when running the bash command:", "most of the packages install without issue. However, I get the error:", "A suggestion from the terminal output is to install some other things:", "so I installed them with", "I still get the same error as above, so I installed sbcl from source using the Linux armhf tarball and the instructions from SBCL's site. The installation was successful (I can invoke sbcl from the command line), but the ROS install still fails (when I run the first bash command I listed).", "Other solutions I've seen just say to delete the ", " directory in the ", " file, but I am hesitant to just do that since I don't know if I will need it and my system can install SBCL"], "answer": [" ", " ", "On older versions of Ubuntu, SBCL is not available as a package.", "ROS uses the package manager (apt) to detect and install dependencies.", "Since you've installed SBCL outside of the package manager, rosdep isn't aware that SBCL is installed. I believe rosdep has an option to skip specific keys; you should consult the help options for rosdep, add the option to skip the sbcl key, and continue with your installation.", "If your specific version of Ubuntu is supported by the UbuntuARM packages, you should use those instead; you will save a significant amount of time installing and compiling things, and the dependency issues have already been figured out and supplied for you. (in the case of SBCL on older versions of Ubuntu, I patched ROS to remove the lisp bindings and the dependency on SBCL completely).", "Thanks! I re-ran the ", " command with the ", " option and it seems to have worked out. I did not try anything with UbuntuARM packages since it seems I could get by without doing so, but for future readers, that looks like a good option."], "question_code": ["rosdep install --from-paths src --ignore-src --rosdistro hydro -y\n", "$ E: Package 'sbcl' has no installation candidate\n", "$ ...\n$ However the following packages replace it:\n$  sbcl-source sbcl-doc\n", "sudo apt-get install sbcl-source sbcl-doc\n", "sbcl", "src"], "answer_code": ["rosdep install...", "--skip-keys='sbcl'"], "url": "https://answers.ros.org/question/202612/package-sbcl-has-no-installation-candidate/"},
{"title": "Use several UR arm with one robot and UR5KinematicsPlugin", "time": "2015-05-08 03:30:57 -0600", "post_content": [" ", " ", " ", " ", "I'm building a robot which is made of four ur5 arms. So I created a folder with every packages needed (ctrl-bot_gazebo, ctrl-bot_description, ctrl-bot_moveit_config). I spend time configuring and now I can do plan planning with RVIZ+Moveit for each arm of the robot.", "After days of configuration everything can work (almost everything). I can do path planning and see in gazebo the movement.\nWhen I try the UR5 alone I found that this solver is amazingly fast (less than 100ms most of the time)\ncompared to kld_kinematics_plugin/KLDKinematicsPlugin", "My problem is that I can't use the ur_kinematics/UR5KinematicsPlugin because I added a prefix to each arm (N1_, N2_, N3_, N4_). So to build my robot I load 4 times a ur5 arm with 4 different prefixes in my robot's urdf.", "If I choose the  ur_kinematics/UR5KinematicsPlugin for my robot I get some error :\nI launch\n    roslaunch ctrl-bot_moveit_config ctrl-bot_moveit_planning_execution.launch sim:=true", "Then I get this", "It seems than the solver is reverted to default because ur_kinematics expect the name without prefixes.\nHowever if I look at ur_kinematics/src/ur_moveit_plugin.cpp :", "It looks like it can be used  with prefixes, but I have no clue of How to do it.", "The kld_kinematics_plugin/KLDKinematicsPlugin need 3 seconds or more to compute and can fail a lot of time. Even if I enable replanning.", "So my goal is to be able to use ur_kinematics solver but with my robot. I don't understand why ur_description is designed to be used with prefixes but the ur_kinematics solver is not.\nMaybe I'm missing something obvious."], "answer": [" ", " ", " ", " ", "It's a bit messy, but perhaps ", " can help. It was never merged, but it does show how ", " can be configured for ikfast plugins.", "Issue is that moveit_ikfast plugins get loaded more than once, and need their parameters to be defined as private parameters of the node they get loaded by. See also ", " on ", ".", "PS: \"a robot made up of four ur5 arms\"? You cannot make a statement like that and not provide us with a screenshot :).", "Edit:", "Thank you, doesn't look very straight forward. I hardcoded my prefix in ur_kinematics/src/ur_moveit_plugin.cpp to try. I was wrong, the computation time is now the same.", "The PR contains many more changes than needed. In fact, I think only the change to ", " is something you could use (but you'd need to make sure other instances of the plugin (such as in a ", ") also access that parameter).", "Thank you, doesn't look very straight forward. I hardcoded my prefix in ur_kinematics/src/ur_moveit_plugin.cpp to try. I was wrong, the computation time is now the same. It may be because I have 4 arms so the collision computation is more time consuming. The planning failure rate is almost the same", "For the screenshot I'll do it. but it's an ugly robot with 4 arms and a box between.\nThe problem is that I aim industrial application and the motion planning is not reliable, I feel a bit stuck...", "Have you checked ", " and the related ", "?", " I did the line deletion suggested in the ticket  ", " \nand now it's using RRTconnect and 40 times faster (from 2 seconds to 0.05), no more failures. However if I move the arm to a tricky position to path plan to another position it gets stuck. ", "However if I move the arm to a tricky position to path plan to another position it gets stuck.", "stuck as in? For planning related questions, I'd recommend you send a message to ", ", as they may know more."], "question_code": ["[ERROR] [1431072866.823796592, 3808.550000000]: Kin chain provided in model doesn't contain standard UR joint 'shoulder_lift_joint'.\n[ERROR] [1431072866.823849841, 3808.550000000]: Kinematics solver of type 'ur_kinematics/UR5KinematicsPlugin' could not be initialized for group 'N1_manipulator'\n[ERROR] [1431072866.824097006, 3808.550000000]: Kinematics solver could not be instantiated for joint group N1_manipulator.\n", "ur_joint_names_.push_back(arm_prefix_ + \"shoulder_pan_joint\");\nur_joint_names_.push_back(arm_prefix_ + \"shoulder_lift_joint\");\n.......\n"], "answer_code": ["~arm_prefix", "moveit-users", "move_group"], "url": "https://answers.ros.org/question/208830/use-several-ur-arm-with-one-robot-and-ur5kinematicsplugin/"},
{"title": "costmap_2d clear obstacles after every update", "time": "2015-07-15 16:16:40 -0600", "post_content": [" ", " ", "Hi,\nI am creating costmaps of highly dynamic obstacles and want it to be efficiently cleared just before every update, so that each time only the current point cloud is projected into the map and there is no trace of what was previously projected.\nAny help is appreciated.\nThanks."], "answer": [" ", " ", "I don't believe you can do this with the costmap layers as they are currently structured, but you could write a subclass of the obstacle_layer that clears all information from the map on every iteration. ", "There is a protected member resetMaps(), I tried to change its scope to public and called it in the layered_costmap function that updates maps. But I can't see any topic published. Even if I reset everything to the original source code pulled from Git, it compiles and runs, but there is no topic", " ", " ", "If you take a look at the costmap_2d documentation (", ") the parameter ", " allows you to increase the frequency at which the costmap is updated. Maybe if you sincronize this frequency with the rate of your laser sensor, you can keep up with a real time visualization in the costmap (with the associated resource consumption). ", "The update_frequency is already set to the rate at which I publish my point cloud( the costmap takes as input my custom point cloud), but the clearing is done through raytracing, I dont want that to happen, but the costmap to be cleared before each iteration", " ", " ", " ", " ", "Have you  tried", "as described ", "?"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "~<name>/<source_name>/observation_persistence (double, default: 0.0)", " ", " ", " ", " "], "answer_code": ["update_frequency"], "url": "https://answers.ros.org/question/213832/costmap_2d-clear-obstacles-after-every-update/"},
{"title": "robot_localization how to set up tf", "time": "2015-08-30 18:40:44 -0600", "post_content": [" ", " ", " ", " ", "My understanding of a fundamental concept is way off somewhere and was hoping someone could set me straight :)", "I am trying to use robot_localization to fuse PTAM+IMU.", "I have read through the docs and tutorials for the robot localization package but I am still new to ROS so I am having trouble understanding how tf works with robot_localization.", "I have an IMU topic publishing from a pixhawk via mavros: /mavros/imu/data", "and I also have a ptam topic: /vslam/pose", "Lets say that the orientation of both sensors are aligned with a positional offset on the y of 50cm.", "I am guessing that I am now suppose to set up a tf system in code that represents the physical model (with the 50cm offset) and then broadcast the tf system so that robot_localization can use it. Is that correct?", "Or am I suppose to use the frame_ids provided by the sensors?", "Also if anyone knows of any step by step tutorials for something like this then please let me know. Thanks!", "Ok so I tried using the frame_ids from the sensor messages and put those in the launch file for robot_localization. usb_cam is the frame_id from the /vslam/pose and fcu is from /mavros/imu/data. I'm not using a map frame.", "Now robot_localization publishes to the /odometry/filtered topic. When I view the tf tree on rviz it doesn't look right but I am thinking that I have not aligned the axes right?", "I've been trying to get this right but still not sure if this is even the right way to use robot_localization?!?!"], "answer": [" ", " ", "I would start by reading through ", ", and also the ", " wiki page.", "Your odometry frame_id is typically \"odom\" and is a world-referenced frame. In other words, if you start your robot and drive it forward five meters and left 3 meters, your position in the ", " frame should be (5, 3). However, your position in this frame is subject to drift (e.g., just fusing wheel encoders and IMU data).", "Your map frame_id is typically \"map\" and is identical to the odom frame, except that your position in the map frame is not guaranteed to be continuous, and is ", " subject to drift over time. For example, if you are including GPS data, your position will probably jump around a bit, but over time, your position in that frame will be more accurate than a position estimate that only used wheel encoders and IMUs.", "The base_link frame_id is typically \"base_link\" and is rigidly affixed to your robot. Most velocity (twist) data is reported in this frame. For example, a simple differential drive wheeled robot can only have +/- X and +/- yaw velocity.", "For your problem, you need to make sure each message has a frame_id specified, and then make sure you produce a transform (e.g., via static_transform_publisher) that specifies the transform from the frame_id of the message to one of the frame_ids listed above. For IMU data, you'll need to specify a ", "-->", " transform (assuming you use those two frame_id names). For the PTAM data, the frame_id probably defaults to ", " or ", ", but you'll have to verify that. See ", " tutorial for more information on preparing sensor data.", "Thanks for your help Tom. I've read through those links but I still don't understand the relationship between tf, the sensor frame_ids and the robot_localization launch file. Feeling really dense right now. I'll have another read, maybe something will click this time.", "So I have 2 messages from third party packages and they each have a frame_id: fcu (imu) and usb_cam (ptam). Do I then make 2 tf links using static_transform_publisher: base_link->fcu and odom->usb_cam. Then in robot_localization do I reference base_link and odom in the launch file???", "Still stuck. Where do these map, odom, base_link frames come from? Do I generate them?", "The frame_ids are just names. The transforms between them are all managed by ", ". So you can use the ", " library to specify a transform from frame A to frame B, and then broadcast it. Other nodes can then consume that transform by also using the ", " libraries.", "So, e.g., if you have a LIDAR that is mounted on its side on your robot, and it senses an obstacle 5 meters away at 30 degrees (from the LIDAR's origin), you might need to know where that point is for the ", " frame, or even in the world (e.g., ", ") coordinate frame.", "Thanks again Tom! Hope that wasn't too painful for you. I didn't know what I was missing so didn't even know the right questions to ask. I think I get it now."], "question_code": ["<param name=\"odom_frame\" value=\"usb_cam\"/>", "<param name=\"base_link_frame\" value=\"fcu\"/>", "<param name=\"world_frame\" value=\"usb_cam\"/>"], "answer_code": ["tf", "tf", "tf"], "url": "https://answers.ros.org/question/216750/robot_localization-how-to-set-up-tf/"},
{"title": "Hector SLAM minimum requirement spec", "time": "2015-08-10 06:24:40 -0600", "post_content": [" ", " ", " ", " ", "Hi! I want to use hector mapping on board computer such as Q7-BT, BeagleBone.\nso I have to select CPU and RAM. Please tell me minimum requirement CPU and RAM spec to run hector mapping. "], "answer": [" ", " ", "While I haven't used it in a live mapping setup on the Raspberry Pi so far, I tried processing a bag file on a Raspberry Pi2. That consumed 10% CPU of one core on the Pi, so it looks like it's worth trying out on similar platforms."], "url": "https://answers.ros.org/question/215522/hector-slam-minimum-requirement-spec/"},
{"title": "hector slam raspberry problem", "time": "2015-08-13 05:04:06 -0600", "post_content": [" ", " ", " ", " ", "Hello everybody,", "I use to create maps Hector Slam. So far everything has worked.\nI will now use a Raspberry Pi 2 with ubuntu to create maps but I always get error.", "Hector Slam starts without problems and Rviz connects without problems over the network with the Raspberry.", "In the area of laser scanners in Rviz, is an error message which reads:", "I think there is a performance problem, because it worked on my computer without any problem.", "Here's my TF Tree", "Here's a screenshot of the error", "Has anyone any idea how I can solve this problem?", "The Laser Scanner is Hokuyo UTM-30LX-EW", "Thanks + greetings", "Max"], "answer": [" ", " ", "This sounds like you might be experiencing a time sync problem between computers. The rate at which hector_mapping publishes tf data suggests that it either cannot keep up with the incoming LIDAR data, or there is a tf/time sync problem that causes huge waiting delays. I tested things on a RPi2 as described in ", " and CPU consumption was about 10%, so I don't think the machine is too slow normally.", "Did you compile hector_mapping yourself? If so, you should specify the build type:", "Otherwise, things might get built without optimization and can be very slow.", "thank you. It works. That was the solution to the problem. The CPU consumption is about 25 % instead of over 100% :-D"], "question_code": ["For frame [laser]: No transform to fixed frame [map]. TF error: [lookup would require extrapolation into the future. Requested time 2529.375686649 but the latest data is at time 2529.225683316, When looking up transformable from frame [laser] to frame [map]]\n"], "answer_code": ["catkin_make -DCMAKE_BUILD_TYPE=Release\n"], "url": "https://answers.ros.org/question/215790/hector-slam-raspberry-problem/"},
{"title": "specific odom frame responsibility", "time": "2015-12-15 08:24:35 -0600", "post_content": [" ", " ", " ", " ", "I am trying to understand the responsibility I have with regards to publishing the odom TF frame.  Specifically, we have a mobile robotic system that is intended to (and does) operate via ROS and are finalizing complete robotic control stack. ", "There is a lot written (and have read/reread) about the map --> odom --> base_link relationship.  I understand that the odom frame is 1) where the robot starts in a map and 2) can \"drift.\"   But what I would like to understand is what, specifically, needs to be/should be done programatically to move the odom frame as my robot progresses.   It seems to be my responsibility to translate what is going on into this \"drift\", and thus, I should publish a map->odom->base_link transform tree and should be \"drifting\" the odom frame?  But how should this be done?  Move it where and by how much relative to the uncertainty?  Or am I not understanding: I often read that it is the localization nodes that publish the map->odom relationship.", "(it also seems strange to me that, if I understand correctly, I am to drift the odom transform which is where the robot starts as opposed to base_link, where it is presently)", "I understand that wheel odometry error and uncertainty accumulates as the robot moves and I too understand how calculate and keep track of that uncertainty.  Again, seeking clarification on what one is supposed to do with that knowledge with regards to moving the odom frame?  Where do I move it and by how much?  ", "(Most of what is written on the subject [and there is a fair amount] is helpful if you are writing a client or trying to get a platform performing SLAM in gazebo, but it isn't clear how it is supposed to work if you are authoring the actual controlling components and stack. Have read rep 105, and all postings I can find on ROS answers, but would greatly appreciate some clarification.)"], "answer": [" ", " ", "I'll try to answer your remaining question from your comment:", "The only question that remains is this; a localizer will reposition/move the odom->base_link tree branch by placing it in the \"correct\" position in the map...but how does it position this branch exactly? Does it place the base_link on top of the corrected pose and odom shifts accordingly?", "So a localization node like amcl typically localizes the ", " frame (or internally possibly directly a sensor frame such as ", " instead), so lets assume for a time-stamp ", " (e.g. the laser message time stamp) it has computed the transformation ", ". It then can look up the corresponding odometry estimate from tf as the ", " transformation at time ", ". By computing ", " it knows what to publish to tf as the ", " transformation.", "You can think of ", " as the correction of the dirft of the odometry. Initially, when the robot is just starting and ", " is ", " (identity transformation), it simply corresponds to the starting position of the robot, which is the initial offset of odometry (which typically always starts with ", "). As the robot moves, the odometry computed e.g. from wheel encoders accumulates error over time, i.e. ", " does not correctly estimate the robot position relative to the starting position, but rather drifts away from the correct value more and more. Localization algorithms such as amcl that reference the robot absolutely with respect to a map do not have this unbound dirft, so in each step they correct for the odometry drift by adjusting ", " a little more. In other words, ", " changes in correspondance to your odometry drift. The overall result is that combined transformation ", " corresponds to the correct robot pose with respect to the map. The intermediate frame ", " may drift away more and more, but until you eventually run into numerical issues, it does not matter, since any other nodes that want to know the true position of your robot will look up ", " from TF and nodes that need a smooth relative estimate of the robot motion can observe how ", " changes over time.", "Got it, have complete understanding now.  Thank you sincerely.", "Sure thing. Maybe you could mark one of the answers as accepted, such that it is clear to everyone that this is solved?", "Better summary than the tutorials, I think.", " ", " ", "I got an answer, thanks to Brian Gerkey for clarifying for me...posting here for reference of others.", "In the map->odom->base_link relationship, it is the localization package's responsibility to publish a map->odom frame and NOT the robot controller's responsibility as I had thought.  A robot stack builder's responsibility is to publish the odom->base_link, and the laser scan data.   The localization engine (amcl or what have you) will consume /map (presuming a map_server instance is running), /scan (you are publishing laser scans)  and the odom->base_link (where you think you are) TF.   Amcl will then  attempt to place the robot in the map and will place/move the odom frame accodingly in the map by publishing the map->odom transform. ", "odom frame may jump and move non-continuously, but base_link should move continuously within that odom->base_link frame.  So the frame may jump, but the robot is always moving smoothing relative to the odom frame within odom->base_link. ", "The odom->base_link relationship is where one might be applying a kalman filter or other means to create the best possible guess of where the robot is relative to where it started.   The localization engine will shift the entire relationship...I am guessing (and want to confirm) that it will align the base_link TF to where it localizes the robot to actually be on the map based on laser scans data.  On rviz you'd see a shift or jump of the whole odom->base_link frame.  ", "I think this is a fair summary. Does that answer all your questions, or are things still unclear?", "Clear now.  I would add that amcl will adjust the map<>odom branch and place such that base_link of odom<>base_link will align exactly in position and pose of its determination of where the robot is in the map. Further, it publishies amcl_pose as a separate message which will also align w base_link."], "answer_code": ["base_link", "laser_link", "t", "map -> base_link", "odom -> base_link", "t", "map -> odom = (map -> base_link) x inverse(odom -> base_link)", "map -> odom", "map -> odom", "odom -> base_link", "0", "0", "odom -> base_link", "map -> odom", "map -> odom", "map -> odom -> base_link", "odom", "map -> base_link", "odom -> base_link"], "url": "https://answers.ros.org/question/222567/specific-odom-frame-responsibility/"},
{"title": "Why the increased system usage?", "time": "2015-12-10 08:30:31 -0600", "post_content": [" ", " ", "I have a Raspberry Pi B running Raspbian and ROS Groovy with two nodes. The first node controls the Pi camera and publishes the feed which is subscribed to on a separate PC. The second node publishes data received from a custom daughter board that is subscribed to by the PC, and subscribes to data published by the PC and sends that data to the daughter board. The camera streams fine by itself, and the other node operates great by itself. If both nodes are running, the camera is still fine, but the second node never updates data (the PC and daughter board never receive quality data). I plugged the Pi into a monitor for troubleshooting and noticed the green system usage bar is maxed out when running the second node.", "I figured the Pi could not handle this robust of a node and decided to upgrade to the Pi 2.", "Here's where things get interesting/confusing.", "The Pi 2 is running Ubuntu with ROS Indigo. Both nodes start, but now the camera feed lags by 2-3 seconds. I decided to check the system usage for both Pi's and got the following results:\nPi B:                                          Pi 2: ", "\n      PiCameraNode: 5.5-5.8%             PiCameraNode: 3.3-4.0% ", "\n      ServerNode:      87.4-88.9%         ServerNode:      122.6-124.0%", "Why would the Pi 2 which is significantly more powerful need more resources for the same code?\nAny ideas as to what could cause this, or how to fix the problem?\nAny help would be appreciated.", "As noted by Humpelstilzchen, the %CPU is per core so there is no problem with 125%CPU on the Pi2.\nI was able to decrease the lag of the camera to about 0.5 seconds by adding \"gpu_mem=512\" to \"/boot/config.txt\". I will play around with it some more and see how low I can get the lag."], "answer": [" ", " ", "Sorry, didn't got it. You said the Pi1 was to slow for your task, which also seemed to be confirmed by your values: 99%+6%=95% CPU usage, which is way too close to 100%. This indicates that the PI is throwing all its idle time to ServerNode which is still not enough.", "The Pi2 on the other hand seem to be fast enough with its multiple cores, it is using 125% = 1,25 CPU cores to handle the node.", "Thanks for the rapid response. I did not realize that %CPU was for one core. \nNow I just need to solve the camera latency issue.", " ", " ", "Your description of the ServerNode using a full CPU core is suspicious given that it's only acting as an intermediary between the computer and the daughter card; particularly given that it still uses a full core even when moved to a larger CPU.", "You should inspect the code for your ServerNode and make sure that it isn't busy-waiting or otherwise consuming CPU cycles when it's just waiting for events."], "url": "https://answers.ros.org/question/222311/why-the-increased-system-usage/"},
{"title": "Error when I open rviz", "time": "2016-01-30 15:52:33 -0600", "post_content": [" ", " ", " ", " ", "Hi! I have a question: when I launch a launch file to open rviz for hector slam, sometimes I have this message:", "What can be the reason? Thank you!"], "answer": [" ", " ", " As described in this question:  ", "  ; exit code -11 indicates a segfault. ", "Segfaults are a pretty generic error, and there's no specific fix.", " You might want to consult the rviz troubleshooting page:  ", "  or try running rviz in gdb. ", "Thank you so much!"], "question_code": ["[rviz-4] process has died [pid 12780, exit code -11, cmd /opt/ros/indigo/lib/rviz/rviz -d rviz_cfg.rviz __name:=rviz __log:=/home/user/.ros/log/3deb1b84-c78d-11e5-8892-0c607639bb9d/rviz-4.log].\nlog file: /home/user/.ros/log/3deb1b84-c78d-11e5-8892-0c607639bb9d/rviz-4*.log\n"], "url": "https://answers.ros.org/question/225441/error-when-i-open-rviz/"},
{"title": "move_base acting weird with nodes running on different hosts.", "time": "2016-02-08 04:56:04 -0600", "post_content": [" ", " ", " ", " ", "Hello All,", "Using examples from \"ROS by example\" to get move_base working, the trajectory is erratic when the code, apart from rviz, is running on ANOTHER host. It gets there in the end, but is moves very strange, as if it is very \"drunk\"...", "The commands I use are:", "When all commands are run on my regular PC (PC, i5, ubuntu 14.04, indigo), all is well.\nBut when the  2 roslaunch commands are running on another host the strange paths occur.", "That other host is a quad core i.MX 6Quad from Freescale, 1GHZ, 1GB, running ubuntu 14.04 with indigo.\nThere is no swapping, and nothing else is running there.", "Communication is via a 5GHz Wifi link that can sustain 15MB/sec.", "I would assume that for this simple setup the performance of the arm-system and wifi-link is not the problem.\nWhat could be the cause of this, and how can I proceed in nailing this down further?", "EDIT:", "roswtf gives the SAME output in both situations. It does give the following error:", "but that probably is not relevant now.", "move_base often does not start properly, the initialization normally ends with \"odom received!\", but sometimes it takes a long time (20 seconds) for that last line to appear.\nIt also sometimes does not come at all. I already asked about that in my question \"remote core with move group\".\nWhen I stop the hung program it complains about: Failed to contact master.\nBut it uses it all the time....", "When I use the move_base parameters that come with this example, I get the following warnings when the controller and move_base are running on the arm-board.", "I find it strange that calculation one loop takes that much time, given that both global and local maps are empty.\nAm i missing something?", "If I lower the rate to 1 Hz it basically works, but it has hard time following the track, and most of the time it looks like an highly unstable system. After changing some more parameters I can get it  better, but only a little bit: not usable.", "If move_base now only gives a single /cmd_vel per second, I can imaging that it is difficult to keep it on track.\nWouldn't it be better if move base gave a complete trajectory to the final goal, and update that each second?\nThen the rates of move_base and execution the trajectory could be decoupled.", "In summary, two questions. Why does the \"odom received!\" sometimes not come or after a long delay?\nAnd ...", "Communication is via a 5GHz Wifi link that can sustain 15GB/sec.", "Are you sure that is correct? That is 15 Gigabyte per second. What kind of technology are you using?", "Oops. 15MB/sec of course, Changed it.", "do you get any useful feedback when you run roswtf ?", "I see no differences, please see the edit in my question."], "answer": [" ", " ", " ", " ", "I've figured it out. The control loop on the arm-board takes about 0.7 seconds, so you have to set the rate accordingly.\nIf you do that it just works, which will include the weirdness.\nThis is due to the design of move_base in that it only sets a new /cmd_vel after each loop.\nI now realise that this is exacly what I see.", "So the question is basically answered, but I want a better behaviour of course.\nI see several possibilities.", "First, and that is what I will use as a workaround, is running move_base on the main PC.\nAnd this works perfectly.\nBut because we anticipate to have 10 of these robots running around, I really want the code to run on the arm-board. ", "Getting a more powerful computer on the robot would be defeat, especially because my intuition says that it should be possible with the current processor.\nAnd a more powerful computer would need more power etc.", "So in the end I probably need to make move_base faster.\nAs I already wrote in my question a possibility would be to let move_base deliver the complete path, instead of /cmd_vel messages.\nThen a separate node could use these paths to create /cmd_vel messages at a higher rate.\nThe complete path is already available, as seen in rviz.\nAt some future point I will design this, or does something like this already exists?", "Thanks for the help, Sietse", "How did you install ROS on the i.MX? Was that a from-source install? If so, did you build everything with optimisations turned on?", "I used the regular ROS repository for the arm architecture. Anything to gain by a source install?", "For the ubuntu install I used the recipe from ", "Anything to gain by a source install?", "No. The release binaries are already built with optimisations turned on. It's just that ppl sometimes forget to enable them when doing a from-source installation.", " ", " ", "Here are a few typical checklist items for a multiple machine setup:", "Is the network setup correct (i.e.\ndoes communication work\nbidirectionally)? If it is wrong, it\nsometimes is possible to list topics\non a machine for instance, but no\ndata can be received when\nsubscribing.", "Is there a timesync offset? If clocks\nbetween machines are not synced all\nsorts of weird things can happen due\nto components waiting for tf much\nlonger than they should (or failing\ncompletely).", "Is CPU consumption on one of the\nmachines much too high (A low power\nARM CPU could be overburdened and\nunable to keep up)?", "Is bandwidth good enough so\nall data can be transfered (If the\ntransmitted bandwidth is close to\nmaximum there's a pretty good chance\nof comms dropouts).", "Also, I think you refer solely to move_base here? If so, you should probably edit your question to replace all references to \"move_group\" (which is part of MoveIt! arm motion planning and unrelated to move_base).", "Thanks. Network, time and bandwidth are all ok I think. Only CPU load is a bit high.The core that does the move_base hovers around 80% when running. But that probably doesn't explain why the initialization of the move_base command often takes so long or doesn't finish"], "question_code": ["  roscore\n  roslaunch rbx1_bringup fake_turtlebot.launch\n  roslaunch rbx1_nav fake_move_base_blank_map.launch\n  rosrun rviz rviz -d `rospack find rbx1_nav`/nav.rviz\n", "ERROR The following nodes should be connected but aren't:\n * /move_base->/move_base (/move_base/global_costmap/footprint)\n * /move_base->/move_base (/move_base/local_costmap/footprint)\n", "[ WARN] [1455012218.435531811]: Map update loop missed its desired rate of 3.0000Hz... the loop actually took 1.0281 seconds\n[ WARN] [1455012218.953750145]: Control loop missed its desired rate of 3.0000Hz... the loop actually took 0.5194 seconds\n"], "url": "https://answers.ros.org/question/226036/move_base-acting-weird-with-nodes-running-on-different-hosts/"},
{"title": "stereo_image_proc pointcloud extremely low frequency", "time": "2016-02-03 02:14:33 -0600", "post_content": [" ", " ", "Hello,", "I have a stereo vision system. Therefore, I'm using stereo_image_proc to get the rectified images as well as the pointcloud. As far as the rectified images, I get them at the same frequency as I get the raw images from the cameras (30 Hz), but the pointcloud is only 2 Hz, which makes it hard to do something with that. I only have the node to obtain the images from the cameras and stereo_image_proc. Is there anything else that I should be doing? Could stereo_Image_proc take ", " to process the point cloud?", "Thanks for the help."], "answer": [" ", " ", "Dense stereo is a pretty CPU-intensive operation, so depending on resolution, number of disparities and importantly, the CPU and load of the machine used, it can be (much) slower than the rate at which images come in. If you are interested in only a certain range band, you can adjust/reduce min_disparity and disparity_range as described ", ". This reduces the amount of block matching comparisons performed and should reduce CPU consumption.", "I have followed the tutorial and when I increase the disparity_range I see that my camera node (ueye_cam) starts dropping frames. Could is be that stereo_image_proc requests 'too much' from the cameras and they cannot keep up?", "I think this could only be the case if you run everything in a single nodelet manager and use low number of threads for it. If you run cameras and stereo image proc in separate processes, the only explanation I see is high CPU consumption leading to the driver starving for CPU (?)."], "question_code": ["ariel@ariel-GT70:~$ rostopic hz /stereo/left/image_raw\nsubscribed to [/stereo/left/image_raw]\naverage rate: 30.038\n    min: 0.019s max: 0.042s std dev: 0.00508s window: 25\naverage rate: 30.111\n    min: 0.019s max: 0.042s std dev: 0.00496s window: 55\naverage rate: 29.964\n    min: 0.019s max: 0.042s std dev: 0.00490s window: 85\naverage rate: 30.007\n    min: 0.019s max: 0.042s std dev: 0.00502s window: 115\naverage rate: 30.039\n    min: 0.019s max: 0.043s std dev: 0.00500s window: 145\n\nariel@ariel-GT70:~$ rostopic hz /stereo/left/image_rect_color \nsubscribed to [/stereo/left/image_rect_color]\naverage rate: 31.532\n    min: 0.017s max: 0.042s std dev: 0.00653s window: 19\naverage rate: 30.272\n    min: 0.017s max: 0.049s std dev: 0.00610s window: 49\naverage rate: 30.328\n    min: 0.017s max: 0.049s std dev: 0.00607s window: 79\naverage rate: 30.168\n    min: 0.017s max: 0.049s std dev: 0.00615s window: 109\n^Caverage rate: 30.208\n    min: 0.017s max: 0.049s std dev: 0.00615s window: 123\n\nariel@ariel-GT70:~$ rostopic hz /stereo/points2 \nsubscribed to [/stereo/points2]\naverage rate: 1.827\n    min: 0.547s max: 0.547s std dev: 0.00000s window: 2\naverage rate: 2.222\n    min: 0.296s max: 0.547s std dev: 0.11000s window: 4\naverage rate: 2.050\n    min: 0.296s max: 0.635s std dev: 0.11259s window: 6\naverage rate: 2.053\n    min: 0.296s max: 0.635s std dev: 0.10111s window: 8\naverage rate: 2.099\n    min: 0.245s max: 0.635s std dev: 0.11812s window: 11\n"], "url": "https://answers.ros.org/question/225745/stereo_image_proc-pointcloud-extremely-low-frequency/"},
{"title": "catkin version of the hector_slam package: mapping issue", "time": "2016-02-01 10:03:21 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am new to ROS. I have been trying to work with ", " tutorial for logged data and I noticed that the ", " version of the ", " does not give good mapping results as compared to the ", " installation using package manager (", "). There is a significant difference in the scan matching .I am using the default ", " given in the github without any change. The trajectory of the robot is also different, which I believe is due to the scan mismatch. ", "Am I missing something here or is this a known issue? I also tried replacing the ", " executable in the ", " directory from the ", " and it works fine.", "To build the catkin package I followed:", "I also tried the ", " method described ", ". But the result does not improve. I dont have enough points to attach the images.", "update: I am using Dual core CPU @2.60GHz with 4 GB RAM with 60 GB disk space (dual boot with windows). OS: 64 bit Ubuntu 14.04.I had the same issue with other machine with i7 processor 4GB RAM(dual boot). I am building without optimization (catkin_make).", "Thanks in advance,"], "answer": [" ", " ", "What kind of machine are you using? If it has low CPU power (such as a Raspberry Pi2) and you accidentally build without optimization, this might explain the issues observed. If you could edit your question with additional information that would be helpful.", "For reference, with optimization on the tutorial ran with 10% CPU consumption on  a single CPU on a RPi2.", "To build with optimization, run", "I tried building it with optimization on  and it worked well. The CPU consumption was around 50- 60 % with rviz consuming 30-35 % of the CPU and hector_mapping(5%-10%). In case of Debug build the CPU consumptions remains at 100%", "If the answer was correct, please mark it solved to indicate that no further action is required to others."], "question_details": [" ", " ", "clone the ", " catkin stack from ", " into ", " folder", "Source the setup file in the workspace\u00b4s ", " folder", " the ", "Play the bag file provided in the tutorial ", "Save the geotiff node."], "question_code": ["hector_slam", "catkin", "hector_slam", "hector_slam", "sudo apt-get install ros-indigo-hector-slam", "tutorial.launch", "hector_mapping", "catkin_HS/devel/lib/", "/opt/ros/indigo/lib/hector_mapping", "hector_slam", "src", "catkin_make", "devel", "roslaunch", "tutorial.launch", "wstool"], "answer_code": ["catkin_make -DCMAKE_BUILD_TYPE=Release\n"], "url": "https://answers.ros.org/question/225561/catkin-version-of-the-hector_slam-package-mapping-issue/"},
{"title": "robot_localization with mocap data as alternative to GPS/NavSatFix?", "time": "2016-03-02 13:27:28 -0600", "post_content": [" ", " ", "If one has a mocap system that is giving reasonable x,y positioning coordinates but one still wants/needs to fuse that with an IMU with robot_localization, it seems I would be forced to turn my X,Y coordinates from the mocap into NavSatFix messages, (ie, GPS coordinates) such that they can be fed to robot_localization. I could do this via UTM, but this seems really inefficient.    ", "(While one might think the mocap itself would be enough to localize, some mocap systems still jitter somewhat and can have some big outlying readings, particularly when the robot gets close to the boundaries of the system.  Fusing IMU data with would be an ideal good solution.)", "I'd like to be able to feed robot_localization these mocap X,Y coordinates in liu of NavSatFix messages.", "Is there a recommended approach for this? ", "Best,\nGregory "], "answer": [" ", " ", " ", " ", "The state estimation nodes in ", " don't care about GPS data; the tutorials just use GPS data as a means of showing how to work with two state estimation node instances (one each for the ", " and ", " frames) and provide insight into how to use ", ".", "For your case, all you need to do is make sure that your IMU data and mocap data have the same coordinate frame, or that a transform exists between the two. The output of your mocap system just needs to be one of the ", " for the state estimation nodes.", "The EKF and UKF state estimation nodes in ", " are designed to fuse pose, velocity, and linear acceleration data from various sensor sources into a pose estimate. A typical use case involves fusing wheel encoder odometry data with IMU data. You can also fuse data from motion capture systems or localization packages. The coordinate frames in which said data should be reported are given in ", ". You can change the names of those frames via the state estimation node parameters, but the principles remain the same. Additionally, you can define transforms from any coordinate frame into one of those principal frames if you need to.", "A separate node exists in ", " that converts actual latitude and longitude coordinates into the robot's world frame (i.e., one of the world frames in REP-105) so that it can be fused. This is ", ". It is meant to work with devices that produce a ", " message that contains actual latitude and longitude coordinates. This node is not required for operation of the state estimation nodes, and only serves to allow users to work with GPS devices. Furthermore, GPS is by no means required for the state estimation nodes.", "In your case, your mocap system is producing X, Y coordinates. The message it generates likely has a header with a ", ". So, assuming your mocap system generates a ", " message (note that it can also be an ", " message), you can just do this:", "ok, so navsat_transform_node's job is to take gps and turn into a message the state estimators can consume, right?  So I can bypass it then. Makes sense.  Now the mocap data is absolute position information, should I be thinking about ways to make sure it is more weighted than other sensor data?", "Ah, your comment changed. You can control that weighting using the covariance matrices in the sensor data messages. Kalman filtering is just optimal weighted averaging, so it really just comes down to tuning those covariance matrices.", "Yeah, I did change my comment as I thought more about what you had written and understood it better.  But that previous comment to which you responded had to do with me not quite understanding where GPS fit into state estimation...I thought it would just be another sensor stream, but it isn't"], "answer_code": ["r_l", "navsat_transform_node", "r_l", "r_l", "navsat_transform_node", "NavSatFix", "frame_id", "PoseWithCovarianceStamped", "Odometry", "<param name=\"pose0\" value=\"your/mocap_data/topic\"/>\n\n<rosparam param=\"pose0_config\">[true,  true,  false,  <!-- X and Y will be fused -->\n                                false, false, false,\n                                false, false, false,\n                                false, false, false,\n                                false, false, false]</rosparam>\n"], "url": "https://answers.ros.org/question/228022/robot_localization-with-mocap-data-as-alternative-to-gpsnavsatfix/"},
{"title": "Laser scanner choose", "time": "2016-04-19 03:12:27 -0600", "post_content": [" ", " ", "Hello, i am working on an AGV Project with 2D SLAM. Now, I survey different sensors like SICK LMS-100, and Hokuyo UTM-30LX. i can't tell distinct different between them from the spec and the price are also similiar. but i notice that in many cases, AGV prefer SICK and Small Robot using Hokuyo. Except for the size, Is there any specific reasons? Thanks!!"], "answer": [" ", " ", "I think it's mainly the size; as you wrote, the price + specs for the SICK LMS-100 and the Hokuyo UTM-30LX are roughly similar. However, many smaller robots use the Hokuyo URG-04LX, which is completely different: cheaper, has lower power consumption but also much worse specs than the other two. By the way, SICK has recently added a line of small (Hokuyo-like) scanners, the SICK TiM 5xx line, that are available in a range of different prices and specs.", "Thanks for your reply Martin. The point i am curious about is that if the price and spec are similiar, why AGV prefer LMS-100 but not UTM-30LX(maybe i was wrong) as it is much smaller. Is there any advantage of LMS-100 when it applied on AGV  ?", "I don't really know, sorry. Perhaps you should make a list of requirements (angular resolution, range, aperture, safety class (resistance against dust/water), resistance against ambient light etc., and then ask SICK and Hokuyo directly why their scanner is better than the other. :)", "Thanks Martin, i think you might come out some good point that i can ask."], "url": "https://answers.ros.org/question/232235/laser-scanner-choose/"},
{"title": "Can you somehow use txt files to make a rosbag?", "time": "2016-04-14 03:10:01 -0600", "post_content": [" ", " ", "Hi!", "With the use of the program Eclipse I get sensordata and encoderdata saved in .txt files after a testdrive with a robot. \nIs it somehow possible to use these .txt files and convert them together to a rosbag? \nOr are there other ways to test gmapping instead of a rosbag?", "Thanks!"], "answer": [" ", " ", "You could write a python program to parse the text file and republish the sensor data on ros topics.  rosbag could record the output, or it could run live with other ros nodes that consume the published topics.  The other way to test gmapping would be with simulated data rather than recorded data using a simulator like gazebo."], "url": "https://answers.ros.org/question/231887/can-you-somehow-use-txt-files-to-make-a-rosbag/"},
{"title": "How let other catkin packages depend on a library built with ExternalProject_Add(...)? [closed]", "time": "2016-05-02 07:57:30 -0600", "post_content": [" ", " ", "Hi,", "I encountered difficulties wrapping an external driver into a catkin package such that other catkin packages can depend on it. The driver is provided by a company as open-source, it builds without cmake, and it is distributed through a public mercurial repository. ", "To conveniently pull in upstream updates, I wanted to have the build process checkout and build a specific commit. I know this is discouraged behavior for releasing ROS packages through the build-farm. But for the moment I do not intend to release the package through the build-farm.", "I tried using the CMake command ", " because of the following question:\n ", "Here are the details. In my package ", " I successfully build a library called ", ". ", "'s CMakeLists.txt looks like this:", " Here is a link to the actual file:\n ", "My second package ", " depends on ", " on gives this error:", "Here is a link to CMakeLists of ", " :\n "], "answer": [" ", " ", "Does anybody know how I declare that the library ethercat will be build through ", "?", "One option would be to use CMake's ", " support for that. Let the external library build, install everything in the proper locations (ie: ", " etc) and then import the library. ", " importing, export it in your ", " call. Dependent packages should now automatically pick it up.", "Furthermore, how can I make other catkin packages wait for that build target to finish?", "This will most likely start to work as soon as you import & export the library properly.", "Thank you for your answer! It pointed me into the right direction.", " ", " ", "gvdhoorn's answer pointed me into the right direction. I provide my own solution for documentation.", "Here is the CMakeLists of ", " which checks out and builds the external mercurial repository:", "Important notes: (1) ", " has to be called AFTER the build and import calls. (2) Exporting the include files at ", " with ", " was not possible, that directory does not exist when ", " is interpreted.", "The depending package ", " has the following CMakeLIsts:", "Important note: I have to directly depend on the build target ", " to halt compilation until all the h-files and so-files in ", " have been downloaded and built. The target ", " was not enough. Also, I did not find out how to export this target through a CMake variable.", "So, this a working solution, but there is room for improvement...", "A bit late, but:", "I have to directly depend on the build target ", " to halt compilation until all the h-files and so-files in ", " have been downloaded and built.", "by explicitly adding the ", " to the exported targets of ", ", you should be able to avoid having to manually call ", " for consuming targets.", "You can do that with something like:", "This statement ", " needs to be placed ", " the call to ", ", but I'm not sure about that."], "question_code": ["igh_eml", "ethercat", "igh_eml", "project(igh_eml)\n\nfind_package(catkin REQUIRED)\n\ncatkin_package(\n  INCLUDE_DIRS ${CATKIN_DEVEL_PREFIX}/include\n  LIBRARIES ethercat)\n\ninclude(ExternalProject)\nExternalProject_Add(upstream_igh_eml\n# #--Download step----------\n  HG_REPOSITORY http://hg.code.sf.net/p/etherlabmaster/code         # URL of mercurial repo\n  HG_TAG 792892ab4806                # Mercurial branch name, commit id or tag\n  TIMEOUT 30           # Time allowed for file download operations\n  # #--Update/Patch step----------\n  UPDATE_COMMAND touch <SOURCE_DIR>/config.h COMMAND mkdir -p <SOURCE_DIR>/include <SOURCE_DIR>/lib    # Source work-tree update command\n  # #--Configure step-------------\n  CONFIGURE_COMMAND touch <SOURCE_DIR>/ChangeLog COMMAND autoreconf -i <SOURCE_DIR> COMMAND <SOURCE_DIR>/configure --prefix=<INSTALL_DIR> --disable-kernel --enable-hrtimer --enable-tool  # Build tree configuration command\n  # #--Build step-----------------\n  BUILD_IN_SOURCE 1         # Use source dir for build dir\n)\n\nExternalProject_Get_Property(upstream_igh_eml source_dir)\n\nadd_custom_command(TARGET upstream_igh_eml POST_BUILD\n    COMMAND cp \n      ${source_dir}/../../include/ecrt.h \n      ${CATKIN_DEVEL_PREFIX}/include/\n)\n\nadd_custom_command(TARGET upstream_igh_eml POST_BUILD\n     COMMAND cp \n     ${source_dir}/../../include/ectty.h\n     ${CATKIN_DEVEL_PREFIX}/include\n)\n\nadd_custom_command(TARGET upstream_igh_eml POST_BUILD\n    COMMAND cp \n    ${source_dir}/../../lib/libethercat.so\n    ${CATKIN_DEVEL_PREFIX}/lib\n )\n", "omnidriver_ethercat", "igh_eml", "CMake Error at /home/georg/ros/indigo/boxy_refactoring/devel/share/igh_eml/cmake/igh_emlConfig.cmake:141 (message):\n  Project 'omni_ethercat' tried to find library 'ethercat'.  The library is\n  neither a target nor built/installed properly.  Did you compile project\n  'igh_eml'? Did you find_package() it before the subdirectory containing its\n  code is included?\n", "omnidrive_ethercat", "ethercat", "ExternalProject_Add"], "answer_code": ["ExternalProject_Add", "CATKIN_PACKAGE_LIB_DESTINATION", "catkin_package(LIBRARIES ..)", "igh_eml", "cmake_minimum_required(VERSION 2.8.3)\nproject(igh_eml)\n\nfind_package(catkin REQUIRED)\n\ninclude(ExternalProject)\nExternalProject_Add(upstream_igh_eml\n  HG_REPOSITORY http://hg.code.sf.net/p/etherlabmaster/code\n  HG_TAG 792892ab4806\n  TIMEOUT 30\n  UPDATE_COMMAND touch <SOURCE_DIR>/config.h COMMAND mkdir -p <SOURCE_DIR>/include <SOURCE_DIR>/lib\n  CONFIGURE_COMMAND touch <SOURCE_DIR>/ChangeLog COMMAND autoreconf -i <SOURCE_DIR> COMMAND <SOURCE_DIR>/configure --prefix=<INSTALL_DIR> --disable-kernel --enable-hrtimer --enable-tool\n  BUILD_IN_SOURCE 1\n  INSTALL_DIR ${CATKIN_DEVEL_PREFIX}\n)\n\nadd_library(ethercat SHARED IMPORTED)\nset_target_properties(ethercat PROPERTIES IMPORTED_LOCATION ${CATKIN_DEVEL_PREFIX}/lib/libethercat.so)\nadd_dependencies(ethercat upstream_igh_eml)\n\ncatkin_package(\n    LIBRARIES ethercat\n)\n", "catkin_package", "${CATKIN_DEVEL_PREFIX}/include", "catkin_package", "catkin_package", "omnidrive_ethercat", "cmake_minimum_required(VERSION 2.8.3)\nproject(omni_ethercat)\n\nfind_package(catkin REQUIRED COMPONENTS ... igh_eml ...)\n\ncatkin_package(\n  CATKIN_DEPENDS ... igh_eml ...)\n\ninclude_directories(include ${catkin_INCLUDE_DIRS})\n\nadd_executable(omni_ethercat src/omni_ethercat.cpp src/omnilib/omnilib.c src/omnilib/realtime.c)\ntarget_link_libraries(omni_ethercat ${catkin_LIBRARIES})\nadd_dependencies(omni_ethercat upstream_igh_eml)\n", "upstream_igh_eml", "igh_eml", "ethercat", "upstream_igh_eml", "igh_eml", "upstream_igh_eml", "igh_eml", "add_dependencies(.. upstream_igh_eml)", "list(APPEND ${PROJECT_NAME}_EXPORTED_TARGETS upstream_igh_eml)\n", "catkin_package(..)"], "url": "https://answers.ros.org/question/233420/how-let-other-catkin-packages-depend-on-a-library-built-with-externalproject_add/"},
{"title": "How to stream RGB and Depth at 30Hz simultaneously?", "time": "2016-05-23 20:13:50 -0600", "post_content": [" ", " ", " ", " ", "How to stream RGB and Depth at 30Hz simultaneously?", "I wrote two nodes which subscribe rgb and depth images. They can work at 30Hz when I run only one of the nodes.\nHowever, the performance reduces to 20Hz when I launched the two nodes together.", "How to solve the problem? Note that the openni_launch publishes the rgb and depth images at 30Hz all the time."], "answer": [" ", " ", "Hi,", "Transferring two ", " (such as RGB and Depth) at 30Hz can be a ", " and that is probably why the ", " to 20Hz. You can confirm this by launching your nodes and checking how much CPU is being consumed, it will likely be close to 100%.", "If performance is an issue for you, implement your nodes as ", " and register them into openni's nodelet manager to avoid the costly memory transfer between processes.   ", "I hope this helps.", "Thank you for answer. But I have a 16-core i7 CPU, it is so powerful that the CPU load is far from 100%. The subscribed images are written to SSD.\nDo you have any nodelets sample code? I am new to this area."], "url": "https://answers.ros.org/question/235049/how-to-stream-rgb-and-depth-at-30hz-simultaneously/"},
{"title": "What're the pros and cons of ROS-I compared to other industrial robotics operating systems?", "time": "2016-06-16 04:38:55 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "I'm new to ROS but became very interested in it at first glance ;)", "As we know, ABB, FANUC, Yaskawa and KUKA are the top 4 industrial robotics companies. Seems all of them have already started trying to use ROS-I on some of their products, but that's still in an early stage.", "So, I'm wondering, what are the pros and cons of ROS-I compared to the operating systems developed by these companies?", "And what obstacles ROS-I faces to be massively deployed in industrial robots?", "I've did quite some searches but still couldn't find satisfying answers; they failed to explain the \"why\"s hence not very convincing.", "Thanks,", "snakeninny", "Edit:\nThanks to ", ", I've updated my question to a better and more accurate one: What's the relationship of ROS-I and other robotics operating systems?", "I'll refrain from answering the rest, but:", "[..] what are the pros and cons of ROS-I compared to the operating systems developed by these companies [..] And what obstacles ROS-I faces to be massively deployed in industrial robots?", "I get the impression you think ROS-Industrial intents to (cont'd)", ".. ", " whatever industrial robot controllers are running now. ", ". We \"only\" (there's more to it, but I'll skip that) strive to integrate existing technology, and to make it easier for cutting edge research to transition to commercial settings.", "Also: (cont'd)", "..", "As we know, ABB, FANUC, Yaskawa and KUKA are the top 4 industrial robotics companies. Seems all of them have already started trying to use ROS-I on some of their products, but that's still in an early stage.", "I would be interested to know your source(s) for that statement.", "Thanks for the quick responses. From ", ", ", " and ", ", all their statuses are \"developmental\", i.e. \"This software is not yet production ready code\", so I guess it's still far from deployment.", " And can you describe the differences (of any aspects) between ROS-I and the current operating systems those companies use? Thanks.", "The statuses are developmental because the components are still being developed (ie: actively), not just maintained. Also: I was more interested in your statement about \"", "\"", "And can you describe the differences (of any aspects) between ROS-I and the current operating systems those companies use? Thanks.", "My question would be: why? Industrial controllers do one thing: control motion the best they can. That is only a single - relatively small - part of ROS.", "I googled \"KUKA ROS\" and found ", ", which is a very basic question. I think if the integration of ROS and KUKA is quite mature, google would give me some more \"advanced\" results."], "answer": [" ", " ", " ", " ", "The better one may be: What's the relationship of ROS-I and other robotics operating systems?", "Now I can try to answer your question: if with ", " you are actually referring to the (often) custom, hard real-time operating systems found on industrial robot controllers (I'll refer to these as vendor OS), then the relationship is simple: ROS-Industrial provides packages that implement a bridge to expose access to the motion control and other capabilities of those vendor OS which allows a ROS application to control industrial robots just as they would a PR2, a Fetch or any other ROS-enabled robot.", "Most of these ", " include small user-level programs that are installed on the controller that communicate using networking sockets with a ROS node on an ", " PC. The industrial controller is in complete control of the ", " of the motion, ROS provides the trajectories. Controller-level safety is handled by the controller, low-level motion control is handled by the controller, nothing is replaced, only integrated.", "If you will, the ROS-Industrial (driver) packages provide an abstracted control interface to the functionality supported by the industrial robot controller, effectively removing all / most vendor-specific aspects from how one would normally interact with such controllers.", "There are many more components in ROS(-Industrial), but almost all of them have no (direct) relationship with industrial robot controllers.", "What I understand is that ROS-I is kind of a layer between ROS users/developers and vender OS, i.e. it's like a wrapper of various vendor OS APIs and provides a universal ROS-I API. Correct me if I'm wrong.", "Based upon this question, I have a new one at ", ". Would you mind taking a look? Thanks.", " ", " ", "ROS-I brings the rich world of developed solutions (ROS packages) and the ones in research to be integrated to the industrial robots (manipulators), the main issue that ROS has is a lack of reliability and safety concern. Those are basically the main points in which ROS-I is working together with the robot vendors, industry can't afford having a robot that suddenly behave in an unexpected manner.\nPros of ROS-I:", "Cons of ROS-I:", " At the end is a matter of a few moths (I hope) to make ROS-I reliable, user friendly and safe enough to take all vendor OS out of the market. (I'm also working on this, look at:  ", " ) ", "the main issue that ROS has is a lack of reliability and safety concern", "May I ask what're the main reasons of the lack of reliability and safety? I think ROS was from labs and lack of practical usages is probably one.", "Let's developers create solutions that are cross robot, so no matter the brand, the same solution will run for any ROS-I compatible robot.", "So is this what I meant in the other answer?", "takes too much time to prepare and perform the same high level movements", "What's causing the time consumption? Simply because ROS-I is a wrapper?", "Industry requires a reliable and precise robot ... giving to the user more control in the way that ", "So that means ROS-I still lacks some of the key functions vendor OS have, right?", "Based upon this question, I have a new one at ", ". Would you mind taking a look? Thanks."], "answer_details": [" ", " ", " ", " ", "Let's developers create solutions that are cross robot, so no matter the brand, the same solution will run for any ROS-I compatible robot.", "You can integrate the robot capabilities to existing solutions (ROS packages) that enhance the capabilities and functionalities of the robot in the production line: Like object recognition, point cloud, et c.", "Brings the open-source way of development (community) to the industrial robots, letting industries be less dependent on a specific vendor.", "Programming a tasks becomes totally independent of the vendor specific OS.", "It is not reliable yet.", "takes too much time to prepare and perform the same high level movements (like welding) that a robot can perform while being easily program with its own vendor programming interface.", "Industry requires a reliable and precise robot that can perform the same movement more than 10.000 times without ever having weird behaviours. So far, MoveIt is not able to guarantee that this won't happen, Descartes gives some hopes in this aspect, giving to the user more control in the way that the robot must move, but is too clumsy to use, so, for creating a task, it is not as fast as using the vendor way of programming the robot.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/237173/whatre-the-pros-and-cons-of-ros-i-compared-to-other-industrial-robotics-operating-systems/"},
{"title": "Automated rosbag on start up", "time": "2016-06-16 10:20:30 -0600", "post_content": [" ", " ", "My plan is to use a remote computer to create a rosbag.\nI was wondering what is the best practice to automatically start a rosbag record instance after boot on an Ubuntu 14.04 Odroid system. Any experiences ?"], "answer": [" ", " ", "I have done that using systemd services that launches a roslaunch file from a bash script.\nYou can also do it using the ubuntu's \"startup applications\" or \"sessions\" to launch any app at startup.", "In any case, be careful with rosbag files because they are really disk space consuming.", "Thanks for the pointer. I eventually got it working with Upstart instead of Systemd since it was used on my system.", "I just migrated my project to Ubuntu 16.04LTS and need to get Systemd to do exactly what you're describing Pablo, could you possibly post an example bash script here? Cheers", " ", " ", "You can easily do that with ", " which starts a launch files on boot as a service.", "The problem I see with upstart is that is deprecated from 15.04. Ubuntu people tried to introduce upstart to replace SystemV init... but the debian community finally moved to systemd. New versions of ubuntu works now with systemd.", "I am aware however the target OS is trusty which still uses upstart."], "url": "https://answers.ros.org/question/237241/automated-rosbag-on-start-up/"},
{"title": "publisher/subscriber of the same topic on the same node", "time": "2016-07-22 06:39:46 -0600", "post_content": [" ", " ", "Hi, it's not clear to me what is the default ROS behavior if I a have node which publishes and subscribes to the same topic.\nAre the published messages automatically received by the subscriber?", "If the answer is yes, how I can prevent my node to re-read the messages that the node itself is publishing?", "If the answer is no, how can I allow my node to read the messages that the node itself is publishing?", "Thank you!"], "answer": [" ", " ", "I am not entirely sure what you are trying to achieve but I'll give it a try. From your question I see two possible ways you would like to use that:", "You create a message in part of your node and want to process it in a different function:", "If this is he case, I wouldn't know why you would transfer the message via a tcp connection instead of just calling the function directly if both ", " and ", " are in the same node.", "If you are trying to modify data on a topic and then republish it on the same topic:", "then this is completely ambiguous and you will create an endless-loop because ", " will trigger itself by publishing. It might sound like a solid idea to take data on a topic, modify it and then send it off to a different compent which is supposed to work on the modified data:", "The problem here is that spam does not consume the message. What you would actually get is that ", " receives ", " and ", " while spam receives ", " and turns it into ", " which is then received by ", " and ", " and so forth.", "I think the issue here is more conceptual. Messages on a topic are not consumed by the receiver and, therefore, you cannot receive and publish on the same topic without creating an endless-loop. Every other behaviour would be completely counter intuitive. Also, you cannot choose which messages to receive and which to ignore.", "Solution: create a different topic for one of them via remapping either the incoming or the outgoing topic. ", "Hope that answers your question.", "Also, you cannot choose which messages to receive and which to ignore.", "that's not entirely true. You could check the name of the publishing node by using a ", ", and ignore your own.", "Brittle though.", "I think the issue here is more conceptual.", "That could certainly be true. Perhaps this is an xy-problem. But we'd need more information to determine that.", "I agree that you could find out who sent the message but then every node on that topic would have to know which nodes are publishing and which to ignore. That is indeed quite brittle as well as soon as you adopt this paradigm and keep adding publishers to the same topic.", "No -- every node would (trivially) have to be able to identify ", " and ignore only those messages.", "Sure, but that only works if every node is supposed to work with all the messages from the other nodes. Say node 3 would have to use the messages of node 1 and node 2. So you cannot just modify and expect the receiving nodes only to get the modified message.", " ", " ", "It seems to me that the heart of your question is: what happens when a message is published (on a specific topic)?", "The behind-the-scenes ROS machinery communicates that data to all subscribers for that topic. Whether a subscriber is in the same node (i.e., computational process), in a different node on the same machine, or in a different node on a totally different machine, the message will be put in a subscriber node's callback queue for processing. Message reception is independent of the message's source and/or location.", "To ignore a message that originated within the same node requires a means of identifying the source of the message. One way to do this, as noted by ", " in the comments of another answer, is to query the MessageEvent for the node name. Another would be to add that information to the message data.", "One scenario where this might be useful is in a robotic swarm application where data needs to propagate to all agents, but individual agents have incomplete global information. Agents re-broadcast events from other agents to their neighbors upon first receiving them. But to avoid network storms, an agent needs to ", " re-broadcast messages it's already seen ", " not re-broadcast messages for which it is the originator.", "Note that it's not clear to me that one would use ROS for this, rather than custom, highly optimized and specialized software. Furthermore, I'm not familiar with the literature for network data propagation -- I'm sure this has been covered in sensor network and smart dust research."], "answer_code": ["publisher = rospy.Publisher(\"/mytopic\", ...)\nrospy.Subscriber(\"/mytopic\", eggs, ...)\n\ndef spam():\n    message.a = 1\n    message.b = 2\n    publisher.publish(message)\n\ndef eggs(msg):\n    print msg.a + msg.b\n", "spam", "eggs", "publisher = rospy.Publisher(\"/mytopic\", ...)\nrospy.Subscriber(\"/mytopic\", spam, ...)\n\ndef spam(msg):\n    msg.a += 1\n    publisher.publish(msg)\n", "spam", "msg.a = 1 ---> spam(msg) ---> msg.a = 2 ---> foo\n", "foo", "msg.a=1", "msg.a=2", "msg.a=2", "msg.a=3", "foo", "spam"], "url": "https://answers.ros.org/question/240136/publishersubscriber-of-the-same-topic-on-the-same-node/"},
{"title": "How to deal with limit tag when joint limits are changeable (depended on other links) and not fixed?", "time": "2017-01-10 04:43:03 -0600", "post_content": [" ", " ", " ", " ", "My robot has not fixed joint limits. On every position is has a different joint limits other side it will hit parent/child link. Or it will go out of working area because I removed support links (closed loop)  that are increasing max payload. What I think is to add an extra dummy link to parent/child link to make collision detection and prevent crashing. But i can't deal when it is going out of working area. \nCan I add a mathematic equation on limits? ", "If not I should add again support links (counterweights for payload), close the open loop, prevent collision with parent links and then it will never go out of working area.. but i find it too difficult to close the open loop. Is there any way to deal with it? ", "Question: How to make support_link_2 follow a specific point/axe on link_3", " Video of the custom robot:  ", "Structure:"], "answer": [" ", " ", "I can write a more extensive answer later, but for now: no, there is no support for festooning joint limits anywhere in ROS. Not in URDF, SDF or any consumers of those formats (", ", ", " or Gazebo). MoveIt! does also not support this.", "The big assumption in almost everything that deals with robot kinematics and geometry is that the structure is defined once, loaded at application startup and does not change.", "Perhaps if you can describe your robot better (diagram or picture), we can suggest alternatives / work-arounds.", "Note that I'm not saying it'd be impossible to add such support, but that is not what you are asking for, I believe.", "Please take a look on video", "You may want to take a look at ", "I use mimic on joint 3 which is depended on joint 2, but using mimic joint on support_links i am not able to close the open loop."], "answer_code": ["joint_state_publisher", "robot_state_publisher"], "url": "https://answers.ros.org/question/251637/how-to-deal-with-limit-tag-when-joint-limits-are-changeable-depended-on-other-links-and-not-fixed/"},
{"title": "compiling the package [costmap_2d] is extremely slow on Jetson TK1", "time": "2016-12-28 08:27:55 -0600", "post_content": [" ", " ", "Have anyone experienced compiling ", " on a Nvidia Jetson TK1 with Ubuntu 16.04 and ROS Kinetic? I've tried to compile it on 2 such boards. They all compiled very very slow!\nIn the extreme case, I tried to compile ", " in my own catkin workspace with the command ", " with or without ", ", ", ". In this case, ", " simply failed to finish compiling. It stuck at", "The linking procedure lasts forever, never succeed to accomplish.", "The only way I made it finish is by invoking ", " instead of ", ", and the linking still takes very long time when compared to other packages (like lasting 10min+ or so)."], "answer": [" ", " ", " ", " ", "I have finally succeeded to compile ", " with ", " alone. Just push my patience to the extreme, and it took 40+ minutes to link to success!", "Still, can any one kindly point out why is this happening, any hints will be appreciated. Thanks!", "Linking can be a very RAM-intensive process; perhaps your system is using swap while linking?", "Thank you for answering!\nIt probably didn't used the swap, the TK1 has 4GB memory. And I even tried to compiled with only one thread (", "). I also tried ", " to see memory consumption, there are more than 2GB free memory available."], "question_code": ["costmap_2d", "costmap_2d", "catkin_make", "-j5", "--use-ninja", "costmap_2d", "Linking CXX executable /home/ubuntu/catkin_ws/devel/lib/costmap_2d/costmap_2d_cloud\nLinking CXX executable /home/ubuntu/catkin_ws/devel/lib/costmap_2d/costmap_2d_markers\n", "catkin_make_isolated", "catkin_make"], "answer_code": ["costmap_2d", "catkin_make", "-j1", "free -m"], "url": "https://answers.ros.org/question/250868/compiling-the-package-costmap_2d-is-extremely-slow-on-jetson-tk1/"},
{"title": "Orbbec Astra on ROS issues", "time": "2017-01-16 17:38:12 -0600", "post_content": [" ", " ", " ", " ", " Recently purchased a Deep Learning Robot.  ", "    Mine came with the Orbbec Astra instead of the Asus Live Pro.  I'm trying to get it up and running but having issues. In RViz, I can see RGB fine, but, the depth sensors don't work when I toggle them on.  I'm brand new to ROS and following the Turtlebot tutorials.   I found this thread on the subject.  ", "   but, I'm not exactly sure how to proceed.   ", "More Info:", "I'm getting this..", "also getting these as well", "I ran this on the robot: ", "and this on the PC: ", "Thanks, Billy", "Probably not the answer you're looking for, but seeing as you actually bought this configuration from a company: have you tried contacting them and asking this question? We're happy to help, but this would seem like a prime example of the sort of question you'd ask the company's support department.", "I bought it never assembled from a 3rd party.  Not direct from the company.  Strange thing is, their site still showing the Asus Axiton Pro Live vs the Orbbec Astra.", "In your rviz screen shot (on discourse) it shows status error,\nIf you expand the status (arrow on the left of the status), what is the error?"], "answer": [" ", " ", " ", " ", "There's not a lot for us to go on here.", "My typical debugging process would be to ask and answer the following questions:", "Please try to go through these steps and answers these questions on your own. If you get stuck, feel free to ", " with additional information, and please include the details, either copy and paste commands and their output, or include a screenshot of rviz.", "Given that the transforms for ", " is missing and the ", " parameter is missing, it seems like the main turtlebot launch file isn't running on your robot. I believe this is the ", " in the turtlebot package, but you should consult the manual to be certain.", "I also find it odd that you're running rviz on the robot, and the Orbbec driver on your desktop. It seems to be working, but it implies that you're visualizing things on the robot (are you chasing it around with a monitor attached?), while the Orbbec sensor is plugged in to your desktop and, presumably, stationary.", "I cut'n pasted the commands backwards.. I am indeed running rivz on the laptop and the orbbec on the robot.   It seems that I missed the VERY ", " step of:  Make sure the minimal software has already been launched on the robot and you have configured your network correctly."], "answer_details": ["Is there an error displayed in rivz? If you expand the display properties it will show you some status that usually includes number of messages or points received, and sometimes errors.", "Which topic is rviz subscribing to for \"depth sensors\" ? This should be listed in rviz display that you're enabling when you try to view the point cloud.", "Which topics are the Orbbec Astra driver publishing? You can start to identify this by figuring out which node(s) are the Orbbec Astra driver, and then run rqt_graph or rosnode info to see which topics it's publishing", "Sanity check: Is rviz subscribing to a reasonable topic? Compare the topic that rivz is subscribing to with the list of topics that the driver is publishing.", "Is there data published on the topic that rviz is subscribing to? Use ", " or ", " to see if there is data published on that topic that rviz is subscribing to", "If there is data published, why can't rviz display it? This usually goes back to the top, looking at rviz to see why it's not displaying the data.", " ", " ", " ", " "], "question_code": ["In PointCloud: Transform [sender=unknown_publisher] For frame [camera_rgb_optical_frame]: Fixed Frame [base_footprint] does not exist\n\nIn Registerd PointCloud: Transform [sender=unknown_publisher] For frame [camera_rgb_optical_frame]: Fixed Frame [base_footprint] does not exist\n", "In Grid: For frame [odom]: Fixed Frame [base_footprint] does not exist\n\nRobotModel: Parameter [robot_description] does not exist, and was not found by searchParam()\n", "roslaunch turtlebot_bringup 3dsensor.launch", "roslaunch turtlebot_rviz_launchers view_robot.launch"], "answer_code": ["rostopic echo", "rostopic hz", "base_footprint", "robot_description", "minimal.launch"], "url": "https://answers.ros.org/question/252139/orbbec-astra-on-ros-issues/"},
{"title": "Several errors wthen subscribing to two topics", "time": "2017-01-26 13:11:01 -0600", "post_content": [" ", " ", " ", " ", "When I subscribe to one topic everything works perfectly but when I try to subscribe to two topics I get a variety of errors", "For reference I have two callback functions(messageCb1 and messageCb2) and two different subscribers(ex1 and ex2).  Both subscribers subscribe to the same message type but each is from a different topic.  The message type is a custom message with a Pose and two floats."], "answer": [" ", " ", "My problem was trying to run two serial nodes in launch. Fixed and works wonderfully.", " ", " ", "My first thought is that you are trying to send too much data over serial. You mention that the messages that you are receiving contain a pose and two floats. That means they are (7*64 + 32 + 32) bits per message. This gives ~512 bits per message, and you are subscribing to two topics containing these messages.", "If you are using the default baud rate of 9600, you will likely have issues. If both of your topics were publishing at 20Hz, that would result in 20,480 bits/s (20", "512). Plus there should be a bit of overhead for communication on top of that.", "Also, I believe rosserial_arduino's buffer is about 80 bytes (I read that somewhere, but I cannot recall where). Two of your messages consume 128 bytes, which would be problematic if my memory of an 80 byte buffer is correct.", "So, your options then would be:", "You can increase the baud rate with the arduino by adding to your arduino script:", "Along with the parameter for the rosserial_arduino node:", "I'll look into increasing the baud rate. I am already using a baud rate of 57600.", "Ah, I missed that from your log output, sorry. I also missed that it states the subscriber buffer size as 512 bytes. Would you be calling spin() on the arduino close to how frequently the messages are being published? Just to avoid messages piling up.", "I think I figured out my problem. I haven't two serial nodes launching. Will check and respond tomorrow.", ": you don't need / can't have two ", " server nodes running over the same serial port.", "This was in fact the problem."], "answer_details": [" ", " ", " ", " ", "Increase the baud rate, and hope the buffer is not the limiting factor", "Find a way to reduce your message size (do you need all 6 degrees of freedom in the pose?)", "Throttle the frequency which your messages are published (can whatever your messages are being used for respond at that rate?)", " ", " ", " ", " "], "question_code": ["[INFO] [1485455415.530806]: ROS Serial Python Node\ninit done\n[INFO] [1485455415.598943]: Connecting to /dev/ttyACM0 at 57600 baud\n[INFO] [1485455415.730069]: ROS Serial Python Node\n[INFO] [1485455415.759043]: Connecting to /dev/ttyACM0 at 57600 baud\n[INFO] [1485455417.851490]: Note: subscribe buffer size is 512 bytes\n[INFO] [1485455417.851787]: Setup subscriber on external1/tag_pose [apriltags_ros/MetaPose]\n[INFO] [1485455417.862592]: Setup subscriber on external2/tag_pose [apriltags_ros/MetaPose]\n[INFO] [1485455417.974516]: Note: subscribe buffer size is 512 bytes\n[INFO] [1485455417.974869]: Setup subscriber on external1/tag_pose [apriltags_ros/MetaPose]\n[INFO] [1485455430.471515]: wrong checksum for topic id and msg\n[ERROR] [1485455432.946574]: Lost sync with device, restarting...\n[INFO] [1485455432.993689]: wrong checksum for topic id and msg\n[INFO] [1485455435.496778]: wrong checksum for msg length, length -2304\n[INFO] [1485455435.502251]: chk is 10\n[ERROR] [1485455447.947238]: Lost sync with device, restarting...\n[WARN] [1485455453.022465]: Serial Port read failure: object of type 'int' has no len()\n[INFO] [1485455453.028041]: Packet Failed :  Failed to read msg data\n[INFO] [1485455453.028446]: msg len is 8\n[INFO] [1485455458.150668]: wrong checksum for topic id and msg\n[INFO] [1485455493.152246]: wrong checksum for topic id and msg\n[ERROR] [1485455493.157915]: Lost sync with device, restarting...\n[INFO] [1485455493.291012]: Setup subscriber on external2/tag_pose [apriltags_ros/MetaPose]\n[WARN] [1485455500.782483]: Serial Port read failure: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\n[INFO] [1485455538.406165]: wrong checksum for topic id and msg\n[INFO] [1485455558.412407]: wrong checksum for msg length, length -257\n[INFO] [1485455558.412790]: chk is 8\n[ERROR] [1485455580.912865]: Mismatched protocol version in packet: lost sync or rosserial_python is from different ros release than the rosserial client\n[INFO] [1485455580.913300]: Protocol version of client is unrecognized, expected Rev 1 (rosserial 0.5+)\n"], "answer_code": ["nh.getHardware()->setBaud(57600);\n", "<param name=\"baud\" value=\"57600\"/>\n", "rosserial"], "url": "https://answers.ros.org/question/253100/several-errors-wthen-subscribing-to-two-topics/"},
{"title": "Can anyone tell me how to control the DC motors using ROS and Arduino", "time": "2017-02-12 00:23:47 -0600", "post_content": [" ", " ", " ", " ", "i am fairly new to ROS. I'm trying to control 2 DC motors through ROS and Arduino. I've been using rosserial package and also i've tried ros_arduino_bridge.", "The title is fort a short 'title', could you please try to edit your question into a title (one line) and a question (as long as needed?)"], "answer": [" ", " ", "On the smaller Arduinos with only 2kb RAM I've had trouble using rosserial because of the amount of RAM consumed, so I've abandoned that for ros_arduino_bridge. If you are using a motor driver not already supported by ros_arduino_bridge, you'll have to modify the code in a couple of places.", "First, in ROSArduinoBridge.ino, look at this code (lines 48-64):", "You need to uncomment the definition of USE_BASE, comment out ROBOGAIA, and then add your own define for your motor controller, say ", ".", "Then, in motor_driver.ino, you'll need to add code to support that motor driver. The bottom of that file looks like this:", "You'll need to add an ", " before the ", " for ", ", and add a definition of ", ".", "There are other changes required to support encoders different from the ones already coded. You might compare with the changes Marco Walther made to support the Pololu A-Star board with onboard motor driver. (I've contributed to his mods.)", " ", " ", "how do you like to controll the Motors?\nthe Turnspeed? or the Postion? or only on and off?", "Greet Anton", "first i want to turn it on and off the apply speed control also.", " may this help\n ", " \nthe other questen is with kind of motor driver Chip ddo you have? ", "i'm using rki 1340 dc motor driver..and thanks for the link"], "answer_code": ["//#define USE_BASE      // Enable the base controller code\n#undef USE_BASE     // Disable the base controller code\n\n/* Define the motor controller and encoder library you are using */\n#ifdef USE_BASE\n   /* The Pololu VNH5019 dual motor driver shield */\n   #define POLOLU_VNH5019\n\n   /* The Pololu MC33926 dual motor driver shield */\n   //#define POLOLU_MC33926\n\n   /* The RoboGaia encoder shield */\n   #define ROBOGAIA\n\n   /* Encoders directly attached to Arduino board */\n   //#define ARDUINO_ENC_COUNTER\n#endif\n", "#define RKI1340_MOTOR_CONTROLLER", "#else\n  #error A motor driver must be selected!\n#endif\n", "#elif", "#else", "RKI1340_MOTOR_CONTROLLER", "void setMotorSpeeds(int leftSpeed, int rightSpeed)"], "url": "https://answers.ros.org/question/254460/can-anyone-tell-me-how-to-control-the-dc-motors-using-ros-and-arduino/"},
{"title": "Why does ROS have nodes?", "time": "2017-03-17 16:19:42 -0600", "post_content": [" ", " ", "I've read the documentation about nodes and topics, but they just describe what nodes and topics are, not ", " they are useful. Granted, I'm very new to designing robot source code and I am guessing that there are particular cases when they are needed, I have just yet to see them. I'm currently working for a robotics startup (as an intern) that has a pretty messy code base and I really want to clean it up--but I want to use best practices and a very good design. AFAIK, nodes are useful to support inter process communication. Well, there are many parts in the system I'm looking at and I think they've made too many nodes where there doesn't really need to be. It seems like nodes are being used in place of designing a proper architecture that passes information internally. ", "Therefore, I'd like to hear the community's opinion on nodes. Should we avoid using them unless absolutely necessary? When are they absolutely necessary? What is the problem with having one monolithic process? "], "answer": [" ", " ", " From  ", "The use of nodes in ROS provides several benefits to the overall system. There is additional fault tolerance as crashes are isolated to individual nodes. Code complexity is reduced in comparison to monolithic systems. Implementation details are also well hidden as the nodes expose a minimal API to the rest of the graph and alternate implementations, even in other programming languages, can easily be substituted.", "Separate nodes gives better fault tolerance and debugging; it's much easier to see what's broken when a single node is crashing, and the OS prevents nodes from overwriting the memory of other nodes.", "Using a pub-sub middleware like ROS also hides the thread/process synchronization that happens in any large, multithreaded system and provides a more obvious way to distribute the processing load across many cores.", "Obviously there's a performance hit for serializing and deserializing messages, but for small messages this is negligible. If you're passing around large messages such as images or point clouds, the performance gain from using ", " outweighs their complexity.", "For more best practices, have a look at the ", " section of the wiki.", "Was there ever an instance when you found that putting a piece of code into a separate node was extremely beneficial or mission critical to do? I'd appreciate to hear some personal experience. I'm trying to architect a design which allows for future modularity (which is why I want a feel for nodes).", "I would add to ", "'s answer that ROS is (a form of a) component based software framework, making CBSE possible. Nodes are just the concrete implementation of the ", " concept in ROS, which offer services and consume and produce datastreams. Packages then group several such components ..", ".. in coherent sets (although guarding the coherency is the responsibility of the author / maintainer, so is not necessarily guaranteed).", "My personal experience is that if you see all of ROS as one concrete example of a more abstract conceptual framework (ie: CBSE), things make sense sooner."], "url": "https://answers.ros.org/question/257292/why-does-ros-have-nodes/"},
{"title": "ROS Battery Status", "time": "2017-02-16 17:08:11 -0600", "post_content": [" ", " ", "Hi,", "Last December, my team and I participated in the first Maritime\u00a0RobotX Challenge is an international team competition uniquely designed to evolve into a multi-platform competition to include maritime, aerial and submersible tasks to broaden students\u2019 exposure to robotics applications and technologies.\u00a0Our team utilized the ROS for some tasks, such as Simultaneous Localization and Mapping (SLAM), Path Planning, and Color Recognition. I am a power system lead of the team, and I have been trying to program in ROS to check the status of our lead acid (standard car) batteries as well as some Lithium Ion battery packs. Unfortunately, none of the team members knows how to approach to this problem. Please help!", "Duplicate of ", "How about using the ", "? You publish a diagnostic message with the current battery value.", "Check out "], "answer": [" ", " ", "There's a pretty substantial gap between writing software and checking the state of your batteries, and you'll need support from your hardware to cross that gap.", "At the most basic level, battery monitoring is generally done by measuring the battery voltage, measuring and integrating the current through the battery, or both. This is an entire field of study with chemistry and is a bit too broad to cover here, but it's important to remember that the relationship between battery capacity and voltage can be highly non-linear depending on your battery chemistry.", "I'd suggest that you buy (or possibly build) hardware that can measure the SOC (state of charge) of your batteries, and then connect that hardware to your computer and write a ROS node that translates that data into a ", " message. I believe there are dashboard widgets that can consume and display this message in a reasonable way.", "P.S. - your question makes reference to a demo, but doesn't link to it. ROS is a very broad community, and most of us aren't familiar with all of the demos, so a link would be helpful."], "url": "https://answers.ros.org/question/254899/ros-battery-status/"},
{"title": "Controlling each UAV separately in a UCTF project", "time": "2017-02-28 07:14:49 -0600", "post_content": [" ", " ", " ", " ", "I have a multiple UAV launched in Gazebo world using the ", " project which launch two groups of gold and blue colors which compete against each other in a game cube and grass pitch in the Gazebo world. The main goal is to implement path planning algorithm for autonomous collision avoidance.", "After spawning both blue team and gold team e.g spawn_blue 5 10 & e.g spawn_gold 15 19 in a terminal. In attempt to understand the system I did the following a) I attempted to get control over one UAV in a team e.g iris_5 in blue team. b) Give the UAV position coordinates to follow in ros waypoint navigation c) Inspected each team control script, control_team_blue and control_team_gold to understand how to controlling team behavior. ", "Therefore I would like to ask for help to understand the following questions: "], "answer": [" ", " ", "UCTF is flying N vehicles at a time simulating as closely as possible real flight configurations. Such as running the real autopilots. This is an active subproject of the SASC project, and under active development. At the end I hope to fully document how to run it for public consumption but right now we're focused on supporting others inside the project.", "Each vehicle has it's own ROS master on which it talks. If you watch the console output of the startup you can see the masters come up on specific ports.", " Contact each drone on it's own master URI. You have full contact there. Or you can use QGroundControl or other ground station.  ", "I'm not sure I understand this question. I think in general you're missing that there are multiple masters. ", "Again see each individual master. ", "Also if you're just wanting to simulate multiple drones and not the full SASC setup I recommend using the --no-payload option, it will turn off a lot of integrations specific for the SASC project. ", "To clarify on the fourth question, the control.py script of the project subscribes to /uav_namespace/mavros/state topic. However, when echoing the topic in the terminal, no data is published yet.", "For the fifth question, I thought if I know exactly where the interest topic is published(/mavros/state and /mavros/global_position/global) then I can run it thereafter then the control_team script running.", "To also add information about my project, in summary I want to be able to control each UAV using waypoint navigation, also using the existing path planning algorithms which comes with the SASC project."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "In summary how does the UCTF game concept function?", "How can I find the publisher for each UAV for the mavros topics : mavros/state and /mavros/global_position/global e.g /iris_5/mavros/state and /iris_5/mavros/global_position/global?", "How can I send waypoints to a UAV separately e.g iris_5?", "Why does the state of each UAV not published to mavros e.g /iris_5/mavros/state", "How can I find where the system publishes of each UAV of /mavros/state and /mavros/global_position/global?"], "answer_code": ["In summary how does the UCTF game concept function?\n", "How can I find the publisher for each UAV for the mavros topics : mavros/state and /mavros/global_position/global e.g /iris_5/mavros/state and /iris_5/mavros/global_position/global?\n", "How can I send waypoints to a UAV separately e.g iris_5?\n", "Why does the state of each UAV not published to mavros e.g /iris_5/mavros/state\n", "How can I find where the system publishes of each UAV of /mavros/state and /mavros/global_position/global?\n"], "url": "https://answers.ros.org/question/255812/controlling-each-uav-separately-in-a-uctf-project/"},
{"title": "robot web tools - hosting page in AWS", "time": "2017-03-10 17:29:57 -0600", "post_content": [" ", " ", " Hey All,\nI'm trying to connect my robot to the web for easier user interface. I found the very helpful libraries in  ", " , where I used the NAV2DJS widget to connect my turtlebot, move_base server and map server to a browser.  ", "I went further and put the html file in an Amazon Web Server Ubuntu machine under /var/www/html/project_name, thinking that I could now access the webpage from any device, no matter which network they are on. The caveat I'm coming across is that I can only interact with the map on machines that are on the same LAN where roscore is running. I'm no web programmer by any means and my hunch is that the websockets aren't moving across different networks, only among the same network.", "Question, how can I run the NAV2DJS widget in AWS and control the robot from different machines on any network?"], "answer": [" ", " ", "You'll need to make sure that the machine that is running ", " -- and probably the TF2 web publisher as well -- is accessible from outside your own network.", "Depending on your local network setup that might mean that you'll have to open up some ports in your firewall (", " for starters if you used the defaults) and / or will need to forward them in your NAT device (typically a consumer router).", "After that is done edit your HTML file to make sure that clients will try to connect to a public IP where those ports are reachable. If you have a cable internet or ADSL connection, that would most likely be the public IP address of your home router.", "Note that this is not really a ROS problem: any time you'd want to make a resource available to hosts outside your own network you'd have to follow similar procedures."], "answer_code": ["rosbridge_server", "9090"], "url": "https://answers.ros.org/question/256690/robot-web-tools-hosting-page-in-aws/"},
{"title": "Latency between nodelets", "time": "2017-04-12 04:35:23 -0600", "post_content": [" ", " ", "Dear all,", "I have implemented stereo_image_proc with customized algorithms. Now I am trying to measure the latency in the pipeline.", "I have observed that each nodelet has inconsistency latency ranging form 0 to 60ms. \nI have measured the latency as follows\nRos_latency = (nodelet1_publishing_time - nodelet2_subscribing_time)", "How can I minimize these delays so that making the whole system consistent.", "Thanks in advance", "I guess you mean nodelet2_callback_time or something similar? The subscribing time is not very meaningful in this investigation. How exactly do you measure the time? Do you call ros::Time::now() after publishing and at the beginning of the receiving callback?", "Yes, I am referring to the nodelet2 callback time. I am using gettimeoftheday() to just before publishing in nodelet1 and just after the callback called in nodelet2. Also, I am using the exact sync polocy.", "So your timing suggests that indeed the ROS system takes varying amounts of time to call your callback. How could that be ", " the case? ROS internal maintenance and callback queues/schedulers is not deterministic in the end. However, I am not sure if 60ms is maybe too much to call it Jitter."], "answer": [" ", " ", "To get a more specific answer you'll need to provide a lot more information about how to reproduce your issue. In particular you need to understand how any specific component is integrated into the larger system. ", "Questions that would be helpful for improving performance. ", "If you can also get 0ms latency that suggests that it can go quickly. And then the question is why is it sometimes not delivering it quickly in other cases.", "My speculation guessing at many of the answers to the above question is that you're callback queue is being starved for threads/processing time when you get the large latency numbers. When resources are exhausted we have many ways to gracefully degrade. Everything is queued etc, and if queues overflow content will get dropped. Adding a large processing load in other callbacks can starve other callbacks in the queue etc.", "Yes, I am running the nodelets in the same process and all the nodelets are running.\nThe nodelets are consuming not more than 20 threads at a time.\nAvailable threads to the manager are 88. \nNothing else is running on the system except the roscore and rosbag play.", " ", " ", " ", " ", "In short: No, I think you will not be able to reduce the latency between the two nodelets. This is because it originates in the internal ROS system and tweaking it is most possibly overkill. ", "\nIf there actually is a hard timing requirement, you have two options. Weakening the requirement or using a real time system.", "In general, varying latencies between message publishing and receiving are expected and there is few one can do about it.", "However, usually you do not need stable latencies. In most cases you are just interested in ", " multiple topics. If that is what you want, have a look at  ", " , especially point 7, Policy-Based Synchronizer. There are example for synchronizing multiple topics either by exact or by approximate time. ", "I am not just interested in stabilizing them. I am interested more in reducing the whole latency.\nThe whole system can afford < 70ms of latency. The peak latency I am observing was 130ms and the average latency was around 110ms.", "If the 70 ms are a hard requirement you are in need for a real time system. Even if you manage to decrease the mean latency there can be no guarantee that it will not take longer at some point. Afaik timing assumptions are just not possible without RT.", "In the overall system the latency observed was varying due to the latency between nodlet2 and nodelet3. This has spikes. Sometimes it was 0 ms delay and sometimes it was 50 ms delay. Is there a way to reduce this. That will be great if we can.", "I think the answer is no. Updated my answer to reflect that.", "On desktop hardware messaging back and forth can get into the kHz range for closed loop systems over loopback network. As such 60ms strongly suggests something is blocking or being starved for resources on an order of magnitude higher than the simple internal communication overhead."], "answer_details": ["Are you running the nodelets in the same process?", "Is there other nodelets running?", "How many threads are available in the nodelet manager? How many do you expect used at any given time?", "Are you CPU constrained?", "What else is running on the system?", "What else is publishing or subscribing in the node on the system?", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/259074/latency-between-nodelets/"},
{"title": "Can anyone explain the name of every distribution of ROS?", "time": "2017-05-15 05:33:53 -0600", "post_content": [" ", " ", " ", " ", "I think it must be very interesting.", " means come back with glory?The Image of every distribution is really fantastic.", " :D"], "answer": [" ", " ", " ", " ", "ROS release names are based on / inspired by turtle species names (the animals). I'm not sure they actually 'mean' anything apart from what mental imagery they evoke.", "The names are typically chosen after consultation with the community:", "As to why turtles are used, I think it's ", ".", "Thanks.Hope I can learn it better."], "question_code": ["Diamondback"], "url": "https://answers.ros.org/question/261759/can-anyone-explain-the-name-of-every-distribution-of-ros/"},
{"title": "How should a subscriber interpret multiple messages upon subscription?", "time": "2017-05-30 14:54:41 -0600", "post_content": [" ", " ", " ", " ", "If I have multiple nodes who each latch a message onto a particular topic, a node that later subscribes to that topic will receive multiple messages immediately upon subscription.  If those message types do not include a timestamped header, how should that message pair be interpreted?", "The way I wish (and previously thought) topics were conceptualized, a topic would represent a stream of messages that supersede each other (like, the current position of something).  In that case, it seems like latching should be \"per topic\".  When a new subscriber subscribes to the topic, it makes perfect sense that they would like to see what the current state of the topic is, even before another message is published.  In this case, I would expect that when a publisher latches a message, any other latched message should be unlatched -- future subscribers will want the most recent message on that topic, regardless of source, and messages prior to the most recent one are now obsolete.  And this is, in fact, the behavior that publishers in the same node have.", "But, when the publishers are in different nodes, multiple messages are sent to a new subscriber.  This is different from my conceptualization.  I understand that the way topics work, in fact, is that latched messages are resent by their publisher (on a per-node basis) to a new subscriber whenever it detects that new subscriber.  But WHY does this behavior happen?  What is the rationale behind having all publishers with latched messages send them upon a new subscription?  In my conceptualization, the idea is \"a new subscriber should be able to know the current state of the topic upon subscription, and that state is defined by the most recent message published to the topic\".  What is the equivalent idea for receiving multiple messages upon subscription?", "EDIT: To clarify, the difference between a new subscriber connecting to a topic and immediately receiving two latched messages, and an old subscriber receiving those messages when they were published, is that approximate time synchronization is lost.  Assume a topic with superseding messages (and see gvdhoorn's answer for why that might not be a good assumption) that don't include a timestamped header.  If publisher 1 publishes and latches message 1, and then an hour later, publisher 2 publishes and latches message 2, the old subscriber knows that message 2 represents the true state of the topic.  The new subscriber receives message 1 and message 2 in a random order upon subscription and has no way of knowing which message represents the state of the topic.  See answers for responses to this."], "answer": [" ", " ", " ", " ", "(I'm just a user, as you are, so the below is based on my understanding of what is there, not what it was designed to do or how it was designed)", "This probably does not completely answer your question, but may be something to think about: say I have ", " publishers on a single topic (something like ", "). Each msg includes an ", " field or similar, and each publisher sets this ", " field to indicate the originating temperature sensor (maybe the location, maybe just a numeric ID).", "If we follow your idea -- each topic contains the 'latest state' -- then we have a problem: which of the ", " sensor msgs represents 'the latest state'? There is no single message that contains the state of the entire system (which is an ambiguous thing anyway). Only by keeping track of msgs that have been received, and storing the data in some internal model would consumers be able to piece together what the aggregate state of all temperature sensors is.", "It could still make sense for these publishers to latch their publications though: it allows late joiners to instantly become aware of the latest available state of all temperature sensors indepenent of their sampling & publication period. That is what latching is for.", "There is no requirement (or even a convention) for topics to represent \"the current state\" of anything, unless the author of the node, and thus the designer of the dataflows, decides that it makes sense to him. Named, typed topics are basically an addressing scheme for dataflows, nothing more. It is then the responsibility of the consumers of those dataflows to figure out how to handle incoming messages.", "Adding timestamps (ie: headers) to messages is one way in which producers can make it easier for consumers to make sense of incoming messages. The ", " in the example above is of course similar to what a ", " provides with its ", " and ", " fields, although it's only the space coordinate of a sensor. The time coordinate is missing from the message.", "Some additional comments:", "I would expect that when a publisher latches a message, any other latched message should be unlatched -- future subscribers will want the most recent message on that topic, regardless of source, and messages prior to the most recent one are now obsolete.", "Your main assumption here is that ", " topics ", " carry some sort of message that encodes some kind of 'state union'. That is not true.", "And this is, in fact, the behavior that publishers in the same node have.", "Slight nuance: only if those publishers publish to the same topic.", "In my conceptualization, the idea is \"a new subscriber should be able to know the current state of the topic upon subscription, and that state is defined by the most recent message published to the topic\".", "and that is how latching works, but it's up to the subscriber to make sense of the incoming messages. See my earlier example for why your idea ...", " ", " ", "Based on understanding from gvdhoorn's answer, my answer to this question is that topics can be multi-channel or single-channel.  My question describes single-channel topics -- that is, topics that describe the state of exactly one thing.  I think any message type that doesn't have a channel-differentiating field nor a message-prioritizing field must constitute a single-channel topic, and I don't think multiple latched messages makes any sense in this situation.", "For instance, consider a small LCD that subscribes to topic /status_msg which carries std_msgs/String.  Whenever it receives a message, it prints that message on the LCD.  There's no space for multiple messages; any new message erases the previous one.  If two publishers latch messages to this topic, when the LCD subscribes, we get undefined and undesirable behavior -- the desired behavior would be for only the most recent message to be delivered to the LCD upon subscription.", "This can mostly be resolved by adding a message-prioritizing field such as a timestamp.  If the /status_msg topic above carried a message containing both a string field and time field, then the LCD could discard the older of the two latched messages even if it happened to arrive after the newer message upon subscription.", "If topics were only single-channel, I think that's an ugly solution -- we potentially have multiple zombie publishers consuming resources yet producing messages (to new subscribers) that will never be useful, and it makes many message types essentially invalid (in the general case) for use in topics.", "But, topics are not only single-channel.  A single topic may contain many channels, each channel representing the state of exactly one thing.  A good, simple example is gvdhoorn's where the topic /temperatures carries a message type that contains an id field, describing the source of the temperature measurement (\"ambient\", \"intake\", \"engine\", etc), and a float field for temperature.  In that case, different publishers have current information for different channels so it makes sense for multiple publishers to latch messages simultaneously.  A more complicated example is how TF works on /tf_static.", "One alternative to multi-channel topics might have been (prior to when ROS was actually implemented) to make all topics single-channel, and then have more sophisticated subscription rules (like subscribing to /temperatures/*, which would listen for messages from /temperatures/sensor1, /temperatures/sensor2, etc).  But, that just trades topic content complexity for subscription complexity.  And, it limits the channel identifiers to strings (sections of topic address), which the actual implementation does not limit.  So, I think I see the design philosophy now and it seems pretty reasonable given those considerations.  I happen to work nearly exclusively with single-channel topics so I miss some capabilities and constraints that would exist if not for multi-channel topics, but at least now I think I understand."], "answer_code": ["n", "temperatures", "id", "id", "n", "id", "std_msgs/Header", "frame_id", "stamp"], "url": "https://answers.ros.org/question/262858/how-should-a-subscriber-interpret-multiple-messages-upon-subscription/"},
{"title": "Managing large video streams (4K ...)", "time": "2017-05-17 10:52:57 -0600", "post_content": [" ", " ", "Hi,\nWe want to equip our car mounted system with recording capability for high-quality video streams, possibly several Full HD and maybe 4K cameras. We need frames from those cameras to be recorded with a timestamps synchronized with other sensors (GPS,...) and to be stored at reasonable framerate (15-30 FPS). What is generally a good approach for implementing this?", "Currently we have 4 Full HD webcams in the system connected to a laptop through a single USB 2.0 port and we already face serious problems. The USB capacity prevents us to set the full resolution (ROS complains on startup of the nodes) so we have to run at 720p and 15 fps. Even with this, we only record around 7 fps on our Core i7 laptop. I suspect that the on-the-fly compression to jpeg (frames are stored compressed) can be the problem. I do not see this scaling very well. Do we need multiple USB controllers, multiple PCs, LAN based cameras (e.g. PointGrey)? Or is recording frames as JPEG images instead of a video codec (e.g. H264) even a good practice? It seems very wasteful to me. Is a more reasonable approach to record each stream as a separate video using an external SW (not ROS) and just note the time of the first frame? Would timing for the following frames be reliable? We can tolerate error of 500 ms over recording duration of 1 hour.", "Thanks!", "I have no real experience with this, but if you're not processing your video in real-time / while capturing it, I would try not to use ROS for the capture. As you already wrote, there are other ways to capture video streams, which are probably more efficient than storing individual raw / jpeg ..", ".. shots in a directory / bag file. The 'only' problem would indeed seem to be the sync between your video streams and other sensor data. If your capture system encodes a (wall) clock into your container then you should be able to sync everything using that. A small node that then decodes the ..", ".. video stream and uses the time code as the ROS timestamp could be an option. Or -- and probably more efficient -- use the embedded time codes to convert your video containers into bag files. That would all be off-line, so should reduce problems with resource usage that you now have...", ".. After conversion to bag files, you can then process things as you would normally do.", "gvdhoorn: Yes, that is the most likely path we are going to take. For our later application we need random access to frames anyway so it would then make sense to extract images as separate files and skip the bags completely."], "answer": [" ", " ", "I've run into the same issue but my application was tolerant of having a reduced frame rate so I haven't really solved it, but have thought about some approaches:", "If the camera can output mjpg directly then it should be possible for a ros node to grab that stream and reformat into the ros jpg format without cpu consuming decompression and re-compression.  I don't know if any nodes can do that now.  The quality of the time stamping is an unknown in that case.", "What is the minimally expensive/sized/power-consuming embedded board that can connect to a single usb camera and capture and compress at full resolution and frame rate?  Every camera could be paired with an embedded board.  It would be nice to have gigabit ethernet, so uncompressed output would also be an option, that rules out the Pi and similar boards.  The original Pi is not powerful enough, I don't know about the 3- and if the board costs more than $100 then the aggregate price starts to approach that of a small low cost gige camera (which won't produce compressed output, but can be very good quality, have roi functions, and can have different lenses, though not autofocus or powered zoom without additional cost and complexity...).", "The nice thing about the consumer usb cameras is that they can be good quality for the price, the economy of scale of skype/video-chat helps that.", "Or go with the industrial gigabit ethernet cameras, but also pair them each with embedded boards (which need to have gigabit ethernet) for compression- but the cost here starts to get high.  The gigE cameras are going to provide the bests solution to synchronization (you can have digital pulse signals sent to each camera to trigger frames, or possibly record in the camera the time of arrival of a lower rate sync pulse), but your 500ms seems so large that it doesn't require that.", "Another route is the network camera which is usually aimed at the security market, which are sort of the reverse of the gige camera in that they might have only compressed output but no full quality raw images.  Interchangeable lenses are possible too, and they are lower cost.  I think there are some standard data stream formats meant for dvrs.  I've seen some ros driver support but I haven't looked into it much.", "It would be great to hear from anyone else already doing any of this rather than going into a lengthy integration r&d effort to select systems and pair them and test them out.", "What I have actually done is throw additional computers at the problem, allocating different tasks among them- separate out compression and recording and cpu intensive real-time critical stuff onto different systems as much as possible (though issues with network bandwidth can arise)."], "url": "https://answers.ros.org/question/261964/managing-large-video-streams-4k/"},
{"title": "What is the best mapping/localization package for using pre-made maps?", "time": "2017-05-18 21:40:52 -0600", "post_content": [" ", " ", "I am currently attempting to map a building using the gmapping package, but I have some issues with the accuracy of my map. Right now I'm relying on the wheel odometry and a 2D laser scan. I figure I can make the map more accurate if I can use an existing floor plan and localize based on that, but I'm not entirely sure of how to draw and dimension a map for usage with a package. I also don't know which package would allow me to import this map and best localize based on it. Any help would be greatly appreciated."], "answer": [" ", " ", "The ", " package is a popular choice for localizing within premade maps. However, I've only ever used it to localize within maps I created using ", ". You could theoretically fill out a ", " message by hand, or through some other method, to generate your map, and then publish it to amcl. However, this may not work how you expect, as the robot's view of the building is percepted only through its laser scan. Anything that is the same height as the laser will be picked up in a scan, including chairs, desks, plants etc. Just inputting walls into a map won't work if there is anything except bare walls within your building.", "I would personally recommend improving the odometry and using gmapping to get the map.", "This is about the answer I was thinking I was going to get. Fortunately for my case, all objects in the room are stationary, and square. However, I was thinking it would be more time consuming and difficult to navigate effectively based off of an existing map than it would be to just use gmapping."], "url": "https://answers.ros.org/question/262103/what-is-the-best-mappinglocalization-package-for-using-pre-made-maps/"},
{"title": "[ROS2] generating DDS .idl files from .msg ROS Message Format", "time": "2017-08-09 10:26:31 -0600", "post_content": [" ", " ", "Hi all,", "I'm trying to create .idl descriptions from the ros message description format.", "There is the ", " package and cmake macro.", "How would I use this for instance for the std_msgs or trajectory_msgs package?\nI'm currently trying", "In ", "Thanks\nJochen", "Are you trying to do this to use the IDL outside ROS2, or are you trying to generate msgs for your ROS2 package?", "yes, i'm trying to use that idl with plain rti dds on a non ros machine", "i feel like i'm just missing a small step or so. there is a rosidl_generator_dds_idl__arguments.json file generated in the build folder.. seems like the python script is not called correctly or so", "Are you getting an error message?", "Also, you should be able to get the idl files out from a normal ROS 2 build. They will have been installed I believe.", "i do get an error message from the install target which is looking for  /ros2_ws/build/std_msgs/rosidl_generator_dds_idl/std_msgs/msg/idl/Bool_.idl\" and does not find the file.\nI do not see the .idl files installed somewhere. that would be the easiest thing of course :-)"], "answer": [" ", " ", "You can get the ", " files that we generate for a ", " file if you build from source and copy them out of the installation folder. However, you need to build with an rmw implementation that actually uses the ", " files for them to be generated. The reason you don't see them, probably, is that you're using our default rmw implementation of Fast-RTPS which does not generate or consume ", " files currently.", "However, if you build, for example, with Connext it will generate and install the files to the install folder, in the case of ", " it would be in ", ". The reason you have to use Connext (or an rmw implementation like it) is that the generated ", " files are specific to the underlying vendor. Despite being a standard there are small variations in the use of the ", " files between vendors. For example, the way in which you specify a \"key field\" in the ", " file differs between vendors, I believe it is different for Connext and OpenSplice at least.", "So the easiest way would be to use Connext or OpenSplice and build from source. Then the ", " files will be generated for you and installed. No matter which you use the ", " file produced would be a good starting place as the differences between them are small.", "You should also be able to use the generator directly to generate generic ", " files from our ", " files (all the code and tools are there), but you might have a hard time right now since we don't do that ourselves. If you find a change that would make that easier then let us know, we'd be happy to make that use case better even though we don't need it ourselves.", "works like a charm, thanks a lot !\nsince i want to use rti dds anyways on the other machine it's the best solution", " How can i access the published and subscribed data by accessing the idl files ?", " please don't ask questions in comments. This is a solved question from close to a year ago people won't be reviewing this question.. Also you already asked this question [here](https://answers.ros.org/question/293158/how-can-i-use-the-ros2-idl-files-to-connect-with-a-non-ros-machine/", " : noted for future and deleted the redundant question"], "question_code": ["rosidl_generate_dds_interfaces", "rosidl_generate_dds_interfaces(std_msgs_idl IDL_FILES ${msg_files}OUTPUT_SUBFOLDERS \"idl\")\n", "std_msgs/CMakeLists.txt"], "answer_code": [".idl", ".msg", ".idl", ".idl", "std_msgs", "path/to/install/share/std_msgs/msg/dds_connext", ".idl", ".idl", ".idl", ".idl", ".idl", ".idl", ".msg"], "url": "https://answers.ros.org/question/268396/ros2-generating-dds-idl-files-from-msg-ros-message-format/"},
{"title": "Easiest way for non-ROS program to talk to a ROS node", "time": "2017-08-03 06:58:27 -0600", "post_content": [" ", " ", " ", " ", "I have a totally ROS-independent application (the \"app\", say written in python, and launched from the command line) running on the same computer (i.e. the linux instance, with direct access to the same disk). The app has no ROS includes or dependencies. Note that because of this I don't have to rely on http per se. ", "What is the best/simplest way to have that app to talk to a ROS node? Performance is not an issue as the amount of data going back and forth would be very small. To be specific, the data could be represented in JSON file of a few hundred bytes being sent and received every few seconds.", "For example, one way is to read and write a shared file which is very general and generic but not so elegant. ", "Is it possible to read and write ROS parameters from outside ROS?", "What are better ideas?"], "answer": [" ", " ", " ", " ", "You don't give us any info on what you'd like to communicate, so this is hard to answer, but: there is a similar / duplicate question from just a few hours ago. See ", ".", "But this -- and ", " -- are essentially duplicates of many others. Please try to use the search or Google (add ", " to your query).", "Edit:", "To be specific, the data could be represented in JSON file of a few hundred bytes being sent and received every few seconds.", "That is exactly what the tools (", ") in the answer to ", " actually do: provide you with a JSON bridge to a ROS nodegraph.", "I have done a ton of googling", "this is probably google and my search bubble with them, but the first result for me when searching for ", " is ", ".", "Edit2: another option might be ", ".", "and another one ", ", in particular ", ". It wasn't meant for this, but could be an option. Just run client and server on the same machine.", "Actually those are not essentially duplicates as I am not talking about networking. I have done a ton of googling and looking at libraries and talking to developers and the result is still open. I won't go through the results of all my research here (no room) but I will update my question.", "You're asking how to exchange data with a ROS application. ROS is networked, so any interchange infrastructure is going to use networking. It's the most natural way to do it.", " I see that, but my needs are so lightweight and all the libraries I've seen seem to be pretty heavy weight so I am not quite satisfied with the options I have so far, which is why I am consulting the community!", "heavy weight", "on the ROS side or the non-ROS side? Working with JSON should not need too many dependencies.", "And ", " is basically two nodes?", "on the ROS side, and not just in terms of my programming but on the load it would put on the on board processor which is a Pi for now. I realize that it doesn't have to be on the onboard, but for design reasons I prefer that.", "Have you tried ", "? I cannot imagine that it is resource intensive for just a few hundred bytes of json each second.", " ", " ", " I had to deal with similar situation in my project where I had to communicate data back and forth between ROS and iOS app. The approach that I found the easiest was to create a TCP server-client application using Python. You can develop a ROS node which is also a TCP server (I used twisted protocol of python for this (you can refer to ( ", " . This server, being a ROS node can have access to all the ROS parameters. You can then create a TCP client in python which can then receive the information (which could be any message or ROS parameters which you would like to transfer) from the TCP server. ", " ", " ", "I've done this ", " I'm sure you could adapt that code pretty quickly."], "answer_code": ["site:answers.ros.org", "rosbridge_suite", "ros json", "rosbridge_suite", "rosbridge_suite"], "url": "https://answers.ros.org/question/267988/easiest-way-for-non-ros-program-to-talk-to-a-ros-node/"},
{"title": "Fix subcriber rate and drop other messages", "time": "2017-07-26 06:58:15 -0600", "post_content": [" ", " ", "I have an IMU topic that is published at ~1kHz and I want my suscriber ton only process 1 message out of 50, discarding the other messages. Is there a c++ way to do this ?", "I am aware of the ", " command but that's not what I am looking for."], "answer": [" ", " ", " ", " ", "Keep a counter in the callback that returns without doing anything unless ", "- but there is a deserialization performance hit involved there", "The topic_tools throttle should not have the deserialization penalty depending on how ", " works.   ", " A nodelet version of throttle would be nice to eliminate additional copies between throttle and your subscriber- and it turns out it already exists:  ", "  (is there any reason why there should be a non-nodelet version of throttle that isn't just a wrapper around the nodelet?).  You would have to make your subscriber a nodelet and put it into the same nodelet manager as the throttle.  Copying 50 messages a second is not a big proportion of the cpu being compared to the resources required for handling the other 950 messages, so maybe the nodelet isn't worth the trouble. ", "Additionally, in ", " it appears to be using a template system rather than the generic ShapeShifter message that the regular throttle uses, which means it is deseerializing and reserializing everything, making it slower than regular throttle as the test result below shows.", "The template system also requires you to create your own nodelet that instantiates the type you want to throttle, which is non-trivial, but ", " and I've made an ", ".", "A different and hacky approach is to keep track of time since the last processed subscribed message in your callback, and do a blocking sleep with the remaining 1/50th of a second while also having a  subscriber queue_size of 1. ", "If the queue is full and a new message\n  arrives, the oldest message will be\n  thrown out. Additionally, the message\n  is not actually deserialized until the\n  first callback which needs it is about\n  to be called.", "One unexpected result is that a python subscriber plus regular throttle doesn't run any faster than a python or C++ node subscribing to the full rate Imu topic.", "The rospy queue throttling node got increasingly behind with the messages it was receiving, while the C++ version worked fine and so far is the best solution for the most efficient throttle.", " All the code is on  ", "Yeah obviously but the fact is that I need to save as much CPU as possible so I was looking for a solution that prevent the call of my callback function. Is there any solution to do that ?", "No. ", " needs to throttle the pub rate of the sending party. It's either the middleware, an intermediate node or the receiver. The work will have to be done somewhere.", "'Easiest' would probably be to lower the pub rate of the IMU. If you want to avoid callbacks, don't send messages.", "Thanks for your answer. The problem is, I cannot lower the publishing rate and adding an intermediate node will increase CPU use. I will go for the counter since at least it will avoid some useless operations.", "So does the IMU not have a ROS node? How is it publishing the messages? Could you not publish 1/50 msgs there? Or do you have other consumers that ", " the 1kHz rate?", "The imu topic is published by a ros node that only serves as a bridge from a propretary library (used by several sensors) and I cannot modify that for now.", "If you can't change the node that forms the bridge, then I'm afraid there is no way to change this. Contrary to some other middlewares, ROS1 does not separate communication from computation, meaning that you can't change anything about the communication part without changing the source of the node.", "I added some additional suggestions above, though they are more questions than answers- maybe someone else could weigh in on what is happening in the code or a set of comparison tests could be set up.", "The nodelet throttle is a good suggestion, but it would not avoid having those msgs being published first. If that is not a problem for OP, then it would be a viable option."], "answer_details": ["A python node publishing Imu at 1000 Hz takes about 14% cpu.", "regular throttle takes about 7% cpu", "nodelet throttle (in a wrapper) takes 10% cpu", " takes 2% cpu, though the queue isn't getting updated properly.", " takes 5% cpu and timing seems solid", "The python node subscribing to Imu (and doing nothing with it) at the throttled 50 hz is negligible.", "Python imu subscriber without throttling takes 8% cpu", "C++ imu subscriber without throttling takes 7% cpu ", " ", " ", " ", " "], "answer_code": ["counter % 50 == 0"], "url": "https://answers.ros.org/question/267413/fix-subcriber-rate-and-drop-other-messages/"},
{"title": "nodes and topics -like i'm 5", "time": "2017-08-23 17:50:18 -0600", "post_content": [" ", " ", "I am doing the ROS beginner tutorials and am on the 'understanding nodes' one. What actually is a topic? I see that a node is essentially an executable file and they communicate w eachother using topics but what is a topic?"], "answer": [" ", " ", "A topic is the way nodes communicate with each other. Think of it like a newsletter--the terminology for topics is exactly the same.", "Say you have a node that generates images, and it wants to send them to other nodes. The node ", " that it is going to publish ", " messages on a ", ". The topic has a URI, which is a path like ", " or ", " (it can be whatever you want it to be). The ROS master keeps track of what nodes have advertised what topics.", "Now say you have another node that processes images, and it wants to get them from the other node. This node ", " to ", " messages on the same topic.", "The ROS master sees the publisher and the subscriber, and it tells the nodes how to connect directly. Now, whenever the first node publishes an image, it gets sent (over a TCP socket) to the subscribing node. A topic can have any number of publishers and any number of subscribers; every subscriber gets every message from every publisher.", "Any questions?", "Perhaps to make it more explicit: a topic is essentially nothing more than a name (or an address). It is not a magic entity that is actually, concretely involved in the exchange of msgs, but is only used to bring producer and consumer together.", "After that they will exchange information directly, ..", ".. without caring about the topic or the master.", "And also note that using topics (alone) is just one such way to implement a communication system such as ROS. There are also ", " systems (where you get all msgs of a certain type (say the ", " that ", " mentions)) ..", ".. and ", " systems (in which you express interest in msgs that have a certain contents, with a filter expression fi (ie: I want all msgs that have a field called ", " with a value ", ")).", "thank you, helpfull", "If this answered your question, then you should mark it as correct."], "answer_code": ["sensor_msgs/Image", "/image", "/vision/image", "sensor_msgs/Image", "sensor_msgs/Image", "field0", "464"], "url": "https://answers.ros.org/question/269466/nodes-and-topics-like-im-5/"},
{"title": "Octomap in moveit", "time": "2017-09-10 16:31:48 -0600", "post_content": [" ", " ", " ", " ", "I  already can see octomap in rviz but the problem is the octomap is quite different from the real world kinect sees. I don't know why. Do I need to adjust any parameter for configuration file?\nBesides, the refresh rate is quite low. Anybody know how to change refresh rate of octomap?\nThanks for any suggestion.\n", " "], "answer": [" ", " ", "Octomap updates are pretty expensive CPU-wise, so you might be limited by that. You can check CPU consumption using the ", " command line utility. Also note that you should be using the Depth Image Occupancy Map Updater (not the PointCloud Occupany Map Updater) with RGBD-type cameras as it tends to provide better performance (see ", ").", "Regarding the shape of the octomap it would be helpful if you could post/link a screenshot (by editing your question). It should be noted that RGBD-cameras generate fairly noisy data (esp. at longer ranges), so deviations from how the real world looks can be expected.", "Hi Kihlbrecher,\nI attached the screenshot. In the real world, there should be a desk in left front of the arm. But in the octmap, I don't understand why there is a big block with no shape. Even I put something on the desk, it shouldn't be like that. It is too inaccurate. \nThanks", "It's kind of hard to make out how the exact setup looks like. One obvious thing to do would be using the Camera display in rviz and using it to reproject the octomap into the camera image to check it for consistency.", "Now my another problem is I want to use depth image from meta glass which runs on window system. I already transited data image from window to linux and published it on a topic. But even I let moveit to subscribe that topic, it can't form octomap. Is that because I need to modify type of depth image", "You have to make sure that the proper frames and camera_info are published for that image."], "answer_code": ["top"], "url": "https://answers.ros.org/question/270619/octomap-in-moveit/"},
{"title": "hector_mapping reducing cpu consumption?", "time": "2014-04-09 22:18:13 -0600", "post_content": [" ", " ", " ", " ", "Hi\nI'm using hector_mapping on pandaboard es with arch linux+hydro installed. I have hokuyo urg 04lx. However when i start mapping with default settings. \nrosrun urg_node urg_node\nroslaunch hector_mapping mapping_default.launch\n(Static transform from base_footprint to laser)\nNothing else\nHowever, when i type \"top\" to see its cpu usage. It shows nothing below %105.. how can i reduce this. It this normal? Am i doing something wrong?\nIs it better option for me to use laser_scan_matcher + gmapping", "Edit:\nI installed ros-base from Aur repository using yaourt.\nAfter that i cloned hector_slam from from github to my catkin directory and catkin_make only hector_mapping (i installed every dependencies of it)"], "answer": [" ", " ", " ", " ", "To complete @StefanKohlbrecher's answer, the problem is that you built a part of the packages without the proper release flags. Basically, when you use the AUR's PKGBUILDs, the release flag is forwarded to CMake, but if you compile from source with ", ", you need to take care of this yourself.", "That being said, I just added ", " and its dependencies to the AUR. Remove whatever you installed with ", ", install ", " with ", ", and you should be just fine. If you encounter any other missing packages in the AUR, please send a package request ", ", or even better: make a fork, generate the PKGBUILDs yourself, and send me a pull request. Note that generating, compiling, installing and sending to the AUR the missing PKGBUILDs took me less than 5 minutes, so this is definitely worth installing things this way, rather than relying on ", ".", " ", " ", " ", " ", "I haven\u00b4t tried things on Pandaboard, so not sure what to expect. That being said, I also noticed that CPU consumption lately was much higher than normal (nearly maxing out a i7 core on our USAR vehicles). As there haven\u00b4t been changes to the code in quite some time, I suspect that there might be a problem with one of the used libraries or the build settings. Can you edit your question with information on how you generated the executables (installed from .debs or compiled yourself)?", "/edit: There seems to be something wrong with compile flags, see ", "/edit2: I\u00b4d recommend building explicitly in Release mode as suggested in above ticket, just to make sure.", " ", " ", " ", " ", "I think it is normal. I run hector_mapping on an Intel PC and I used to see hector_mapping over 100% all the time.", "You can adjust the parameters (for me, they're in hector_mapping/mapping_default.launch) to ignore as much of the laser scan as possible. Bump up  ", ". Bump down ", " and ", ". Maybe there are a few other parameters that have some effect. You can see a list of params on the wiki page. In hector_nav.launch, decrease the map size.", "A hacky fix: you can install cpulimit and throttle the CPU usage to whatever you please. I'm using 80% without any noticeable ill effects:", "This should only happen when you run things on a very weak CPU, otherwise it is very likely that you're compiling with no optimization.", "Thanks. With the parameter changes, CPU load has dropped from ~100% to 30%. Add to CMakeLists instead of typing ", ":", "add_executable(hector_mapping ...)", "add_dep...", "target_link_lib...", "set_target_properties(hector_mapping PROPERTIES DCMAKE_BUILD_TYPE \"Release\")"], "answer_code": ["catkin_make", "hector_mapping", "catkin_make", "ros-hydro-hector-mapping", "yaourt", "catkin_make", "laser_min_distance", "map_pub_period", "laser_max_distance", "cpulimit -e hector_mapping -l 80\n", "catkin_make -DCMAKE_BUILD_TYPE=Release"], "url": "https://answers.ros.org/question/151466/hector_mapping-reducing-cpu-consumption/"},
{"title": "Turtlebot 3 Questions", "time": "2017-10-22 20:03:05 -0600", "post_content": [" ", " ", " ", " ", "Can order a Turtlebot3 Waffle without an SBC or with an alternate SBC, and whether if I substitute the SBC am I now in an unsupported configuration?"], "answer": [" ", " ", "Turtlebot3 Burger and Waffle are sold as package so customizing your order will be difficult. ", "\nTurtlebot3 is running on the ROS based on Linux, I believe you can use the SBC as long as it supports Linux(Ubuntu 16.04), USB port, Wi-Fi connection. ", "\nPlease check the operation voltage/power consumption of the SBC as OpenCR might not support power recommendation for a custom SBC. ", "\nPlease understand that customizing the original package may void the product warranty."], "url": "https://answers.ros.org/question/273784/turtlebot-3-questions/"},
{"title": "What are the 360 degree cameras compatible with ROS?", "time": "2017-12-25 13:27:33 -0600", "post_content": [" ", " ", "Hi,\nI want to know what are the 360 degree cameras compatible with ROS platform? "], "answer": [" ", " ", "We've been using the Ricoh Theta S successfully, using the ", ". It's a bit tedious to use as it has to be switched into \"webcam\" mode using buttons on the camera everytime it is used however. Also, right now, it appears one is limited to 720P images containing both fisheye images side by side via mjpg. The camera itself can do 1080P/H264, but this still doesn't work with current Linux Kernels AFAIK.", "What I'd really be interested in is a nice 360 deg camera with a USB3.0 interface, but it seems the focus for consumer cams is on WiFi unfortunately.", "For example, I used a normal monochrome camera to do feature learning with deep learning. Can I use it for ricoh theta camera without making any changes? That's why I asked that question. All-round cameras do not give a flat image. They give a global image.", "You'll have to perform some sort of calibration and projection of the fisheye image for things to work. The kalibr calibration toolbox is a very starting point for that.", "thank you very much Mr. Kohlbrecher"], "url": "https://answers.ros.org/question/278172/what-are-the-360-degree-cameras-compatible-with-ros/"},
{"title": "Recommended hardware for ROS development", "time": "2017-12-10 08:06:50 -0600", "post_content": [" ", " ", "I'm buying a computer (laptop or desktop) for ROS development. In particular I want Gazebo to run nicely. It will be just ubuntu 16.04, no need to dual boot. It should have a SSD. ", "Can someone say what configuration they're using and happy with? Thanks! ", "I use an Alienware 17 inch laptop at work and at home an old Lenovo ThinkPad T420 as my main computer. I have no issues with either. Gazebo + RViz can be slow, but that happens even when I'm my work desktop which is basically a server with 64GB of RAM."], "answer": [" ", " ", "My setup:", "For Gazebo, you want a CPU with a high single core frequency, rather than many cores (ie, choose i7 over a server type CPU with 16 cores at 2.2GHz). Using htop to see resource usage, it seems that Gazebo server has two main threads that consume decent CPU resources, and Gazebo client has one main thread. There are then a few other threads that consume small amounts of CPU. The i7 runs great and Gazebo is always above 0.98 realtime, even in large environments (see below).", "For RAM, the only real benefit to adding more is if you are going to simulate huge areas and create large maps using many different sensor streams. When running a 100mx100m Gazebo world, with 30+ tree models, a 3D camera (depth image and point clouds) and 2D laser scanner, 100mx100m Octomap and multi-layer Grid Maps at 0.1m resolution, my RAM usage is about 5GB.", "A graphics card makes very little difference for Gazebo since it isn't exactly graphics-intensive. The model above uses about 200MB of graphics memory and the card is only running at about 35% capacity. You could likely even skip the graphics card altogether and use the onboard graphics just fine.", "My 120GB SSD size is ok for simulation, but if you want to save large bagfiles of sensor (point cloud or image) data, I'd go for something larger, or pair it with a cheap+large spinning disk HDD. We generate bagfiles from our actual machine that produce about 30GB of sensor data per minute! But I'm sure you won't have this requirement.", "We have another office PC that has an i5 CPU and GTX 1060 and it struggles to run Gazebo server and client simultaneously. I'd suggest getting the best CPU you can afford and splitting the rest of the money across the other, less-critical components."], "answer_details": ["i7-7700k", "16GB RAM", "GTX 1050 Ti graphics card", "H170 chipset motherboard", "120 GB SSD", " ", " ", " ", " "], "url": "https://answers.ros.org/question/277708/recommended-hardware-for-ros-development/"},
{"title": "Jerky movements of UR10 robot with ur_modern_driver and MoveIt", "time": "2017-11-28 16:55:31 -0600", "post_content": [" ", " ", "We have a problem with sometimes jerky moves of UR10 (and UR3 before).", "We are controlling our UR10 robot via MoveIt + ur_modern_driver combination. All works generally well, trajectories are planned and moves are smoothly executed. We have however problem that quite often the move has some \"jerky\" slow-downs/pauses. Robot slows down for a brief moment and then catches up - the moves are then not smooth. It does not happen continuously - generally move is smooth, but with longer moves it can happen once/twice during path execution.", "It seems that the  modern driver uses 125Hz interface and generates quite a lot of traffic, which we suspect might be the cause. Seems that jerkyness increses if we run some CPU-intensive/rostopic intensive ROS nodes (like gazebo). We are running the whole ROS environment in a docker container with --network=host and --privileged on fairly fast 8 core machine with plenty of RAM. We had similar problems with UR3 before.", "We have not yet deeply investigated (we are going to) - but before we do, maybe someone had similar problems and know possible reasons ?", "Have you checked the ", "?", "It might be a known issue. The driver closes the loop with the robot controller over the network, so dropped/late pkts can cause what you are describing.", "Thanks! I spent some time reviewing the issues and the pull requests/discussions and have some things to explore (the comments from ", " below reflected my findings pretty accurately. This week we are going to test the refactored branch, TCP socket options mentioned there."], "answer": [" ", " ", " If you run kinetic, checkout  ", " .\nThis work will probably replace the previous branch in kinetic+ and has some adjustments also w.r.t. latency.\nAdditionally make sure you run a low-latency linux kernel on your machine to avoid random delays. ", "The driver sends target poses to the controller with 4*125 Hz, but the controller performs an emergency stop (", ") if it doesn't receive a target in even a single cycle of the 125Hz update loop. With generic kernel and lots of activity on the machine that is very much possible though and we saw similar behavior in our lab too.", "It's gone since we changed the kernel and use the new branch.", "Thanks ", "! I've been reading through the code this weekend (5 hours by train today ;) ) and all the issues/ pull requests and came to same conclusions! The \"do_brake\" comment however was something I have not realised, but it makes sense. I will test all of it and let you know if it helped.", " - the refector by Simon Rasmussen is definitely an improvement. I tested it and on top of being way more readable and structured, it is also improving smoothness of moves. There are still some slow-downs during the move. I also tested the low-latency kernel but I see no improvements there.", "I am going to suggest some improvements and findings in the refactored modern driver github repo. After reviewing and understanding in detail how the driver works, we have some thoughts and proposals on some changes of the architecture that migh improve it, but i need to consult it with authors"], "answer_code": ["do_brake"], "url": "https://answers.ros.org/question/276854/jerky-movements-of-ur10-robot-with-ur_modern_driver-and-moveit/"},
{"title": "MoveIt: how to actuate individual joints that do not form a chain?", "time": "2018-01-09 13:27:41 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm working on a project at my university in which we are designing and simulating an industrial robot system. We have created a URDF file that contains the robot system, as well as the platform it sits on. There are also two movable caterpillar tracks on this platform, which we need to be able to control programmatically. We've created a MoveIt configuration for the robotic arm that the robot system consists of, which works properly.", "To give you a bit of an image (literally) of how the system and the tracks look like:\n", "My problem arises when I want to control the two movable tracks, which each consist of just a single joint. The relevant parts of the robot's URDF are included below; for brevity, I left out the rest. I want to be able to control the individual joints ", " and ", " from Python, but I have not yet been able to find out how...", "I've tried to make a planning group with just ", " in it without selecting a kinematics solver, hoping that I would be able to use ", " in order to get easy control over the track. The MoveIt Setup Assistant allowed me to do so, so I tried it out in some code, here's the relevant snippet:", "This seemed to work fine. The print statements printed the correct values and ", " caused the correct rotation to be shown in RViz. But when I replaced ", " with ", ", I got the following message in the console in which I launched the test:", "The following appeared in the console in which I launched the planning execution launch file:", "Most likely your MoveIt configuration is incomplete. The error message seems to indicate that either MoveIt has no ctrlrs for the joints that it has a plan for, or the it couldn't execute the plan with the ctrlrs it ", " have.", "You'll have to show us your ", " (or similar file).", "Btw: a ", " of 118,270 Kg? That is one large/heavy construction.", "Also: please attach your image directly to your question. I've given you enough karma for that.", "And: it's perfectly possible to set joint value targets for groups that don't have an IK solver, so that is not the issue here.", " Thank you for your comment and for the karma, I've attached the image directly to my question and I've also included the ", " file.", "As you can see, it currently contains only one controller. However, if I give it a name, then the robotic arm no longer works. Adding ", " and ", " to the existing controller leaves me with the same error as before.", "As for the mass, it is indeed one heck of a large construction. The platform that ", " represents is the top part of a tensioner used for laying pipes in the ocean. A horizontal version of the system is shown ", "At the very least I'd add controllers for each of the different groups, that are responsible for ", " the joints in that group.", "Then make sure you have ", " action servers for each of those controllers (or have a single server that accepts goals for all those joints).", "Right now you just don't have a controller that can execute the trajectory that MoveIt came up with."], "answer": [" ", " ", " ", " ", "With the help of ", " I have been able to fix the problem. Here's how..", "First of all, I noticed that the ", " was launched by the ", " in the ", ", so in order to get those topics in the right namespace, I changed this  in that file:", "to this:", "and added the following to aggregate the ", " topics of each controller into the general ", " topic.", "The rest of that file is still the same as the template given in the tutorial. As for the real robot's drivers, they do not exist yet, nor do we even have access to such a robotic arm. The aim of our project is mainly to provide a simulation as a proof of concept of the system. ", "As for ", " file, I've added the controllers of the two tracks and gave them names:", "So now the joint_trajectory_action of each is in the right namespace. But I noticed that the messages published to ", ", ", " and ", " contained the standard joint names (joint_1, joint_2 ..., joint_6) instead of the joints my system has. After some looking around, I found out how to change my ", " accordingly:", "The ", " topics of these three controllers now contain the right joint names and the ", " is now correctly able to aggregate all of these to the general ", " topic. All of the above combined solved my problem. Now I'll just need to do this for the gripper as well... :)", "Now that you have more JTA (and drivers, as the ", " is essentially a driver), you'll need to make sure that consumers of ", " (ie: ", " msgs) still see the complete state of your system.", "In this case that means that you'll need to add something that ..", ".. aggregates ", " msgs from the different drivers, coalesces them and then republishes on ", ".", "See ", " for that.", "Btw: if there is no need for the two tracks to ever do ", ", I would actually recommend that you put both of them in their own groups. That will avoid any potential issues with MoveIt deciding that it needs to move the one when you only want to move the other.", "I was literally just reading about the ", " parameter for ", ", but your example made its usage a lot more clear, thanks! I'll update the answer with the final solution once I have put the two tracks back into their own planning groups.", "Just to be clear: you obviously don't want ", " in there, but the ", " publishers of all your groups. It was really just an example.", "Yeah, I figured that, hahaha :) I've updated my answer and marked it as correct. Thanks again!", "Good to hear that you got it to work.", "Also +1 on reporting back on how you finally implemented all of this.", "And finally: note that your issue was not at all caused by the joints not being \"part of a chain\".", "Obvious now perhaps, but just noting this for future readers."], "question_code": ["track2_joint", "track3_joint", "track2_joint", "group.set_joint_value_target", "robot = moveit_commander.RobotCommander()\njoint = robot.get_joint('track2_joint')\nmoveGroup = moveit_commander.MoveGroupCommander('Track2')\ncurrentJointValues = moveGroup.get_current_joint_values()\nprint(\"Min bound: \" + str(joint.min_bound()))\nprint(\"Value: \" + str(currentJointValues))\nprint(\"Max bound: \" + str(joint.max_bound()))\n\ncurrentJointValues[0] += pi * 1/3\nmoveGroup.set_joint_value_target(currentJointValues)\nmoveGroup.plan()\n", "plan", "plan()", "go()", "[ INFO] [1515523071.981027951]: ABORTED: Solution found but controller failed during execution", "[ INFO] [1515523071.563018404]: Combined planning and execution request received for MoveGroup action. Forwarding to planning and execution pipeline.\n[ INFO] [1515523071.563289980]: Planning attempt 1 of at most 1\nDebug:   Starting goal sampling thread\nDebug:   Waiting for space information to be set up before the sampling thread can begin computation...\n[ INFO] [1515523071.564900737]: Planner configuration 'Track2' will use planner 'geometric::RRTConnect'. Additional configuration parameters will be set when the planner is constructed.\nDebug:   RRTConnect: Planner range detected to be 1.256640\nInfo:    RRTConnect: Starting planning with 1 states already in datastructure\nDebug:   RRTConnect: Waiting for goal region samples ...\nDebug:   Beginning sampling thread computation\nDebug:   Stopped goal sampling thread after 10 sampling attempts\nDebug:   RRTConnect: Waited 0.010084 seconds for the first goal sample.\nInfo:    RRTConnect: Created 5 states (2 start + 3 goal)\nInfo:    Solution found in 0.012115 seconds\nInfo:    SimpleSetup: Path simplification took 0.001880 seconds and changed from 4 to 17 states\n[ERROR] [1515523071.662697635]: Joint trajectory action failing on invalid joints\n[ WARN] [1515523071.663024659]: Controller  failed ...", "controllers.yaml", "mass", "controllers.yaml", "track2_joint", "track3_joint", "base_link", "FollowJointTrajectory"], "answer_code": ["joint_trajectory_action", "industrial_robot_simulator", "pacbot_moveit_planning_execution.launch", "<!-- run the robot simulator and action interface nodes -->\n<group if=\"$(arg sim)\" >\n  <include file=\"$(find industrial_robot_simulator)/launch/robot_interface_simulator.launch\" />\n</group>\n", "<!-- run the robot simulator and action interface nodes -->\n<group if=\"$(arg sim)\" ns=\"arm1\" >\n  <include file=\"$(find industrial_robot_simulator)/launch/robot_interface_simulator.launch\" />\n</group>\n<group if=\"$(arg sim)\" ns=\"track2\" >\n  <include file=\"$(find industrial_robot_simulator)/launch/robot_interface_simulator.launch\" />\n</group>\n<group if=\"$(arg sim)\" ns=\"track3\" >\n  <include file=\"$(find industrial_robot_simulator)/launch/robot_interface_simulator.launch\" />\n</group>\n", "joint_states", "/joint_states", "<node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\">\n  <param name=\"/use_gui\" value=\"false\" />\n  <rosparam param=\"/source_list\">[/arm1/joint_states, /track2/joint_states, /track3/joint_states]</rosparam>\n</node>\n", "controllers.yaml", "controller_list:\n- name: arm1\n  action_ns: joint_trajectory_action\n  type: FollowJointTrajectory\n  joints: [cart1_joint, main1_joint, elbow1_joint, forearm1_joint, wrist1_joint, gripper1_joint]\n- name: track2\n  action_ns: joint_trajectory_action\n  type: FollowJointTrajectory\n  joints: [track2_joint]\n- name: track3\n  action_ns: joint_trajectory_action\n  type: FollowJointTrajectory\n  joints: [track3_joint]\n", "/arm1/joint_states", "/track2/joint_states", "/track3/joint_states", "joint_names.yaml", "arm1:\n  controller_joint_names: [cart1_joint, main1_joint, elbow1_joint, forearm1_joint, wrist1_joint, gripper1_joint]\ntrack2:\n  controller_joint_names: [track2_joint]\ntrack3:\n  controller_joint_names: [track3_joint]\n", "joint_states", "joint_state_publisher", "/joint_states", "industrial_robot_simulator", "/joint_state", "JointState", "JointState", "/joint_state", "source_list", "joint_state_publisher", "/move_group/fake_controller_joint_states", "JointState"], "url": "https://answers.ros.org/question/279286/moveit-how-to-actuate-individual-joints-that-do-not-form-a-chain/"},
{"title": "is it possible to communicate between two ROS systems, exploring in one, and SLAM in the other ?", "time": "2018-01-19 20:07:56 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone, \nI have question about if it possible for one ROS system to do an exploring (Raspberry pi) and transfer data to other ROS system (LINUX) To Use SLAM to draw a map ? \nif yes, can someone tells me the main idea or how to start \nThank you", "*Because i have seen communication between two ROS system, but i was not sure if is it possible to the extension of using SLAM and RVIZ to have the map on the other machine", "What do you mean by", "two ROS systems", "? Two computers both running ROS on the same network with the same Master? Something else?", "What i mean a Raspberry pi with Jessie OS on it, that runs ROS\nbut since the Raspberry pi can not handle RVIZ", "I will run RVIZ on another computer"], "answer": [" ", " ", " The short answer is \"yes.\" Take a look at  ", "  to see how to set that up. ", "In practice you'll need to be careful about managing the data flowing over the network. You should run ", " and the motor controllers and sensor drivers on the RPi so that your robot is responsive and immune from network hiccups. On the desktop you can run SLAM and exploration; these consume lots of CPU but generate small messages like tf transforms. The challenge is getting the sensor data up to your desktop reliably. Laser data is pretty small, but pointclouds can be huge, so you'll probably need to downsample on the RPi. It really depends on your situation."], "answer_code": ["roscore"], "url": "https://answers.ros.org/question/280349/is-it-possible-to-communicate-between-two-ros-systems-exploring-in-one-and-slam-in-the-other/"},
{"title": "plotting real time data", "time": "2017-06-26 09:52:49 -0600", "post_content": [" ", " ", " ", " ", "I want process the data from some ros nodes and make a real time plot, preferably using matplotlib. What is the easiest way to do this? ", "For example if I would like plot the average of two values in the ros network, or add a legend to the plot. I know about rqt_plot, but its usefulness is very limited since it can only plot already defined messages.. and you can't even change the axis limits afaik!", "Any tips? The best thing would be if there was a way to continually update a matplotlib figure and have it displayed as the roscore is running. ", " Take a look at  ", "  - I haven't tried it yet but I'd like to hear if it addresses some of your needs. ", "There is an infinity of answers on stackoverflow treating the continuous update of a matplotlib figure."], "answer": [" ", " ", " ", " ", "I ended up doing the following. It works in real time as long as pyplot can keep up with the messages. ", " ", " ", "I did liveplotting in the past in two ways:", "Your prefered way is the matplotlib so thats pretty easy and I explain a little bit more: ", "There you need a ", ". It makes an animation by repeatedly calling a function/method. You can read more ", ". ", "Furter in the repeatedly called function you should do something like:", "so there you append new data and than set it. What you now need to do different is that you setup a ROS Subscriber to your data you want to publish and in the callback function/method you need to append the new data and just set in the \nrepeatedly called function the new data. Thats only a close look about how you can do it to give you an idea.", "The other way is using c++ and writing for example a rviz plugin with QT. I prefere here the ", " for live plotting. But thats a little bit tricky and needs more time when you have no idea about QT. But for me it generates better looking plots and when you are used to it its very easy too. ", "When you need further help please ask! It isnt too detailed. Just to give you an idea.", "this works! but I've always found FuncAnimation to be cumbersome to work with. If anyone has other ideas I'm still interested", " ", " ", "There are many tools to plot.", " tool is quite nice to use (compared to rqt_plot)", " this is my favourite tool its so fast plotting data and so easy to use! Try it yourself! ", "sudo apt-get install ros-kinetic-plotjuggler", "Start in launch file:", "If anyone knows how to start from launch file a saved layout.xml  Would be great for sharing :) ", "Hi, thanks for recommending PlotJuggler (I am the author). I super appreciate it.", "PlotJuggler uses old fashion command line arguments instead os ros params. Display them with -h. You can do:", "<node pkg=\"plotjuggler\" type=\"PlotJuggler\" name=\"my_plot_Juggler\" args=\"--layout your_filename\"/>", "Yeah I saw that --layout our just -l option but it does not work for me If I start gazebo paused.I get cannot find curve with ... error. And if I just run the sim. for a small amount of time and then try to load my layout I get the streamer named ROS Topic Streamer cannot be loaded.", " If you can describe in more detail the problem and/or the desired behavior here,  ", " \nI will be happy to improve it. ", " ", " ", "I would recommend rqt_plot. Don't forget you can plot the ", ". Almost anything can be represented by them. It sounds like what you want to plot can be a Float32 message. ", "Also, click on the green checkmark on the rqt_plot gui above the plot area to access the axis max/min settings. ", "It's a bit time consuming to have to readjust axis settings manually for different experiments. I am really looking for a way to customise plots programmatically. Also, afaik there is no way to change the x-axis in an rqt plot, like if I want to plot x/y coordinates or motor input vs speed."], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "rosrun plotjuggler PlotJuggler ", "Go to streaming and start ros_topic_streaming", "simply select your topics you wanna stream", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["#!/usr/bin/env python \nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport rospy\nfrom qualisys.msg import Subject\n\ndef plot_x(msg):\n    global counter\n    if counter % 10 == 0:\n        stamp = msg.header.stamp\n        time = stamp.secs + stamp.nsecs * 1e-9\n        plt.plot(msg.position.y, msg.position.x, '*')\n        plt.axis(\"equal\")\n        plt.draw()\n        plt.pause(0.00000000001)\n\n    counter += 1\n\nif __name__ == '__main__':\n    counter = 0\n\n    rospy.init_node(\"plotter\")\n    rospy.Subscriber(\"position_measurements\", Subject, plot_x)\n    plt.ion()\n    plt.show()\n    rospy.spin()\n", "animation.FuncAnimation", "xdata.append(frame)\nydata.append(np.sin(frame))\nln.set_data(xdata, ydata)\n", "<!-- start plotjuggler -->\n<node pkg=\"plotjuggler\" type=\"PlotJuggler\" name=\"my_plot_Juggler\" args=\"\" />\n"], "url": "https://answers.ros.org/question/264767/plotting-real-time-data/"},
{"title": "Specifying TCP velocity using ur_modern_driver", "time": "2018-03-12 02:59:51 -0600", "post_content": [" ", " ", "The trajectory planner of the ", " states: \"This method does cubic interpolation between two points, given the absolute time between the, the time\nat which the position should be found, and the velocity at the two points. Note that this function does not\nadhere to any constraints and, as it is a cubic interpolator, is not jerk continuous\"", "How can we limit the TCP velocity to 250 mm/s as specified by safety standards for collaborative robots?"], "answer": [" ", " ", " ", " ", "How can we limit the TCP velocity to 250 mm/s as specified by safety standards for collaborative robots?", "You'll have to do that outside of the driver, as the driver does not have any concept of Cartesian velocity, nor does it concern itself with the safety aspects that you refer to.", "The ", " only exposes a ", " action server. As the name implies, that server accepts goals containing a ", ". There is no support for Cartesian trajectories at this point (there is no support at the ROS API level, they could be sent to the controller using the ", " topic).", "Edit: I'm also slightly confused by your question. Every UR robot comes with a built-in safety system that monitors robot safety and will stop the robot if it violates any of the built-in restrictions. Layering safety is always a good idea, but at the very least, the built-in safety of the robot should prevent it from violating the relevant standards.", " does not subvert, circumvent or disable any of these checks in the controller, so you can still rely on the robot controller to take care of safety (on the robot level only, of course. System-level safety is not the controller's responsibility).", "Okay, I understand. However, if I look at the rostopic ", ", I'm assuming the linear velocity values to be in (m/s) units and sometimes they go beyond 0.25 m/s limit. Is there any data on what is the limiting tool velocity imposed by UR controller?", "Note also that limiting the TCP velocity to 250mm/s is not always required, if I interpret the relevant standards correctly.", "That data is coming directly from the controller, the driver does not do any processing or control anything there.", "Is there any data on what is the limiting tool velocity imposed by UR controller?", "I would recommend to consult the relevant documentation that UR provides about your controller. ..", ".. All safety aspects are documented, as well as the safety system and how to configure it. Assuming you have a CB3 controller, there are extensive documentation and configuration options available.", "I checked the CB3 documentation and it's possible to set hard on the TCP max speed (default 1500 mm/s) to 250 mm/s. However is this recommended? Also you say,  limiting the TCP velocity to 250mm/s is not always required. This is ambiguous?", "re: is this recommended: I wouldn't know.", "re: not always required: iirc, there are various 'zones' around a collaborative robot and the position of the co-worker(s) relative to those zones is what determines what the robot can and can't do. I'd recommend to review the literature about this."], "question_code": ["ur_modern_driver"], "answer_code": ["ur_modern_driver", "/urscript", "ur_modern_driver", "/tool_velocity"], "url": "https://answers.ros.org/question/285090/specifying-tcp-velocity-using-ur_modern_driver/"},
{"title": "TF issue with robot_localization", "time": "2017-12-11 04:35:33 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI'm fairly new to ROS. I'm trying to fuse wheel-odometry, GPS and IMU using ", " node.\nI'm attaching a rosbag which contains data published by the sensors. The car is essentially going in a circle. I've done sensor data publication exactly as specified by the robot_localization wiki.  Yet the output of the robot_localization is incorrect and weird, it gives wrong Twist output data.\nI've already spent a week trying to solve this problem any help will be appreciated\nPS: data from all the sensors are pretty accurate, I've not added any noise to the sensors.\nThe bagfile : ", "\nTF tree :", "Wheel-Odometry : \nchild_frame_id = 'fusion/base_link'\nheader.frame_id = 'fusion/base_link'", "IMU:\nheader.frame_id = 'base_footprint'", "GPS:\nheader.frame_id = 'fusion/base_link'"], "answer": [" ", " ", "It worked after I removed all the forward slashes from the frame_ids", "Great! Glad you found a fix.", " ", " ", " ", " ", "Please post full sample messages for every sensor input.", "Just looking at your config and the ", "s you posted, I can make the following recommendations:", "But I can't say much more without sample messages. Downloading and reviewing bags is time-consuming, so it would help me a great deal if you could post some sample messages.", "I apologise for posting a reply so late, I solved it after a few days of me posting question.\nHere's what I did.\n1. I did change the frame_id of wheel odometry at later stage but since I was only fusing Longitudinal Velocity it worked then.2. ", " from all the frame_id"], "answer_details": [" ", " ", " ", " ", "Your odometry ", " should ", " be in the ", " frame. The pose data should be in a world-fixed frame (e.g., ", "). The ", " is fine.", "Get rid of the rejection thresholds. Those are advanced parameters that should be avoided until the system is behaving the way you want.", " ", " ", " ", " "], "question_code": ["ekf_se_odom:\n  frequency: 30\n  sensor_timeout: 2\n  two_d_mode: true\n  transform_time_offset: 0.0\n  transform_timeout: 0.0\n  print_diagnostics: true\n  debug: false\n\n  map_frame: map\n  odom_frame: odom\n  base_link_frame: fusion/base_footprint\n  world_frame: odom\n\n  odom0: /fusion/odom\n  odom0_config: [false, false, false,\n                 true,  true,  true,\n                 true,  false, false,\n                 false, false, false,\n                 false, false, false]\n  odom0_queue_size: 10\n  odom0_nodelay: true\n  odom0_differential: false\n  odom0_relative: false\n\n  odom1: odometry/gps\n  odom1_config: [true,  true,  false,\n                 false, false, false,\n                false, false, false,\n                 false, false, false,\n                 false, false, false]\n  odom1_queue_size: 10\n  odom1_nodelay: true\n  odom1_differential: false\n  odom1_relative: false\n\n  imu0: fusion/imu/data_raw\n  imu0_config: [false, false, false,\n                false, false, false,\n                false, false, false,\n                true , true,  true,\n                true,  true,  true]\n  imu0_nodelay: true\n  imu0_differential: false\n  imu0_relative: true\n  imu0_queue_size: 5\n  imu0_pose_rejection_threshold: 0.8                 # Note the difference in parameter names\n  imu0_twist_rejection_threshold: 0.8                #\n  imu0_linear_acceleration_rejection_threshold: 0.8  #\n  imu0_remove_gravitational_acceleration: true\n\n  use_control: false\n  stamped_control: false\n  control_timeout: 0.2\n  control_config: [true, false, false, false, false, true]\n  acceleration_limits: [2, 0.0, 0.0, 0.0, 0.0, 3.4]\n  deceleration_limits: [2, 0.0, 0.0, 0.0, 0.0, 4.5]\n  acceleration_gains: [0.8, 0.0, 0.0, 0.0, 0.0, 0.9]\n  deceleration_gains: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n\n  process_noise_covariance: [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0.06, 0,     0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0.025, 0,     0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,     0.025, 0,    0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0 ..."], "answer_code": ["frame_id", "frame_id", "base_link", "child_frame_id"], "url": "https://answers.ros.org/question/277770/tf-issue-with-robot_localization/"},
{"title": "Different goals in one action server", "time": "2018-02-05 22:11:21 -0600", "post_content": [" ", " ", "I have a bit of an issue trying to figure out the best way to implement an action server with different goals. Is it an acceptable practice to use enums if goals are very different from each other or do I need to have multiple action servers? Here is a bit of a background:", "I have multiple nodes that need to control arms. I have one node that controls robot arms and accepts float values to specify goals, each arm can only go up and down, left and right. For my application, I only have a few routines that arms need to follow, for example picking an object up. Those routines always send the same values to the arm controlling node.\nTo avoid repeating sending the same set of instructions from every node when I need to pick an object up, I want to create another node that will have those routines implemented, so the other nodes only need to specify which routine they need and let the new node take care of everything else. ", "From what I understand (I don't have any experience with action servers, so please correct me if I am wrong), I should create an Action Server to move the arms, and the rest of the nodes will be sending goals to it. My concern is, pickup routine will be very different from drop off. I know I can use enums for different routines, and then have a switch statement in the Execute Callback, which should work fine. Are there any issues with using that approach? "], "answer": [" ", " ", "Are there any issues with using that approach?", "It's a design decision, and what you describe is a valid design.", "One reason I can think of not to do it is that you increase coupling between the 'one action server' and the clients. Only clients that know about the special ", " will be able to communicate with your action server. This reduces reusability of both client and server, as you've now (unnecessarily) leaked implementation details of the way your intermediate node works to your consumers.", "It also influences composability of your nodes/application: adding a new 'behaviour' will require you to extend the ", ", which could potentially mean updating the sources of all your clients (to deal with special circumstances fi). If you create an action server per 'behaviour', you completely decouple clients from servers and adding a new behaviour comes down to adding a new server (which should hav no impact on your clients).", "I have one node that controls robot arms and accepts float values to specify goals, each arm can only go up and down, left and right", "are the 'floats' joint space angles/poses (potentially after some conversion / at a different abstraction level)? If so, it might make sense to reuse existing infrastructure, such as ", ", ", " and the ", " action (and associated server implementations). If it's Cartesian (ie: EEF/TCP) coordinates, similar infrastructure could be created, analogous to that for joint poses (there may already be community maintained components for that). This could greatly reduce the work that needs to be done.", "To avoid repeating sending the same set of instructions from every node when I need to pick an object up, I want to create another node that will have those routines implemented, so the other nodes only need to specify which routine they need and let the new node take care of everything else. ", "This sounds like either something that could be called a 'process planner' (ie: an entity that abstracts and encapsulates the implementation details (ie: motion, activation sequence, dealing with errors) of a particular behaviour) or a 'motion primitive' (ie: 'pick up', 'place', 'move obj x from y to z'). The process planner only requires higher level goals or tasks, decomposes them into smaller subtasks and then either executes them or coordinates other components while they execute them.", "In your current design, you describe only a single piece of information that you will provide in the task description: the value of the enum. If you anticipate that there will be (or there already is) more information that needs to be provided to the process planner, that would again be an indication that separate actions (and thus servers) could be a good idea. They would additionally increase the semantic value of the actions used.", "If 'different goals' only means: different joint/Cartesian targets to move your robot to, then having multiple servers doesn't make much sense (semantically, it's all \"move robot to x, y, z\"). If it's really different ", " that you're abstracting, ", " multiple servers would work."], "answer_code": ["enum", "enum"], "url": "https://answers.ros.org/question/281859/different-goals-in-one-action-server/"},
{"title": "API for changing a camera resolution", "time": "2018-03-27 05:31:10 -0600", "post_content": [" ", " ", " ", " ", "What is the standard ROS way for implementing on-the-fly resolution changing for a camera node?", "I haven't found any official articles on this, also I haven't found any popular camera nodes, supporting this. This feature is critical for my application, I want to implement it for e. g. cv_camera, but what should be the API?", "Should it be dynamic_reconfigure parameters? Actually, I have never seen such parameters in a real-live package.", "Or should it be some ROS service? set_camera_info, maybe?", "Or maybe just standard parameters should be used, so the node should poll them periodically?"], "answer": [" ", " ", "I don't think there is much of a standard around this, because most robots don't have run-time reconfigurable cameras (the resolution is often encoded in the launch file). dynamic_reconfigure is probably the most popular approach although most of the time it is used for parameters like white balance. One example is the ", " drivers.", "I'd generally avoid \"standard parameters\" for this, since you probably want the feedback that things have changed. I'd suggest there are 4 possible ways which would all be \"ROS-like\":", "I think one of the reasons there is no standard API is next to the things ", " already mentioned, consumers of camera driver output mght not be capable of dealing with dynamically changing resolutions of images they receive. They should, but they might not. And it's not just images, afaik ..", ".. camera calibration is resolution dependent as well, so that would need to be updated too. Consumers of that need to be made aware of changes. I'm not sure the current ", " and ", " nodes/libs are setup for that.", "When you use subscribeCamera, you get updated image and cameraInfo message in your callback at the same time. So this is OK.", "True, but that doesn't necessarily mean that every consumer always uses the latest version of those msgs. If the assumption (until now) has been that ", " doesn't change, why not cache the first one and keep using that?", "I believe, that all of the standard CV nodes use the latest cameraInfo message. You ", " do that because of two reasons: camera resolution may change; camera lens settings may change (optical zoom, etc).", "So, if there are nodes, that do cache camera info, they are incorrect.", "I'm not saying that your approach is wrong, or that you're not correct. I'm saying that -- given my experience with humans writing computer programs -- that there may be quite some nodes that have (implicit) assumptions about how all of this works. Those may be wrong, true. I'm just trying to ..", ".. make clear that adding support for dynamically changing resolutions in a camera driver may not be all that is needed for this to work properly. You need to take the entire pipeline into account.", "This is a similar discussion as with \"dynamically updating ", "\".", "Yes, this is clear. The code, I need to work with images does it correctly, with the last camera info message.\nThe open source nodes I examined, worked the same."], "answer_details": ["ROS Service -- probably the simplest and most straight forward, you definitely get a response that things have completed. In general, services are a good idea if the service will return almost immediately. If it is a longer running operation, you might want to look at:", "ROS Actions -- this is like a service, but implements a full state machine over several topics. It can be interrupted (unlike a service). The code complexity will be a bit higher than a service.", "Dynamic reconfigure -- if you want to be able to use GUI tools like rqt_reconfigure, this might be the best option. The downside I could see with this approach is that you could get \"partial changes\" after you change each parameter with a tool like rqt_reconfigure. You might have to encode a lot of logic to handle that, to avoid sending bad updates down.", "image_pipeline - if you have several \"standard\" configurations, this might be a possible approach. This is commonly found on a lot of robots which have a fixed camera resolution but want to offer downsampled versions for certain operations (e.g. inserting into a costmap). The image_pipeline package has a number of nodes/nodelets that can sample/convert your images. The upside is that there is no \"switching\" cost, the downside is that more processing might take place (although lazy subscribers can help here, as I believe all of the image_pipeline nodes/nodelets will stop doing work if they have no subscribers).", " ", " ", " ", " "], "answer_code": ["image_transport", "image_pipeline", "CameraInfo", "robot_description"], "url": "https://answers.ros.org/question/286667/api-for-changing-a-camera-resolution/"},
{"title": "Catkin_make error with geometry2 package", "time": "2018-04-09 23:26:53 -0600", "post_content": [" ", " ", " ", " ", "I'm attempting to use catkin_make to build the 'imu_tools' package ", ". But when runnings catkin_make I get the error down below.", "My main issue is that I dont really understand what the error message is trying to tell me. I've found the package ", " that I should install somewhere, but I'm struggling to figure out how to do it properly.", "Sorry if this is a very basic question, I'm still very new to ROS, and programming in general. Feel free to ask for more information.", "Thanks for your time"], "answer": [" ", " ", "You are right in thinking that you are missing packages. You could manually install them, but this could be time consuming if you are missing a bunch of them. You can use ", " to automatically install ros \n dependencies for a given package. Once you've installed rosdep (instructions in link above), you can move into the top level of your catkin workspace and run the command ", ", which will automatically install all of the deps that you need for imu_tools. ", "That worked! ", "I was trying to use rosdep to install the dependencies earlier except I was using ", " which said it installed dependencies, but really didn't. Could you explain that command that worked?", "'s answers is the correct one, but I just wanted to point to ", " as an example workflow that shows how to build packages from sources.", "If you really must build things from sources (instead of using ", "), that is.", "That's a really informative answer, would be great if it was a part of the tutorial or something"], "question_code": ["CMake Warning at /opt/ros/kinetic/share/catkin/cmake/catkinConfig.cmake:76 (find_package):\n  Could not find a package configuration file provided by \"tf2_geometry_msgs\"\n  with any of the following names:\n\n    tf2_geometry_msgsConfig.cmake\n    tf2_geometry_msgs-config.cmake\n\n  Add the installation prefix of \"tf2_geometry_msgs\" to CMAKE_PREFIX_PATH or\n  set \"tf2_geometry_msgs_DIR\" to a directory containing one of the above\n  files.  If \"tf2_geometry_msgs\" provides a separate development package or\n  SDK, be sure it has been installed.\nCall Stack (most recent call first):\n  imu_tools/imu_filter_madgwick/CMakeLists.txt:4 (find_package)\n\n-- Could not find the required component 'tf2_geometry_msgs'. The following CMake error indicates that you either need to install the package with the same name or change your environment so that it can be found.\n"], "answer_code": ["rosdep install --from-paths src --ignore-src -r -y", "rosdep install imu_tools", "apt-get"], "url": "https://answers.ros.org/question/288091/catkin_make-error-with-geometry2-package/"},
{"title": "rosidl type support and new language", "time": "2018-05-24 09:22:51 -0600", "post_content": [" ", " ", "Context: trying to implement support for Ada.", " I'm trying to understand how should I bridge the gap from .msg files to Ada data types. I've read this:\n ", "  (unfortunately the rosidl API is not available it seems) and checked relevant rosidl, typesupport packages, but the separation between source templates, generated files and installed headers is causing me some headaches. ", "Binding the C rcl is easy from Ada, and I already have that. I am able to retrieve an untyped message from a subscription, using the C rcl API and using the ", " as an opaque type. My main concern right now is if I'm barking the wrong tree, in regard to how data is serialized/deserialized. In the docs I read:", "type support means: meta data or functions that are specific to a given type and that are used by the system to perform particular tasks for the given type. The type support for a given message might include things like a list of the names and types for each field in the message. It might also contain a reference to code that can perform particular tasks for that type, e.g. publish a message.", "However, I'm failing at finding these functions, and am unsure if they're intended for client lib writers or for the RMW implementations. The ", " and ", " seem obvious candidates but if I'm not mistaken they're more or less boilerplater-y for creation/destruction, and calling the appropriate typesupport macro.", "So, in essence, I'm currently hitting a wall at the point of the macros that return a type support struct. I see that each ", " contains a void pointer, and a handler function ", " whose purpose I cannot ascertain.", "Besides any general advice on this, one concrete question I have is: am I already at the point at which I have to interpret the raw bytes using the ROS-to-DDS equivalences of message fields, or should I use some C functions that I'm failing to understand to make things simpler? The former seems doable but quite some work that I'm not sure the other client libraries are doing, so I suspect I'm missing something.", "Anyway, thanks for the patience and thanks for any help.", "After spinning a bit more on all this, I think I might use the ", " files in my own Ada generated files, and indeed the ", " for allocations of dynamic sizes... But I'm unsure if this is the expected path. The C++ struct headers don't seem to rely on the C ones, for example.", "But the python ones do seem to follow this path", "And after digging some more in the rclcpp I see that there are indeed the calls I was missing to rcl/rmw, so I guess the answer to my question is the second path of leveraging all the rosidl C generated header files. Would be nice if someone in the know could confirm though.", " I'd recommend looking at the Python implementation. It's in a similar situation to you where it is using the C data structures but still generates Python objects for each msg type:  ", "C++ indeed does not use the C data structures. Instead it generates its own structures and therefore its own typesupport for those structures. So those are the two ways you can approach it, Python reuses the C structs but wraps them in a new structure, but C++ defines both types and typesupport.", "Thanks, ", ". I have now seen the wrappers in Python and I'll try to do the same. I do not understand though how C++ can generate its own typesupport. Is this not bypassing rcl/rmw somehow? Are there any advantages in doing it that way? Ada is closer to C++ than Python and I'd like to understand"], "answer": [" ", " ", "You can either use existing data structures and associated type support or you can create new data structures and associated type support objects. Python, for example, just reuses the C data structures, so it doesn't need its own type support as well. However, the Python implementation could have, instead, generate its own version of data structures (maybe as ", "'s), but then every rmw implementation that doesn't use the introspection type support would need to generate code to handle these ", "'s.", "At the core, three is a function that most type support objects have, which is to convert the user's data structure into something the middleware handle. So as an example, consider ", ". It generates a C data structure called ", " but for Connext (as an example, because it has it's own type support) needed to publish this, we'd first need to convert it to a compatible type Connext could consume. That might be either a CDR buffer (", ") or a DDS type generated from IDL like ", " (something like that). So a pair of functions to convert to and from these types would be stored in the opaque part of the type support. If you wanted to create yet different data structures you'd need matching type support for all supported rmw implementations.", "The type supports are paired between the data structure type (C or C++ is all we have right now) and the implementation that needs to operate on those types. So for example, you can see here how Connext supports both C and C++ type supports:", "In the future, once some technical details have been sorted out, it might be possible to simplify this interface so that we don't need rmw implementation specific type supports. This might be possible by instead making the interface only send/receive CDR serialization buffers which are untyped (", "'s). But for now, we still need to convert directly to/from the user's in memory representation (whether that be a C, C++, or Ada data structure) to the equivalent DDS data structure for a given vendor.", "There's also the \"introspection\" type support, which can be used by any rmw implementation that chooses to do so, and it works by operating on meta data about the message rather than having explicit conversion functions. It can be pairs with \n DDS's X-Types or used directly. This is what FastRTPS does right now. You can see it here for both C and C++:", "Very thorough explanation. The missing piece that was throwing me off is the common intermediate representation that you say don't yet exists. I don't like the idea of doing something that is not totally abstract for the transports like a new typesupport, so I'll rely on the C typesupport. Thanks!", "Another though: could one use the introspection typesupport from the client side (instead of the rmw implementation side, as you explained) to retrieve and set message contents? It seems so from the headers I see there.", "Answering to myself: yes, it is doable.", " ", " ", "For completeness I'll address my experience with the introspection typesupport that's suggested in the accepted answer.", "Indeed, I was able to create/edit messages using the functions generated by ", " and ", ". Furthermore, by using dlopen/dlsym to retrieve the symbols corresponding to messages by their name, I entirely avoid generating files from templates. At the cost of some runtime penalty, of course."], "question_code": ["rosidl_message_type_support_t", "*__functions.h", "*__type_support.h", "rosidl_message_type_support_t", "rosidl_message_typesupport_handle_function", "<type>__struct.h", "__functions.h"], "answer_code": ["PyObject", "PyObject", "std_msgs/String", "std_msgs__msg__string", "uint8_t[]", "dds::std_msgs::msg::string", "uint8_t[]", "rosidl_generator_c", "rosidl_typesuppport_introspection_c"], "url": "https://answers.ros.org/question/292170/rosidl-type-support-and-new-language/"},
{"title": "memory and cpu utilization of all running nodes", "time": "2017-02-14 16:25:34 -0600", "post_content": [" ", " ", "Hi All,\nI would like to know if ros provides to capture the memory and cpu utilization of all the running node. Usually \nrosnode all command will give the list of the all running node but not sure how to list the memory and cpu utilization of all the running nodes.does this facility provide  by the ros framework ?  "], "answer": [" ", " ", "In indigo, there is the ", " package which provides this functionality.", "it doesn't work on kinetic due to compatibility issue . Could u suggest for kinetic any package.", " ", " ", "There is a GUI-based tool ", " which shows CPU and memory consumption of running nodes:\n", "There is an official release, so you can install it via ", "."], "answer_code": ["sudo apt install ros-${ROS_DISTRO}-rqt-top"], "url": "https://answers.ros.org/question/254689/memory-and-cpu-utilization-of-all-running-nodes/"},
{"title": "Linking problem with ${catkin_LIBRARIES}", "time": "2018-05-22 12:57:43 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone!", "I'm currently trying to become familiar with ROS (I'm using Kinetic on Ubuntu 16.04 LTS) and followed the beginner tutorials. However, when I try to build the simple publisher/subscriber with ", " ( ", " , Section 3.), I get the following error: ", "etc...", "So from my understanding (I did quite a lot of research in several discussion forums), this error actually appears, because the ROS libraries are not linked correctly to the executable. In my CMakeLists.txt I made sure that I added ", ", the CMakeLists.txt looks as follows:", "For debug purposes I added the ", " command to double-check the value of ${catkin_LIBRARIES}, it is", " Since I'm pretty new to ROS and Ubuntu in general, I don't really know how to proceed. Seems like someone had a similar problem before, but never really solved it:  ", "So if you have any ideas on that, let me know.", "Thanks in advance!", "EDIT 01: Added Code of listener.cpp and talker.cpp", "listener.cpp", "please share the talker.cpp code. i guess, something wrong with cpp file", " To be honest, I just copied the code from this tutorial:  ", "  (see EDIT 01)\nMoreover, I get the same error when I try to build other nodes, for example  "], "answer": [" ", " ", "There seems to be something wrong with how you've set up your computer or how you've installed ROS.", "In particular, it looks like all of the unresolved symbols involve std::string, which changed between gcc 4.9 and gcc 5.", " The default compiler on 16.04 looks like it's gcc 5.3, and the ROS packages will be compiled with that version of gcc, but your ticket on the velodyne repo indicates that you've building with gcc 4.9. (This demonstrates the reverse problem:  ", "  ) ", "I am not sure that gcc 4.9 can build code that is compatible with the standard library that ships with gcc 5 and newer.", "I ", " recommend that you use the default compiler that comes with your system.", " Please, how do you use the default compiler?", "The default compiler is the default; you shouldn't need to do anything special to use it, but if you've done anything to change the default compiler, you should undo that. There are so many different ways to override the default compiler, that I can't describe how to check for or undo all of them.", " If you know that you have changed your default compiler and do not know how to restore it, perhaps consult a local linux expert,  ", "  , or as a last resort reinstall Linux. ", "Thank you.", "Note: this was fixed in ", " and/or ", ".", " ", " ", " ", " ", "I guess this is the issue with the compiler. Just reinstall the ", " and ", ". check the version of ", " 'g++ -v`.", "I am using ", " in ", " and version of gcc and g++ are same ", ". ", "Try to reinstall using ", " ", " ", "Your CMakeLists.txt looks fine. Can you make sure you source ", " before you run ", "? ", "If that does not solve the problem, try running ", " or removing ", " directory in your workspace. ", "Thanks for the answer, but unfortunately it didn't help. I'm already sourcing ", " and ", " in .bashrc. Also ", " doesn't make any difference.\nI also tried deleting and reinstalling the entire catkin workspace..", " ", " ", " Thanks a lot for your help guys! With the default gcc compiler (in my case 5.4.0) I was able to build the packages. There was actually no need to reinstall the compiler, I created an update alternative when I changed the compiler version for another project (as described here:  ", " ). ", "Sometimes solutions can be so simple.. Thanks again!"], "question_code": ["catkin_make", "[ 64%] Built target beginner_tutorials_generate_messages_py\n[ 70%] Linking CXX executable /home/fabian/catkin_ws/devel/lib/beginner_tutorials/talker\n[ 82%] Built target beginner_tutorials_generate_messages_nodejs\n[ 88%] Linking CXX executable /home/fabian/catkin_ws/devel/lib/beginner_tutorials/listener\n[ 88%] Built target beginner_tutorials_generate_messages\nCMakeFiles/talker.dir/src/talker.cpp.o: In function `main':\ntalker.cpp:(.text+0x70): undefined reference to `ros::init(int&, char**, std::string const&, unsigned int)'\ntalker.cpp:(.text+0xcc): undefined reference to `ros::NodeHandle::NodeHandle(std::string const&, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&)'\ntalker.cpp:(.text+0x29a): undefined reference to `ros::console::initializeLogLocation(ros::console::LogLocation*, std::string const&, ros::console::levels::Level)'\n", "target_link_libraries(listener ${catkin_LIBRARIES})", "cmake_minimum_required(VERSION 2.8.3)\nproject(beginner_tutorials)\n\n## Find catkin and any catkin packages\nfind_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs genmsg)\n\n## Declare ROS messages and services\nadd_message_files(FILES Num.msg)\nadd_service_files(FILES AddTwoInts.srv)\n\n## Generate added messages and services\ngenerate_messages(DEPENDENCIES std_msgs)\n\n## Declare a catkin package\ncatkin_package()\n\n## Build talker and listener\ninclude_directories(include ${catkin_INCLUDE_DIRS})\n\nadd_executable(talker src/talker.cpp)\n## Debug Message\nmessage(STATUS \"catkin_LIBRARIES: \" ${catkin_LIBRARIES})\ntarget_link_libraries(talker ${catkin_LIBRARIES})\nadd_dependencies(talker beginner_tutorials_generate_messages_cpp)\n\nadd_executable(listener src/listener.cpp)\n## Debug Message\nmessage(STATUS \"catkin_LIBRARIES: \" ${catkin_LIBRARIES})\ntarget_link_libraries(listener ${catkin_LIBRARIES})\nadd_dependencies(listener beginner_tutorials_generate_messages_cpp)\n", "message", "/opt/ros/kinetic/lib/libroscpp.so/usr/lib/x86_64-linux-gnu/libboost_filesystem.so/usr/lib/x86_64-linux-gnu/libboost_signals.so/opt/ros/kinetic/lib/librosconsole.so/opt/ros/kinetic/lib/librosconsole_log4cxx.so/opt/ros/kinetic/lib/librosconsole_backend_interface.so/usr/lib/x86_64-linux-gnu/liblog4cxx.so/usr/lib/x86_64-linux-gnu/libboost_regex.so/opt/ros/kinetic/lib/libxmlrpcpp.so/opt/ros/kinetic/lib/libroscpp_serialization.so/opt/ros/kinetic/lib/librostime.so/opt/ros/kinetic/lib/libcpp_common.so/usr/lib/x86_64-linux-gnu/libboost_system.so/usr/lib/x86_64-linux-gnu/libboost_thread.so/usr/lib/x86_64-linux-gnu/libboost_chrono.so/usr/lib/x86_64-linux-gnu/libboost_date_time.so/usr/lib/x86_64-linux-gnu/libboost_atomic.so/usr/lib/x86_64-linux-gnu/libpthread.so/usr/lib/x86_64-linux-gnu/libconsole_bridge.so\n", "#include \"ros/ros.h\"\n#include \"std_msgs/String.h\"\n\n/**\n * This tutorial demonstrates simple receipt of messages ..."], "answer_code": ["gcc", "g++", "gcc -v", "ros/indigo", "ubuntu 14.04", "4.8.4", "sudo aptitude reinstall gcc g++", "/opt/ros/kinetic/setup.bash", "catkin_make", "catkin_make clean", "build", "/opt/ros/kinetic/setup.bash", "/home/fabian/catkin_ws/devel/setup.bash", "catkin_make clean"], "url": "https://answers.ros.org/question/291910/linking-problem-with-catkin_libraries/"},
{"title": "how does Moveit communicate to Gazebo?", "time": "2018-06-07 05:40:19 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I was able to simulate the UR5 and control it in simulation from RViz, fololwing these steps:", "Now I would like to know how to control the real UR5, but first I need to know how RViz is talking to Gazebo in order to make the robot moves. Does anyone know what kind of input is needed by gazebo? Or better, what kind of information is being sent to Gazebo when I press \"plan and execute\" in RViz? I suspect they are the joints states, but I don't know where to read this information.", "Thank you for your help in advance.", "Edit: Thanks a lot for your answer, now everything is a bit more clear. But I have another question at this point.\nI tried to communicate with the real hardware and I was able to move the robot running \"test_move.py\" from the \"ur_modern_driver package\".\nAfter that I tried to move the robot with MoveIt! following this tutorial: ", "Problems arise when launching this command:", "In the terminal the following lines appear:", "So that when i launch MoveIt (by typing ", ") to try planning and execution, the planning is ok but the execution never works.\nAnd the same is with using the limited version (", ").", "Do you have any idea why this happens?", "Please do not post answers, unless you are answering your own question. For short interactions, use comments. For updating us with new information, or anything else, edit your original question. Use the ", " button/link for that.", "I've already merged your answer into your question.", "Also: your update sounds like a different issue, almost unrelated to your initial question (which was about how Gazebo interacts with a ROS application).", "Your update is about real hw and a driver that is not used with Gazebo, so please ask this in a new question (if you can't find earlier Q&As)."], "answer": [" ", " ", "I'm going to answer this in a bit of a weird way, but hopefully that gets the point across really quickly.", "Does anyone know what kind of input is needed by gazebo? ", "None.", "Or better, what kind of information is being sent to Gazebo when I press \"plan and execute\" in RViz? ", "Nothing.", "I suspect they are the joints states, [..]", "No, they're not.", "The best way to think about ROS<->Gazebo integration is to consider Gazebo (or more specifically: a simulation of a specific robot / system) as a stand-in for your real robot / hw.", "Where real hw needs drivers to get info out of your sensors into your ROS node graph, and actuation commands out of your node graph and 'into' your actuators, a Gazebo simulation (considered as surrogate hw) will need similar components.", "We don't call those ", " though, but ", ". They serve a similar function: they 'extract' data from Gazebo, transform that to msgs and publish those as sensor data. Similarly, these plugins accept actuation commands and pass them on to virtual actuators.", "So pendantically I believe my answers above are correct: Gazebo ", " does not need any input data, and there is nothing sent to Gazebo itself either. The plugins are the entities that deal with those dataflows.", "As to your concrete use-case: people typically use ", " when simulating serial manipulators with Gazebo. This is a plugin that essentially allows you to run controllers from ", " inside Gazebo ", ".", "Communication with these controllers (running on the virtual hw) is then identical to how you'd communicate with drivers: via topics. For a serial manipulator that would probably be the ", " that accepts ", " action goals (input) and the ", " publishing ", " messages (output).", "MoveIt then consumes the ", " messages and submits the action goals.", "Final comment (and perhaps even the real answer): MoveIt doesn't actually communicate with Gazebo. In fact, it doesn't even know anything about Gazebo, or even what it's communicating with - period.", "This is all hidden by the action interfaces and topics that it uses."], "question_code": ["$ roslaunch ur_gazebo ur5.launch\n\n$ roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true\n\n$ roslaunch ur5_moveit_config moveit_rviz.launch config:=true\n", "roslaunch ur5_moveit_config ur5_moveit_planning_execution.launch limited:=true\n", "[ WARN] [1529329845.474785699]: Waiting for /joint_trajectory_action to come up\n[ WARN] [1529329851.474995251]: Waiting for /joint_trajectory_action to come up\n[ERROR] [1529329857.475168563]: Action client not connected: /joint_trajectory_action\n", "roslaunch ur5_moveit_config moveit_rviz.launch config:=true", "limited:=true", "edit"], "answer_code": ["joint_trajectory_controller", "joint_state_controller", "JointState"], "url": "https://answers.ros.org/question/293459/how-does-moveit-communicate-to-gazebo/"},
{"title": "questions about trajectory_msgs/JointTrajectory.msg", "time": "2018-06-18 23:05:21 -0600", "post_content": [" ", " ", " ", " ", "Hi guys. I'm so confused about trajectory_msgs/JointTrajectory.msg.\nLet's say I type ", "There are three joints here, my question is which joint has the trajectory such as points:", "How to specify all three different joints' trajectory using the command above? ", "Wish you guys can help me with my problems.", "Best\nYuan"], "answer": [" ", " ", " ", " ", "There are three joints here, my question is which joint has the trajectory such as points [..]", "With three joints, the message you show encodes a single trajectory with a single point (the ", " list contains only a single entry).", "That single point has three values in each of the ", " and ", " fields, one for each joint.", "There is a problem though: ", " contains three (3) elements, while both ", " and ", " contain four (4). That is invalid (the fourth value will be ignored by consumers).", "How to specify all three different joints' trajectory using the command above? ", "If I understand you correctly: just add more entries to the ", " field (which is a list). Something like the following should work:", "be sure to update ", " for each point.", "See also the documentation for ", " and ", ".", "Note: I can't remember seeing ", " being a list, so you might want to verify that. That field is not linked to the nr of joints in your trajectory.", "gvdhoorn, thank you so much, you do clear my doubt. I've creats multiple points and the robot behave as same as I expect.", "However, I do feel like I'm just implement waypoints in the joint space. Here is what I'm really doing: From matlab I've got thousands of data of each joint,positions and velocities with tiny time step. Do you think it's good idea to use this joint trajectory controller? maybe some recommendation?"], "question_code": ["rostopic pub /nao_dcm/RightLeg_controller/command trajectory_msgs/JointTrajectory '{joint_names: [\"RKneePitch\",\"RHipPitch\",\"RHipRoll\"], points: [{positions:[0,0.9,0,0],velocities:[0,0.2,0.2,-2],time_from_start: [3.0,0]}]}' -1\n", "[{positions:[0,0.9,0,0],velocities:[0,0.2,0.2,-2]\n"], "answer_code": ["points", "positions", "velocities", "names", "positions", "velocities", "points", "{\n  \"joint_names\": [\n    \"RKneePitch\",\n    \"RHipPitch\",\n    \"RHipRoll\"\n  ],\n  \"points\": [\n    {\n      \"positions\": [ .. ],\n      \"velocities\": [ .. ],\n      \"time_from_start\": [ .. ]\n    },\n    {\n      \"positions\": [ .. ],\n      \"velocities\": [ .. ],\n      \"time_from_start\": [ .. ]\n    },\n    {\n      \"positions\": [ .. ],\n      \"velocities\": [ .. ],\n      \"time_from_start\": [ .. ]\n    }\n    ,\n    ...\n  ]\n}\n", "time_from_start", "time_from_start"], "url": "https://answers.ros.org/question/294456/questions-about-trajectory_msgsjointtrajectorymsg/"},
{"title": "An alternative to data processing inside callback functions?", "time": "2016-02-24 16:28:23 -0600", "post_content": [" ", " ", "I want to perform calculations on the data from my subscriptions the instant it arrives.\nI could of course just do this inside the callback function, but i would insted like to perform these calculations outside of the callback function, right after it has returned.\nThe callback function will then just pass the data to a queue which another function will use for processing.", "My question is then: How can this be achieved?."], "answer": [" ", " ", " ", " ", " now I see that I ", " edit my answer, oops. Please make sure and read mitch722 's answer below! Original answer follows:", "By moving the calculations outside of the callback (recommended, of course!), you're explicitly decoupling calculations from the callback mechanism, thereby ", " losing the ability to begin processing the \"instant it arrives\". With that said, what you probably desire is an ", " (that operates with its own event queue, not simply a subsidiary function callback). That's beyond the scope of this answer.", "What you could do instead -- much easier, but likely uses a bit more CPU and is a bit less responsive -- is to use a combination of ", " and your own data queue. In your data callback, put the data in your queue; in the timer callback, process an (likely all) element that is in the queue (if none, just return).", "Note that by using a ", ", you're utilizing the ROS callback system. Assuming single threaded spinning, there's no need to worry about data synchronization. But note also that a typical ", " will process ", " messages in the callback queue, so special measures would need to be taken to process one message at a time (e.g., guarantee that there will always be a timer event between queued data messages).", "Wouldn't simply pushing the incoming message into an application/node level queue and then assigning that queue to a (pool of) worker threads give you that \"", "\"? That isn't too complicated, and you also don't need to resort to implementing a polling system as you describe.", "Yes, as you say, that's effectively an ", " as I used the term. With no language specified, I assume C++, so shy away from recommending multi-threading. Plus, it still seems to me that a ", " of high enough frequency is the easiest method, albeit not the most efficient/timely.", "Sorry for the late response: I am using C++, yes. After thinking about it, what I think i need here is something like a producer-consumer design. So I'm now wondering if there are ROS native functionalities to do this or if i have to do it from scratch? I should prob go read up on the design pattern", "What I described ", " a producer-consumer. The publisher is the producer, the timer callback is the consumer. The issues are the delay and interleaving message reception/processing. Y'know, if polling is a concern, you could add a one-shot timer in the message callback.", " ", " ", "I know this is a late answer but I'm adding it after doing a review of someones code and found they had implemented it blindly because it is \"recommended, of course!\" by kramer.", "Why do you need to move calculations outside the callback? The only case it's really valid is if you are filtering messages. Ie you expect more messages than your module can/wants to process so you need to throw away some of them, in which case the subscriber message queue will fill up so you may not be processing the latest message.", "In that case either add more AsyncSpinner threads to process your callback or have a calculation thread running to process your calculations. If you use multiple AsyncSpinners then use mutexes for the data accessed across mutliple threads or make your callback stateless (only depend on the variables passed into the callback) - which is a good idea anyway.", "You can also have the callback post a message into a queue (or post a semaphore) which is exactly what you did to get the message into the callback in the first place and have another thread blocking on a queue receive (or a semaphore pend) then process your data. This is the correct implementation of the producer - consumer method.", "If you are running in any normal system where you aren't spamming your modules with messages and you want to process all the messages, then handle it in the callback.\nHandling all the processing in the callback makes the code easier to read and maintain and actually REDUCES the processing load because you are not needlessly passing data around. The data has to be processed somewhere, the more it's passed around the more overhead is added. The above answer by kramer suggests moving the processing to a Timer thread. This is the worst method of all. It means that you are now adding a polling loop into your code. It's just a pain - don't do it.", "You have an event driven architecture given to you by ros and are using it! Don't use it to then create a polling architecture.", "Unless you implement some checking, you won't know if you're dropping messages. Ie if 2 messages come in before the timer goes off.", "You're adding a delay between receiving the message and processing it. If you're passing the message between many module then you might be adding a ton of latency into your system and if you aren't adding delay then you're adding a ton of processing because of the fast polling loop.", "It's extremely difficult to profile your code and work out how much load you're inducing. If you're not sending any messages your loops are all still running checking for messages. If you're sending around lots of messages you can still only process them as fast as your polling loop so you'll likely be dropping messages and not ...", "1st, IIRC, I can only comment on answers, not post another, each comment limited to ~250 chars. Let me say that your response contains many valid points. Thank you for taking the time and effort to respond. Honestly, it's much appreciated.", "2nd, I have to point out that my answer acknowledged both increased computation and time requirements. I think the key word in your 1st paragraph is \"blindly\"...it sounds like your code reviews are beneficial.", "3rd, the above said, my assumption is that the OP is ", " asking their question blindly, but has a specific need or desire. To which I provided an answer, whereas you did not. Rather, you ignored and then overrode the explicit request.", "4th, you claim that message filtering is the ", " really valid case for moving processing out of callbacks. If that's not hyperbole, I question whether you have put enough thought into the issue to make the claim at all.", "5th, the time, computation, and memory penalty from copying data in a callback for later processing is often relatively and situationally cost-free. ", "; while I still say it's recommended (for most non-trivial nodes), one should not do it blindly.", "6th, IMO, the choice of using an AsyncSpinner is not to be taken lightly. In my experience, a node complex enough to benefit from an AsyncSpinner also benefits from moving calculations out of callbacks, allowing one to separate the data and control paths (i.e., the algorithm from its fodder).", "7th, it seems to me that if you're not using mutexes with an AyncSpinner, then either: 1) you don't actually ", " an AsyncSpinner and should be using a single thread, or 2) your application just hasn't blown up yet. :)", "8th, perhaps I haven't put enough thought into this, but it seems to me that using multiple AsyncSpinners in a single node indicates structural issues that might be resolved with a code refactor. Or a lack of understanding of the ROS callback system."], "answer_code": ["Timer", "spin()", "Timer"], "url": "https://answers.ros.org/question/227483/an-alternative-to-data-processing-inside-callback-functions/"},
{"title": "How to integrate ROS and Django?", "time": "2018-08-12 22:39:10 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "i have to integrate ROS and Django.But i have no idea how to integrate it with ros. like, what kinda ros_msgs we have to use, where we have to start the node.", "I have an android app and robot. After pressing button on app, the robot should start working. Also I want to display sensor values of the robot on the app.", "Can anyone help me?", "Im using ROS Kinetic on UBUNTU 16.04.", "What do hope to achieve by integration? It sounds like you have to design your desired architecture first.", "Yeah, we have no idea what you're trying to do if you don't give us any specifics. Kind of like saying \"combine phone into bicycle\" without giving any context.", "I have an android app and robot. After pressing button on app, the robot should start working. Also I want to display sensor values of the robot on the app. @autonomy@bouke", "define 'working' !", "Working means just a trigger, like permission to start the program."], "answer": [" ", " ", "You've got a very difficult task ahead of you, given how you describe the requirements.", "On the surface, what you're trying to achieve sounds like this: ", " I don't see any reason to get Django involved here other than it being a requirement to run the app.", "Have you looked at ", "? It isn't very well documented but it does work and I've used it successfully in an Android app to send/receive simple data as well as maps and images. To send data to the robot you would create a button in the app, create a connection to the master, then make the button send a message on a topic of your choosing - sounds like it could be an empty message just to get the robot moving.", "To communicate the other way you would create a subscriber for every single sensor topic you would display. The message types will depend on what kind of sensor data you are consuming. Then you would display the data on your interface as it comes in.", "That's the gist of it. There are very many resources out there, I suggest your break down your massive goal into simpler tasks first.", " ", " "], "url": "https://answers.ros.org/question/300403/how-to-integrate-ros-and-django/"},
{"title": "Integrating ROS with an existing C++   code base", "time": "2018-08-21 18:14:00 -0600", "post_content": [" ", " ", "I have an existing C++ code base building  in Eclipse and running under Ubuntu 16.04 and I would like to incorporate some ROS (melodic) functionality (nodes, services, etc, for starters).  I've been reading the tutorials and documentation about catkin and such, and also done some searching, but I'm still unsure of the best way to make this happen.  For example, is there a way that I can add the ROS build constructs to the existing Eclipse project?  Or do I need to create a new project, and import the source?  In the latter case, do I need to add lines for all the cpp files to CMakeLists.txt manually?", " I've also found this:  ", "   , which is surely relevant, but doesn't quite describe my situation.  ", "If someone could point me in the right direction, I'd appreciate it.", "Thanks!"], "answer": [" ", " ", " ", " ", "Can you clarify what sort of integration you are looking for?", "Best practice is to keep non-ROS code \"non-ROS code\" and to create separate, relatively small (or thin) wrappers that make use of functionality provided by libraries etc.", "Doing it that way, there should be no need to import any sources or existing infrastructure into a ROS-aware IDE: just develop the non-ROS code as you always do, make sure the necessary parts can be found (by CMake) and keep the ROS adapters as ROS pkgs in a ROS workspace.", "CMake has all the support for such a workflow (and Catkin too).", "Edit:", "it sounds like in the setup you're describing, the executables would end up being the ROS nodes, and other system code would be called via libraries. Yes?", "that was my suggestion yes.", "I was hoping there was a way to basically add some ROS functionality to our project, sort of like a 3rd party SDK, rather than essentially ROS-ifying our whole project. This is starting to look like a naive notion...", "As to the type of integration you mention in your comment on ", ": see ", " (", " talks a bit about it).", "Note: this is not a typical workflow / setup, so you could run into some issues. There will also not be many people able to help you, as I believe 98% of ROS (1) users would be doing the regular Catkin based approach combined with my suggestion (calling into libraries from ROS nodes).", "Another alternative way to integrate with ROS is to not integrate at all.", "That may sound strange, but you could just use an IPC with ", " on the ROS side, and some JSON parsing/emitting code on the other. Of course this could be done the other way around as well: write a ROS node that implements / uses the ROS API that you'd like to offer / consume and use any IPC system to communicate with your existing system (gRPC, etc).", "This is a commercial product, not a research project, so I need to have a clean, maintainable design.", "And personal perhaps, but please refrain from such comments. You're implying that all \"research project\" code is not clean or maintainable. I know that is a popular thing to claim, but it is far from true and not a very nice thing to say or write.", " , thanks very much; your comments are very interesting, since that's actually what I would prefer to do.  This is a commercial product, not a research project, so I need to have a clean, maintainable design.  So we currently have an executable that spawns some subsystem threads.", "My thought was to use create a ROS service out of one of those threads and a ROS service client out of another.  But it sounds like in the setup you're describing, the executables would end up being the ROS nodes, and other system code would be called via libraries.  Yes?", "Or are you suggesting wrapping the ROS stuff somehow?  Thanks!", " thanks again for the ideas, and especially for pointing me to ", "  It gets exactly to the crux of my initial reaction to ROS and it's very good to hear folks talking and thinking about it.", "Re commercial vs research use, please accept my apologies.  I didn't mean to imply that all research code is not maintainable, etc.  Just that some research-related code may have a relatively short life-span, and hence may not have maintainability as a priority or goal, understandably.", " ", " ", "I think you're on the right track. I would create a new catkin package and add the executable and all its source files to the CMakeLists.txt file. You can also add any non standard library dependencies and compiler flags if you need to. You should be able to compile the same project without any code changes, so within catkin but with no ROS calls to test everything is working.", "Then you can start adding the ROS functionality you mentioned. ", "You can edit the new source directory in eclipse by adding an 'existing source repository'. Personally I build my packages on the command line but you can setup build and run commands within eclipse if you want. ", " thanks very much.  I believe I can do what you're suggesting.  I was hoping there was a way to basically add some ROS functionality to our project, sort of like a 3rd party SDK, rather than essentially ROS-ifying our whole project.  This is starting to look like a naive notion..."], "url": "https://answers.ros.org/question/301171/integrating-ros-with-an-existing-c-code-base/"},
{"title": "Load two urdf models in one launch file", "time": "2018-08-28 12:06:39 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "i'm trying to load two models in one lunch file. I have one file for a robot arm and one for the base, that is intended to carry the robot arm around. Each separated works but as i try to load them together only the last model is loaded. Basically that's not surprising for me since i load both with the \"robot_description\" parameter and therefor overwrite the first loaded model. But how is it done the correct way? As i name the descriptions different, rviz complains about not having a properly set \"robot_description\" parameter. Keeping one \"robot_description\" and naming the other different leads to a white model with TF complaining about not having a transform tor every of the links. So, how to load both models from one launch file?", "Thanks to everyone.", "UPDATE:", "I tried what gvdhorn suggested. I created a file \"jaco_mp470.xacro\", that only contains the following lines:", "And than tried to load it with:", "But rviz tells me that the robot_description parameter is not loaded. So what do i miss?", "Snippet of old launch file:"], "answer": [" ", " ", " ", " ", "So, how to load both models from one launch file?", "you don't.", "The (implicit) assumption is that there is only a single ", " that contains the urdf for your entire application. You can definitely have multiple parameters containing snippets of urdf, but almost every node that consumes urdf will assume that the parameter is called ", ".", "Pushing things in namespaces is supported, so that could allow you to have multiple ", " parameters, but then you run into the issues you already mentioned.", "So in your specific case: create a composite xacro that 'marries' an instance of the Kinova model with your ", ". Use a ", " joint for that fi. If you make sure to model that in a way that reflects reality, you can then even use that to do planning for both base and manipulator at the same time.", "Then load that composite xacro -- which is now also a top-level xacro -- into the ", " parameter and everything should start working.", "Edit:", "I tried what gvdhorn suggested. I created a file \"jaco_mp470.xacro\", that only contains the following two lines:", "And than tried to load it with:", "But rviz tells me that the robot_description parameter is not loaded. So what do i miss?", "Just including the two xacros is not enough.", "That is similar to ", " in C/C++ and then not using any of the functions defined in those headers. ", " doesn't magically know that you want to insert two ", " of those models in your xacro, where those should be placed in the scene or how they should be connected to each other.", "There are quite a few examples 'out there' that show you how to create a composite xacro properly, but I'm going to link you to this one: ", ".", "Notice how there are three aspects to this:", "Skip any of those three and ", " will abort with an error or return an empty document, causing any node expecting something in ", " to not be able to find anything.", "(note: the example I linked you to actually creates a xacro macro that instantiates and connects everything. If your world is simple enough, or you'll never need more than one of it, you can ignore that. You would need ", " top-level xacro to call that macro or else you'll run into the same problem again with an empty document (ie: definition but no instantiation or use))", "Btw:", "this is a really strange line to have there.", "The fake controller manager is not a suitable source of ", " messages. It's not a simulator.", "jea, you are right. That line seems not to be needed. I just ripped apart and reassembled the xacro-file from the jaco arm till the arm finally was displayed in rviz. So that's why the line ended up there.", "About the xacro file: it seems like i'm still doing something wrong. I updated my question. May i ask you to have a look at it? Maybe you find what i'm still missing.", "Oh yea, totally forgot: thanks for your answer.", "Since working with ROS i came across many questions without a real answer (especially for beginners). So i would like to post my final file to fix the models. But i don't have it working yet. Will tick as answered once i have it.", "Problem for now is indeed that i don't have a xacro macro. My urdf.xacro just starts with <robot name=\"...\" xmlns:xacro=\"...\"> and ends with </robot>. So how do i create an instance of each model in this case??", "as I wrote in my answer: you don't need to create a macro. Just instantiate the two models and add the joint as you've already done.", "Now i get it. the <xacro:[robot_name]=\"\" prefix=\"[???]\"/> is how the instance is initialized. Thought that's part of the macro. Sorry, i'm totally new to this. And it seems like it still doesn't work.", " ", " ", "HI, i am working on load two different model in r viz as you ( one is robot arm, the other is mobile robot). However, this two robot does not connect together. The manipulator is in static position and the mobile robot will move around. Would you have any advice how could i edit to use in one robot_description parameter ?", " ", " ", "Ok, once again the complete things i have done:", "in the launch file:", "in the xacro that joints together the urdf's:", "Very very thanks to gvdhoorn, who took the time to respond to all my comments. The above is his solution just typed down my me for any beginner as a complete solution (in hope that i have no typos again)."], "answer_details": ["ing the definition", "calling the macro (or 'instantiating the model')", "connecting everything together using one or more joints", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["<robot xmlns::xacro=\"http://www.ros.org/wiki/xacro\">\n\n    <xacro:include filename=\"$(find mp470_description)/urdf/mp470.xacro\"/>\n    <xacro:include filename=\"$(find kinova_description)/urdf/j2s7s300.xacro\"/>\n\n    <xacro:mp470 prefix=\"mp470\" />\n    <xacro:j2s7s300 base_parent=\"root\" /> <!-- taken from another xacro_macro file -->\n\n    <joint name=\"jaco_to_base\" type=\"fixed\">\n        <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n        <parrent link=\"mount\"/>\n        <chiled link=\"root\"/>\n    </joint>\n</robot>\n", "<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find total_model)/urdf/jaco_mp470.xacro'\" />\n<node name=\"joint_state_publisher\" pkg=\"....... />\n<node name=\"robot_state_publisher\" pkg=\"....... />\n", "<!-- platform, designed by myself-->\n<param name=\"mp470_description\" command=\"cat $(find mp470_description)/urdf/mp470.xacro\" />\n\n<!-- just the model of the arm, no controlers and no MoveIt -->\n<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find kinova_description)/urdf/j2s7s300_standalone.xacro'\"/>\n<node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\">\n  <param name=\"/use_gui\" value=\"false\"/>\n  <rosparam param=\"/source_list\">[/move_group/fake_controller_joint_states]</rosparam>\n  <param name=\"zeros/j2s7s300_joint_2\" value=\"3.1415\"/>     \n  <param name=\"zeros/j2s7s300_joint_4\" value=\"3.1415\"/> \n  <param name=\"zeros/j2s7s300_joint_6\" value=\"3.1415\"/> \n</node>\n\n<!-- rviz told me to use this line just once -->\n<node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" respawn=\"true\" output=\"screen\" />\n"], "answer_code": ["robot_description", "robot_description", "robot_description", "mp470", "fixed", "robot_description", "<xacro:include filename=\"$(find mp470_description)/urdf/mp470.xacro\"/>\n<xacro:include filename=\"$(find kinova_description)/urdf/j2s7s300_standalone.xacro\"/>\n", "<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find total_model)/urdf/jaco_mp470.xacro'\" />\n<node joint_state_publisher\" pkg=\"....... />\n<node name=\"robot_state_publisher\" pkg=\"....... />\n", "#include <..>", "xacro", "include", "xacro", "robot_description", "<rosparam param=\"/source_list\">[/move_group/fake_controller_joint_states]</rosparam>\n", "JointState", "<!-- don't forget the [$(find xacro)/xacro.py] or Permission denied error will come up -->\n<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find [folder_name])/urdf/jaco_mp470.xacro'\"/>\n\n<!-- optionally if needed -->\n<node name=\"jiont_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\">\n    <param name=\"zeros/j2s7s300_joint2\" value=\"3.141\"/>\n    <param name ............ />\n</node>\n<node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\"/>\n", "<robot name=\"base_arm\" xmlns::xacro=\"http://www.ros.org/wiki/xacro\">\n\n    <xacro:include filename=\"$(find mp470_description)/urdf/mp470.xacro\"/>\n    <xacro:include filename=\"$(find kinova_description)/urdf/j2s7s300.xacro\"/>\n\n    <!-- very basic xacro written by myself. no macros and stuff, just links and joints -->\n    <xacro:mp470 prefix=\"mp470\" />\n\n    <!-- taken from another xacro file. this model is created calling a macro (?)-->\n    <xacro:j2s7s300 base_parent=\"mount_link\" /> \n\n    <!-- not needed for me since the desired link is already named correctly -->\n    <!-- but should work like this -->\n    <joint name=\"jaco_to_base\" type=\"fixed\">\n        <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n        <parrent link=\"mount\"/>\n        <chiled link=\"root\"/>\n    </joint>\n</robot>\n"], "url": "https://answers.ros.org/question/301848/load-two-urdf-models-in-one-launch-file/"},
{"title": "How to use actions between multiple machines?", "time": "2018-07-24 15:52:35 -0600", "post_content": [" ", " ", "I'm using two Raspberry Pi 3s, using the Kinetic distribution, operating on Ubuntu 16.04.4, and coding in Python.", "One of my RPIs acts as an action client and communicates to the other RPI to control a motor through an action server. I wasn't sure how to create the .action file between the two devices, so I created a file on each RPI and made sure that their contents were exactly the same and named the same way. I'm assuming this is not the right way to do this. ", "When I tried running my code I encountered the following error:", "Can someone explain how to create the message files for multiple machines the correct way?"], "answer": [" ", " ", " ", " ", "Hi,", "It seems ROS is comparing the action file signatures from both devices in order to assure they are the same.", "My suggestion is: put all related code into one single package (with server and client). Make sure the package is building correctly. Then copy the entire package folder from one device's workspace to another and heck if it works.", "If so, consider using some version control system to keep your code synchronized across all needed devices.", "Cheers,", "Code generated from identical msg IDL should result in exactly the same message. The fact that the connection is dropped indicates that the files used to generate the action code are ", " identical.", "Suggestion: put all ", " (and ", " and ", ") files in their own pkg and use a VCS ..", ".. to manage and distribute ", ".", "No need to put everything in a single package. In fact, that just makes it harder to re-use your messages/actions/services in other packages.", "I agree. If other packages are going to use your ", " (and ", " and ", "), you should create individual packages for re-use purposes. But make sure it works the simplest possible way first. A running code is better than a beatiful one.", "There is nothing beautiful about separating out your ROS API IDL from your business logic.", "In fact, adding everything to the same pkg runs the risk of introducing the issue with build ordering between the consumers of your msg defs and the code generation targets."], "question_code": ["[WARN] [1532463151.058451]: Could not process inbound connection: Client [/Motor_server] wants topic [/Motor/goal] to have datatype/md5sum [myPackage/MotorActionGoal/db2c943e960d8ab05a1b2456743aa16c], but our version has [myPackage/MotorActionGoal/baf12fadcf56882a7d77ecbc447eecbb] Dropping connection.\n"], "answer_code": [".action", ".msg", ".srv", ".action", ".msg", ".srv"], "url": "https://answers.ros.org/question/298474/how-to-use-actions-between-multiple-machines/"},
{"title": "How to continuously publish to a topic using classses?", "time": "2018-09-10 00:40:47 -0600", "post_content": [" ", " ", "Hello I have the working code below. It basically gets an input from my joystick button to act as a on and off switch (a single press will turn it on and the second press will turn it off). However, the output I get from this code are the following:", "1st press:", "2nd press:", "I want to continuously see the output of the topic and not only when I press the button. Here is my code:", "I tried putting while(ros::ok()) but I don't know where to insert it. ", "You mean that you have the output only when the button is pressed ? Normally a joystick is continously publishing its data, can you tell us what is the publishing rate of the topic joy ? ( ", ")", "But what you can do is :", "Create a thread in the constructor of the class ", "boost::thread t = boost::thread(boost::bind(&spinthread));", "Use the thread to spin", "void spinthread() \n{\n    ros::spin()\n}", "Create a global variable in the callback and publish it in the main", "(publishing inside a while(ros::ok()) loop)", "I can see joy publishing perfectly with rostopic echo joy. But I want to continuously output the state of the button where it is pressed once (ON state = True) or pressed a second time (Off state = False)."], "answer": [" ", " ", " ", " ", "You can do it like that :", "You will be continously publishing the value of ", " which will be changed each time the callback is called. You can also add a timer and change the publishing rate as you want using ", "Creating a thread allows you to collect the data continously in the background so that if you have time consuming functions you don't miss messages that couldn't have been received since the program was still in the function.", "+1 for showing an example.", "Can you clarify for future readers why you call ", " in a separate thread? Would ", " not work here (but even that seems not needed)?", "Also: please add a ", " in there. This example utilises 100% CPU even when it's doing \"nothing\".", "I added a little precision but I'm not familiar with ", " so I can't say how it works. (but I would be glad to know if you can explain it to me)", "Take a look at ", ". ", " does just about what you do here manually.", "So is it better to use it rather than create the thread manually ?", "My suggestion would be to always use standard functionality -- if it is available. It is less work for you, recognisable for others and follows conventions.", "Note btw: if you go for multithreaded spinning that any callback you register for a topic ", " be threadsafe.", " ", " ", "Thanks to ", " and @gvdhoom. Here is my working solution. I didn't implement the spin thread since it would consume a lot of cpu resource. ", "Well you also use 100% of the CPU like that since you're not sleeping. Defining a rate doesn't apply it, you have to add ", " after ", ".", " ", " ", "Hi\nyou can use ros ", " and publish your msg continuously", "example code:", "can I just insert this in the code as another member function?"], "question_code": ["data: True\ndata: False\n", "data: False\ndata: False\n", "#include <ros/ros.h>\n#include <std_msgs/Float32.h>\n#include <sensor_msgs/Joy.h>\n#include <std_msgs/Bool.h>\n\nclass Override\n{\npublic:\n    Override();\n\nprivate:\n\n    void joyCallback(const sensor_msgs::Joy::ConstPtr& joy);\n    ros::NodeHandle nh_;\n    ros::Publisher override_pub;\n    ros::Subscriber joy_sub_;\n    bool currentReading;\n    bool lastReading;\n    bool flag;\n};\n\nOverride::Override()\n{\n\n  override_pub = nh_.advertise<std_msgs::Bool>(\"override_status\", 1);\n  joy_sub_ = nh_.subscribe<sensor_msgs::Joy>(\"joy\", 10, &Override::joyCallback, this);\n\n}\n\n\n\nvoid Override::joyCallback(const sensor_msgs::Joy::ConstPtr& joy)\n{\n\n  std_msgs::Bool override_status;\n\n  if (joy->buttons[4] == 0.0){\n    currentReading = false;\n\n  }else currentReading = true;\n\n  if (currentReading && !lastReading) {\n    flag=!flag;\n    if (flag) {\n      override_status.data = true;\n    }\n    else {\n      override_status.data = false;\n    }\n  }\n  lastReading = currentReading;\n  override_pub.publish(override_status);\n}\n\nint main(int argc, char** argv)\n{\n  ros::init(argc, argv, \"override_node\");\n  Override override;\n  ROS_INFO(\"Running\");\n  ros::spin();\n}\n", "rostopic hz /joy"], "answer_code": ["//You libraries here\n#include <boost/bind.hpp>\n#include <tile_planner/reach_goals.h>\n\n//Create a thread only for ros spin\nvoid spinthread()\n{\n    ros::spin();\n}\n\n//Deifne a global variable\nstd_msgs::Bool override_status;\n\n//Construcor\nOverride::Override()\n{\n//Define sub and publishers and start the thread\n  boost::thread t = boost::thread(boost::bind(&spinthread));\n}\n\n//Callback\nvoid Override::joyCallback(const sensor_msgs::Joy::ConstPtr& joy)\n{ \n//Your callback routine here where you set override_status \n}\n\nint main(int argc, char** argv)\n{\n  ros::init(argc, argv, \"override_node\");\n  Override override;\n  //Set rate at 10Hz \n  ros::Rate r(10);\n  while(ros::ok())\n  {\n    override_pub.publish(override_status);\n    r.sleep();\n  }\n}\n", "override_status", "ros.sleep()", "ros::spinOnce()", "AsyncSpinner", "ros::Rate::sleep()", "AsyncSpinner", "AsyncSpinner", "#include <ros/ros.h>\n#include <std_msgs/Float32.h>\n#include <sensor_msgs/Joy.h>\n#include <std_msgs/Bool.h>\n\n\nbool flag;\n\n\nclass Override\n{\npublic:\n    Override();\n  void joyCallback(const sensor_msgs::Joy::ConstPtr& joy);\n  void Publisher();\n    std_msgs::Bool override_status;\n\nprivate:\n    ros::NodeHandle nh_;\n    ros::Publisher override_pub;\n  ros::Subscriber joy_sub_;\n  bool currentReading;\n    bool lastReading;\n  std_msgs::Float32 button_status;\n\n};\n\nOverride::Override()\n{\n\n  override_pub = nh_.advertise<std_msgs::Bool>(\"override_status\", 1);\n  joy_sub_ = nh_.subscribe<sensor_msgs::Joy>(\"joy\", 10, &Override::joyCallback, this);\n\n\n}\n\nvoid Override::joyCallback(const sensor_msgs::Joy::ConstPtr& joy)\n{\n   button_status.data=joy->buttons[4];\n\n  if (button_status.data == 0.0){\n    currentReading = false;\n  }else currentReading = true;\n\n  if (currentReading && !lastReading) {\n    flag=!flag;\n    if (flag) {\n      override_status.data = true;\n    }\n    else {\n      override_status.data = false;\n    }\n  }\n  lastReading = currentReading;\n\n}\n\nvoid Override::Publisher()\n{\n  override_pub.publish(override_status);\n}\n\n\n\n\nint main(int argc, char** argv)\n{\n  ros::init(argc, argv, \"override_node\");\n  ROS_INFO(\"Node Started...\");\n  Override override;\n  ros::Rate r(10);\n  while (ros::ok()){\n    override.Publisher();\n    ros::spinOnce();    \n  }\n}\n", "r.sleep();", "spinOnce()", "ros::Timer timer = nh_.createTimer(ros::Duration(0.1), timerCallback);\n\n.\n.\n.\n\nvoid callback(const ros::TimerEvent& event)\n{\n  override_pub.publish(override_status);\n}\n"], "url": "https://answers.ros.org/question/302866/how-to-continuously-publish-to-a-topic-using-classses/"},
{"title": "Looking for existing Electromyograph EMG/EEG, or voltage messages", "time": "2018-10-19 04:10:40 -0600", "post_content": [" ", " ", " ", " ", "I am in the process of implementing a ROSBag writer for an electromyograh device.", "Before implementing my own messages, I would like to know if such messages already exist.", "Basically I al looking for Electromyograph EMG/EEG messages, or, if they don't exist, a basic voltage message of the type:", "I don't know if there already exists the message you need. But you can create your own ROS message and use that. Look ", " for more information.", ".. that could become a standard in the (near) future."], "answer": [" ", " ", "I'm not aware of a message set for EEG/EMG.", "There was a lightning talk about a (consumer) EEG device last ROSCon (", "), but that doesn't seem to include reusable messages.", "Might make sense to propose some new msgs .."], "question_code": ["Header header\nfloat64 voltage\nfloat64 variance\n"], "url": "https://answers.ros.org/question/306234/looking-for-existing-electromyograph-emgeeg-or-voltage-messages/"},
{"title": "How display pointcloud messages in browser using rosjs ?", "time": "2012-09-13 03:29:53 -0600", "post_content": [" ", " ", " ", " ", "Hi guys, ", "\nI have found the same ", " but the answer seems to be obsolete.\nConcerning, ", ". I can't find some JavaScript files used in this tutorial. Especially ", ".  ", "They are not included in rosbridge nor wviz packages. I can't find them on ", " ", " \nAre they obsolete ? Where can i find the last version? ", "By the way, how can I display my pointcloud from ROS messages in my webpage using rosjs.", "\nThanks,", "\nBeuBeu"], "answer": [" ", " ", " ", " ", "Hi Beubeu,", "as Dejan pointed out wviz now lives in ", " stack. Wviz uses javascript files included in ", "Just to test, you could trying installing wviz and adding a laser scan display in the scene. ", "If (that works) && (you want to make your own custom web page) && (you need help) then let's take the topic to ", ".", "If you are interested in visualizing point clouds, then please use Brown's pcl_filter package (it will throttle the data and prevent Chrome from crashing). In the near future rosjs_visualization (and wviz) will use compression to reduce the bandwidth consumption. Until that day, pcl_filter is our best friend.", "Ben", " ", " ", "Web visualization was revamped recently, please have a look at ", " If that does not help you further please contact ", " he is the maintainer of the stack.\nD.", " ", " ", "I'm afraid the ", " might be considered as outdated as there doesn't seem to have been update since ", " (6 yrs ago?).", "Meanwhile, ", " in ", " might help (I haven't tried yet though)."], "answer_details": ["rosjs_visualization (this package might be what you're looking for btw), ", "rosjs_common, and, ", "rosjs_resources", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["ros3djs"], "url": "https://answers.ros.org/question/43781/how-display-pointcloud-messages-in-browser-using-rosjs/"},
{"title": "ROS2 and custom mutexes", "time": "2018-11-07 10:33:46 -0600", "post_content": [" ", " ", "Hi,", "Can a node which waits on a generic mutex block ROS2? For example, is it that SingleThreadedExecutor may \"get out of threads\" in such a case? If yes, does ROS2 provide its own mutexes, which \"yield\" a waiting thread back to the executor?"], "answer": [" ", " ", "If you block (for any reason, including acquiring a mutex) in a callback and are using a SingleThreadedExecutor, it will prevent other callbacks from being called.", "You can either, avoid blocking and defer the work (asynchronous programming) or you can use a multi threaded executor, which can still get exhausted, but at least will allow other callbacks to be called at the same time so long as there are more thread available.", "If you just need to ensure two callbacks do not get called at the same time (you would otherwise use a scoped lock with a shared mutex in the root of each callback) then you can instead put them in the same \"MutuallyExclusiveCallbackGroup\" (this is the default behavior if you just call ", ") and the then the executor will ensure they are not called concurrently in a multi threaded executor. This prevents you from having N callbacks, all waiting on the same mutex, from consuming and blocking in all N threads of your multi threaded executor.", "See:", "We've got some documentation about executors and callback groups in the works but it's not public yet."], "answer_details": [" ", "slides 17-19", " ", "13 min and on", " ", " ", " ", " "], "answer_code": ["create_{timer, subscription, service, client}"], "url": "https://answers.ros.org/question/307821/ros2-and-custom-mutexes/"},
{"title": "Intel realsense D435 Pointcloud2 without RGB", "time": "2018-11-20 10:36:21 -0600", "post_content": [" ", " ", " ", " ", "Dear all, ", "I was wondering if there is a way to have access to the ", " instead of the ", " in the ", " ROS Package. ", "The idea is to use as less as possible the CPU computational power.", "I have looked at ", " but it was not working properly.", "Thanks,"], "answer": [" ", " ", "I got the solution in the last version of the  ", ".", "The following configuration should be defined in order to achieve the PointCloud withou RGB:", "Unfortunately the current version does not support the ", " therefore the result is not the same as I wished cause when you choose the ", " the PointCloud will have more than twice of Points which leads to have even a higher CPU consumption with respect to the PointCloud with RGB.", " ", " ", "we meet similar issue in ros2_object_analytics,  subscribe pointcloud xyzrgb and split to rgb and xyz. to reduce transport bandwidth.", "The code for reference.\n", ", ", "Thanks a lot for your answer. I will try it and if it solves my problem I will mark it as an answer. ", "But I really like to deactivate the XYZRGB PointCloud2 from the driver source.", " ", " ", "My fork will do this for you ", ". ", "Also, if you're just looking for something to work, I release a snap package as well. Source is ", ", but available on the snap store. Install instructions for the snap store are in the readme as well.", "The ros realsense drivers haven't been stable since the r200's so if you're trying to do other things, I would consider forking and working on it.", "Thanks a lot for your answer. I really like to try your fork but unfortunately I did not achieve to compile it. I use Linuxu 16.04 with Kernel 4.15.0-39-generic; ROS kinetic and the librealsense2 installed on my Linux."], "answer_code": ["<!-- Intel Realsense D435 driver -->\n<include file=\"$(find realsense2_camera)/launch/rs_camera.launch\">\n<arg name=\"pointcloud_texture_stream\" default=\"RS2_STREAM_ANY\"/>  \n<arg name=\"enable_pointcloud\"         default=\"true\"/>\n<arg name=\"enable_color\"        default=\"false\"/>\n<arg name=\"enable_depth\"        default=\"true\"/>\n<arg name=\"enable_infra2\"        default=\"false\"/>\n<arg name=\"enable_infra1\"        default=\"false\"/>\n<arg name=\"align_depth\"         default=\"false\"/>\n<arg name=\"depth_width\"         default=\"640\"/>\n<arg name=\"depth_height\"        default=\"480\"/>\n</include>\n"], "url": "https://answers.ros.org/question/308978/intel-realsense-d435-pointcloud2-without-rgb/"},
{"title": "Clone all packages from a yaml file?", "time": "2018-12-18 04:25:53 -0600", "post_content": [" ", " ", "For ROS2, you can clone all registered packages with a simple command:", " How can you do the same for the ROS 1 package listed in this yaml file: \n  ", "Thanks. "], "answer": [" ", " ", " ", " ", "Edit:", "just checked and apparently ", " accepts an ", " argument that: \"specif[ies] all release packages\":", "Running the following command gets me a ", " file with approx 500 KB of info on all released packages in ROS Kinetic:", "Note that this also includes information on their dependencies. Adding ", " is probably not needed, as they can only depend on other ROS packages or system dependencies, and all ROS packages are already in the selection, but it won't hurt.", "Note: this gets you a ", " file for all packages ", " (ie: released as of when you invoke that command).", " For ROS2, you can clone all registered packages with a simple command: [..] How can you do the same for the ROS 1 package listed in this yaml file:  ", "I don't believe there is tooling that will \"clone all registered packages\" for you. Or at least, not directly with/from that particular ", " file. The ", " file does not contain the same sort of abstractions / information as the ", ".", "The former is a plain listing of repositories that ", " can clone for you (using ", " or another vcs tool), while ", " contains all sorts of meta-data about packages and repositories.", " can use that information to generate a ", " file for you that is (at least conceptually) the same as the ", " file: ", " files contain the same sort of information, but are typically consumed by ", " (", " can also consume them).", "As ", " is a plain ", " file, you could consider creating a simple Python script that loads it and then uses the ", " dicts in it to clone a specific version of ", " packages registered for that distribution. I'm not aware of a tool that does this already, as it's not a typical use of ", ".", "If you'd like to do things a little more structured, you could actually use ", " to generate ", " files specifically for a set of packages (so including their (transitive) dependencies).", "You may actually be able to get ", " to generate a ", " file for all packages in the ", " though, by using certain parameters. I haven't checked whether that is possible.", "This is (somewhat) related to the work we're doing with the ", ", but there we go back in time first and then generate ", " files."], "question_code": ["wget https://raw.githubusercontent.com/ros2/ros2/master/ros2.repos\nvcs import ~/ros2_ws/src < ros2.repos\n"], "answer_code": ["rosinstall_generator", "ALL", "  pkgname               catkin package names, rosbuild stack names or variant\n                        names. Use 'RPP' to specify all packages available in\n                        the current environment. Use 'ALL' to specify all\n                        release packages (only usable as a single argument).\n", ".rosinstall", "rosinstall_generator --rosdistro=kinetic --deps --tar ALL > all.rosinstall\n", "--deps", ".rosinstall", ".yaml", "ros2.repos", "distribution.yaml", "vcs", "git", "distribution.yaml", ".rosinstall", "ros2.repos", ".rosinstall", "wstool", "vcs", "distribution.yaml", ".yaml", "release", "distribution.yaml", ".rosinstall", "rosinstall_generator", ".rosinstall", "distribution.yaml", ".rosinstall"], "url": "https://answers.ros.org/question/310959/clone-all-packages-from-a-yaml-file/"},
{"title": "Unabe to Install freenect in ROS-Melodic/Ubuntu 18.04", "time": "2019-01-27 04:59:02 -0600", "post_content": [" ", " ", " I wanted to install freenect in ROS to take depth images using Kinect XBOX 360 1473. I cloned the repository from  ", "  and kept it in ~/catkin_ws/src under the folder name freenect_stack. Upon running catkin_make in ~/catkin_ws I get the following error: ", "Please help. Any input is welcome."], "answer": [" ", " ", "I was able to install this, and I don't know if it's going to work like I'm expecting but I now have libfreenect and the ros freenect_launch and other freenect_node stuff all installed in Melodic on Ubuntu18.04, per the commands below:", "Compile/install libfreenect from source, ", " an apt install of libfreenect-dev (if you installed that via apt it shouldn't hurt, it just won't work.  No need to remove that apt package afaik):", "Then, cd into your catkin workspace source, (I'm assuming yours is \"~/catkin_workspace/src\" but you may need to change that if yours is different).  Clone the repo, then cd up one directory back into your catkin workspace root, and run catkin_make:", "I'm on Ubuntu 18.04 running ROS Melodic, and was able to get this installed the day of this post (July20-22, 2019).", "Whoever is running this repo really should just update the package to make it installable via apt, it already appears to work with Melodic, it's just grief for us noobs.. ", "(just kidding, I appreciate the work that's been put into this; without it my $8  Xbox 360 Kinect from a thrift store wouldn't be of any use to me.  Would really like to help make this easier to install on Melodic, but the repo looks so dead I don't even want to put in a pull request..)", "Let me know if you run into problems with this; I haven't tested it at all but it ", " install.", "It's still a very useful tool you know?  What's 2019 if you can't build robots of doom out of scrap consumer electronics?..  ", " ", " ", "Is there a reason that you can't install the binary?", "Anyways, try installing the dependencies before compiling. From the root of your workspace, run", "Is there a reason that you can't install the binary?", "afaict, ", " hasn't been released for Melodic.", " Thanks. I figured as much, wasn't sure though", "I guess libfreenect does not work with melodic :(", "The error I got when I ran rosdep:  ", " Thanks", "I'm having the same issue. I have libfreenect and libfreenect-dev installed, but freenect_stack won't build.", "same problem :-(", "see my answer posted itt; I was able to get it installed and preliminary tests suggest it works fine."], "question_code": ["[ 21%] Building CXX object freenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/src/nodelets/driver.cpp.o\nIn file included from /home/athul/catkin_ws/src/freenect_stack/freenect_camera/src/nodelets/driver.h:54:0,\n                 from /home/athul/catkin_ws/src/freenect_stack/freenect_camera/src/nodelets/driver.cpp:39:\n/home/athul/catkin_ws/src/freenect_stack/freenect_camera/include/freenect_camera/freenect_driver.hpp:4:10: fatal error: libfreenect/libfreenect.h: No such file or directory\n #include <libfreenect/libfreenect.h>\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nfreenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/build.make:62: recipe for target 'freenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/src/nodelets/driver.cpp.o' failed\nmake[2]: *** [freenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/src/nodelets/driver.cpp.o] Error 1\nCMakeFiles/Makefile2:2697: recipe for target 'freenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/all' failed\nmake[1]: *** [freenect_stack/freenect_camera/CMakeFiles/freenect_nodelet.dir/all] Error 2\nMakefile:140: recipe for target 'all' failed\nmake: *** [all] Error 2\nInvoking \"make -j1 -l1\" failed\n"], "answer_code": ["cd  ~    \ngit clone https://github.com/OpenKinect/libfreenect.git    \ncd libfreenect    \nmkdir build   \ncd build    \ncmake -L ..    \nmake\nsudo make install\n", "cd ~/catkin_workspace/src\ngit clone https://github.com/ros-drivers/freenect_stack.git\ncd ..\ncatkin_make\n", "~/catkin_ws$ rosdep install --from-paths src -i\n~/catkin_ws$ catkin_make\n", "ERROR: the following packages/stacks could not have their rosdep keys resolved\nto system dependencies:\nfreenect_camera: Cannot locate rosdep definition for [libfreenect]\n"], "url": "https://answers.ros.org/question/313893/unabe-to-install-freenect-in-ros-melodicubuntu-1804/"},
{"title": "Releasing a package with a dependency that is not in ROS ecosystem", "time": "2019-01-31 12:08:05 -0600", "post_content": [" ", " ", "Hi, I'm trying to release a package that depends on other ROS package but is not indexed nor published yet.\nI use it as a submodule to clone in my repo or simply just adding the dependency packages to the workspace and compile the hole files along mine. \nNow the build farm fails because it can't resolve the rosdep key.\nIs it possible to do this release with a non-published dependency? \nThanks "], "answer": [" ", " ", " ", " ", "Unfortunately, afaik the answer is \"no, not at this time\".", "The ROS buildfarm uses a Debian style approach to building packages, so all dependencies must either be released ROS packages ", " system dependencies that have been released as well.", "Is there a reason the dependency is not released? If there is none, I would first release your dependency and then release the dependent (you could even do it at the same time if they are hosted by the same repository).", "I haven't tried this, but I have speculated if you add a git submodule to this dependency in your repo, when the buildfarm clones it, it might grab that code too to build.", "Recursive cloning is supported by ", " since ", ", but I'm not sure that is just for locally run jobs or also on the buildfarm, and whether release jobs support it.", "Perhaps ", " or ", " can say something about that.", "From a system hygiene perspective, I cannot recommend completely vendoring a non-trivial dependency and I would go as far as saying that to do so for code that will be accessible via ABI/API to package consumers is a no-go. Releasing the dependency via Debian/Ubuntu or the ROS buildfarm is preferred"], "answer_code": ["ros_buildfarm"], "url": "https://answers.ros.org/question/314383/releasing-a-package-with-a-dependency-that-is-not-in-ros-ecosystem/"},
{"title": "Modify the robot_description parameter at runtime", "time": "2019-01-28 05:29:41 -0600", "post_content": [" ", " ", "Hello,", "I am trying to put identified offsets to the nominal URDF-values of my industrial robot. Since I do not want to modify the support package of my robot, I thought a good way to insert the identified rotary and translation offsets would be to change the ", " on the parameter server.\nIs there a more elegant way to fulfill my goal, or do I really have to modify the string from the ", " parameter?\nIs it even possible or advisable to change values of the robot description at runtime?", "Thanks for all recommendations.", "Is it even possible or advisable to change values of the robot description at runtime?", "it's possible, but there are a whole host of potential issues with it.", "I can't find them right now, but there have been discussions about this in the past. Main issue is that consumers of URDFs do not ..", ".. expect the parameter (so ", ") to ever change. So even if you update it, almost no readily available software that consumes URDF/XACRO will ever load it more than once, and will not see your updated content.", "Well that sounds pretty disappointing. Are there any possibilities to include my identified URDF-Offsets, without directly changing the URDF then?", "it's possible, but there are a whole host of potential issues with it.", "That's what I thought. I figured that the robot description is probably loaded once by all instances, that use it (like moveit)."], "answer": [" ", " ", "To close this: It seems like that modifying the URDF is the only \"proper\" way of modifying the robot_desription parameter, so that it can be used by any software consumer, that reads the URDF/XACRO.", "The ", " package does the same thing: \nCalibrated results are stored inside of a new modified urdf file.", "You might want to take a look at the YAML support in xacro. See ", " for a prototype of how that can be used for these kind of use-cases."], "question_code": ["robot_description", "robot_description", "robot_description"], "url": "https://answers.ros.org/question/313954/modify-the-robot_description-parameter-at-runtime/"},
{"title": "Can gmapping package be used for commercial purpose to make a map?", "time": "2019-02-26 00:22:20 -0600", "post_content": [" ", " ", "Would you let me know, what are the licensing conditions of gmapping package? Can we use it for commercial purpose to make a map? "], "answer": [" ", " ", "Note that ROS answers is a Q&A page focused on the technical issues on ROS. Thus, most (all?) people here come from technical backgrounds.", "Any responses to questions like this are under no circumstances to be taken as legal advice! This is especially true since there are a lot of variations of different licenses out there.", "If you are interested in this topic, there are a lot of good websites out there, e.g. ", ", which give good overviews. But they all agree in that they are also NOT to be taken as the only source of legal adivce (and obviously exclude any warranty on what they say/suggest)!", "The above aside:", "According to ", ", ", " is currently licensed as", "License: CreativeCommons-by-nc-sa-2.0", "The ", " stands for ", ". I guess this should answer your question (mostly...)", "Mostly because...", "And now this is where it gets really tricky with legal advice. So I'll stop talking and only advice to to consult someone with real expertise in OSS-issues..."], "answer_details": [" has changed to BSD 3-clause", " where all contributors to the forked ", " package (which still is CC-by-nc-sa) agree to change the license to BSD 3-clause as well.", " ", " ", " ", " "], "answer_code": ["gmapping", "nc", "non-commercial", "gmapping"], "url": "https://answers.ros.org/question/316637/can-gmapping-package-be-used-for-commercial-purpose-to-make-a-map/"},
{"title": "ROSPY: Interrupting with a control-c", "time": "2018-08-01 12:09:50 -0600", "post_content": [" ", " ", " ", " ", "Sometimes when my raspy node is running (launched with rosrun) I can interrupt it with a control-c, and sometimes not. \nWhat's the best practice for making sure that my raspy nodes see a control-c and allow me to do a clean shutdown?", "What about this case, where I think it all looks correct and nothing in the loop is long running, and yet I often have to do ^c many many times before it responds. Oddly shift ^c seems to interrupt better. I don't even know what shift ^c is!", "Are you checking ", " during your loops?  ", "Yes, in some cases. But I was looking at sample code and I saw some examples that didn't do that. Also I found that even if I did rosy.is_shutdown() it seemed sometimes like the control-c didn't work. So I thought there might be a best practice for ensuring that I can always kill my nodes", ": ", " already answered the previous version of your question, and you accepted the answer. It's considered bad form to edit a question after there is an accepted answer. It would be better to post a new question as a follow-up (which I believe you already did: ", ")."], "answer": [" ", " ", "If your node does some long-running processing then that will block interrupting it with control-c. Here is some simple code that demonstrates this:", "I set up the nested loops so it \"processes\" for about 5 seconds on my computer. If I press ", " during that time, it won't actually handle the interrupt until the nested loops finish.", "If you need your node to quickly shut down at any time for some reason (e.g. notifying other nodes rapidly), then you need to plan your time-consuming processing. There are a number of strategies you can use. Here are a few:", "Break your processing down into smaller chunks, and manage them so that you have more frequent checks of if the node has been shut down or not. Depending on the node's functionality, you may also want to preserve processing results in between chunks for some reason. You can process one chunk in each iteration of your node's main ROS loop, for example.", "Put the processing in a separate thread, and if necessary use a semaphore or similar communication mechanism to control that thread's execution. The main thread would run in a standard ROS processing loop. However this is mostly duplicating the effort of ", ", just with more control over ", " you stop processing.", "If you are using a recent enough version of Python, then use ", " and impress your friends.", "Just put calls to ", " throughout your code. It's a little ugly and could leave some system state in a mess, but it's a simple and effective approach, especially if you don't want to deal with threads.", "It's also important to note that if your node is not able to respond to ", " then it is also not able to respond to new data arriving on topics, service calls, action results, timers, and anything else that you register callbacks for. Any time you block the node (i.e. spend a long time without giving the ROS infrastructure some CPU time) you effectively freeze the node from the point of view of the rest of the system. If you need a node that does long running processing ", " is responsive to new data, service requests, or similar then you need to use one of the strategies mentioned above to separate long-running processing from the main ROS loop.", "Thanks for the excellent insights. Basically only at is_shutdown is there\u2019s a chance to detect a possible control-c.", "Not entirely true. Calls such as ", " and ", " will also detect a ", "."], "question_code": ["    rate = rospy.Rate(5)\n    count_log = 0\n\n    while not rospy.is_shutdown():\n        count_log += 1\n        if (count_log % 10 == 0):\n            print(\"\\n#%s [%s] %s=%1.2f\" % (count_log, self.state,\n                                         self.m.closest_dir, self.m.closest_dist))\n        if (self.m is None):\n            pass\n        elif (self.state == \"find_wall\"):\n            self.handle_find_wall()\n        elif (self.state == \"follow_wall\"):\n            self.handle_follow_wall()\n        elif (self.state == \"emer_stop\"):\n            self.handle_emer_stop()\n        rate.sleep()\n", "rospy.is_shutdown()"], "answer_code": ["import rospy\nimport time\n\nrospy.init_node('blag')\nwhile True:\n    print('beginning sleep')\n    for i in range(1000):\n        for j in range(1000):\n            for k in range(500):\n                pass\n    print('ended sleep')\n    if rospy.is_shutdown():\n        print('shutdown')\n        break\nprint('finished')\n", "Ctrl-C", "rospy.is_shutdown()", "rospy.is_shutdown()", "Ctrl-C", "rospy.spin()", "rate.sleep()", "Ctrl-C"], "url": "https://answers.ros.org/question/299328/rospy-interrupting-with-a-control-c/"},
{"title": "Organizing point cloud from HDL-32E", "time": "2013-04-01 23:41:24 -0600", "post_content": [" ", " ", " ", " ", "Hi there,", "in order to accelerate the data processing on my HLD-32E I would like to organized the data. To this point, I could, under Octave, compute manually an point cloud. But I do actually have no idea how to implement it in C++ and the suggested functions, such as KDTree, don't seems to be fast enough for this task... ", "Is there somewhere a topic giving information about how to work with those Point clouds in ROS?"], "answer": [" ", " ", " ", " ", "Are you using the ROS ", " stack to provide the data?", "If so, the ", " package handles converting the raw data packets received from the device into ROS ", " messages. The HDL-32E spews out about 700,000 points per second in a highly permuted order, the HDL-64E provides about 1,000,000 points per second in a very different and less regular order. So, velodyne_pointcloud keeps things simpler by always producing \"unordered\" point clouds as the data arrive from the device.", "To prevent that unordered representation from losing useful information, velodyne_pointcloud publishes its data using a custom PCL point definition: ", ", defined in the ", " header. A ", " differs from the standard PCL ", " (X, Y, Z, and intensity) in adding an additional field containing the \"laser ring number\" from which the point was measured. ", "Each ring number corresponds to a device-specific laser number, but the numbers are reordered so that ring 0 corresponds to the innermost laser (at the lowest angle) and ring 31 corresponds to the outermost laser (at the highest angle) of a 32E (that would be ring 63 on a 64E). That encoding ensures that adjacent lasers are labelled with adjacent ring numbers, which is not at all true of the hardware laser numbers. The actual mapping is defined in the ", " header, provided by the velodyne_driver package. The permutation for a 32E is the same as the first 32 lasers of the 64E, so the same conversions work for both.", "So, it should theoretically be possible to produce an ordered point cloud from the unordered PointXYZIR data by using the ring numbers as rows in a 2D representation. I have not heard of anyone actually doing that, and I suspect that the results might become somewhat confused when the device is in motion.", "Sorry for the long delay, but it could finaly organize the pointcloud and now I can segment and classify the point cloud in less than 0.2s ;)", " Do you mind describing how you were able to do that? Did you use PCL at all?", "Hi. I need the pointcloud from Velodyne VLP16. You mentioned that \"velodyne_pointcloud publishes its data using a custom PCL point definition: velodyne_pointcloud::PointXYZIR\". Do you mean that is a topic? After I do roslaunch \"velodyne_pointclound VLP16_points.launch\", nothing like that are found", "For Velodyne VLP16, nothing like velodyne_pointcloud::PointXYZIR are published. Please tell me why", "The topic name is ", " I don't know if this is still useful for you, but I wrote some code with PCL to organize a VLP16 point cloud and now I have a 16xN point cloud. I am still doing some tests, but you can find my code here:  ", ", did you ever get that code fully functional? I've tried it out but haven't been able to get it working so far.", " ", " ", " Use the function I wrote here:  ", "I had to solve this problem recently in order to use the cloud for mesh creation with the Point Cloud Library (PCL). At the time I couldn't find anyone else who had solved this problem, so I had to write the code myself following the suggestions here from ", ". I've added to core functions to the repo linked above, as they're a bit too long to put here. Hopefully this will save anyone after me ~1-2 weeks effort. ", "How it works is this: every Velodyne LiDAR unit fires its lasers in a pre-defined sequence that repeats. E.g. 0, 16, 1, 17, 2, 18... But not every laser beam returns to the unit, so some parts of the cloud need to be filled in with points whose coordinates are ", " to preserve the organized nature of the cloud. My function creates a 2D organized cloud where there is one row for each \"ring\" of velodyne data. It then fills in each row, tracking how many points are in each row, i.e. what \"column\" of data is currently being filled in. When it moves from one column of data to the next, it fills in any gaps with ", " points.", "Be careful how you use the organized cloud after this, as some PCL functions will crash if you input a cloud with ", "s, or worse, silently destroy the organized nature of the cloud without warning you.", ": would this be something to contribute to some more visible package? I'm thinking ", " or something similar, but then for lidars. Perhaps even the velodyne package itself?", "That might certainly be a good idea, the velodyne package would be the best fit as the function depends on the specific operation of Velodyne LiDARs. How time consuming is that process likely to be? Unfortunately I have precious little spare time right now.", "re: time: no idea. I'd suggest just opening an issue on the velodyne tracker and seeing whether there is any interest.", " ", " ", " ", " ", "An organized point cloud is just a point cloud where some information about the sensor that captured the \"depth image\" is given. E.g. for a kinect with a resolution of 640 x 480 pixels, the properties of an organized point cloud would be set to", "Internally, the point cloud stores all 3D points in a vector that contains", "The important bit is that ", " measured points have to be stored in the point cloud. As soon as some (invalid, ...) points are filtered, the cloud is no longer organized.", "So if you have a laser that e.g. provides 1.000 points per line and scans 500 lines per \"picture\" (?), you'd just push_back the data points into the point cloud in the order which they were acquired (starting with the first one of the first line, ending with the last on eof the last line). Then set width to 1.000, height to 500, and voila!", "Disclaimer: I haven't worked with lasers much, so apologies for the poor terminology! The above of course only makes sense when there's no motion in the scene during the time all lines are captured..."], "answer_code": ["/velodyne_points", "NaN", "NaN", "NaN", "laser_filters", "width: 640\nheight 480\nisOrganized = true.\n", "point(0,0), point(1,0), ..., point(638, 0), point(639,0),\npoint(0,1), point(1,1), ..., point(638, 1), point(639,1),\n...\npoint(0,479), point(1,479), ..., point(638, 479), point(639,479)\n"], "url": "https://answers.ros.org/question/59743/organizing-point-cloud-from-hdl-32e/"},
{"title": "Combines message for image and information gives \"invalid initialization error\"", "time": "2019-04-29 12:45:36 -0600", "post_content": [" ", " ", "Hello,", "I am currently trying to implement a message that includes the images as well as the detected bounding boxes.\nThe publisher works very well, but the subscriber gives me the following error message:", "This is the message in question:", "Header header ", "Header image_header", "BoundingBox[] bounding_boxes", "sensor_msgs/Image im", "And this is my subscriber:"], "answer": [" ", " ", " ", " ", "Isn't the problem here that you're trying to use ", " to subscribe to a topic which isn't actually carrying ", "s, but ", "?", "That's also what the error message seems to tell you. Paraphrasing, it would read:", "You'll probably have to switch to a regular ", " instead of using ", "'s.", "Edit:", "Oh, right yes... But how could I rewrite it? So that I still can use the image_transport functionality?", "I'm not sure. Possibly ", " using a custom message, but using something like ", " to combine related messages based on their timestamps (so only publish a list of boundingboxes (one from ", " preferably) with the same timestamp as the ", " that the detection was run on, then later recombine the two using ", ").", "I'm not an ", " expert though, so there could be a different option.", "There is a tutorial on writing your own transport plugin (", "), but I don't believe that is something that would fit your use-case.", "Edit 2:", "Edit: Thank you! Now it works. But what I am still not sure about: Is it now slower? Because i don't use the image transport functionality?", "Using ", " doesn't make anything magically faster. The \"only\" thing ", " does is making it easier to use different compression algorithms (called \"transports\" in ", " lingo) without having to deal with their details in image producers and consumers. Simply configure a specific transport to be used, and without recompiling or changing anything in your publishers or subscribers it will be used. Transparently.", "In the case of compression, this actually has the potential to make things ", " (as compression adds overhead), but that obviously depends on a number of other factors (size of images vs bandwidth, latency, etc).", "So with switching to a regular ", ", you lose all of that. But you had already lost that, as you were using a regular ", " as well.", "Oh, right yes...\nBut how could I rewrite it?\nSo that I still can use the image_transport functionality?", "Edit: Thank you! Now it works. But what I am still not sure about: Is it now slower? Because i don't use the image transport functionality?"], "question_code": ["In file included from /usr/include/boost/function/detail/maybe_include.hpp:18:0,\n                 from /usr/include/boost/function/detail/function_iterate.hpp:14,\n                 from /usr/include/boost/preprocessor/iteration/detail/iter/forward1.hpp:52,\n                 from /usr/include/boost/function.hpp:64,\n                 from /opt/ros/kinetic/include/ros/forwards.h:40,\n                 from /opt/ros/kinetic/include/ros/common.h:37,\n                 from /opt/ros/kinetic/include/ros/ros.h:43,\n                 from /home/marvin/catkin_ws/src/efr_object_detection_camera/src/template_matching.cpp:5:\n/usr/include/boost/function/function_template.hpp: In instantiation of \u2018static void boost::detail::function::void_function_invoker1<FunctionPtr, R, T0>::invoke(boost::detail::function::function_buffer&, T0) [with FunctionPtr = void (*)(const darknet_ros_msgs::ImageWithBoundingBoxes_<std::allocator<void> >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&]\u2019:\n/usr/include/boost/function/function_template.hpp:940:38:   required from \u2018void boost::function1<R, T1>::assign_to(Functor) [with Functor = void (*)(const darknet_ros_msgs::ImageWithBoundingBoxes_<std::allocator<void> >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&]\u2019\n/usr/include/boost/function/function_template.hpp:728:7:   required from \u2018boost::function1<R, T1>::function1(Functor, typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type) [with Functor = void (*)(const darknet_ros_msgs::ImageWithBoundingBoxes_<std::allocator<void> >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&; typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type = int]\u2019\n/usr/include/boost/function/function_template.hpp:1077:16:   required from \u2018boost::function<R(T0)>::function(Functor, typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type) [with Functor = void (*)(const darknet_ros_msgs::ImageWithBoundingBoxes_<std::allocator<void> >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&; typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type = int]\u2019\n/home/marvin/catkin_ws/src/efr_object_detection_camera/src/template_matching.cpp:55:26:   required from here\n/usr/include/boost/function/function_template.hpp:118:11: error: invalid initialization of reference of type \u2018const darknet_ros_msgs::ImageWithBoundingBoxes_<std::allocator<void> >&\u2019 from expression of type \u2018const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >\u2019\n           BOOST_FUNCTION_RETURN(f(BOOST_FUNCTION_ARGS));\n           ^\n", "void imageCallback(const darknet_ros_msgs::ImageWithBoundingBoxes& msg) {\n    cv_bridge::CvImagePtr cv_ptr;\n    try {\n        cv_ptr = cv_bridge::toCvCopy(msg.im, sensor_msgs::image_encodings::BGR8);\n        Mat image = cv_ptr->image;\n        for(size_t i = 0; i < msg.bounding_boxes.size(); i++)\n        {\n            darknet_ros_msgs::BoundingBox bBox= msg.bounding_boxes[i];\n            double minx = bBox.xmin;\n            double miny = bBox.ymin;\n            double maxx = bBox.xmax;\n            double maxy = bBox.ymax;\n            match(image);\n        }\n\n        cv::waitKey(30);\n    } catch (cv_bridge::Exception& e) {\n        ROS_ERROR(\"Could not convert from '%s' to 'bgr8'.\",\n        msg.im.encoding.c_str());\n    }\n}\n\nint main(int argc, char **argv) {\n    ros::init(argc, argv, \"template_matching_node\");\n    ros::NodeHandle nh;\n    cv::namedWindow(\"view\");\n    cv::startWindowThread();\n    image_transport::ImageTransport it(nh);\n    image_transport::Subscriber sub = it.subscribe(\"/darknet_ros/ImageWithBoundingBoxes\", 1,\n            imageCallback);\n    ros::spin ..."], "answer_code": ["image_transport::Subscriber", "Image", "darknet_ros_msgs::ImageWithBoundingBoxes", "error: invalid initialization of reference of type \u2018darknet_ros_msgs::ImageWithBoundingBoxes&\u2019 from expression of type \u2018boost::shared_ptr<const sensor_msgs::Image>\n", "ros::Subscriber", "image_transport", "sensor_msgs/Image", "message_filters", "image_transport", "image_transport", "image_transport", "image_transport", "Subscriber", "Publisher"], "url": "https://answers.ros.org/question/322153/combines-message-for-image-and-information-gives-invalid-initialization-error/"},
{"title": "ros_control requirements", "time": "2019-04-17 16:18:47 -0600", "post_content": [" ", " ", " ", " ", "Hi", "Required info:\nDistro: Kinetic, Ubuntu 16.04\nThe robot is running on a gaming laptop and brings up ", ", ", " and the ", " and sends data over home wifi.\nI view the data on a remote PC via RVIZ.", "Most of my experience up to this point has been with simulated robots and gazebo. I have gone through the tutorials on making a real robot and have a fully built bot IRL. It teleoperates fine and I can make maps, though my odometry appears to suffer from inaccuracy. I am trying to eliminate issues that might be causing it, such as simply not having configured my robot properly. ", "I have read through the wiki pages on the navigation stack and I would like to better understand at what point ", " becomes necessary in a robot build please. I am referring to this diagram: ", ". ", "My robot has two motors with quadrature encoders being driven by a roboclaw motion control board. it uses the custom ", " which subscribes to cmd_vel and publishes ", " plus some motor status messages. It has its own PID control built in.", "At what stage would the roboclaw_node appear in the attached diagram? I am not sure if I am missing something from my build. As mentioned, at present I am teleoperating the robot in order to make maps. I am not navigating yet. ", "This is the urdf representation of my real robot ", " Now If that were in gazebo, I would put libdiffdrive in the .gazebo file and set the front caster to have zero friction and gazebo would take care of creating robot movement, which is reflected in RVIZ. However now of course, RVIZ only has the drive wheel encoder odometry to inform its position. Is RVIZ really giving me an accurate representation of movement if there is no model to inform it how the three wheels, 2 x drive plus 1 x caster, interact?", "Is ros_control and RobotHW always needed in every build? Or does the roboclaw node effectively take care of everything?"], "answer": [" ", " ", "Some comments (slightly pedantic, but I feel important):", "I would like to better understand at what point ros_control becomes necessary in a robot build", "never. Using ", " is a choice, and robots can (and have) been built without it and function perfectly fine.", "[..] RVIZ only has the drive wheel encoder odometry to inform its position [..] Is RVIZ really giving me an accurate representation of movement [..]", "Please understand: RViz does not do ", " else but visualise data streams. It does not calculate odometry, or the pose of your robot. It's not even giving you \"an accurate representation of movement\". It just renders a 3D model at a certain 6D pose. But all of that information has to come from outside, as RViz is just a consumer of data.", "Thank you, that answers my question.", "Note that reusing ", " can definitely be beneficial (as certainly the controllers it provides can save you a lot of work), but it's never a requirement."], "answer_code": ["ros_control", "ros_control"], "url": "https://answers.ros.org/question/321347/ros_control-requirements/"},
{"title": "SPARC arch: rosout node loads CPU on 100%", "time": "2019-03-12 07:58:59 -0600", "post_content": [" ", " ", " ", " ", "We experienced strange behaviour on our embedded SPARC-like system. Just after start, the ", " process loads CPU on 100%. We would be very appreciated if you guys help us to reveal the reason of such behaviour.", "Also, how many processes should ", " run?", "I've seen that happen when you abuse the parameter server, its meant for static parameters at bringup not to set throughout use (if you do that)", "Probably won't solve your problem, but you can reduce the work done by rosout node in a ", ": disable topics in log lines, disable file logging.", "NB: ROSOUT_DISABLE_FILE_LOGGING is not yet in official melodic release, but there are other ways of disabling rosout file logging.", "I also meet this problem, rosout loads one full cpu kernel"], "answer": [" ", " ", " ", " ", "After some attempts to reveal the source of problem I found a solution. There are no correct realizations of ", " system entity on some VLIW and SPARC-like platforms. This leads any function using ", " to return immediately after calling. In threaded implementation such fun\u0441tions consume all CPU time just for continuous function calls. This issue can be solved with replacement of all SteadyTimer-related calls to equivalents which use ", ". ", " There is brilliant solution on GitHub made by Christopher Wecht:  ", "  His solution makes ROS core true platform-independent on the described case. ", "To integrate Christopher's solution into the existing ROS sources it is only needed to replace ", " directory with variant from ", ". After that compiled ROS works properly on VLIW platforms.", "To integrate Christopher's solution into the existing ROS sources it is only needed to replace ", " directory with variant from ", ". ", "please be aware that you are now responsible for keeping ", " (and all pkgs in it) up-to-date. The fork you link to is (at the time of writing) 8 commits behind ", ".", " It looks like there is a pull request to merge those changes into ros_comm:  ", "  . At this time, it looks like there are still some open discussions that need to be resolved before it can merge. "], "question_code": ["rosout", "roscore"], "answer_code": ["SteadyTimer", "SteadyTimer", "WallTimer", "src/ros_comm/roscpp", "src/ros_comm/roscpp", "ros_comm", "melodic-devel"], "url": "https://answers.ros.org/question/318372/sparc-arch-rosout-node-loads-cpu-on-100/"},
{"title": "IMU + AMCL for localization", "time": "2014-01-09 04:23:42 -0600", "post_content": [" ", " ", "Hi everyone,", "Let me write my question out like this:", "Now I am a bit lost with what I need for localization. I have used AMCL in the past with a kinect and odometry data for localization, but now I want to mainly be using the IMU sensor. I read somewhere that I can use robot_pose_ekf for this, but it forces the need for odometry (visual or mechanical), why is this so?", "I wanted to start with just localization through the IMU sensor and AMCL, but how would I go about doing this?", "I am a bit lost with what I need, so some general overview of possibilities would be appreciated.", "Thanks in advance!"], "answer": [" ", " ", "AMCL uses a map along with sensor data that tells it something about where it might be in the map for localization. IMU data is not useful for providing an localization estimate within the map, whereas laser scans (or depth data) is. ", "robot_pose_ekf requires odometry because IMU data (from cheap consumer IMUs at least) is very noisy, and integrating that over long periods of time will get you very poor results.", "The answer depends on what exactly you are trying to do.", "If you have odometry and IMU data, use robot_pose_ekf. It doesn't do global localization like AMCL, but if you know where the robot started, it will maintain a better estimate of its pose than odometry or IMU alone.", "If you have odometry, a depth sensor, and a map, use AMCL. It will let you perform global localization within the map.", "Thanks for the reply. So a combination of AMCL for global localization and IMU + odometry for local localization, would that make sense? Or is IMU + odometry sufficiently reliable?", "Again, it depends what you want to do. You can certainly use both at the same time, which is what is done on the PR2. Using just robot_pose_ekf, the pose estimate will drift a bit over time, though I don't have a feel for how much.", "Oke thanks for your answer. I am indeed interested in a similar situation as with the pr2, so I know what to do now.", " ", " ", "Checkout this package ", "."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "I have IMU data from an IMU sensor", "I do not always have a xtion available for helping in localization (it could be occupied with another task).", "I could get odometry data from the wheels."], "url": "https://answers.ros.org/question/116406/imu-amcl-for-localization/"},
{"title": "Subscriber - perform callback on 'most current' data received", "time": "2019-05-15 01:45:25 -0600", "post_content": [" ", " ", " ", " ", "Hey everybody,", "I've been messing a bit with a subscriber node and i'm getting stuck.", "Little bit of background:\nI have a topic where a camera publishes images with a frequency of 10 hz (ish).\nI have another node subscribed to this topic. There the image is processed using open CV.\nThe processing takes about 1 second. This means for every 10 images, 1 image is processed.", "Now when i look at the images received by the subscriber i see that the images received are delayed, meaning that the next callback image 2 is processed, which was made about 0.9 second ago. The 3rd callback, image 3 is processed, leading to a delay of 1.8 s.  etc...\nI think this has something to do with the queue size.\nI tried messing around with the basic chatter nodes from the tutorial to understand the problem a bit better and i made a simplified version of the problem:", "publisher", "subscriber:", "Now, when i run both nodes, the publisher publisher at 1 message / s\nThe subscriber receives the first message, waits 2 seconds, then receives the second message, etc.\nWhen i stop the publisher, the subscriber keeps receiving messages(probably since they are stored in a buffer)", "Then set the queue size to 1, and ran the nodes again.\nThis gave me the same result.", "I want the subscriber to do something with the most up to date message, so deleting or not caring about all the messages received before.\nThe reason for this is because i'm using this for a vision application, so i need the processing to process the latest image.", "So, my question:", "Kind regards,\nJan Tromp.", "Is there a way to tell my subscriber (or publisher) to delete all stored messages or to start callback on the latest received image?", "I'm sure you've already looked for solutions to your problem before posting here.", "Could you add what you've tried and what didn't work?", "It might also help if you could link to Q&As here on ROS Answers that you've found, so that we don't suggest the same things.", "I did look around for options to solve this problem, but apperantly i was searching in the wrong direction.\nI found a solution and explained it in my answer."], "answer": [" ", " ", " ", " ", "I don't want the node to process images when the robot is moving. so this parameter is there so i can tell the node \"now i want you to actually process an image\"", "It would appear that you have the following situation:", "The question you posted seems to be an example of an ", ": you're asking about a detail of a solution you've picked, instead of about the actual problem. The answers you receive are then focused on the implementation detail, instead of the bigger picture.", "I would propose to implement what you describe in the following way:", "Such a setup would seem to cover all your requirements: it only processes images when requested and it only works on the \"latest data\".", "In addition it doesn't require any bookkeeping in your node, no disabling of callbacks, no ", " tweaking, does not poll the parameter server at a high frequency and uses appropriate communication patterns for the interaction between service consumer and provider.", "There does not seem to be a need to store ", "s at all in the service provider, nor is there a need to be notified of each and every message that gets published to the camera's topic. If the camera driver was not publishing continuously, this would be a different matter, but as it stands now I believe sampling a single message from the continuous stream whenever it is needed makes the most sense.", "PS: I would recommend to use an action server here as they essentially allow you to invoke them as asynchronous services (ie: non-blocking to your caller). It's slightly more complicated to implement, but the advantages outweigh that easily.", " ", " ", " ", " ", "Check the rospy subscriber ", ".\nThe queue size might default to infinite so you can try adding queue_size=1:", "Actually, I believe to achieve the behaviour the OP asked for, the ", " should be set to ", " instead. I'm basing this on ", ", where it apparently worked for the OP. The docs state:", "queue_size (int) - maximum number of messages to receive at a time. This will generally be 1 or None (infinite, default).", "So ", " may not be the correct choice.", "The camera i'm working with is a pickit 3d camera. it is a \"ROS integrated camera\", meaning the camera has a ROS server running on its controller. The controller publishes the camera images (rgb and pointcloud, for this question i'm only interested in the rgb). \nBecause the publisher node is in the controller, i am not able to change its code, hence i'm not able to change the queue size.", "I think if this was possible this would fix the problem though.", "Thanks for your reply!", "The ", " should be set on the subscriber. Not the publisher.", "Oh really?\nBut how does that work in the python code then?\nIf you look at my publisher and subscriber node the publisher is the one where queue_size is set, the subscriber doesn't have this at all.", "Am i missing something?", "Here's the documentation: ", ". Note the ", " argument.", "But again: this may not actually work. ", " is unclear.", "Okay i see, didn't know this was possible.\nThough, this is a good solution for most of the people having this problem, in my case i do think i need the parameter (see my own answer).\nThe reason for this is because i have the camera on a robot arm. The parameter \"do callback_test\" will decide whether the callback function is actually executed.\nThis is necessary since I don't want the node to process images when the robot is moving. so this parameter is there so i can tell the node \"now i want you to actually process an image\"", "In ", " one of the answers is using an ", ". I want to tell beforehand, the image is published on a topic so I (think I) have to use a subscriber.", "The reason for this is because i have the camera on a robot arm. The parameter \"do callback_test\" will decide whether the callback function is actually executed. This is necessary since I don't want the node to process images when the robot is moving. so this parameter is there so i can tell the node \"now i want you to actually process an image\"", "This starts to sound like an ", ".", "In any case: it's perfectly possible to receive a single message from a topic. See ", " for C++ and ", " for the Python version of that.", "so i can tell the node \"now i want you to actually process an image\"", "if you want to be able to coordinate computation like that, I'd indeed recommend to use something other than topics, such as an action.", "\nIn this case you can have a process_image() method that checks whether you need to process the image in the callback rather than messing with the callback execution. You can do smth like this:", " ", " ", " ", " ", "Funny, I did look around a lot but i didn't really know what to look for.\nAfter typing this question, i was about to continue to another part of the project as i came across \n", "\nIt suggested to 'dump' the stored images by letting the callback \" temporarily do nothing\".\nSo i implemented this to my code and it did solve my problem.\nThe updated subscriber and publisher are below.", "subscriber:", "The output of the subscriber gave the following:", "After the 3rd print i set the parameter \"/do_callback\" to false, which made the subscriber just \"pass\" the callback.\nThen, when i made the parameter high again after 24 seconds, the subscriber printed out the information again, using the newest received data!", "This is a work-around at best and I would not recommend you'd implement it this way. It is not needed.", "Additionally: invoking ", " in msg callbacks is not recommended, as it incurs quite a bit of overhead to retrieve the parameter value on each callback (essentially a full XMLRPC call).", "This is a work-around at best and I would not recommend you'd implement it this way. It is not needed.", "What do you mean with it's not needed?", "quite a bit of overhead to retrieve the parameter value on each callback (essentially a full XMLRPC call).", "What do you mean with this?", "I'm sorry i'm not that familiar with the ROS vocabulary and english is not my main language :p"], "answer_details": ["camera (driver) that continuously publishes ", " messages to a topic", "physical camera is mounted on the robot EEF", "image processing should happen only when requested", "write a service or action server and expose a ", " service or action (I would suggest to use a more descriptive name, this is of course just an example)", "in the service/action callback, use ", " (", ") to retrieve a single ", " from the camera stream", "process the image according to whatever parameters have  been passed as part of the service request or the action goal", "return the result of the processing to the caller", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["#!/usr/bin/env python\n# license removed for brevity\nimport rospy\nfrom std_msgs.msg import String\n\ndef talker():\n    pub = rospy.Publisher('chatter', String, queue_size=10)\n    rospy.init_node('talker', anonymous=True)\n    rate = rospy.Rate(1) # 10hz\n    while not rospy.is_shutdown():\n        hello_str = \"hello world %s\" % rospy.get_time()\n        rospy.loginfo(hello_str)\n        pub.publish(hello_str)\n        rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        talker()\n    except rospy.ROSInterruptException:\n        pass\n", "#!/usr/bin/env python\nimport rospy\nfrom std_msgs.msg import String\n\ndef callback(data):\n    rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n    rospy.sleep(2)\n\ndef listener():\n    rospy.init_node('listener', anonymous=True)\n\n    rospy.Subscriber(\"chatter\", String, callback)\n    print(\"done with callback\")\n    # spin() simply keeps python from exiting until this node is stopped\n    rospy.spin()\n\nif __name__ == '__main__':\n    listener()\n"], "answer_code": ["sensor_msgs/Image", "ProcessImage", "rospy.wait_for_message(..)", "Image", "queue_size", "Image", "rospy.Subscriber(\"chatter\", String, callback, queue_size=1)\n", "queue_size", "None", "None", "queue_size", "pub = rospy.Publisher('chatter', String, queue_size=10)\n", "queue_size", "The parameter \"do callback_test\" will decide whether the callback function is actually executed. This is necessary since I don't want the node to process images when the robot is moving. so this parameter is there so i can tell the node \"now i want you to actually process an image\"\n", "def callback(data):\n    if not process_image()\n       return\n    image processing logic...\n", "#!/usr/bin/env python\nimport rospy\nfrom std_msgs.msg import String\n\ndef callback(data):\n    if rospy.get_param(\"/do_callback_test\") == True:\n        rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data)\n        rospy.sleep(2)\n    else:\n        pass\n\ndef listener():\n    rospy.init_node('listener', anonymous=True)\n\n    rospy.Subscriber(\"chatter\", String, callback)\n    print(\"done with callback\")\n    # spin() simply keeps python from exiting until this node is stopped\n    rospy.spin()\n\nif __name__ == '__main__':\n    listener()\n", "[INFO] [1557910478.891510]: /listener_18047_1557910462361I heard hello world 1557910472.84\n[INFO] [1557910480.897341]: /listener_18047_1557910462361I heard hello world 1557910473.84\n[INFO] [1557910482.902606]: /listener_18047_1557910462361I heard hello world 1557910474.84\n[INFO] [1557910498.843681]: /listener_18047_1557910462361I heard hello world 1557910498.84\n[INFO] [1557910500.852232]: /listener_18047_1557910462361I heard hello world 1557910499.84\n", "rospy.get_param(..)"], "url": "https://answers.ros.org/question/323187/subscriber-perform-callback-on-most-current-data-received/"},
{"title": "/tf_static wait for all static transforms?", "time": "2019-05-06 02:52:49 -0600", "post_content": [" ", " ", "Hi everyone,", "is there any way to determine whether all so far available static transforms have been captured by a subscriber? ", " seems to be a latched topic. So I'll assume there's some way to check if there are latched messages available, is that correct?", "Cheers,\nHendrik", "Latched publishers should send \"all\" messages upon connection of a (late) subscriber. Immediately.", "I don't believe there is a way to check how many \"outstanding messages\" there are, but I could be wrong.", "(it's also conceptually difficult I believe: msgs can be published at any time, even with latched publishers, so when would you know that you've received \"all of them\"?)", "I just need a way to detect whether I've gotten all statically defined tf frames. One alternative solution might be to have them configured in a central place and rely on that configuration everywhere I can instead of rostopics...", "I just need a way to detect whether I've gotten all statically defined tf frames", "at a particular point in time then and for a given a priori known set, you mean? Conceptually \"whether I've gotten all statically defined tf frames\" seems impossible, in general. TF is a distributed system. That makes it difficult (but not impossible) to achieve a guaranteed, deterministic and coherent view of a shared state.", "One alternative solution might be to have them configured in a central place and rely on that configuration everywhere I can instead of rostopics...", "this sounds like you have another issue, which prompted you to ask this question.", "Probably have, yes. The problem is that I have a project that interacts with ROS but does not really rely on ROS. It has its own representation of coordinate systems that I'm trying to synchronize with ROS (TF) the moment it starts. The start-up procedure includes synchronizing the TF frames with the internal representation. The reason I do that is to be able to use the project without ROS/TF in completely different setups and completely different data sources (such as ROS topics).", "And that is not working for you? I expect it doesn't, otherwise you wouldn't be posting here, but your last comment does not seem to contain any description of a problem.", "It kind of works by ignoring errors that occur as long as some ", " frames are not processed yet. It's kind of a race condition, where my script is trying to work with frames that my synchronization callback hasn't received yet. The errors of course vanish after all expected frames have been processed from ", ". It's just that I would like to streamline this so that these errors really ", " occur if a frame is ", " missing for whatever reason (I don't necessarily configure the frames all by myself) and not just because they are ", ". I don't want to rely on time to ", " the corresponding frames ", ". I would like to be sure. The static frames are usually known in a certain environment but I, as the developer, don't have to know the setup, however, the script should. That's ..."], "answer": [" ", " ", " ", " ", "It's kind of a race condition, where my script is trying to work with frames that my synchronization callback hasn't received yet.", "Afaik there is no difference between static or regular transforms in this case: in all cases consumers should check whether transforms are available before trying to use them. TF2 provides infrastructure for that: ", " fi, TF1 had ", ".", "Even if it were just a regular ROS application (ie: not an integration) consumer code should check before use.", "I don't want to rely on time to consider the corresponding frames missing. I would like to be sure", "and you could, if you'd use the appropriate functionality for that (ie: ", " and/or ", ").", "Ah, some ", " or ", " is a good idea, actually. I haven't thought about that yet... I'll just have to implement it for my own transform manager... shouldn't be that big of a deal. ", "Thanks!"], "question_code": ["/tf_static", "tf_static", "tf_static"], "answer_code": ["waitForTransform(..)", "canTransform(..)", "waitForTransform(..)", "waitForTransform", "canTransform"], "url": "https://answers.ros.org/question/322495/tf_static-wait-for-all-static-transforms/"},
{"title": "Service buffer overrun only when service is persistent", "time": "2018-01-09 09:13:28 -0600", "post_content": [" ", " ", " ", " ", "I'm using a rospy service call to a custom c++ gazebo library. Ever time this service is called, gazebo takes a certain number of simulation steps, and responds with the new sensor measurements. I'm running many trials, so I'd like to run this as quickly as possible, so I started using ", ".", "and my cpp service server is:", "When the step_service in the python code is persistent, I get the errors:", "This is no matter the size of my response (I commented out most of the response, still errors) or the size of my request. It also happens during the ", " call.", "Everything works perfectly when things are not persistent. However, since I'm calling this as fast as possible, I'd like to remove the wasted computation for resetting the TCP connection every time.", "How fast are you calling the service? Does the node have time to wait for a Service result and does it actually ", "? Or are you, like, calling the Service in a single run of your script a hundred times without any ", "ing?", "I do not call spin! This makes sense now. I'm surprised it worked without the persistent call.", "Also: Doesn't a service call block? So it would always wait for a response (I may have misunderstood you)", "Since rospy doesn't have a spinOnce function, how would you recommend I tackle this?", "Well, you are right, service calls are blocking. Also, I just found the following on the ", "Advanced users: you don't need to create a ROS node in order to make service calls with a rospy.ServiceProxy. ", "So the spin in the calling node is not ...", "required. But in the service server, you still need it, IMO, but there ", " is ok.", "It is strange though, that it seems to work without the persistent connection. But this could be a side-effect of re-creating the connection, and, in that course, delivering the message. But this is just a guess..."], "answer": [" ", " ", "In my case buffer overrun only occurs, when using Services with python3! So switching to python 2.7 would help (if possible).", "Did you test this? If so, I'll accept it as an answer. I just turned off persistent, which is not ideal but worked.", "Yes I did. Problem is, that persistent service calls are quite time consuming."], "question_code": ["rospy.wait_for_service('step_service')\nself.step_service = rospy.ServiceProxy('step_service', ActionService, True)\naction = np.zeros((12,1))\nself.state = self.step_service(action)\n", "step_service = rosnode->advertiseService(\"/step_service\", &GazeboRosStepper::step_srv, this);\n\nbool GazeboRosStepper::step_srv(gazebo_ros_stepper::ActionService::Request  &req, gazebo_ros_stepper::ActionService::Response &res)\n{\n    step\n    fill response with joint angles, velocities (24 doubles)\n    return true\n}\n", "rospy.service.ServiceException: service [/step_service] responded with an error: b'Buffer Overrun'\n", "spin", "spin", "spin"], "url": "https://answers.ros.org/question/279247/service-buffer-overrun-only-when-service-is-persistent/"},
{"title": "Costmap2DROS transform timeout", "time": "2015-08-05 06:51:07 -0600", "post_content": [" ", " ", " ", " ", "I am trying to run amcl, move_base and map_server on my robot. Everything starts fine but after I get \"Odom received\" warning messages start to show up:", "Costmap2DROS transform timeout.\n  Current time: 1438775060.1538,\n  global_pose stamp: 1438775056.1344,\n  tolerance: 0.3000", "and", "[ WARN] [1438775074.022202129]:\n  MessageFilter [target=map\n  base_laser_link ]: Dropped 100.00% of\n  messages so far. Please turn the\n  [ros.costmap_2d.message_notifier]\n  rosconsole logger to DEBUG for more\n  information.", "They come from the Costmap2D plugin. I runned tf_monitor and I got:", "What could be the problem causing the timeout?", "When viewing the frames, I see that robot_state_publisher publishes with 50 Hz but somehow all published links have a delay of -0.5 seconds, which cannot be possible!", "I kept looking at it in real time with tf_monitor and the time of the most recent transform from robot_state_publisher is -0.5 for some time, then it jumps to +0.5. This is very confusing.", "I kept tf_monitor running for few minutes and it seems like the delays are increasing:", "Did you solve it?how to solve this(Costmap2DROS transform timeout. Current time: 1438775060.1538, global_pose stamp: 1438775056.1344, tolerance: 0.3000).\nthank you"], "answer": [" ", " ", "You can change the tolerance of the tf listener when you declare it, for example for a tolerance of 10 seconds just do         self.tf_listener = tf.TransformListener(10) but this is just for debugging and is not a good idea for long term use.", " ", " ", "There is a related question (duplicate?) ", ", will post same answer in both.", "In our case it was a performance issue, the computer was not strong enough to manage the load. I changed rviz configuration to prevent pointcloud visualisation, laserscanner vis, and furthermore I was having  a process consuming 100% CPU (a node which was not sleeping while spinning). After those fixes, now everything is back to normal.", " ", " ", " ref :  "], "question_code": ["RESULTS: for all Frames\n\nFrames:\nFrame: /base_footprint published by unknown_publisher Average Delay: -0.0456822 Max Delay: 0.10752\nFrame: base_front_laser_link published by unknown_publisher Average Delay: -0.545968 Max Delay: 0\nFrame: base_link published by unknown_publisher Average Delay: -0.545971 Max Delay: 0\nFrame: base_rear_laser_link published by unknown_publisher Average Delay: -0.545966 Max Delay: 0\nFrame: odom published by unknown_publisher Average Delay: -0.108352 Max Delay: 0.0916446\n\nAll Broadcasters:\nNode: unknown_publisher 82.2074 Hz, Average Delay: -0.360456 Max Delay: 0.10752\n", "RESULTS: for all Frames\n\nFrames:\nFrame: /base_footprint published by unknown_publisher Average Delay: 2.57823 Max Delay: 17.9659\nFrame: base_front_laser_link published by unknown_publisher Average Delay: -0.408636 Max Delay: 0.626485\nFrame: base_link published by unknown_publisher Average Delay: -0.40864 Max Delay: 0.626482\nFrame: base_rear_laser_link published by unknown_publisher Average Delay: -0.408634 Max Delay: 0.626486\nFrame: odom published by unknown_publisher Average Delay: 3.65717 Max Delay: 16.6323\n\nAll Broadcasters:\nNode: unknown_publisher 82.0346 Hz, Average Delay: -0.124652 Max Delay: 1.20379\n"], "url": "https://answers.ros.org/question/215256/costmap2dros-transform-timeout/"},
{"title": "Question on indoor navigation and face tracking - RPLidar or RGB-D camera or both", "time": "2019-06-18 07:41:02 -0600", "post_content": [" ", " ", "My requirement is to do indoor navigation/obstacle avoidance and also face detection and tracking using a ground robot (not UAV). I know that I can use RPLidar for creating occupancy grid maps but for face detection, Iidars will not work and I will need to use an RGB-D camera for detection and tracking.\nNow, I do not want to buy both a Lidar (for SLAM and navigation) and a depth sensing camera (for face detection and tracking) since it will be expensive to buy both.\nMy question is - can I create 2D maps using an RGB-D camera and will it be fine to use it for indoor purposes in natural light and darkness also (for face detection). Or a Lidar will outperform an RGB-D camera when it comes to navigation."], "answer": [" ", " ", "If you want to choose a single sensor, using an RGBD camera is the way to go. \nYou can check turtlebot2 family, for example, that uses this config.", "The RGBD will allow for 3D perception, but you'll need more CPU consuming algorithms to map and localize with that sensor.\nA LIDAR in the other hand is usually cheaper, has a greater range, precision and framerate, but you'll only see a \"slice\" of the world. ", "It boils down to which kind of application you have in mind.", " ", " ", "you could use the depth image to laserscan node ", " to generate lidar data for 2d mapping"], "url": "https://answers.ros.org/question/326113/question-on-indoor-navigation-and-face-tracking-rplidar-or-rgb-d-camera-or-both/"},
{"title": "[ndt_matching] gpu version died", "time": "2019-07-27 09:00:55 -0600", "post_content": [" ", " ", "Log file: \nmethod_type: 2\nuse_gnss: 1\nqueue_size: 1\noffset: linear\nget_height: 0\nuse_local_transform: 0\nuse_odom: 0\nuse_imu: 0\nimu_upside_down: 0\nimu_topic: /imu_raw\nlocalizer: velodyne\n(tf_x,tf_y,tf_z,tf_roll,tf_pitch,tf_yaw): (1.2, 0, 2, 0, 0, 0)\nUpdate points_map."], "answer": [" ", " ", "Thank you for reporting the bug.", " is an algorithm that consumes a lot of memory.\nIf the map is large or the resolution of the NDT is small, the memory will be insufficient and the process will die.\nA similar bug has been identified for the CPU implementation ", ".", "Since ", ",", " and ", " have known bugs, I recommend using", ".", "If you like, please report the bug to autoware gitlab.\n", "thanks for your reply, and I have reported this bug to gitlab.", "@yamato-ando Are there Issues on the Autoware.ai Gitlab site for these known bugs? I don't see them anywhere.", " I'm sorry. Later I will report the bugs I know."], "question_code": ["the ndt_matching node with pcl_generic method works fine,\nbut pcl_anh_gpu method always crashed when started\n", "gpu: nvidia gtx980m\ngpu driver version: 410\ncuda version: 10.0\ncudnn version: 7.6.1.34\nros: melodic\nautoware: 1.12.0 and master\n", "1. in simulation tab, select bag file (demo data provided by autoware), press start and pause\n2. in setup tab, press tf and vehicle model\n3. in map tab\n    select and press point cloud (demo data provided by autoware)\n    select and press vector map (demo data provided by autoware)\n    select and press tf (demo data provided by autoware)\n4. in simulation tab, press pause again to resume, \n    in rviz, verify that the pcd map and vector map are loaded correctly\n5. in sensing tab, click voxel_grid_filter with default parameters\n6. in computing tab, click nmea2tfpose with default parameters\n    select pcl_anh_gpu method and click ndt_matching\n7. in simulation tab, press pause to resume\n8. then ndt_matching died with error message as below\n", "Error: out of memory /home/leon/autoware.ai/src/autoware/core_perception/ndt_gpu/src/VoxelGrid.cu 181 terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >' what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument [ndt_matching-1] process has died [pid 8436, exit code -6, cmd /home/leon/autoware.ai/install/lidar_localizer/lib/lidar_localizer/ndt_matching __name:=ndt_matching __log:=/home/leon/.ros/log/a41e9918-b072-11e9-96f6-9cb6d01138d9/ndt_matching-1.log]. log file: /home/leon/.ros/log/a41e9918-b072-11e9-96f6-9cb6d01138d9/ndt_matching-1*.log\n"], "answer_code": ["pcl_anh_gpu", "pcl_anh", "pcl_anh_gpu", "pcl_anh", "pcl_openmp", "pcl_generic"], "url": "https://answers.ros.org/question/329433/ndt_matching-gpu-version-died/"},
{"title": "ament_cmake: Confused about ament_export_dependencies and ament_export_interfaces", "time": "2019-08-21 14:26:21 -0600", "post_content": [" ", " ", "I'm programming on ROS2 Dashing and trying to port some packages, but the ", " haven't answered my questions", "What does ", " actually do?\nWhy is it needed if I already declared my dependencies with ", " or ", "?", "Finally, why do I hardly see ", " in the ROS2 codebase if that's the recommended way to expose library projects for downstream consumption?"], "answer": [" ", " ", "In your example you have ", ", but that's incorrect. ", " takes only package names.", "In short,  ", " sets up stuff for your package, ", " and ", " set up stuff for packages that depend on your package.", "When a package calls ", ", CMake looks for a file called ", " or ", ". ", " calls this a ProjectConfig.cmake file. The ProjectConfig.cmake sets up everything another project would need to depend on this one. It ", " like ", ", creates ", " like ", ", and finds packages ", " depends on.", "First your package needs to use ", " when it's being built. ", " is a helper function that makes sure your executable or library has everything it needs to use ", ". It will call ", ", ", ", etc. ", "Once your package is built/installed, other packages might need ", " too. For example, if your package includes ", " in a header it installs, then packages that include your header will need to know where ", "'s headers are too. This is where ", " comes in . Calling ", " will make your package's ProjectConfig.cmake contain a ", " call plus logic to combine CMake variables from ", " into your package's CMake variables. For example, it will make ", " contain everything in ", ". The purpose is so packages don't have to find your dependencies. Use ", " on all dependencies of yours that downstream packages also need.", " is a little different. This creates imported targets, which is a replacement for CMake standard variables in a ProjectConfig.cmake. It will likely be used more in the future, but for now ", " with it that needs to be fixed. It won't replace ", ", but it will change how information like include directories and libraries are communicated between packages.", "Thank you! So what about the PUBLIC/PRIVATE/INTERFACE specifiers on ", " and ", "? What effect (if any) do these have on consumers of my package/library?", "My understanding of those is lacking. On consumers of your package? I think the keywords only have an effect on consumers if your package export targets. They do have an effect within targets of your package, but the CMake documentation for ", " and ", " probably explains better than I can.", "I submitted an issue, as this seems like a bug that it affects people who link to my library via ament_target_dependencies but not via target_link_libraries. "], "question_code": ["ament_export_dependencies(my_target ...)", "ament_target_dependencies(my_target ...)", "target_link_libraries(my_target PUBLIC ...)", "ament_export_interfaces(${PROJECT_NAME} HAS_LIBRARY_TARGET)"], "answer_code": ["ament_export_dependencies(my_target)", "ament_export_dependencies()", "ament_target_dependencies()", "ament_export_dependencies()", "ament_export_interfaces()", "find_package(MyDependency)", "mydependency-config.cmake", "MyDependency-config.cmake", "MyDependency_INCLUDE_DIRS", "MyDependency::somelibrary", "MyDependency", "MyDependency", "ament_target_dependencies(my_target MyDependency)", "MyDependency", "target_link_libraries(my_target ${MyDependency_LIBRARIES})", "target_include_directories(my_target PUBLIC ${MyDependency_INCLUDE_DIRS})", "MyDependency", "<my_depdenency/something.hpp>", "MyDependency", "ament_export_dependencies()", "ament_export_dependencies(MyDependency)", "find_package(MyDependency)", "MyDependency", "YourPackage_INCLUDE_DIRS", "MyDependency_INCLUDE_DIRS", "ament_export_dependencies()", "ament_export_interfaces()", "ament_export_dependencies()", "target_link_libraries", "target_include_directories", "target_include_directories()", "https://cmake.org/cmake/help/v3.5/command/target_link_libraries.html"], "url": "https://answers.ros.org/question/331277/ament_cmake-confused-about-ament_export_dependencies-and-ament_export_interfaces/"},
{"title": "How can you install a package using catkin build?", "time": "2019-08-21 14:24:26 -0600", "post_content": [" ", " ", "I'm trying to use a script from the package bag_tools (", ") ", "First instruction is to :", "However I had made my workspace using ", ".  (I'm not aware of the details of the difference between catkin_make and catkin build).\nHow do I install bag_tools using catkin build?", "TL;DR: What's the catkin build equivalent of \"catkin_make install --pkg bag_tools\""], "answer": [" ", " ", "Any time I need to find out the equivalent ", " command for a ", " command (or vice versa), I consult ", ".", "In this case, it tells me the command you want is ", " followed by ", ".", "You can check if installation has been set by just running ", " and viewing the output."], "question_code": ["catkin_make install --pkg bag_tools\n"], "answer_code": ["catkin_tools", "catkin_make", "catkin config --install", "catkin build", "catkin config"], "url": "https://answers.ros.org/question/331276/how-can-you-install-a-package-using-catkin-build/"},
{"title": "How to specify dependencies with \"foo_msgs\" catkin packages", "time": "2013-01-18 05:14:23 -0600", "post_content": [" ", " ", " ", " ", "Say I have two catkin packages, ", " and ", ", in my workspace.  Here's how I currently have them set up:", "I find that if I ", ", the message isn't generated.  Indeed, ", " is a no-op.  ", " works, however.  In order to get ", " to build correctly, I must add the following line to its ", ":", "Is this by design?  I'd expect that building the package ", " would automatically generate all its messages.  Is there a way to make that happen?", " I've approved WilliamWoodall's answer, although KruseT's was just as useful.  (I also added the ", " line to ", "'s ", ", which I initially forgot.)", "It turns out my solution is correct; the ", " auto-target should be added as a dependency of the ", " target.  Note that there is some disagreement about whether a different solution should be supported by catkin; KruseT started a discussion on the topic ", ".", "Since this type of explicit dependency auto-target (", " and ", ") is necessary for using ROS messages/actions/services in any executable, library, or script, I think it should be better documented (I found no reference to it in ", ").  KruseT opened a related rosdistro issue ", "."], "answer": [" ", " ", " ", " ", "Your projects are setup correctly (mostly), you just need to run ", " with no arguments.", "First update ", ":", "Using ", " is by design or necessity, however you look at it, because we cannot know or assume that ", "'s targets (executables or libraries) use and therefore depend on the messages which are getting generated by ", ".", "Then just execute ", " with no arguments.", "If you want to build ", " explicitly (not the whole workspace) then as of pull request ", " you can do ", ".", "Calling ", " is not sufficient because that is instructing ", " to invoke the ", " make target which does not exist. tkruse's solution simply adds a ", " target which depends on the ", " target, allowing it to be callable and causing the ", " target to be generated. This is not something we do by default because packages often define targets with the same name as the project which would immediately cause a conflict.", "The only reliable way to build an entire package (including all of its targets) is to go to the package's build space and invoke ", ", which is what ", " does.", "I setup an example repository here:", "Hi WilliamWoodall, this is very helpful!  Clearly the _gencpp dependency needs to go somewhere explicitly.  You list it in foo's CMakeLists file.  In that case, if foo_msgs is already installed (so catkin is only building foo), will the foo_msgs_gencpp dependency be correctly resolved by catkin?", "Yes, CMake will ignore targets which are not defined, you could add ", " and it will build with no warnings.", "Great!  This is my favorite solution, since it doesn't introduce new targets (foo_msgs), and explicitly encodes the dependency between the foo (binary) target and the generated cpp messages.", "Also, I want to stress about catkin_make arguments: (a) sometimes it is useful to build only particular targets (yes, targets, not packages), and (b) running catkin_make with no arguments doesn't help here; the _gencpp target is still required to ensure targets are built in the correct order.", "(a) building specific target is already supported by \"catkin_make\", any argument without a special meaning is passed straight forward to \"make\", see \"catkin_make --help\" for details.", "Hi Dirk!  I understand this, I just wanted to correct the answer.  I believe that WilliamWoodall's assertion that \"running catkin_make with no arguments\" would somehow fix my problem is incorrect.", "It looks like the version of CMake on Ubuntu Xenial and newer now complains of non-existent targets. Switching to ", " seems to make it more happy.", " ", " ", " ", " ", "I believe the modern way of doing this is to add a dependency on ", ", as specified on ", ". It should look something like this:", "You should check that `${catkin_EXPORTED_TARGETS}` is set to something before passing it to `add_dependencies(...)`.", "https://gist.github.com/wjwwood-snippets/5979727", "Doesn't the fix for this issue make this check redundant? ", "Ah yes, I forgot we added that.", "The documentation link above is broken. The page can be found ", ".", " ", " ", " ", " ", "catkin_make with an argument just passes that argument to make. That a make target for the package exists is more by chance than design, I'd guess it is added by cmake for each subfolder (but not with nested folders). Maybe you can open a ticket on github to add the feature of a dedicated target to make. However a small problem exists, frequently a package named foo defines an executable named foo, so the target names overlap each other. Not sure whether any clean solution is possible. At least it wont be straightforward.", "What you can do to cause \"make foo_msgs\" not be a noop is in package foo_msg, add a dependency like this:", "which is equivalent to", "but easier for copy&paste", "[Update3: this is wrong:", "Update: add_dependencies is definitely the wrong way to go, the way you use it (in foo, add dependency to foomsg). add_dependency should never cross catkin project boundaries. For you it only works coincidentally (because catkin cheats cmake conventions), it will break build in other cases or when building the projects in isolation.]", "Update3: So it seems that indeed currently, calling add_dependencies accross package boundaries it currently the only recommendable way to achieve that the headers generated by foo_msg exist before they are being consumed by a target in foo.", "We'll discuss this in the buildsystem SIG and maybe there will be a cleaner solution in the future.Discussion here: ", "Ok, so the add_dependencies(foo foo_msgs_gencpp) solution is the right way to go?", "Apologies kruset, I'm still confused.  What exactly should I add to my CMakeLists file(s) for this example?  I haven't found anything but the _gencpp dependency (across catkin project boundaries) to work.", "There is no reason to call ", " with arguments... You are not telling catkin_make what packages you want to build you are just passing targets to make, and the project target and executable/library targets can overlap.  It is better to just invoke ", " with no arguments.", "I partially agree and partiall disagree, WilliamWoodall.  Yes, clearly the user of catkin should be aware that catkin_make targets correspond directly with executable/library targets.  I've found it very useful to only make/remake particular targets during development for compilation time purposes.", "kruset, thanks for the clarification; this appears to be the best solution I've found.  Perhaps it should go in a tutorial?  Setting up this type of package structure (foo and foo_msgs) seems to be a very standard practice, and that I had such a hard time getting it to work properly is unfortunate.", "The reason ", " is a no-op is that there is no make target for a package by default. In stead there is a Makefile for each time ", " is called in CMake.  The correct thing to do in order to build a package is to go to its location in the build folder and invoke make.", "In my example, if I run: ", " all of the messages are generated. This is basically what ", " will do: ", "Regarding documentation, I opened ", "."], "question_code": ["foo", "foo_msgs", "catkin_make foo", "catkin_make foo_msgs", "catkin_make foo_msgs_gencpp", "foo", "CMakeLists.txt", "foo_msgs", "include_directories()", "foo", "CMakeLists.txt", "foo_msgs_gencpp", "foo", "_gencpp", "_genpy"], "answer_code": ["catkin_make", "foo", "cmake_minimum_required(VERSION 2.8.3)\nproject(foo)\n\nfind_package(catkin REQUIRED COMPONENTS foo_msgs)\n\ncatkin_package()\n\ninclude_directories(include ${catkin_INCLUDE_DIRS})\n\nadd_executable(foo_node src/foo_node.cpp)\nadd_dependencies(foo_node foo_msgs_generate_messages_cpp)\n", "add_dependencies(...)", "foo", "foo_msgs", "catkin_make", "foo_msgs", "catkin_make --pkg foo_msgs", "catkin_make foo_msgs", "catkin_make", "foo_msgs", "foo_msgs", "foo_msgs_generate_messages_cpp", "foo_msgs_generate_messages_cpp", "make [all]", "catkin_make --pkg", "add_dependencies(foo_node bar_does_not_exist)", "${catkin_EXPORTED_TARGETS}", "${catkin_EXPORTED_TARGETS}", "add_custom_target(${PROJECT_NAME} DEPENDS ${PROJECT_NAME}_gencpp)\n", "# add_custom_target(foo_msgs DEPENDS foo_msgs_gencpp)\n", "catkin_make", "catkin_make", "caktin_make foo_msgs", "project(...)", "cd build/catkin_demos/foo_msgs &amp;&amp; make", "catkin_make --pkg foo_msgs"], "url": "https://answers.ros.org/question/52744/how-to-specify-dependencies-with-foo_msgs-catkin-packages/"},
{"title": "Have you typed 'make' in [lex_common_msgs]?", "time": "2019-09-29 00:00:10 -0600", "post_content": [" ", " ", "ROS Lunar on 64 bit Ubuntu 16 LTS.", "After many failures to install the following node (", ") from source I installed using Apt-get. Install from source requires colcon, AWS SDK, and other dependencies that are generating issues. I have put many hours over several days into this.", "Installed the Kinetic apt binary as no Lunar version available and moved the files to the ", " folder.", "The node starts, logs into AWS, and provides the expected services", "When I use the service to set logger level it works.  When I attempt to use the service for conversation I get the following:", "I have referenced other threads online regarding \"Have you typed 'make' in...? \"and most responses center around sourcing the bash file which I have done.", "The AudioTextConversation.srv file is located in the lex_common_msgs folder. ", "I just don't know what to do with the 'make' question. There are no cpp or py files in the folder. The lex_common_msgs folder has a subfolder with 4 cmake files in it.", "So I'm stuck with the questions: ", "I have a lot of time and effort already into the machine on Lunar. While I have considered changing to Kinetic to get this to work, I'm not confident that doing so would prevent this particular issue and it would take a week or more to get everything else running again.", "Any help is truly appreciated."], "answer": [" ", " ", " ", " ", "It's almost impossible to comment on this without sounding pedantic, but there are some things here worth nothing:", "and you probably had valid reasons, and this is not directed at you, more for future readers of this Q&A, but:", "Install from source requires colcon, AWS SDK, and other dependencies that are generating issues. I have put many hours over several days into this.", "is exactly why building packages from source is generally discouraged.", "Install from source requires colcon, [..]", "The ", " may explain how to use colcon to build the packages, but I doubt it's actually ", ". I don't see anything in those packages that would make them require colcon as a build tool. I would expect ", " and ", " to work just as well.", "Having written all of that:", "This seems to imply that either:", "Especially this:", "The AudioTextConversation.srv file is located in the lex_common_msgs folder. ", "makes me think it's something related to bullet #4.", "This is all fixable I believe, but I'd first like to know your rationale for wanting to use Lunar so badly.", "Edit:", "you don't have lex_common_msgs installed - this is in same package as the lex node", "You may already be aware, but just to make it extra clear: there is a difference between the package being hosted in the same repository and the messages actually being part of the same package. The messages are actually in a separate package, so copying files of the ", " package would not include the message and service files.", "And in this case I believe it's not the ", " files that are important, but the ", " and ", " files generated from those. That's what I meant with \"you may not have it all installed\" and \"you didn't copy everything that was needed\".", "there is indeed a hard coded Kinetic line in one of the CMAKE files in the lex_common_msgs folder - \" set(lex_common_msgs_INSTALL_PREFIX /opt/ros/kinetic)\"", "That line is from a ", " file in the ", " folder. That is only a single folder which contains some metadata consumed by other ROS packages, and the original ", " files. Those are actually not used by other packages, as I mentioned earlier.", "I would suggest not to copy files around. Instead, do the following:", "Thanks for the reply.  I'm not married to Lunar. But I have Lunar installed and invested a lot of time and effort getting other things to work on it, like tensorflow and a ros package for tensorflow/ros integration. ", "Given I'm not positive this issue would be corrected by changing to Kinetic I'm extra hesitant to start over with new distro.", "To your 4 thoughts", "you don't have lex_common_msgs installed - this is in same package as the lex node", "the Kinetic version you copied has hard-coded paths to locations in /opt/ros/kinetic - there is indeed a hard coded Kinetic line in one of the CMAKE files in the lex_common_msgs folder  - \"  set(lex_common_msgs_INSTALL_PREFIX /opt/ros/kinetic)\" and that folder also exists in the kinetic path", "you copied the files to incorrect locations - I was pretty careful - the node mostly works - but will go back and check ...", " - I'm really humbled by how much time you have put into helping me with this question. I will carefully study your response and get back to you on how it goes. I think I have a couple days of work to followup on your update. Thank you again for all your effort. Amazing.", " - first test I ran after your help was ", "and I got the expected output.", "I then deleted the lex_common_msgs from opt/ros/lunar and built in catkin_ws and it was successful in building.  Immediately after reran rossrv show and got same response as before building in catkin_ws.  But the original error remained.  ", "I gave up and installed SYSTEMBACK and set a restore point. After installing Kinetic, the LEX node ran from source without issue. So now I will start again getting the tensorflow bit working.", ". It totally jacked some database and Kinetic would not install. I needed to hand install about 20 packages. These and others:\nros-kinetic-genmsg\nros-kinetic-roscpp\nros-kinetic-gennodejs\nros-kinetic-rosconsole\nros-kinetic-message-generation\nros-kinetic-cpp-common\nros-kinetic-rosgraph-msgs\nros-kinetic-roscpp-ser", "I then deleted the lex_common_msgs from opt/ros/lunar and built in catkin_ws and it was successful in building. Immediately after reran rossrv show and got same response as before building in catkin_ws. But the original error remained. ", "you may have had to remove the ", " and ", " folder after having removed those copied files. It's often that paths get cached and then things can't be found when you change them afterwards.", "I will never again try to mix distros in this way. It totally jacked some database and Kinetic would not install", "This would probably also have been fixable but it seems you found another way to get things to work.", "I'm not sure what the best way to close this out would be.", "It doesn't seem like I really solved your problem, but it does seem like we figured out that copying files -- while possible -- should not be needed and most likely also should be avoided.", "Normally we mark resolved questions as answered by clicking the checkmark, but perhaps closing it with a \"no longer relevant\" might be more appropriate here."], "answer_details": ["Lunar is EOL (since May this year) and thus no longer supported. Kinetic runs on the same version of Ubuntu and would be a better target.", "Amazon supports Kinetic and Melodic for this exact reason.", "Lunar != Kinetic. Copying binaries from one version to another is brittle at best. I would not be surprised if there are some hard-coded paths in some of the binaries pointing to ", ".", " ", " ", " ", "you don't have ", " installed", "the Kinetic version you copied has hard-coded paths to locations in ", "you copied the files to incorrect locations", "you didn't copy everything that was needed", "remove whatever you have copied of the ...", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Is this never going to work because I am trying to use a Kinetic binary moved to Lunar folder?", "Is there a way to use a Kinetic binary from Kinetic folder in Lunar?", "What is the proper way to address the 'Have you typed 'make' in ...? - And yes, I did actually try to simply type 'make' in that folder."], "question_code": ["opt/ros/lunar", "b2@b2:~/catkin_ws$ rosservice list\n/lex_node/get_loggers\n/lex_node/lex_conversation\n/lex_node/set_logger_level\n/rosout/get_loggers\n/rosout/set_logger_level\n", "b2@b2:~/catkin_ws$ rosservice call /lex_node/lex_conversation \"{content_type: 'text/plain; charset=utf-8', accept_type: 'text/plain; charset=utf-8', text_request: 'Make a reservation', audio_request: {data: ''}}\" && sleep 1\nERROR: Unable to load type [lex_common_msgs/AudioTextConversation].\nHave you typed 'make' in [lex_common_msgs]?\n"], "answer_code": ["/opt/ros/kinetic", "catkin_make", "catkin build", "ERROR: Unable to load type [lex_common_msgs/AudioTextConversation].\n", "lex_common_msgs", "/opt/ros/kinetic", "lex_node", ".srv", ".h", ".py", ".cmake", "share/lex_common_msgs", ".srv", "rossrv show lex_common_msgs/AudioTextConversation\n", "build", "devel"], "url": "https://answers.ros.org/question/334178/have-you-typed-make-in-lex_common_msgs/"},
{"title": "What global and local path planning algorithms do ros use?", "time": "2019-08-27 09:00:19 -0600", "post_content": [" ", " ", "What is ros default global path planning algorithm? ", "what is ros default local path planning algorithm? ", "Are other algorithm implemented in ros (such as D star, potential field) such that one can decide which one to use for either local and/or global path planning?", "I assume the default global and local path panning are the one used in rViz when I do \"2D Nav Goal\" as well as in Gazebo when I send navigation goal request to the move_base server. Is this correct?", "You should check out the navigation tutorials, and the specific wikis for the global and local planners. ROS is open-source so you can view the source code yourself and add any custom flavor"], "answer": [" ", " ", "Funny you should ask this, I am actually trying to write an evaluation paper at the moment. The default local planner presented to you in the ", " is the ", " (and Trajectory Rollout). The default ", " can be specified as many (including D* as you wrote). Personally, for autonomous vehicles, I prefer using ", " as my local planner. There are a plethora of planners out there that you can search through and evaluate.", "As for your comments on RVIZ and Gazebo, these are just tools. RVIZ is a visualization tool and Gazebo is a simulation tool. The ", " you mentioned is just a graphical way for you to publish a ", " message on topic ", ". Without an implementation of the aforementioned navigation stack, your robot won't move anywhere.", " ", " ", "Please consult the documentation. ", "The default plugins in the navigation stack are Dijkstra/A* depending on parameters and DWA for local planning.", "You can implement any number of algorithms as you chose. Some folks may have implemented what you're asking for but they're not \"implemented in ros\", its implemented as a pluginlib plugin by someone who found that algorithm interesting for their uses and were kind enough to release it publicly. I'd do some digging if you're looking for something in particular and don't want to implement yourself. ", "Rviz and Gazebo aren't related to navigation, those are just the visualizer and simulator. I'd also suggest you go through some of the tutorials as prefpkg21 recommends. "], "answer_code": ["2D nav goal", "/move_base_simple/goal"], "url": "https://answers.ros.org/question/331734/what-global-and-local-path-planning-algorithms-do-ros-use/"},
{"title": "Robot localization fusing wheel and visual odometry", "time": "2018-02-15 20:27:27 -0600", "post_content": [" ", " ", " ", " ", "Hello. \nI'm trying to fuse whell and visual odometry in robot_localization package. But I get insufficiently accurate data on output. The robot is lost more strongly, than at use only a wheel odometry. It is especially noticeable that the robot is lost when it approaches very close to the wall.\nI'm using kinect and rtab_map for visual odometry. ", "This is my launch file to robot_localization.", "<launch>\n    <node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization\" clear_params=\"true\">", "</launch>", "What is right way to fuse whell and visual odoms? How should I set <rosparam param=\"odom0_config\"> and <rosparam param=\"odom1_config\">?"], "answer": [" ", " ", "The reasons for you problem might be:", "1- you need to adjust the process noise covariance from the parameter file because the model is not describing the kinematics accurately.", "2- the covariances you are using for each odometry is not precise and therefore the ekf can diverge.", "As for how you should set the config parameter of each odometry, this depends on the output of these odometries.", "i understand from your launch file is that your wheel odometry gives the pose [x, y, yaw] and also yaw angular velocity. if this is not true adjust your file. ", "The same for visual odometry, in your file you configured it to give you [vx, vy] if this is not correct adjust your file given that the config parameter is arranged as follows:", "[x, y, z, roll, pitch, yaw, vx, vy, vz, vroll, vpitch, vyaw, ax, ay, az]", " ", " ", " ", " ", "My high-level recommendation:", "Trust wheel odometry for linear velocity, trust monocular odometry for angular velocity, feed these to EKF and compute pose.", "Reasoning:", "It would make a lot more sense to feed rate (velocity) measurements from wheel encoders and visual odometry into your EKF and let the EKF fuse the rate measurements to compute pose. If you can provide good values for noise covariances for these various rate measurements it is possible to get good pose information. Obviously, tuning the process noise is a time consuming process. ", "Monocular visual odometry is quite good at measuring rotation but has no idea of scale of motion so you should NOT use its pose estimate or linear velocity estimate, only its angular velocity estimate.", "Stereo visual odometry can determine the scale of motion but is susceptible to high noise, so you CAN use its linear velocity and angular velocity measurements but be careful of its linear velocity measurements.", "Wheel odometry is good for providing linear and angular velocities but it can be susceptible to wheel slippage. A lot of wheel slip leads to bad odometry but my guess is that if your slip is nominal (regular concrete floor or road) it is best to trust this the most, especially for linear motion.", "How to make it better:", "Get an IMU as well."], "question_code": ["<remap from=\"odometry/filtered\" to=\"odom\" />\n\n\n  <param name=\"frequency\" value=\"20\"/>  \n\n  <param name=\"sensor_timeout\" value=\"0.1\"/>  \n\n  <param name=\"two_d_mode\" value=\"false\"/>\n  <param name=\"transform_time_offset\" value=\"0.0\"/>\n\n  <param name=\"odom_frame\" value=\"odom\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom\"/>\n\n  <param name=\"odom0\" value=\"odom_base\"/>\n   <param name=\"odom1\" value=\"odom_visual\"/>\n\n  <rosparam param=\"odom0_config\">[true, true, false,  \n                                  false, false, true, \n                                  false, false, false,   \n                                  false, false, true,\n                                  false, false, false]</rosparam>\n\n  <rosparam param=\"odom1_config\">[false, false, false, \n                                  false, false, false, \n                                  true, true, false,   \n                                  false, false, false,\n                                  false, false, false]</rosparam>\n\n  <param name=\"odom0_differential\" value=\"false\"/>\n <param name=\"odom1_differential\" value=\"false\"/>\n\n  <param name=\"odom0_relative\" value=\"false\"/>\n<param name=\"odom1_relative\" value=\"false\"/>\n\n  <param name=\"odom0_queue_size\" value=\"10\"/>\n\n  <param name=\"debug\"           value=\"false\"/>\n  <param name=\"debug_out_file\"  value=\"debug_ukf_localization.txt\"/>\n\n  <param name=\"alpha\" value=\"0.001\"/> \n  <param name=\"kappa\" value=\"0\"/> \n  <param name=\"beta\" value=\"2\"/>\n</node>\n"], "url": "https://answers.ros.org/question/282900/robot-localization-fusing-wheel-and-visual-odometry/"},
{"title": "How to export CMake settings (CMAKE_MODULE_PATH) to other packages?", "time": "2019-11-02 18:02:50 -0600", "post_content": [" ", " ", " ", " ", "I want to add a ", " file to my project which contains many multiple catkin projects.  Normally with cmake the way I would do this is set the CMAKE_MODULE_PATH variable to point to a cmake directory and then I could use the cmake command ", " to include a script from that directory.  Is there a good way to go about doing that in a catkin project so that other catkin projects that depend on my project could also use this cmake script?  ", "The context is I am trying to find the right way to go about adding a ", " file to a project like ", " which contains many different catkin projects and I'd like to only add my file in one place and ideally have it affect all the dependent catkin projects."], "answer": [" ", " ", " ", " ", "You can do this with a ", " (which is essentially a snippet of CMake) and exporting the location to it in your ", " call in the ", " of the package from which you'd like to export this.", "See ", " for a Q&A where this is done for exporting compiler flags from a package, and ", " for an example where this is used to export a non-standard include path.", "You should be able to append to ", " in the same way, as the extras-file contains just regular CMake. Be sure to specify the correct path to ", ": it would need to be a path relative to the package that you're calling ", " in and also make sure to ", " the extras-file as otherwise consumers of your package will not be able to find it when using an ", ".", "As the paths to your package may be different in the ", " and ", " space, you may have to use some templating to get the correct paths generated for you. ", " does something similar (from ", "):", "See also the ", " for more information on this.", "And a final comment: while this can certainly be a valid approach to export certain information from CMake packages (it's essentially just CMake), be aware that it's easy to \"overdo\" this. That could result in creating tightly-coupled packages, which would be undesirable."], "question_code": [".cmake", "include", ".cmake", "moveit"], "answer_code": ["catkin_package(.. CFG_EXTRAS ..)", "CMakeLists.txt", "CMAKE_MODULE_PATH", "CMAKE_MODULE_PATH", "catkin_package(.. CFG_EXTRAS ..)", "install(..)", "devel", "install", "gencpp", "@[if DEVELSPACE]@\n  ...\n@[else]@\n  ...\n@[end if]@\n"], "url": "https://answers.ros.org/question/336670/how-to-export-cmake-settings-cmake_module_path-to-other-packages/"},
{"title": "benchmarking between SLAM alghoritms", "time": "2019-03-20 03:10:52 -0600", "post_content": [" ", " ", "Hello,", "I'm trying to make a benchmark between GMAPPING and Cartographer SLAM alghoritms.\n   I have made 2D maps with gmapping and cartographer on ROS kinetic.(occupancy grid map format , i have saved maps with map_saver)\n   I need a roadmap about how to compare maps. Also I have to make a ground-truth map and compare the maps against this ground truth map.", "My questions;\n  1. How can I make a ground truth map, which tools can be used?\n  2. How can I compare pgm files(maps) I know that first I have to allign my maps and then compare but how?", "Thanks for your help."], "answer": [" ", " ", "I know it's quite late to answer this but I recently did compare two results from two SLAM packages. For the comparison I used a Python package called ", " that is meant to be used to compare odometry and SLAM. ", "I have described my full workflow in ", " but in short it consists of the following steps:", "What I was missing in my experiments is groundtruth but if you have this information you will easily be able to add it.", " ", " ", "The best way to have ground truth is to make the ground the robot rolls on (literally) :) Make a Gazebo world for your robot to roll around in so that you know exactly where everything is to compare to. Then run the different SLAM implementations over the _exact same_ mapping run's data of that space and compare.", "As for comparison techniques, many are available, I'd recommend consulting other SLAM benchmarking papers for their methodologies and using one or a variant of one for formal comparison. ", " ", " ", "As ", " said, easiest way to test their accuracy is a simulation environment which you could directly access to the ground truth data. This was what I did to compare effects of navigation's parameters' changes 3 years ago. ", "But; maybe in a testing environment, you could measure exact positions of multiple points in a room, then navigate the robot manually to those positions continuously on maps generated by both those algorithms. After navigating between those points enough, you could have enough data to compare the performance.", "Or let's say that this is an academic project and you could create an environment which you can place different sized AR tags everywhere. You can continuously collect the differences between navigation beliefs on generated maps by both packages and AR tag positioning reports. ", " ", " ", "I don't really know to answer your first question, but I would suggest to try running several SLAM algorithms and keep as ground truth the one that you think performs better.", "As for the second question, there exists this package ", " that implements the evaluation between two different OGMs. ", "\nIt is applying the Nearest Neighbor method, for every point of the map that is considered to be an obstacle, calculates the distance between these points and finds the Overall Mean Square Error.", " your feeedback is greatly appreciated. I was able to build and run \nthe mentioned package (env: ros kinetic, opencv 2.4.13 , cv_bridge build with opencv 2.4.13) in order to benchmark SLAM-generated OGM maps (cartographer vs. gmapping).  Alignment and map merge is successful, but the difference is it shown in Result Field of  \"Feature Matching MSE\" window and is it in meters? Thanks in advance", " glad that this package helped you. Yes, the result is shown there and MSE is calculated in meters, and the Quality Metric is a normalized MSE. So, I would suggest to take into account the Q, when you evaluate maps, as a more well defined and bounded metric"], "answer_details": ["Record a bag file while the robot is moving around space", "Replay the data while running slam packages of your choice, record these as bag files again", "Run a script that will convert the logged tf to a message supported by evo on every bag file", "Merge two bag files", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/318916/benchmarking-between-slam-alghoritms/"},
{"title": "Is it correct to set topic names from parameters?", "time": "2020-01-28 21:17:07 -0600", "post_content": [" ", " ", "This is a mix of a practical question and an academic question.", "When, if ever, is it correct to set topic names from parameters? If it is, how do you handle aspects such as the default topic name and clashes with remappings in launch files?", "Just a quick note: if remapping worked slightly better (", ", ", " and ", " fi) I would actually answer \"never\" to your question. As it is, using parameters ", " sometimes be the faster option to \"get something to work\".", "Remapping is part of configuration of the application, not the node. It's an activity that happens outside of the node itself, as it is part of the composition step of your application.", "If you start using parameters for this, separation of concerns seems violated to me, as suddenly nodes become responsible for 'knowing' how to integrate into a composite. That's not something I'd like.", "Edit: just noticed the ", " tag. As there is also a ", " tag, I linked to issues in ", ".", "I tried to start a conversion and I was down voted for it :(", "The voting here is whether you agree or disagree with the stated answer to try to build a consensus on what is the best approach. It's not a measure of the value of your statements or the validity of your concerns. But if an individual doesn't think that the answer is something that someone in the future should follow that's why they would downvote it."], "answer": [" ", " ", "My quick answer is \"only when the number of topics being used is unknown\". For example, in the navigation stack where you use parameters to control which sensor topics are subscribed to in order to detect obstacles.", "At all other times I think remapping should be used.", "However, if there are cases where remapping is not powerful enough, then parameters might be appropriate, but I would say that's a work around and instead we should try to improve remapping to allow for more complex cases. For example, in ROS 2 we described how you could use regex-like behavior in remapping, e.g. remap ", " to ", "(see ", "), which more cases than were allowed in ROS 1.", "I would say that's a work around and instead we should try to improve remapping to allow for more complex cases", "This is my feeling, too.", " ", " ", " ", " ", "I'll go with a couple examples", "Situation 1: You have a configurable number of sensors you'd like to subscribe to. You don't know at compile time what they are, or how many there are going to be. Examples: Navigation stack's voxel layer allowing many sensors. Remaps don't make sense because you'd then have to create unbounded ", " subscribers for every sensor topic because you don't know how many of ", " sensors a user might want. Another example in SLAM Toolbox allowing for multiple laser scanners for a similar purpose. Now, once you make a parameter for one, its inconsistent to not do them for all.", "Situation 2: You have created something similar to a ROS1 nodelet that is intended to be part of a nodelet chain. You have a multi-stage processing pipeline for a depth camera or something for filtering or data processing. In that case, you have a fixed input and a fixed output for each stage, probably just manipulating the same data. In this situation, you could remap everything, but that's very, very ugly and complex in a launch file. I never want my complexity in an XML file I have to sit there and read through a debug. I will always create those topics as parameters so that there's a configuration file that has the namespaced node with the ", " and ", " so that when I clearly read a configuration file, I can clearly see what's input goes to what output. And this is especially compact if there's a directional graph like structure to your processing pipeline. A simple yaml file is easier to read and maintain than an XML file with 10 stages. This situation is getting a little ranty but one more point: would you rather have 5 yaml files for different configurations or 5 launch files with different configurations? ", "Situation 3: You're writing something for outside consumption. That can be \"outside\" as in outside your organization (ei open source) \"outside\" as in outside this project (ei an external facing topic for the Navigation project like a sensor or output result). If its ", " \"outside\" in one of these ways,  I will probably not create a parameter because the likelihood of me needing to change it are slim to none. And for the case I do, remapping is still available. ", "If a package contains any of these situations, I automatically parameterize all other topic names for ", ". Once you do one, it would be inconsistent to not do it with all. Since output topics are still consistent (a map is a map is a map, and only one of them, etc) I would not parameterize the output topic. ", "tl;dr, by my use, I pretty much always parameterize inputs, but nearly never do outputs, unless in Situation 2. Typically, I will parameterize topics related to sensor data almost instinctively because its \"outside\". I deal with clashes of remapping by not using ...", "My comments on your comments:", "re: situation 1: that seems like a slightly different case, as the topic names are not \"set from parameters\" I feel. You have ", " items as part of a parameter dict, which then causes, eventually, ", " topics to be created.", "re: situation 2: remapping with nodelets in a processing pipeline: that's actually exactly what the pattern is that is very often used :) See the ", " fi. They all have fixed ", " and ", " topics which get remapped to connect everything together.", "I never want my complexity in an XML file I have to sit there and read through a debug. ", "so now you split the complexity between two places: the logic in the nodelet to create the topics based on parameters and the ", " + ", " file that loads the parameters. I've seen many things go wrong with ", " and ", " files ...", "Re-reading your answer ", ", this:", "I deal with clashes of remapping by not using remapping. I see that mostly as a debug tool and shouldn't be used in production settings when it can be helped.", "is a really strong opinion. Could you describe a bit more the \"clashes of remapping\"? I'm not sure I understand that.", ": could you perhaps give an example of how parameters would solve \"clashes with remappings in launch files\" (seeing as you brought it up)?", "If remapping is considered part of an application level (or subsystem level) configuration activity, then I'm not entirely sure how clashes could occur. The \"hand of god\" putting the system together should be aware of those (and if perhaps not a priori, at least a posteriori).", "I deal with clashes of remapping by not using remapping. I see that mostly as a debug tool and shouldn't be used in production settings when it can be helped.", "is a really strong opinion.", "I agree that this an atypical opinion in my experience, and I actually think that if you're not using remapping then your nodes are not designed to be reused. That might acceptable in some cases, but generally I wouldn't recommend it.", "For example, a driver node for a laser should publish to ", " not ", ", and remapping should be used to setup the topic name as you want. The author of the node should keep it generic, and users should not edit the source of the driver just to avoid using remapping.", "Also if your node is using parameters to determine the topic name, e.g. two ...", "I believe that configurations have no place in the launch system. Remapping implies that you have robot specific configurations inside of launch files. I think a certain level of configuration within the launch files are fine, things that are either direct arguments or derivatives of arguments into the launch file, or pulling from environmental variables, etc. ", "What I don't want, is hardcoded values in my launch files that makes them hard to be reused. If there's a configuration element, I want that in a parameter file that is easier to control and version control. The most frustrating type of debugging I do is when I have to read through XML and find that there was a remap and either its wrong, inconsistent with documentation, or misspelled. This was a major point actually from the ROSCon 2019 talk on common bugs found in ROS code. ", "Atypical?, that's my ...", ", my assertion is that the code itself should have supported a topic parameter to not require the need for a remap. Not that you've remapped a topic in a launch file, it is now a hard-coded parameterization outside of the configuration files you need to keep an eye on if you are trying to share a codebase with potentially dozens of variations of robots. I find that additional thing to be unscalable.", "The 2 situations you mention are both solved by parameters, that doesn't create launch dependencies on configurations.", ": if launch files are not what orchestrates your ROS application, what do you use? Because I believe that is the level ", " was mostly considering when he asked the question.", "You seem to be thinking of launch files that wrap one or two nodes.", "I still don't really understand how reading a ", " is any easier than an ", " file, but that may be personal.", "Well I think that asks another question that's not directly related. I'm talking about launch files that wrap one or two nodes, sure, but when you wrap then those in a launch file to bring up a robot / full application, you're largely missing reuse opportunities if there's remaps in the launch files. You then require to have different launch files for different configurations of robots rather than being able to have the nodes configure themselves based on parameters or globals. ", "XML contains the configurations of remaps, sure, but also contains a bunch of other stuff. If you're trying to change a configuration or debug a problem for why X data isn't getting to Y node, then all the other stuff (that I'd argue is what launch files are meant for) is \"in the way\" for configuration debug and management.", ": could you perhaps give an example of how parameters would solve \"clashes with remappings in launch files\" (seeing as you brought it up)?", "I don't think they would. If you have a parameter to set a node topic, and then someone tried to use a remapping, I'm not sure which would take priority and either way it would lead to confusion. Not using the tool designed for the task you are doing to avoid clashing with your use of another tool to do that task seems backwards to me."], "question_code": ["eloquent", "melodic", "ros_comm"], "answer_code": ["/front/*/image_raw", "/front_new/\\1/image_raw", "N", "N", "input_topic: XYZ", "output_topic: ABC", "N", "N", "pcl_ros", "input", "output", ".launch", ".yaml", ".launch", ".yaml", "scan", "/my/namespace/for/my/project/scan_unflitered", ".yaml", ".xml"], "url": "https://answers.ros.org/question/342777/is-it-correct-to-set-topic-names-from-parameters/"},
{"title": "Why no /opt/ros/kinetic/lib/*.so soname version?", "time": "2020-02-04 10:10:05 -0600", "post_content": [" ", " ", " ", " ", "Here's what I can see (abbreviated) on a typical Ubuntu Xenial installation of Kinetic:", "Here's a view of the \"host multiarch\" directory on the same installation:", "Why do the former lack soname versions? Why do only the latter have soname versions? Why does ", " export ", " such that the former path precedes the latter (causing the linker to find the _unversioned_ shared objects in lieu of the versioned ones)?", "Follow up question: Is there anything obviously wrong with doing something like the following?", "I.e., Is generating symbols files for every unversioned shared object a reasonable solution to this problem?"], "answer": [" ", " ", " ", " ", "More of a comment as this is all IIUC.", "Why do the former lack soname versions?", "There is no ABI compatibility guarantee for any of the ROS 1 packages. See ", " for an early rejected REP that dealt with this subject.", "In addition to this (or maybe because of it, although one does not exclude the other), there is no soname policy in ROS 1. So most of the libraries shipped in ROS 1 packages will not have the (ABI) version appended.", "Why do only the latter have soname versions?", "There are some exceptions which do have it, such as OpenCV, but that is because the upstream projects they come from already have them. MoveIt would be another exception.", "Why does ", " export ", " such that the former path precedes the latter (causing the linker to find the _unversioned_ shared objects in lieu of the versioned ones)?", "This I can't answer conclusively. That's probably something ", " could answer.", "note that there is some work in the direction of ABI compatibility (ie: explicitly stating it and checking for it).", " for instance adds the ", " attribute to the ", " element in package manifests (", "). And ", " gained the ability to run an ABI checker for certain jobs with the merge of ", ".", "AFAIK there is still no \"Debian style\" ABI compatibility guarantee, but the situation is improving.", "There are quite a few old(er) discussions about this on the ", " mailing list IIRC. You may want to look for those if you're interested in the history around this subject.", "I've posted an amendment to my original question --- one that concerns the generation of ", " files for the ROS unversioned shared objects. It seems to do what I want, but I'd like to hear other opinions.", "That should really be a new question.", "And I'm starting to wonder whether this is going in the direction of an xy-problem. You don't mention anywhere what ", " you are trying to solve here (but again: that should be in a new question).", " Here's the problem: Build and package something for Debian that depends on symbols from any ROS library. How does one discover the run-time dependencies if those packages do not provide symbols files?", "So this is an xy-problem. The question here is only part of it, and your last edit goes into the direction of a solution you've already chosen yourself.", "I would suggest to post that as a new question and accept the answer on this one.", "Unless you want to wait for ", " to comment on the last parts of course.", "Although you may want to post that as a separate question as well, as it doesn't seem connected to the first question(s) about sonames.", "As to the symbol files: starting with Melodic the buildfarm generates ", " packages for all ", "s which include all the symbols.", "I think you're confusing the ", " (automatically produced by debhelper >= 9.20151219) with the ", " (files produced by ", " and consumed by ", ")."], "question_code": ["$ find /opt/ros/kinetic/lib -maxdepth 1 -type f -name '*.so' -exec objdump -p {} \\; | grep SONAME | head\n  SONAME               libresource_retriever.so\n  SONAME               libpcl_ros_filters.so\n  SONAME               libinteractive_markers.so\n  SONAME               libtf.so\n  SONAME               librosbag.so\n  SONAME               librosconsole_bridge.so\n  SONAME               libnodeletlib.so\n  SONAME               libpcl_ros_features.so\n  SONAME               liburdf.so\n  SONAME               libsimple_message_dummy.so\n", "$ find /opt/ros/kinetic/lib/x86_64-linux-gnu/ -maxdepth 1 -type f -name '*.so.*' -exec objdump -p {} \\; | grep SONAME | head\n  SONAME               libopencv_video3.so.3.3\n  SONAME               libopencv_rgbd3.so.3.3\n  SONAME               libopencv_stitching3.so.3.3\n  SONAME               libopencv_xfeatures2d3.so.3.3\n  SONAME               libopencv_viz3.so.3.3\n  SONAME               libopencv_imgproc3.so.3.3\n  SONAME               libopencv_xphoto3.so.3.3\n  SONAME               libopencv_features2d3.so.3.3\n  SONAME               libopencv_ximgproc3.so.3.3\n  SONAME               libopencv_bgsegm3.so.3.3\n", "setup.sh", "LD_LIBRARY_PATH", "find \"/opt/ros/${ros_distro}/lib\" -maxdepth 1 -type f -name '*.so' -exec dpkg -S {} + \\\n    | tr -s ': ' '\\t' \\\n    | while read package sofile; do\n    version=\"$(dpkg -s \"${package}\" | awk '/^Version: /{print $NF}')\"\n    output=\"/etc/dpkg/symbols/${package}.symbols\"\n    mkdir -vp \"$(dirname \"${output}\")\"\n    dpkg-gensymbols -e\"${sofile}\" -p\"${package}\" -v\"${version}\" -O\"${output}\"\ndone\n"], "answer_code": ["setup.sh", "LD_LIBRARY_PATH", "compatibility", "version", "ros_buildfarm", "ros-users", "symbols", "-dbg", ".deb", "dpkg-gensymbols", "dpkg-shlibdeps"], "url": "https://answers.ros.org/question/343251/why-no-optroskineticlibso-soname-version/"},
{"title": "Writing tf messages to bag using rosbag Python API", "time": "2020-01-29 11:08:27 -0600", "post_content": [" ", " ", "I have some simulated images and camera poses, which I would like to write to a bag using the Python rosbag API. I have successfully written the images and I am now trying to write the poses to the /tf topic. I can write the messages, but rosbag play and rviz doesn't recognize the frames in the tf tree.", "These are my imports:", "I use the following function to generate the message:", "And I then write to the bag:", "When I launch this in RViz I have the following in my launch file:", "Which populates the static transform between the map and center_scene.", "Now RViz won't recognize these transforms.", "If I inspect the bag:", "If I play the bag using rosbag CLI and echos the /tf topic I get:", "ie. the transforms are only published once and then nothing happens.", "Any suggestions to how to fix this is much appreciated. "], "answer": [" ", " ", "A comment: you have this:", "this seems like you're treating ", " as an Euler RPY triple, or perhaps some other variant of Euler.", "The values you show in your ", " seem to confirm this.", "Know that ", " is actually a ", ". You cannot assign RPY triples to them and expect things to work.", "The ROS wiki has a bit of info on quaternions: ", ".", "Thank you. My assumption was that the actual values didn't really matter and I would convert them to quaternions once I got it to publish the pose.", "I have made a workaround where I write a StampedPose to the bag and during the launch and playback of the file, I listen to the StampedPose topic and publish it on the tf tree. That works.", "That should not be necessary. And it's a rather nasty workaround actually.", "My assumption was that the actual values didn't really matter", "that's not necessarily true: RViz (and other consumers of TF frames) are starting to ignore unnormalised quaternions. Which yours certainly are.", "I did the proper conversion and now everything seems to work. If you want to add it as an answer I'll accept it :) Thank you for your help.", "There you go.", "Good to hear you got it to work."], "question_code": ["from std_msgs.msg import Header\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_msgs.msg import TFMessage\n", "def to_pose_stamped(cam, header, frame_id, child_frame_id):\n    camera_msg_config = TFMessage()\n    camera_msg = TransformStamped()\n    camera_msg_config.transforms = []\n    camera_msg.header = header\n    camera_msg.header.seq = 0\n    camera_msg.header.frame_id = frame_id\n    camera_msg.child_frame_id = child_frame_id\n    camera_msg.transform.translation.x = cam.tx\n    camera_msg.transform.translation.y = cam.ty\n    camera_msg.transform.translation.z = cam.tx\n    camera_msg.transform.rotation.x = cam.rx\n    camera_msg.transform.rotation.y = cam.ry\n    camera_msg.transform.rotation.z = cam.rz\n    camera_msg.transform.rotation.w = 1\n    camera_msg_config.transforms.append(camera_msg)\n    return camera_msg_config\n", "camera_pose = to_pose_stamped(obs.camera, std_header, \"center_scene\", \"camera\")\nbag.write(\n    '/tf', camera_pose, camera_pose.transforms[0].header.stamp\n)\n", "  <node name=\"static_transform_publisher\" pkg=\"tf\" type=\"static_transform_publisher\" args=\"1 1 1 0 0 0 1  map center_scene 10\"/>\n", "path:        src/test.bag\nversion:     2.0\nduration:    16.0s\nstart:       Jan 29 2020 17:50:26.11 (1580316626.11)\nend:         Jan 29 2020 17:50:42.07 (1580316642.07)\nsize:        303.2 MB\nmessages:    606\ncompression: none [404/404 chunks]\ntypes:       sensor_msgs/Image  [060021388200f6f0f447d0fcd9c64743]\n             tf2_msgs/TFMessage [94810edda583a504dfda3829e70d7eec]\ntopics:      /stereo/left/image_rect_color    202 msgs    : sensor_msgs/Image \n             /stereo/right/image_rect_color   202 msgs    : sensor_msgs/Image \n             /tf                              202 msgs    : tf2_msgs/TFMessage\n", "WARNING: no messages received and simulated time is active.\nIs /clock being published?\ntransforms: \n  - \n    header: \n      seq: 0\n      stamp: \n        secs: 1580316626\n        nsecs: 113162040\n      frame_id: \"center_scene\"\n    child_frame_id: \"camera\"\n    transform: \n      translation: \n        x: 5.971\n        y: 47.849\n        z: 5.971\n      rotation: \n        x: -1.3\n        y: 15.6591\n        z: -6.38888\n        w: 1.0\n---\ntransforms: \n  - \n    header: \n      seq: 0\n      stamp: \n        secs: 1580316626\n        nsecs: 113162040\n      frame_id: \"center_scene\"\n    child_frame_id: \"projector\"\n    transform: \n      translation: \n        x: 23.801\n        y: 83.23601\n        z: 23.801\n      rotation: \n        x: 35.03401\n        y: 5.03358\n        z: 9.73858\n        w: 1.0\n---\n"], "answer_code": ["camera_msg.transform.rotation.x = cam.rx\ncamera_msg.transform.rotation.y = cam.ry\ncamera_msg.transform.rotation.z = cam.rz\n", "rotation", "rostopic echo /tf", "rotation"], "url": "https://answers.ros.org/question/342851/writing-tf-messages-to-bag-using-rosbag-python-api/"},
{"title": "Network setup for multiple ethernet ports", "time": "2020-02-11 20:33:35 -0600", "post_content": [" ", " ", " ", " ", "Currently, I'm considering below multi-machine topology using ros1.", "I assigned static ip to each eth port as below.", "p.s.", "The reason why I connect multi-machine directly, rather than using switch is that", "Edit: Thank you for your reply.", "Could you possibly clarify below settings based on your answer?", "I'm still a bit confused.", "Assume that all IP assignments follow the above diagram(in the main text)", "The point I'm confused about is ROS_IP of master machine.", "I've exported one of IP addresses as ROS_IP at the master(192.168.0.3, rather than 192.168.0.4).", "In this case, it seem like that slave machine #2 is unreachable from the master since slave machine #2 regards 192.168.0.4 as a master.", "Best regards,", "An observation:", "I connect multi-machine directly, rather than using switch [because] the data needed for slave machine 1 is not for slave machine 2, vice versa", "If you get a good switch, the backplane should have sufficient bandwidth to accommodate packet exchange at full speed over all (most) ports. Layer 2 routing will make sure packets only get sent to where they should go. A hub would broadcast all traffic. A switch does not."], "answer": [" ", " ", " ", " ", "In case of a multi-homed system that is running the master, you need to make sure to set ", " to a hostname or IP address that is reachable by the hosts on each respective segment.", "As you have all interfaces in the same network, it doesn't really matter what you do, as all hosts will be able to reach all hosts.", "So you could choose to either use the ", " or ", " address for ", ".", "If you'd used different networks, then it would start to matter which IP address you'd use for each host.", "Edit: after your edit:", "All your network interfaces are in the same network (ie: ", "). Or at least, that's what I'm assuming, as you don't mention the netmask that you've configured (but I'll assume it's ", " or ", ").", "So it shouldn't matter which IP for your master you configure, as both ", " and ", " should be reachable by all machines. Try ", " and ", " from both Slave1 and Slave2. You should always get responses.", "Pick one of the IPs of your master and configure it as the ", " on ", " machines (including the master itself).", "Don't set ", " to an IP. Use ", " only.", "And don't set ", " to ", " for your master. Set it to ", ".", "And again I will state that with a good switch, this should not be necessary.", "Thank you again for your kind reply.", "I'm sorry but I couldn't understand the last comment you gave(i.e. good switch).", "Could you point out my misunderstanding?", "My understanding is as follow:", "Let's assume that the master machine publishes 6 topics(topic 1~6) through one eth port that connected to the good switch.", "And assume that both of slave machine #1, #2 are also connected to this good switch using dedicated eth line.", "If the required bandwidth of each topic is 0.333Gbps each, the single eth btw master machine and good switch will be overloaded.", "A good switch (ie: not a consumer level device) would support ", " or ", " (wikipedia article ", ").", "You'd use multiple NICs on your \"master machine\", combine their bandwidth and end up with a single, logical link between the switch and your PC. This logical link would have only a single IP (it would be seen as a single device by the OS).", "That way you could combine multiple Gbit links into 2, 3, 4 or higher Gbit/s links.", "An alternative could be to get a switch which supports 10Gbit/s links. Give you master PC a 10Gbit/s link to the switch. Use regular 1 Gbit links for the slaves."], "question_details": [" ", " ", " ", " ", " ", " ", " 192.168.0.1", " 192.168.0.2", " 192.168.0.3", " 192.168.0.4", "Master machine publishes large size of data", "But the data needed for slave machine 1 is not for slave machine 2, vice versa", "I want full bandwidth through assigning the dedicated eth-line for each slave"], "question_code": ["export ROS_MASTER_URI=http://192.168.0.3:11311", "export ROS_HOSTNAME=192.168.0.1", "export ROS_MASTER_URI=http://192.168.0.4:11311", "export ROS_HOSTNAME=192.168.0.2", "export ROS_IP=192.168.0.3", "export ROS_MASTER_URI=http://localhost:11311", "export ROS_HOSTNAME=$ROS_IP"], "answer_code": ["ROS_MASTER_URI", ".3", ".4", "ROS_MASTER_URI", "192.168.0", "/24", "255.255.255.0", ".3", ".4", "ping 192.168.0.3", "ping 192.168.0.4", "ROS_MASTER_URI", "ROS_HOSTNAME", "ROS_IP", "ROS_MASTER_URI", "localhost", "http://$ROS_IP:11311"], "url": "https://answers.ros.org/question/343874/network-setup-for-multiple-ethernet-ports/"},
{"title": "What is a ROS package?", "time": "2011-02-14 09:40:04 -0600", "post_content": [" ", " ", " ", " ", "Information on ROS packages.", "good \"karma-question\"!"], "answer": [" ", " ", "Software in ROS is organized in packages. A package might contain ROS nodes, a ROS-independent library, a dataset, configuration files, a third-party piece of software, or anything else that logically constitutes a useful module. The goal of these packages it to provide this useful functionality in an easy-to-consume manner so that software can be easily reused. In general, ROS packages follow a \"Goldilocks\" principle: enough functionality to be useful, but not too much that the package is heavyweight and difficult to use from other software. ", "See ", " for more details.  "], "url": "https://answers.ros.org/question/9036/what-is-a-ros-package/"},
{"title": "Modify multiplot x-axis", "time": "2018-08-01 12:29:40 -0600", "post_content": [" ", " ", "Hello ROS community,", "In ", ", when using ", " or message ", " for displaying data, the x-axis becomes impossible to parse in a meaningful way. Times are represented in unix-time that is to say on the order of 1,500,000,000 seconds and display as \"1.52824e+09\". So it's really unintuitive to know how long a particular sequence is, let alone any given tick mark.", "Ideally I'd to ", "But I'm not finding any of these features in ", ". The goal is for my team to use this really approachable ", " tool, but have meaningful time values.", "I have a few ideas on how to get around this, none of them good:", "Thoughts? Help?", "Thank you.", "I don't know about ", ", but I believe ", " support all your points under 'ideally'.", "Looking at ", ", it looks like it does support point 1 from my \"ideally\". I'm pursuing that for now."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Change the x-axis to \"shift\" the times to staring at 0 (0 = the start of the bag),", "have a sub-scale (like ", " does)", " display the times differently (\"1.52824e+09\" \u2192 \"10:32:06 PM\")", "Add a \"human readable\" time stamp to our headers and use those on the x-axis", "Add some sort of 0-based time in our headers and use those on the x-axis", "Post-process each bag to convert times to start at 0", "Write/wrap ", " to convert times on-the-fly and publish to be consumed by ", "Somehow use ", " to simulate a bag starting at time 0"], "question_code": ["rqt_multiplot", "rqt_plot", "rqt_multiplot", "rqt_multiplot", "rosbag play", "rqt_multiplot", "/clock", "rqt_multiplot", "PlotJuggler"], "url": "https://answers.ros.org/question/299331/modify-multiplot-x-axis/"},
{"title": "board: minimum hardware requirements? single board? full motherboard? nuc? mini-itx?", "time": "2014-07-09 12:57:38 -0600", "post_content": [" ", " ", "hi. we're trying to decide what board we should use for our robot prototype. any advice/recommendation is much appreciated.  we're looking for an affordable board that can handle kinect and kobuki.", "our robot is fairly similar to turtlebot:", "from our initial research:", "singleboard computer (i.e. raspberry pi, beaglebone black, etc):  the cost is really good but it seems like they can't handle kinect well.  we haven't done extensive work with ros for ubuntu arm yet, but there seems to be some limitations vs. ros full desktop.", "full motherboards (i.e. intel i5 or i7):  they definitely can do the job, but the costs are really high.  and the processing power seems to be overkilled.", "midrange options like NUC, mini-ITX, etc:  i have no idea how they perform and if they work well with kinect and kobuki", "any recommendation/feedback/advice is much appreciated."], "answer": [" ", " ", "Personally, I think most of the single board computers don't have enough computing power to handle the kinect well, and they'll also be a bit slow for doing motion planning for an arm. Some of the newer quad-core, 1.5GHz+ boards might be sufficient (Odroid U3 and up, Radxa Rock, new Qualcomm boards, and probably others).", "The performance of the NUC and other mini-ITX boards will be very dependent on which CPU and how much memory they have. You can get most of these boards with a wide range of CPUs, so the decisions about which CPU and which x86 form-factor to choose are not strongly correlated.", "I would make the decision about which CPU and how much memory you need before deciding on a form factor. The original turltlebots managed to squeeze by with a low-powered, dual-core Intel Atom processor and 1 or 2GB of RAM. For a more complex robot, you should probably move up to an Intel i5 or an i7.", "Once you have your CPU picked out, look for low-powered motherboards that support it. Personally, I think the NUC would make a great robot because some of the newer NUC support a wide-range voltage input that shouldn't need much or any extra regulation when running from a battery. This part of the NUC spec is NOT OBVIOUS, and NOT SUPPORTED BY ALL NUCs, and you have to read the full technical specification VERY CAREFULLY to find it, but it's there. Some of the NUCs also have a alternate 2-pin molex power connector on the board that will be more secure than the external barrel connector. There are also a fair number of other low-power motherboards that have single power supply inputs (mostly 12V), and the PicoPSU power supplies that are very compact and will run from a DC input.", "RAM is pretty cheap these days, so buy as much as you can fit in your motherboard. Probably 4GB at the bare minimum.", "If you're building a real, production robot, be prepared to buy and try several different computer configurations before you settle on the final board.", "thanks ", ". most of our computation is done on the cloud, so locally we only need a cpu+board to handle some light-weight tasks and act as a communication device with the cloud.  the heaviest local task is kinect+kobuki for navigation. do you think a mini-itx with atom e3800 + 4g ram works?", "I would err towards the more expensive computer for an initial prototype, so that you can get something running, and then work on optimizing the software to decrease the computing power required. If you have a very limited budget you could try to borrow a laptop with a similar CPU to test with.", "This of this this way: if you buy the cheaper computer and it isn't powerful enough, you've wasted time, money, and you still have to go buy something more expensive."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "we'll use kinect for depth camera", "we'll use kobuki base or build our our differential drive 4 wheel drive base.", "we'll use have one or two arms with dynamixels.", "we would love to have low power consumption so that the robot can operate as long as possible", "and of course, it runs ROS"], "url": "https://answers.ros.org/question/185761/board-minimum-hardware-requirements-single-board-full-motherboard-nuc-mini-itx/"},
{"title": "Interpreting data from ROS openni_tracker", "time": "2014-05-04 22:57:16 -0600", "post_content": [" ", " ", "Hi,", "I am subscribing to the ", " data from ", ". Here is my program where I am using ", " function to get the data published over ", ".", "I know ", " returns the origin vector translation between the two ", " messages used in ", " function. Similarly I have an idea that ", " returns a quaternion representing the rotation between the two ", " messages used in ", " function. Here is an excerpt of the output I get after running the program.", "Here are my questions:", "Cheers!", "Hello thank you verymuch for your code. But I can't find a way to get joint positions with this code . My openni_tracker is working... Please let me know how can I do this so that I can see all the joint coordinates or specific ones in real-time in terminal..", "I'm waiting for your answer."], "answer": [" ", " ", "ROS message datatypes are standardized in ", " The positions are in meters, and quaternions don't have units. For more on quaternions I suggest looking here:  "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "I am finding it hard to understand these output numbers. Can someone explain or direct me to sources I can consult to relate these numbers to actual physical movements or rotations?", "I googled but couldn't find what unit these numbers represent. cm, mm or what?", "Any other resource that can help me extract meaningful data from these numbers would be appreciated."], "question_code": ["/tf", "openni_tracker", "lookupTransform()", "/tf", "#include <ros/ros.h>\n#include <tf/transform_listener.h>\n#include <stdio.h>\n\nusing namespace std;\n\nint main(int argc, char** argv){\n  ros::init(argc, argv, \"my_tf_listener\");\n\n  ros::NodeHandle node;\n  tf::TransformListener listener;\n\n  ros::Rate rate(10.0);\n\n  while (node.ok()){\n    tf::StampedTransform transform;\n    try{\n      listener.lookupTransform(\"/openni_depth_frame\", \"/left_elbow_1\", ros::Time(0), transform);  \n      }\n    catch (tf::TransformException ex){\n         ROS_ERROR(\"%s\",ex.what());\n         }\n    double left_elbow_1_x = transform.getOrigin().x();  \n    double left_elbow_1_y = transform.getOrigin().y();\n    double left_elbow_1_z = transform.getOrigin().z();\n\n    cout <<\"left_elbow_1_x \" <<left_elbow_1_x<<endl;\n    cout <<\"left_elbow_1_y \" <<left_elbow_1_y<<endl;\n    cout <<\"left_elbow_1_z \" <<left_elbow_1_z<<endl;\n\n    try{\n      listener.lookupTransform(\"/openni_depth_frame\", \"/right_elbow_1\", ros::Time(0), transform);\n    }\n    catch (tf::TransformException ex){\n      ROS_ERROR(\"%s\",ex.what());\n    }\n\n    double right_elbow_1_x = transform.getRotation().x(); \n    double right_elbow_1_y = transform.getRotation().y(); \n    double right_elbow_1_z = transform.getRotation().z();\n    double right_elbow_1_w = transform.getRotation().w();\n\n    cout <<\"right_elbow_1_x \" <<right_elbow_1_x<<endl;\n    cout <<\"right_elbow_1_y \" <<right_elbow_1_y<<endl;\n    cout <<\"right_elbow_1_z \" <<right_elbow_1_z<<endl;\n    cout <<\"right_elbow_1_w \" <<right_elbow_1_w<<endl;\n\n    rate.sleep();\n    }\n  return 0;\n}\n", "tf::StampedTransform::getOrigin()", "/tf", "lookupTransform", "tf::StampedTransform::getRotation()", "/tf", "lookupTransform", "left_elbow_1_x 2.35472\nleft_elbow_1_y 0.146672\nleft_elbow_1_z 0.299355\nright_elbow_1_x 0.331682\nright_elbow_1_y -0.58186\nright_elbow_1_z 0.308364\nright_elbow_1_w 0.675527\n"], "url": "https://answers.ros.org/question/160686/interpreting-data-from-ros-openni_tracker/"},
{"title": "Efficiency issues when separating processing pipline into nodes?", "time": "2016-04-12 13:03:37 -0600", "post_content": [" ", " ", "I am porting an existing codebase to ROS and am running into an efficiency issue that seems endemic to ROS's architecture, and looking for the right way to address it.", "My robot is driven by computer vision. When an image is captured, a series of processing steps is run in sequence, each building off of the data computed previously (e.g., \"detect features\", \"compute location\", \"determine motion\"), ending with motors being actuated. It is essential to minimize the delay between an image being captured and the robot driving its motors in reaction to that image.", "In the original codebase, processing occurred in a single thread running a loop: it grabbed the latest available image, then stepped through the processing chain. If it couldn't process at the full framerate of the camera, frames would be missed, but always the processing would start with the latest available image.", "In ROS, guided by documentation, I have split every image processing step into a separate nodelet. Consider now a case in which I have two processing phases, Phase 1 and Phase 2, each with its own nodelet. Now imagine that the camera emits a frame every 10ms, Phase 1 takes 20ms, and Phase 2 takes 40ms.", "The issues I see:", "In reality there are actually more phases, multiplying the complexity.", "It seems to me that this processing pipeline really wants to be a single nodelet with a single thread. But this seems to go against ROS's design philosophy. And if I implement it that way, of course I lose access to the wonderful ROSsian ability to run and debug each node(let) independently, playing back intermediate data streams from bag files, etc..", "So my current plan is to implement everything as separate nodelets for development, but deploy using a combo nodelet that ties the whole processing chain together, reusing inner implementation classes from the separated nodelets.", "Does this sound reasonable? It seems like a common issue; are there other ways people have tried? Thanks in advance!", "Some quick comments: 1) would using a buffer of length 1 avoid the ", " problem? 2) make sure you're publishing (and subscribing) ", " msgs, otherwise you'll not get zero-copy behaviour. 3) locking + nodelets is not a good idea. It's a msg-passing system, even if it's single-process.", " sorry, just noticed this comment. (1) Apparently not, see  ", "  (2) yes, I am (3) I'm not sure what locking you're referring to. But even if (1) were possible, it still doesn't solve the first bullet point above. "], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Half of all the data Phase 1 produces will never get consumed. This wastes a lot of CPU and slows everything down.", "If each nodelet does its processing in callbacks (even using multiple callback-processing threads), the buffers of images and of Phase 1 results will reach their capacity. Given the way buffers are implemented, each new frame that is processed will then be a stale, old frame from the back of the buffer, rather than the latest available one.", "If each nodelet instead has its own processing thread, there will be a lot of code to maintain thread safety of every callback (also thread switching overhead, though this will probably be minimal) that all seems unnecessary given the straightforward processing pipeline that is desired. It also doesn't address the first issue."], "question_code": ["..ConstPtr"], "url": "https://answers.ros.org/question/231762/efficiency-issues-when-separating-processing-pipline-into-nodes/"},
{"title": "Change URDF link position", "time": "2019-01-02 05:29:07 -0600", "post_content": [" ", " ", "Hi,", "I have a lab setup in urdf, which I am using in RViz / MoveIt!. Based on the real world calibration, I would like to move some objects a bit (a table with an offset). The link is connected to the world with a fixed joint currently.", "Is there a way to do this dinamicaly, during runtime?  ", "Thanks in advance\nBalint Tahi\nPPM Robotics AS"], "answer": [" ", " ", " ", " ", "While it's certainly possible to update link lengths in parsed URDFs (it's just an in-memory representation of the xml based file format after all) there are two problems with doing that:", "So I believe the answer to your question would be \"yes, it is supported, but probably not in a way that would fit your use-case\".", "What is typically done is to write a custom, purpose-built calibration node that as its input takes a URDF / xacro, performs a calibration and then outputs an updated URDF. An example would be the ", " package fi -- although that focuses on robots specifically (ie: kinematic chains).", "What could be an alternative to updating URDFs themselves -- which is very limiting as you would not be able to (easily) output xacro fi -- could be to use ", "'s ", ": place offsets and distances / link lengths in the ", " file and have the calibration tool update/output that.", "That would still need a restart / reload of any URDF consumers, but ", "s are much easier to read/write by both humans and machines than a potentially unlimited nested ", " hierarchy.", "This connects to a much wider discussion about the (perceived) immutability of URDF that has taken place at venues such as ROSCon and the ", " category on ROS Discourse.", "Note that comments typically focus on URDF and how this is a serious problem with it, but I don't necessarily agree with that: keeping state consistent across multiple, asynchronously participating entities in a distributed system is just not a solved problem. The same issues would arise when using SDF for instance (although that format would solve a few others).", "There are certainly avenues to explore (I'd still like to take a closer look at ", " fi), that will take time and someone (or someones) will need to spend that time to investigate a replacement", "Thanks for the quick answer. Then I will try the \"alternative way\", updating a yaml and using xacro first. I am using only rviz and moveit as a consumer, that shouldn't be a problem to restart."], "answer_details": ["even though ", " is (typically) a \"global\" parameter, changes to the in-memory representation local to one consumer will not be forwarded (or persisted) to the parameter automatically. But even if they did, then ..", "none of the (officially supported) consumers of URDF expect ", " to change at runtime, so they don't typically check the parameter for changes nor parse it again to update their internal model, not even if those consumers are found inside the same process (ie: node)", " ", " ", " ", " "], "answer_code": ["robot_description", "robot_description", "xacro", ".yaml", ".yaml", "xacro"], "url": "https://answers.ros.org/question/311788/change-urdf-link-position/"},
{"title": "Rosserial Arduino - Serial Port failure when interrupted", "time": "2016-06-03 17:53:01 -0600", "post_content": [" ", " ", "Hello, I am trying to make a DC motor controller using an arduino mega 2560 and ", ". The motors used are ", ".", "Ideally I want to use the rosserial library in such a way that I can send commands to both motors, and read the current position, as well as the current through the motor (the driver gives you this info). Whether this feedback will be an advertised topic or a service is still to be determined, given the problems I am having. ", "Here is the code running in my Arduino. I'm not a huge fan of using arduino libraries, so it is mostly microcontroller code (registers). The class DCMotorPID is my own and has been extensively tested. This whole code works perfectly on its own without the ROS communication on top. "], "answer": [" ", " ", "Thanks for the suggestions Paul. I actually managed to fix the problem, which was a sum of many little ones. I'm leaving some tips for anyone who might encounter this in the future:", " ", " ", "Nothing jumps to mind as a problem in your code. Does Hello World run on the arduino mega? If it does, I suggest you break the system down to try & discover exactly what's breaking it, and why. ", "Some suggestions:\n- Publish \"Hello\" in your main loop and when you stop seeing it come out in ROS is the exact time when the system died\n- Turn the motor by hand (vs sending a command). If it crashes when you turn it by hand then it's the encoder interrupts, as you suspect. If not, then something else is going on.\n- Blink the arduino LED in your main loop. Does it keep blinking when the serial port read failure happens? If not, Arduino has crashed.", "Hope this helps", "Paul"], "answer_details": ["One of the problems was the line TCCR5B = (1 << CS51); that should be TCCR5B |= (1 << CS51);\nI have no idea why this is a problem, since that register should be all zero except for the bit CS51.... but it works with the pipe, and doesn't work without it.", " Another thing to take into account for people losing sync over time, is to make sure your baud rate matches the crystal you are running on your board/arduino. For arduinos with 16Mhz clock, running at 115200 is not a good idea, since this baud rate yields a 2.1% maximum error in the communication. Sadly, the rosserial_python won't accept custom baud rates like 250000 which yield no error. For more info consult this link:  ", " .  ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["/********************************************************************************\n  Includes\n********************************************************************************/\n#include <Arduino.h>\n#include <ArduinoHardware.h>\n#define USE_USBCON 1\n#include <ros.h>\n#include <avr/io.h>\n#include <avr/interrupt.h>\n#include <std_msgs/Int32.h>\n#include <std_msgs/Int16.h>\n#include <DCMotorPID.h>\n\n/********************************************************************************\n  Macros and Defines\n********************************************************************************/\n\n#define BAUD_RATE 115200\n#define SAMPLING_FREQ 100.00\n#define DT 1/SAMPLING_FREQ\n\n/********************************************************************************\n  Global Variables\n********************************************************************************/\n\n//DcMotorPID(PWM, DIR, FB, D2, SF)\nDCMotorPID m1(6,8,A0,4,12);\nDCMotorPID m2(7,9,A1,4,12);\n\n// Global variables for ROS \nros::NodeHandle nh;\n\n//ROS subscribers and their callbacks\nvoid sub_cmd1_cb (const std_msgs::Int32& pos1_msg)\n{\n  m1.command= pos1_msg.data;\n}\nros::Subscriber<std_msgs::Int32> sub_cmd1(\"command1\", sub_cmd1_cb);\n\nvoid sub_cmd2_cb (const std_msgs::Int32& pos2_msg)\n{\n  m2.command= pos2_msg.data;\n}\nros::Subscriber<std_msgs::Int32> sub_cmd2(\"command2\", sub_cmd2_cb);\n\n/********************************************************************************\n  Program Code\n********************************************************************************/\nvoid setup()\n{\n  //Initialize PID\n  m1.init();\n  m2.init();\n\n  //ROS initialization\n  nh.getHardware()->setBaud(BAUD_RATE);\n  nh.initNode();\n  nh.subscribe(sub_cmd1);\n  nh.subscribe(sub_cmd2);\n\n  //Configure External Interrupts to handle encoder signal\n  EICRA |= (1 << ISC30) | (1<<ISC10);  // INT3 & INT1 (channel A) interrupts on both flanks\n  EIMSK |= (1 << INT3) | (1<<INT1);   // INT3 & INT1 interruptions enabled\n\n  //Initialize Timer5 for sampling frequency 100Hz\n  TIMSK5 = (1 << TOIE5);    //enable timer overflow interrupt for Timer5\n  TCNT5 = 45535;            //set counter to 45535, 20000 clicks will be 10 ms\n  TCCR5B = (1 << CS51);     //start timer5 with prescaler=8\n}\n\nvoid loop()\n{\n  nh.spinOnce();\n}\n\n//Takes up about 156 us\nISR(TIMER5_OVF_vect)\n{\n  // Reset the timer5 count for 10ms\n  TCNT5 = 45535;\n  //PID routine\n  //Proportional term\n  long error1 = m1.getError();\n  long error2 = m2.getError();       \n\n  //Integral term\n  m1.integral = m1.integral + (error1 * (float)DT);         \n  m2.integral = m2.integral + (error2 * (float)DT);         \n\n  //Derivative term\n  m1.derivative = (error1 - m1.last_error) * (float)SAMPLING_FREQ;\n  m2.derivative = (error2 - m2.last_error) * (float)SAMPLING_FREQ;  \n\n  // Compute Duty Cycle\n  long output1 = m1.KP*error1 + m1.KI*m1.integral + m1.KD*m1.derivative;\n  long output2 = m2.KP*error2 + m2.KI*m2.integral + m2.KD*m2.derivative;\n  m1.setSpeed(output1);\n  m2.setSpeed(output2);\n\n  // Remember last state\n  m1.last_error = error1;\n  m2.last_error = error2;\n}\n\n//Encoder handling routine for Motor1 (channel A is PD3, channel B is PD2)\n//Interrupt time is about 4us\nISR(INT3_vect)\n{\n  unsigned char enc_state;\n  unsigned char dir;\n  enc_state = ((PIND & 12) >> 2);             //read the port D pins 2 & 3\n  dir = (enc_state & 1) ^ ((enc_state & 2) >> 1); //determine direction of rotation\n  if (dir == 1) m1.position++; else m1.position-- ..."], "url": "https://answers.ros.org/question/236051/rosserial-arduino-serial-port-failure-when-interrupted/"},
{"title": "How to power off Kinect from software", "time": "2015-01-25 08:32:39 -0600", "post_content": [" ", " ", " ", " ", "I have a Kinect connected to the onboard Linux-based PC on my robot. \nHowever, in many scenarios I am not using the Kinect, so I would like to be able to turn it off from command line to save power.", "I tried the following:", "This works, in the sense that the power light of the Kinect goes off for a moment, and the USB device is removed. However, the Kinect gets redetected within a second - re-enabling it.", "Following results in the same behaviour:", "Note that I am using a 3.4 kernel, and following does ", " work:", "Is there a way to disable the Kinect from software?", "Thank you!"], "answer": [" ", " ", " ", " ", "I found that the command", "works, but ", "What works reliably for me is the following:", "Note that the 2.5 seconds sleep is crucial - 2 seconds will not work (too fast), 3 seconds neither (too slow). This was so on both machines on which I tested this.\nIt seems that the sweet spot is when the Kinect USB Hub is detected, but not yet any of the actual Kinect devices:", "When troubles finding the sweet spot, you can just \"bruteforce\" it, as follows:", "Finally, to re-enable:", " ", " ", "The kinect is powered by an external 12V power supply, so turning it completely off from software will probably require special hardware. There may be ways to reduce the Kienct's power consumption without turning it completely off.", "There are a few steps that I would take if I was investigating this:", "Now look at all of your power measurements, compute (or measure) their affect on your battery life, and figure out which steps actually cause the system to use more power.", "I suspect the big power consumers in your system are actually the kinect drivers and the algorithms, and that you can significantly reduce your power consumption just by stopping those nodes when you're not using them. The ", " package may be useful for implementing this.", "I did some measurements with a crude power meter (resolution: 0.01A), and found following:\n- Kinect connected to power supply only: 0.00 A\n- Kinect connected to power supply, and to USB: 0.06A going to 0.04A\n- Kinect driver running: 0.06A going to 0.04A\n- Kinect app running: >0.06A", "So yes, it seems you are right, it is not worth bothering about those 40mA extra being used while idling.", "Also thanks for the capabilities pointer, will definitely look into that!", "But the Kinect V2 will give us a reason to worry about in the future. With 12.2V: Kinect connected to power supply only: 0.00 A - Kinect connected to power supply, and to USB: ~0.38A - Kinect app running: ~1.18A(+-0,06A)", "I just detected a major flaw in my current measurement setup, and retested:\nKinect connected to power supply only: 0.00 A (baseline) - Kinect connected to power supply, and to USB: 0.50A stabilizing at 0.38A - Kinect driver running: 0.38A - Kinect app running: >0.45A", "... which is in line with what ", " reports.", "So it seems to make sense to try disabling the kinect completely - as that would save about 0.38A", "You'll probably need to build a custom circuit to disconnect power to the kinect when you're not using it. To determine if that's a worthwhile effort or not, try to figure out your current battery life, total power consumption, and how much run-time you'll gain by disabling the kinect."], "answer_details": [" ", " ", " ", " ", "Measure the power consumption of your system, without the kinect plugged in. This gives you a baseline \"zero\" to compare future results against.", "Plug in the kinect, and measure the power consumption without the ROS drivers running.", "Run the ROS drivers, and measure the power consumption. I suspect most of the power draw associated with the kinect is actually associated with streaming data from it and processing that data.", "Start up the rest of your ROS nodes, and measure the increased power consumption associated with running your algorithms. (I suspect this is significant).", " ", " ", " ", " "], "question_code": ["\necho '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "\necho '1' > /sys/bus/usb/devices/1-1/remove\n", "\necho '0' > /sys/bus/usb/devices/1-1/power/autosuspend_delay_ms\"\necho 'auto' > /sys/bus/usb/devices/1-1/power/control\"\n"], "answer_code": ["echo '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "echo '1-1' > /sys/bus/usb/drivers/usb/unbind; sleep 2.5; echo '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "[ 3451.492033] usb 1-1: new high-speed USB device number 109 using ehci-pci\n[ 3451.624348] usb 1-1: New USB device found, idVendor=0409, idProduct=005a\n[ 3451.624351] usb 1-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\n[ 3451.624860] hub 1-1:1.0: USB hub found\n[ 3451.624970] hub 1-1:1.0: 3 ports detected\n", "for (( i=0 ; i < 10 ; i++)) ; do echo '1-1' > /sys/bus/usb/drivers/usb/unbind; sleep 0.5; done\n", "echo '1-1 > /sys/bus/usb/drivers/usb/bind\n"], "url": "https://answers.ros.org/question/201704/how-to-power-off-kinect-from-software/"},
{"title": "ROS Nodes and topics - are nodes really decoupled?", "time": "2015-02-14 10:21:04 -0600", "post_content": [" ", " ", " ", " ", "Note how the two sides are decoupled.\n  All the hokuyo_node node does is\n  publish scans, without knowledge of\n  whether anyone is subscribed. All the\n  rviz does is subscribe to scans,\n  without knowledge of whether anyone is\n  publishing them. The two nodes can be\n  started, killed, and restarted, in any\n  order, without inducing any error\n  conditions.", "We all know these statements that using topics, decouples ROS nodes. However reading the documentation on how the connection between the nodes established makes me question this statement. However in the transport section states the following:", "Given a publisher URI, a subscribing\n  node negotiates a connection, using\n  the appropriate transport, with that\n  publisher, via XMLRPC. The result of\n  the negotiation is that the two nodes\n  are connected, with messages streaming\n  from publisher to subscriber.", "Each transport has its own protocol\n  for how the message data is exchanged.\n  For example, using TCP, the\n  negotiation would involve the\n  publisher giving the subscriber the IP\n  address and port on which to call\n  connect. ", " The nodes exchange a\n  Connection Header that includes\n  information like the MD5 sum of the\n  message type and the name of the\n  topic, and then the publisher begins\n  sending serialized message data\n  directly over the socket.", "So my question is, are the publisher and the subscriber really decoupled? After all in order the subscriber to get a message it has to open TCP/IP connection the the publisher port. And if the publishers are ", " the subscriber needs to open ", " connections. This means that it's not completely true that the subscriber won't be bothered if one of the publishers fails. So what exactly is meant by that statement that the nodes are decoupled? "], "answer": [" ", " ", " ", " ", "tl;dr: yes, there are direct connections between nodes (so they need to know each others 'names' (IP addresses)), but that is only for efficiency reasons, and any dropped TCP connections between nodes will be gracefully handled by the ROS middleware (and thus invisible to nodes at the application/user level of the API).", "While you are correct that there are TCP connections between nodes, I don't think that that is what the ", " in the first quote you included refers to.", "The following is an excerpt from the paper ", " by Patrick Eugster, et al., which for me is the clearest description of ", " I know (you can read ", " here as being equivalent to a middleware):", "The decoupling that the event service provides between publishers and subscribers can be decomposed along the following three dimensions (Figure 2):", "It appears that the ROS middleware does not implement these aspects on all levels, as you mentioned. At the application level (where you interact with the ROS C++/Python/X API), nodes do not care about the names of other nodes, only about topics. There we have ", ".", "But the data will have to get to subscribers in some way. ", " uses TCP for that (in the case where ", " is used) and for efficiency reasons does that by direct connections between nodes (otherwise data would have to be copied from publisher to intermediate entity, then to each subscriber). So at the ", " level in ROS, there is no space decoupling (with the default ", ").", "ROS does not currently seem to support ", ", as subscribers and publishers need to be running for messages to be exchanged. This seems a consequence of the direct TCP connections between nodes: if one of the endpoints of those connections isn't \"up\", messages will ...", "Fabulous answer, much appreciated, ", "!", "The only question that arise in your answer is - since nodes are establishing TCP/IP Connection to each other to improve efficiency, this implies that a given subscriber has to open ", " TCP/IP connections in order to get messages from ", " publishers. Then the Space decoupling is also questionable.", "Well yes, it implies that you need ", " connections if you have ", " subscriptions to different publishers. But as I wrote: you need to get the data across somehow.", "There have been experiments with other message transports (ROS terminology), such as UDP multicast and shared memory (see ", " fi), and although these don't use ", " connections, they have their own drawbacks (routability, etc).", " If they need to make direct connection, it means that they will know about each other info/reference, so it is not space decoupling? Also, when making/opening connections, the publisher will know how many subscribers by the number of connections. Please explain , thanks", " you wrote \"Space decoupling: The interacting parties do not need to know each other\", but if they make connection then they know each other. I'm new to ROS, so I'm a bit confused here. pls explain :)", "I believed I had explained that in my answer (did you click ", "?) but: yes, at ", ", there is ", " space coupling in ROS, as the TCP/IP connections will need to be established somehow. At the abstraction level where it matters though (user code), there is ", ".", "And: the Eugster et al. article (that I only quoted from) is not specific to ROS, and the dimensions of decoupling proposed in it work for all (?) middlewares making use of the considered interaction styles. So you'll find (some of) the same 'problems' in OROCOS, YARP, etc. Also in DDS and OPC-UA."], "answer_details": [": The interacting parties do not need to know each other. The publishers publish events through an event service and the subscribers get these events indirectly through the event service. The publishers do not usually hold references to the subscribers, neither do they know how many of these subscribers are participating in the interaction. Similarly, subscribers do not usually hold references to the publishers, neither do they know how many of these publishers are participating in the interaction.", ": The interacting parties do not need to be actively participating in the interaction at the same time. In particular, the publisher might publish some events while the subscriber is disconnected, and conversely, the subscriber might get notified about the occurrence of some event while the original publisher of the event is disconnected.", ": Publishers are not blocked while producing events, and subscribers can get asynchronously notified (through a callback) of the occurrence of an event while performing some concurrent activity. The production and consumption of events do not happen in the main flow of control of the publishers and subscribers, and do not therefore happen in a synchronous manner.", " ", " ", " ", " "], "answer_code": ["ros_comm", "TCPROS"], "url": "https://answers.ros.org/question/203129/ros-nodes-and-topics-are-nodes-really-decoupled/"},
{"title": "robot_localization tf tree structure", "time": "2015-08-31 19:28:28 -0600", "post_content": [" ", " ", "I am having trouble getting robot_localization to work.", "I have read through the robot_localization wiki, the tf wiki and the REP-105/103 doc but I still don't understand how to structure the tree for robot_localization.", "I have 2 frames_ids from IMU and PTAM messages.", "Concepts i don't understand:", "The only way I have ever managed to get robot_localization to produce messages in the /odometry/filtered topic is to use the sensor frames directly but this doesn't seem the correct way to do it.", "Any example code or step by step instructions would be great. Docs have not helped me with my confusion. I just don't understand how to piece it all together."], "answer": [" ", " ", "I think this is really more of a ", " question. Let's ignore sensors and such for a second and just focus on two coordinate frames, ", " and ", ". When your robot starts out, it is at position (0, 0) in the ", " frame. It then drives, say, four meters forward, turns left, and drives six meters. Its position in the ", " frame is now (4, 6). This information can be expressed in two ways:", " produces both (1) and (2).", "Now let's say we have an IMU on your robot, but instead of facing forward, we turn it sideways. Additionally, we mount it 0.3 meters to the left of the robot's centroid. We can represent this offset as a transform, and use the ", " package to continuously send out a transform from ", " to your IMU's ", " (e.g., ", "). Now, before we can use that IMU data, we need to make sure to transform it into the correct coordinate frame. In this case, ", " uses the ", " libraries to look up what the transform is between ", " and ", ", and then \"corrects\" (transforms) the IMU data into the robot's body (base_link) frame, and then passes it on to the filter to be fused.", "Now let's say you have a LIDAR on your robot, and you define a ", "->", " transform using ", ". Your LIDAR detects an obstacle 10 meters away at 30 degrees. You have a node somewhere that needs to know where that obstacle is in the ", " frame and not the ", " frame, so it uses the ", " libraries to look up a transform from ", " to ", ", which produces the location of that obstacle in the ", " frame.", "Note that you can name the frames whatever you want. REP-105 just gives suggestions that are commonly used. For PTAM, you can see if PTAM lets your specify the ", ". If it does, you can set it to odom. If not, you can either (a) just set the ", " and ", " parameters to ", ", or you can specify a static transform of 0 between ", " and ", ".", "Yes! Thanks Tom, that has helped a lot. Now to try and put the theory to use...", "Great I think it's working now. Now I'm just getting some major drift, but that's another story ;)\nCan't thank you enough for your help, you have saved me hours of frustration.", "No problem! Good luck."], "answer_details": ["A ROS message (in this case, nav_msgs/Odometry) that contains the robot's world pose (and velocity, but let's ignore that for now). We fill out this message with the robot's current pose and send it off to be consumed.", "A transform from the ", " frame to the ", " frame. This contains the exact same information, but is sent via ", " instead.", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Where do these map, odom, base_link frames come from? Do I generate those using tf as a virtual representation of the real world sensors and their relative positions and orientations?", "If so how do I link the supplied frames to the tree I create? In the doc it says map -> odom -> base_link. Do I also link base_link -> imu and odom -> ptam?", "Finally do I supply robot_localization with the odom and base_link or imu and ptam frames?"], "answer_code": ["tf", "tf", "robot_localization", "static_transform_publisher", "frame_id", "robot_localization", "tf", "static_transform_publisher", "tf", "frame_id", "odom_frame", "world_frame"], "url": "https://answers.ros.org/question/216826/robot_localization-tf-tree-structure/"},
{"title": "Using a joint with motor on other than parent link thru Transmission", "time": "2016-02-10 03:52:10 -0600", "post_content": [" ", " ", " ", " ", "I have made a custom robot arm and I'm making an URDF file for that.\nsituation:", " I've searched in the tutorials on URDF and \"transmissions\" came up.\n ", "Are there some examples or best practices on how to set up this situation in the URDF model so I can have correct behaviour in Moveit etc?", "edit:\nWith correct behaviour I mean generated trajectories that match the situation", "Thanks in advance,\nBas"], "answer": [" ", " ", "Tbh: I don't think you necessarily need to use ", "s for this.", "Transmissions can be useful, but I'm not sure ", " useful if you have a simple position controlled manipulator that you wish to make ROS-compatible.", "The level at which most control nodes / ", " controllers interact with hardware of this kind is at the (abstract) joint level, where the specifics about ", " that joint is finally controlled (and through which mechanism) isn't necessarily available / important. As long as the set of joints you model in your URDF (and thus 'expose' to the rest of ROS (ie: MoveIt, etc)) can be controlled in some way to execute the planned motion, things should just work", ".", "Things to check / take care of (off the top of my head):", "the reason I look into this, is that the positions of the JointTrajectoryPoint need to be postprocessed otherwise. For example when the lower arm (+ all above) rotate, the angle of the upper arm wrt the lower arm doesn't change. But physically not rotating the motor while moving the lower arm does.", "Sounds similar to joint-2-3 coupling of some industrial robots (see ", " fi). One approach would be to handle this in your lower-level controller, and expose only 'clean' joints to ROS.", "Also: I'm not sure ", "s can be used to express that relationship (but I'm by no means an expert). This sounds like something a ", " joint could do, but afaik those cannot actually be actuated. Hence the approach to abstract such relationships and delegate to lower-level controllers.", "Thanks, Although it feels a bit unnatural I'll use yout proposed approach and do the math in the low level controller. I'll leave the other possibilities (and lack of transmission examples) for another time.", "Well another approach could be to handle the joint linkages in a custom ", " controller or the hardware interface plugin. Personally I'd put it at the lowest level of abstraction that can comfortably deal with it, so as to shield it from higher layers. Let's you reuse as much as possible."], "answer_details": ["make sure all ", " definitions in ", "s correspond to your hardware (ie: ", " for \"positive angle increase maps to positive rotation of frame\" and ", " for the inverse)", "have a controller capable of consuming ", " messages that is able to map between URDF ", " names and your hardware / lower level controller (either something custom or using any one of the available abstractions (", " fi))", "make sure origins of URDF ", "s correspond with their locations in your real system (don't just assume things are ok because your EEF frame ends up in the right Cartesian location)", "define position, velocity, acceleration and effort limits correctly: MoveIt fi uses those to not only avoid planning invalid paths, but also when populating the ", ", ", " and ", " members of ", " messages", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "lower arm, mounted on base ", "upper arm mounted on lower arm ", "motor for upper arm mounted on base", "motor and upper arm connected via belt."], "answer_code": ["transmission", "ros_control", "axis", "joint", "1", "-1", "JointTrajectory", "joint", "ros_control", "joint", "velocity", "acceleration", "effort", "JointTrajectoryPoint", "transmission", "mimic", "ros_control"], "url": "https://answers.ros.org/question/226204/using-a-joint-with-motor-on-other-than-parent-link-thru-transmission/"},
{"title": "Turtlebot 2 kinect type(xbox360 version or PC version?)", "time": "2016-05-22 23:05:25 -0600", "post_content": [" ", " ", "Hi, I'm tring to build a turtlebot 2 by myself instead of buying one on Clearpath.\nWhen considering buying a kinect, there're 2 types available, one for xbox360, and another PC version.\nWhich one should I take? Or do they both work fine."], "answer": [" ", " ", " ", " ", "I bought once an old Kinect for XBOX 360 for less than 80$ and it worked fine.", "In the Microsoft website, they said to prefer Kinect for PC because it contains improvement on the SDK and the API... but, naturally, this the Windows SDK, not the Linux one, which means that it's the same for ROS users to use Kinect for XBOX or Kinect for PC (unless you are using ROS for Windows which is not a stable version)", "My advice, if you want to buy a new Kinect, is to prefer the Kinect v2 for XBOX ONE, which is better than Kinect v1 for XBOX 360 :", "And of course, this version is ", "The actual ", " including the USB adapter is 150$.", "If you prefer to although to buy Kinect for XBOX 360, make sure it includes the USB adapter. ", "EDIT:", "Thanks a lot for your answer and advice. It's really helpful. I've been always wondering wether Kinect v2 will work with ROS or not, cause there really wasn't much mention about this in the Turtlebot tutorial.", ": here are several information just wrong! Kinect v1 supports 1280x960@<15fps, too. Kinect v1 has a Title motor, Kinect v2 does not! The effective resolution of Kinect v1 is about 220x170. Using Kinect v2 on a robot is a bad idea: extreme noise, reflections and more power consumption.", "Ah thank you ", " for these informations. To be honest, I never tried v2 so I didn't know for the noise and reflections. Theorically it seems good, but in practice, as you said, we need to deal with extra power consumption, and extra computations.", ": I had high expectations, too. But I'm very disappointed by Kinect v2. Kinect v1 has a low spatial resolution but due to structured light you get at each position good data or no data, but never wrong data.", "Further disadvantage of Kinect v2: The distances depend on the surface color, too. Dark surfaces have a larger distance.", "Ah i see ", ", thanks a lot, I was planning to buy one! In their website, Microsoft said that it can track more articulations (I was planning to use it for detecting gestures). But if the distances are not so accurate, it will lead to false mapping I suppose?", ": It depends very strong on material and angles. For detecting gestures the kinect v2 should be fine. Alternatively you can take a look at the new Intel SR300, which was build for detecting gestures in short range (<120cm).", "Ah yes i've seen the Intel SR300, but still the short range is a problem in my case and I think Kinect is more suitable for a mobile robot since it can detect the full body joints from a range of 3 to 4m"], "answer_details": ["HD resolution: 1960x1028 30fps (compared to 640x480 for v1)", "Larger range depth sensor: 512x424 (compared to 320x240)", "Larger field of view: 70x60 degrees (compared to 57x43)", "USB 3.0 support", "It seems that v2 doesn't include a tilt motor anymore!", "Don't forget that v2 has more power consumption than v1", " ", " ", " ", " "], "url": "https://answers.ros.org/question/234963/turtlebot-2-kinect-typexbox360-version-or-pc-version/"},
{"title": "Robot_localization diagnostic question #2", "time": "2016-05-14 15:48:34 -0600", "post_content": [" ", " ", "I'm still underway attempting to fine-tune an instance of (ekf) robot_localization using gps and single imu devices.  The diagnostic message for the ekf filter \"appears to be functioning properly.\" However the next message states \"odometry/filtered topic status message:  Frequency too low.\"  The value is less than 5Hz with the frequency of both the ekf and the navsat transform are at 60 Hz.  The imu delivers at around 55 Hz (set at 16ms ~ 62.5 Hz) using the ImuFilterNodelet of  imu_filter_madgwick; so there is some latency.", "My goal is to get as close as possible to 60 Hz to enable the gps/filtered topic to deliver a data set that allows around this frequency for post processing sync with dynamic images.  How can I rectify the filtered topic status with this latency?", "Thank you as always,\nB2256", "Can you please post sample inputs and config files? The EKF can run at hundreds of Hz even with multiple inputs, so I suspect that something else is amiss.", " Thanks Tom.  Here is the dropbox link:   ", "Still getting low output when initiate bag recording; bandwidth on the Beaglebone Black perhaps.", "The link only appears to have launch files. Do you have sample input messages?", "Input messages are now on the above dropbox link as \"TRIAL007_INPUT.odt\".  Still suspecting bandwidth issue with the Beaglebone Black, but hopefully some launch file or configurations can be identified for the source of error?  Thank you Tom."], "answer": [" ", " ", "I'm not sure what's causing your trouble, as I don't really see anything wrong with the data. However, you may want to look into the following:"], "answer_details": ["Since ", " your high-frequency data appears to fall victim to decreased frequency when bagging, can you try bagging to another disk? It seems like very strange behavior to me, and unrelated to ", ".", "Can you run ", " or ", " to see if something is consuming a lot of CPU? Also, let me know how much CPU ", " is using.", "I noticed in your config file that the ", " flag and corresponding path are specified, although I recognize that ", " is set to false. Just an FYI: if you're running live and turn on debug mode, it's going to eat a lot of CPU and slow down the filter.", "Are you running from binaries? If so, which version? If you're running from source, which version, and did you build in release mode?", " ", " ", " ", " "], "answer_code": ["r_l", "top", "htop", "r_l", "debug", "debug"], "url": "https://answers.ros.org/question/234361/robot_localization-diagnostic-question-2/"},
