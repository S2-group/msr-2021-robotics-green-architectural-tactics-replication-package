[
{"title": "Stereo vs mono visual odometry (CPU power)", "time": "2017-09-12 16:06:48 -0600", "post_content": [" ", " ", "Hi,", "I'm trying to choose a suitable visual odometry package that would need as less CPU resources as possible. I'm not so studied in this stuff and don't have stereo camera to compare with, just a mono camera. So my question is: which algorithms need less CPU resources, mono or stereo ones? Is it a big difference?"], "answer": [], "url": "https://answers.ros.org/question/270800/stereo-vs-mono-visual-odometry-cpu-power/"},
{"title": "Is this motherboard/CPU powerful enough for Kinetic image processing?? [closed]", "time": "2012-12-16 12:46:38 -0600", "post_content": [" ", " ", " ", " ", "I was told here that the dual core atom D525 was not good enough for kinetic imaging processing and mapping and controlling arm navigation, that it may be to slow for it.. Im wondering how would the E-350 AMD perform?? is it the same as the atom D525?", "Im looking at this: www.newegg.com/Product/Product.aspx?Item=N82E16813157228", "I will use an arbotix board to control servos, and the board with ROS to process kinetic image, and move its built in servo", "this is the post: answers.ros.org/question/50327/new-to-ros-want-to-build-a-robot/ "], "answer": [], "url": "https://answers.ros.org/question/50603/is-this-motherboardcpu-powerful-enough-for-kinetic-image-processing/"},
{"title": "Kinect power circuit working well with turtlebot but does not works with other irobot create base, why? [closed]", "time": "2013-08-01 23:47:04 -0600", "post_content": [" ", " ", " ", " ", "I wonder the circuit board designed by the clear path robotics to connect the turtlebot kinect to the DB25 connector of turtlebot base (Irobot create base) is not working with other independent Irobot create base. ", "I am amazed why it happens so ......... where I robot create is same in both the condition. Can any one tell me what is the issue."], "answer": [], "url": "https://answers.ros.org/question/70663/kinect-power-circuit-working-well-with-turtlebot-but-does-not-works-with-other-irobot-create-base-why/"},
{"title": "kinect green light keeps freshing. Seems no driver", "time": "2013-09-25 19:54:27 -0600", "post_content": [" ", " ", " ", " ", "I have installed freenect from", "and also blacklist gspca_kinect(I am using Ubuntu 12.04 LTS)", "But when I try the command ", "It says that", "and then I try \n                                lsusb", "it shows me like", "These are kinect's and I could not find something like camera or others.", "Could you please help me? Since for this Sept I could not even run kinect on Ubuntu \nand I feel confused.", "Thank you so much and I will wait for the reply.", "I'm having this problem too. Where you able to find a solution?", "Does it work using openni_launch? I'm using Ubuntu 12.04 with Hydro and not having any issues with my Kinects.", "Do you mean that you can launch Kinect successfully? What is your version of Kinect?\nCould you plz leave your email and we could discuss directly.", " please post your comment as an answer in order to be able to mark this question properly as closed. Thx."], "answer": [" ", " ", "It turns out that it was the specific kinect. Changing the kinect seemed to fix it. I'll have another chance to use it on Tuesday or Thursday so I can grab some more information and test it. ", ", how would i grab the version? feel free to email me at ", " in the next 48hours.", "Did you still use a Kinect for Windows or chaged for a Kinect for XBOX? I'm having the exact same problem with a Kinect for Windows 1517.", "Any update, anyone?"], "question_code": ["                sudo apt-get install ros-hydro-freenect-stack\n", "                roslaunch freenect_launch freenect.launch\n", "                [ INFO] [1380174487.842752473]: No devices connected.... waiting for devices to be connected\n", "               Bus 002 Device 015: ID 045e:02c2 Microsoft Corp. \n               Bus 002 Device 016: ID 045e:02be Microsoft Corp. \n               Bus 002 Device 017: ID 045e:02bf Microsoft Corp.\n"], "url": "https://answers.ros.org/question/83391/kinect-green-light-keeps-freshing-seems-no-driver/"},
{"title": "Error gazebo 1.9.1 loading controller manager + power monitor [closed]", "time": "2013-11-07 20:32:11 -0600", "post_content": [" ", " ", "Hey,", "I have some problems with gazebo 1.9.1 . I'm using ros groovy with ubuntu 12.04lts. I try to get gazebo to work with a model of the ur10 from universal robots. I installed gazebo as described here gazebosim.org/wiki/Tutorials/1.9/Installing_gazebo_ros_Packages.", "Here is my problem:\nWhen i launch the robot to gazebo i'm getting the following error messages. ", "In the folder catkin_ws/devel/lib i can only find ", " , there is no ", ".", "Thank you, in advance.\nEqua"], "answer": [], "question_code": ["Error [Plugin.hh:127] Failed to load plugin libgazebo_ros_controller_manager.so: libgazebo_ros_controller_manager.so: cannot open shared object file: No such file or directory\n\nError [Plugin.hh:127] Failed to load plugin libgazebo_ros_power_monitor.so: libgazebo_ros_power_monitor.so: cannot open shared object file: No such file or directory\n"], "url": "https://answers.ros.org/question/98921/error-gazebo-191-loading-controller-manager-power-monitor/"},
{"title": "Figuring out the power of a YAML file.", "time": "2014-01-29 06:15:08 -0600", "post_content": [" ", " ", " ", " ", "I have no prior experience in writing a YAML file. I want to know something related to it's capability.", "Suppose,", "* instead I need to ", " So, I want ", " ", "Let's hope that above scenario is possible. ", " ", "Thanks in advances for your valuable suggestions... "], "answer": [" ", " ", "Background: YAML is a generic file format, and can be used to specify many, many things. In this particular context it's used to specify the configuration of the ", " nodelet, but be aware that you will see the YAML format used elsewhere for completely different things.", "I will confine my answer specifically to using YAML to configure the ", " nodelet. ", "1) As far as I'm aware, you can't reference outside data from a YAML file. That said, a YAML file ", " a text file, so it's easy to modify.", "2) The ", " node will not automatically pick up changes to the yaml file. It does have an interface through ", " that can be used to load a new configuration file.", "Further things to consider:", "The ", " nodelet is a nodelet; this means that it needs to be run within a nodelet manager. It's usually run as part of the core Kobuki/Turtlebot2 launch, but if you're trying to use it on another robot, you'll need to learn a little about nodelets and how to run a nodelet manager in order to use it.", "If you're trying to change the priorities of the input topics dynamically at run time in order to force a particular input, you're probably designing your system wrong. The purpose of a multiplexer like this is to prioritize and choose the input topic automatically; if you set the priorities correctly on startup, the mux should automatically choose whichever active topic has the highest priority.", "If you're looking for a more generic mux that doesn't automatically do topic prioritization and switching, have a look at the ", ".", ", thanks for your comment. I am wondering why nodlet (", ") is required for   implementing mux.yaml? I have seen here (", "); which use nodelet for mux.yaml. Since I am using P3AT robot with lms200 laser range finder; is it possible to achieve desired behavior of mux.yaml. As we all know that ROS autonomous module has limited chance of failure, then what is the point of setting timeout? How we know that mux.yaml has worked properly. Right now, after completing the goal set in RVIZ, I can opt for teleop without using mux.yaml. But problem arises when we try to go for teleop without completing the goal set in RVIZ", "YAML is a data description format and is not runnable; it merely defines the configuration for something else.", " ", " ", " ", " ", "Concerning YAML and variables (your ", "), you can use anchors (&) and references (*). This makes modifying YAML files easier when the same data is used multiple times in the same YAML file (anchors/references ", "). In your case, this could look like:", "subscribers:\n- name:        \"Teleoperation\"\n  topic:       \"input/teleop\"\n  timeout:     1.0\n  priority:    *priority1\n- name:        \"Autonomouse Navigation\"\n  topic:       \"input/move_base\"\n  timeout:     2.0\n  priority:    *priority2\n", "You can check the result ", ". However, if you want to achieve a more complex behavior, there is no equivalent to ", " in YAML, but you could generate YAML files with a Python script, it's quite simple and you could use a modified version of this:", " Thank you for the whole information. In the first answer ", " mention that YAML file can't access the outside data. The yaml file you have written gives the priority in the yaml file itself. Based on your info I think its better to write python script which access external text file (for priorities) and creates a yaml file.", ": this is what I would do, since conversions between YAML and Python are really straightforward."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Suppose in move_base.launch file, I have written something which launch mux.yaml ( ", " ). Which look like below:"], "question_code": ["subscribers:\n- name:        \"Teleoperation\"\n  topic:       \"input/teleop\"\n  timeout:     1.0\n  priority:    7 \n- name:        \"Autonomouse Navigation\"\n  topic:       \"input/move_base\"\n  timeout:     2.0\n  priority:    4\n"], "answer_code": ["yocs_cmd_vel_mux", "yocs_cmd_vel_mux", "yocs_cmd_vel_mux", "yocs_cmd_vel_mux", "#!/usr/bin/env python2\nimport yaml\n\ndef generate_subscriber(s_name, s_topic, s_timeout, s_priority):\n    return dict(name = s_name,\n                topic = s_topic,\n                timeout = s_timeout,\n                priority = s_priority)\n\ndata = dict(\n    subscribers = tuple([\n        generate_subscriber(\"Teleoperation\", \"input/teleop\", 1.0, 7),\n        generate_subscriber(\"Autonomous Navigation\", \"input/move_base\", 2.0, 4)\n    ])\n)\n\nwith open('data.yaml', 'w') as outfile:\n    outfile.write(yaml.safe_dump(data, default_flow_style=False))\n"], "url": "https://answers.ros.org/question/124076/figuring-out-the-power-of-a-yaml-file/"},
{"title": "can not start ipacan_open tutorial examples on powerball", "time": "2014-05-12 11:33:23 -0600", "post_content": [" ", " ", " ", " ", "Hello, \nOn Ubuntu 12.04 (64 bit ) i have installed ROS Groovy and packages:\nhttps ://raw.github.com/ipa320/schunk_robots/groovy_dev/groovy.rosinstall\nand ipa_canopen_tutorials.", "After lauching file i get this output :", "canopen_ros: /usr/include/boost/smart_ptr/shared_ptr.hpp:418: T* boost::shared_ptr<t>::operator->() const [with T = const urdf::Joint]: Assertion `px != 0' failed."], "answer": [], "url": "https://answers.ros.org/question/164031/can-not-start-ipacan_open-tutorial-examples-on-powerball/"},
{"title": "powerlink on ROS", "time": "2014-10-25 07:52:47 -0600", "post_content": [" ", " ", " ", " ", "hi at all,", "i wanna connect my industrial robot via Powerlink to my Laptop, which works as ROS-Master and provides the motion plans. So my question is, is there already a Powerlink interface existing? Or have somebody experience with industrial networks on ROS especially Powerlink? Or is it possible to use the Simple Message on Powerlink?"], "answer": [" ", " ", " ", " ", "Short answer: no, afaik, no (publicly available) interfaces to Powerlink networks exist for ROS(-Industrial).", "Longer: I don't know of any publicly available nodes for interfacing with Powerlink networks. It would be a nice addition to ROS-Industrial though, as it would make it possible to interface with any Powerlink compatible devices / actuators, and a number of currently unsupported robots that provide Powerlink interfaces.", "If you have an actual programmable (embedded) controller running at the other end of your Powerlink connection, you could theoretically implement a ", " server application and use the Simple Message protocol for everything (and only use Powerlink as your transport, instead of TCP/IP). Using ", " over Powerlink was not something that was considered when the protocol was first designed, but could be done.", "I think though, that it would make more sense to create a Powerlink interfacing node (bridge) for specific devices on your network (similar to how the ", ", ROS Ethercat (using ", ") and the various ROS CAN wrappers work). As soon as you have access to your encoders and actuators though such a (set if) ROS node(s), you could then control them from a ", " controller, or something similar. That could then be accessible to other parts of ROS, such as MoveIt.", "Another approach could be to create OROCOS components, which could then also be integrated with ROS.", "You might want to send a message to the ROS-Industrial ", " about this as well. Perhaps someone has implemented something, but hasn't announced it yet. If you are interested in creating something yourself, I'm sure there will be other interested members, which could potentially support you.", " ", " ", " There are several packages for interfacing with Schunk robots. The first one I found searching the ROS wiki is:  ", "Tully, are you perhaps confusing PowerCube with PowerLink?", "Sorry, I assumed the PowerCubes talked over PowerLink. See ", "'s answer."], "answer_code": ["simple_message", "simple_message"], "url": "https://answers.ros.org/question/195910/powerlink-on-ros/"},
{"title": "Create base powers off when launching turtlebot_bringup minimal.launch, how is this fixed?", "time": "2016-07-26 12:58:03 -0600", "post_content": [" ", " "], "answer": [], "question_code": [" In my lab we're using a create base turtlebot, and each time we try to run turtlebot_bringup minimal.launch the create base simply powers off.  Upon further inspection we discovered that it turns off after the handshake of the serial port communication. Is this a known bug? Does anyone have any idea how to fix this? We have two turtlebots with create bases and they both have the same problem. \n\n We are able to run the turtlebots and leave them on for long periods of time, so the batteries are fine. It's only when we run the minimal.launch that they then power off. Any help is appreciated!\n"], "url": "https://answers.ros.org/question/240442/create-base-powers-off-when-launching-turtlebot_bringup-minimallaunch-how-is-this-fixed/"},
{"title": "arbotix output power", "time": "2017-04-12 09:17:04 -0600", "post_content": [" ", " ", "I want to toggle a lamp from my arbotix. The lamp needs 5V and aprox. 20mA. I was trying to get some information about max. current, etc. in the documentation but there is really no information about it. Can I use for this purpose digital output pins?", "This doesn't really seem ROS related. I think these boards are sold and supported by Trossen Robotics, so you might have better luck if you ask on the Trossen Robotics forums."], "answer": [], "url": "https://answers.ros.org/question/259098/arbotix-output-power/"},
{"title": "initializing powercubes not successful. error: Could not reset module # during init.", "time": "2017-10-11 13:05:45 -0600", "post_content": [" ", " ", "Hi,", " I am trying to make PW70 V6 work on ROS using this(  ", "  ) package. ", "I could be able to build this package, but I cannot initialize PW70.", "First of all, I changed the ", " file for my CAN setting and module ids as below.", " (I guess this means 500k, right?)", "...", "...", "Then, I ran ", " and ", " and clicked the 'init' button on the dashboard.", "Then it shows the error like this.", "Does anybody know how to resolve this problem?", "I am using IXXAT CAN module and running ROS indigo.", "The unit works when I manually send a CAN message using ", ", but I want to operate PW70 on ROS using this package.", "Thanks in advance,\nHanjun", "Hello, did you have any success with this? I am trying to solve the same problem"], "answer": [], "question_code": ["config/ptu_driver.yaml", "can_baudrate: 500", "can_device: can0", "can_module: SOCKETCAN", "modul_ids: [13, 14]", "robot.launch", "dashboard.launch", "[ INFO] [1507700589.955168930]: ...initializing powercubes not successful. error: Could not reset module 13 during init. Errorcode during reset: 1 Try to init once more.", "cansend"], "url": "https://answers.ros.org/question/272795/initializing-powercubes-not-successful-error-could-not-reset-module-during-init/"},
{"title": "Cannot build power_supplies (rosmake) package", "time": "2017-11-27 03:20:15 -0600", "post_content": [" ", " ", " ", " ", " Newbye here. Hi all.\nI'm trying to compile this ros project ( ", " ) in my Elementary OS Loki (Ubuntu 16.04). ", " I correctly install ROS following this guide ( ", " ); I created build folder, cmake .., but make doesn't show anything and I cannot create a binary file. ", "What should I do? Thanks", "EDIT: \nOk let's start saying that it's my first ROS attempt and I have to compile a project that I have no knowledge of :/ It seems to be a ROS project than I'm here to ask help :) Thanks in advance.\nNow, I removed the old custom cmake build folders and after installing and setting env I did", "I cannot just catkin_make because it says:", "This is the output:", "Finally, ", "doesn't output anything and in my build folder I have:", "I created build folder, cmake ..", "this confuses me: that is not a typical ROS development practice.", "Can you add some more detail to your post describing what it is that you did/do ", "?", "Guessing: do you have a compiler installed? What is the output of ", " and ", "?", "Of course I have compiler installed: gcc/g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609", "If that is the case then you're going to have to show us the output of ", ".", "Just to reiterate: running ", " and ", " yourself is not how you typically build ROS pkgs.", "I edited the original post with catkin_make output and more info", "Two things:", "For 2) see ", ".", "For 1): if you really want/need to use the package, either build it using ", " or convert it to a Catkin package. If you're just starting out, neither is very simple.", "For future proofing this, I'd convert the pkgs to Catkin: ", ".", "Actually I only want to compile them (ctx2140 and m4atx monitors) in order to use them in a Ubuntu 16.04 env. Since you have a clearer vision than me, what's the best way to do it? Thank you for the patience", "I would take those packages and convert them into Catkin pkgs. See the migration tutorial I linked you to."], "answer": [], "question_details": [" ", " ", " ", " ", "the package you link to is not a Catkin package. So ", " and friends will not touch it", "ROS workspaces are not package directories. They are directories ", " package directories."], "question_code": ["gorgo@dago:~/power_supplies$ catkin_make --source .\n", "The specified source space \"/home/gorgo/power_supplies/src\" does not exist\n", "Base path: /home/gorgo/power_supplies\nSource space: /home/gorgo/power_supplies\nBuild space: /home/gorgo/power_supplies/build\nDevel space: /home/gorgo/power_supplies/devel\nInstall space: /home/gorgo/power_supplies/install\n####\n#### Running command: \"cmake /home/gorgo/power_supplies -DCATKIN_DEVEL_PREFIX=/home/gorgo/power_supplies/devel -DCMAKE_INSTALL_PREFIX=/home/gorgo/power_supplies/install -G Unix Makefiles\" in \"/home/gorgo/power_supplies/build\"\n####\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.12\") \n-- Configuring done\n-- Generating done\nCMake Warning:\n  Manually-specified variables were not used by the project:\n\n    CATKIN_DEVEL_PREFIX\n\n\n-- Build files have been written to: /home/gorgo/power_supplies/build\n####\n#### Running command: \"make -j8 -l8\" in \"/home/gorgo/power_supplies/build\"\n####\n", "gorgo@dago:~/power_supplies/build$ make -j8 -l8\n", "gorgo@dago:~/power_supplies/build$ ls\ncatkin_make.cache  CMakeCache.txt  CMakeFiles  cmake_install.cmake  Makefile\n", "which gcc", "which g++", "catkin_make", "cmake ..", "make", "catkin_make", "rosmake"], "url": "https://answers.ros.org/question/276734/cannot-build-power_supplies-rosmake-package/"},
{"title": "Running Turtlebot minimal.launch on a PC, no battery state.", "time": "2018-03-18 13:55:30 -0600", "post_content": [" ", " ", "Trying to connect to a kobuki, but with a PC rather than a laptop, for testing reasons.", "When trying to execute ", "I get the warning", "Which is understandable, as on a PC that folder does not exist. However, this causes ROS to hang indefinitely.\nIs there any way to work around this?"], "answer": [], "question_code": ["roslaunch turtlebot_bringup minimal.launch\n", " [WARN] [1521399651.235331]: Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]\n"], "url": "https://answers.ros.org/question/285774/running-turtlebot-minimallaunch-on-a-pc-no-battery-state/"},
{"title": "Is Raspberry Pi 3 enough powerful for the advanced slam algorithm", "time": "2019-05-16 03:06:38 -0600", "post_content": [" ", " ", "Hi all,\nSo far all the slam algorithm I am testing on turtlebot3 are run on laptop computer (which usually need RViz UI).\nI am thinkig if it is possible to transfer these program to turtlebot3 on-bord computer (Raspberry Pi 3, but with ARM64) but maybe need remove the RViz from some roslaunch file..", "Do you think this is possible? \nAlso, is there any concern to Raspberry Pi 3's capability to handle the SLAM algorithm?", "Thanks a lot!", "Mac"], "answer": [], "url": "https://answers.ros.org/question/323321/is-raspberry-pi-3-enough-powerful-for-the-advanced-slam-algorithm/"},
{"title": "Advice for IMU and GPS powered outdoor path following", "time": "2020-02-06 15:47:19 -0600", "post_content": [" ", " ", "Hello,", "I am developing an autonomous lawnmower (its a large zero-turn mower.) I have a basic understanding of ROS and have programmed some smaller projects with it, but I am not sure what packages to turn to for this project. My project is very simple because the target field is empty of obstacles. I simply want to mow the field once with centimeter accurate GPS and and then have the mower repeat the same path without missing any grass. The move_base and navigation stack options I have seen seem overly complicated for my simple problem. ", "Although this isn't part of the question, just for reference, I control the two steering arms with actuators. I am using ROS1 Kinetic on Ubuntu 16. I have hall-effect sensors in the wheels but have struggled to get accurate data from them.", "My best understanding at this point is that I could use robot_localization to publish the GPS coordinates as the actual location of the mower, but what path following node can I use? I don't want to plan any new path or look for obstacles ( I actually have a cheap LiDAR already programmed separately for safety, it checks for people or animals walking in the front of the mower)", "My current plan is to write my own controller that follows a reset path, but I am beginning to think that I would be wasting my time because there are probably better solutions that I just don't know about already written.", "Any advice is appreciated! Thank you!", "This sounds a lot like ", " (but smaller scale, obviously).", "The terminology you are looking for (I believe) is \"pure pursuit\".", "There should be a nr of packages that provide those kinds of controllers.", "Thank you for the tip on \"pure pursuit\" and the other question, that is what I am looking for"], "answer": [" ", " ", "If there are features you can map against, I would highly advise to use the lidar to localize the robot. The GPS units in the <$100 range are not centimeter accurate to my knowledge, in my experience, a ublox gps which is what quadcopters use are good to several meters (3m radius or diameter). Once localized, the IMU sensor measurement drift should be insufficient for you to deviate much from a path, along with using the GPS to provide geo-fencing. ", "Thank you for your response, I am using two Emlid Reach RS+ units with RTK to get the centimeter accuracy. My lidar has a view of 12 meters, but my target field is 2 acres and the lidar wouldn't see any features most of the time."], "url": "https://answers.ros.org/question/343494/advice-for-imu-and-gps-powered-outdoor-path-following/"},
{"title": "does rtabmap actually use computing power of jetson tx2 with opencv3.3?", "time": "2020-02-12 00:49:13 -0600", "post_content": [" ", " ", " ", " ", "I am asking about proper installation of rtabmap and its components on nvidia jetson tx2, to fully use computing power of this board.", "I read from ", ":", "To use rtabmap_ros on Jetson, you can follow the instructions above if you don't care if OpenCV is built for Tegra. However, if you want rtabmap to use OpenCV 4 Tegra, we must re-build vision_opencv stack from source too to avoid conflicts with vision_opencv stack binaries from ros (which are linked on a not optimized version of OpenCV). Here are the steps:", "...", "1) Does it means that rtabmap works properly on jetson only with opencv4?", "2) (not related to rtabmap) In nvidia sdkmanager I see that to install opencv4 I need jetpack 4.3 on ubuntu 18.04. Is there any possibility to install opencv4 on ubuntu 16.04 with jetpack 3.3, L4T 28.2.1? if yes, will rtabmap run properly in this case?"], "answer": [], "url": "https://answers.ros.org/question/343877/does-rtabmap-actually-use-computing-power-of-jetson-tx2-with-opencv33/"},
{"title": "ros3d.js - placing new object on viewer scene changes reference coordinate axes", "time": "2020-02-12 02:17:52 -0600", "post_content": [" ", " ", "Hello,", "I'm utilizing ros3d.js for visualization of turtlebot. and additionally, i intend to spawn 3d objects on top of the urdf map plane. And the map has coordinate axes displayed for reference purposes as shown below.", "But, whenever I do spawn an object (say a cube), the coordinate axes turns from an arrow to a cube (red-blue-green group).\nI've attached the resultant snapshot below.", "Would greatly appreciate your help offered.", "Best\nVishal"], "answer": [], "url": "https://answers.ros.org/question/343887/ros3djs-placing-new-object-on-viewer-scene-changes-reference-coordinate-axes/"},
{"title": "URDF, Attaching two joints to one link for a gripper", "time": "2020-01-09 13:57:06 -0600", "post_content": [" ", " ", " ", " ", "HI everyone im trying to make my own gripper , but i had a problem , how can i can attach two joints to the same link? for example LINK \"A\" attach with link c(green color) and   \"A\" attached with B , im used mimic tag but it dosnt work , its any one can help me please? here is my urdf code  AND an image of my gripper\n", "\nI'm searched about \"virtual links\" and \"dummy\" but i cant find anything, well thanks for reading this post =) hope you can help me!", "ROS VERSION : KINETIC"], "answer": [" ", " ", "The URDF format does not support closed loops of links so cannot represent a four bar linkage such as your gripper.", "There is a solution though. URDF models can describe ", " where the angle of one joint is defined to have a linear relationship to the angle of another joint. In your case one of the four revolute joints on each side of your gripper would be a normal joint, two others would be mimic joints, and the final joint wouldn't need adding at all. This is not an ideal solution and only works for parallel four bar linkages, but it will work in your case.", "Here's an example of this being used in a ", ".", "Thank's for your fast response , i will try it!"], "question_code": ["<?xml version=\"1.0\"?>\n<robot name=\"gripper\">\n    <link name=\"world\"/>\n                            <!-- BASE -->\n    <link name=\"base_link\">\n        <visual>\n            <geometry>\n                <mesh filename=\"package://odam_arm/meshes/GRIPPER_DAE/base.dae\" scale=\"1 1 1\"/>\n            </geometry>\n        </visual>\n        <material name=\"blue\">\n            <color rgba=\"${0/255} ${0/255} ${255/255} 1.0\"/>\n        </material>\n        <origin rpy=\"0.0 0.0 0.0\" xyz=\"0.0 0.0 0.0\"/>\n    </link>\n\n\n    <joint name=\"fixed_joint\" type=\"fixed\">\n        <parent link=\"world\"/>\n        <child link=\"base_link\"/>\n    </joint>\n\n    <!-- LEFT PART BASE ACTUADOR  -->\n\n    <link name=\"actBody_Link\">\n        <visual>\n            <geometry>\n                <mesh filename=\"package://odam_arm/meshes/GRIPPER_DAE/ActuadorBody.dae\" scale=\"1 1 1\"/>\n            </geometry>\n        </visual>\n        <material name=\"blue\">\n            <color rgba=\"${0/255} ${0/255} ${255/255} 1.0\"/>\n        </material>\n        <origin xyz=\" 0 0 0.0\" rpy=\"0.0 0.0 0.0\"/>>\n    </link>\n\n\n    <joint name=\"actBody_Joint\" type=\"revolute\">\n        <parent link=\"base_link\"/>\n        <child link=\"actBody_Link\"/>\n        <origin xyz=\" 0.044661  -0.022722  0.016772   \" rpy=\"0.0 0.0 0.0\"/>\n        <axis xyz=\"0 0 1 \"/>\n        <limit effort=\"200\" lower=\"-0.02\" upper=\"0.13\" velocity=\"1.0\"/>\n\n    </joint>\n\n    <!-- LEFT PART SHAFT ACTUADOR OR C PART -->\n    <link name=\"actShaft_Link\">\n        <visual>\n            <geometry>\n                <mesh filename=\"package://odam_arm/meshes/GRIPPER_DAE/ActuadorShaft.dae\" scale=\"1 1 1\"/>\n            </geometry>\n        </visual>\n        <material name=\"blue\">\n            <color rgba=\"${0/255} ${0/255} ${255/255} 1.0\"/>\n        </material>\n        <origin xyz=\"0 0 0.0\" rpy=\"0.0 0.0 0.0\"/>>\n    </link>\n\n\n    <joint name=\"actShaft_Joint\" type=\"fixed\">\n        <parent link=\"actBody_Link\"/>\n        <child link=\"actShaft_Link\"/>\n        <origin xyz=\"0.029065  -0.091559  0.004383   \" rpy=\"0 0 0.3\"/>\n        <axis xyz=\"0 1 0 \"/>\n        <limit effort=\"200\" lower=\"0.0\" upper=\"0.05\" velocity=\"1.0\"/>\n        <!-- <mimic joint=\"part1_Joint\" multiplier=\"1\" offset=\"0\"/> -->\n    </joint>\n        <!-- PART5  OR A PART-->\n    <link name=\"part5_Link\">\n        <visual>\n            <geometry>\n                <mesh filename=\"package://odam_arm/meshes/GRIPPER_DAE/parte5.dae\" scale=\"1 1 1\"/>\n            </geometry>\n        </visual>\n        <material name=\"blue\">\n            <color rgba=\"${0/255} ${0/255} ${255/255} 1.0\"/>\n        </material>\n        <origin xyz=\"0 0 0.0\" rpy=\"0.0 0.0 0.0\"/>>\n    </link>\n\n\n    <joint name=\"part5_Joint\" type=\"revolute\">\n        <parent link=\"base_link\"/>\n        <child link=\"part5_Link\"/>\n        <origin xyz=\"0.043003 ..."], "url": "https://answers.ros.org/question/341365/urdf-attaching-two-joints-to-one-link-for-a-gripper/"},
{"title": "VirtualBox issues with USB-CAM and IMAGE-VIEW", "time": "2019-11-10 17:02:18 -0600", "post_content": [" ", " ", "Hello,", "I am currently practicing opencv with ROS. I run it in various VMs with VirtualBox on a Windows Host Machine. Right now I am having some sort of formatting issue when I try and run image-view or my opencv bridge subscriber node. ", "I am unable to upload images but the out put has static for the top 10th of the screen and the rest is a dull green color.\nMy subscriber node looks like:", "I run:", "I have tried in both melodic on ubuntu 18 and kinetic on ubuntu 16 with the same results. I also run into the same error when i simply run ", "I read that someone switch from VMWare to Parallels and it fixed their issue...but I would like to keep working with VirtualBox.", "Any Suggestions?"], "answer": [], "question_code": ["#!/home/mhyde/vEnvs/rosPy/bin/python\n\n'''\nRun a WebCam Feed Using OpenCV\n\nFrom the ROS course on Udemy:\n\nhttps://www.udemy.com/course/ros-essentials/\n\n'''\n\nimport sys\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\nimport cv2\n\nbridge = CvBridge()\n\ndef imageCallback(imageData):\n    '''\n    Recieves the image sensor data and converts it to an\n    OpenCV format using CV Bridge\n    '''\n    print 'Received Image'\n    #Convert ROS Image into OpenCV Image\n    try:\n        cvImage = bridge.imgmsg_to_cv2(imageData, \"bgr8\")\n    except CvBridgeError as error:\n        print error\n    #Now ROS can work directly weith OpenCV\n\n    (rows, cols, channels) = cvImage.shape\n    if cols > 200 and rows > 200:\n        cv2.circle(cvImage, (100, 100), 90, 255)\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    cv2.putText(cvImage, \"WebCam Activated with ROS + OpenCV\",\n                (10, 250), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n    cv2.imshow(\"Image Window\", cvImage)\n    cv2.waitKey(3)\n\ndef main(args):\n    '''\n    Main Function. Initializes the Subscriber Node for\n    Image Processing\n    '''\n    rospy.init_node('image_converter', anonymous=True)\n    imageSub = rospy.Subscriber(\"/usb_cam/image_raw\", Image, imageCallback)\n    try:\n        rospy.spin()\n    except KeyboardInterrupt:\n        print \"Shutting Down ... \"\n\n    cv2.destroyAllWindows()\n\nif __name__ == '__main__':\n    main(sys.argv)\n", "rosrun usb_cam usb_cam_node _pixel_format:=yuvu\nrosrun mypkg imagePubSub.py\n", "rosrun image_view image_view\n"], "url": "https://answers.ros.org/question/337274/virtualbox-issues-with-usb-cam-and-image-view/"},
{"title": "/driver/init service enables the active state (power enabled) [closed]", "time": "2019-12-28 09:05:51 -0600", "post_content": [" ", " ", " ", " ", "I am a beginner in using ROS. I was able to run Elmo regulator using the ROS can_open package. After calling /driver/init service, the controller is activated and remains in the enabled state. Can any parameter be changed so that calling the /driver/init service enters the \"power disabled\" - standby mode? I can't find the package the service /driver is in?", "After several days of testing, I came to the conclusion that it is not driver/init service supports state machine transitions. Can_motor_node manages drive state transitions via drive/init service. After running can_chain_node without the can_motor_node module, I can manually control the controller state machine."], "answer": [], "url": "https://answers.ros.org/question/340769/driverinit-service-enables-the-active-state-power-enabled/"},
{"title": "What is the max linear acceleration of Husky A200?", "time": "2019-10-20 12:37:46 -0600", "post_content": [" ", " ", "Dear all,", "I am not experienced in doing robotics. I have been trying to find the max linear acceleration of Husky A200 in order to set real constraint for simulation purposes.", "What I did was to find out the drive power of the Husky A200 (", ").", "Drive Power = 400 W continuous (4 wheels), thus Drive Power = 100 W continuous (1 wheel).", "Using", "Acceleration = Power/(Mass x Velocity), where max velocity = 1 m/s (also in the link above) and Mass = 57.69 kg", "I got", "Max Acceleration = 1.733 m/s^2", "Is the calculation correct? If yes, what does it mean when Max Acceleration > Max Velocity in magnitude? Does it just mean that the velocity will just be capped at 1 m/s even if the velocity can be increased by 1.733 m/s in a second?", "Thanks for viewing."], "answer": [" ", " ", " ", " ", "According to ", " which is shipped by Clearpath themselves, the max linear acceleration is ", " m/s^2:", "This is not necessarily the ", " acceleration limit, but is what they configure the ROS-side controllers with.", "Note that this same file is used with the simulated version (by ", " from ", ". So the official simulation uses the same limits.", "PS: seeing as you've tagged this with ", ": is there a specific reason you can't use the provided ", " package?", "Thank you very much. Yes, I am using gazebo as part of the simulation but I think I might have missed this one out.", "Is there any specific reason why you cross-posted this to Robotics Stack Exchange (", ")?", "I posted on two website just in case one forum is inactive. I have closed the post in Robotics Stack Exchange already. Thanks again!", "I posted on two website just in case one forum is inactive.", "In the future: please don't cross-post. It's not very nice, as in the best case, it leads to split discussions, but typically (and that would be the worst case) it leads to duplication (ie: waste) of effort.", "Post in one place and give people time to respond to your question.", "Roger that. Thanks for the info."], "answer_code": ["3.0", "  # Velocity and acceleration limits\n  # Whenever a min_* is unspecified, default to -max_*\n  linear:\n    x:\n      has_velocity_limits    : true\n      max_velocity           : 1.0   # m/s\n      has_acceleration_limits: true\n      max_acceleration       : 3.0   # m/s^2\n  angular:\n    z:\n      has_velocity_limits    : true\n      max_velocity           : 2.0   # rad/s\n      has_acceleration_limits: true\n      max_acceleration       : 6.0   # rad/s^2\n", "gazebo", "husky_gazebo"], "url": "https://answers.ros.org/question/335763/what-is-the-max-linear-acceleration-of-husky-a200/"},
{"title": "Find correct tag to publish a ros topic", "time": "2019-10-01 09:29:27 -0600", "post_content": [" ", " ", "Hi everyone.\nI create a wireless receiver plugin in order to obtain all the RSSI signals from the wifi transmitter.\nHere is my plugin:", "Now i import my sensor plugin into an urdf file", "But when i launch my simulation i see my /wireless_power topic but it does not publish nothing.", "When i try using an SDF file i can echo the topic and read all the informations.", "So where is the <tag> i need to insert into the <sensor> </sensor> tag in order to publish correctly my topic?", "Please help me!", "Thanks in advance!"], "answer": [], "question_code": ["#include \"wifi_ros_plugin.h\"\n\n#include <gazebo/physics/physics.hh>\n#include <gazebo/transport/transport.hh>\n#include <std_msgs/Float64MultiArray.h>\n\nusing namespace std;\nusing namespace gazebo;\nusing namespace sensors;\n\nWifiRosPlugin::WifiRosPlugin():\n    SensorPlugin ()\n{}\n\nvoid WifiRosPlugin::Load (SensorPtr _sensor, sdf::ElementPtr _sdf)\n{\n    parentSensor = dynamic_pointer_cast<WirelessReceiver> (_sensor);\n    parentLink = dynamic_pointer_cast<physics::Link> (physics::get_world ()->EntityByName (parentSensor->ParentName ()));\n\n    if (!parentSensor) {\n        gzerr << \"WifiRosPlugin must be instantiate within a Wireless Transmitter Sensor.\\n\";\n        return;\n    }\n\n    // Init ROS\n    if (!ros::isInitialized ()) {\n        int argc = 0;\n        char **argv = NULL;\n\n        ros::init (argc, argv, \"wireless_ros_plugin\");\n    }\n\n    rosNode = new ros::NodeHandle (\"wireless_ros_plugin\");\n    wirelessPub = rosNode->advertise<std_msgs::Float64MultiArray> (\"wireless_power\", 1);\n\n    updateConnection = parentSensor->ConnectUpdated (\n                bind (&WifiRosPlugin::SensorUpdated, this));\n}\n\nvoid WifiRosPlugin::SensorUpdated ()\n{\n    std_msgs::Float64MultiArray powerMsg;\n    vector<double> unorderedPowerVector, powerVector;\n    vector<int> indexVector;\n\n    // Find transmitters\n    for (SensorPtr currSensor : SensorManager::Instance ()->GetSensors ()) {\n        if (currSensor->Type () == \"wireless_transmitter\") {\n            WirelessTransmitterPtr transmitter = dynamic_pointer_cast<WirelessTransmitter> (currSensor);\n            int txIndex;\n            double signalStrength;\n\n            try {\n                txIndex = stoi (transmitter->ESSID ());\n            } catch (const invalid_argument &e) {\n                gzerr << \"ESSID must be numerical\";\n                return;\n            }\n\n            signalStrength = transmitter->SignalStrength (parentLink->WorldPose (), parentSensor->Gain ());\n\n            unorderedPowerVector.push_back (signalStrength);\n            indexVector.push_back (txIndex);\n        }\n    }\n\n\n    // Order power vector\n    powerVector.resize (unorderedPowerVector.size ());\n    for (int i = 0; i < powerVector.size (); i++)\n        powerVector[i] = unorderedPowerVector[indexVector[i]];\n\n    // Build msg\n    powerMsg.layout.dim.resize (1);\n    powerMsg.layout.dim[0].size = powerVector.size ();\n    powerMsg.layout.dim[0].stride = powerVector.size ();\n    powerMsg.layout.dim[0].label = \"received_powers\";\n\n    powerMsg.data = powerVector;\n\n    wirelessPub.publish (powerMsg);\n}\n\nGZ_REGISTER_SENSOR_PLUGIN(WifiRosPlugin)\n", "<!-- GAZEBO PLUGIN WIRELESS RECEIVER -->\n  <gazebo reference=\"receiver\">\n    <sensor name=\"wirelessReceiver\" type=\"wireless_receiver\">\n        <plugin name=\"receiver_plugin\" filename=\"libwifi_ros_plugin.so\"/>\n        <always_on>1</always_on>\n        <update_rate>1</update_rate>\n        <visualize>true</visualize>\n        <transceiver>\n          <min_frequency>2412.0</min_frequency>\n          <max_frequency>2484.0</max_frequency>\n          <power>14.5</power>\n          <gain>2.5</gain>\n          <sensitivity>-90.0</sensitivity>\n        </transceiver>\n      </sensor>\n  </gazebo>\n"], "url": "https://answers.ros.org/question/334350/find-correct-tag-to-publish-a-ros-topic/"},
{"title": "How can I catch errors in rosserial publish() ?", "time": "2019-07-12 17:28:50 -0600", "post_content": [" ", " ", "Simple  really - I have a robot that uses an arduino to control the motors. It gets velocity commands from a cmd_vel topic, either from a Raspberry Pi that runs the ROS master, or when it detects radio control signals on three pins and publishes to cmd_vel itself.", "I assume that if the Pi crashes for some reason the ROS stack will collapse and I won't be able to publish these messages. In that case I'd like to stop the robot and revert to manual radio control, writing the received pulses to the motor controller directly instead of generating the messages.", "Is there a way to detect that the publish() has failed ? I can't find anything that points to return codes or anything, so it seems that ROS messaging is 'fire-and-forget' only. Is this the case ?", "Thanks for any help,", "David", "Perhaps you can use the ", " method on the rosserial NodeHandle?", "Where would you want to detect that \"publish() has failed exactly\"? On the Pi, or the Arduino?", "In any case: would it not be possible to do this with timeouts? Keep track of when the last ", " was received and start your countermeasures whenever the timeout is reached.", "I can't find anything that points to return codes or anything, so it seems that ROS messaging is 'fire-and-forget' only", "That would be true for publish-subscribe, yes. There are no guarantees (not on reception, processing at all or timely processing by clients). If that is not what you need/want, you'd have to use a different interaction pattern.", "These are good ideas and questions. I'll check out connect(), should have thought of looking at the node handle for clues :)", "Ideally I'd do the detection on the arduino as it's less likely (I think) to become \"unavailable\" as it's powered through the same circuit as the motors and motor controller. If the main circuit breaker trips then everything stops. However the Pi (which normally would issue the automated move commands via cmd_vel) could go offline separately. In that case I'd want to be able to control it manually from the radio controller to stop any runaway.", "I think the timeout point is critical too. I've moved to a \"send move commands every 100ms and just stop if nothing is received for 200ms\" as a safety check after the robot headed at 10km/h towards a double plate glass door, and luckily hit a ...", "A mobile base driver without a command timeout is asking for trouble.", "That should be there, regardless of fail-over to a radio input.", "Couldn't agree more :)"], "answer": [], "question_code": ["connected()", "Twist"], "url": "https://answers.ros.org/question/328438/how-can-i-catch-errors-in-rosserial-publish/"},
{"title": "How to do wireless communication between robots and desktops?", "time": "2019-09-19 05:16:49 -0600", "post_content": [" ", " ", "I'm currently controlling an autonomous navigating robot (turtlebot2) with the attached pc.\nHowever the attached pc doesn't have enough power to do image processing such as openpose, so I want to use a more powerful machine to control the robot. I'm thinking of using a desktop to do wireless communicate with the  robot.\nThank you in advance for your help."], "answer": [" ", " ", "ROS is an inherently distributed network system. You can run nodes on any computer attached to the network containing your robot as long as the ROS network parameters are setup correctly. See ", " for details of setting this up. As long as the computers have a network connection then you can swap processing nodes between computers as you need.", "Is your turtle bot attached to a Wi-Fi network in your current setup? It is common for people to use a laptop or desktop with a wireless network to view sensors and send commands to the robot.", "In addition to this, you can check the ", " on how to run ROS on multiple machines. ", " question also addresses the problem."], "url": "https://answers.ros.org/question/333399/how-to-do-wireless-communication-between-robots-and-desktops/"},
{"title": "py_trees_ros experiences", "time": "2018-12-03 13:23:56 -0600", "post_content": [" ", " ", "Hello community,", "I'm interested in users, who have experiences with py_trees_ros.\nIs there anyone who is using python behavior trees in real worlds robots.\nIs py_trees_ros powerful enough to solve small autonomous wheeled bot behaviours ?\nIt seems for me, that py_trees_ros is an excellent highlevel prototyping environment for learning ros and robotics.", "Thanks in advance for some hints or links to realworld examples.", "Cheers\nChrimo", "Not an answer but there are some discussions that might help. I just listed on ", ".", "yes, I saw this theoretical discussions and I read all the related docs.\nWhat about practice ? \nPros and cons at real life projects ?"], "answer": [" ", " ", " ", " ", "I have been involved in the development of py_trees over the years:", "Before it was released as open source, it was originally developed at Yujin Robot to handle the application layer of the fleet robots that they were developing. Among the original goals were:", "It took a few iterations to get there, but ultimately we met all of our goals. We even found that the control engineers started moving their decision making logic to the behaviour trees when it didn't have low-latency requirements (e.g. docking) simply because it got their components into a framework which was monitored, logged, mocked and allowed them to interact with other parts of the robot (e.g. the notifications subsystem) without having to drag that subsystem in as a dependency for the control module (e.g. docking LED or sound notifications as it passed through different states in the docking process).", "These days it has been picked up by CARLA for their scenario layer in autonomous driving simulations and Toyota Research Institute is doing the same internally. Blue Ocean Robotics has been using it for a while and I have been notified of a few other robotics companies in the valley area taking advantage of it, but I do not know to what extent."], "answer_details": ["Simple enough for an intern to take care of the application layer (req: python programming skills)", "Apply to the robots we were deploying at various field tests around the world", "Provide sufficient tooling for monitoring, logging, replay - basically allow us to root cause problems at the application level", "Work easily with other tools that mock the robot so we could put the application layer under CI", " ", " ", " ", " "], "url": "https://answers.ros.org/question/309879/py_trees_ros-experiences/"},
{"title": "How to speed up moveit execution velocity?", "time": "2019-03-11 21:08:03 -0600", "post_content": [" ", " ", "I find the moveit execution speed does not release the full power of my industrial robot.\nI have already set the max velocity factor to 1.0, and the speed still unsatisfactory.", "MoveIt generate pleasant plans, however how can I speed up the execution?\nDo Mass and Inertia property in URDF influence MoveIt execution speed? "], "answer": [" ", " ", "You should check your ", " and set the values according to your robot's specifications.", "Especially the ", " limits are important. If they are not set, they default to ", " (rad/s^2), which may lead to slow movements.", "I would also ask ", " what sort of driver/interface he has to his \"industrial robot\". There are many and for a good portion of them they cannot reach 100% performance of the robot, due to poor motion interfaces provided by the controller."], "answer_code": ["joint_limits.yaml", "joint_limits:\n    joint1:\n        has_velocity_limits: true\n        max_velocity: 2\n        has_acceleration_limits: true\n        max_acceleration: 0.4\n", "acceleration", "1.0"], "url": "https://answers.ros.org/question/318314/how-to-speed-up-moveit-execution-velocity/"},
{"title": "How to use different types of dynamixels x-series and pro series together?", "time": "2019-04-19 10:17:16 -0600", "post_content": [" ", " ", "I wave a humanoid robot project and I'd like to use the pro series dynamixel servos for the shoulder joint and the x-series for the lower arm. I know that i need different voltages for the servos, but can I use the same RS485 communication line if the are all the R-type servos?", "I have a OpenCM9.04 with the expansion board and an OpenCR1.0 board that I am evaluating, although I plan to use a Raspberry pi as my ROS node. ", "I guess I am trying to figure out if I can have all the servos connected to one of the boards, and then power them separately. Or will I need to have to separate data paths (and therefore use two USB ports) on the raspberry pi.", "Thanks"], "answer": [], "url": "https://answers.ros.org/question/321464/how-to-use-different-types-of-dynamixels-x-series-and-pro-series-together/"},
{"title": "RTAB-Map using realsense R200, Did not receive data since 5 seconds! Make sure the input topics are published", "time": "2019-02-19 01:07:29 -0600", "post_content": [" ", " ", " ", " ", "Hi, i'm new to RTAB-Map and ros. i follow the tutorial in how to use intel realsense R200 to do rtabmap.\nthe robot i use is turtlebot3 with intel realsense R200 with ubuntu 16.04 (amd64) ros kinetic.\ni already run the realsense_camera R200_nodelet_rgbd.launch in the robot.\nthe error i get are :", "/rtabmap/rtabmap subscribed to (approx sync):\n   /odom,\n   /camera/rgb/image_rect_color,\n   /camera/depth_registered/sw_registered/image_rect,\n   /camera/rgb/camera_info,\n   /scan", "the display in rviz show nothing but black screen with blue green lines.", "i have check the rostopic hz for each of the topic there. the results for /camera is a bit delayed like 1 - 3 no data then data received.", "the launch file i use for rtabmap :", " See  ", "Could you please advise me how you set up and start everything using R200 in remote mode:\n"], "answer": [], "question_code": ["<launch>\n    <group ns=\"rtabmap\">\n      <node name=\"rtabmap\" pkg=\"rtabmap_ros\" type=\"rtabmap\" output=\"screen\" args=\"--delete_db_on_start\">\n\n            <param name=\"frame_id\" type=\"string\" value=\"base_link\"/>\n\n            <param name=\"subscribe_depth\" type=\"bool\" value=\"true\"/>\n            <param name=\"subscribe_scan\" type=\"bool\" value=\"true\"/>\n\n            <remap from=\"odom\" to=\"/odom\"/>\n            <remap from=\"scan\" to=\"/scan\"/>\n\n            <remap from=\"rgb/image\" to=\"/camera/rgb/image_rect_color\"/>\n            <remap from=\"depth/image\" to=\"/camera/depth_registered/sw_registered/image_rect\"/>\n            <remap from=\"rgb/camera_info\" to=\"/camera/rgb/camera_info\"/>\n\n            <param name=\"queue_size\" type=\"int\" value=\"10\"/>\n\n            <!-- rtabmap parameters -->\n\n            <param name=\"RGBD/NeighborLinkRefining\"  type=\"string\" value=\"true\"/>\n            <param name=\"RGBD/ProximityBySpace\"      type=\"string\" value=\"true\"/>\n            <param name=\"RGBD/ProximityPathMaxNeighbors\"         type=\"string\" value=\"10\"/>\n            <param name=\"RGBD/AngularUpdate\"         type=\"string\" value=\"0.01\"/>\n            <param name=\"RGBD/LinearUpdate\"          type=\"string\" value=\"0.01\"/>\n            <param name=\"RGBD/OptimizeFromGraphEnd\"  type=\"string\" value=\"false\"/>\n            <param name=\"Reg/Force3DoF\"              type=\"string\" value=\"true\"/>\n            <param name=\"Reg/Strategy\"               type=\"string\" value=\"1\"/>\n            <param name=\"Vis/MinInliers\"             type=\"string\" value=\"5\"/>\n            <param name=\"Vis/InlierDistance\"         type=\"string\" value=\"0.1\"/>\n            <param name=\"Rtabmap/TimeThr\"            type=\"string\" value=\"700\"/>\n            <param name=\"Mem/RehearsalSimilarity\"    type=\"string\" value=\"0.45\"/>\n            <param name=\"Grid/FromDepth\"             type=\"string\" value=\"false\"/>\n\n      </node>\n    </group>\n</launch>\n"], "url": "https://answers.ros.org/question/316018/rtab-map-using-realsense-r200-did-not-receive-data-since-5-seconds-make-sure-the-input-topics-are-published/"},
{"title": "How to debug this?  Callbacks stop.", "time": "2019-03-04 13:17:01 -0600", "post_content": [" ", " ", "I have a very standard looking mobile base controller written in Python.   It subscribes to cmd_vel messages and publishes odometry.  Odometry is published from inside a loop that includes rate.sleep and the cmd_vel callback talks directly to the hardware.    Both have lots of debug messages inside so I can know what they are doing.   It runs about 10 to 20Hzon a Raspberry Pi 3B and uses about 6% of a processor core.   It seems to run fine and the robot base moves and turns as expected.", "Here is the problem:   It only works for a while, perhaps 5 to 10 minutes of random driving around and then the hardware motor controller never gets another update and the robot just keeps driving with the last motor speeds until power cycled.", "At first, I suspect the motor controller but no... ", "I see (using rostopic echo /cmd_vel) thatcmd_vel messages are being published correctly and no odometry is beibg published.  It seems like the base controller is terminated.   But no...  ", "\"rosnode ping\" works as does rosnode info.  I se that my controller node is running and subscribed to cmd_vel but the callback is not being called and the loop that runs the odometry publisher is not running either", "The effect is as if the node were stuck inside spinOnce, not frozen because ping and info work, just stuck.", "The odd thing is that is all works fine for a while, then stops.   I start it all with one launch file and \"control-C\" stops everything", "QUESTION:  How do I debug this?  I have many debug log messages and none are logged because I assume the callback and lop are not running.", "The effect is exactly as if by base controler node were stuck in", "As this is running over wireless: can you run a ", " (and/or a ", ") of the ", " topic on the robot to see whether that also stops?", "Also:", "the hardware motor controller never gets another update and the robot just keeps driving with the last motor speeds until power cycled.", "independent of this problem: I would really recommend to implement a command time-out in your mobile base driver. That is something all mobile robots ..", ".. should have and prevents communication issues from doing any real damage with robots repeating or continuing the last received command.", "Yes, I have a timeout.   If no cmd_vel messages are received I send a stop command to the hardware.   But as I wrote, the while-rosok, dostuff() rate..sleep() loop does not run so time outs are not detected. (will write a watchdog node that monitors odometry and compares to cmd_vel. kills power)", "It was not clear to me from your description that event processing / callback queues stop working. I had understood that it just appeared that no callbacks were being called / messages were not being received.", "Do you have any other nodes on that system that exhibit the same problem? If you ..", "Yes.  I have monitored cmd_vel.  the published cmd_vel is good and reacts to sensors.  rosnode info shows the base controller is subscribed to cmd_vel.    There is no difference between when it works and not that I can see with rostopic echo cmd_vel.   rosnode ping works on all nodes.", ".. disable parts of the base driver (ie: writing to/reading from hw fi), does this also occur?", "How does your driver communnicate with the motor controller? Memory reads/writes? Serial port? Something else? If a serial port read hangs in a callback (for instance), that could explain this.", "At first, I suspect the motor controller but no... ", "can you add how you reached this conclusion?"], "answer": [], "question_code": ["rostopic echo", "rosbag record", "cmd_vel"], "url": "https://answers.ros.org/question/317463/how-to-debug-this-callbacks-stop/"},
{"title": "Five processes are started within one second while the node is running.", "time": "2019-02-13 12:15:32 -0600", "post_content": [" ", " ", "My environment is ubuntu 16.04 + ros kinetic.", "I have seven ros nodes running on a mobile robot, and use glog to log running status and data.", "Yesterday, the robot was powered on normally and worked fine for one hour. Without any other interruption, at 9:08am there were five new processes being created with the same node name within one second, and there were five new log files created. This happened on six ros nodes, so there were 30 new processes created within one second. However, at the same time, the original process was still running and writing log files, until four minutes later, at 9:12am. The original process died and stopped logging. The new five processes kept running and writing logs until 9:26am, a collision happened.", "A potential relevant observation is that, usually when I used htop to monitor computing resource, I noticed multiple processes are running under one rosnode. But I don't know why and when will this happen. Could anyone give any hints or clues? ", "Thanks.", "Are you sure these are processes? Could they be threads?", "They have different pids, both shown in htop, and in log filenames. So I called them \"processes\". And all these five \"processes\" were writing independent log files at the same time. And for each node, they are all simple single thread node."], "answer": [], "url": "https://answers.ros.org/question/315588/five-processes-are-started-within-one-second-while-the-node-is-running/"},
{"title": "How to setup the config for robot_localization", "time": "2019-02-01 15:18:38 -0600", "post_content": [" ", " ", "I am currently working on a robot car. The car publishes odom, but the orientation part is not very accurate. As you can see from the picture attached. The green dots is the trace of odometer reading, the red dots is the true robot location, calculated by a particle filter. The odom is based on a VESC. There is also an imu unit on the car, I am thinking fusing the imu and odom to get a better odometery reading. ", "A few people recommend the robot_localization package. I took a look, but not sure how to setup the configuration file, especially odom0_config and imu0_config matrix. ", "For my odom and imu, all elements of the covariance matrix are 0. What is the best settings for odom0_config and imu0_config matrix and other \"important\" parameters for robot_localization ROS package? "], "answer": [], "url": "https://answers.ros.org/question/314534/how-to-setup-the-config-for-robot_localization/"},
{"title": "Get 3D positions of detected objects of a Deep Learning model using TensorFlow/Keras/YOLO/...", "time": "2019-02-12 02:41:41 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone.", "My intention is to obtain the TFs of certain objects that are detected using a depth camera built in a mobile robot and a deep neural network via TensorFlow, Keras or ", ". I have found so far that those APIs work with RGB images but not with stereo depth images. Also I found a marvelous package called ", " that provides the position of the detected objects in the 3D space via TFs which you can visualize using RViz.", "Since the detection algorithms of this package are based on Image Matching (SIFT, SURF, BRIEF, ORB, ...), it is quite limited for my purposes considering alternatives much more powerful like the use of neural networks for object detection based on ImageNet pretraining.", "I am kind of stuck at this point and not sure how an implementation can solve this issue.\nIs there any way to relate ", " package with TensorFlow detections?\nIf not, is there any way to do that without this package?", "extra question: how can I filter specific objects to be detected?", "Yesterday I almost posted this same exact question. I had found ", " but not find-object-2d. This first package lets you use e.g. YOLO with RealSense or Kinect. I'm looking into retraining/transfer learning atm but will be back.", "To clarify, ros_object_analytics is geared towards the RealSense users, I was thinking of ", " when talking about Kinect.", "Exactly, I found it after posting the package you mentioned. The problem is that apparently publishes 3D boundary boxes (cubes) but not the TFs. Nonetheless, also found this useful package which does exactly that, it is called ", ".", ", it seems to return a pose in the center of the pointcloud. I'd like to fit an existing mesh onto the found object or to detect a specific point in it for the pose.", "Yeah, you can modify such piece of code so it returns the mean of the pixel distances of the boundary box.\nHave you managed to run ros_object_analytics successfully?", "I launched the nodes and seen the camera correctly infer some of the 20 objects in yolov2 (the example cnn) and I'm working on training on my own dataset. I didn't do much testing apart from what basic stuff was suggested, it seemed to run fine, are you having issues?", "How can I use this package to train own data set in Yolov3 or Retinanet and get the 3D bounding box of the detected objects? I like to use the 3D Bounding Box in Moveit to do collision checking. Any idea or help?"], "answer": [], "url": "https://answers.ros.org/question/315362/get-3d-positions-of-detected-objects-of-a-deep-learning-model-using-tensorflowkerasyolo/"},
{"title": "robot_localization problems", "time": "2018-09-28 21:02:13 -0600", "post_content": [" ", " ", " ", " ", "I'm now using robot_localization package on Ubuntu 16.04. I want to fuse the data of pose (given by orb_slam2), imu and gps signal collected by DJI Matrice 100. But the result looks strange. I'm not sure whether my configuration is right or not. The gps odometry (the green arrows in the picture) is not showing the ground truth path (a rectangular path on a fixed height), and the odometry/filtered_map (the red arrows) fluctuates badly along all axis when gps is fused and its path doesn't make sense. I will put my configuration file here and please help me check if there are errors. Thanks!", "I'm sorry that I can't upload files or images because I don't have enough points. Please visit my google drive. \n", "\nThe bag file is also in this folder. The pose information given by orb_slam is also recorded and can be directly put into the ekf node. ", "The tf between base link and imu or gps is set to [0 0 0 0 0 0] because for now I'm not sure how imu and gps are mounted on the drone. This might be a problem. ", "/orb_slam/posewithcov is the pose information given by orb_slam2. ", "The imu reads 0 when the drone is facing northeast, so I set yaw_offset to 0.7853. I'm not sure if this is right because the heading of drone may not be the heading of imu. \n    <launch>\n    <arg name=\"output_final_position\" default=\"false\"/>\n    <arg name=\"output_location\" default=\"test.txt\"/>", ", did you manage to solve this? I'm doing something similar but using only the IMU and GPS on the Matrice 100. I find that my odometry filtered estimates just jumps around"], "answer": [" ", " ", "There's quite a bit going on here!", "If you can update your question and add some sample messages from each sensor source, I'd appreciate it.", "I'd be very careful about fusing multiple sources of absolute pose information. I don't know enough about the SLAM package in question to comment on how it behaves, but if the package has any divergence or has a poor loop closure (is it running live SLAM, or just providing a pose estimate in your map?), then the SLAM position estimate and the GPS position estimate are going to differ wildly, and it'll just jump back and forth between those state estimates.", "The same goes for your IMU data. You are fusing absolute orientation data from your IMU, but there's no guarantee that it will match your SLAM positions. For example, if you start your robot in a pose where the IMU reads pi/4 radians, and then drive straight forward one meter, your SLAM package, if not using your IMU absolute orientation, is going to probably read an XY position of ", ", but that disagrees with the orientation of the IMU, which would suggest that we should be at ", " after driving forward one meter.", "In any case, if you are fusing multiple absolute pose sources, you need to make sure they are in the same coordinate frame, or that they have a transform defined that will make them so.", "My suggestion to you is to start with just the \"tier 1\" (odom frame) EKF. Get that looking the way you want, then move on to the second tier (map frame) EKF. If I were you, I'd probably fuse my full 3D pose from the SLAM package in the odom-frame EKF, and maybe just fuse the angular velocities from the IMU. If you lack a velocity measurement source, I'd stop fusing the linear acceleration data from the IMU, too.", "Hi Tom, thank you very much for the answer and suggestion! Recently, I decided to write a simple sensor fusion node for our drone, and now I have some questions. ", "In robot_localization,the EKF predict function use the system model to calculate the predicted state_X,and all the sensors' message  is being take to the measurements.It is the same as robot_pose_ekf in some kind."], "answer_details": [" ", " ", " ", " ", "My IMU topic type is sensor_msgs/Imu. If I want to get the x,y,z velocity, do I just need to integrate the x,y,z acceleration?", "The EKF updates predictions using one input and one observation right? I decide to take imu as the input. What about the SLAM position and GPS position? Do I need to make them into one observation?"], "question_code": ["<param name=\"/use_sim_time\" value=\"true\" />\n\n<node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"imu_link\" args=\"0 0 0 0 0 0 base_link body_FLU\" />\n<node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_gps\" args=\"0 0 0 0 0 0 base_link gps\" />\n\n<node name=\"test_ekf_localization_node_ekf\" pkg=\"robot_localization\" type=\"ekf_localization_node\" clear_params=\"true\">\n\n  <param name=\"frequency\" value=\"30\"/>\n  <param name=\"sensor_timeout\" value=\"0.1\"/>\n  <param name=\"two_d_mode\" value=\"false\"/>\n\n  <param name=\"map_frame\" value=\"map\"/>\n  <param name=\"odom_frame\" value=\"odom\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom\"/>\n\n  <param name=\"transform_time_offset\" value=\"0.0\"/>\n  <param name=\"transform_timeout\" value=\"0.0\"/>\n\n  <param name=\"pose0\" value=\"/orb_slam/posewithcov\"/>\n  <param name=\"imu0\" value=\"/dji_sdk/imu\"/>\n\n  <rosparam param=\"pose0_config\">[true , true , true ,\n                                  false, false, false,\n                                  false, false, false,\n                                  false, false, false,\n                                  false, false, false]</rosparam>\n\n  <rosparam param=\"imu0_config\">[false, false, false,\n                                 true , true , true ,\n                                 false, false, false,\n                                 true , true , true ,\n                                 true , true , true ]</rosparam>\n\n  <param name=\"pose0_differential\" value=\"false\"/>\n  <param name=\"imu0_differential\" value=\"false\"/>\n\n  <param name=\"pose0_relative\" value=\"false\"/>\n  <param name=\"imu0_relative\" value=\"false\"/>\n\n  <param name=\"imu0_remove_gravitational_acceleration\" value=\"false\"/>\n\n  <param name=\"print_diagnostics\" value=\"true\"/>\n\n  <param name=\"pose0_queue_size\" value=\"10\"/>\n  <param name=\"imu0_queue_size\" value=\"10\"/>\n\n  <param name=\"debug\"           value=\"false\"/>\n  <param name=\"debug_out_file\"  value=\"debug_ekf_localization.txt\"/>\n\n  <rosparam param=\"process_noise_covariance\">[0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                                              0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                                              0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0 ..."], "answer_code": ["(1, 0)", "(0.707, 0.707)"], "url": "https://answers.ros.org/question/304491/robot_localization-problems/"},
{"title": "RESULT_OPERATION_TIMEOUT error on raspberry pi 3 B when interfacing RPLIDAR A1 with rplidar_ros package", "time": "2018-10-29 22:00:55 -0600", "post_content": [" ", " ", " ", " ", "I am trying to interface ", " with my ", " Model running ubuntu 16.04 and ROS ", ".", "I am receiving the following error: ", " when I use ", "I tried using multiple OS images and the standalone SDK but the error persists.", "Using the SDK:\nI am receiving the following error when I run ", ": ", "I receive the following error when I run ", ": ", "It works fine on my PC running Ubuntu 16.04 and ROS kinetic with same USB cable and usb to serial converter.", "I have the correct permissions set for /dev/ttyUSB0 and the correct udev rules and the user is a member of the dialout group.\nIs this due to Raspberry Pi 3 B only having USB 2.0 ports? Or is the port blocked? Help me find a solution", "EDIT: ", "1] THE SUMMARY WHEN I RUN ", "2] OUTPUT of ", " ", "3] OUTPUT of ", "Can anyone give the steps to configure raspberry pi ports so that I can figure out if I'm missing something?", "Can you paste the last lines of dmesg after inserting the rplidar?", "The first thing to check is whether ", " actually exists. Some platforms use different names for (serial to) USB ports. After having inserted the rplidar, what is the output of ", "?", "THE FOLLOWING IS THE OUTPUT OF ", "OUTPUT FOR ", "Can you please paste more dmesg lines after trying to read the scanner?? My guess is a power failure from the rpi to supply the current for the scanner.", " UPDATED dmesg log:  ", " \nWhen you mentioned power failure, I looked closely & noticed that the motor speed is slightly slower when connected to RPi compared to my PC. It is not a huge difference in speed but it is noticeable. ", "I am powering my RPi with a USB cable. It is highly possible that the cable is dodgy.", "It is not possible for me to get an alternate power source at this hour. I will try a new power source and let you know first thing tomorrow morning. Thank you very much sir."], "answer": [], "question_code": ["RESULT_OPERATION_TIMEOUT", "roslaunch rplidar_ros rplidar.launch", "ultra_simple /dev/ttyUSB0", "Error, cannot bind to the specified serial port /dev/ttyUSB0.", "simple_grabber /dev/ttyUSB0", "Error, operation timed out", "roslaunch rplidar_ros rplidar.launch", "SUMMARY\n========\n\nPARAMETERS\n * /rosdistro: kinetic\n * /rosversion: 1.12.13\n * /rplidarNode/angle_compensate: True\n * /rplidarNode/frame_id: laser\n * /rplidarNode/inverted: False\n * /rplidarNode/serial_baudrate: 115200\n * /rplidarNode/serial_port: /dev/ttyUSB0\n\nNODES\n  /\n    rplidarNode (rplidar_ros/rplidarNode)\n\nROS_MASTER_URI=http://ubiquityrobot.local:11311\n\nprocess[rplidarNode-1]: started with pid [2130]\n[ INFO] [1455209103.213060755]: RPLIDAR running on ROS package rplidar_ros. SDK Version:1.9.0\n[ERROR] [1455209107.721312732]: Error, operation time out. RESULT_OPERATION_TIMEOUT! \n[rplidarNode-1] process has died [pid 2130, exit code 255, cmd /home/ubuntu/x/devel/lib/rplidar_ros/rplidarNode __name:=rplidarNode __log:=/home/ubuntu/.ros/log/708a8734-d0dc-11e5-a840-9bebfff8ab49/rplidarNode-1.log].\nlog file: /home/ubuntu/.ros/log/708a8734-d0dc-11e5-a840-9bebfff8ab49/rplidarNode-1*.log\nall processes on machine have died, roslaunch will exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\n", "dmesg", "usb 1-1.4: Product: CP2102 USB to UART Bridge Controller\nusb 1-1.4: Manufacturer: Silicon Labs\nusb 1-1.4: SerialNumber: 0001\ncp210x 1-1.4:1.0: cp210x converter detected\nusb 1-1.4: cp210x converter now attached to ttyUSB0\n", "ls -al /dev/ttyUSB0", "crw-rw---- 1 root dialout 188, 0 Feb 11 16:35 /dev/ttyUSB0\n", "/dev/ttyUSB0", "ls -al /dev/ttyUSB0", "dmesg", "usb 1-1.4: Product: CP2102 USB to UART Bridge Controller\nusb 1-1.4: Manufacturer: Silicon Labs\nusb 1-1.4: SerialNumber: 0001\ncp210x 1-1.4:1.0: cp210x converter detected\nusb 1-1.4: cp210x converter now attached to ttyUSB0\n", "ls -al /dev/ttyUSB0", "crw-rw---- 1 root dialout 188, 0 Feb 11 16:35 /dev/ttyUSB0\n"], "url": "https://answers.ros.org/question/307109/result_operation_timeout-error-on-raspberry-pi-3-b-when-interfacing-rplidar-a1-with-rplidar_ros-package/"},
{"title": "UTM-30LX not detected on /dev/ttyACM(*) by Ubuntu", "time": "2018-11-06 10:21:01 -0600", "post_content": [" ", " ", " I am trying to connect my UTM-30LX to ubuntu 16.04 running ROS kinetic using the urg_node. I have installed URG and I am following this tutorial  ", " \n(for the Hokuyo_node as I could not find a tutorial for the urg_node). All I have done so far is connect the lidar to my computer through USB and power it. I see no mention on Hokuyos website of linux drivers. Whenever I type ls -l /dev/ttyACM0 ubuntu spits out ls: cannot access '/dev/ttyACM0': No such file or directory. I have run various commands and I've realised that ubuntu is not detecting the UTM-30LX at all, It registers no change on /dev whenever the usb is connected "], "answer": [" ", " ", "There is a trick I do when debugging USB connected devices. I save the output of ", " to a file with the device connected and without the device connected. Then ", " the two files to see the details of the device, if there is no difference then the USB device is not registered by the OS at all. The process is:", "With the device unplugged:", "Then plug the device in and run:", "Hope this helps.", " ", " ", "lsusb ", "should give a device with ID 15d1:0000:", "Bus 001 Device 002: ID 15d1:0000  ", "If that is not the case, you have a problem that is outside of ROS, maybe your sensor is broken?"], "answer_code": ["lsusb -vvv", "diff", "lsusb -vvv > unconnected.txt\n", "lsusb -vvv > connected.txt\ndiff unconnected.txt connected.txt\n"], "url": "https://answers.ros.org/question/307740/utm-30lx-not-detected-on-devttyacm-by-ubuntu/"},
{"title": "teb_local_planner generates infeasible local plan, global plan is OK, for car-like robot", "time": "2018-11-01 02:36:00 -0600", "post_content": [" ", " ", " ", " ", "I have an Ackermann steering drive based robot or in other words ", ". I am using ", " with ", " for path planning and control.", "This is how my local and global plan look like:", "You can see that the global plan (in green) is quite straightforward for the chosen goal and starting point. The local plan (in blue) is all over the place. And the robot ends up and gets stuck far away from the global plan. I suspect something is wrong with my parameter configuration, but I cannot figure out what.", "Here are my params for the ", ":", "Most of the parameters are set to the default values. Some notable exceptions:", "Have you checked if your position published on odom is valid and the robot is doing what he is supposed to do (e.g. driving forwards when local planner is commanding positive speeds)? ", "I can try your parameters in a simulation but they should be alright."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", ". I have a car like robot with max steering angle of 22 degrees and wheel base of 0.34 m.", ". I increased it from 1 to 500 so that turning radius constraints\nare enforced.", ". I want the robot to favour forward motion."], "question_code": ["move_base", "teb_local_planner", "TEBLocalPlanner", "TebLocalPlannerROS:\n\n odom_topic: /vesc/odom\n map_frame: map\n transform_tolerance: 0.3\n\n # ******* Trajectory **********\n\n teb_autosize: True\n dt_ref: 0.4\n dt_hysteresis: 0.1\n global_plan_overwrite_orientation: True\n allow_init_with_backwards_motion: True\n max_global_plan_lookahead_dist: 3.0\n feasibility_check_no_poses: 2\n\n # ********** Robot **********\n\n max_vel_x: 1.5\n max_vel_x_backwards: 1.0\n max_vel_y: 0.0\n max_vel_theta: 2.5\n acc_lim_x: 2.0\n acc_lim_theta: 4.0\n\n # ********************** Carlike robot parameters ********************\n min_turning_radius: 0.82\n wheelbase: 0.34               \n cmd_angle_instead_rotvel: False \n\n\n footprint_model:\n   type: \"line\"\n   line_start: [0.0, 0.0]\n   line_end: [0.3, 0.0]\n\n # ********** GoalTolerance **********\n xy_goal_tolerance: 0.2\n yaw_goal_tolerance: 3.0\n free_goal_vel: False\n\n # ********** Obstacles **********\n\n min_obstacle_dist: 0.20\n include_costmap_obstacles: True\n costmap_obstacles_behind_robot_dist: 0.3\n obstacle_poses_affected: 30\n inflation_dist: 0.25\n costmap_converter_plugin: \"\"\n costmap_converter_spin_thread: True\n costmap_converter_rate: 5\n\n # ********** Optimization Parameters **********\n\n weight_kinematics_forward_drive: 100.0\n weight_kinematics_turning_radius: 500.0\n weight_acc_lim_x: 0.0\n\n # ********** Parallel Planning Parameters **********\n enable_homotopy_class_planning: True\n", "min_turning_radius: 0.82", "weight_kinematics_turning_radius: 500.0", "weight_kinematics_forward_drive: 100.0"], "url": "https://answers.ros.org/question/307380/teb_local_planner-generates-infeasible-local-plan-global-plan-is-ok-for-car-like-robot/"},
{"title": "TEB doesn't work with non-lethal obstacle", "time": "2018-11-12 03:56:31 -0600", "post_content": [" ", " ", " ", " ", "Currently I'm using teb for local path planner with ", " costmap, where existing lethal obstacle (cost >100) and non-lethal obstacle (cost < 100). And also, ", " set ", " to ", "for both global planner and local planner such that all full spectrum of intermediate values are used.", "When I tried to move cross ", " obstacle: the global path plan made a path which was round the ", " obstacle, but when movebase moving, TEB would cross the ", " obstacle (which just like the ", " obstacle didn't exist). ", "I am not sure that: Does TEB have the ability to walk round the ", " obstacle according to cost map value? ", "(Please forgive my poor English.)", "Thanks for helping.", "Edit 1 (rviz display):", "The \"red\" area is non-lethal area which has <100 cost value. Blue line is global path. Red arrows are TEB path.", "Is the obstacle in the static or the obstacle layer of the costmap? if obstaacle_layer, are you including the obstacle_layer plugin for the local_costmap as well? Does the obstacle show up in the local costmap in RViz?", "hi ", " , thanks for commenting. I updated the question with rviz display. For  your questions:\nObstacle is in static_layer and obstacle information only contains on costmap. \nI do this because I wanna movebase to \"avoid\" some tunnels on ground which has <100 cost if existing less cost plan.", "umm..i am not sure again if the local costmap shows any obstacles or not. As for the global costmap, it is present, hence the planner plans around it. TEB, as you observe, however does not seem to see the obstacle in the local costmap(*) and due to its time optimization tries to pick the faster..", "..path that goes over the obstacle. One more possibility is that the weights for optimization of TEB aren't well balanced for your case. You can try to find a better set of values there, or a quick method, invoke the global viapoints function of TEB, setting the values very small...", "..so that TEB follows global path closely (This would incur an overhead on time and processing power).", "Could you please also specifically test if both, global and local costmaps show this particular obstacle in RViz?", " Because I don't have enough score to upload image, cannot show you the rviz display. But I'm sure that non-lethal obstacle shows in local and global costmaps. \nAnd yes. Currently, I change viapoint parameters to make better. Curious, if TEB can do it by default. \nThanks", "TEB should be able to if it sees the obstruction in costmap! Have you set a round footprint(like in image above) in the TEB params as well(footprint model, etc)?"], "answer": [], "question_code": ["static_layer", "static_layer", "trinary_costmap", "false", "non-lethal", "non-lethal", "non-lethal", "non-lethal", "non-lethal"], "url": "https://answers.ros.org/question/308210/teb-doesnt-work-with-non-lethal-obstacle/"},
{"title": "Which package is best for slam and Kinect?", "time": "2012-05-04 16:50:06 -0600", "post_content": [" ", " ", " ", " ", "I want to do mapping with the kinect, so my robot can avoid obstacles and move around. Looking around there seems to be several options, but not sure which is better (better = low computational requirements):", " has a lot of interest and builds cool looking 3D world, but I believe it runs slow (2 fps) and requires a lot of computational power.", " seems to do what I want, but is designed for laser rangers and not the kinect, so I am not sure if  the overhead of converting from kinect->laser_scans now makes it perform as number 1 above. ", " seems to do a similar job to rgbslam (and it is also designed for the kinect) but there is no mention of performance. ", "I think these are the predominate packages for mapping. Would appreciate any thoughts on these or suggestions of what I am missing.", "Unfortunately I am also have difficulty getting any of these to compile on Fuerte for OSX.", "Thanks."], "answer": [" ", " ", " ", " ", "I can't tell you what is best, but you can speed up rgbdslam significantly by ", "However, the first three options and using ORB will decrease the accuracy.", " ", " ", "Go for ", ", it is the best practical slam out there in all type of sensors and robots. See the comparison in section 2 of this paper: "], "answer_details": ["decreasing the feature count parameters", "decreasing the \"geodesic_candidates\" and \"min_sampled_candidates\"", "setting the kinect driver to QVGA (e.g., using dynamic_reconfigure)", "using SIFTGPU features, if you have a GPU, ORB otherwise", "compile in \"Release\" configuration", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/33310/which-package-is-best-for-slam-and-kinect/"},
{"title": "RVIZ only allows setting goal position in 1 axis for PhantomX Reactor Arm", "time": "2018-12-05 17:34:49 -0600", "post_content": [" ", " ", " ", " ", "I have a PhantomX Reactor arm running with the U2D2 controller and SMPS2Dynamixel. I have RVIZ and MOVEIT! running just fine with 1 exception. I have limited ability to set the goal position. ", "When I grab the \"goal ball\" I can drag it around the screen, but the goal position of the arm does not update. When I release the ball it jumps back to the end effector and the goal position does not update.", "When I click on the directional arrow arrows to move the arm, the green and Blue arrows do the same thing. They move and then when I release the click from dragging them around they disappear.", "However when I grab the red arrow and move the arrow around, the goal position of the arm moves along with the red arrow. When I release the click on the red arrow the goal position is updated.", "Also when I click on the ", " position for the goal position the arm randomly selects a goal position. ", "The arm successfully plans and executes movement from its current position to any goal position without limitations. However I have serious limits on which goal position I can select, I can only select randomly generated goal positions or modify 1 Axis (red arrow).", "Any insight would be greatly appreciated!", "I don't know this particular robot, but if it has anything less than 6 dof you may be experiencing IK difficulties.", "When you select a random valid pose is it changing all the joint angles? Also does your arm initialize fully extended? In the fully extended position it's usual that the only valid first move is to move the end effector towards the Base.", "PeteBlackerThe3rd When I select ", " all of the joint angles change. On initialization the arm appears in RVIZ reflected as it is in reality. There is no change from the first command to subsequent commands.", "Are you able to record a screencast and upload it so we can see what's happening? I'm having a bit of difficulty understanding what's going on."], "answer": [], "question_code": ["<Random Valid>", "<Random Valid>"], "url": "https://answers.ros.org/question/310064/rviz-only-allows-setting-goal-position-in-1-axis-for-phantomx-reactor-arm/"},
{"title": "Using Herkulex Smart Servos with ROS and MoveIt", "time": "2018-12-15 05:36:11 -0600", "post_content": [" ", " ", "I'm currently in the process of building a revolute robot arm powered by Herkulex servos and I wanted to check with the community to see if anyone else already has a ROS driver node for these servos.", "I've found this ", " by user lukaszmitka which provides some basic functionality. There is also an empty wiki ", ". But so far I've not found anything else ROS related out there.", "At the moment I'm planning to extend lukaszmitka's work to create a ROS node to interface with an arbitrary length chain of servos which provides the joint_state topic and joint_trajectory action server required by MoveIt. Before I get started I wanted to check if anyone know of any existing work in this area I haven't found so I don't re-invent the wheel.", "Thanks."], "answer": [" ", " ", "Hey,", "I'm making HerkuleX's ROS package lately.", "I plan to add I_JOG and S_JOG soon.", "I'm currently testing the addition of these two JOG modes.", "Hi ", " good work. I also built a functioning ", " about a year ago after I found one didn't exist [] Feel free to have a look and use any parts you want. I was advised to build a Herkulex interface to the standard ", " system so that they could be used with many existing package such as moveit and the diff_drive_controller. Have you considered adding that interface.", "No, not yet", "I've been to ROS for five months now, so I'm trying to make those things.", "We'll continue to add work as we create platforms like Robotics' TURTLEBOT3 and OpenMANIPULATOR.", "I'll put it on my github when it's done."], "url": "https://answers.ros.org/question/310756/using-herkulex-smart-servos-with-ros-and-moveit/"},
{"title": "MoveIt and Schunk PowerCube", "time": "2018-07-11 14:48:02 -0600", "post_content": [" ", " ", " ", " ", "Hello I am trying to move a physical Schunk LWA3 DOF 7, Schunks PowerCube drivers, and Moveit! ", "I started with the moveit_setup_assistant to build the package with no errors", "Currently I use this launch file to bring everything up. ", "</launch>", "Here is my ", "Here is my ", "When I launch the top launch file everything launches fine. I then run a rosservice call to /driver/init and the trace shows that powercubes were initialized correctly. ", "After that initialization the trace loops and shows:", "Then when attempt to launch $rosrun schunk_bringup dashboard_lwa.launch and attempt to use the gui to execute a command I get:", "After this im not 100% sure what to do. Do I have to write an action server client to send the cob_trajectory_controller FollowTrajectory msgs? ", "Ultimately I want to use MoveIt! to control the physical robot but I cant seem to connect them", "System details:", "Any guidance you guys can provide on PR2 and their controllers would be much appreciated!", "Best,", "Kyle"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Ubuntu 14.04 ", "ROS Indigo", "Peak PCAN FD PCIe "], "question_code": ["<launch>\n\n<arg name=\"useTrajectory\" default=\"true\"/>\n\n<!-- upload robot_description -->\n<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find schunk_hardware_config)/lwa/urdf/lwa.urdf.xacro'\" />\n\n<!-- start robot_state_publisher -->\n<node pkg=\"robot_state_publisher\" type=\"state_publisher\" name=\"robot_state_publisher\"/>\n\n<!-- send ROBOT parameters to parameter server -->\n<rosparam command=\"load\" ns=\"/script_server/arm\" file=\"$(find schunk_default_config)/config/lwa_joint_configurations.yaml\"/>\n\n<!--   \n<rosparam command=\"load\" file=\"$(find schunk_urdf)/config/joint_names.yaml\"/>\n--> \n\n<!-- startup lwa -->\n<node name=\"arm_controller\" pkg=\"schunk_powercube_chain\" type=\"schunk_powercube_chain\" cwd=\"node\" respawn=\"false\" output=\"screen\" >\n    <rosparam command=\"load\" file=\"$(find schunk_hardware_config)/lwa/config/lwa.yaml\"/>\n</node>\n\n<!-- The trajectory controller listens for JointTrajectoryFollowAction and sends velocity commands to the CANopen node -->\n<node ns=\"arm_controller\" name=\"joint_trajectory_controller\" pkg=\"cob_trajectory_controller\" type=\"cob_trajectory_controller\" cwd=\"node\" respawn=\"false\" output=\"screen\" />\n\n<!-- teleop \n<include file=\"$(find schunk_bringup)/tools/teleop.launch\" />\n-->\n\n<include file=\"$(find schunk_urdf)/launch/move_group.launch\">\n    <arg name=\"publish_monitored_planning_scene\" value=\"true\" />\n</include>\n\n<include file=\"$(find schunk_urdf)/launch/moveit_rviz.launch\"/>\n", "  controller_manager_ns: pr2_controller_manager\n  controller_list:\n      - name: arm_controller\n        action_ns: follow_joint_trajectory\n        default: true\n        joints:\n                - arm_1_joint\n                - arm_2_joint\n                - arm_3_joint\n                - arm_4_joint\n", "<launch>\n\n    <arg name=\"moveit_controller_manager\"\n       default=\"pr2_moveit_controller_manager/Pr2MoveItControllerManager\"/>\n    <param name=\"moveit_controller_manager\"\n         value=\"$(arg moveit_controller_manager)\"/>\n\n    <arg name=\"controller_manager_name\"\n       default=\"pr2_controller_manager\" />\n    <param name=\"controller_manager_name\"\n         value=\"$(arg controller_manager_name)\"/>\n\n    <arg name=\"use_controller_manager\" default=\"false\" />\n    <param name=\"use_controller_manager\"\n         value=\"$(arg use_controller_manager)\" />\n\n    <rosparam file=\"$(find schunk_urdf)/config/controllers.yaml\"/>\n\n</launch>\n", "[ INFO] [1531334913.821277479]: Waiting for operationmode service to become available\n", "[ERROR] [WallTime: 1531338986.348944] Parameter /script_server/arm/service_ns does not exist on ROS Parameter Server, aborting...\n"], "url": "https://answers.ros.org/question/297042/moveit-and-schunk-powercube/"},
{"title": "unexpected robot behaviour with navigationstack", "time": "2018-08-21 07:32:02 -0600", "post_content": [" ", " ", "Hello everybody,", "i have some problems again and hope someone can help me. Following scenario:", "I'm pretty sure the move_base package is quiet powerful, but now comes the weird thing: the robot doesn't follow the path at all. It seems like it is ignoring the path at all and trying to find a way to the goal of it's own with horrible results. Something like:", "One idea i had is that this might be the escape sequences that the move_base-node provides. So i tried to switch them off. The base is still doing point turns and also driving backwards.", "So, before i start programming my own node that takes the path (list of stamped points) and calculate the velocity commands for x/theta, is anybody here experienced enough for being able to make a guess based on the \"files\" above what might cause this behavior?", "Thanks for everybody taking the time for reading this.", "Lets start with the shortened .launch file:", "And here are the .yaml files - unfortunately they are all separate in the tutorial and i don't know if i can mix them all together in one big file (especially since two of them are loaded with a name space tag):", "[costmap_common_params.yaml]", "[local_costmap_params.yaml]", "Alot of things don't make alot of sense here. Your max, min velocity and acc are same. you width or height defined for global map, lookup what all these params do and change them accordingly, and if it still shows weird behaviour then update the question.", "min/max are set to the same value by purpose. By that i want to make sure that i always know, how fast the robot is. It's simply just allowed to drive with 0.1m/s . Since the max vel is so slow the acc is set to the same value (higher wouldn't have made sense in my mind)."], "answer": [" ", " ", " Your robot is 1 meter wide but inflation set to 0.25. Goal tolerance at 0.05 seems excessively restrictive.  Can you post a screen shot of path and costmap from RVIZ? There is a Nav Stack tuning tutorial that helped me.  "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "I have a robot platform, diff-drive with laser scanner mounted pointing forward", "i have already recorded a map with slam (g-mapping i guess was the one i used in the end)", "i have set up the navigation-stack with move_base like described ", "i can see everything in rviz (map, cost map, base, laser scans)", "i can set a goal in rviz and (most of the time) get a nice looking path ", "turning 180deg and driving in the opposite direction. Or, ", "instead of just passing straight through some obstacles in front of it, turn 270deg and than heading directly towards one of them. Or", "ignoring the curve in the plan at the very beginning and just drive the first 1.5meters straight, than make the 90deg turn that has summed up in order to follow the rest of the path"], "question_code": ["<launch> \n    <include file ... \" />          <!-- joystick, motordriver, odometrie, inverse kinematik -->\n    <node pkg=\"map_server\"  ... />  <!-- load saved map -->\n    <node pkg=\"amcl\"    ... />  <!-- correct odom errors -->\n\n    <node pkg=\"move_base\"   type=\"move_base\"  name=\"move_base\" respawn=\"false\"  output=\"screen\">\n                <rosparam file=\"$(find nav_config)/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" /> \n                <rosparam file=\"$(find nav_config)/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" /> \n                <rosparam file=\"$(find nav_config)/local_costmap_params.yaml\" command=\"load\" /> \n                <rosparam file=\"$(find nav_config)/global_costmap_params.yaml\" command=\"load\" /> \n                <rosparam file=\"$(find nav_config)/base_local_planner_params.yaml\" command=\"load\" /> \n                <rosparam file=\"$(find nav_config)/move_base_params.yaml\" command=\"load\" />\n    </node>\n\n\n    <param name=\"robot_description\" ... />\n    <node name=\"robot_state_publisher\" ... />\n</launch>\n", "map_type: costmap\nobservation_sources: laser_scan\ntransform_tolerance: 0.25\nobstacle_range: 5.0\nraytrace_range: 5.0\ninflation_radius: 0.25\nxy_goal_tolerance: 0.05\nyaw_goal_tolerance: 0.25\n\nfootprint: [\n  [0.50, 0.35],\n  [-0.30, 0.35],\n  [-0.30, -0.35],\n  [0.50, -0.35]\n]\n\nlaser_scan: {\n  sensor_frame: laser_link,\n  data_type: LaserScan,\n  topic: /mp470/laser_scan,\n  marking: true,\n  clearing: true\n}\n", "local_costmap:\n  global_frame: odom\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency ..."], "url": "https://answers.ros.org/question/301121/unexpected-robot-behaviour-with-navigationstack/"},
{"title": "How do I calibrate odometry ?", "time": "2018-08-12 12:51:58 -0600", "post_content": [" ", " ", "Hi, I've been working on a cheap robot powered by Ros : RPi 3 and all-in-one kit with chassis and 4 motors.\nSo it's a differential drive, powered by ros_control with diff_drive_controller.", "I used the wheels encoder (front_left and front-right only) to compute odometry. I made a few tests and my inline odometry was perfect (5 meters in on direction computes effectively 5 meters in ROS with sensors).", "I was implementing navigation stack with Xtion when I realized odometry was jumping on first turn, making it impossible to map the room.\nSo i made a few more test : I draw a 1.5m square on floor, and set my grid to 1.5 meter cell length.", "When I drive 1.5m straight forward :", "Everything goes well.\nAs soon as I turn the car (here 90 degrees to the right) :\n", "It looks a lot more like 270 degrees...", "A few reasons why : ", "Motors are really cheap and induce a lot of drift (2 kinds) : sometimes the front wheel is turning but not the rear wheel. The robot turns a bit but translate as well, and it's not computed. Sometimes the wheel just drifts without the robot moving.", "How can I take that in account for computing odometry ?", "I double(triple) checked my values for track and wheel diameter, problem appears only when turning, straight forward odometry is computed just fine.", "Also, when I try to visualize odometry topic on rviz, it crashes with this message : ", "Has someone got any clue ?", "Thanks a lot", "There are numerous reasons why you would be getting drift in your odometry: integration errors, sensor noise, etc. This is inherent in dead reckoning. You should look into using something like a Kalman filter like the ", "Thanks jayess I'll try to look into that and will get feedback here !", "Be the robot turning the wrong way, or be it turning too far the right way?", "It's turning too far the right way. I think Jayess is right, i need to implement accelerometer, but the one i bought seems to not be operational..."], "answer": [], "question_code": ["rviz: /build/ogre-1.9-mqY1wq/ogre-1.9-1.9.0+dfsg1/OgreMain/src/OgreNode.cpp :405 : virtual void Ogre::Node::setPosition(const Ogre::Vector3&):  assertion \u00ab !pos.isNaN() && \"Invalid vector supplied as parameter\" \u00bb failed.\n"], "url": "https://answers.ros.org/question/300376/how-do-i-calibrate-odometry/"},
{"title": "Release process of buildfarm", "time": "2018-07-24 03:46:58 -0600", "post_content": [" ", " ", "Hi,", "currently I have a failing build (xenial, x64) in Travis due to a missing package on the shadow-fixed repository.", "The package is \"ros-kinetic-moveit-planners-ompl\", yet I don't want to solve this special issue, but gain more understanding of the overall process.", " In the build farm I see that everythink works:\n ", " On the status page of  ", "  I can see that there is currently no green light on (testing aka shadow-fixed)\n ", " Also on  ", " \nthere is no debian package for the respective architecture. ", "How could I gain more insight what is happening here and why?"], "answer": [" ", " ", "When a new package is built and added to the buildfarm repository, packages which depend on it are removed from the repo before being rebuilt with the newly built dependency in order to prevent syncing packages with an inconsistent and possibly incompatible state to the testing and eventually main repositories.", "Syncs to the testing repo wait for the last in-progress build to finish before running automatically. If a package fails and doesn't get rescheduled before a sync runs and there are enough packages built successfully, the sync will occur and remove those packages from the testing repository. Later, when the build is retriggered, the missing packages should sync once all packages in the distro finish.", "The buildfarm is designed to be somewhat ", " with failures do to operational glitches being resolved automatically by later syncs."], "url": "https://answers.ros.org/question/298368/release-process-of-buildfarm/"},
{"title": "Stage not displaying execution time, robot's position or axis", "time": "2018-08-31 10:44:51 -0600", "post_content": [" ", " ", "Hi,", "I'm trying to use stage in order to perform some simulations. I'm currently using the Melodic distro on a Ubuntu 18.04.  When I start a simulation on stage, some information are not availabe such as robot's positions when I click on one of them, the Y and X axis, or the execution time. Previously, I had the Kinetic version of ROS and all those information were  available as soon as I started stage. I uploaded an image as an example of my execution. It's possible to see that the Y and X axis are not displayed, the bottom left corner were the execution time was supposed to be is just an empty space, and no information regarding the clicked robot (the red one with a green bounding box) is available. ", "I already tried to reinstall the stage package but it didn't work. ", "Here is part of my '.world' file regarding stage's GUI window:"], "answer": [], "question_code": ["window\n(\n    size [ 650.000 500.000 ] \n    scale 36.995   \n    center [ 0 0 ]\n    rotate [ 0  0 ]\n    show_data 1 \n)\n"], "url": "https://answers.ros.org/question/302179/stage-not-displaying-execution-time-robots-position-or-axis/"},
{"title": "the error about  \"roslaunch\"when driving the teleoperating part", "time": "2018-06-23 14:49:21 -0600", "post_content": [" ", " ", " ", " ", " I just following the guide:  ", " \nand when testing turtleBot Installation using the command: ", "it just run successfully(seems to) and print this :", "Afaik this is not a problem: the computer (laptop?) running the turtlebot software doesn't expose any battery information through ACPI, so the ", " cannot read it. It prints that info and then exits. All the other nodes should still be running."], "answer": [], "question_code": ["roslaunch turtlebot_bringup minimal.launch\n", "SUMMARY\n========\n\nPARAMETERS\n * /app_manager/auto_rapp_installation: False\n * /app_manager/auto_start_rapp: \n * /app_manager/capability_server_name: capability_server\n * /app_manager/local_remote_controllers_only: False\n * /app_manager/preferred: [{'rocon_apps/chi...\n * /app_manager/rapp_package_blacklist: []\n * /app_manager/rapp_package_whitelist: ['rocon_apps', 't...\n * /app_manager/robot_icon: turtlebot_bringup...\n * /app_manager/robot_name: turtlebot\n * /app_manager/robot_type: turtlebot\n * /app_manager/screen: True\n * /app_manager/simulation: False\n * /app_manager/use_gateway_uuids: False\n * /bumper2pointcloud/pointcloud_radius: 0.24\n * /capability_server/blacklist: ['std_capabilitie...\n * /capability_server/defaults/kobuki_capabilities/KobukiBringup: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiBumper: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiCliffDetection: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED1: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED2: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiWheelDropDetection: kobuki_capabiliti...\n * /capability_server/defaults/std_capabilities/Diagnostics: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/DifferentialMobileBase: kobuki_capabiliti...\n * /capability_server/defaults/std_capabilities/LaserSensor: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/RGBDSensor: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/RobotStatePublisher: turtlebot_capabil...\n * /capability_server/defaults/turtlebot_capabilities/TurtleBotBringup: turtlebot_capabil...\n * /capability_server/nodelet_manager_name: capability_server...\n * /capability_server/package_whitelist: ['kobuki_capabili...\n * /cmd_vel_mux/yaml_cfg_file: /home/luzixuan/tu...\n * /description: Kick-ass ROS turtle\n * /diagnostic_aggregator/analyzers/input_ports/contains: ['Digital Input',...\n * /diagnostic_aggregator/analyzers/input_ports/path: Input Ports\n * /diagnostic_aggregator/analyzers/input_ports/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/input_ports/timeout: 5.0\n * /diagnostic_aggregator/analyzers/input_ports/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/kobuki/contains: ['Watchdog', 'Mot...\n * /diagnostic_aggregator/analyzers/kobuki/path: Kobuki\n * /diagnostic_aggregator/analyzers/kobuki/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/kobuki/timeout: 5.0\n * /diagnostic_aggregator/analyzers/kobuki/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/power/contains: ['Battery', 'Lapt...\n * /diagnostic_aggregator/analyzers/power/path: Power System\n * /diagnostic_aggregator/analyzers/power/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/power/timeout: 5.0\n * /diagnostic_aggregator/analyzers/power/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/sensors/contains: ['Cliff Sensor', ...\n * /diagnostic_aggregator/analyzers/sensors/path: Sensors\n * /diagnostic_aggregator/analyzers/sensors/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/sensors/timeout: 5.0\n * /diagnostic_aggregator/analyzers/sensors/type: diagnostic_aggreg...\n * /diagnostic_aggregator/base_path: \n * /diagnostic_aggregator/pub_rate: 1.0\n * /icon: turtlebot_bringup...\n * /interactions/interactions: ['turtlebot_bring...\n * /interactions/pairing: True\n * /interactions/rosbridge_address: localhost\n * /interactions/rosbridge_port: 9090\n * /interactions/webserver_address: webapp.robotconce...\n * /mobile_base/base_frame: base_footprint\n * /mobile_base/battery_capacity: 16.5\n * /mobile_base/battery_dangerous: 13.2\n * /mobile_base/battery_low: 14.0\n * /mobile_base/cmd_vel_timeout: 0.6\n * /mobile_base/device_port: /dev/kobuki\n * /mobile_base/odom_frame: odom\n * /mobile_base/publish_tf: True\n * /mobile_base/use_imu_heading: True\n * /mobile_base/wheel_left_joint_name: wheel_left_joint\n * /mobile_base/wheel_right_joint_name: wheel_right_joint\n * /name: turtlebot\n * /robot/name: turtlebot\n * /robot/type: turtlebot\n * /robot_description: <?xml version=\"1....\n * /robot_state_publisher/publish_frequency: 5.0\n * /rosdistro: indigo\n * /rosversion: 1.11.21\n * /turtlebot_laptop_battery/acpi_path: /sys/class/power_...\n * /use_sim_time: False\n * /zeroconf/zeroconf/services: [{'domain': 'loca...\n\nNODES\n  /zeroconf/\n    zeroconf (zeroconf_avahi/zeroconf)\n  /\n    app_manager (rocon_app_manager/rapp_manager.py)\n    bumper2pointcloud (nodelet/nodelet)\n    capability_server (capabilities/capability_server)\n    cmd_vel_mux (nodelet/nodelet)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    interactions (rocon_interactions/interactions_manager.py)\n    master (rocon_master_info/master.py)\n    mobile_base (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    turtlebot_laptop_battery (laptop_battery_monitor/laptop_battery.py)\n\nauto-starting new master\nprocess[master]: started with pid [30240]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to 5e19e66a-771c-11e8-a2ed-1008b1db0ce9\nprocess[rosout-1]: started with pid [30253]\nstarted core service [/rosout]\nprocess[robot_state_publisher-2]: started with pid [30271]\nprocess[diagnostic_aggregator-3]: started with pid [30272]\nprocess[mobile_base_nodelet_manager-4]: started with pid [30273]\nprocess[mobile_base-5]: started with pid [30275]\nprocess[bumper2pointcloud-6]: started with pid [30285]\nprocess[cmd_vel_mux-7]: started with pid [30298]\nprocess[turtlebot_laptop_battery-8]: started with pid [30309]\nprocess[capability_server-9]: started with pid [30319]\nprocess[app_manager-10]: started with pid [30329]\nprocess[master-11]: started with pid [30333]\nprocess[interactions-12]: started with pid [30341]\nprocess ...", "turtlebot_laptop_battery"], "url": "https://answers.ros.org/question/295138/the-error-about-roslaunchwhen-driving-the-teleoperating-part/"},
{"title": "Is it possible to launch a urdf file in gazebo without using xacro and clean up urdf file", "time": "2018-06-12 23:46:01 -0600", "post_content": [" ", " ", " ", " ", "I am using this launch file to launch a urdf file into gazebo. But it comes as \"libGL error: failed to create drawable\". What could be the problem. Is it also possible that moment of intertias of different parts being too low is causing the problem?", "libGL errors are probably issues with your graphics drivers, and are probably unrelated to your code. I would look for a built-in gazebo example and try running that to confirm that gazebo works.", "It's definitly related with the graphic drivers and it can even be that your graphic card isn't powerful enough. I have those error messages too but it doesn't  prevent gazebo from running."], "answer": [], "question_code": ["<launch>\n\n  <!-- these are the arguments you can pass this launch file, for example paused:=true -->\n  <arg name=\"paused\" default=\"false\"/>\n  <arg name=\"use_sim_time\" default=\"true\"/>\n  <arg name=\"gui\" default=\"true\"/>\n  <arg name=\"headless\" default=\"false\"/>\n  <arg name=\"debug\" default=\"false\"/>\n  <arg name=\"model\" default=\"$(find firstrobot)/urdf/robot1.urdf\"/>\n\n  <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched -->\n  <include file=\"$(find gazebo_ros)/launch/empty_world.launch\">\n    <arg name=\"debug\" value=\"$(arg debug)\" />\n    <arg name=\"gui\" value=\"$(arg gui)\" />\n    <arg name=\"paused\" value=\"$(arg paused)\"/>\n    <arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/>\n    <arg name=\"headless\" value=\"$(arg headless)\"/>\n  </include>\n\n  <param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(arg model)\" />\n\n  <!-- push robot_description to factory and spawn robot in gazebo -->\n  <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\"\n        args=\"-z 1.0 -unpause -urdf -model robot -param robot_description\" respawn=\"false\" output=\"screen\" />\n\n  <node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\"  name=\"robot_state_publisher\">\n    <param name=\"publish_frequency\" type=\"double\" value=\"30.0\" />\n  </node>\n\n</launch>\n"], "url": "https://answers.ros.org/question/293985/is-it-possible-to-launch-a-urdf-file-in-gazebo-without-using-xacro-and-clean-up-urdf-file/"},
{"title": "help~ please , failure install ros on raspberry pi", "time": "2017-05-01 15:39:46 -0600", "post_content": [" ", " ", " ", " ", "i am trying to install the desktop version. everything goes fine until i use this command line :", "it keeps told me this: ", "and if try to install it by: ", "\nit gives me:", "very urgent, what should i do???", "Since you are installing on a raspberry pi, you don't have much computing power available and so Gazebo is not likely to be needed. Unless you actually need Gazebo, you would be better off installing the bare-bones configuration and installing other packages as you need them."], "answer": [], "question_code": ["rosdep install --from-paths src --ignore-src --rosdistro indigo -y -r --os=debian:jessie\n", "ERROR: the following rosdeps failed to install\napt: command [sudo -H apt-get install -y gazebo7] failed\napt: Failed to detect successful installation of [gazebo7]\n", "sudo -H apt-get install -y gazebo7", "Reading package lists... Done\nBuilding dependency tree \nReading state information... Done\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\ngazebo7 : Depends: libgazebo7 (= 7.0.0+dfsg-2osrf1~jessie1) but it is not going to be installed\nDepends: libsdformat4 but it is not going to be installed\nDepends: libtbb2 but it is not installable\nRecommends: gazebo7-plugin-base but it is not going to be installed\nE: Unable to correct problems, you have held broken packages.\n"], "url": "https://answers.ros.org/question/260813/help-please-failure-install-ros-on-raspberry-pi/"},
{"title": "Integrate free space into octomap", "time": "2018-04-03 09:51:25 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am looking into how octomap integrates information from PointCloud2 into the Octree structure of octomap. ", "\n   Specifically for free space coming from a LIDAR. Let's take the velodyne_pointcloud sensor simulator for example. ", " ", "I give a more general explanation below.\n   But after reading the code for the octomap_server, at OctomapServer.cpp does the last else of the insertScan method insert free space of rays that make it all the way through here? Am I correct in my interpretation that laser beams that have not reached any obstacle are marked as NaN?", "I understand that for each point in the point cloud the space between the origin and the point is marked as free. ", "\n   Is there a way to mark as free space where there is no obstacle to generate the point in the point cloud? ", "\n   As there are few obstacles in the world I am using, the part of the world completely free stays as unexplored.  ", "It's easier to see graphically. ", "\n Here is the occupied space with the ground, a wall, and a minor obstacle:", "The robot is at the center of the purple \"circle\". Try to memorize the position relative to the wall - is several maximum resolution voxels away. The corresponding free space (from the same perspective) is here in green:   ", "After paying a bit of attention, the image shows that there is much more explored, free space between the UAV and the wall than in any other direction. ", "\n This could just be because of the sensor capabilities/position. However, the sensor is a velodyne (360\u00ba sensing). Plus the robot had just spin on himself (yaw spin) to overcome any sensor issue. ", "\n Here is a closer view of the sensor: ", " ", "I'm not sure if this is just octomap being designed to work only with the points of the point cloud or if there is some parameter that I tune to change the data integration behavior."], "answer": [" ", " ", "In case anyone has the same issue. ", "\nThe octomap_server incorporated only free space when the distance from the origin (found by tf) and the point (in the point cloud) is larger than the maximum range.   ", "I'm working with a simulated Hokuyo sensor (LaserScan). In this case, the laser beams that do not detect an obstacle have inf value. That translated into a point cloud is a NaN (not a very far away point).", "My solution: fork the laser_geometry package and create a node that translates inf into a point at maximum range. ", " I case anybody needs it  ", " ", " The repo where I am using this is  ", " \nThere is no directly launchable launch file because I'm using docker with px4 (which requires some setup). But the instructions are at the jupyter notebook on folder _generate_plots. Look at try 9. ", "The main things are that your launch has: ", "\n- ", " ", "\n- The topic for octomap_server is free_cloud and  the maximum range declared here matches the definition in the model.", "A bit late perhaps, but wouldn't ", " have helped here? Using ", " would remove the need for your fork, it would seem.", "It looks promising but I didn't know the package before :) If I end up switching it I'll post the alternative solution. I did feel strange that none had needed this before, a million times :)"], "answer_code": ["<node name=\"free_cloud\"         type=\"free_cloud_node\"      pkg=\"laser_free_to_cloud\"/>", "<node pkg=\"octomap_server\" type=\"octomap_server_node\" name=\"octomap_builder\" output=\"screen\">\n    <param name=\"frame_id\" type=\"string\" value=\"map\" />   \n    <remap from=\"cloud_in\" to=\"/free_cloud\" />\n    <param name=\"latch\" value=\"false\" />\n    <param name=\"publish_free_space\" value=\"true\"/>\n    <param name=\"sensor_model/max_range\" value=\"10\"/>\n</node>\n", "laser_filters"], "url": "https://answers.ros.org/question/287390/integrate-free-space-into-octomap/"},
{"title": "teensy 3.6 independant node over ethernet", "time": "2017-02-09 04:35:13 -0600", "post_content": [" ", " ", "I am working on a project that I expect to use upwards of 6 MCUs to control steering, power, sensors & manipulation. The laptop I am running on has two USB ports and I want to avoid running multiple instances of rosserial or TCP interpreter nodes that then publish & subscribe. Ideally it will make the system more flexible and will allow the laptop not to be a bottle neck, it can just do the heavy processing.", "It is probably clear that ROS is something new to me but I enjoyed the tutorials. Having used ros-serial to interface with turtle-sim (S-Bus decoding from RC Transmitter) I can see there is a lot of potential but I feel as though even the embedded hardware world is starting to move away from serial networks. Has anyone looked at developing a full ROS node on the new teensy 3.6 hardware. It has a significant ammount of ram and flash space that could hopefully hold a full stack.", "Has anyone done any work on something similar? Would this be a worthwhile task to develop if not? Is there some major challenges I am overlooking?", "Hi ", "Did you get anywhere with this approach ", "I have a similar problem with my robotic platform", "Cheers,\nNick ", "No I didn't but I think an interim solution would be to run threads that maintain a TCP connection with each host device transmitting and updating the relevant topics. This has obvious overheads though", " please don't use an ", " to ask a ", ". This isn't a forum. You can ask a new question and reference this question or leave a comment."], "answer": [], "url": "https://answers.ros.org/question/254225/teensy-36-independant-node-over-ethernet/"},
{"title": "Stdr and RViz", "time": "2018-02-09 19:46:22 -0600", "post_content": [" ", " ", " ", " ", "My understanding: STDR provides a simulator of a 2d Robot and it's environment (map). Which means that it listens for variations of cmd_vel and moves an imaginary robot on a simulated field. The field can have obstacles, and this imaginary robot will not be able to power through the walls. Presumably you can cmd_vel all you like and your odom will tell you that you've not moved.", "Separate but connected, STDR also lets me visualize the map and the robot to allow me to see what's going on. And I can create more robots and give them different sensors all that will publish on the expected Topics.", "Did I get that right?", "So my question is then: Why does Stdr have a GUI and still sometimes we use it with RViz?  In other words, why would you need RViz when using Stdr? I am sure this question comes from my not quite understanding what RViz does and it's relationship with other packages. ", "\"It's just that it's a nice tool to render the dataflows running through your application in context (ie: their spatial and temporal relationships) in a 3D world.\" -- I thought RViz just did 2d?", "In general I am not exactly clear yet what RViz displays that STDR gui does not. The map is from the same /map service and the robots position, orientation, and sensor displays are the same aren't they? Thanks!"], "answer": [" ", " ", " ", " ", "Why does Stdr have a GUI [..]", "Because it's convenient to be able to verify that your robot's ", " (ie: the data it publishes on topics and uses for its control algorithms (in the broadest sense)) has some correspondance (ie: makes sense) to the 'real world', which is what STDR is simulating.", "and [why do] we use it with RViz? ", "Because for humans it's infinitely easier to assess whether the internal state of your robot makes sense by looking at a 3D graphical representation than by looking at raw msg contents streamed onto a console by something like ", ".", "In other words, why would you need RViz when using Stdr?", "You don't ", " to use RViz at all. It's just that it's a nice tool to render the dataflows running through your application in context (ie: their spatial and temporal relationships) in a 3D world.", "I am sure this question comes from my not quite understanding what RViz does and it's relationship with other packages.", "RViz doesn't really have a relationship with other packages. You can use ROS and have a robot be completely controlled by ROS without ever touching or using RViz.", "You would probably be missing out on a great visual debugging tool and one of the easier ways to get an insight into what your robot is doing though. And all 'for free', as RViz is just one other subscriber to the data that gets published by your nodes anyway.", "Finally, in this particular context -- which is not unique to STDR, but would seem to be similar to RViz+Gazebo, RViz+V-REP, etc -- the idea is that the simulator is only a ", " for a real robot. There is no 'GUI' for your real robot or the real world. Considering this, having RViz running next to your simulator doesn't seem so redundant any more I believe.", "Edit:", "I thought RViz just did 2d?", "No. RViz is a full 3D rendering application. See ", " video (old, but it shows RViz' capabilities well). It's just that in a lot of navigation setups, it's used with its ", " view, which makes it look like it's 2D.", "In general I am not exactly clear yet what RViz displays that STDR gui does not. The map is from the same /map service and the robots position, orientation, and sensor displays are the same aren't they?", "Again, that may be true for this particular setup, but is not true in general (especially not when you replace your simulator with the real world).", "RViz shows you the world ", " (or: as your nodes publish their messages). Your simulator GUI shows you the internal state of the simulator.", "Those two do not necessarily have to be identical.", "RViz is a generic tool, it's not limited to use with STDR.", "See extended question in original post please. Thanks!"], "answer_code": ["rostopic echo .."], "url": "https://answers.ros.org/question/282296/stdr-and-rviz/"},
{"title": "Kinect node failing (both freenect and openni)", "time": "2018-03-01 03:47:28 -0600", "post_content": [" ", " ", " ", " ", "I am getting an error when i launch freenect or openni", "The error is listed below ", "Additional debug information ", "I've got the same error messages. Does anyone have a solution?", "It's lack of memory . If you are using something like the RPI , use something more powerful .", "Also try setting the respawn parameter true", "Thanks for the fast reply. In my setup, I already use a Odroid xu4, which is more powerful than the RPI, but the error msg still remains. I will try set te respawn parameter to true."], "answer": [], "question_code": ["roslaunch freenect_launch freenect.launch\nroslaunch openni_launch openni.launch\n", "`camera/camera_nodelet_manager-1] process has died [pid 20440, exit code -6, cmd /opt/ros/kinetic/lib/nodelet/nodelet   manager __name:=camera_nodelet_manager __log:=/home/ubuntu/.ros/log/682c9cf4-1d32-11e8-b8c6-00044b65e5df/camera-camera_nodelet_manager-1.log].\nlog file: /home/ubuntu/.ros/log/682c9cf4-1d32-11e8-b8c6-00044b65e5df/camera-camera_nodelet_manager-1*.log\n[FATAL] [1519897834.875999385]: Failed to load nodelet '/camera/ir_rectify_ir` of type `image_proc/rectify` to manager     `camera_nodelet_manager`\n", "USB device disappeared, cancelling stream 82 :(\nUSB camera marked dead, stopping streams\nwrite_register: 0x0006 <= 0x00\nsend_cmd: cmd=0003 tag=0012 len=0004: -4\nsend_cmd: Output control transfer failed (-4)\nwrite_register: send_cmd() returned -4\nterminate called after throwing an instance of 'std::runtime_error'\nwhat():  freenect_process_events error\n"], "url": "https://answers.ros.org/question/284056/kinect-node-failing-both-freenect-and-openni/"},
{"title": "Where can I find outlet templates for PR2 Plug package?", "time": "2018-02-10 21:08:39 -0600", "post_content": [" ", " ", " I have been trying to get the pr2_plugs package (allowing PR2 robots to plug themselves into power outlets autonomously) working and tested. I obtained the code from the Github repository listed on the pr2_plugs ROS page ( ", " ). The launch files (and previous questions on this forum) reference pre-existing outlet template models, which seem to at some point have been provided in the pr2_plugs_actions folder. In particular, I am referring to outlet templates such as green_2x1white, which I assume models a power outlet. However, I have been unable to locate these outlet templates in the various branches of the pr2_plugs repository and anywhere. Would anyone be able to point me toward these files so that I can download them and use them to test the functionality of the pr2_plugs package? ", "Thanks!"], "answer": [" ", " ", " ", " ", "This is not an authoritative answer (as in: OSRF/ex-Willow Garage ppl should perhaps way in), but after some searching it would appear that the plug template files were never actually really \"provided in the pr2_plugs_actions\" folder, but downloaded (at build time) by CMake from ", " (see ", "). This was removed in ", " for some reason (the commit message is unfortunately rather terse and doesn't tell us why: \"bugs\").", "That host doesn't exist any more, but it would seem the files on that host (or at least the ones that you seem interested in) have been moved to ", ", specifically to ", ".", "I want to note though, that given the age of the packages and the fact that ROS has progressed (ie: changed) so much since then, I'm not confident things will work out-of-the-box, so even with those files, you'll probably run into problems.", "Edit: ", " appears to be the most recent fork of ", " (at least on github). The last commit message says: \"Fully compiles and runs in simulation\" (but that was 2 years ago).", "PR2 maintenance was taken over by Clearpath when WillowGarage shut down. If you have a PR2 you might want to contact them (one of their employees (\"TheDash\") appears to have done most of the maintenance of ", ")."], "answer_code": ["pr.willowgarage.com", "download.ros.org", "pr2_plugs", "pr2_plugs"], "url": "https://answers.ros.org/question/282382/where-can-i-find-outlet-templates-for-pr2-plug-package/"},
{"title": "Camera matrix drastically changes between successive calibrations of low-res camera", "time": "2017-12-23 05:56:44 -0600", "post_content": [" ", " ", "I'm using camera_calibration package to calibrate a camera using checkerboard (in gazebo environment). The resolution of the camera is 84x84, and I've managed to calibrate the camera thrice.", "I'd also like to add that all 'X', 'Y', 'scale' and 'skew' bars were not green."], "answer": [], "question_code": ["camera matrix\n18.869876 0.000000   42.232541\n0.000000   18.817910 46.843425\n0.000000   0.000000   1.000000\n\nprojection\n19.568193       0.000000       41.369775   0.000000\n0.000000         19.412340     45.832273   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n-0.003044 0.000774 -0.000227 -0.001105 0.0000\n\n\n\ncamera matrix\n59.194174 0.000000   41.422579\n0.000000   59.220697 43.212139\n0.000000   0.000000   1.000000\n\nprojection\n58.686417       0.000000       40.837182   0.000000\n0.000000         58.767673     42.702490   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n-0.004351 0.007664 0.000151 -0.000845 0.0000\n\n\n\ncamera matrix\n46.885763 0.000000   41.564895\n0.000000   46.850151 41.490714\n0.000000   0.000000   1.000000\n\nprojection\n46.414661       0.000000       41.097325   0.000000\n0.000000         46.358101     41.072972   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n0.002144 -0.001044 0.000596 0.000203 0.0000\n"], "url": "https://answers.ros.org/question/278107/camera-matrix-drastically-changes-between-successive-calibrations-of-low-res-camera/"},
{"title": "Camera matrix drastically change between successive calibrations of a low resolution camera", "time": "2017-12-23 05:57:50 -0600", "post_content": [" ", " ", "I'm using camera_calibration package to calibrate a camera using checkerboard (in gazebo environment). The resolution of the camera is 84x84, and I've managed to calibrate the camera thrice.", "I'd also like to add that all 'X', 'Y', 'scale' and 'skew' bars were not green.", " Calibration window:  ", " \nGazebo env.:  ", "How big is your checkerboard? Did you verify the corner positions? 84x84 is an extremely(!) small camera, what do you try to simulate?"], "answer": [], "question_code": ["camera matrix\n18.869876 0.000000   42.232541\n0.000000   18.817910 46.843425\n0.000000   0.000000   1.000000\n\nprojection\n19.568193       0.000000       41.369775   0.000000\n0.000000         19.412340     45.832273   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n-0.003044 0.000774 -0.000227 -0.001105 0.0000\n\n\n\ncamera matrix\n59.194174 0.000000   41.422579\n0.000000   59.220697 43.212139\n0.000000   0.000000   1.000000\n\nprojection\n58.686417       0.000000       40.837182   0.000000\n0.000000         58.767673     42.702490   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n-0.004351 0.007664 0.000151 -0.000845 0.0000\n\n\n\ncamera matrix\n46.885763 0.000000   41.564895\n0.000000   46.850151 41.490714\n0.000000   0.000000   1.000000\n\nprojection\n46.414661       0.000000       41.097325   0.000000\n0.000000         46.358101     41.072972   0.000000\n0.000000         0.000000       1.000000     0.000000\n\ndistortion\n0.002144 -0.001044 0.000596 0.000203 0.0000\n"], "url": "https://answers.ros.org/question/278108/camera-matrix-drastically-change-between-successive-calibrations-of-a-low-resolution-camera/"},
{"title": "The new path planner(ftc_local_planner) can't work with hector_mapping together", "time": "2017-07-18 22:11:56 -0600", "post_content": [" ", " ", " ", " ", "Hi, all, this question is similiar to the question we have asked ", ". But we found new situations, so we change the subject and ask a new question here. ", "We are using hector mapping and move_base to implement an online mapping and path planning.  We changed the combination of global planner and local planner. ", "If the global planner is  ", ", \nthe local planner is ", ", the robot moves fine and builds the map, although the terminal outputs following warning.", "[ WARN] [1500425881.365911556,\n  793.010000000]: Control loop missed its desired rate of 4.0000Hz... the\n  loop actually took 0.5200 seconds", "[ INFO] [1500425881.365978957,\n  793.010000000]: Got new plan", "[ WARN] [1500425881.784682569,\n  793.430000000]: Control loop missed its desired rate of 4.0000Hz... the\n  loop actually took 0.4200 seconds ", "Then if the local planner is ", ",  the global planner is navfn or other methods (We tested a linear global planner programmed by us). The robot almost doesn't move(only with rotation in place sometimes), and the following warning outputs. We also decreased the update frequency of costmap and the control frequency, but it didn't work. ", "We also tried other slam package like ", ". The robot works fine. ", "Can anyone who is familiar with move_base give us advices?", "Thanks!", " For experiments in simulator(gazebo), the hardware is with Intel\u00ae Core\u2122 i5-6500 CPU @ 3.20GHz \u00d7 4 , memory is 15.6 GiB. We thinks such hardware is totally enough for running hector and ftc_local_planner. So it is so weird that the above warning information is output. As I am told from some questions in  ", " , this is usually because the CPU power or memory  is not enough for running such packages. ", "Hi,", ". Thank you for your advices! After by adding the time calculation into the source code in sereral different places. We found that the most time-consumed part is the function getXPose in ", ". ", " Since getXpose is often called by other functions, so the accumulative time is so much that blocks the execution. ", "However, when running with other mapping pacakges such as gmapping, karto, ", "\nWe can change the ...", "as I understand you are trying to simultaneously map and navigate. have you tried using ", " with a previously built map?", "Thank you! We didn't use amcl with a previously built map. In this case, a map server is enough. How the map is built is not important. So the mapping packages don't need to be launched.", "Yes, that is the idea. But I was actually wondering if your objective is indeed building a map while navigating?", "Our objective is not only building a map. The robot also have to finish other tasks such as cleaning room(with coverage path planning) like cleaner vacuum robot. So the tasks and map building are running at the same time. We also obtain the robot's pose from map building to guide the path planning.", "Have you tried ", "?", "No. Actually we also considerd ", " which uses their own move_base node. But we think even if we run them successfully, we can't use the specified planner, our problem is not solved either", "We think the key to solve this problem is: how to modify the move_base(or the planner)  or hector mapping  to make them work together successfully.", "Edit my answer"], "answer": [" ", " ", " ", " ", "Thanks for your measurements ", ". \nThis log calculation time is occurs of to slow tf. Hector_mapping and ftc_planner use often tf transformations. So tf is overwhelmed. \nI have minimized the calls of tf transformations and put this changes for the moment in an new branch \"fix_transformation\". If I have test the changes in detail I will merge it to the master branch.", "This could coused because the ftc_planner need to much time to calculate. \nCan you show your ftc_planner paramaters?", "And how big is your global map with your are build (Resolution)?", "Drives your robot fine, if you use ftc_planner on a already created global map (without hector mapping)?", "I cant reconstruct this problem. If I use the same parameter the planner works fine with hector mapping. ", "I have add a Debug output in the ftc_planner. Now you should see how long the plan calculation is. Mostly it should lover than 0.0005 seconds It is shown the calculation time in seconds.", "The calculation time of the ftc_planner is only dependet on the global plan lenght and its resolution. So what is your global planner?", "You can also activate the debug output at the hector map (in the launch file) to see the calculation time of it.", "My Output with same parameters and navfn in gazebo simulation:", "The calculation of the path is very simple (you can look in die driveForward Methode at the code). There is no couse because the calculation is so long. ", "But i have also used some times the turtlebot, and there is very slow in calculation because many\nbackground processes. But my turtlebot wasnt so slow.", "I have install the turtlebot packages and drives the turlebot in simulation. The calculation isnt so high like yours >0.01 seconds. I have run: Gazebo, RVIZ, Turtlebot amcl_demo.launch (with ftc_local_planner) and hector_mapping. I have deactivate \"pub_map_odom_transform\" at hector_mapping.\nAnd I have nicer parameters for FTCPlanner:", "Out global planner is ", ". We tested it mostly in simulated environment by gazebo . If we change this global planner to be navFn\uff0c the problem still exists.\nThanks!", "ok. I have no plan why it doesnt work with hector. For my personal interest can you check the Debug output of ftc_planner (the new git push before an hour), and how big the calculcation time is? (Enable Debug output with roslaunch rqt_logger_level rqt_logger_level)", "roslaunch rqt_top rqt_top could also be usefull.", "Thanks! You mentioned the amcl_demo.launch. Did you do your experiments with a known map built by hector mapping in advance?  We didn't use amcl. We launched  hector mapping and movebase at the same time. It's a navigation with online mapping. The robot's pose is obtained from mapping, not from amcl", "Because I cant reconstruct this problem. If you want use ftc_planner you must show where the calculation problem is. Thereforce add duration debug outputs in the driveToward in ftc_planner.cpp. Or you must use dwa_planner with hector (But with personal experience dwa drives very worse)", "We updated the question, please read it and give us advices, Thanks!"], "question_code": ["[ WARN] [1500431832.147879220, 6740.630000000]: Map update loop missed its desired rate of 1.0000Hz... the loop actually took 21.1200 seconds\n[ WARN] [1500431854.738732473, 6763.210000000]: Control loop missed its desired rate of 4.0000Hz... the loop actually took 22.5800 seconds\n[ WARN] [1500431854.738900263, 6763.210000000]: Map update loop missed its desired rate of 1.0000Hz... the loop actually took 21.5800 seconds\n"], "answer_code": ["[DEBUG] [1501069889.603913517, 2303.245000000]: Planning...\n[DEBUG] [1501069889.606579979, 2303.245000000]: Got Plan with 65 points!\n[DEBUG] [1501069889.606638318, 2303.245000000]: Generated a plan from the base_global_planner\n[DEBUG] [1501069889.606699296, 2303.245000000]: Planner thread is suspending\n[DEBUG] [1501069889.883098172, 2303.514000000]: Publishing feedback for goal, id: /move_base-1-2287.511000000, stamp: 2287.51\n[DEBUG] [1501069889.883148549, 2303.514000000]: Publishing feedback for goal with id: /move_base-1-2287.511000000 and stamp: 2287.51\n[DEBUG] [1501069889.883180475, 2303.514000000]: Got a new plan...swap pointers\n[DEBUG] [1501069889.883201015, 2303.514000000]: pointers swapped!\n[DEBUG] [1501069889.883239073, 2303.514000000]: FTCPlanner: Old Goal == new Goal.\n[DEBUG] [1501069889.883253688, 2303.514000000]: In controlling state.\n[DEBUG] [1501069889.883484852, 2303.515000000]: FTCPlanner: max_point: 37, distance: 0.256672, x_vel: 0.233333, rot_vel: 0.022912, angle: 0.017184\n[ INFO] [1501069889.883600382, 2303.515000000]: FTCPlanner: Calculation time: 0.001000 seconds\n[DEBUG] [1501069889.883617439, 2303.515000000]: Got a valid command from the local planner: 0.233, 0.000, 0.023\n[DEBUG] [1501069889.883634019, 2303.515000000]: Full control cycle time: 0.000611030\n", "FTCPlanner:\n  max_x_vel: 0.2\n  max_rotation_vel: 2.0\n  min_rotation_vel ..."], "url": "https://answers.ros.org/question/266759/the-new-path-plannerftc_local_planner-cant-work-with-hector_mapping-together/"},
{"title": "Reach closer position to goal with obstacle in the middle", "time": "2017-09-27 04:16:11 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "My problem is the following: I'm working in a project in which a robot has to perform several activities and while navigating it is possible that an object appears in the way. I want it to stop in front of the object and sometimes that happens if the global planner plans. But if the robot can see that the path is blocked from some distance the global planner doesn't even plan, which means that the robot doesn't move at all from a far distance, which is not what I want.", "I tried to initialize a fake costmap so that I could plan without the observations of the lasers, but that didn't work out. Tried to configure local and local costmaps.", "details:I'm using a front and rear hokuyo for localization and obstacle avoidance, move base, dwa and ros planner.", "Any one has any idea how to solve this?", "update:\n", "\nIn this image, the red circle is the goal I want to reach, the green arrow is pointing to my robot(bunch of TF's) and the blue arrow is pointing to my non-mapped object. Global costmaps in light grey, local costmaps in colors.\nSo, in this situation, if I give the goal to the robot it won't even plan because the goal is unreachable. What I want is to get as close to the object as possible. For example, I tried to plan and choose the first reachable position close to the object, but I can't even get a plan to the goal.", "Thanks in advance.", "Can you please add a screenshot of the situation with the local and global costmap displayed?", ", I posted an update on my original post. Thanks for your help!", "Obviously it doesn't plan because the whole path is blocked in the global costmap. I have no idea on how to bring the robot closer the to obstacle in this situation. Also the question is: What is a closer position? Is it somewhere below the goal? Above the goal?", "What might help is to tune the global costmap to a smaller inflation_radius so the global planner might think that it can reach the goal. But this will probably introduce problems in other situations...", "Exaclty. My initial thought was: I'm gonna try to get a plan and go through the plan poses until i get one that is feasible. But that doesn't work, i can't even get a plan. I tried to create a fake costmap, so it could plan through one and execute through another. But I don't think it's possible", "You're right ", ". I don't wan to decrease it to much, might cause some serious crashes", "If you only decrease on the global costmap it should not crash into any wall. Crash avoidance is the job of the local costmap", "I don't think that solves my problem. What i need is a way to fake plan it"], "answer": [], "url": "https://answers.ros.org/question/271626/reach-closer-position-to-goal-with-obstacle-in-the-middle/"},
{"title": "uvc_camera gets a terrible image and usb_camera doesn't set framerate", "time": "2012-10-31 07:30:30 -0600", "post_content": [" ", " ", "I have a Logitech c920 webcam that I want to use with ROS. I originally installed uvc_camera, but it gets a terrible quality image from the camera. Then I tried usb_cam, which gets a fantastic quality image, but only at 10fps (which just isn't good enough).", " ", "So my question is:\nHow do I get a good image out of uvc OR\nHow do I set frame rate with usb_camera", "It should be noted that guvcview gets a fantastic image using the uvc driver, though ros does not.", "launch file contents:", "I have exactly the same problem with the weird color settings for the uvc_cam. To the slow frame rate of the usb_cam: do you use an usb2.0 port? $lsusb will show you...", "I have exactly the same problem with a BW analog cam (with USB dongle on video0): vlc shows the video correctly, cheese shows the video correctly, both usb_cam and uvc_camera show purple video with different shades of green/red. No matter what pixfmt parameter I set, they always fall back to YUYV.", " After a couple of hours spent fighting with this problem, I solved the issue. I discovered that the correct pixel format for my camera was \"uyvy\". For future reference, I used this launch file:  "], "answer": [" ", " ", " ", " ", "Since you have Logitech C920, which has hardware H264 encoding which is an awesome thing.\nFor usb_cam you can try it by changing the value of pixel_format to H264. Change ", " to ", ".\nThat should work and you'll get high fps with low resource usage. ", "If I do as you suggested I get \n[FATAL] [1383197110.463584559]: Unknown pixel format.\n[ERROR] [1383197110.463895717]: VIDIOC_STREAMOFF error 9, Bad file descriptor", "Okay uvc_cam accepts it, but the frame rate is still slow.", " ", " ", "I had a similar problem with my c920 and the ros usb_cam node ---- VERY low frame rate (~5fps).  On top of that, the H264 option didn't work for my c920 with the usb_cam node.  Try these settings, it gave me full frame 1920x1080 with 30fps on usb_cam node.  Notice the pixel format is mjpeg.  Using these settings,  I was able to just rosbag the /usb_cam/image_raw/compressed topic and display it in rviz just fine.  "], "question_code": ["FOR usb_cam:\n<launch>\n    <node name=\"camera\" pkg=\"usb_cam\" type=\"usb_cam_node\" output=\"screen\" >\n        <param name=\"video_device\" value=\"/dev/video1\" />\n        <param name=\"image_width\" value=\"1280\" />\n        <param name=\"image_height\" value=\"720\" />\n        <param name=\"pixel_format\" value=\"yuyv\" />\n        <param name=\"camera_frame_id\" value=\"webcam\" />\n    </node>\n</launch>\n\nFOR uvc_camera:\n<launch>\n  <node ns=\"camera\" pkg=\"uvc_camera\" type=\"camera_node\" name=\"uvc_camera\" output=\"screen\">\n    <param name=\"width\" type=\"int\" value=\"1280\" />\n    <param name=\"height\" type=\"int\" value=\"720\" />\n    <param name=\"fps\" type=\"int\" value=\"30\" />\n    <param name=\"frame\" type=\"string\" value=\"webcam\" />\n    <param name=\"device\" type=\"string\" value=\"/dev/video1\" />\n  </node>\n</launch>\n"], "answer_code": ["<param name=\"pixel_format\" value=\"yuyv\" />", "<param name=\"pixel_format\" value=\"H264\" />", "<launch>\n  <node name=\"usb_cam\" pkg=\"usb_cam\" type=\"usb_cam_node\" output=\"screen\" >\n    <param name=\"video_device\" value=\"/dev/video1\" />\n    <param name=\"image_width\" value=\"1920\" />\n    <param name=\"image_height\" value=\"1080\" />\n    <param name=\"pixel_format\" value=\"mjpeg\" />\n    <param name=\"camera_frame_id\" value=\"usb_cam\" />\n    <param name=\"io_method\" value=\"mmap\"/>\n  </node>\n</launch>\n"], "url": "https://answers.ros.org/question/47209/uvc_camera-gets-a-terrible-image-and-usb_camera-doesnt-set-framerate/"},
{"title": "The hector_mapping and move_base can't work together ,but cartographer can.", "time": "2017-07-09 08:00:51 -0600", "post_content": [" ", " ", " ", " ", "Hi , all :", "I want to use hector_mapping and move_base to work together on the turtlebot which has a rplidar,but when I shart up move_base to get a path planning, I find move_base continuously print the alarm information, ", "which leads to the robot path planning is not smooth. The robot dosen't move. I have already reduced update_frequency, publish_frequency of  global_costmap and local_costmap.  I also decrease the controll frequence of planner in move base. However, move_base still continuously output the warning information. The robot dosen't move a little.", "PS:  when I only use hector_mapping build a map, everything is ok. who can tell me why move_base print this warn and how to solve this problem ? Thanks.", "\nI also tried the ", "  and ", " with movebase. They all worked fine. ", "Actually, we have monitored the usage of CPU and memory when running these SLAM packages. We have the order of consumption is ", ".", "Hopefully anyone give us any advices!", " For experiments in simulator(gazebo), the hardware is with Intel\u00ae Core\u2122 i5-6500 CPU @ 3.20GHz \u00d7 4 , memory is 15.6 GiB. We thinks such hardware is totally enough for running hector and ftc_local_planner. So it is so weird that the above warning information is output. As I am told from some questions in  ", " , this is usually because the CPU power or memory  is not enough for running such packages. ", "We found new situations and asked a new question here, please move to ", ". Thanks!"], "answer": [" ", " ", "Hello,in my opinion,the move_base package work in a different environment.Move_base need the amcl and odom information,but the hector don't approve that.It seems that move_base use a different map message,too.\n       And,how about your hector map?Did it build correctly?I run the hector map with some errors.", "hi, the mapping process is without move_base. After a local map( eg. a room) is built, the move_base node is used. We use such combination(we don't use amcl) to implement an online mapping and path planning. Thank you!", "We found new situation and asked a new question here, please move to ", " Thanks!"], "question_code": ["[ WARN] [1499649044.602318070, 109.050000000]: Control loop missed its desired rate of 3.0000Hz... the loop actually took 3.1000 seconds\n[ WARN] [1499649044.602556838, 109.050000000]: Map update loop missed its desired rate of 3.0000Hz... the loop actually took 2.7667 seconds\n"], "url": "https://answers.ros.org/question/265834/the-hector_mapping-and-move_base-cant-work-together-but-cartographer-can/"},
{"title": "Georeferencing map coordinates", "time": "2017-07-31 01:07:27 -0600", "post_content": [" ", " ", "I was wondering how ROS handles georeferencing of map coordinates. My robot currently sets its position when it starts up as position 0.0. However this doesn't give any information about where I am in on earth.", "How do I convert map coordinates to GPS coordinates and back again? That is, how do typical ROS-powered robots do it? TF doesn't seem suitable for this because converting GPS to map coordinates isn't as simple as a matrix transformation, if I understand correctly.", "I'm also trying to figure out how to do this with map_server. As I understand, the map_server is initialized with a yaml file containing map coordinates of the underlying map. However, my robot could start up at any point on earth, so the yaml file would always be incorrect, if I set it to a particular map coordinate. How can I set up the map_server based on a GPS coordinate, rather than a map coordinate?"], "answer": [" ", " ", "It is true if you do large scale navigation that the euclidian assumptions of most maps does not hold. however if you use multiple maps they can make a pretty good approximation (That's basically how UTM coordinates work)", " describes how coordinate frames between map and earth frames is recommended to work.", "Can you define what \"large scale\" vs \"small scale\" means? Is 600 kilometers considered large scale? or 6,000 kilometers?", "I didn't specify as it's entirely dependent on your application and how precisely you need to know position. If you have higher tolerance you can allow larger approximations by using larger tangent planes."], "url": "https://answers.ros.org/question/267708/georeferencing-map-coordinates/"},
{"title": "Suggestions for drone simulator with ROS + Gazebo", "time": "2017-11-08 14:40:50 -0600", "post_content": [" ", " ", " ", " ", " Hi there, I am looking for a decent starting point for a drone project. I've checked the TUM_SIM ( ", " )  ", "And it seems quite stable, nevertheless, anyone has some experience with drones and ROS? What do you recommend? And also for the motion planning, should moveit work for a task like this? Or should I look into other 3d planners?\nThanks for the tips!", "It's a little hard to give suggestions without any knowing what you're looking for.", "Sorry if I was being a bit to vague, I am looking for a good simulator package to integrate with gazebo (like the tum_sim) in which I can simulate an environment, control my drone and plan movement.I know gazebo is powerful enough but I don't want to start from scratch, so far the tum_sim looks good"], "answer": [" ", " ", "my experience with drones is connected with the use of the ", " simulator in conjunction with the ", ".", "MAVROS - ", " - ", "another one Microsoft ", ", but i didn't use it.", "Thanks! This seems just what I need!", " ", " ", " is a good one I've used with success. ", "I'm not sure what can be used for 3D planning though. I'm pretty sure MoveIt does not support that (I think it only supports planar holonomic platforms) but I could be wrong. "], "url": "https://answers.ros.org/question/275417/suggestions-for-drone-simulator-with-ros-gazebo/"},
{"title": "Installing Microsoft Kinect on Ubuntu 16.04 to Use skeletal Tracking", "time": "2017-10-20 09:06:45 -0600", "post_content": [" ", " ", " Hi Guys,\nI'm really stuck with getting the Kinect for windows working on my linux laptop running ROS Kinetic with Ubuntu 16.04 LTS. I've downloaded 'freenect2' drivers and got the test stream working from this link fine.\n ", " \nHowever, my aim is to use skeletal tracking. I found a ROS package  ", "  and installed this as well openni_camera and openni_launch. Whenever I try to run roslaunch openni_launch openni.launch i get an error message saying it cannot find the device 'no device connected' despite the kinect being plugged into the identical usb port as before and powered on. with the command lsusb three Microsoft Corp connections appear and the white light on the adapter of the Kinect is powered up. Whenever I try to run 'rosrun openni_tracker openni_tracker' with roscore it gives Cannot Launch node of this type ! error message. Is this a lost cause on Ubuntu Kinetic?  ", "N.B. I tried downloading the KInectSensor avin2 drivers and installed them with the install.sh command. I also installed Nite v1.5 and installed it with the install.sh command. None of these make any difference. I also tried openni2_tracker but this brings up the same 'no devices connected!' ", "Any help would be really appreciated.", "I had similar problems when I tried to get my kinect set up. Finally resorted to just running the kinect in Unity3D on a windows machine and using rosbridge to communicate with my ros machine. I had no problems with this method...", " Try looking at the Linux section here:  ", "Also, this doesn't appear to be ROS-related so you may have more success in getting answers from another site such as Stack Exchange or the like. If you do post a question on another site, please include a link to it here to help others.", "it is ROS related as I'm trying to use ROS packages openni_tracker and openni_launch", "Right. But, the issue seems to be a hardware issue. Did you try the troubleshooting guide that I posted?"], "answer": [], "url": "https://answers.ros.org/question/273657/installing-microsoft-kinect-on-ubuntu-1604-to-use-skeletal-tracking/"},
{"title": "Segmentation fault with graphic object in Rqt plugin", "time": "2017-09-10 03:59:38 -0600", "post_content": [" ", " ", "I am building my own rqt plugin. \nI found this ", " and I managed to include it in my rqt plugin. As creators said, it \"uses a lot of CPU power\", but I managed to have it working after the pression of a key", "these lines of code add the widget", "these lines creates the callback", "with this calling the widget works when pressing \"A\" on keyboard", "after that I successfully subscribe to the joy node, I tried to use it in my plugin like", "Joy and all the other callbacks work, but if I tried to use the attitude callback in the joy subscription, I obtain "], "answer": [" ", " ", "You don't want to be changing qt displays directly from ros callbacks, they are owned by different threads.  I'm not sure if repaint() actually violates that thread boundary or not, assuming it does you would avoid it by something like", "and then in the joy callback:", " You could alternatively have a qt timer that only updates the attitude widget at 30 Hz or less which would limit cpu usage (look at  ", " ).   The attitude widget api is calling repaint for setRoll and setPitch, it would be good to modify it so it repaint is called separately since roll and pitch are always getting updated at the same time and then there would be half the paint events. ", "It works, thank you. I have to specify for others they may can read that the first of your line has to be written into the plugin class but outside the constructor, the second line has to be inside the constructor. But now it works!!"], "question_details": [" ", " ", " ", " ", " (The widget can not update itself so fast) I try to lighten the widget by deleting the hover thing and the white lines, but it does not change my situation", "How can I check, and maybe solve this?", "Turn the widget into an openGL widget would help me? (I never tried openGL widget, never build them)"], "question_code": ["#manual insert attidue into attitude layout \nself.attitude_w = AttitudeIndicator()\nself.attitude_w.setPitch(self.pitch)\nself.attitude_w.setRoll(self.roll)\nself._widget.attitude_layout.addWidget(self.attitude_w)\n", "def _ATTITUDE(self, ROLL, PITCH):\n    self.attitude_w.setRoll(ROLL)\n    self.attitude_w.setPitch(PITCH)\n", "#random attitude\nself.shortcut_a = QShortcut(QKeySequence(Qt.Key_A), self._widget)\nself.shortcut_a.setContext(Qt.ApplicationShortcut)\nself.shortcut_a.activated.connect(lambda: self._ATTITUDE(self.roll,self.pitch) #lambda allow to call function with args)\n", "try:\n  self.myjoy = Float32MultiArray\n  self.myjoy = rospy.Subscriber(\"joy\", Joy, self.joyread, queue_size=1)\nexcept ValueError, e:\n    rospy.logerr('Error connecting topic (%s)'%e)    \n\n    def joyread(self,data):\n        self.myjoy = data\n        self.sensing0 = 10*self.myjoy.axes[0]\n        self.sensing1 = 10*self.myjoy.axes[1]\n        self.pitch = 10*self.myjoy.axes[3]\n        self.roll =  10*self.myjoy.axes[2]\n        self._sensors(self.sensing0, self.sensing1) # this works continously\n        self._ATTITUDE(self.roll,self.pitch) #this made the plugin crash\n", "Segmentation fault (core dump created)\n"], "answer_code": ["self.do_update_attitude = QtCore.pyqtSignal(float, float)\nself.do_update_attitude.connect(self.update_attitude)\n...\ndef update_attitude(self, roll, pitch):\n    self.attitude_w.setRoll(roll)\n    self.attitude_w.setPitch(pitch)\n", "self.do_update_attitude.emit(self.roll, self.pitch)\n"], "url": "https://answers.ros.org/question/270591/segmentation-fault-with-graphic-object-in-rqt-plugin/"},
{"title": "moveit demo.launch, rviz initializing forever", "time": "2017-06-28 10:25:39 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone, here is my problem:", "I setup my model using the setup assistant of moveit. Everything seems to work. I export the package, save it under src/ in the catkin_ws/. Then I run \"roslaunch my_package demo.launch\" .", "Here the process starts until the terminal says in green \" You can start planning now! \" The RViz initializing picture is freezed forever. ", "However, I get in the terminal  \"[INFO] [1498661559]: MoveGroup context initialization complete\".", "What do you think about that? is my PC not strong enough or do I have another problem?\nThank you in advance,\nStefano", "Details: ", "-I can run RViz stand-alone, no problem. ", "-I am using a 4 years old laptop, intel i3 quadcore 2.53GHz, geforce 610M.", "-I am using Ubuntu 16.04 LTS installed by a friend's usb flashkey", "-can I run gazebo if the grafics drivers are not installed?", "Complete terminal:", "Can you run RViz stand-alone? What sort of hardware are you using? A regular desktop PC, a raspberry pi, something else? Which Linux version is this, how did you install it, do you have the graphics drivers for your video card installed? Please ", " your question and add these details.", "I made my robot model using the setup assistant as well. The next message I get after the \"You can start planning now\" is \"loading robot model mybot\". So maybe there is something wrong with your robot model. Did you write the urdf file yourself? Have you successfully included it in setup assistant?", ": Thanks for guiding my question. I updated it.", " : Yeah, I dont get \"loading robot model mybot\". I made the urdf file using the SolidWorks plug-in \"export to urdf\". I included it successfully in the setup assistant, i can move my joints and see my model. How ever I have some warnings", "I corrected the errors in the warnings. I have no more warnings. Still no response from RViz. I hope i don't have to reinstall ros ^^.", "can I run gazebo if the grafics drivers are not installed", "sometimes. It won't run very well, but it'll probably produce some image.", "It might help if you copy-paste the complete terminal output into your question (use the ", " button (the one with ", " on it)). Can you tell us anything about the model you're trying to load? How big are the meshes? If they are too detailed, loading can be very slow.", "in the options within the SlidWorks plug-in I could choose corse meshes or fine meshes. I choose fine...  How ever I dont get until the \"loading robot model mybot\" phase (see terminal in the updated question). In addition i left my computer loading for 3-4 hours with no results.", "I am going to try anyway with a corse meshing. I keep you updated."], "answer": [" ", " ", "Guys the problem was the nvidia optimus as gvdhoorn said. I switched the driver from \"nvidia binary driver\" to \"Nouveau display driver (open source)\".\nNow it works fine. So the file were not the problem. Thanks again for your help. It makes things really easier. Have a good day <3", "If you have the chance -- and the perseverance -- I would really recommend you try to figure out a way to get optimus to work properly: the nouveau drivers really don't cut it when you start working with any 'real' ROS applications, especially not when you start visualising a lot of data in RViz."], "question_code": ["    zampieri@zampieri-Hack:~/catkin_ws$ roslaunch moveit_config_complete_arm_3 demo.launch \n... logging to /home/zampieri/.ros/log/8bbb4536-5bd0-11e7-b431-7ce9d3541a7d/roslaunch-zampieri-Hack-26924.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://zampieri-Hack:45617/\n\nSUMMARY\n========\n\nPARAMETERS\n * /joint_state_publisher/use_gui: False\n * /move_group/allow_trajectory_execution: True\n * /move_group/arm/longest_valid_segment_fraction: 0.005\n * /move_group/arm/planner_configs: ['SBLkConfigDefau...\n * /move_group/arm/projection_evaluator: joints(joint_1,jo...\n * /move_group/controller_list: [{'joints': ['joi...\n * /move_group/jiggle_fraction: 0.05\n * /move_group/max_range: 5.0\n * /move_group/max_safe_path_cost: 1\n * /move_group/moveit_controller_manager: moveit_fake_contr...\n * /move_group/moveit_manage_controllers: True\n * /move_group/octomap_resolution: 0.025\n * /move_group/planner_configs/BFMTkConfigDefault/balanced: 0\n * /move_group/planner_configs/BFMTkConfigDefault/cache_cc: 1\n * /move_group/planner_configs/BFMTkConfigDefault/extended_fmt: 1\n * /move_group/planner_configs/BFMTkConfigDefault/heuristics: 1\n * /move_group/planner_configs/BFMTkConfigDefault/nearest_k: 1\n * /move_group/planner_configs/BFMTkConfigDefault/num_samples: 1000\n * /move_group/planner_configs/BFMTkConfigDefault/optimality: 1\n * /move_group/planner_configs/BFMTkConfigDefault/radius_multiplier: 1.0\n * /move_group/planner_configs/BFMTkConfigDefault/type: geometric::BFMT\n * /move_group/planner_configs/BKPIECEkConfigDefault/border_fraction: 0.9\n * /move_group/planner_configs/BKPIECEkConfigDefault/failed_expansion_score_factor: 0.5\n * /move_group/planner_configs/BKPIECEkConfigDefault/min_valid_path_fraction: 0.5\n * /move_group/planner_configs/BKPIECEkConfigDefault/range: 0.0\n * /move_group/planner_configs/BKPIECEkConfigDefault/type: geometric::BKPIECE\n * /move_group/planner_configs/BiESTkConfigDefault/range: 0.0\n * /move_group/planner_configs/BiESTkConfigDefault/type: geometric::BiEST\n * /move_group/planner_configs/BiTRRTkConfigDefault/cost_threshold: 1e300\n * /move_group/planner_configs/BiTRRTkConfigDefault/frountier_node_ratio: 0.1\n * /move_group/planner_configs/BiTRRTkConfigDefault/frountier_threshold: 0.0\n * /move_group/planner_configs/BiTRRTkConfigDefault/init_temperature: 100\n * /move_group/planner_configs/BiTRRTkConfigDefault/range: 0.0\n * /move_group/planner_configs/BiTRRTkConfigDefault/temp_change_factor: 0.1\n * /move_group/planner_configs/BiTRRTkConfigDefault/type: geometric::BiTRRT\n * /move_group/planner_configs/ESTkConfigDefault/goal_bias: 0.05\n * /move_group/planner_configs/ESTkConfigDefault/range: 0.0\n * /move_group/planner_configs/ESTkConfigDefault/type: geometric::EST\n * /move_group/planner_configs/FMTkConfigDefault/cache_cc: 1\n * /move_group/planner_configs/FMTkConfigDefault/extended_fmt: 1\n * /move_group/planner_configs/FMTkConfigDefault/heuristics: 0\n * /move_group/planner_configs/FMTkConfigDefault/nearest_k: 1\n * /move_group/planner_configs/FMTkConfigDefault/num_samples: 1000\n * /move_group/planner_configs/FMTkConfigDefault/radius_multiplier: 1.1\n * /move_group/planner_configs/FMTkConfigDefault/type: geometric::FMT\n * /move_group/planner_configs/KPIECEkConfigDefault/border_fraction: 0.9\n * /move_group/planner_configs/KPIECEkConfigDefault/failed_expansion_score_factor: 0.5\n * /move_group/planner_configs/KPIECEkConfigDefault/goal_bias: 0.05\n * /move_group/planner_configs/KPIECEkConfigDefault/min_valid_path_fraction: 0.5\n * /move_group/planner_configs/KPIECEkConfigDefault/range: 0.0\n * /move_group/planner_configs/KPIECEkConfigDefault/type: geometric::KPIECE\n * /move_group/planner_configs/LBKPIECEkConfigDefault/border_fraction: 0.9\n * /move_group/planner_configs/LBKPIECEkConfigDefault/min_valid_path_fraction: 0.5\n * /move_group/planner_configs/LBKPIECEkConfigDefault/range: 0.0\n * /move_group/planner_configs/LBKPIECEkConfigDefault/type: geometric::LBKPIECE\n * /move_group/planner_configs/LBTRRTkConfigDefault/epsilon: 0.4\n * /move_group/planner_configs/LBTRRTkConfigDefault/goal_bias: 0.05\n * /move_group/planner_configs/LBTRRTkConfigDefault/range: 0.0 ...", "101010"], "url": "https://answers.ros.org/question/265030/moveit-demolaunch-rviz-initializing-forever/"},
{"title": "Performance of serial communication when using Arduino rosserial?", "time": "2017-06-05 11:16:21 -0600", "post_content": [" ", " ", "I collected the data from a sensor (sampling frequency is 100 Hz) with Arduino serial communication (without using ROS), and now I'm trying to collect the same data from same sensor using ROS (rosbag) with Arduino Mega on the Raspberry Pi 3. However, when I plot the data collected, it seems to be a little bit quite bad compared to when I didn't use ROS. ", "I'm wondering if using the Arduino with ROS is not reliable for the frequency 100 Hz (I mean sometimes it may send wrong data), or is it because of Raspberry Pi 3 is not powerful enough to run ROS reliably?", "I'm just asking to know if the problem is from ROS or Raspberry Pi side or not, so I can identify the root of my problem.", "Thank you."], "answer": [], "url": "https://answers.ros.org/question/263202/performance-of-serial-communication-when-using-arduino-rosserial/"},
{"title": "Publishing 2D Array with sensor_msgs/Image", "time": "2017-04-05 08:47:03 -0600", "post_content": [" ", " ", " ", " ", "I want to publish a 2D array, which is populated by int values. These values are delivered by a radar sensor and represent the reflected power of detected targets. So these values give information about the distance between the detected objects and the sensor.", "This is an extract of my source code:", "int", " rasterImage;         //array-variable\nint sizeX;                    //amount of pixel along X-axis\nint sizeY;                    //amount of pixel along Y-axis\nint index;                   //represents value of reflected power", "pointCloud = new int* [sizeX];\nfor (int x = 0; x < sizeX; x++)\n{\npointCloud[x] = new int [sizeY];\nfor (int y = 0; y < sizeY; y++)\n{\nint index = (sizeX * y) + x;\npointCloud[x][y] = rasterImage[index];\n}\n}", "for (int y = 0; y < sizeY; y++)\n{\nfor (int x = 0; x < sizeX; x++)\n{\nprintf(\"%d \", pointCloud[x][y]);\n}\nprintf(\"\\n\");\n}", "I want to create a image to visualize the detected objects. Now the problem is to fill the matrix data into the message and to publish rasterImage via sensor_msgs/Image...", "depth_image.height = sizeY; ", "\ndepth_image.width =  sizeX; ", "\n//depth_image.encoding = rgb8; ", "\ndepth_image.is_bigendian = false; ", "\ndepth_image.step = sizeX * sizeY; ", "\ndepth_image.data = rasterImage; --> ERROR MESSAGE", "ERROR: no match for \u2018operator=\u2019 (operand types are \u2018sensor_msgs::Image_<std::allocator<void> >::_data_type {aka std::vector<unsigned char,=\"\" std::allocator<unsigned=\"\" char=\"\"> >}\u2019 and \u2018int*\u2019)", "Has anybody an advice to fix this problem?", "==============================================", "If I use std:vector<uint8_t> rasterImage, I get this error:", "error: cannot convert \u2018std::vector<unsigned char=\"\">\u2019 to \u2018int", ", int", ", double", ")\u2019\n         res = Walabot_GetRawImageSlice(rasterImage, &sizeX, &sizeY, &sliceDepth, &power);", "The function I'm using to get the values is this one:", "How to solve this problem?", "Thanks!", "I was about to devel into something Similar with the Walabot do you have your code on Github or anywhere?"], "answer": [" ", " ", "First of all try to use std::vector instead of raw pointers. It is best practice to do so. Your code is in c-style, not cpp style.\nYour problem can be solved doing what i have mentioned."], "answer_code": ["int** pointCloud; //double pointer for 2D Array with dimension sizeX and sizeY\nstd::vector<uint8_t> rasterImage; //array-variable\nint sizeX; //amount of pixel along X-axis\nint sizeY; //amount of pixel along Y-axis\nint index; //represents value of reflected power\n\npointCloud = new int* [sizeX];\nfor (int x = 0; x < sizeX; x++)\n{\n    pointCloud[x] = new int [sizeY];\n    for (int y = 0; y < sizeY; y++)\n    {\n        int index = (sizeX * y) + x;\n        pointCloud[x][y] = rasterImage[index];\n    }\n}\n\nfor (int y = 0; y < sizeY; y++) { for (int x = 0; x < sizeX; x++) { printf(\"%d \", pointCloud[x][y]); } printf(\"\\n\"); }\n\nsensor_msgs::Image depth_image;\n\ndepth_image.height = sizeY;\ndepth_image.width = sizeX;\n//depth_image.encoding = rgb8;\ndepth_image.is_bigendian = false;\ndepth_image.step = sizeX * sizeY;\ndepth_image.data = rasterImage;\n"], "url": "https://answers.ros.org/question/258626/publishing-2d-array-with-sensor_msgsimage/"},
{"title": "Bachelor's Thesis. Is ROS for me? A non hardcore programmer", "time": "2017-03-28 04:32:40 -0600", "post_content": [" ", " ", "Hi!", "I have a Bachelor's thesis with an end-effector on a robot, KUKA KR210. I am mostly an mechinical engineer but i start to get intrested in programming and robots. For this project i am thinking about having cameras on the end-effector as a safety feature to give the end-effector \"eyes\" and can act as an emergancy stop if a human is to close. \nThis end-effector is used in an industry production line and needs to be safe for a fitter to work along side with", "Is this possible to implement? If yes, how should i do it?", "Note; i am very new to this am afraid, but really like to learn and make it work", "I don't want to discourage you, and for a BSc it's probably ok, but a camera connected to a RPi with ROS is ", " going to get safety-rated. What you are describing (\"emergency stop if a human is to close [..] needs to be safe for a fitter to work along side with\") is definitely possible ..", ".. but please consider that companies in this line of work (ie: safety-lasers, co-bots) spend tens of thousands of dollars developing and getting their equipment certified. A KR210 is easily capable of killing a human, so please be careful: ", " make your RPi the only system.", "Thanks for the reply!", "I see, so to get safety-rated level there's a need for a more powerful computer? Or is it the concept with cameras and the pictureprocessing that makes the reactiontimes to slow to be used as an emergency stop?", "No, my point was that making software and hardware ", " suitable for safety critical applications (which acting as an emergency stop is an example of) is hard and difficult. I'd recommend you try and talk to someone with knowledge in this field before you start.", "But this all depends on ..", ".. what the scope of your work is: if this is a one-off experiment for a BSc, and you're tutored by knowledgable people ", " your KR210 has additional safety measures in place, then it ", " be ok to test with your ROS application.", "But don't put this in a production system -- with uninformed ..", ".. users -- where there is a risk of really hurting people or damaging equipment.", "Again, scope is important here, but I just wanted to point to some possible dangers.", "and finally: ", "so to get safety-rated level ..", "there are some experimental computer vision based systems that do something similar to this, but 1) I don't know whether they've already been certified and 2) \"getting safety-rated\" is probably not doable for you.", "The scope, a robot picks up a plasticpanel and then place it under a car, then a fitter will assemble it with screws. The car will move with a constant speed and the robot with the panel will move along. So we need to make the layout and the end-effector safe for the fitter with .."], "answer": [" ", " ", " ", " ", "Yes, is possible, I would suggest instead to place the camera over/front of the robot where you can have a fixed view pane of the Anlage, and the people that is stepping into the cell", "Many tech are developed in that area, you can play with AR and markers, to define Virtual Cages for the Robot.", "OpenCV, ARUCO or any other C++ based image porcessing tool/framework will help as startpoint.", "Thanks for the reply!", "Let me see if i understand you right. So you think it's a better way to have control over the collaborativ area and limit the robots movements in it?", "Can it be programmed to make the robot stop as an \"emergency stop\" if an human gets to close to avoid an collision?", "Or does the robot need to be equipped with, for instance, distance sensors?", "KUKA KR210 is not MRK able as far as I know, so you'll need to be able to trigger the emergency stop as far as the distance between person and Roboter is too close, that it can represents a danger for the person.", "you can define Virtual Rooms or Virtual Cells so the robot can move free inside that area as soon as no person is there...", "Options you have:\n- you can define a infra red carpet, that is triggering a emergency stop when person is in the  \"Danger-Zone\"\n- you can use stereo cameras and make a deepth of the image so you can identify persons.(that is hard core BV)\n-you can use ultrasound, you have many options!", "I think this is the way we will go, using either Pilz systems or maybe SICK cameras to set up zones!"], "url": "https://answers.ros.org/question/258053/bachelors-thesis-is-ros-for-me-a-non-hardcore-programmer/"},
{"title": "Collision issue", "time": "2017-02-18 07:10:17 -0600", "post_content": [" ", " ", " ", " ", "So I am a little confused, my robot plans the route fine but then piles in to the obstacles, the green line is always spot on what it needs to do, the SLAM positions the robot fine etc.  but the actual route it takes (purple) takes it far too near obstacles", "Second question, why is my /map never moving with my robot?", "I have a footprint, and collision enabled on the URDF.  Could this be a motor control issue?"], "answer": [], "question_code": [" local_costmap:\n   global_frame: /map\n   robot_base_frame: /scanmatcher_frame \n   update_frequency: 5.0 \n   publish_frequency: 2.0\n   static_map: false \n   rolling_window: true \n   width: 4.0 # The width of the map in meters. default 10\n   height: 4.0 # The height of the map in meters. default 10\n   resolution: 0.05 # The resolution of the map in meters/cell. default 0.05\n   transform_tolerance: 0.5 \n\nglobal_costmap:\n   global_frame: /map\n   robot_base_frame: /scanmatcher_frame \n   update_frequency: 1.0 \n   publish_frequency: 0.5\n   static_map: true # \n   transform_tolerance: 0.5\n\n\nobstacle_range: 2.5 \nraytrace_range: 3.0\nfootprint: [ [0.08, 0.08], [-0.08, 0.08], [-0.08, -0.08], [0.08, -0.08] ] \ninflation_radius: 0.3 \nmap_type: costmap \nobservation_sources: scan bump \n\nscan: {data_type: LaserScan, topic: scan, marking: true, clearing: true, min_obstacle_height: 0.25, max_obstacle_height: 0.35}\nbump: {data_type: PointCloud2, topic: mobile_base/sensors/bumper_pointcloud, marking: true, clearing: false, min_obstacle_height: 0.0, max_obstacle_height: 0.15}\n\nTrajectoryPlannerROS:\n# Robot Configuration Parameters\n  max_vel_x: 0.5\n  min_vel_x: 0.1\n\n  max_vel_theta:  1.5  \n  min_vel_theta: -1.5 \n  min_in_place_vel_theta: 1.0\n\n  acc_lim_x: 0.6\n  acc_lim_theta: 1.0\n\n# Goal Tolerance Parameters\n  yaw_goal_tolerance: 3.14 \n  xy_goal_tolerance: 0.15 \n\n# Forward Simulation Parameters\n  sim_time: 3.0\n  vx_samples: 6\n  vtheta_samples: 20 \n\n# Trajectory Scoring Parameters\n  meter_scoring: true\n  pdist_scale: 0.6\n  gdist_scale: 0.8 \n  occdist_scale: 0.01 \n  heading_lookahead: 0.325 \n  dwa: true \n\n# Oscillation Prevention Parameters\n  oscillation_reset_dist: 0.05 \n\n# Differential-drive robot configuration\n  holonomic_robot: false\n"], "url": "https://answers.ros.org/question/255029/collision-issue/"},
{"title": "Rviz crashing (core dumped) when running with Gazebo", "time": "2016-12-01 05:45:04 -0600", "post_content": [" ", " ", "Hi to all,", "I'm following a tutorial, for which I have to run a launch file that runs Gazebo (with a wold with three robots) and runs also rviz. What happens constantly is that rviz always crashes, while Gazebo keeps running. This happens also if I run rviz from command line with rosrun (while Gazebo is still on), terminating with the message", "Is it possible that my laptop is not powerful enough?", "Are you running this in a virtual machine?", "No, I'm running everything in a real Ubuntu 14.04. Funny thing is that if I run the same exact launch file on another laptop (definitely less powerful than mine), both gazebo and rviz can live together. I have to imply that there should be other problems on my laptop...", "What kind of graphics hw do you have in your laptop? Do you have the proper drivers installed? A laptop I have here (9 years old) prints: ", ", so the ", " is a bit suspicious (you get that when drivers aren't installed).", "I have a Nvidia GeForce 640M. The drivers are the standard-default ones that come with ubuntu 14.04 (nouveau). I tried installing the proper ones (nvidia-375), but I just messed up xorg (graphic interface wasn't starting anymore). I rolled back, but yet no gazebo and rviz together working.", "Well, I think that's your problem. The ", " ones just aren't up to the task (yet). How did you install the nvidia ones? Via the ", " applet? That should have worked. Running the installer manually isn't really needed on Ubuntu."], "answer": [], "question_code": ["[ INFO] [1480592319.442365758]: rviz version 1.11.15\n[ INFO] [1480592319.442474349]: compiled against Qt version 4.8.6\n[ INFO] [1480592319.442517957]: compiled against OGRE version 1.8.1 (Byatis)\n[ INFO] [1480592319.648792445]: Stereo is NOT SUPPORTED\n[ INFO] [1480592319.648914712]: OpenGl version: 3 (GLSL 1.3).\nSegmentation fault (core dumped)\n", "OpenGl version: 3.3 (GLSL 3.3)", "OpenGl version: 3 (GLSL 1.3)", "nouveau"], "url": "https://answers.ros.org/question/249193/rviz-crashing-core-dumped-when-running-with-gazebo/"},
{"title": "seperate marker color with rviz markers", "time": "2016-09-06 22:49:55 -0600", "post_content": [" ", " ", " ", " ", "When i try to give different marker colour to different marker of type point in rviz, the color of previous point also changes to the current marker color, How can i specify different colour for different marker in rviz? I want to show the detected points with their colour in rviz for a computer vision task so that when a unique point is detected with its position and colour its colour also will be visualized.", "Now when i changed color of q, the color of point p also changes to green. That is my problem, because of which i cant visualize how my computer vision algorithem is performing the required task.", "Did you write a maker publisher program? You need to put your source code here."], "answer": [" ", " ", "I don't really know what the circle looks like. The circle is 3D or 2D? if 2D, which plane? I mean the circle is in plane(x,y), (x,z) or (y,z)? And for point(x,y,z), is ", " always zero?", " its parametric equation of circle in 3d,  ", " . and this:  "], "question_code": ["\ufeff#include <ros/ros.h>\n#include <sensor_msgs/PointCloud2.h>\n#include <pcl_conversions/pcl_conversions.h>\n#include <pcl/point_cloud.h>\n#include <pcl/point_types.h> \ncloud_cb (const sensor_msgs::PointCloud2ConstPtr& input)\n{\n    pcl::PointCloud<pcl::PointXYZ> output;\n    pcl::fromROSMsg(*input,output);\n\n\n    visualization_msgs::Marker points, line_strip, line_list, text_view_facing;\n    points.header.frame_id = line_strip.header.frame_id = line_list.header.frame_id = text_view_facing.header.frame_id = \"/camera_depth_frame\";\n    points.header.stamp = line_strip.header.stamp = line_list.header.stamp = text_view_facing.header.stamp = ros::Time::now();\n    points.ns = line_strip.ns = line_list.ns = text_view_facing.ns = \"lines\";\n    points.action =line_strip.action = line_list.action = text_view_facing.action = visualization_msgs::Marker::ADD;\n    points.pose.orientation.w = line_strip.pose.orientation.w = line_list.pose.orientation.w = text_view_facing.pose.orientation.w = 1.0;\n    points.id = 0; \n    line_strip.id = 1;\n    line_list.id = 2;  text_view_facing.id = 3;\n    points.type = visualization_msgs::Marker::POINTS;\n    line_strip.type = visualization_msgs::Marker::LINE_STRIP;\n    line_list.type = visualization_msgs::Marker::LINE_LIST;  text_view_facing.type = visualization_msgs::Marker::TEXT_VIEW_FACING;\n    points.scale.x = 0.005;  text_view_facing.scale.z=0.1;\n    points.scale.y = 0.005;\n    line_strip.scale.x = 0.0005;  text_view_facing.color.b=1.0;  \n    line_strip.color.b = 1.0;  text_view_facing.color.a=1.0;  text_view_facing.text=\"cup\"; \n    line_strip.color.a = 1.0;  \n    points.color.r = 1.0f;  \n    points.color.a = 1.0;\n    line_list.scale.x = 0.005;\n    line_list.color.a = 1.0;\n    line_list.color.b = 1.0f;\n    geometry_msgs::Point p,q,r,s,t,u;\n   p.x = 19; p.y = 10; p.z = 9;  // ** just for demonstration purpose/ eg.\n  points.points.push_back(p); marker_pub.publish(p);\n  points.color.g = 122; points.color.r=100; points.color.b=0;   //  ** changing the color of new marker\n q.x= 34; q.y=34; q.z=45;\n  points.points.push_back(q); marker_pub.publish(q);\n}\n"], "url": "https://answers.ros.org/question/243231/seperate-marker-color-with-rviz-markers/"},
{"title": "Movebase: robot move backwards when it detects an obstacle", "time": "2016-03-29 06:53:38 -0600", "post_content": [" ", " ", "Hi all,", "What is the parameter that controls how far backwards the robot goes when it detects an obstacle at close proximity? Is it an issue related to the local planner or a setting in move_base?", "Thanks", "<launch>\n  <master auto=\"start\"/>", "<arg name=\"no_static_map\" default=\"false\"/>", "<arg name=\"base_global_planner\" default=\"navfn/NavfnROS\"/> \n  ", "<arg name=\"base_local_planner\" default=\"base_local_planner/TrajectoryPlannerROS\"/>", "<node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\"/>\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\"/>", "</node>\n</launch>"], "answer": [" ", " ", "Moving backwards is handled by base_local_planner. The speed that the robot backs up at is set by escape_vel parameter."], "question_code": ["TrajectoryPlannerROS:\n    # Robot Configuration Parameters\n    acc_lim_x: 0.35        #was 0.5\n    acc_lim_y: 0.35         #was 0.5\n    acc_lim_theta: 0.35     #was 0.5\n    max_vel_x: 0.35         #was 0.5\n    min_vel_x: 0.05\n    max_vel_theta: 1.0\n    min_in_place_vel_theta: 0.05 #was0.4\n    escape_vel: -0.06\n    holonomic_robot: false\n\n    # Goal Tolerance Parameters\n    xy_goal_tolerance: 0.1 #was 0.3\n    yaw_goal_tolerance: 0.1 #was 0.3 lower means harder to achieve\n    latch_xy_goal_tolerance: false\n    path_distance_bias: 20.0 #default 0.6 - wasn't used - see pdist_scale\n    goal_distance_bias: 0.8 #default 0.8 - wasn't used - see gdist_scale\n\n    # Forward Simulation Parameters\n    sim_time: 10.0\n    sim_granularity: 0.025\n    vx_samples: 5\n    vtheta_samples: 20\n    controller_frequency: 20.0 #has to be the same as in move_base.launch\n\n    # Trajectory Scoring Parameters\n    meter_scoring: true\n    pdist_scale: 0.8\n    gdist_scale: 0.6\n    occdist_scale: 0.10 #was 0.01 \n    heading_lookahead: 0.325 - was default \n    heading_scoring: false \n    dwa: false\n    global_frame_id: odom\n\n    # Oscillation Prevention Parameters\n    oscillation_reset_dist: 0.05\n", "<rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/local_costmap_params.yaml\" command=\"load\" />\n\n<rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/global_costmap_params.yaml\" command=\"load\" />\n\n<rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/base_local_planner_params.yaml\" command=\"load\" />\n\n<param name=\"base_global_planner\" value=\"$(arg base_global_planner)\"/>\n<param name=\"base_local_planner\" value=\"$(arg base_local_planner)\"/>  \n<param name=\"recovery_behavior_enabled\" value=\"true\"/> \n<param name=\"recovery_behaviour_enabled\" value=\"true\"/> \n<param name=\"clearing_rotation_allowed\" value=\"true\"/>\n<param name=\"controller_frequency\" value=\"20.0\"/> \n<param name=\"planner_patience\" value=\"5.0\"/>\n<param name=\"planner_frequency\" value=\"0.0\"/>\n<param name=\"controller_patience\" value=\"15.0\"/>\n\n<!--<param name=\"oscillation_timeout\" value=\"40\"/> -->\n\n<!-- Remap into namespace for cmd_vel_mux switching-->\n<remap from=\"cmd_vel\" to=\"/RosAria/cmd_vel\" />\n"], "url": "https://answers.ros.org/question/230417/movebase-robot-move-backwards-when-it-detects-an-obstacle/"},
{"title": "Problem when fusing IMU and Odometry", "time": "2016-06-06 06:54:54 -0600", "post_content": [" ", " ", " ", " ", "Hi, I am trying to use robot localization to fuse the data from an XSense IMU and Odometry Source (Pioneer). The output is not what i expect actually. The resulting odometry looks similar (almost exact) to the odometry of the pioneer? So the filter is not improving or at least changing the result?", "This is my launch file:", "Here is an sample bagfile together with robot localization: ", "Here is the same bagfile without robot localization: ", "\n--- Update ---", "I added this fixed odometry covariance:", "Maybe i am missing something obvious?\nThanks in advance!", "--- Update ---", "I tried some different covariance matrices but nothing changed but when i changed my launch file like so:", "then i got following result: ", "\nits an image, green is the wheel odometry and red the ukf result and blue is the ground truth (i know, really bad drawn^^)", "What i do not understand is why the imu is making the result worse? \nThis data is captures outside on a sidewalk, could this be a problem?\nI am pretty sure that the imu is directed in the right direction but it looks weird...", "-- Update --", "Here is the sample output of each source, first odometry, then imu:", "so you set the covariances of the Odometry by hand? How you do that. Are you actually calculating them or set them to a static value?", "for the odometry i set the covariance to a static value, the imu was already publishing a covariance matrix", "I think thats the main problem. What values did you choose? When you got small values in the covariancematrix of the odometry and high values in the imu covariancemartrix the robot_localisation will \"trust\" the odometry data much more than the imu data und so the imu data has almost no influence.", "i added the covariance matrix to the question, i'll try some larger values and see how it performs", "Please post a sample message from each input. I realize there's a bag, but it's often enough to just see the raw data. See my answer for more information, though."], "answer": [" ", " ", " ", " ", "I can see the following issues in your config:", "Regardless, the main crux of your issue is that you are essentially telling the filter to fuse only the data from the Pioneer odometry. Absolute pose data will have a greater effect on filter output than velocity data, so your differentiated IMU pose (which gets turned into velocity by ", ") is effectively being ignored (really, it's just smoothing the transition between yaw measurements from your wheel odometry). Also, if your IMU provides angular velocity, it typically comes from a completely different sensor (i.e., a gyroscope as opposed to a magnetometer), so you should really be fusing yaw velocity. Try this instead:", "You may want to disable yaw in your IMU altogether if you find it suddenly shifts for no reason while driving. Magnetometers are notoriously awful in the presence of sources of distortion. You may also want to re-enable X and Y in your odometry and disable X and Y velocity. Experiment with it, but whatever you do, don't enable yaw on your Pioneer wheel odometry.", ":", "Your Pioneer odometry appears to be missing a ", ". Not sure why that is, but it will prevent the EKF from using the velocity data in the odometry message.", "thank you very much, i tried what you suggested and mostly it works quite fine. As you already mentioned i had sudden shifts, probably because of tall buildings next to the robot...\nall in all i found a configuration which works out for now (and thanks for the child frame id hint) :)", "I'm guessing it's because your IMU \"odometry\" sensor has a lot of drift. Compare the outputs of each filter and I'm sure you'll see that. If you want to validate it, try integrating your velocity measurements from your IMU odometry topic yourself."], "answer_details": ["You are fusing two absolute sources of absolute pose data: you have yaw set to ", " for both of your inputs. Unless you have your covariances properly tuned, I would advise against this. Pick your best source of yaw data (likely the IMU) and only set yaw to true for that. Technically, since you have ", " mode turned on for your IMU, you are really fusing yaw from the robot and yaw \"velocity\" (differentiated yaw) from the IMU, so you won't see the heading jumping back and forth, but see below for information on why this is probably backwards.", "You have ", " and ", " turned on for the IMU, which is a bit meaningless, as I believe ", " mode is applied in that case.", "You're not using any velocity data. Unlike pose data, fusing disparate source of velocity data, even when the covariances aren't all that well tuned, will cause less erratic behavior.", " ", " ", " ", " "], "question_code": ["<param name=\"frequency\" value=\"30\"/>\n<param name=\"sensor_timeout\" value=\"0.1\"/>\n<param name=\"base_link_frame\" value=\"base_footprint\"/>\n<param name=\"two_d_mode\" value=\"true\"/>\n\n<param name=\"imu0\" value=\"imu/data\"/>\n<rosparam param=\"imu0_config\">[false, false, false,\n                           false, false,  true,\n                           false, false, false,\n                           false, false, false,\n                           false,  false, false]</rosparam>\n<param name=\"imu0_differential\" value=\"true\"/>\n<param name=\"imu0_remove_gravitational_acceleration\" value=\"false\"/>\n\n<param name=\"odom0\" value=\"/odom/fixed_wheel\"/>\n<rosparam param=\"odom0_config\">[true, true,  false, \n                            false, false, true, \n                            false, false, false, \n                            false, false, false,\n                            false, false, false]</rosparam>\n\n  <param name=\"odom0_relative\" value=\"true\"/>\n  <param name=\"imu0_relative\" value=\"true\"/>\n", "Topic: /imu/data == xsense imu data\nTopic: /odom/fixed_wheel == pioneer improved odometry (by hand added covariances)\nTopic: /odom/ukf == current results of robot localization\nTopic: /scan/sick == laser scanner for reference\nTopic: /tf\nTopic: /tf_static\n", "odom.pose.covariance =  boost::assign::list_of(1e-2) (0) (0)  (0)  (0)  (0)\n                                             (0) (1e-2)  (0)  (0)  (0)  (0)\n                                             (0)   (0)  (1e6) (0)  (0)  (0)\n                                             (0)   (0)   (0) (1e6) (0)  (0)\n                                             (0)   (0)   (0)  (0) (1e6) (0)\n                                             (0)   (0)   (0)  (0)  (0)  (1e-3) ;\n", "    <param name=\"imu0_differential\" value=\"true\"/>\n    <param name=\"odom0_differential\" value=\"true\"/>\n", "header: \n seq: 4343\n stamp: \n  secs: 1464844455\n  nsecs: 543424495\n frame_id: odom\n child_frame_id: ''\npose: \n pose: \n position: \n  x: -68.2792701104\n  y: 122.427959131\n  z: 0.0\n orientation: \n  x: 0.0\n  y: -0.0\n  z: -0.629156547298\n  w: -0.777278610919\n covariance: [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0 ..."], "answer_code": ["differential", "relative", "differential", "differential", "differential mode", "<param name=\"imu0\" value=\"imu/data\"/>\n<rosparam param=\"imu0_config\">[false, false, false,\n                               false, false, true,\n                               false, false, false,\n                               false, false, true,\n                               false,  false, false]</rosparam>\n<param name=\"imu0_differential\" value=\"false\"/>\n\n<param name=\"odom0\" value=\"/odom/fixed_wheel\"/>\n<rosparam param=\"odom0_config\">[false, false, false, \n                                false, false, false, \n                                true, true, false, \n                                false, false, true,\n                                false, false, false]</rosparam>\n\n<param name=\"odom0_relative\" value=\"false\"/>\n<param name=\"imu0_relative\" value=\"true\"/>\n", "child_frame_id"], "url": "https://answers.ros.org/question/236161/problem-when-fusing-imu-and-odometry/"},
{"title": "ROS isn't recognizing Create", "time": "2016-07-20 16:08:42 -0600", "post_content": [" ", " ", "Hey guys,", "So I'm trying to run turtlebot with an irobot create, but whenever I run the minimal.launch file I get the following error/warning:", "[ERROR] [WallTime: 1469047993.959124] Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create.", "The create is on, plugged into the USB port, and the port path is /dev/tty/USB0. Any idea what the problem is? Is there some driver I need to install? I've heard it could be the battery but I haven't received any error messages regarding the battery being low.", "Any answers are appreciated. Thanks!"], "answer": [" ", " ", "Have you followed ", " steps too?", "Basically, open the ", " file in your home directory and add", "and also add your user to the dialout group:\n", ", where ", " is your username."], "answer_code": [".bashrc", "# Export the turtlebot variables\nexport TURTLEBOT_BASE=create\nexport TURTLEBOT_STACKS=circles\nexport TURTLEBOT_3D_SENSOR=kinect\nexport TURTLEBOT_SERIAL_PORT=/dev/ttyUSB0\n", "sudo adduser username dialout", "username"], "url": "https://answers.ros.org/question/239958/ros-isnt-recognizing-create/"},
{"title": "Could XV-11 break or discharge? [closed]", "time": "2016-03-09 02:47:40 -0600", "post_content": [" ", " ", " ", " ", "Hi! I use a neato xv-11 to do a scan. Until a few days ago I had no problem, now sometimes does not work because it shows very few points and a red circle appears in the center. What do you think it might be? It is powered by USB. Thank you!"], "answer": [" ", " ", "I solved the problem increases the motor voltage that was lowered. Now it works well!", "Please accept your own answer using the checkbox at left instead of changing the title to closed.", "Ok! Thank you!"], "url": "https://answers.ros.org/question/228583/could-xv-11-break-or-discharge-closed/"},
{"title": "Got Kobuki, got errors on an Ubuntu laptop running ROS Kinetic", "time": "2016-06-22 15:14:51 -0600", "post_content": [" ", " ", " ", " ", "I just got my Kobuki a few days ago so I was happy to play with it. But then I got an error while trying to ", " the Kobuki workspace within Turtlebot. The error first was:", "After removing kobuki_keyop, and fiddling with some other stuff (including to get Sophus and put it in the Kobuki directory), I got the error", "Then, when I ran catkin_make_isolated, I didn't get an error. I got:", "but then when I run catkin_make, same error. Please help me ASAP!\n", "Edit: I would also like to run RViz with my Kinect and Kobuki (Kinect works fine with RViz and OpenNI_Launch, I believe) and would like to run minimal.launch because of that. I would really like to do SLAM and navigate! I'm just a beginner.\n", "Edit 2: Now my Kinect's not working when I plug it in to power and USB. Great. Probably a faulty wire.", " I'm sorry that I don't know how to answer your question, but if someone does they will answer you asap. Your best bet is to keep working on the issue and continuing to post updates on your progress with more information. You may want to have a look at  ", "Agree with ", ". Please follow the link he posted. I have some experience in building turtlebot codes on ROS ", "  (and follow links there for more tickets), although not sure if this helps you. "], "answer": [], "question_code": ["catkin_make", "CMake Error at /home/ayan/turtlebot/devel/share/ecl_exceptions/cmake/ecl_exceptionsConfig.cmake:141 (message):\n  Project 'kobuki_keyop' tried to find library 'ecl_exceptions'.  The library\n  is neither a target nor built/installed properly.  Did you compile project\n  'ecl_exceptions'? Did you find_package() it before the subdirectory\n  containing its code is included?\n", "CMake Error at /opt/ros/kinetic/share/catkin/cmake/catkin_workspace.cmake:95 (message):\n  This workspace contains non-catkin packages in it, and catkin cannot build\n  a non-homogeneous workspace without isolation.  Try the\n  'catkin_make_isolated' command instead.\nCall Stack (most recent call first):\n  CMakeLists.txt:63 (catkin_workspace)\n\n\n-- Configuring incomplete, errors occurred!\nSee also \"/home/ayan/kobuki_catkin/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/ayan/kobuki_catkin/build/CMakeFiles/CMakeError.log\".\nInvoking \"cmake\" failed\n", "==> Generating an env.sh\n<== Finished processing package [1 of 1]: 'sophus'\n", "Kinetic"], "url": "https://answers.ros.org/question/237810/got-kobuki-got-errors-on-an-ubuntu-laptop-running-ros-kinetic/"},
{"title": "Draw coordinate system in Rviz", "time": "2016-05-27 14:46:38 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am trying to visualize camera and laser poses in Rviz. Currently I can draw an arrow pointing in the X direction like you can see on the attached figure. Basically what I would like to do is replace the single green and yellow arrows by an axis similar to the one overlapping the yellow arrow in the figure.", "The code that draws the single arrow pointing in the X direction is the following:", "Thank you for your help,", "Dr1T", " EDIT: Upon searching a little more I came across this question:  ", "It seems to be exactly the same issue. The selected answer says that I have to create 3 arrow markers for each pose. But this would require quite a bit of changes in code that I am not familiar with.", "The second answer seems to be what I want without changing much, could someone give an example of how to implement it (adding a Pose display type and changing its shape to \"Axis\")?", "Rviz has a built in TF display, if you're using TF.", "Don't think I am using TF, since the \"Frame\" filed in the TF display does not have the frame I am using. The current code publishes a single markerarray in a single frame which contains the point clouds and poses for all sensors. Each color represents a different sensor.", "Then, I recommend you get familiar with TF and use it properly. Then, this you want to do is going to be much easier and you will have tons of advantages when dealing with your sensor data.", "Late update. TF really is the best option, but I ended up creating 3 arrow markers since changing to TF, in this specific case, would require a lot of changes and I didn't have much time to do it. Thanks for your answers."], "answer": [], "question_code": ["for(int n=0;n<lasers.size();n++)\n{\n    {\n    marker_lasers[n].header.frame_id = \"/my_frame3\";\n    marker_lasers[n].header.stamp = ros::Time::now();\n\n    std::stringstream ss;\n    ss << \"Laser \" << n;\n\n    marker_lasers[n].ns = ss.str();\n    marker_lasers[n].action = visualization_msgs::Marker::ADD;\n\n    marker_lasers[n].type = visualization_msgs::Marker::ARROW;\n\n    marker_lasers[n].scale.x = 0.2;\n    marker_lasers[n].scale.y = 0.1;\n    marker_lasers[n].scale.z = 0.1;\n\n    geometry_msgs::Pose laser = lasers[n];\n    std_msgs::ColorRGBA color;\n\n    color = colormap.color(n);\n\n    marker_lasers[n].pose.position.x=laser.position.x;\n    marker_lasers[n].pose.position.y=laser.position.y;\n    marker_lasers[n].pose.position.z=laser.position.z;\n\n    marker_lasers[n].pose.orientation.x=laser.orientation.x;\n    marker_lasers[n].pose.orientation.y=laser.orientation.y;\n    marker_lasers[n].pose.orientation.z=laser.orientation.z;\n    marker_lasers[n].pose.orientation.w=laser.orientation.w;\n\n    marker_lasers[n].color=color;\n\n    marker_list.update(marker_lasers[n]);\n    }\n}\n"], "url": "https://answers.ros.org/question/235414/draw-coordinate-system-in-rviz/"},
{"title": "Devel environment for C++?", "time": "2015-12-22 14:00:57 -0600", "post_content": [" ", " ", "do u guys just use emacs/vim + cscope/ctags ? ", "eclipse CDT would give u real-time compile/syntax check, and easier and more powerful code browsing/refactoring, would it be possible to use eclipse CDT? anybody had experience using CDT with ROS code base?"], "answer": [" ", " ", " ", " ", "Please see ", " wiki page and particularly for ", ". If you are on a modern ROS distro with ", " build system, the following will generate Eclipse-project that you can import:", "The above command will generate ", " project for ", " you have in your workspace. The project files ", " and ", " will be placed inside ", " directory of your catkin workspace.", "The following will generate the ", " and ", " files for only the current directory. Useful if only one package is necessary.", " ", " ", "Yes, it is possible to use the Eclipse CDT. I am using it myself. You can import the project like any other C++ project but you need to change the C++ build settings to use catkin_make (catkin_make --pkg <your package=\"\">) and your build directory to your catkin_ws directory."], "answer_code": ["catkin", "catkin_make --force-cmake -G\"Eclipse CDT4 - Unix Makefiles\"\n", ".project", ".cproject", "build", ".project", ".cproject", "cmake -G \"Eclipse CDT4 - Unix Makefiles\"\n"], "url": "https://answers.ros.org/question/222974/devel-environment-for-c/"},
{"title": "Is it compulsory for a mesh file(urdf) in STL format?? Can i add a solid works file as a mesh file?", "time": "2015-11-04 12:34:24 -0600", "post_content": [" ", " ", "My design which i got from another source is in stl format and contains lot of unwanted objects like holes,screws,extrusions etc..in it.So i need to delete all those unwanted parts from my mesh file.(to improve processing power) So is it ok to use a solidworks file (i am planning to design it from scratch) or modify the stl file using any stl editor?"], "answer": [" ", " ", " ", " ", "URDF only works with mesh files. You can use STL or DAE, that is the only two that are supported that I am aware of. ", "I (and probably most people) prefer DAE because you can output the color of your model from your mesh software. STL doesn't support color so you have to add that information in the URDF and even then I don't think you can have multiple colors on one object. ", "If you are making real/important changes to the part I would suggest modeling it in SolidWorks and exporting. If you are just simplifying the part by removing unimportant features, I would recommend working with the existing STL. ", "I prefer using ", " for modifying meshes. Blender is free, very extensive, cross platform, portable, and awesome - there can be a bit of a learning curve but there are tons of video tutorials available. The main feature I use for what you're talking about is the ", ". I just leave all those unimportant features and decimate the whole model - its like lowering the resolution of it. ", "You can also use Blender to paint/color your parts and export DAE so you don't have to add color information in your URDF. ", "EDIT: ", "One last thing, Solidworks exports STL, but like I said that doesn't carry color along with it. Solidworks does NOT export .dae. Just looking at Blender and Solidworks I notice that Solidworks can export .wrl and Blender can import .wrl. And .wrl does support color. So, you might/should be able to export .wrl from Solidworks, import it into Blender, then export the .dae from Blender ... that way you would have the color that you set in Solidworks. "], "url": "https://answers.ros.org/question/220445/is-it-compulsory-for-a-mesh-fileurdf-in-stl-format-can-i-add-a-solid-works-file-as-a-mesh-file/"},
{"title": "Rviz visualization of cube trajectories from green to red", "time": "2015-11-19 15:12:42 -0600", "post_content": [" ", " ", "#!/usr/bin/env python", "import roslib; roslib.load_manifest('visualization_marker_tutorials')\nfrom visualization_msgs.msg import Marker\nfrom visualization_msgs.msg import MarkerArray\nimport rospy\nimport math", "topic = 'visualization_marker_array'\npublisher = rospy.Publisher(topic, MarkerArray, queue_size=10)", "rospy.init_node('register')", "markerArray = MarkerArray()", "count = 0\nMARKERS_MAX = 100", "while not rospy.is_shutdown():", "# We add the new marker to the MarkerArray, removing the oldest marker from it when necessary\n   for x in range(0,10):\n      marker = Marker()\n      marker.header.frame_id = \"/map\"\n      marker.type = marker.CUBE # set our shape to be a cube\n      marker.action = marker.ADD\n      marker.scale.x = 1.0\n      marker.scale.y = 1.0\n      marker.scale.z = 1.0\n      marker.color.a = 1.0\n      marker.color.r = (50 + (x*2)) / float (255)\n      marker.color.g = 170 / float (255)\n      marker.color.b = 25 / float (255)\n      marker.pose.orientation.w = 1.0\n      marker.pose.position.x = 0\n      marker.pose.position.y = 0\n      marker.pose.position.z = 0", "So I am trying to create a trajectory from position in x  going from 0 to 1 with color changing from green to red.", "So far this is the code that I have and testing. It is publishing but does not seem to change color and only one box appears. ", "Any suggestions?"], "answer": [], "question_code": ["  #Remove marker when necessary\n  #Adding new marker each time in the Marker Array\n\n  if(count > MARKERS_MAX):\n     markerArray.markers.pop(0)\n\n  markerArray.markers.append(marker)\n\n  # Renumber the marker IDs\n\n  id = 0\n  for i in markerArray.markers:\n     i.id = id\n     id += 1\n\n  # Publish the MarkerArray\n  publisher.publish(markerArray)\n\n  count += 1\n\n  rospy.sleep(0.01)\n"], "url": "https://answers.ros.org/question/221248/rviz-visualization-of-cube-trajectories-from-green-to-red/"},
{"title": "motors different speeds", "time": "2015-04-02 05:49:31 -0600", "post_content": [" ", " ", " ", " ", "hello,\nI'm implementing SLAM using ROS.\nand i'm using my SuperDroid platform ( a square robot with 4 wheels).\nand i'm controlling it using Roboteq SDC21xx.\nthis controller, controls each two motors together. So, the two left motors are treated as one. The same for the right ones. and i'm doing this because my motor controller have only two channels and i'm forced to combine the motors.", "Anyhow,\nNow, when I want to implement Odometer, i'll have to bring the left and right encoder. but ", " is that each motor brings up different traveled distance (or number of revolutions) because each motor has a different speed.", "to overcome the problem of the different speeds of motors, people suggested that i give different power to each motor. (in my case, i give each two motors the same power).", "this solution still doesn't provide exact solutions (which in odometer it is very important to get PRECISE results).", "any suggestions? ideas? help?\nhow about getting another controller?"], "answer": [" ", " ", "Two things to note:", "1- Your robot is a skid-steer system (as you described). Any turning or zero radius turn will show wheel slippage (already your pure wheel odometry wont be 100% perfect). Its hard to get wheel odometry to naturally not have error, but a differential diff system would get you the closer to that point.", "2- It also depends on what type of encoders your have; how much resolution does it have? Do they have hardware noise filters? ", "Bottom line is, having one feedback source (like wheel encoder data) for the location of your robot is not ideal; I would recommend that you use a kalman filter like for example the ROS package robot_localization and let it fuse your wheel odometry data, imu data, gps data, etc.. ", "One thing more thing to keep in mind, I posted a question similar to this topic a while back (you can view it ", ") about assigning covariance values to your wheel odometry. In the post you will see in one of the answers on how you model how much error you are expecting with every increment of data. Your raw wheel odometry will need to be sending the covariances values in your setup for robot_localization. ", "If you set that up, you'll have a filtered odometry estimate ALOT closer then you would have with just pure wheel odometry, which you can then use as a input into SLAM (but from my own experience, i dropped slam after seeing how well of a odometry estimate robot_localization gave me)", "Hope this helps a bit"], "url": "https://answers.ros.org/question/206499/motors-different-speeds/"},
{"title": "Navigation Stack Computation from External Computer", "time": "2015-04-03 02:43:11 -0600", "post_content": [" ", " ", "Hello everyone!", "I am working on a ROS Indigo project where we are going to be reading all of our sensor information from an Ubuntu Trusty install on a BeagleBone Black Rev C and sending that information over the appropriate topics (odometry, laser scan topics, etc) to an external computer over the network (also Ubuntu Trusty running ROS Indigo), which will then send back the appropriate twist messages to the base controller node that exists within the workspace on the BeagleBone Black. So, the only nodes that should be running on the BeagleBone are the IMU Node, the SICK LMS wrapper node, and the tf_configuration nodes (We still haven't figured out how to get those set up exactly, so insight on that is also appreciated).", " I had been following the tutorial here:  ", "My only concern is that the only way for me to get my_robot_name_2dnav package to compile correctly with the 'move_base' dependency that is apparently required was for me to install 'ros-indigo-navigation' on the BeagleBone. My concern with this is that we don't want the BeagleBone to be computing any of this information as it is at almost 80% CPU usage just to read and send the LaserScan message from the LIDAR.", "Am I doing this correctly? Or am I missing something to connect the navigation stack to the external computer that will be able to handle all of the processing necessary?", "Thanks for all input", "Do you not want move_base installed on the beaglebone due to storage limitations on beaglebone? or do you just not want move_base running on the beaglebone?", "I just do not want move_base running on the beaglebone. It's going to require too much processing power, when we have an external computer to do that.", "so if you dont care about it being installed, then you can still install move_base on the beaglebone, but just dont run the move_base node on it. instead run it on your external computer. Did you properly setup your ", " ?", "So, what you're saying is that it won't matter that it's on there or not. So, I can technically take that dependency off? Or would that screw something up? And from there I should just run 'roslaunch move_base move_base_0.3_to_0.2.launch' from the external computer?", "Yes, we got that working last week! It's amazing how fast it sends LIDAR information over the network. It's only ~1 sec delay, which is way better than we expected.", "Ah, I just found where it shows to create the move_base.launch file. I think it's finally starting to make a little sense!"], "answer": [" ", " ", " ", " ", "Continuing off the comments, I dont know how your system/packaging is setup, so its hard to say if you want to take the dependency off or not. For example, if you have say 1 github repo that you clone to both the beaglebone and the external computer so that they both have the same files, then you probably wouldnt want to remove move_base as a dependency since it will also affect your external computer.", "Ideally, I think you may want to setup your system in a way similar to how Clearpath organizes there ", " for their jackal robot . You would basically divide the different parts related to the robot into separate repos. For example you could have say two repo's:", "That way you could only clone the ", " repo to your beaglebone (and not even have to install/have move_base on the beaglebone) and clone the ", " repo to your external computer (where you would have move_base listed as a dependency just like they tell you to in the setting up the navigation stack tutorials)", "If you setup your network correctly, then just run the sensors and robot driving related stuff on your beaglebone, and run move_base on your external computer. You can also look at the ", " to launch nodes on different machines, from a single computer (all on same network)", "\nMy understanding of it (may be wrong) is that \"jackal\" is a common package used amongst all the different types of jackal setups (simulation or real world). Things like the URDF of the robot, and custom messages the robot sends. Jackal_robot is specific to the physical robot itself (boot up/onstart procedures). If you spend some time looking through clearpath's repo's you will learn ALOT. It just takes time and curiosity :) ", "Brilliant. Absolutely brilliant. Will update if I run into problems. Thank you so much!", "Could you help distinguish the difference the \"jackal_robot\" packages and the \"jackal\" packages?", " The jackal_robot package/repo contains things that are completely specific to the real world jackal robot (i.e. scripts for network setup, robot_upstart, etc..). The jackal package contains common info that can be used across any testing platform (simulation, or real world)"], "answer_details": [" (this repo could contain any driver/sensor related items)", " (this repo would have your move_base related items)", " ", " ", " ", " "], "answer_code": ["robot_control", "robot_navigation", "robot_control", "robot_navigation"], "url": "https://answers.ros.org/question/206556/navigation-stack-computation-from-external-computer/"},
{"title": "Problem in determining a link language while using external libraries", "time": "2015-09-22 07:38:21 -0600", "post_content": [" ", " ", "Hi I am a PhD student and quite new to ROS trying to make a ROS package to use my Robot. The company who built the robot has also provided us some C++ libraries and codes, i.e. libRover.h and libRover.cc. Inside these codes there are some functions from different classes that help us to read status of the robot and also send commands to it. For example:", "Now what I am trying to do is to create a package that executes some of this functions, for example to read the voltage of the battery, and publish the data as ROS messages. I put the *.h and the *.cc files in my_package/include. I also set up my CMakeLists.txt as follows:", "Here is my catkin_make log:", "The problem arises from the part of CMakeList.txt where I say", "I figured it out by removing the elements of this argument and observing the error on the next one. In other words, when I remove \"Rover\" from the add_library arguments I get a similar error from the next one, like:", "Can anybody tell me what I am missing?", "Thank you very much "], "answer": [" ", " ", "Finally I managed to solve the problem. After all it was not that complicated. So basically the first step is to include the folder which contains your  source files (", ".cc [or maybe .cpp]) to the directories. In my case it's LibRover", "Then making a library out of source files, in my case I call it Rover:", "of course adding the executable:", "and adding your executable as a dependency:", "linking the executable to the library that you just created above:", "and finally installing the header files:", "more information can be found in this link: "], "question_code": ["int Rover::getSpeed ( Time & _timestamp,\nfloat & _left,\nfloat & _right \n) \n\nGet the speed of the tracks [in rad/s].\n\nParameters\n[out] _timestamp the current timestamp [in s].\n[out] _left the average speed of the front and rear left tracks [in rad/s].\n[out] _right the average speed of the front and rear right tracks [in rad/s].\nReturns\n0 if success, an error code otherwise.\n", "cmake_minimum_required(VERSION 2.8.3)\nproject(donkey_rover)\n\n## Find catkin macros and libraries\n## if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)\n## is used, also find other catkin packages\nfind_package(catkin REQUIRED COMPONENTS\n  roscpp\n  rospy\n  std_msgs\n)\n\n\n\n\ncatkin_package(\n  INCLUDE_DIRS include LIBRARIES donkey_rover \n  CATKIN_DEPENDS roscpp rospy std_msgs\n  DEPENDS system_lib\n)\n\n\ninclude_directories(\n  include\n  ${catkin_INCLUDE_DIRS}\n)\n\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -Wall -I../include\")\nset(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -L../include\")\n\nadd_library(Rover STATIC BogieScanner.cc CANopenHelper.cc CANopenMaster.cc Drive.cc Track.cc libRover.cc)\n## Declare a C++ executable\nadd_executable(rovstate src/rovstate.cpp)\n\n## Add cmake target dependencies of the executable\n## same as for the library above\n# add_dependencies(donkey_rover_node ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})\nadd_dependencies(rovstate donkey_rover_generate_messages_cpp)\nadd_dependencies(rovstate ${catkin_EXPORTED_TARGETS})\n\n## Specify libraries to link a library or executable target against\ntarget_link_libraries(rovstate libRover.a pthread\n   ${catkin_LIBRARIES}\n )\n\nadd_subdirectory(include)\n\ninstall(DIRECTORY include/${PROJECT_NAME}/\n   DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}\n   FILES_MATCHING PATTERN \"*.h\"\n   PATTERN \".svn\" EXCLUDE\n )\n", "CMake Error: Cannot determine link language for target \"Rover\".\nCMake Error: CMake can not determine linker language for target: Rover\n-- Generating done\n-- Build files have been written to: /home/sherpa/catkin_ws/build\nmake: *** [cmake_check_build_system] Error 1\nInvoking \"make cmake_check_build_system\" failed\n", "add_library(Rover STATIC BogieScanner.cc CANopenHelper.cc CANopenMaster.cc Drive.cc Track.cc libRover.cc)\n", "CMake Error: Cannot determine link language for target \"STATIC\".\nCMake Error: CMake can not determine linker language for target: STATIC\n-- Generating done\n-- Build files have been written to: /home/sherpa/catkin_ws/build\nmake: *** [cmake_check_build_system] Error 1\nInvoking \"make cmake_check_build_system\" failed\n"], "answer_code": ["include_directories(\n  include\n  LibRover\n  ${catkin_INCLUDE_DIRS}\n  ${Boost_INCLUDE_DIRS}\n  ${GSTREAMER_INCLUDE_DIRS}\n)\n", "add_library(Rover LibRover/BogieScanner.cc LibRover/CANopenHelper.cc LibRover/CANopenMaster.cc LibRover/Drive.cc LibRover/Track.cc LibRover/libRover.cc)\n", "add_executable(rovstate src/rovstate.cpp)\n", "add_dependencies(rovstate  your_package_generate_messages_cpp ${catkin_EXPORTED_TARGETS})\n", "target_link_libraries(rovstate Rover\n  ${catkin_LIBRARIES}\n  ${Boost_INCLUDE_DIRS}\n  ${GSTREAMER_INCLUDE_DIRS}\n )\n", "install(TARGETS Rover\n        ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}\n        LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}\n        RUNTIME DESTINATION ${CATKIN_GLOBAL_BIN_DESTINATION})\n\ninstall(DIRECTORY include/${PROJECT_NAME}/\n   DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}\n   FILES_MATCHING PATTERN \"*.h\"\n   PATTERN \".svn\" EXCLUDE\n )\n"], "url": "https://answers.ros.org/question/218015/problem-in-determining-a-link-language-while-using-external-libraries/"},
{"title": "Interfacing arduino over ethernet", "time": "2015-06-19 19:05:46 -0600", "post_content": [" ", " ", "I'm using a teensy 3.1 that is more powerful than a normal arduino and i've got a wiz550io attached.", "I know the rosserial library to interface with arduino using USB", "Is there a way to make arduino talk with ros over ethernet.? Anyone got an example ?"], "answer": [" ", " ", "Do you have allready a working arduino Library? for UDP or TCP communication? if so, you can wirte a subsciber and a publicher which packs your information in ros topics"], "url": "https://answers.ros.org/question/211764/interfacing-arduino-over-ethernet/"},
{"title": "battery_state_of_charge publishes nothing on Pioneer3DX", "time": "2014-08-26 04:42:47 -0600", "post_content": [" ", " ", "Hi,", "in the rosaria documentation is written that, if the pioneer supports battery_state_of_charge the topic is published. The topic is published, but there is never any data comming. Does anyone know if the Pioneer3DX supports battery_state_of_charge ? If it supports it, what do i have to do to get the values ? ", "regards peter "], "answer": [" ", " ", "i do not use the Pioneer3DX \nif have the useful points ,tell me ."], "url": "https://answers.ros.org/question/191251/battery_state_of_charge-publishes-nothing-on-pioneer3dx/"},
{"title": "Why RPLidar + gmapping has a bad result?", "time": "2015-03-29 08:20:51 -0600", "post_content": [" ", " ", " ", " ", "Now I have a turtlebot 2, and I want to build a map. As gmapping needs odometry, so I think it would have a better result than hector_mapping. As RPLidar has 360 degree while kinect has only 57 degree, so while gmapping, I think RPLidar would has a better result. ", "However, to my disappointment, the map builded by RPLidar is too bad! ", "I found that the map builded by rplidar + hector_mapping is really good.", "But I really want to know why RPLidar + gmapping could not have a good result, does anyone know this? ", "Thank you !!", " ", "I have the same problem. I'm working to some tests about this, maybe can be useful for you too:", "Thank you very much!!", "Wow!!!!   You are so great!!!  Thank you, thank you so much!!!", "Glad to have been helpful!", "Thank you for taking the time to give me a hand. Now I have uploaded the RVIZ picture, in the picture,the red or green lines or points are the laser scan data. I increased the size of the laser size.", "Now compared with yours, I think the only difference is the tf transform, let me try again. thanks!", "I am sorry to have kept you waiting so long. I am a newer to ROS, and I have just learned how to use the rosbag. I upload the bagfile to my network disk. ", "\nI am sorry that I used a Chinese skydrive. To download the bagfile, click the ", " .", "Hey Tang, Could you please teach with using rplidar and hector together\uff1f", "I am sorry that I see your question until now, and have you solved your question?"], "answer": [" ", " ", " ", " ", "You will be happy to know that I fixed my problem with gmapping and LPLidar. You can find a full documentation about the fix ", " (including source code modification of course): I hope can be useful for you too!", "GMapping use two parameters to define laser beam range:", "(Description taken by ros gmapping wiki)", "The long lines you see on your map are created when the laser measured range is between maxUrange and maxRange. You can try to set maxUrange = maxRange. By the way I think the real problem is another: why your laser measures this range if there is a wall?", "Can you take a picture of RViz where laser scan data is shown too? I suggest to set decay to 120 secs so we can see more data. I want to check if long lines are really generated by laser measures of if they are artefacts of GMapping.", "Can you register and upload a bag file of your test? Remember to move the robot in your environment recording data in order to have an idea of the expected map. You should record tf and your laser scan topic.", "Further question: on the posted RViz screenshot I can see an entry referring AMCL, are you running it too?", "I run some test on your bag data. This the result of odometry test:", "Since you are working in large environment (I measured abut 5 meters square room) you can be happy about it! Any way I suggest you to follow this ", " (if you already didn't): it will help you not only to improve your robot performances but you can also learn something about navigation stack in ROS.", "BTW I ran GMapping with the parameters I suggested and, actually, I reproduced the same problem you claim about (sorry, my environment is not so large so I never experienced it!). Fortunately the solution is easy. Set following parameters:", "I got this result:", "Maybe it is not perfect but:", "I am sorry for my disturbing you again, I used all the GMapping parameters just like yours. However, the map has many long lines that over the border. Did you have encountered this problem? I have uploaded the map picture in the question above.", "Thank you!", "I\u2019m sorry to have kept you waiting so long. Now my map is much better! All of this is under your help, Thank you so much\uff01\uff01", "Glad to have been helpful!"], "answer_details": ["maxRange: the maximum range of the sensor. If regions with no obstacles within the range of the sensor should appear as free space in the map, set maxUrange < maximum range of the real sensor <= maxRange;", "maxUrange: the maximum usable range of the laser. A beam is cropped to this value.", "on my simulation map -> odom transformation came from your bag file instead my GMapping node. When you will run it on your robot result should be better (please post a picture... I'm curios!);", "RPLidar has a great value/price rate but it have short range laser (look ", " to appreciate more RPLidar value/cost rate!!). This will cause no optimal performance working in large environment. Any way a trick exists in order to have better result: move you robot close to the wall instead cross the centre of your room and you will se that built map ...", " ", " ", " ", " "], "answer_code": ["odom_frame: odom \ndelta: 0.025\nminimumScore: 50\nmaxRange: 5.5\nmaxUrange: 5.5\nlinearUpdate: 0.2\nangularUpdate: 0.25\ntemporalUpdate: 5\n"], "url": "https://answers.ros.org/question/206185/why-rplidar-gmapping-has-a-bad-result/"},
{"title": "rosmsg   wont show message??", "time": "2015-01-30 11:39:50 -0600", "post_content": [" ", " ", "I at moment trying to learn about ROS using the guide provided by this page.  Up until now it has been running fine, but for some reason i am not getting the same output as i was supposed to here ", "The output i receive is this ", "Unable to load msg\n  [beginner_tutorials/Num]: Cannot\n  locate message [Num]: unknown package\n  [beginner_tutorials] on search path\n  [{'rosconsole':\n  ['/opt/ros/indigo/share/rosconsole/msg'],\n  'catkin':\n  ['/opt/ros/indigo/share/catkin/msg'],\n  'angles':\n  ['/opt/ros/indigo/share/angles/msg'],\n  'image_view':\n  ['/opt/ros/indigo/share/image_view/msg'],\n  'carrot_planner':\n  ['/opt/ros/indigo/share/carrot_planner/msg'],\n  'urdf':\n  ['/opt/ros/indigo/share/urdf/msg'],\n  'rosgraph':\n  ['/opt/ros/indigo/share/rosgraph/msg'],\n  'rqt_py_console':\n  ['/opt/ros/indigo/share/rqt_py_console/msg'],\n  'nodelet_topic_tools':\n  ['/opt/ros/indigo/share/nodelet_topic_tools/msg'],\n  'rqt_graph':\n  ['/opt/ros/indigo/share/rqt_graph/msg'],\n  'rotate_recovery':\n  ['/opt/ros/indigo/share/rotate_recovery/msg'],\n  'theora_image_transport':\n  ['/opt/ros/indigo/share/theora_image_transport/msg'],\n  'rocon_python_comms':\n  ['/opt/ros/indigo/share/rocon_python_comms/msg'],\n  'qt_gui':\n  ['/opt/ros/indigo/share/qt_gui/msg'],\n  'filters':\n  ['/opt/ros/indigo/share/filters/msg'],\n  'ecl_exceptions':\n  ['/opt/ros/indigo/share/ecl_exceptions/msg'],\n  'navfn':\n  ['/opt/ros/indigo/share/navfn/msg'],\n  'dwa_local_planner':\n  ['/opt/ros/indigo/share/dwa_local_planner/msg'],\n  'rosgraph_msgs':\n  ['/opt/ros/indigo/share/rosgraph_msgs/msg'],\n  'smclib':\n  ['/opt/ros/indigo/share/smclib/msg'],\n  'ecl_devices':\n  ['/opt/ros/indigo/share/ecl_devices/msg'],\n  'rocon_gateway_utils':\n  ['/opt/ros/indigo/share/rocon_gateway_utils/msg'],\n  'roscpp_serialization':\n  ['/opt/ros/indigo/share/roscpp_serialization/msg'],\n  'rqt_rviz':\n  ['/opt/ros/indigo/share/rqt_rviz/msg'],\n  'rosbuild':\n  ['/opt/ros/indigo/share/rosbuild/msg'],\n  'qt_gui_cpp':\n  ['/opt/ros/indigo/share/qt_gui_cpp/msg'],\n  'rosauth':\n  ['/opt/ros/indigo/share/rosauth/msg'],\n  'tf':\n  ['/opt/ros/indigo/share/tf/msg'],\n  'rqt_publisher':\n  ['/opt/ros/indigo/share/rqt_publisher/msg'],\n  'roslang':\n  ['/opt/ros/indigo/share/roslang/msg'],\n  'geometric_shapes':\n  ['/opt/ros/indigo/share/geometric_shapes/msg'],\n  'kobuki_driver':\n  ['/opt/ros/indigo/share/kobuki_driver/msg'],\n  'smach_ros':\n  ['/opt/ros/indigo/share/smach_ros/msg'],\n  'genlisp':\n  ['/opt/ros/indigo/share/genlisp/msg'],\n  'map_server':\n  ['/opt/ros/indigo/share/map_server/msg'],\n  'interactive_markers':\n  ['/opt/ros/indigo/share/interactive_markers/msg'],\n  'dynamic_reconfigure':\n  ['/opt/ros/indigo/share/dynamic_reconfigure/msg'],\n  'ecl_containers':\n  ['/opt/ros/indigo/share/ecl_containers/msg'],\n  'diagnostic_aggregator':\n  ['/opt/ros/indigo/share/diagnostic_aggregator/msg'],\n  'ecl_license':\n  ['/opt/ros/indigo/share/ecl_license/msg'],\n  'smart_battery_msgs':\n  ['/opt/ros/indigo/share/smart_battery_msgs/msg'],\n  'robot_state_publisher':\n  ['/opt/ros/indigo/share/robot_state_publisher/msg'],\n  'visualization_msgs':\n  ['/opt/ros/indigo/share/visualization_msgs/msg'],\n  'diagnostic_updater':\n  ['/opt/ros/indigo/share/diagnostic_updater/msg'],\n  'ecl_streams':\n  ['/opt/ros/indigo/share/ecl_streams/msg'],\n  'resource_retriever':\n  ['/opt/ros/indigo/share/resource_retriever/msg'],\n  'rqt_topic':\n  ['/opt/ros/indigo/share/rqt_topic/msg'],\n  'smach':\n  ['/opt/ros/indigo/share/smach/msg'],\n  'ecl_mpl':\n  ['/opt/ros/indigo/share/ecl_mpl/msg'],\n  'stdr_parser':\n  ['/opt/ros/indigo/share/stdr_parser/msg'],\n  'ecl_math':\n  ['/opt/ros/indigo/share/ecl_math/msg'],\n  'rqt_action':\n  ['/opt/ros/indigo/share/rqt_action/msg'],\n  'rqt_top':\n  ['/opt/ros/indigo/share/rqt_top/msg'],\n  'rosbridge_server':\n  ['/opt/ros/indigo/share/rosbridge_server/msg'],\n  'diagnostic_msgs':\n  ['/opt/ros/indigo/share/diagnostic_msgs/msg'],\n  'move_base':\n  ['/opt/ros/indigo/share/move_base/msg'],\n  'rosboost_cfg':\n  ['/opt/ros/indigo/share/rosboost_cfg/msg'],\n  'genmsg':\n  ['/opt/ros/indigo/share/genmsg/msg'],\n  'xacro':\n  ['/opt/ros/indigo/share/xacro/msg'],\n  'turtle_tf2':\n  ['/opt/ros/indigo/share/turtle_tf2/msg'],\n  'rqt_robot_dashboard':\n  ['/opt/ros/indigo/share/rqt_robot_dashboard/msg'],\n  'pluginlib':\n  ['/opt/ros ..."], "answer": [" ", " ", "It looks like the ROS tools can't find your beginner_tutorials package.", "It will not ", " the package/directory eventhoug it inside of it. \ni tried ", " it just say that such file doesn't exist.", "Is your package in a catkin workspace? The devel folder should be in the top level of your workspace, and the setup.bash should be generated when you run catkin_make.", "workspace is made by using catkin_init_workspace, and catkin_make.  I have been getting the right output from the previous sections of the tutorials... But this  section doesn't seem to be correct..", " i don't know if it help but here is my workspace.. \nMy workspace is given here..  ", "You have two copies of the beginner_tutorials package in your workspace; one at the top level (wrong) and one inside the src folder (right). ROS cannot find any packages that exist outside of the src folder in your workspace.", "exactly...  Problem fixed."], "answer_details": ["You should make sure that you've sourced the devel/setup.bash in your workspace", "You should make sure that your package is named beginner_tutorials", "You can check that ROS can find your package with ", " ", " ", " ", " "], "answer_code": ["rospack find beginner_tutorials", "rospack find", "source devel/setup.bash"], "url": "https://answers.ros.org/question/202129/rosmsg-wont-show-message/"},
{"title": "rapp manager errors turtlebot minimal.launch", "time": "2015-03-10 11:21:21 -0600", "post_content": [" ", " ", " ", " ", "Hi,", " I just installed ubuntu 14.04.2 (trusty) on my netbook, and followed the directions here ( ", " ) to install ROS indigo. Then I installed the TurtleBot packages here ( ", " ). I test the setup by running roscore on the master machine (different physical machine, but same setup as described), and then minimal.launch on the netbook. It seems to be working for about 15 seconds, but then it crashes when the rapp manager times out.  ", "Does anybody know what might be going on? At first I thought it was a problem with communication between machines, but they can ping each other fine and all environment variables seem to be set:", "asdf@asdf:~$ printenv | grep ROS\nROS_ROOT=/opt/ros/indigo/share/ros\nROS_PACKAGE_PATH=/opt/ros/indigo/share:/opt/ros/indigo/stacks:/home/asdf/thesis/ROS/helloworld/turtlebot_driver_test/:/home/asdf/thesis/LoadManager/catkin_ws/\nROS_MASTER_URI=http://squirrel:11311\nROS_HOSTNAME=asdf\nROSLISP_PACKAGE_DIRECTORIES=\nROS_DISTRO=indigo\nROS_IP=10.8.190.94\nROS_ETC_DIR=/opt/ros/indigo/etc/ros", "Here's the output of minimal.launch:", "asdf@asdf:~slaunch turtlebot_bringup minimal.launch \n... logging to /home/asdf/.ros/log/dfa94692-c740-11e4-8dbb-001c230e5560/roslaunch-asdf-2863.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", " started roslaunch server  ", "PARAMETERS\n * /app_manager/auto_rapp_installation: False\n * /app_manager/auto_start_rapp: \n * /app_manager/capability_server_name: capability_server\n * /app_manager/local_remote_controllers_only: False\n * /app_manager/rapp_package_blacklist: []\n * /app_manager/rapp_package_whitelist: ['rocon_apps', 't...\n * /app_manager/remote_controller_blacklist: []\n * /app_manager/remote_controller_whitelist: []\n * /app_manager/robot_icon: turtlebot_bringup...\n * /app_manager/robot_name: turtlebot\n * /app_manager/robot_type: turtlebot\n * /app_manager/screen: True\n * /app_manager/simulation: False\n * /bumper2pointcloud/pointcloud_radius: 0.24\n * /capability_server/blacklist: ['std_capabilitie...\n * /capability_server/defaults/kobuki_capabilities/KobukiBringup: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiBumper: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiCliffDetection: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED1: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED2: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiLED: kobuki_capabiliti...\n * /capability_server/defaults/kobuki_capabilities/KobukiWheelDropDetection: kobuki_capabiliti...\n * /capability_server/defaults/std_capabilities/Diagnostics: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/DifferentialMobileBase: kobuki_capabiliti...\n * /capability_server/defaults/std_capabilities/LaserSensor: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/RGBDSensor: turtlebot_capabil...\n * /capability_server/defaults/std_capabilities/RobotStatePublisher: turtlebot_capabil...\n * /capability_server/defaults/turtlebot_capabilities/TurtleBotBringup: turtlebot_capabil...\n * /capability_server/nodelet_manager_name: capability_server...\n * /capability_server/package_whitelist: ['kobuki_capabili...\n * /cmd_vel_mux/yaml_cfg_file: /opt/ros/indigo/s...\n * /description: Kick-ass ROS turtle\n * /diagnostic_aggregator/analyzers/input_ports/contains: ['Digital Input',...\n * /diagnostic_aggregator/analyzers/input_ports/path: Input Ports\n * /diagnostic_aggregator/analyzers/input_ports/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/input_ports/timeout: 5.0\n * /diagnostic_aggregator/analyzers/input_ports/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/kobuki/contains: ['Watchdog', 'Mot...\n * /diagnostic_aggregator/analyzers/kobuki/path: Kobuki\n * /diagnostic_aggregator/analyzers/kobuki/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/kobuki/timeout: 5.0\n * /diagnostic_aggregator/analyzers/kobuki/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/power/contains: ['Battery', 'Lapt...\n * /diagnostic_aggregator/analyzers/power/path: Power System\n * /diagnostic_aggregator/analyzers/power/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/power/timeout: 5.0\n * /diagnostic_aggregator/analyzers/power/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/sensors/contains: ['Cliff Sensor', ...\n * /diagnostic_aggregator/analyzers/sensors/path: Sensors\n * /diagnostic_aggregator/analyzers/sensors/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/sensors/timeout: 5.0\n * /diagnostic_aggregator/analyzers/sensors/type: diagnostic_aggreg...\n * /diagnostic_aggregator/base_path: \n * /diagnostic_aggregator/pub_rate: 1.0\n * /icon: turtlebot_bringup...\n * /interactions/interactions: ['turtlebot_bring...\n * /interactions/pairing: True\n * /interactions/rosbridge_address: localhost\n * /interactions/rosbridge_port: 9090\n * /mobile_base/base_frame: base_footprint\n * /mobile_base/battery_capacity: 16.5\n * /mobile_base/battery_dangerous: 13.2\n * /mobile_base/battery_low: 14.0\n * /mobile_base/cmd_vel_timeout: 0.6\n * /mobile_base/device_port: /dev ...", "Odd. Now I installed rocon on the master and get this error:", "/opt/ros/indigo/lib/python2.7/dist-packages/bondpy/bondpy.py:114: SyntaxWarning: The publisher should be created with an explicit keyword argument 'queue_size'.", "Hmm. Ran it again and got this error. I don't understand what could be causing such different errors.", " bondpy.py warning is created by  ", " . I have confirmed that it does not cause any problem. "], "answer": [" ", " ", "Oops. My network settings were not correct. It's all fixed.", "Thanks for the follow up. With my +1 now you can accept your answer (with the checkmark at left) so others know it's solved.", "It says I need >25 points to accept my own answer...", "Sorry I read the wrong field. I lowered it to 10. You should be good now.", "Stupid question time: which network settings?"], "url": "https://answers.ros.org/question/204666/rapp-manager-errors-turtlebot-minimallaunch/"},
{"title": "OpenCV4Tegra and ROS - HowTo? (Jetson TK1)", "time": "2015-02-07 05:26:17 -0600", "post_content": [" ", " ", "Hi all,", "I'm trying to \"force\" ROS to see OpenCV4Tegra as the official OpenCV installed on my Jetson TK1 so that cv_bridge does not install libopencv-dev from Ubuntu repo.", " I tried to follow the guide posted here\n ", "but as you can see I faced a problem that I could not solve until now.", "Does anyone succeeded on it?", "OpenCV4Tegra has GPU support and it is optimized for Tegra TK1 SoC, so I want to use its power for my algorithms ;)", "which L4T version are you on?", " I'm using the latest 21.2.2 hacked with TheGrinch kernel", "I forgot where I read this, but that dummy package works as is for 19.3, and that you have to slightly modify the dummy package for it to work with 21.2 opencv4tegra.", "Not next to my board right now, but i will post instruction later on today if i figure it out", "I tried every kind of modifications, but I cannot figure out what I was missing, I'm really curious.", " As long as I get the solution I will post it on my guide to configure Jetson TK1 for a robotics application:\n ", "Naturally I will be really happy to post a link to \"you\" (website, blog, ecc) thanking for the help"], "answer": [" ", " ", "Please refer to the solution here.", " ", " ", " ", " ", "Okay so i successfully installed cv-bridge, and can compile some sample code from the cv bridge tutorials, and run the node no problem. There still need to be some modifications, but i made a modified .deb for ros-indigo-cv-bridge and changed around a few things:", "First off, the default armhf deb for ros-indigo-cv-bridge sets different lib paths for setting the libraries in the cmake. For example, in the cv_bridgeConfig.cmake inside the .deb:", "needs to instead be", "I took out the ", " out of the path because opencv4tegra libraries are installed in /usr/lib.", "To get the cv-bridge debian so i could edit it, i did the following commands:", "All ", "does is extracts the .deb WITH the DEBIAN folder so that you can edit the DEBIAN/control file.\nIn the control file, i deleted a few things: ", ", ", ", ", ", ", ",  since these were all already installed by opencv4tegra debian. ", "Once i edited all those things, i think built the package like so: ", "and then finally just use ", " to install it.", "I dont think i missed any steps, but attached is ", ". Just use dpkg -i to install it (after CUDA and opencv4tegra have been installed)", "It would be nice if the ros arm buildfarm actually had a cv-bridge debian for the jetson.. maybe?", " I wanted to clarify when i edit the DEBIAN/control file; when i say i \"removed\" ", ", ", ", ", ", ", ", all the re-movement does is remove what dependencies the package installer should check for; ", "EX) if there is a dependency listed in the control file, and the package manager sees that it is not installed on the system it will try to install that dependency (separately, like as its own .deb file). So since we know that ", ", ", ", ", ", ", " is already installed from the opencv4tegra .deb , we can remove them from the 'Depends:' line in DEBIAN/control", " Steps to take if you just download the .deb i made: ", " ", " The modified cv-bridge.deb i made is only a quick fix/hack. I am currently working on making a permanent fix by modifying just the oepncv4tegra.deb, so that you wont have to use the quick fix cv-bridge hack and can update cv-bridge whenever with apt-get upgrade. Should have this done within the next day or two. For now i have rearranged things around so opencv4tegra libs actually in fact ...", "Great work... I'm going to test your deb file. Only a doubt: cv_bridge depends only on libopencv-dev, libopencv-python, libopencv-core2.4 and libopencv-imgproc2.4? Nothing from the otther OpenCV modules?", "i dont believe i saw anything about opencv modules listed in the DEBIAN/control file, there are a few other packages the cv bridge relies on but only anything opencv related, i took out (i will try to correct it later rather then remove the listed dependencies)", " you modified the deb file to remove the dependencies on \"standard OpenCV\"... let's make the next step: let's add dependencies on opencv4tegra-repo_l4t, opencv4tegra and opencv4tegra-dev.\nIn this way we are sure to install it correctly. Don't you think?", " I followed your guide and create my own deb since you need to change the versione of the library to prevent apt-get to update it with ros-indigo-cv-bridge from repository.\nSo for cv-bridge we are ok... but there are a lot of other packages under ROS that relies on OpenCV...", "... so we must create a fake \"deb\" for each one of them :-|\nTry for example \nsudo apt-get install ros-indigo-rgbd-launch ros-indigo-openni2-camera ros-indigo-openni2-launch\nwe must to find a way to tell to apt-get that OpenCV is installed.", "The list of packages to be modified is the following:\nActually the list of packages to be hacked is the following:\nros-indigo-cv-bridge\nros-indigo-depth-image-proc\nros-indigo-image-geometry\nros-indigo-image-proc\nros-indigo-openni2-launch\nros-indigo-rgbd-launch", "ros-indigo-rqt-common-plugins \nros-indigo-rqt-image-view ", "\nros-indigo-rqt-robot-plugins \nros-indigo-rqt-rviz \nros-indigo-rviz", "yup, i was well aware of this. The fix i put up is only a quick hack that did the job in order to just use opencv. It will most definetely break if you update. Im working on a more permanent fix now, where only thing that will have to be modified is the opencv4tegra.deb, not the cv-bridge deb"], "answer_code": ["set(libraries \"cv_bridge;/usr/lib/arm-linux-gnueabihf/libopencv_videostab.so;/usr/lib/arm-linux-gnueabihf/libopencv_video.so;.....\n", "set(libraries \"cv_bridge;/usr/lib/libopencv_videostab.so;/usr/lib/libopencv_video.so;....\n", "sudo apt-get install -d ros-indigo-cv-bridge                  ## -d just downloads, not install\n\ncd /var/cache/apt/archives\n\nsudo cp ros-indigo-cv-bridge_1.11.6-0trusty-20141201-2058-+0000_armhf.deb ~/Downloads\n\ncd ~/Downloads\n\nmkdir ros-indigo-cv-bridge-extracted\n\nsudo dpkg-deb -R ros-indigo-cv-bridge_1.11.6-0trusty-20141201-2058-+0000_armhf.deb ros-indigo-cv-bridge-extracted\n", "dpkg-deb -R", "libopencv-dev", "libopencv-python", "libopencv-core2.4", "libopencv-imgproc2.4", "sudo dpkg-deb -b ros-indigo-cv-bridge-extracted ros-indigo-cv-bridge-tegra_1.11.6-l0g1x-2.deb", "sudo dpkg -i ros-indigo-cv-bridge-tegra_1.11.6-l0g1x-2.deb", "libopencv-dev", "libopencv-python", "libopencv-core2.4", "libopencv-imgproc2.4", "libopencv-dev", "libopencv-python", "libopencv-core2.4", "libopencv-imgproc2.4", "sudo dpkg -i <my_file>.deb", "sudo apt-get update", "sudo apt-get install libopencv4tegra libopencv4tegra-dev"], "url": "https://answers.ros.org/question/202643/opencv4tegra-and-ros-howto-jetson-tk1/"},
{"title": "cannot launch node of type [rocon_interactions/interactions_manager.py]", "time": "2014-10-15 06:51:55 -0600", "post_content": [" ", " ", " ", " ", "I'm new to the turtlebot and have setup both my workstation and the turtlebot as described in the tutorial. Both machines are running Ubuntu 14.04 and Indigo Igloo. I've been dashing my head against some errors for a while and haven't found any similar posts on the forums. If anyone has suggestions I'd appreciate them.", "When I run on the turtlebot:", "roslaunch turtlebot_bringup minimal.launch", "and on the workstation: ", "rqt -s kobuki_dashboard", "to bringup the dashboard I can see the colour of the battery, warnings, and other features along the top. However, the content is grey and I get errors and warnings as follows:"], "answer": [" ", " ", "I don't know what caused the issue, but I wiped , reinstalled, and re-made everything and it worked.", "Could you mark it as resolved if it is no longer problem?", "I tried, but it doesn't let me accept my own answer.", "What exactly did you wipe? ROS? or Turtlebot packages?", "I also want to know what you wiped? All the ROS system?"], "url": "https://answers.ros.org/question/195018/cannot-launch-node-of-type-rocon_interactionsinteractions_managerpy/"},
{"title": "ROS on read-only filesystem", "time": "2014-06-25 13:08:46 -0600", "post_content": [" ", " ", "Hey,", "I am using an Odroid U2 on a robot and from time to time it can occur that this device (running Ubuntu 13.09, a Linaro distro) becomes read-only after an incorrect shutdown (power is cut from the device). The usual fix for this is to remount it as read-write or execute fsck.ext4 to fix any incorrect inodes etcetera on the SD card.", "I was wondering though what the possibilities were to make sure this cannot even happen. Preferably I would be able to cut the power at any given time without corrupting the SD card. I imagine that a read-only file system to begin with could work, as nothing needs to be written anyway so no corruption would occur. Is this possible? Does anyone have experience with this setup? If so, if you can point me in the right direction, that'd be great :)", "Best regards,\nHans"], "answer": [], "url": "https://answers.ros.org/question/175008/ros-on-read-only-filesystem/"},
{"title": "Minimum Hardware Requirement for ROS", "time": "2014-09-19 16:15:41 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm wondering what is the minimum hardware requirement for ROS? We are trying to build a rover with least power consumption. So we need to select the hardware which is energy efficient and can have ROS run on it."], "answer": [" ", " ", "This is a very tough question, because it depends on which software and which ROS nodes you want to run, and how much performance you need. The \"right\" answer for your application could be anything from a BeagleBone to an Intel i7-based desktop, depending on what you want to use it for.", "This question has been asked before on ROS Answers in various forms. You may want to review those questions and answers as well."], "url": "https://answers.ros.org/question/193078/minimum-hardware-requirement-for-ros/"},
{"title": "Subscribe and publish velocity [closed]", "time": "2014-08-08 18:28:34 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I have a code where I want to see the linear velocity of a powered wheelchair and then publish linear velocity to it.\nAn example of the code is:", "--", "--", "To subscribe to the cmd_vel topic I do:", "My question is... What do I insert in the velCallback function? What does it contain? What should the header of the function be?", "Thank you!"], "answer": [" ", " ", " ", " ", "If you want to synchronize the data and use one callback for both of the topics, have a look at the ", ".", "Edit:", "The callback should look something like this (no guarantee that this works though, beware of typos):", "Please have a look at ", ") to see how subscribers and publishers work.", "I would also suggest to have a queue size of at least 1: ", "Edit2:", "For the case you mentioned in the comments something like this could work (not tested, beware of typos):", "You only have to make sure that the node publishing the original ", " values publishes to ", " now which you can achieve via ", " the topic in the launch file. The node I posted above will take that value and pass it on to ", " or change the linear speed if necessary and then pass it on to ", ". ", "Since the callback is running in a different ", " than your main function you cannot rely on always having a value for your ", " in the main function. So do whatever you need to do with the actual values in the callback. In this easy example this is good enough.", "Hi ", ", do you this this will work?", "What I do in the code I inserted in the comment above is to update linear and angular velocity to the current velocities!", "Then, in main function I do:", "Is this ok?", "I wouldn't create the publisher in the callback because then the publisher will be created when ever a new message arrives, this is unnecessary. Even though a kitten dies every time you use global variable, I would make it a global variable.\nThe thing with the cmd_vel is, that still both messages will reach the robot, so the one with linear.x > 1.8 and your new one and the robot will not know what to do and start stuttering. You have to ", " the output of whatever publishes the cmd_vel in the first place to a new topic and listen to that. Then you pass on every message you get to cmd_vel and change it where necessary.", "Sorry, my bad! I forgot to delete the publisher in the code!", "The code is just like this.. I just updates the current velocity!", "The rest of the code, the comparison to see it its greater that 1.8 I need to have in the main, because in the real code I have, it isn't so simple! So I really need to do this comparison inside main!", "Is it ok this time?", "You have to make sure that callback is not writing to the variable while you are trying to read it. See ", " on how to use boost scoped lock", "This updates at a rate of 40Hz... I will be a matter of milliseconds, I am assuming that shouldn't be a problem!", " ", " ", "yes you can subscribe two or more topics in one node there is example of subscribing two laser scanner and merge the data then publish the msg:", "Hi ", ", my concern now is, what do I insert in the callback function? This verification I do:", "I have to do it in the main, along with the rest of my code... so what do I put in the callback function?"], "question_code": ["ros::Publisher vel_pub_=n.advertise<geometry_msgs::Twist>(\"cmd_vel\", 1);\nif(vel.linear.x > 1.8){\n    vel.linear.x = 1.8;\n    vel_pub_.publish(vel);}\n", "ros::Subscriber vel_sub = n.subscribe(\"cmd_vel\", 0, velCallback);\n"], "answer_code": ["void velCallback(geometry_msgs::Twist::ConstPtr& vel) {\n //Since vel is a const pointer you cannot edit the values inside but have to use the copy new_vel.\n geometry_msgs::Twist new_vel = *vel; \n if(vel->linear.x > 1.8) {\n    new_vel.linear.x = 1.8; \n    vel_pub_.publish(new_vel);\n  }\n}\n", "ros::Subscriber vel_sub = n.subscribe(\"cmd_vel\", 1, velCallback);", "#include \"ros/ros.h\"\n#include \"geometry_msgs/Twist.h\"\n\nros::Publisher pub;\n\nvoid velCallback(geometry_msgs::Twist::ConstPtr& vel)\n{\n   geometry_msgs::Twist new_vel = *vel;\n   if (vel->linear.x > 1.8) {\n     new_vel.linear.x = 1.8;\n   }\n   pub.publish(new_vel);\n}\n\nint main(int argc, char **argv)\n{\n\nros::init(argc, argv, \"my_node\");\nros::NodeHandle n;\n\npub = n.advertise<geometry_msgs::Twist>(\"cmd_vel\", 10);\nros::Subscriber sub = n.subscribe(\"my_cmd_vel\", 10, velCallback);\n\nros::spin();\n\nreturn 0;\n}\n", "cmd_vel", "my_cmd_vel", "cmd_vel", "cmd_vel", "linear_actual", "void velCallback (const geometry_msgs::Twist::ConstPtr& vel_msg) {\n\nros::NodeHandle n;\nros::Publisher vel_pub_=n.advertise<geometry_msgs::Twist>(\"cmd_vel\", 1);\ngeometry_msgs::Twist vel;\n\nlinear_actual=vel_msg->linear.x;\nangular_actual=vel_msg->angular.z;\n}\n", "if(linear_actual > 1.8){\n    vel.linear.x = 1.8;\n    vel_pub_.publish(vel);}\n", "void velCallback (const geometry_msgs::Twist::ConstPtr& vel_msg) {\n\nlinear_actual=vel_msg->linear.x;\nangular_actual=vel_msg->angular.z;\n}\n", "#include \"sensor_msgs/LaserScan.h\"\n#include \"ros/ros.h\"\n\nsensor_msgs::LaserScan downlaser;\nros::Publisher *pub;\nbool new_downlaser=false;\n\nvoid down_laser(const sensor_msgs::LaserScan& msg)\n{\n    new_downlaser=true;\n    downlaser=msg;\n}\n\n\n\n\nvoid up_laser(const sensor_msgs::LaserScan& msg)\n{\n    sensor_msgs::LaserScan scan;\n    scan=downlaser;\n    if(new_downlaser)\n    {\n        ROS_INFO(\"I heard both\");\n\n        for(int i=0;i<msg.ranges.size();i++)\n        {\n            if(downlaser.ranges[i]<msg.ranges[i])\n                scan.ranges.at(i)=downlaser.ranges[i];\n            else\n                scan.ranges.at(i)=msg.ranges[i];\n        }\n    }\n    pub->publish(scan);\n}\n\n\n\n\n\nint main(int argc, char** argv)\n{\n\n    ros::init(argc, argv, \"scan_macher\");\n    ros::NodeHandle nh;\n\n\n\n    ros::Subscriber sub = nh.subscribe(\"upscan\", 10, up_laser);\n    ros::Subscriber sub1 = nh.subscribe(\"downscan\", 10, down_laser);\n\n\n    pub=new ros::Publisher(nh.advertise<sensor_msgs::LaserScan>(\"scan\",10));\n    ros::spin();\n    return 0;\n\n\n}\n", "if(vel.linear.x > 1.8){\n    vel.linear.x = 1.8;\n    vel_pub_.publish(vel);}\n"], "url": "https://answers.ros.org/question/189543/subscribe-and-publish-velocity/"},
{"title": "How can i simulate a car with ROS and Gazebo?", "time": "2011-07-17 21:48:51 -0600", "post_content": [" ", " ", " ", " ", "I want to create my own robot car model and simulate, the steering, the power unit, etc. ", "I'm a novice in ROS and Gazebo.", "Please tell me about this problem detail. And Where could i find a documentation for dummies or Step by Step?", "Thanks."], "answer": [" ", " ", " ", " ", "One of our students created an experimental ", " package for the ART autonomous vehicle in the sandbox branch of the ", " repository.", "That implementation is specific to our vehicle and not complete, but it does run and may be helpful, given the lack of good gazebo documentation.", "There is probably enough demand within the ROS community to justify creating some shared packages for car-like simulation and navigation. As always, the challenge is creating a team with the interest and time to make it happen.", "UPDATE: there is now a ", " for defining interfaces and sharing code.", "Has there been an update to this in the last 3.5 years? We have a car-like vehicle that currently uses JAUS and are looking to move to ROS. It's an outside vehicle with GPS/IMU/Velodyne, teleop and some autonomy. We like the idea of ROS but don't quite know how to start for this type of vehicle.", " has some tools, see ", "  and ", " .", "Took a look at the ackermann_vehicle metapackage: ", " ", " ", "Here is a good start for using ", " ", " ", " ", "You can find some answers ", " (questions regarding Ackermann steering)."], "answer_code": ["utexas-art-ros-package", "gazebo"], "url": "https://answers.ros.org/question/10623/how-can-i-simulate-a-car-with-ros-and-gazebo/"},
{"title": "ROS path planner and inflation", "time": "2014-02-12 02:43:52 -0600", "post_content": [" ", " ", " ", " ", "I have recently made a node to compute a path for my robot using lidar sensor. To do this, I use a costmap ( costmap_2d::Costmap2DROS ) and a planner ( navfn::NavfnROS ). \nI have configured the inflation radius but it seems that the planner don't take the correct value every time.\nFor your information, I use ROS HYDRO (when I used Groovy, I haven't this issue).", "You can find a part of my config file here :", " Here is a video of the problem :  ", "In the video, you can see in gray the unoccupied cells, and in black the occupied/inflated cells. In color (some kind of rainbow) is the potential computed to acheive the asked goal (red arrow). Finally, in green is the computed path every 2 seconds.\nAs you can see, the potential sometime are going on the occupied cells so it seem the inflation radius is changing.", "Have you ever seen this problem? Do you have a solution to that ?", "The code used to compute the path :", "Thanks in advance."], "answer": [], "question_code": ["  inflater:\n    robot_radius: 0.139\n    inflation_radius: 0.14\n    cost_scaling_factor: 10.0\n\n  plugins:\n    - \n      name: obstacles\n      type: \"costmap_2d::ObstacleLayer\"\n    - \n      name: inflater\n      type: \"costmap_2d::InflationLayer\"\n", "void TrajectoryManager::computePath(void)\n{\n    std::vector<geometry_msgs::PoseStamped> global_plan;\n    nav_msgs::Path tmp_path;\n\n    while (sem < 1) {\n            usleep(10000);\n    }\n    sem = 0;\n\n    //make sure we have a costmap for our planner\n    if(planner_costmap_ == NULL){\n            ROS_ERROR(\"move_base cannot make a plan for you because it doesn't have a costmap\");\n            //return false;\n    }\n\n    tf::Stamped<tf::Pose> global_pose;\n    if(!planner_costmap_->getRobotPose(global_pose)){\n            ROS_ERROR(\"move_base cannot make a plan for you because it could not get the start pose of the robot\");\n            //return false;\n    }\n\n    geometry_msgs::PoseStamped start;\n    //if the user does not specify a start pose, identified by an empty frame id, then use the robot's pose\n    //if(req.start.header.frame_id == \"\")\n    tf::poseStampedTFToMsg(global_pose, start);\n\n    current_pose = start;\n\n    if(planner_->makePlan(start, final_pose, global_plan)){\n            //ROS_ERROR(\"planner makes plan\");\n            if(!global_plan.empty()){\n                    global_plan.push_back(final_pose);\n                    //ROS_ERROR(\"globalplan filled\");\n            }\n    }\n\n    tmp_path.header.frame_id = map_name;\n    tmp_path.poses = global_plan;\n\n    // lock\n    my_path = tmp_path;\n    // unlock\n    sem = 1;\n\n}\n"], "url": "https://answers.ros.org/question/128601/ros-path-planner-and-inflation/"},
{"title": "More navigation using Octomap", "time": "2014-04-10 15:16:04 -0600", "post_content": [" ", " ", "Hello everyone,", "Currently, I am building Octomaps in RViz via Octomap server and using them for navigation purposes.  I have been successful in downprojecting or \"pancaking the PointCloud2 points along a specific z-axis range\" from the 3D map and producing a 2D occupancy map.  I save this as a static 2D map and autonomously navigate through the known map with a Turtlebot. In doing this I been able to somewhat use the 3D aspect of Octomap, however I would take advantage of its potential.  One problem I have it the expensive processing power it takes to map a room using Octomap, any ideas for optimizing that would be nice. If you have any suggestions/tips on how I could do more advanced navigation or obstacle avoidance I'd appreciate it.  ", "I am mainly looking for ideas that I can run with and that will help me pursue robotics further.  ", "Thank you! "], "answer": [], "url": "https://answers.ros.org/question/151761/more-navigation-using-octomap/"},
{"title": "Turtlebot outputting over serial, but not receiving", "time": "2014-03-06 16:41:22 -0600", "post_content": [" ", " ", " ", " ", "I have a charging Turtlebot that seems to half-work. When I run ", ", I get the ", " error that so many others have gotten. However, I also am able to open a serial connection in minicom, and I get data from the Turtlebot that seems to be battery charge information.", "How is it that I have a working serial connection, but I can't seem to send commands at all? I've done all the regular chmodding and fixes to udev rules, but I'm at a loss a this point!", "Using ROS hydro, if that helps.", "EDIT -  added the following info:", "Are you (and/or the user running the `minimal.launch`) in the dialout group?", "Yup! Any other ideas? I'm pretty stumped =/", "I have no idea, where did you get the turtlebot from? Perhaps call the distributor, it may be a hardware problem, is it a Kabuki (Turtlebot2) or a iRobot Create base (Turtlebot1)?", "It's a iRobot Create base. We have a few of them, and none of them have worked (tried different cables, different robots, different laptops - nothing!)", "Are you possibly running the node for the Kabuki base instead of the ones for the iRobot create?", "I checked into that, and nope - it's the iRobot create node. I'm also trying to simply connect and send commands via minicom/screen, and it doesn't look like any data is ever sent to the iRobot when I just connect manually (though I'm still getting the battery information sent to my computer!)", "Strange, this is beyond my expertise to fix, it maybe that the cable you are using is not providing bidirectional communication. Maybe someone from the Turtlebot community could give some more insight...", "I've made progress! I tried opening up a serial link in python and it worked \u2013 it seems like there might be an issue with minimal.launch, then. I'll look into it!\n\nThanks for all your help, william!"], "answer": [], "question_code": ["roslaunch turtlebot_bringup minimal.launch", "Failed to open on port /dev/ttyUSB0", "$ ls -la /dev/ttyUSB*\n> crw-rw-rw- 1 root dialout 188, 0 Mar 6 21:23 /dev/ttyUSB0\n$ lsusb\n> ...blahblah...\n> idVendor 0x0403 FTDI ltd.\n> idProduct 0x6015\n$ cat /etc/udev/rules.d/52-turtlebot.rules\n> ATTRS{idProduct}==\"6015\",ATTRS{idVendor}==\"0403\",MODE=\"666\",GROUP=\"dialout\"\n"], "url": "https://answers.ros.org/question/136741/turtlebot-outputting-over-serial-but-not-receiving/"},
{"title": "Hardware to Support ROS, OpenCV, and Motor Control", "time": "2013-12-11 17:29:39 -0600", "post_content": [" ", " ", " ", " ", "I would like to build a robot arm that has sonar and camera sensors to manipulate physical objects.  I intend to use ROS for the motion planning and sensor processing and OpenCV for the camera processing.  I'm early into this project and I've tentatively picked some hardware that I think will get me in the ballpark but I'm not really sure if it will work.", "My choices so far are:  1) Pandaboard ES (dual-core ARM 1.6MHz) to run Lubuntu and ROS (motion planning, sensor processing) and OpenCV for video processing, 2) a Mini Maestro 16-channel PWM motor control driver to drive up to 10 motors (with room to grow), and 3) Talon SR H-bridge controllers for each of the motors.  Note that the motors are 12V and may have max currents up to 60A.  I assume that I'll be able to run the motor encoders back to the GPIO pins of the Pandaboard to form a closed-loop control system.", "I'm worried that the Pandaboard won't have enough CPU power to handle all the image processing, motion control, and motor control.  Is there a better way to handle these needs?  Perhaps a different dedicated board to handle complete PID control of the motors? If so, what specifically would you suggest?  "], "answer": [" ", " ", " ", " ", "Look into the ", " package. It works great with arduino for motor control using the polulo gear motors and encoders. ", "\nThe ", " is another place to look for answers for using ros with arduino. "], "url": "https://answers.ros.org/question/109486/hardware-to-support-ros-opencv-and-motor-control/"},
{"title": "Problem with NXT Ultrasonic Display [closed]", "time": "2012-08-27 04:05:32 -0600", "post_content": [" ", " ", " ", " ", "I want to use the rviz plugin nxt_ultrasonic_display.cpp which receives a nxt_msgs::Range and displays a green cone in front of the robot whose dimensions depends on the value of the range received.", "But when I use it I have a problem, the cone is displayed and it changes its dimension on the basis of the range, the problem is that the mesh of the cone is turned of 90\u00b0 around the z axis from the right position(That is to say that the part which change its dimension on the basis of the range isn't the altitude but the base).", "It is impossible to modify the source code nxt_ultrasonic_diaplay.cpp, so i don't know hoe to fix this problem.\nCan anyone give me some advice?"], "answer": [" ", " ", "the same problem here!, any ideas how to fix?", " ", " ", "me too! please help. ", "thx."], "url": "https://answers.ros.org/question/42371/problem-with-nxt-ultrasonic-display/"},
{"title": "Should I be using ROS?", "time": "2013-09-15 19:53:32 -0600", "post_content": [" ", " ", " ", " ", "I'm making a pretty basic robot powered by a single board Linux computer. It will communicate serially with one servo controller and one microcontroller. It takes in sensor data from both and outputs commands back (there is also a camera, and I will be doing onboard image processing). I want to learn ROS, but I'm not sure how to use it effectively in this situation. Is the normal procedure to make a node that repeatedly reads sensor data and publishes it to a node that reacts to this data and sends it back out? Is there a more effective way to use ROS? How do I know that multiple nodes using serial ports won't 'overlap' and cause errors?"], "answer": [" ", " ", "You should only ever have one node which interacts with the serial port.", "There are a few reasons you might consider using ROS. One way to take advantage of ROS, would be to take all of the data you're receiving over the serial port and publishing it as ROS topics. This way you can record it and play it back off-line using rosbag, you can introspect it on the live system easily using ", ", ", ", or other ", " plugins, and you can visualize it (depending of the type of data it is) in ", ".", "The other way you can take advantage of ROS would be with your camera. ROS provides tools for recording, playing back, and visualizing the camera data. It also probably has a driver for your camera. ROS has good integration with opencv, so if you are using opencv for your image processing. It's is really nice to write a node which reads camera data, does some decision making, and publishes commands to the robot base, because that node can be developed with recorded data on your desktop, and then the exact same node, without modification, can be run on the live robot. ROS does a good job of abstracting the source of the data, whether it is live data, recorded data, or simulated data.", "Hopefully that helps you decide whether or not to use ROS."], "answer_code": ["rostopic echo", "rqt_plot", "rqt", "rviz"], "url": "https://answers.ros.org/question/79606/should-i-be-using-ros/"},
{"title": "Turtlebot Laser Scan Error", "time": "2013-06-28 08:08:58 -0600", "post_content": [" ", " ", " ", " ", "I have a turtlebot 1 (create base) with groovy installed and ubuntu precise (12.04 LTS) on both the laptop and the workstation. After following through the tutorial I got stuck on the 3-d visualization portion. I am using a kinect camera and when I ran ", "roslaunch turtlebot_rviz_launchers view_robot.launch", "The rviz program allows me to see the camera data correctly so I know the kinect is powered up. However the laser data gave me an error.", "For frame [/camera_depth_frame]: No transform to fixed frame [/base_footprint]. TF error: [Lookup would require extrapolation into the future. Requested time 1372440640.091899911 but the latest data is at time 1372440636.899976015, when looking up transform from frame [/camera_depth_frame] to frame [/base_footprint]]", "This was under the Transform [sender=/camera/camera_nodelet_manager]. Which was under status. I got Topic as ok and point as blank. The weird thing was the laser scan was working the run before and I was getting points being received but I couldn't get them to display anywhere. ", "I want to confirm the laser data was being read because when I ran the calibration I was getting Please find wall error. Reading around the site it seem that the issue may come from the laser data not working. ", "I also encountered this from time to time", "Unhandled exception in thread started by\nsys.excepthook is missing\nlost sys.stderr", "when running rviz, which would force close it either when I start up rviz and when I try to view the image from the camera. But it seem to fix itself when I close the terminal and rerun it. ", "The tutorial I used was:\nhttp://ros.org/wiki/turtlebot_bringup/Tutorials/groovy/3D%20Visualisation\nhttp://ros.org/wiki/turtlebot_bringup/Tutorials/groovy/TurtleBot%20Bringup   -> To change the laptop and workstation to the create base. \nhttp://ros.org/wiki/turtlebot_calibration/Tutorials/Calibrate%20Odometry%20and%20Gyro  ->Stuck at step 3"], "answer": [" ", " ", "The TurtleBot simulates laser data from the Kinect data.  To get laser data out you need to have the depth clouds coming in and then depthimage_to_laserscan running to generate the laser scan.  This does not run by default, but is turned on when youdo calibration and by the follower.  If you want to do a different demo or application make sure to include the launch file which turns on the depth camera and does the laser conversions. ", " ", " ", "So, two things:\n1) is the laser being publish?  Make sure the topic is subscribed; you could set the decay time in Rviz to something like 10 second to see if the laserscan match.", "2) is the TF tree working properly? would you like to give some info about your tf setup? ", "3) it might be the problem of frequency publish by the tf tree. But this is very unlikely in your case. ", "you could use the command ", "to check the tf tree. ", "Sorry for the late response, i changed the decay time to 10 seconds in rviz and no results and i took screen shots of the rviz window with tf tree and laser scan http://imageshack.us/g/1/10223651/ is the link to the album with the pictures", "The follower demo in the tutorial works, I can see the lasers coming out of the kinetic camera so the camera looks like it works, the robot was also able to measure the distance between itself and the person. However during any other operation, like calibration and rviz, the laser scan isn't on."], "answer_code": ["view_frames\n"], "url": "https://answers.ros.org/question/66237/turtlebot-laser-scan-error/"},
{"title": "Openni_tracker crashes with a segmentation fault (core dumped). [closed]", "time": "2013-06-12 03:32:22 -0600", "post_content": [" ", " ", " ", " ", "Any idea on what is causing it ? It is running on Ubuntu 12.04 LTS, ROS Fuerte and an intel graphics card.", "gbd run output :", "It also usually happens after I run it once succesfully. The first time it always runs OK, after that it may work once more, but the next will always segfault. After a reboot the same procedure occurs.", "Can you post the backtrace?", "Edited for BT and some details", "I have the same problem than you, more details about when you get this error in my post : ", "/ but I didn't get any answer...", "Since the first time it runs well, maybe the process XnSensorServer does not close properly. After closing the program, try to kill the XnSensorServer.", "Thank you for your suggestion, I haven't been able to test it yet due to a power outage though. As soon as I can I will and post the results !", "That actually seems to work ! I have been testing it for about an hour so far and it seems to do the job ! Thanks a bunch rastaxe ! BTW how do I mark this as solved and your answer as the solution  ?", "I am happy this works for you. I will put my comment directly in the answer section and you can set it as correct."], "answer": [" ", " ", "Since the first time it runs well, maybe the process XnSensorServer does not close properly. After closing the program, try to kill the XnSensorServer.", "This doesn't solve the problem for me... The problem is solved only if I re-plug the Kinect, which is not a good solution..."], "question_code": ["     (gdb) run\n    Starting program: /opt/ros/fuerte/stacks/openni_tracker/bin/openni_tracker \n    [Thread debugging using libthread_db enabled]\n    Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n    [New Thread 0x7ffff1038700 (LWP 8120)]\n    [New Thread 0x7ffff0837700 (LWP 8121)]\n    [New Thread 0x7fffebfff700 (LWP 8122)]\n    [New Thread 0x7fffeb7fe700 (LWP 8127)]\n    [New Thread 0x7fffdf623700 (LWP 8133)]\n\n    Program received signal SIGSEGV, Segmentation fault.\n    0x00007fffe8a56e56 in Segmentation::detectionByPoint(bool, std::vector<Vector2D<int>, std::allocator<Vector2D<int> > > const*) ()\n       from /usr/lib/libXnVFeatures_1_5_2.so\n\n\nBT output :\n#0  0x00007fffe8a56e56 in Segmentation::detectionByPoint(bool, std::vector<Vector2D<int>, std::allocator<Vector2D<int> > > const*) ()\n   from /usr/lib/libXnVFeatures_1_5_2.so\n#1  0x00007fffe8a61b61 in Segmentation::update(Array2D<unsigned short> const&, MotionDetectorByEdges const&, Floor const&, Farfield&, std::vector<Vector2D<int>, std::allocator<Vector2D<int> > > const*) ()\n   from /usr/lib/libXnVFeatures_1_5_2.so\n#2  0x00007fffe8a4769b in SceneAnalyzer::Update(std::vector<Vector2D<int>, std::allocator<Vector2D<int> > > const*, unsigned int) ()\n   from /usr/lib/libXnVFeatures_1_5_2.so\n#3  0x00007fffe8a0f95e in XnVSceneAnalyzer::InitFromSM() ()\n   from /usr/lib/libXnVFeatures_1_5_2.so\n#4  0x00007fffe8a107cb in XnVSceneAnalyzer::UpdateData() ()\n   from /usr/lib/libXnVFeatures_1_5_2.so\n#5  0x00007ffff6d315b7 in ?? () from /usr/lib/libOpenNI.so.0\n#6  0x00007ffff6d3173e in ?? () from /usr/lib/libOpenNI.so.0\n#7  0x00007ffff6d316b3 in ?? () from /usr/lib/libOpenNI.so.0\n#8  0x00007ffff6d31965 in ?? () from /usr/lib/libOpenNI.so.0\n#9  0x00007ffff6d31c0a in xnWaitAndUpdateAll () from /usr/lib/libOpenNI.so.0\n#10 0x000000000040346b in WaitAndUpdateAll (this=<optimized out>)\n    at /usr/include/ni/XnCppWrapper.h:9347\n#11 main (argc=1, argv=<optimized out>)\n    at /tmp/buildd/ros-fuerte-openni-tracker-0.1.3/debian/ros-fuerte-openni-tracker/opt/ros/fuerte/stacks/openni_tracker/src/openni_tracker.cpp:190\n"], "url": "https://answers.ros.org/question/64929/openni_tracker-crashes-with-a-segmentation-fault-core-dumped/"},
{"title": "How to toggle Turtlebot2 (Kobuki) Power for 12V 5A?", "time": "2013-07-05 04:15:51 -0600", "post_content": [" ", " ", "The 19V 2A connector on the Turtlebot2/Kobuki has the nice feature of turning on only when docked/connected to power. However, the other connectors give power continuously and cannot be toggled on and off as far as I can tell from the ROS driver level. I see that the firmware API has some control for these switches. Is there a way to control these toggles through the ROS driver?", "Hi, please take a look at ", "\nHere you can read: \"Netbook recharging connector (only enabled when robot is recharging): 19V/2.1A DC\". \nSo it's a recharging connector only enabled when the robot is recharging directly or on the docking station. atb"], "answer": [], "url": "https://answers.ros.org/question/66682/how-to-toggle-turtlebot2-kobuki-power-for-12v-5a/"},
{"title": "Problem of \"Please point me at a wall.\" with turtlebot_calibration", "time": "2013-02-04 15:48:30 -0600", "post_content": [" ", " ", "I'm a ROS newby attempting to bring up a TurtleBot with ROS Fuerte and Ubuntu 12.04.  I've got the laptop and TurtleBot configured and can get the turtlebot_dashboard going and drive the robot around with a keyboard using the keyboard teleop.  I can also apply power to the Kinect device and see the three requisite Microsoft USB devices with 'lsusb':", "The problem I have is with turtlebot_calibration.  I get the following output:", "Can you view the data from the Kinect?", "Yes, I can view both image and depth data from the Kinect following the instructions here: http://surenkum.blogspot.com/2012/06/getting-kinect-to-work.html"], "answer": [" ", " ", "The problem here turned out to be topics not be remapped properly. This is now fixed in source (", "), version 2.0.2 should be released shortly and so debs will be updated in a few days.", "I've applied the changes, but the problem persists. I'm using Fuerte. Any suggestions?", "I honestly have no idea what shape the Fuerte release is in. I only tried groovy, and would generally suggest using groovy at this point since releases into Fuerte are few and far between.", "I reverted back to the old config and made it work my modifying kinect.launch. So I guess the old config was appropriate on Fuerte. I don't want to switch to Groovy because making everything work took a lot of time, and I don't want to go through that process again, at least not yet!", " ", " ", "I'm having the same issue; the kinet is working well, the scan data is been published in the corresponding topic, but the calibration routine always shows the same error message. Someone has anyidea?"], "question_code": ["Bus 001 Device 010: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nBus 001 Device 017: ID 045e:02ad Microsoft Corp. Xbox NUI Audio\nBus 001 Device 018: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\n", "-------------------------------------------------------------------------\nturtlebot@turtlebot ~ $ roslaunch turtlebot_calibration calibrate.launch\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http;//192.168.0.38;56954/\n\nSUMMARY\n========\n\nPARAMETERS\n * /kinect_laser/max_height\n * /kinect_laser/min_height\n * /kinect_laser/output_frame_id\n * /kinect_laser_narrow/max_height\n * /kinect_laser_narrow/min_height\n * /kinect_laser_narrow/output_frame_id\n * /openni_launch/debayering\n * /openni_launch/depth_frame_id\n * /openni_launch/depth_mode\n * /openni_launch/depth_registration\n * /openni_launch/depth_time_offset\n * /openni_launch/image_mode\n * /openni_launch/image_time_offset\n * /openni_launch/rgb_frame_id\n * /pointcloud_throttle/max_rate\n * /rosdistro\n * /rosversion\n * /scan_to_angle/max_angle\n * /scan_to_angle/min_angle\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n    openni_launch (nodelet/nodelet)\n    openni_manager (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    scan_to_angle (turtlebot_calibration/scan_to_angle.py)\n    turtlebot_calibration (turtlebot_calibration/calibrate.py)\n\nROS_MASTER_URI=http;//192.168.0.38;11311\n\ncore service [/rosout] found\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_breaker_enabler-1]: started with pid [2132]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[openni_manager-2]: started with pid [2133]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[openni_launch-3]: started with pid [2139]\n[ INFO] [1360034582.865846913]: Initializing nodelet with 2 worker threads.\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[pointcloud_throttle-4]: started with pid [2199]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_laser-5]: started with pid [2218]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_laser_narrow-6]: started with pid [2240]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[scan_to_angle-7]: started with pid [2264]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[turtlebot_calibration-8]: started with pid [2266]\n[kinect_breaker_enabler-1] process has finished cleanly\nlog file: /home/turtlebot/.ros/log/0f317c9e-6f42-11e2-a312-74f06d7d6e3b/kinect_breaker_enabler-1*.log\n[ INFO] [1360034585.493648021]: [/openni_launch] Number devices connected: 1\n[ INFO] [1360034585.494195015]: [/openni_launch] 1. device on bus 001:18 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00366910379111A'\n[ WARN] [1360034585.498586615]: [/openni_launch] device_id is not set! Using first device.\n[ INFO ..."], "url": "https://answers.ros.org/question/54279/problem-of-please-point-me-at-a-wall-with-turtlebot_calibration/"},
{"title": "Ubuntu 12.04 and Pandaboard -> fails to open ASUS xtion/ Driver issues for ARM", "time": "2012-08-26 08:41:28 -0600", "post_content": [" ", " ", " ", " ", "I'm using a Pandaboard which runs Ubuntu 12.04 and I try to use openni_launch. Therefore I need to install the OpenNI drivers. I used the tutorial ", " which is pretty good and helped me (and probably a lot more developers) to save some time. Unfortunately, I'm using an Asus xtion (because of the lag of enough usb ports and the fact of less energy consumption). The compiling seems to work well but when I'm trying to run the tutorials I get the following failure msgs:\n\"Open failed: Failed to open the USB device!\" \nBy executing the samples as sudo I get following output:\n\"Open failed: Device Protocol: Bad Parameter sent!\"", "I know that this is probably the wrong forum to ask this question but I know that the robotic community (especially the Raspberry pi owner) are interested to solve this error.", "Would be great get your thoughts about this issue.\nBest regards"], "answer": [" ", " ", "For kinect use, i needed to add a udev rule... perhaps you have to this too."], "url": "https://answers.ros.org/question/42315/ubuntu-1204-and-pandaboard-fails-to-open-asus-xtion-driver-issues-for-arm/"},
{"title": "openni_launch + fuerte + ubuntu 12.04: No image received", "time": "2012-08-27 01:00:41 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I just installed openni_camera and openni_launch stacks. When I launch the driver and rviz using the following commands:", "in rviz, I see a warning telling me that it didn't receive any images. I try to echo the messages sent by ", ", but no messages are published on that topic.", "I need to mention that I get some number of messages from ", " topic. The interesting thing here is that only a number of messages are published by that topic. It stops after a random number of messages.", "There are a few similar problems like mine. Following the proposed answers, I suspect that my drivers might be faulty. But reinstalling them didn't solve the problem.", "Here is a  ", "\nIn this run, I only got 74 point cloud messages. It just stopped at that point. I also didn't get any image messages.", "I deeply appreciate any help.", ": ", "I must also add that when I run the following command:", " and ", ", I can see the camera working for only a few(5-10) seconds, and again it hangs.", " ", "I get lots of errors and warnings but they are said to be harmless, for example", "Exception AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored", "[ERROR] [1346137170.283120890]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressed/set_parameters]", "[ WARN] [1346137171.480522722]: Using default parameters for RGB camera calibration.", "In several questions on Ros Answers, I ran into the problems with the same errors and warning. They all say that ignoring these errors wouldn't do any harm. ", "I paste the full output ", " I cannot comment on the posts, that's why I will post this edit.\nAs pgorczak says, I don't believe that my problem is performance related, I use a powerful computer. In addition to that, even if I don't start rviz or image_view and just echo, let's say, /camera/rgb/image_raw, I don't see any messages on that topic. I believe openni just freezes. ", "I will keep trying to see if I can get it to work. And, thanks a lot pgorczak, I am looking forward to your update.", "Did you check for any suspicious log messages? Try running ", " and ", " before the driver's launch file and see if anything shows up when it hangs.", "I updated the question. Please check edit2.", " were you able to ever solve your problem? I think I am having the same issue. Are you using Ubuntu 12.04 64-bit? Are you using the openni-dev unstable and ps-engine avin2 packages found ", "?", "Yeah, somehow... Please take a look at this reply ", ". In my case, both problems are related"], "answer": [" ", " ", " ", " ", "The first exception is not harmful as you can see ", ".", "When I am using ", ", I also get messages like:", "Still, everything works just fine (image_view and rviz) so those shouldn't be the issue.", "I think running ", " image_view like in your first edit and still getting the freeze pretty much rules out a performance issue--at least concerning rviz. I think (I'm not sure though) the point cloud only gets computed when you subscribe to the points topic which would mean there is pretty much nothing going on computationally when you just display an image with image_view.", "You could also try a different Kinect device or make sure your Kinect works e.g. on Windows with the OpenNI examples to make sure there is no problem with the device itself.", "And my last question would be: What is your version of ROS and your operating system (version)? I'm using ubuntu 12.04 with fuerte and it worked with the OpenNI-packages I got with ", ".", "You could try the driver from openni_camera_deprecated just to see if it gives you different output or some hints on what's going on.", ":", " ", " ", "This post solved my problem about the double advertising", " ", "hope it helps", " ", " ", "Hi,", "I read through the description of your problem and it reminded me of a similar situation I was stuck up in earlier. The solution i found was something to do with the frames settings in your rviz.", "You need to check the fixed frame and the target frame that you have chosen in rviz. Your screenshot did not show the fixed frame. However, you can look at the dropdown box for other options of fixed frame and chose the one in which you would be able to see the rgb image output. You may do a trial and error to find the right one. Usually the fixed frame has to be something like", "/camera/rgb_optical_frame or /camera/base_link", "If you exactly know the various transforms published, you can directly type in the transform relevant to rgb frames in the fixed frame option.", "As far as hanging of your system is concerned, remember that rviz and image topics always consume an ample amount of memory in order to run. Thus, it may get frozen sometimes when it runs out of memory. Always try to use a computer with a high configuration for rviz operations. Besides, you may want to run rviz with the --sync option, i.e.", "rosrun rviz rviz --sync", "This could help you to some extent as it would add necessary delays to synchronize the data flow using buffers.", "Good luck.", "Regards."], "question_code": ["roscore", "rxconsole"], "answer_code": ["roslaunch openni_launch openni.launch", "[ERROR] [1346317018.377804268]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressedDepth/set_parameters]\n[ERROR] [1346317018.384694527]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/compressed/set_parameters]\n[ERROR] [1346317018.413763705]: Tried to advertise a service that is already advertised in this node [/camera/depth_registered/image_rect_raw/theora/set_parameters]\n(...)\n[ WARN] [1346317020.499849507]: Camera calibration file (...)/rgb_A00364A06244047A.yaml not found.\n[ WARN] [1346317020.500131130]: Using default parameters for RGB camera calibration.\n[ WARN] [1346317020.500289570]: Camera calibration file (...)/depth_A00364A06244047A.yaml not found.\n[ WARN] [1346317020.500426152]: Using default parameters for IR camera calibration.\n", "sudo apt-get install ros-fuerte-openni-camera", "openni_camera/OpenNINodelet", "<launch> \n    <node pkg=\"nodelet\" type=\"nodelet\" name=\"openni_manager\" output=\"screen\" respawn=\"true\" args=\"manager\"/>\n    <node pkg=\"nodelet\" type=\"nodelet\" name=\"openni_launch\" args=\"load openni_camera/OpenNINodelet openni_manager\" respawn=\"true\">\n        <!--<param name=\"rgb_frame_id\" value=\"camera_rgb_optical_frame\" />-->\n        <!--<param name=\"depth_frame_id\" value=\"camera_depth_optical_frame\" />-->\n        <param name=\"depth_registration\" value=\"true\" />\n        <param name=\"image_mode\" value=\"VGA_30Hz\" />\n        <param name=\"depth_mode\" value=\"VGA_30Hz\" />\n        <param name=\"debayering\" value=\"EdgeAwareWeighted\" />\n        <param name=\"depth_time_offset\" value=\"-0.055\" /> <!-- Taken from TurtleBot's kinect.launch -->\n        <param name=\"image_time_offset\" value=\"0\" />\n    </node>\n</launch>\n"], "url": "https://answers.ros.org/question/42347/openni_launch-fuerte-ubuntu-1204-no-image-received/"},
{"title": "Enabling the breaker on a Turtlebot Power Board [closed]", "time": "2012-11-09 07:35:34 -0600", "post_content": [" ", " ", " ", " ", "I am using a Turtlebot power board on a regular create for a class project.  I am looking to run a Kinect off of the create, but my project is being run off of Windows.  How do I enable the power board breaker from Windows?  Thank you!", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "answer": [], "url": "https://answers.ros.org/question/47915/enabling-the-breaker-on-a-turtlebot-power-board/"},
{"title": "Gripper Grasp Planner Cluster - gripper model changes during grasp testing", "time": "2013-01-28 06:08:01 -0600", "post_content": [" ", " ", "Hi all,", "I'm testing ", ", which plans grasps on PointClouds as part of the Tabletop Manipulation pipeline. This node requires a configuration file, specifying a model of your gripper & the grasping area.", "I'm running the grasp planner configured for side-grasps only, & displaying the gripper model. It fails to find a valid grasp, so my model needs tweaking, but strangely the planner/Rviz is mangling the model!", "Each time I run the planner, the gripper model is shown in Rviz but with the pieces of the gripper moved in relation to each other! So it's not going to find a solution if it's mangling the model. Or maybe this is just a problem with the visualization markers in Rviz. ", "Please see pictures & config file below.", "Many thanks, ", "\nDavid.", "As a reference, this is the grasping model of the PR2 gripper: ", "\n(Gripper is blue, grasp area is yellow) ", "I'm trying to simulate a simplified human hand. ", "Imagine your thumb points downwards, and you approach an object with your fingers pointing towards it (x-direction). Like grasping a book from a bookshelf. ", "\n(The gripper is shown in green, with the small \"thumb\" pointing out from the screen, and the \"fingers\" would point to the right if they were shown. The grasping area is blue.) ", "These pictures are 3 runs of the grasp planner. Each time the blocks of the gripper model are moved around in relation to each other. ", " ", "Configuration file from cluster grasp planner: "], "answer": [" ", " ", "I've seen that issue as well--as far as I can tell, it's an issue with tf and rviz visualization; it doesn't affect the actual grasp planning.  I'm using a tf broadcaster to broadcast the object frame to draw the gripper model, and my guess is that there are issues with the timing of messages and which tf message rviz is using to draw various boxes.  But I'm not using tf to transform the points into the wrist frame when computing collisions and points-in-the-grasp-box, so the actual grasp planning is unaffected by that. ", "btw, if your 'thumb' really is that short, unless it and the fingers actually move to cover the entirety of your blue grasping area, I would recommending making the blue grasping box at least a bit shorter.", "Thanks ", " I enabled display of the TF frames object_frame & wrist_frame, and they act properly. Also, I removed one gripper_box, so my model has 3 gripper_boxes like the PR2. The boxes now behave properly, but I haven't tested this theory by going back to 4/5 boxes yet  :-)", " Also, I created a new frame specifically as the origin for this grasping model. This frame is passed to object_manipulator. My gripper is asymmetrically positioned to the wrist joint, so this frame has the space_box (grasping area) positioned symmetric to the y & z axes of gripper_boxes.", "...the result is that the pr2_gripper_grasp_planner_cluster now approaches objects with the grasping area symmetric to the z-axis of the object cluster."], "question_code": ["# Gripper area\ngripper_boxes: [ [[0.000, -0.020, -0.053], [0.120, 0.044, 0.03125]], [[0.000, -0.029, 0.001], [0.054, -0.020, 0.051]], [[0.0065, -0.059, 0.00875], [0.0415, -0.029, 0.03125]], [[0.040, -0.089, 0.00875], [0.080, -0.067, 0.03125]] ]\n\n# Grasp area\nspace_boxes: [[ [[0.054, -0.067, 0.013], [0.120, -0.020, 0.023]] ]]\n\njoint_names: \n  right_arm: ['hand_r_thumb_joint', 'hand_r_index_1_joint', 'hand_r_index_2_joint', 'hand_r_index_3_joint', 'hand_r_middle_1_joint', 'hand_r_middle_2_joint', 'hand_r_middle_3_joint']\n  left_arm: ['hand_l_thumb_joint', 'hand_l_index_1_joint', 'hand_l_index_2_joint', 'hand_l_index_3_joint', 'hand_l_middle_1_joint', 'hand_l_middle_2_joint', 'hand_l_middle_3_joint']\n\n# thumb is downwards\npregrasp_joint_angles:\n  right_arm: [1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]\n  left_arm: [1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]\n\n# all closed\ngrasp_joint_angles: \n  right_arm: [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5 ]\n  left_arm: [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5 ]\n\npregrasp_joint_efforts:\n  right_arm: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0 ]\n  left_arm: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0 ]\n\ngrasp_joint_efforts:\n  right_arm: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0 ]\n  left_arm: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0 ]\n"], "url": "https://answers.ros.org/question/53537/gripper-grasp-planner-cluster-gripper-model-changes-during-grasp-testing/"},
{"title": "unknown reason for transform failure", "time": "2012-11-21 00:14:55 -0600", "post_content": [" ", " ", " ", " ", "i'm trying to use the gmapping stack with my robot. i broadcast the following tf every 0.05 seconds:", "i check the result in rviz, if i set the fixed frame as 'base_laser' in the 'global options' tag, everything seems to be fine(the 'LaserScan' tag is always green). But if i set the fixed frame as 'odom' or 'base_link' in the 'global option' tag, the 'LaserScan' tag just change to red and green from time to time. The error message in the 'LaserScan' tag says that 'unknown reason for transform failure'. How can i fix that?"], "answer": [" ", " ", "On systems such as the PR2, the ", " node is used to publish all transforms of a robot model. One important feature of it is that it future-dates static transforms to ensure that static trasnforms can always be looked up. Also have a look at the ", " and ", " for more information on future-dated transform.", "My guess for what happens in your case is that rviz tries to look up the transform between odom (or base_link) to the laser although it hasn't received the transform for the current time yet which will lead to a failure. To fix it, future-date your static transforms."], "question_code": ["current_time = rospy.Time.now()\nodom_quat = tf.transformations.quaternion_about_axis(math.radians(theta), (0, 0, 1))    \nlaser_quat = tf.transformations.quaternion_about_axis(math.radians(-90), (0, 0, 1))\nself.tfBdc.sendTransform((x, y, 0.0), odom_quat, current_time, \"/base_link\", \"/odom\")\nself.tfBdc.sendTransform((0.16, 0.0, 0.2), laser_quat, rospy.Time.now(), \"/base_laser\", \"/base_link\")\n"], "url": "https://answers.ros.org/question/48724/unknown-reason-for-transform-failure/"},
{"title": "Exception in rosserial 'hello world' example", "time": "2013-01-18 08:34:09 -0600", "post_content": [" ", " ", "I am trying to implement rosserial on a new platform (STM32F0-Discovery board, which has a Cortex-M0 processor and is quite a bit more powerful than the Arduino, so I think it should be possible).", "I've created a simple 'hello world' program, based on the hello world example for rosserial_arduino, which simply publishes the std_msgs::String 'hello world'.", "I can upload and run the example on the board, and it receives data over the serial port. Unfortunately, once I run serial_node.py on my host, the board throws an exception (hard fault) in TopicInfo.h at line 42:", "The values of the variables there look a little odd to me, but I'm not exactly sure what they should look like. offset seems correct (13, which matches 'std_msgs::String'). message_type seems corrupted though- it's a jumble of characters with elements of 'std_msgs' and 'String' embedded in it. I'm not sure if that indicates actual corruption, or if the debugger just isn't interpreting it correctly.", "I'm really at a loss- any advice would be appreciated!", "Hey danep, I'm also trying to do the same on STM32F103. Could you tell me as to how you implemented the rosserial on the the stm32?", " I believe I used protocol buffers to handle the memory alignment issues. Unfortunately I haven't used this library in years so I can't be more specific. See my answer here:  "], "answer": [" ", " ", " ", " ", "Okay, so a few things... first of all, I think the garbled variables were due to compile-time optimization. Once I disabled C/C++ optimization, the variables looked much more reasonable.", "Disabling optimization also allowed me to realize that the problem has nothing to do with memcpy- it's due to the following lines:", "The real cause of this error, I think, is a problem with memory addressing. If I add '1' to outbuffer + offset above, the assignment works fine (although the program fails later on). So it would seem that length_message_type is partially overwriting the variable defined just before it. I'll probably open a trac bug for this, but I welcome any suggestions on how to fix this myself.", "EDIT: The problem was indeed with memory alignment. Rosserial was written for the Arduino, an 8-bit platform, but I am implementing it on an STM32F0, a 32-bit platform. On 32-bit platforms, you must take care when writing to RAM that the address is divisible by 4 for proper alignment.", "How would I apply this fix to using rosserial with a chipkit max32?"], "question_code": ["memcpy(outbuffer + offset, this->message_type, *length_message_type);\n"], "answer_code": [" uint32_t * length_message_type = (uint32_t *)(outbuffer + offset);\n*length_message_type = strlen( (const char*) this->message_type);\n"], "url": "https://answers.ros.org/question/52755/exception-in-rosserial-hello-world-example/"},
{"title": "Possible for roslaunch to respawn node after power failure on remote machine?", "time": "2012-08-03 10:10:52 -0600", "post_content": [" ", " ", "I'm attempting to set up power failure or intermittent comms resiliency for remote nodes, and I'm not sure if I need to create a custom solution or if roslaunch might be able to do this for me.", "My simple test has not yielded positive results but I'm not sure if that's expected.", "My simple test situation has indicated that respawn=\"true\" only covers if the single process that represents the remote node goes down will the node be respawned.  If roslaunch is taken down or any parent process, it looks like it will not respawn.", "My gut says there's not any real way around this, but I figure I should ask:", "Can ", " handle respawning remote nodes where remote connection is lost or remote machine goes down via powerfailure?"], "answer": [" ", " ", "I don't believe roslaunch is robust to the remote machine going down.  ", "It has a process management process on the remote machine and I don't think it has the logic to respawn the remote process management process if the machine goes down.  "], "question_code": ["roslaunch"], "url": "https://answers.ros.org/question/40501/possible-for-roslaunch-to-respawn-node-after-power-failure-on-remote-machine/"},
{"title": "\"please point me at a wall\" Calibration Error [closed]", "time": "2012-07-26 22:59:55 -0600", "post_content": [" ", " ", " ", " ", "Hello, ", "I've been able to calibrate my Willow Garage turtlebot just fine up until today. When I powered everything up and tried to calibrate the usual way, I get this error repeating in the terminal where I ran this command:", "Error:", "This repeats forever and ever. Restarting the turtlebot laptop and Create does nothing. There were no changes made to the turtlebot in any way since last time I used it. Any help would be greatly appreciated.", "Blair", "Looks, like there is no laser data.\nCan you check, e.g. in rviz, if you get laser scans, and also if you get kinect data?", "Yep, rviz works just fine. I get all the Kinect data I could possibly want. ", "Do you also get laser data in rviz?", "Yes         ", "I had the same problem.", "I stopped running rviz, put a turtlebot a little farther from a wall,", "relaunched calibrate.launch and it worked well.", "Why don't you try the same procedures?", "So now it starts doing the calibration process, but instead of eventually settling on good calibration values, it always tells me to multiply by ~0.8 to the gyro_scale_correction no matter how many times I run the calibration. The odom_angular_scale_correction seems to be settling ok. Ideas?", "Now, it's settling into a correction scale... I have no idea what happened. Thanks for the help."], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Now, it's settling into a correction scale... I have no idea what happened. Thanks for the help.\nbgagnon (Jul 31 '12)"], "question_code": ["roslaunch turtlebot_calibration calibrate.launch\n", "[ERROR] [WallTime: 1343379431.664575] Please point me at a wall.\n[ERROR] [WallTime: 1343379431.730219] Please point me at a wall.\n[ERROR] [WallTime: 1343379431.794893] Please point me at a wall.\n[INFO] [WallTime: 1343379431.848461] Still waiting for scan\n[ERROR] [WallTime: 1343379431.863741] Please point me at a wall.\n[ERROR] [WallTime: 1343379431.931249] Please point me at a wall.\n[ERROR] [WallTime: 1343379431.994555] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.062440] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.130411] Please point me at a wall.\n[INFO] [WallTime: 1343379432.153411] Still waiting for scan\n[ERROR] [WallTime: 1343379432.194262] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.263563] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.330134] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.394601] Please point me at a wall.\n[INFO] [WallTime: 1343379432.457665] Still waiting for scan\n[ERROR] [WallTime: 1343379432.463357] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.530496] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.594017] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.663488] Please point me at a wall.\n[ERROR] [WallTime: 1343379432.729796] Please point me at a wall.\n"], "url": "https://answers.ros.org/question/39963/please-point-me-at-a-wall-calibration-error/"},
{"title": "gazebo client no icons", "time": "2012-05-16 18:15:48 -0600", "post_content": [" ", " ", " ", " ", "My second problem for today is that gazebo client doesn't show any icons for play, pause, position object, etc.", "\nIn the terminal I see error, which I have seen many times before but always ignored:", "Does libpng warning related to the fact the icons doesn't appear? If so should I remove libpng and install older version? Would it conflict with other ROS packages which are using this library?", "// I tried to remove libpng but it looks like it was installed with my OS, because many other things depends on it and I can't remove libpng12-0 package without messing up whole system (Packages like xorg, gnome, and many others will be removed with bad results)...Not sure how then should I fix this problem...Icons doesn't appear when I tried to move object as well...Just big red, blue and green boxes instead of icons. Any help will be highly appreciated."], "answer": [" ", " ", "I am not sure why this happens sometimes, I have recently seen it even when I built gazebo from scratch.", "The reason is that the version of libpng used by either gzclient it self or one of the dependencies are not correctly compiled and linked, so the version linked is not the one used when compiling....", "I don't intend to spend more time investigating how this happened, but it could relate to openCV shipping with libpng 1.2.46 and referring to it directly in headers, while the default installed in ubuntu 11 and upwards is libpng 1.5.4", "try:", "and see if it attempts to use libpng.so or libpng.12.so, \nyou want it to use the older version \"libpng.so\", and it can be installed by:", "however that will not solve you'r problem as the program still attempts to load the library it was linked to.", "This can be circumvented after installing the extra libpng in ubuntu by running the client like this:", "that loads the libpng from the version required, ", " resolving the dynamically linked libraries. In principle when the stuff in a library is loaded once, it is not necessesary to load it again, and most programs will then use the library pre-loaded.", "This trick will not work for all programs, as a security risk is introduced when allowing the user to manually choose libraries (and perhaps reimplement them), but it will work for gzclient on ubuntu."], "question_code": ["irobot@irobot-desktop:~$ gzclient\nGazebo multi-robot simulator, version 1.0.0\nCopyright (C) 2011 Nate Koenig, John Hsu, Andrew Howard, and contributors.\nReleased under the Apache 2 License.\nhttp://gazebosim.org\n\nMsg Waiting for master\nMsg Connected to gazebo master @ http://localhost:11345\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\nlibpng warning: Application built with libpng-1.2.46 but running with 1.5.4\n"], "answer_code": ["ldd /usr/...../gzclient\n", "sudo apt-get install libpng3\n", "LD_PRELOAD=/usr/lib/i386-linux-gnu/libpng.so gzclient\n"], "url": "https://answers.ros.org/question/34192/gazebo-client-no-icons/"},
{"title": "error with turtlebot_gazebo [closed]", "time": "2012-05-02 10:17:04 -0600", "post_content": [" ", " ", "... logging to /home/mystery/.ros/log/29ff36fe-9493-11e1-9c3e-00262d845d75/roslaunch-mystery-Aspire-5740-1281.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server http://mystery-Aspire-5740:50215/", "PARAMETERS\n * /rosversion\n * /use_sim_time\n * /rosdistro", "NODES\n  /\n    gazebo (gazebo/gazebo)", "auto-starting new master\nprocess[master]: started with pid [1303]\nROS_MASTER_URI=", "setting /run_id to 29ff36fe-9493-11e1-9c3e-00262d845d75\nprocess[rosout-1]: started with pid [1316]\nstarted core service [/rosout]\nprocess[gazebo-2]: started with pid [1319]\nGazebo multi-robot simulator, version 0.10.0", "Part of the Player/Stage Project [", "].\nCopyright (C) 2003 Nate Koenig, Andrew Howard, and contributors.\nReleased under the GNU General Public License.", "gazebo: /home/mystery/Descargas/ogre_src_v1-7-4/OgreMain/include/OgreSingleton.h:80: Ogre::Singleton<t>::Singleton() [with T = Ogre::TerrainPageSourceListenerManager]: Assertion `!ms_Singleton' failed.\n[gazebo-2] process has died [pid 1319, exit code -6].\nlog files: /home/mystery/.ros/log/29ff36fe-9493-11e1-9c3e-00262d845d75/gazebo-2*.log", " ", "... logging to /home/mystery/.ros/log/79b5f7dc-9493-11e1-969d-00262d845d75/roslaunch-mystery-Aspire-5740-1354.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server http://mystery-Aspire-5740:50489/", "PARAMETERS\n * /use_sim_time\n * /robot_pose_ekf/sensor_timeout\n * /diagnostic_aggregator/analyzers/sensors/path\n * /robot_pose_ekf/imu_used\n * /robot_pose_ekf/odom_used\n * /robot_pose_ekf/vo_used\n * /diagnostic_aggregator/analyzers/nodes/timeout\n * /diagnostic_aggregator/analyzers/mode/timeout\n * /pointcloud_throttle/max_rate\n * /diagnostic_aggregator/analyzers/sensors/timeout\n * /diagnostic_aggregator/analyzers/power/type\n * /diagnostic_aggregator/analyzers/power/timeout\n * /diagnostic_aggregator/analyzers/mode/type\n * /kinect_laser/min_height\n * /diagnostic_aggregator/analyzers/digital_io/path\n * /kinect_laser_narrow/max_height\n * /diagnostic_aggregator/analyzers/digital_io/timeout\n * /kinect_laser/max_height\n * /diagnostic_aggregator/analyzers/nodes/path\n * /rosdistro\n * /robot_description\n * /kinect_laser_narrow/output_frame_id\n * /diagnostic_aggregator/base_path\n * /robot_pose_ekf/freq\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/analyzers/digital_io/startswith\n * /diagnostic_aggregator/analyzers/power/path\n * /diagnostic_aggregator/analyzers/mode/path\n * /robot_state_publisher/publish_frequency\n * /diagnostic_aggregator/analyzers/mode/startswith\n * /rosversion\n * /diagnostic_aggregator/pub_rate\n * /diagnostic_aggregator/analyzers/digital_io/type\n * /diagnostic_aggregator/analyzers/sensors/startswith\n * /diagnostic_aggregator/analyzers/power/startswith\n * /kinect_laser_narrow/min_height\n * /kinect_laser/output_frame_id\n * /robot_pose_ekf/publish_tf\n * /diagnostic_aggregator/analyzers/nodes/type\n * /diagnostic_aggregator/analyzers/nodes/contains", "NODES\n  /\n    gazebo (gazebo/gazebo)\n    spawn_turtlebot_model (gazebo/spawn_model)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    robot_state_publisher (robot_state_publisher/state_publisher)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)", "auto-starting new master\nprocess[master]: started with pid [1444]\nROS_MASTER_URI=", "setting /run_id to 79b5f7dc-9493-11e1-969d-00262d845d75\nprocess[rosout-1]: started with pid [1458]\nstarted core service [/rosout]\nprocess[gazebo-2]: started with pid [1472]\nprocess[spawn_turtlebot_model-3]: started with pid [1473]\nprocess[diagnostic_aggregator-4]: started with pid [1474]\nprocess[robot_state_publisher-5]: started with pid [1475]\nprocess[robot_pose_ekf-6]: started with pid [1476]\nprocess[pointcloud_throttle-7]: started with pid [1482]\nprocess[kinect_laser-8]: started with pid [1488]\nprocess[kinect_laser_narrow-9]: started with pid [1501]\nGazebo multi-robot simulator, version 0.10.0", "Part of the Player/Stage Project [", "].\nCopyright (C) 2003 Nate Koenig, Andrew Howard, and contributors.\nReleased under the GNU General Public License.", "gazebo: /home/mystery/Descargas/ogre_src_v1-7-4/OgreMain/include/OgreSingleton.h:80: Ogre::Singleton<t>::Singleton() [with T = Ogre::TerrainPageSourceListenerManager]: Assertion `!ms_Singleton' failed.\n[gazebo-2] process has died [pid 1472, exit code -6].\nlog files ...", " Looks like a ogre/video driver issue. If you want more follow up please ask gazebo questions at:  "], "answer": [], "url": "https://answers.ros.org/question/33159/error-with-turtlebot_gazebo/"},
{"title": "Sensor Calibration - Hokoyu Lidar [closed]", "time": "2012-06-13 04:52:48 -0600", "post_content": [" ", " ", " ", " ", "Hello guys,", "I am working with Husky A200 (Unmanned Ground Vehicle). It has got ", " in it. I am planning to ", " ", ". When the Lidar is powered on, it should calibrate itself, and if there is any deviation in the value, it should then correct itself to the reference value. ", "Has anyone done this before. Any help would be highly appreciated. Thanks in advance.", "With Regards,\nArjun", "What parameters are you trying to calibrate about the lidar? If it's mounted in a fixed position on the robot, then that only needs to be calibrated once and added to the robot's tf tree (or URDF model). I've never needed to calibrate anything else with our hokuyos.", "For reference, Husky URDFs can be acquired here: ", " --- Thank you for your reply. The hokoyu ls mounted in a fixed position on the robot. I want to calibrate it once and as you said , add it to the robots tf tree (or URDF model). Help would be highly appreciated. Thanks in advance.", " - Thank you. I will try this."], "answer": [], "url": "https://answers.ros.org/question/36429/sensor-calibration-hokoyu-lidar/"},
{"title": "Eclipse and Android Tutorial PubSub Errors", "time": "2012-04-03 08:27:59 -0600", "post_content": [" ", " ", " ", " ", "I have been battling through installing a development environment to use rosjava to communicate with a robot (pc) from an android device.  I am relatively new to ROS and Android programming.", "I have managed to finally get pubsub installed on my phone and functioning (from the command line), thanks to a lot of tidbits from everyone on Ros answers.", "I now seem to be having issues getting the project to run from an eclipse environment.  I created a new Android Project and created from existing source, pointing to android_tutorial_pubsub.", "This left me with some unresolved errors.", "My first attempt to fix this I added all the jars I could from android_gingerbread and rosjava until the errors were gone.  I was then able to get the app on my phone but it would fail every time I tried to run it.  I did notice that under properties->android the android_gingerbread reference had a big red X next to it.", "My next attempt was remove all the library references and to create a new android project from existing source and point to android_gingerbread to bring it into eclipse.  This removed all the errors and resulted in a green check mark beside the android_gingerbread reference.", "But now when I try to compile I simply get a java heap error, crashing eclipse.  I've increased the size of the heap but it just keeps happening.", "Is importing android_gingerbread as an Android project not the right way to import it into eclipse?  Is there another environment that I can use to modify the pubsub code?", "I pulled the new files and compiled the rosjava_core fine.  But the Android_core won't compile now.  It fails on UpdateProject.  It can't seem to find the command android in the folder to 'update project'  But if I call android from the directory it opens the SDK manager fine.  So the path is there.", "Ok now parts of android_core compile, had to add -target 1 to the gradle build file to target android-10 based on my target list. I still get a java heap problem when trying to run in eclipse though.   android_honeycomb_mr2 won't compile do I need it for pubsub?"], "answer": [" ", " ", "There were some significant changes in the past couple days that left android_core broken. However, android_core and its documentation have been updated as of today. I suggest pulling and trying again.", " "], "url": "https://answers.ros.org/question/31107/eclipse-and-android-tutorial-pubsub-errors/"},
{"title": "Slam gmapping problems", "time": "2012-02-08 09:42:39 -0600", "post_content": [" ", " ", " ", " ", "Hello all, ", "Trying to do some mapping with a corobot Explorer robot. I have here the terminal output: \nhmt@hmt-snowball:~/roscorobot-code/Corobot/corobot_teleop$ rostopic list\n/PTZ/camera_info\n/PTZ/image_raw\n/PTZ/image_raw/compressed\n/PTZ/image_raw/compressed/parameter_descriptions\n/PTZ/image_raw/compressed/parameter_updates\n/PTZ/image_raw/theora\n/PTZ/image_raw/theora/parameter_descriptions\n/PTZ/image_raw/theora/parameter_updates\n/PhidgetMotor\n/REAR/image_raw\n/bumper_data\n/camera/set_state\n/camera/set_videomode\n/camera_info\n/diagnostics\n/fix\n/gripper_data\n/hokuyo/parameter_descriptions\n/hokuyo/parameter_updates\n/image_raw\n/imu_data\n/infrared_data\n/kinect/depth/image_raw\n/kinect/rgb/image_raw/compressed\n/odometry\n/pantilt\n/phidget_info\n/position_data\n/power_data\n/rosout\n/rosout_agg\n/scan\n/sonar_data\n/spatial_data\n/ssc32_info\n/takepicture\n/tf\nhmt@hmt-snowball:~/roscorobot-code/Corobot/corobot_teleop$ rosbag record -O mydata position_data tf scan odometry \nhmt@hmt-snowball:~/roscorobot-code/Corobot/corobot_teleop$ rosbag play mydata.bag \nWaiting 0.2 seconds after advertising topics... done.", "Hit space to toggle paused, or 's' to step.\n [RUNNING]  Bag Time: 1328743924.828030   Duration: 166.118149 / 332.198020 ", "\nhmt@hmt-snowball:~/roscorobot-code/Corobot/corobot_teleop$ ", "So, as can be clearly deciphered, I recorded a bag file with the /tf and /scan topics. So, how do I use the gmapping utility to generate a map by playing the bag file? ", "Thanks, \n-Hunter A."], "answer": [" ", " ", "Sorry guys, the issue was not any gmapping (Shocking, right?). It was the robot. The driver is messed up beyond my ability to repair. So, the company is fixing it. ", "Thanks anyway for your quick responses, \n-Hunter A. ", " ", " ", "Are you aware of the ", " tutorial? This explains the basics. You can have a look at it and edit your question to be more specific if it still doesn't work.", " ", " ", "Hi Hunter,", "As Stefan said, you can look at the well made tutorial. \nAlso if you are interested you can take a look at the launch file named gmapping.launch in the package corobot_state_tf, that is made to run gmapping on a Corobot. It may help you to understand how to launch gmapping. You should even be able to record the map in your bag file by launch this launch file."], "url": "https://answers.ros.org/question/12946/slam-gmapping-problems/"},
{"title": "rosserial_arduino Processing Messages", "time": "2012-03-20 03:38:40 -0600", "post_content": [" ", " ", " ", " ", "We are trying to set up a publisher/subscriber that will pull in data from a computer (publishing) that is sending drive commands (direction/speed) to the Arduino to control a UGV. We are also looking to have the Arduino send back information (battery_status, turn angle, speed, etc) and we are having issues with getting everything to work. ", "I know this is probably a really stupid question but is there a way that I can print out my str_msg.data to the Arduino serial monitor to make sure that I am actually getting that data and that I am not messing up on that 1st basic step?", "Can you describe the issues your experiencing?", "I am having issues getting the information to \"ping\" back to the sending computer. I am trying to make sure that the arduino is indeed getting what I am sending. So I send the information in a String and i try to pull that data and then have the arduino broadcast it again to prove that it got it.", "In that case did my answer about creating a debug publisher make sense?"], "answer": [" ", " ", "It appears the rosserial_python node is looking for data coming over the serial line to be in a specific format. As such you could not send raw debug information on the same serial port. If your Arduino has more than one serial port, as with the Mega, then you could use Serial to send your debug data to the serial monitor and then use Serial1 to publish and subscribe.", "Alternatively you could create a debug topic and republish to it any data received. From there you could execute the following command to see the data being published to that topic (assuming the topic is called 'debug'):"], "answer_code": ["rostopic echo debug\n"], "url": "https://answers.ros.org/question/30046/rosserial_arduino-processing-messages/"},
{"title": "turtlebot + gazebo + simulated kinect", "time": "2012-02-24 04:04:28 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I'm having problem with reading simulated Kinect raw or rgb image. I'm running electric with turtlebot_gazebo on Ubuntu 10.04. I don't get any of the data to build point cloud in rviz. It is like that Kinect is not publishing to any of the topics even though I see /cloud_throttled [sensor_msgs/PointCloud2] topic.", "The only thing I can read (I assume from kinect?) and visualize in rviz is topic /scan [sensor_msgs/LaserScan].", "Oh, and one quick question. Should turtlebots dashboard work in simulatated environment? Because if I call ", " the dashboard does come up, but only the middle icon under diagnostic is green.", "Thank you for your help!", "Nobody had this problems? Any tips?", "the dashboard is only meant to work with the physical robot"], "answer": [" ", " ", "This is a bug in the gazebo.urdf.xacro file. The camera frame names were not correct and therefore the gazebo plugins couldn't initialize properly. It's fixed in 330:4253a4e5f257 . It's released in turtlebot 0.9.2"], "question_code": ["rosrun turtlebot_dashboard turtlebot_dashboard"], "url": "https://answers.ros.org/question/28410/turtlebot-gazebo-simulated-kinect/"},
{"title": "Navigation stack with a very asymmetric diff drive robot", "time": "2012-03-15 15:55:41 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "We are using ros navigation for a rectangular robot base with the rotation center (O) placed close to one side (sorry for the foolish sketch). M are powered wheels, c castor wheels", "Setting properly the footprint around O (footprint: [[0.18, 0.26], [-0.64, 0.26], [-0.64, -0.26], [0.18, -0.26]]) and base_link also in O, makes the nav stack behave quite bad, getting stuck very easily close to obstacles. The speeds issued by the local planner get very, very low, almost zero. One could said that robot becomes very \"scary\". By other hand, if I fake the footprint and base_link to place the rotation center in the physical center of the robot, navigation improves a lot, and works as expected (as long as you don't try in-place rotations with and obstacle in the back part sides).", "Any similar experience?  Any clue about how to better understand what's happening?", "Thank you very much.\nJorge"], "answer": [" ", " ", "As far as I'm aware, the navigation stack only supports robot footprints that are centered around the center of rotation of the robot. In my experience, the stranger the shape of the robot, the more difficulty the navigation stack has with it. ", "sbpl_lattice_planner has been used for some more eccentric robot shapes, but it takes a little bit more work to get it running.  The default global planners shrink the robot to a point and expand obstacles, which is why they work better for close to circular robots.  It's much faster to do this."], "question_code": ["   ________________\n  |                |\n  | M            c |\n  | O    <--       |\n  | M            c |\n  |________________|\n"], "url": "https://answers.ros.org/question/29835/navigation-stack-with-a-very-asymmetric-diff-drive-robot/"},
{"title": "Kinect problem - [/openni_camera] No devices connected", "time": "2011-11-16 04:17:34 -0600", "post_content": [" ", " ", " ", " ", "Hi, there,\nI'm trying to connect Kinect with turtlebot, but it is not working. First of all, I was following instructions on how to \"hack\" kinect cable and connect it with power board which has gyro. I cut power cable as it was shown and made an adapter to connect with power board. Then, I've connected Kinect with my TrimSlice - I see green light on Kinect start blinking. I've plugged second cable into the power board and run:\nroslaunch turtlebot_calibration calibrate.launch\nAfter all processes has started I consistently see:", "[INFO] [WallTime: 1321466338.673699] Still waiting for scan\n[INFO] [WallTime: 1321466338.975991] Still waiting for scan\n[INFO] [WallTime: 1321466339.277746] Still waiting for scan\n[INFO] [WallTime: 1321466339.580285] Still waiting for scan\n...", "Ctrl+C doesn't stop this process and I would have this output unless I will kill this  process.", "I was searching by error in google and here but didn't get anything which I could apply to my situation. Most of the questions about Kinect related to it work when I can't even start it. Any ideas what I was doing wrong or/and how I can fix it? Thanks.\n-Roman."], "answer": [" ", " ", "can you view images out of the kinect? that would be the first thing to check. when the calibration says waiting for scan, a scan is not being published which could mean in turn that a pointcloud isn't being published"], "url": "https://answers.ros.org/question/11968/kinect-problem-openni_camera-no-devices-connected/"},
{"title": "Failed to put the turtlebot in full mode", "time": "2012-03-13 10:34:01 -0600", "post_content": [" ", " ", "I have the roslaunch turtlebot_bringup minimal.launch running on the turtlebot laptop and the turtlebot dashboard running on my workstation computer but and the icons are green aside from the breakers and the mode icon.  When I attempt to switch the turtlebot's mode to full mode i get the error message:", "\"Failed to put the turtlebot in full mode: service call failed with error: service [/turtlebot_node/set_operation_mode] unavailable\"", "Any ideas as to why i cannot switch modes?", "Is the mode icon grey or red? Do the breaker icons fail to work as well?"], "answer": [" ", " ", " ", " ", "This type of error can occur if you do not have complete network connectivity between computers.  ", "This can also happen if you restart the master without restarting the dashboard. ", " ", " ", "From my experiences, this issue is easily fixed. You ready: restart the robot. If that doesn't work, do this:", "Then, restart. If that doesn't work, charge your turtlebot. ", "This happens every once and a while to me as well. I normally play with it until it works. \nBest of luck, \n-Hunter A. "], "answer_code": ["roscd turtlebot_bringup\nsudo ./[scripts, maybe?]/install.bash <Network interface, probably wlan0>\n"], "url": "https://answers.ros.org/question/29648/failed-to-put-the-turtlebot-in-full-mode/"},
{"title": "what might cause the shoulder_lift_joint not to go all the way up?", "time": "2011-06-18 20:58:54 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm using JointTrajectoryAction to control the arm to follow a trajectory that can be executed well by openrave's controller. Most of the time, it works. But I found one case where the trajectory can be executed by openrave's controller but not JointTrajectoryActionController. ", "Here's a video showing the problem:", "The video first shows the ideal motion of the robot following a trajectory.\nThen it shows the result of the trajectory being executed on the robot via JointTrajectoryAction, where the shoulder_lift_joint does not reach its goal.", "Below is a script showing the problematic motion:", "The problematic trajectory file:", "After the motion completes, the result of /l_arm_controller/state is:", "The trajectory does not contain a point that asks the shoulder_lift_joint to go beyond its soft limit defined in the urdf file which is ~ [-.52, 1.39], but the shoulder_lift_joint still stops at -.35 when it's commended to go to -.45.", "This happens on both the simulated robot in Gazebo and the real robot.", "I tried slowing down the trajectory as well (x10 times), but the shoulder_lift_joint still didn't go to -.45. ", "Below is the output of /joint_states when the motion completes. The effort for /l_shoulder_lift_joint is -0.0023171897976062093.", "The arm also floats in a neutral position when it's not powered."], "answer": [" ", " ", " ", " ", "take ", " for example, you can see 'shoulder_lift_joint' limits are:", "with ", "so safety controllers will start imposing limits before the hardware limits [-0.5236, 1.3963].  In this case, a counter force is applied in the range of [-0.3536, 1.2963] by the safety controller.", "John", " ", " ", "Are you sending commands inside the joint safety limits?"], "answer_code": ["  <!-- Limits updated from Function's CAD values as of 2009_02_24 (link_data.xls) -->\n  <limit lower=\"-0.5236\" upper=\"1.3963\"\n         effort=\"30\" velocity=\"${VELOCITY_LIMIT_SCALE*3.47}\" /> <!-- alpha tested velocity and effort limits -->\n", "  <safety_controller k_position=\"100\" k_velocity=\"10\"\n                     soft_lower_limit=\"${-0.5236+0.17}\" soft_upper_limit=\"${1.3963-0.10}\" />\n"], "url": "https://answers.ros.org/question/10308/what-might-cause-the-shoulder_lift_joint-not-to-go-all-the-way-up/"},
{"title": "How to clear older costmap just before updating the map?", "time": "2011-06-27 09:28:01 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "What I'm trying to do is putting a rolling window on top of each robot (5 robots in total), then inflating the points representing each obstacle near to the robot of interest as soon as a new sensor data is received, but the map will be reset after a new sensor data comes (clearing).", "Here are some of the important parameter settings:", "When I run the simulation, everything seems to be properly set, but somehow obstacles are stored, and they stay there all the time. I've read costmap doc and nav tutorials many times, I checked if the parameters are set, I debugged the costmap and observation buffer source codes and verified that buffer is actually cleared(since observation_persistence is set to 0.0).", "I thought making the sensor as \"marking+clearing\" will help it to overwrite already obtained data automatically. But since it is set as rolling_window with the width and height of 1 meter, sensor couldn't clear the inflated regions since the origin of the sensor becomes outside of the window.\n\"The origin for the sensor at (0.00, 0.00) is out of map bounds. So, the costmap cannot raytrace for it\".", "Having set the raytrace_range 1.0 as in the case of obstacle_range didn't change anything. I was hoping that somehow it will be handled automatically that the sensor data inside the rolling window will be updated and cleared automatically.", "Then, why still see the traces of the dynamic objects, as it is shown in ", "?", "In this video, there are 5 static yellow robots, and five dynamic blue robots, one of which is creating a costmap for itself, and the point clouds being sent to the costmap is shown with red, orange points are obstacles, green regions are the inflated obstacles.", "Many thanks in advance.."], "answer": [" ", " ", " ", " ", "Hi Eitan,", "Current system is very slow since costmap modules -I suppose- were not designed to consider a fully observable, offboard sensing and computing multi-agent platform like this one.", "To be more hacky about clearing the costmap, I dig into the costmap source code. Since the current system with five controllable nodes can achieve at most 30Hz refresh rate, which is half of what it is supposed to be, and we cannot call a service due to the problem you've mentioned, and it will not be that reasonable to make the system spend extra effort on clearing operations, I will clear the map at each updateMap operation.", "It seems Costmap2DROS::resetMapOutsideWindow method does what I'ld like to do. That is, having a rolling window on top of each robot and considering only the latest sensor observation but discarding whole history, clearing everything at each iteration.", "But I don't want to crash other things in dwa_local_planner. Could you please guide me what is the most proper line that I've to insert resetMapOutsideWindow.", "My initial solution is in costmap_2d_ros.cpp line 653:", "It did work indeed, please see ", ", but I'm not sure if it's gonna cause any trouble in dwa_local_planner or other parts of the ros::navigation software that I use for this application.", "Thanks a lot for your assistance..", "Hi, Kadir! I'm struggling with the same issue. And I'm not sure when this two functions should be called? In the beginning of mapUpdate() function?", " ", " ", "In order for the costmap to clear obstacles, it must be presented with a \"clearing\" observation for the previously occupied space. Typically, these observations come from an on-board sensor, such as a laser, from which ray-tracing is performed to clear space. From the video you posted, it looks like you're getting information for the robots' positions from an external sensor. To clear space with an external sensor, you have a couple of options, though they're sort of hacky:", "If you're running navigation 1.3.1 or greater, you can call the \"clear_costmaps\" service which will clear all obstacles from the costmap. Then, upon receiving a new observation from your obstacle sensor, only those obstacles will be put into the map. One thing to note here, however, is that for the period of time between making the service call and receiving a new observation the robot will have an empty costmap and could hit something. This option would be super easy to try out, but does have the caveat of being able to run into something if you're not careful about how you run things.", "Create a node that publishes clearing information in the robot's base_frame. Basically, you'd be faking a laser with a 360 degree field of view. The fake laser won't clear out observations that are actively being asserted by your obstacle detection, but it will clear out stale obstacles. This option is fairly easy to hack together, and doesn't have the race condition above that could lead to an empty map, but remember to set the frame_id of the LaserScan or PointCloud message correctly as otherwise you'll get the \"outside of map bounds...\" error you referenced above.", "Hope this helps.", " ", " ", "That hack should do what you want, it'll make sure to clear the costmap of all obstacles before putting observations into the map, and I don't think it should cause any trouble with the local planner. When you say the current system is slow, I'm assuming that you're talking about performing raytracing operations for fake sensors on all five robots? Perhaps a parameter should be added to the costmap to allow for clearing completely every cycle to support this use case."], "question_code": ["nh.setParam (\"pointcloud/observation_persistence\", 0.0);\nnh.setParam (\"pointcloud/expected_update_rate\", 0.080);\nnh.setParam (\"pointcloud/clearing\", true);\nnh.setParam (\"pointcloud/marking\", true);\nnh.setParam (\"pointcloud/obstacle_range\", 5.0);\nnh.setParam (\"static_map\", false);\nnh.setParam (\"rolling_window\", true);\nnh.setParam (\"width\", 1);\nnh.setParam (\"height\", 1);\nnh.setParam (\"obstacle_range\", 1.0);\nnh.setParam (\"raytrace_range\", 1.0);\n"], "answer_code": ["costmap_->resetMapOutsideWindow(wx, wy, 0, 0);\ncostmap_->updateWorld(wx, wy, observations, clearing_observations);\n"], "url": "https://answers.ros.org/question/10411/how-to-clear-older-costmap-just-before-updating-the-map/"},
{"title": "Pause and resume gmapping?", "time": "2011-11-07 22:15:33 -0600", "post_content": [" ", " ", "Has anyone had success with \"pausing\" a gmapping session (e.g. to change a battery) and starting it up again? Is there a recommended procedure?"], "answer": [" ", " ", " ", " ", "The short answer: This currently is not a supported use case and definitely would require changing core code inside gmapping.", "More detailed explanations can be found here:"], "url": "https://answers.ros.org/question/11852/pause-and-resume-gmapping/"},
{"title": "Camera pose calibration: couldn't find file camera_pose_calibration_cache.bag", "time": "2011-08-08 02:19:57 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "as I followed the camera pose calibration tutorial, I had to include the following lines to my launch files that start up the cameras:", "When I start the altered launch-files, I get the following message:\n Couldn't find file [/tmp/camera_pose_calibration_cache.bag]. Skipping publishing.", "I am able to see the aggregated view of my two cameras and the checkerboard is recognized.\nHowever,  when the bar below each image becomes green, there is no 'beep' sound, indicating that a calibration sample was captured.", "I suppose the problem is the lack of my camera nodes finding the camera_pose_calibration_cache.bag.\nIs there anyone who has a solution?", "Thanks in advance,", "S\u00e9bastien"], "answer": [" ", " ", "The bag file is used to store the results of a previous calibration, so you still see your calibrated cameras in tf after you restart your launch files. This means that the first time you run the calibration, it is normal that the bag won't be there; this is not a problem. ", "To debug your problem, make sure you have rxconsole open when you're running the calibration, and see what kind of feedback you get. For further questions about the camera_pose_calibration, please start a new topic.", "Hello, I am having same problem. The thing is that beside I don't hear any voice, the program does not save any file when I close it with CTRL+C. And I don't see any tf output on rxconsole. I have also checked for write permissions to the folders and they are ok. Could you please help me about this?", " have you gotten this package to work successfully?  I'm stuck on similar problems.", "Yes, create a file named camera_pose_calibration_cache.bag and put the dir location in launch file. It'll fill it up for you ;)", "Thanks! I'm also getting a bunch of other weird errors, I was wondering if anyone had dealt with them or had it running successfully on Hydro: ", "\n\nHave any ideas?  I'm new to ROS (started last semester)."], "question_code": ["<include file=\"$(find camera_pose_calibration)/blocks/calibration_tf_publisher.launch\">\n    <arg name=\"cache_file\" value=\"/tmp/camera_pose_calibration_cache.bag\" />\n</include>"], "url": "https://answers.ros.org/question/10849/camera-pose-calibration-couldnt-find-file-camera_pose_calibration_cachebag/"},
{"title": "Is it safe to change system date/time during execution", "time": "2011-07-26 20:14:21 -0600", "post_content": [" ", " ", " ", " ", "Reading on how ros Time is implemented, it looks like it uses system wall-time. ", "Would it be safe to change system time when ros nodes are running?", "We plan to synchronize the time between robots using NTP on startup using Wi-Fi.", "But if a robot was powered on out of Wi-Fi range, started all it's nodes, and then (once in Wi-Fi range) changed it's system time using NTP, we are afraid that those nodes would start acting weird.", "This question is both for C++ and python implementation of Time."], "answer": [" ", " ", "It is not safe as you already suspected. Timestamps might jump and this might lead to inconsistencies. If you sync with NTP before, there shouldn't be much difference, though.", "It is recommended to use chrony for your application, although there still is the problem that the times might get out of sync. In a comparably short time that should not matter unless you are synching high-frequency sensor data between different machines."], "url": "https://answers.ros.org/question/10739/is-it-safe-to-change-system-datetime-during-execution/"},
{"title": "Is there any way to calculate inertial property of a robot to simulate it in Gazebo", "time": "2011-09-28 06:33:03 -0600", "post_content": [" ", " ", "I have got urdf/xacro files for pioneer3dx robot from \"p2os\" stack, modified them by replacing erratic_robot differential drive plugin and adding transmission for swivel and hubcap (which was not there as many reported a problem in visualizing swivel/hubcap in rviz). Then I tried to simulate pioneer3dx model in gazebo by using erratic_robot_teleop_keyboard node and also using navigation stack (by configuring yaml files).", "My robot is behaving very strangely as it topples, lift backside up....etc. I am sure this is because of inertial properties provided in the model.", "Is there any way to calculate these properties without estimating them by hand...like provide dimensions to any software then it 'll calculate for you??", "Also I have model for schunk powercube arm but without any inertial properties. I don't think schunk 'll provide these information, I have to calculate myself to simulate it in gazebo.", "Looking forward for a quick solution.", "Thanks \nV.N."], "answer": [" ", " ", "One general rule of thumb I use for checking my inertia tensors is:  If total mass of the rigid body is m, and the dimension of the corresponding body is d, then check to see if ixx, iyy, izz are near m*(d/2)^2.", "This is by no means correct, but a sanity check to make sure the moment of inertia are the right order of magnitudes, so the model behaves somewhat physically realistically.", "I've seen too many simulation where mass is 1(kg), and ixx,iyy,izz are also 1(kg*m^2), while the robot is only 10cm in diameter.  This setup usually result in a robot that does not respond to rotational commands easily (given that izz of 1 means there is a 1kg mass at the end of a 1 meter pole).", " ", " ", "I know that SolidWorks can calculate this value for you, and I'm assuming that any other CAD program can as well. ", "As a general rule of thumb, I try to follow a number of guidelines when creating urdf models. These are mostly based on experience, so they should only be taken for what they are.", "You might also want to try the following to make your links more \"solid\":", "You can even use the free Meshlab for this: Filters->Quality Measure and Computations->Compute Geometric Measures. However, Meshlab doesn't document which mass does it suppose (I hope it just uses unit mass) and it provides you the inertia tensor with respect to the origin, not with respect to CoM.", "It assumes unit density. So basically the volume is the mass of the object."], "answer_details": [" ", " ", " ", " ", "Don't make objects too large", "Don't make objects too small", "Avoid modifying the gravity", "Keep masses for dynamic objects as close as possible. (i.e. the simulator will not like it if you place a 1000kg weight on four 1kg wheels. ", "When in doubt, use the identity mass moment of inertia <1 0 0 1 0 1>", " ", " ", " ", " "], "answer_code": ["<gazebo reference=\"back_left_wheel\">\n  <kp>1000000.0</kp>\n  <kd>1.0</kd>\n</gazebo>\n"], "url": "https://answers.ros.org/question/11350/is-there-any-way-to-calculate-inertial-property-of-a-robot-to-simulate-it-in-gazebo/"},
{"title": "Installing turtlebot software on beagleboard.", "time": "2011-06-29 01:40:28 -0600", "post_content": [" ", " ", " ", " ", "Hello I am trying to install turtlebot software on beagleboard so that i can use beagleboard rather then Laptop in turtlebot, its just a try to implement FASTSLAM1.0, I installed ubuntu and ROS on beagleboard from source, Is there any way to install Turtlebot from source. \nThe particular error is: ", " W: Failed to fetch  ", "   Unable to find >>expected entry 'main/binary-armel/Packages' in Release file (Wrong sources.list entry or >>malformed file) ", "E: Some index files failed to download. They have been ignored, or old ones used instead.", "Is it good practise to use beagleboard on turtlebot instead of a high power and high processing laptop for implementing FASTSLAM1.0?"], "answer": [" ", " ", " ", " ", "This is a work in progress and is only meant to supplement the existing TurtleBot tutorials.   ", "sudo apt-get install build-essential python-yaml cmake subversion wget python-setuptools mercurial", "sudo easy_install -U rosinstall", "rosinstall ~/ros \"", "\"", "echo \"source ~/ros/setup.bash\" >> ~/.bashrc", "I am still working on the best way the make all the necessary packages.", "sudo apt-get install python-serial libglut3-dev libcppunit-dev graphviz libxext-dev python-sip-dev libcurl4-openssl-dev unzip libxml2-dev python-wxgtk2.8 libqhull-dev libhdf5-serial-dev joystick libusb-1.0-0-dev flex autoconf libusb-dev automake bluez bison python-bluez libgtk2.0-dev libbluetooth-dev", "The openni-dev, ps-engine (I'm sure that there are others) packages are not currently available from Ubuntu natty arm repositories. Without these packages the Kinect does not work but you can drive the TurtleBot around using the keyboard or joystick.", "Can I do it with Ubuntu 13.04 on beagleboard, and If I can , Is it support get image form kinect camera in turtlebot", " ", " ", "WillowGarage is not building ARM debian packages, so you will have to install from ", ".", " ", " ", " ", " ", "While installing from ", "rosinstall ~/ros \"", "\"", "I got this error :", "Installing ", "  to /home/turtlebot/ros/perception_pcl", "svn: OPTIONS of '", ": Could not read ", "status line: Connection reset by peer (", ")", "ERROR: Failed to install tree '/home/turtlebot/ros/perception_pcl'\n vcs not setup correctly", "turtlebot@omap:~$ svn: Caught signal", "Suggest some solution"], "url": "https://answers.ros.org/question/10435/installing-turtlebot-software-on-beagleboard/"},
{"title": "Logitech webcams very \"dark\" when using uvc_cam", "time": "2011-06-12 08:57:18 -0600", "post_content": [" ", " ", " ", " ", "I am running into an issue with the exposure of my logitech webcams. After launching uvc_cam, either camera (logitech 1 or 2) only works properly after manually changing the camera using uvc_cam_node. If I have /dev/video1 in the camera.launch file, then switching it to /dev/video2 using the uvc_cam_node will make video2 work and vice versa. I am receiving a warning:", "WARN [1307826164.686116661]: [camera] calibration does not match video mode (publishing uncalibrated data)", "When I set it in dynamic_reconfigure and save the settings to a yaml file, it looks like this:", "{absolue_exposure: 2089, brightness: 66, camera_info_url: '', camera_name: camera,\ncontrast: 50, device: /dev/video2, exposure: 1, format_mode: 1, frame_id: /camera,\nframe_rate: 20.0, grain: 82, height: 240, power_line_frequency: 0, saturation: 50,\nsharpness: 142, white_balance_temperature: true, width: 320}", "Best regards, Tim", "Hello Tim, I have downloaded the package form the link: git clone ", ".  I want to know how to build it or make it, thanks"], "answer": [" ", " ", "Okay. Two answers:", "Eventually I'd like to update the driver (maybe once an \"official\" USB camera driver is underway or as part of that work) to work more like ", ", where it only changes out of the auto mode if you explicitly set the driver to use a manual value. I emphasize the eventually in there since I ", " need to write my M.S. thesis instead of work on a camera driver right now ;)", "Hope that helps."], "answer_details": ["The warning you show is perfectly normal if you actually have not yet calibrated your camera at the resolution you are currently running at (or you didn't save the calibration to a file).", "Due to the particular use case we originally forked the driver for, the defaults in the test launch file set the camera to be very \"dark\". What you will want to do is to create your own camera launch file that sets your parameters appropriately. The ones you will have to change to \"undarken\" the camera would either be ", " if you want to manually set the exposure to a fixed value, or change the ", " parameter to an automatic exposure setting. You'll have to play around with dynamic reconfigure to find the appropriate value (but for my Logitech Webcam Pro 9000, the only auto exposure it supports is \"3\"). I can't say exactly for certain what your webcam supports, but you can also play around with it in ", ", which is a Linux tool that uses the same underlying driver interfacing (and so shares parameter constants).", " ", " ", " ", " "], "answer_code": ["absolute_exposure", "exposure", "guvcview", "camera1394"], "url": "https://answers.ros.org/question/10236/logitech-webcams-very-dark-when-using-uvc_cam/"},
{"title": "Turtlebot tutorial - rviz \"2D Pose Estimate\" button", "time": "2011-09-07 04:57:19 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am able to complete all steps in the turtlebot tutorial, except the autonomous navigation part. I am stuck on where I am supposed to click on the \"2D Pose Estimate\" button in rviz and drag an arrow to set the initial location for the robot. I see the map that is loaded, and also the green laser scan dots, as well as the arrow that I drag to set the robots position and orientation. The only problem is that the location of the robot doesn't update after I release the drag.", "Any suggestions?\nThanks!\nDavid", "=================", "\nUpdate Additional questions (9/13)", "Thanks for your answer, Hersh. But the fixed frame option was already set to /map and I still can not get the robot to move. In case it helps, here's the output from the terminal that launched rviz. Thanks!"], "answer": [" ", " ", "This problem can be caused by having an incorrect \"Fixed Frame\" in rviz.  For the \"2d Nav Goal\" and \"2d Pose Estimate\" buttons to work, the Fixed Frame must be set to \"map\".", "The idea is that you are sending pose information to the navigation system, and it needs to know what coordinate frame the information is relative to.  The nav system can only use this data if it is in the \"map\" coordinate frame.", " ", " ", "OK, this should be published as 'comment', but i can't acess the commenting tool. Not enought 'Karma* Ok, still want to share you my similar experiences.", "My turtlebot have the same problem with the 'set estimate pose' command in RVIZ. I get the pose and robot model nicely published and displayed in RVIZ, but i can't set the initial pose, or any navigation goal. My setup sometimes work, sometimes dont.", "The system seems to work on a fresh 'just made map'. trying the day after is more unstable...", "Everything else displays nice on RVIZ, if i manually pushes or rotates the turtlebot, RVIZ updates the movement and add all the red arrow just as if it was working. I just cant set the pose estimate or the 2D nav goal.", "A difference from your tutorials is that i'm not adressing the by 'DNS name' such as turtlebot@marvin, instead i adresse as described in another tutorial by turtlebot@192.168.1.100", "I use a dedicated WLAN with only the turtlebot and a remote computer, same IP:s from day to day. I have checked the network settings as suggested in a similar question ", "Is it likely that this is still an network error ?", "What about the 'ticket,", " is this in progress?", "Any workaround ? is upgrading to 'electric' a way forward ?", "Thank you"], "question_code": ["\n$ rosrun rviz rviz -d `rospack find turtlebot_navigation`/nav_rviz.vcg\n[ INFO] [1315931847.897357347]: Loading general config from [/home/user/.rviz/config]\n[ INFO] [1315931847.905051080]: Loading display config from [/home/user/.rviz/display_config]\n[ WARN] [1315931876.219268690]: Message from [/amcl] has a non-fully-qualified frame_id [map]. Resolved locally to [/map].  This is will likely not work in multi-robot systems.  This message will only print once.\n[ INFO] [1315931976.304023293]: Setting pose: 0.360 -0.107 -0.427 [frame=/map]\n[ INFO] [1315931996.143238028]: Setting pose: 0.322 -0.112 -0.390 [frame=/map]\n[ INFO] [1315932004.627046894]: Setting goal: Frame:/map, Position(-0.084, 0.127, 0.000), Orientation(0.000, 0.000, 0.996, 0.086) = Angle: 2.970\n\n[ INFO] [1315932008.714840910]: Setting goal: Frame:/map, Position(-0.203, 0.249, 0.000), Orientation(0.000, 0.000, 0.987, 0.159) = Angle: 2.822\n"], "url": "https://answers.ros.org/question/11140/turtlebot-tutorial-rviz-2d-pose-estimate-button/"},
{"title": "Gazebo or OpenRave", "time": "2011-03-11 13:54:48 -0600", "post_content": [" ", " ", " ", " ", "We're often in the situation with robot prototypes where several teams and/or government groups seem to always need robot access at the very same time. So we're looking at investing some time putting together a simulation of it under the hood, much like the pr2 simulations. With that done, we can simply hand off a black box (maybe atom powered) running a simulation of the robot to whatever group needs access to it at a higher level.", "I've had a cursory play with gazebo, and have a fake scanner up and running in a 3d world. It looks like I'll have to write something custom like the controller manager plugin for gazebo (can't use pr2's directly, our motor systems are quite different). At this point I kept coming across ros articles about openrave, so...", "What have been people's experiences with gazebo vs openrave for 3d simulations?"], "answer": [" ", " ", " ", " ", "We use OpenRAVE for all our simulations. After spending a few years running with Gazebo, we made the change to OpenRAVE as we found it more flexible, it better suited our needs and I haven't looked back since. Having said that, I haven't tried Gazebo again for almost 2 years and with all the love it's been getting recently, things might have changed.", "The biggest advantages we found were that OpenRAVE's plugin architecture allowed us to simply swap bits in and out (e.g. change the phsyics engine) without having to look deep into the code base, and the combination of a python and C++ API allowed very quick development.", "Agreed completely."], "url": "https://answers.ros.org/question/9392/gazebo-or-openrave/"},
{"title": "need help to install schunk powercube arm controller ros node and corresponding CAN bus drivers [closed]", "time": "2011-10-04 11:30:17 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I was trying to install \"cob_powercube_chain\", a ros node for schunk powercube arm controller. It depends on 3 imortant packages i.e. libm5api, libntcan,libpcan. libm5api is the schunk library and other two (libm5api, libntcan) are CAN bus drivers.\nI couldn't install libntcan when I run the script given in the package. It says", "mst@laptop:~/my_ros/care-o-bot/cob_extern/libntcan$ sudo ./esdcan_make_devices \nPlease do first a \"insmod esdcan_usb331.o or insmod esdcan_usb331.ko ... (depends on kernel version)\"", "However no such file exists to do insmod.....", "my first question is we are attaching arm trhough ESDCAN-PCI (PCI card basically), so is this node working for ESDCAN-PCI331-linux-2.6.x...... driver??", "If so then can anyone please help me to install this libntcan, libpcan drivers. ", "I would be really thankful for this. I am sure someone is working on this and has installed this package and drivers. SO I am looking forward for help.", "THanks "], "answer": [], "url": "https://answers.ros.org/question/11409/need-help-to-install-schunk-powercube-arm-controller-ros-node-and-corresponding-can-bus-drivers/"},
{"title": "Recommended system for complex ROS programs + simulations", "time": "2011-04-25 10:46:51 -0600", "post_content": [" ", " ", " ", " ", "I am looking to build a new computer to run ROS, rviz, and gazebo simulations. I've previously been working on an underpowered Ubuntu VM. What specs (RAM, CPU, GPU...) would you recommend for complex operations like simulating a PR2 or point cloud manipulations? I definitely cannot spend more than ", " so I think dual six core xeon processors are out :-p", "Thanks - Patrick"], "answer": [" ", " ", "At Willow, our standard dev machines are the ", " line from Zareason.  Make sure to get an Nvidia Graphics card, other than that the more upgrades you use the faster it will run. I'd suggest bumping up the memory a little bit, but it's not necessary.   ", " ", " ", "Here's some rough specs on my dev machine. I'm able to run the PR2 in Gazebo simulation and work with Kinect data easily. I can also playback full 30Hz Kinect data (/camera/rgb/points for example) thanks to the speed of the SSD and copious RAM available for caching. ", "Anyways, here's the specs:", "Some of those components have likely been replaced by newer ones, but you get the idea. And really, 6GB of ram is likely plenty, the only time I've seen it go over that was when doing stupid things (such as trying to make a SLAM map with gmapping that was 200m x 200m at 1cm resolution when I didn't need it to be nearly so large). Just watch out for USB 3.0 - some things don't work so well on USB 3.0 right now, whether due to buggy Linux drivers, buggy controllers, etc, I'm not sure.", "For our robots (and just add a good GPU to make it a dev machine) we make sure to have at least 4 simultaneous execution threads (either 2cores, hyperthreaded or 4 cores), 64-bit CPU, 6GB of RAM and a decent SSD (any SandForce-based ones that are ~270MB/s read/write are the current favorite).", "Hope that helps, let me know if you need anymore details.", " ", " ", "I wouldn't think you'd need that much performance. I used p2-gazebo with a Intel Core2 Quad CPU Q9650@3.00GHz and that was fine. I had 4GB and you shouldn't go below that. I didn't run any complicated things though.", " ", " ", "If you want to spent the money, it might be convenient to setup a separate machine for running gazebo. The only thing to watch out for is that you get a proper graphics card, i.e. no onboard intel chipset for running gazebo/rviz."], "answer_details": [" ", " ", " ", " ", "CPU: Intel Core i7 920, 4 cores, 8 threads, 2.66 Ghz", "RAM: 12GB DDR3", "Disk: 120GB OCZ Vertex 2 SSD for code, OS and current work datasets + Standard spinny disk HDDs for longer-term data storage", "GPU: nVidia GTX 275", "OS: Ubuntu 10.10 x64", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/9815/recommended-system-for-complex-ros-programs-simulations/"},
{"title": "AX-12 controller_manager not working", "time": "2011-06-01 12:20:00 -0600", "post_content": [" ", " ", "Out of the blue, the controller_manager from the ax12_controller_core has stopped working.  I am simply launching the file from the tutorial, but it says \"Pinging motor IDs 1 through 5\" then it says \"No motors found, aborting.\"  There is nothing wrong with the power or the data cable.  We can manually control the servo through program we had in windows. But nothing happens in ubuntu and ROS.  Not sure what happened. "], "answer": [" ", " ", "Are you working with USB2Dynamixel dongle? (you can check the device settings: ", "Can you still control them in Window$? I had twice the problem that the ID's were flipped and I had to reset them. Can you still read out the ID's in the Dynamixel program?"], "answer_code": ["$ls -al /dev/ttyUSB0\n"], "url": "https://answers.ros.org/question/10137/ax-12-controller_manager-not-working/"},
{"title": "Filter robot out of laser scan", "time": "2011-08-04 04:00:58 -0600", "post_content": [" ", " ", "I would like to filter out the laser points that hit my robot. There are points above and below the scanner (green thing) that should be removed. I'm thinking I'm supposed to use ", " but am not sure."], "answer": [" ", " ", "There is a ", " package that allows you to filter robot links from your sensor data based on URDF description of the robot.", " ", " ", " ", " ", "In the Turtlebot applications, there is a node called turtlebot_laser that filters out all points within the radius of your base. This is how Willow solved it for the Turtlebot. The only pointer I can give is directly to the ", " as the wiki is not updated."], "url": "https://answers.ros.org/question/10821/filter-robot-out-of-laser-scan/"},
{"title": "How to decrease (openni_)kinect cpu usage in turtlebot pkg", "time": "2011-08-23 01:34:10 -0600", "post_content": [" ", " ", "Hi!", "I've been experimenting with the turtlebot-pkg on a fitpc2 with icreate and kinect but am running into the limits of the cpu now. I noticed that the openni_camera nodelet combined with the throttler uses quite a lot of cpu power. My guess is that this is caused by the fact that the driver publishes data at 30hz, and only after the throttler i get the advantage of the lower framerate (i only need about 5hz). The combination of driver/throttler thus still uses quite some cpu power as it works with the full 30hz datastream.", "Is it possible to lower the framerate in the driver itself, as the cpu-power it uses to publish pointclouds at 30hz is wasted..?"], "answer": [" ", " ", "I believe ", " can adjust kinect driver parameters (or at least downsamples pre-publication, avoiding CPU overhead).", "Note that you can't re-set params for openni_node if it has an subscriber..."], "answer_details": ["Use the GUI: ", "Change params in a launch script: e.g., ", " ", " ", " ", " "], "answer_code": ["rosrun dynamic_reconfigure reconfigure_gui", "<node pkg=\"dynamic_reconfigure\" type=\"dynparam\" name=\"resetter\" args=\"set /openni_node image_mode 8\" />"], "url": "https://answers.ros.org/question/10985/how-to-decrease-openni_kinect-cpu-usage-in-turtlebot-pkg/"},
{"title": "TurtleBot Calibration -- continuous stream of warnings and errors", "time": "2011-09-01 04:14:27 -0600", "post_content": [" ", " ", " ", " ", "I am running the turtlebot calibration procedure for my turtlebot and I am getting a long list of scrolling errors and process restarts when I run the command:", "roslaunch turtlebot_calibration calibrate.launch", "I have run this before and it has worked, but now I am getting a long stream of the following.  I also have a weird error that when I run any ros packages the green power light on the create turns off... but it runs anyway (such as teleop or calibrate).  Namely the light turns off, the diagnostic pane says power system error, but the breaker is lit up green and the dashboard shows it to be in full mode and it responds to keyboard commands.  The error I get when running calibrate.launch is below:"], "answer": [" ", " ", "I have files a ticket to look into what is causing the errors: "], "question_code": ["... logging to /home/turtlebot/.ros/log/4dd2b4ee-d4b2-11e0-89cd-485d607f80f5/roslaunch-turtlebot-laptop-600.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://192.168.11.59:33068/\n\nSUMMARY\n========\n\nPARAMETERS\n * /openni_camera/depth_time_offset\n * /openni_camera/image_mode\n * /kinect_laser/output_frame_id\n * /openni_camera/depth_rgb_translation\n * /pointcloud_throttle/max_rate\n * /openni_camera/depth_mode\n * /openni_camera/shift_offset\n * /scan_to_angle/min_angle\n * /kinect_laser/min_height\n * /openni_camera/depth_rgb_rotation\n * /kinect_laser_narrow/max_height\n * /scan_to_angle/max_angle\n * /kinect_laser/max_height\n * /rosdistro\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /rosversion\n * /openni_camera/debayering\n * /openni_camera/depth_frame_id\n * /openni_camera/image_time_offset\n * /openni_camera/depth_registration\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n    scan_to_angle (turtlebot_calibration/scan_to_angle.py)\n    turtlebot_calibration (turtlebot_calibration/calibrate.py)\n\nROS_MASTER_URI=http://192.168.11.59:11311\n\ncore service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [745]\nprocess[openni_manager-2]: started with pid [752]\nprocess[openni_camera-3]: started with pid [760]\nprocess[pointcloud_throttle-4]: started with pid [767]\nprocess[kinect_laser-5]: started with pid [773]\nprocess[kinect_laser_narrow-6]: started with pid [785]\nprocess[scan_to_angle-7]: started with pid [802]\nprocess[turtlebot_calibration-8]: started with pid [810]\nCaught exception while logging: [Cannot use ros::Time::now() before the first NodeHandle has been created or ros::start() has been called.  If this is a standalone app or test that just uses ros::Time and does not communicate over ROS, you may also call ros::Time::init()]\nCaught exception while logging: [Cannot use ros::Time::now() before the first NodeHandle has been created or ros::start() has been called.  If this is a standalone app or test that just uses ros::Time and does not communicate over ROS, you may also call ros::Time::init()]\n[pointcloud_throttle-4] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/4dd2b4ee-d4b2-11e0-89cd-485d607f80f5/pointcloud_throttle-4*.log\nrespawning...\n[pointcloud_throttle-4] restarting process\nprocess[pointcloud_throttle-4]: started with pid [1057]\n[ INFO] [1314893216.963954647]: [/openni_camera] No devices connected.... waiting for devices to be connected\n[INFO] [WallTime: 1314893217.065955] Estimating imu drift\n[INFO] [WallTime: 1314893217.367307] Still waiting for imu\n[pointcloud_throttle-4] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/4dd2b4ee-d4b2-11e0-89cd-485d607f80f5/pointcloud_throttle-4*.log\nrespawning...\n[pointcloud_throttle-4] restarting process\nprocess[pointcloud_throttle-4]: started with pid [1165]\n[INFO] [WallTime: 1314893217.669826] Still waiting for imu\n[INFO] [WallTime: 1314893217.971408] Still waiting for imu\n[ INFO] [1314893217.972921728]: [/openni_camera] No devices connected.... waiting for devices ..."], "url": "https://answers.ros.org/question/11088/turtlebot-calibration-continuous-stream-of-warnings-and-errors/"},
{"title": "Is my computer powerful enough?", "time": "2011-07-22 09:24:01 -0600", "post_content": [" ", " ", " ", " ", "Hello community,", "My team is constructing an underwater glider that is going to be controlled by a PC104 computer. It is going to have to control/read from servos, pressure sensors, orientation sensors, and such while computing the all the autopilot information. This computer has a 500MHz processor and 256MB of DDR DRAM. Will it be powerful enough to run everything? It is having troubles with the turtle simulation in the tutorial; especially the mimicking turtle...", "Thanks"], "answer": [" ", " ", "closing this question as it has not had activity in over a month", "In general, this question is not answerable by the community, as you are the expert on the your own software needs.  It is possible to use ROS w/o adding significant overhead to what you are doing, so it is likely that your own software will be the bottleneck.  i.e. if your system can run Linux, it can run ROS."], "url": "https://answers.ros.org/question/10696/is-my-computer-powerful-enough/"},
{"title": "Turtlebot Bringup error", "time": "2011-09-02 09:18:58 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "The kubuntu was already installed on laptop, so I installed ros-electric-turtlebot, ros-electric-ros and ros-electric-apps and I configured the networks settings. But when i type \"sudo service turtlebot start\" I get:", "\"start: Job failed to start\". and when I type \"sudo service turtlebot stop\": \"stop: Unknown instance:\"", "This error is shown when I try to use \"roslaunch turtlebot_bringup robot.launch\":", "ERROR: cannot launch node of type [robot_pose_ekf/robot_pose_ekf]:\n  Cannot locate installation of package\n  robot_pose_ekf: [rospack] couldn't\n  find package [robot_pose_ekf].\n  ROS_ROOT[/opt/ros/electric/ros]\n  ROS_PACKAGE_PATH[/opt/ros/electric/stacks]", ".", "Also, when I use  \"roslaunch turtlebot_bringup minimal.launch\", i get these errors:", "[turtlebot_laptop_battery-4] process has died [pid 2985, exit code\n  1]. log files:\n  /home/USERNAME/.ros/log/9db4292a-d59d-11e0-8222-74f06d20ce80/turtlebot_laptop_battery-4*.log", ".", "process[turtlebot_node-3]: started\n  with pid [3147] Traceback (most recent\n  call last): File\n  \"/opt/ros/electric/stacks/turtlebot/turtlebot_node/nodes/turtlebot_node.py\",\n  line 48, in import serial ImportError:\n  No module named serial\n  [turtlebot_node-3] process has died\n  [pid 3147, exit code 1]. log files:\n  /home/USERNAME/.ros/log/9db4292a-d59d-11e0-8222-74f06d20ce80/turtlebot_node-3*.log\n  respawning...", "Does anyone know how can I fix these errors?", "Thanks in advance,", "Lucas"], "answer": [" ", " ", "On your robot you should install ros-diamondback-turtlebot-robot. Currently the TurtleBot software is only stable on ROS diamondback. ", "On your workstation computer you should install ros-diamondback-turtlebot-desktop", "If you have import errors similar to what is listed above then you are most likely missing rosdeps ", "try:"], "answer_code": ["rosdep install turtlebot\n"], "url": "https://answers.ros.org/question/11104/turtlebot-bringup-error/"},
{"title": "Does 'Battery' matter to simulated pr2 robot", "time": "2011-08-15 05:58:48 -0600", "post_content": [" ", " ", " ", " ", "I noticed that, when running pr2 in simulation for a long time, the battery will go down eventually to 0. Does that matter when there's no battery, if so, how to charge it?\nThanks"], "answer": [" ", " ", "No, it does not matter.", " ", " ", "It tries to mimic ", " on the PR2, but has an additional subscription on ros topic ", " to change its own charge state (see ", " for more details)."], "answer_code": ["/gazebo/plugged_in"], "url": "https://answers.ros.org/question/10903/does-battery-matter-to-simulated-pr2-robot/"},
{"title": "How to pass arrays in custom messages", "time": "2011-09-02 09:29:54 -0600", "post_content": [" ", " ", " ", " ", "Hello all,\nI have a node publishing temperature data from a string of 8 batteries written in python. Each battery has 8 temperature sensors, and so rather than building a message with 64 fields I have it broken down into 8 arrays each of length 8. I have written a healthMap node in C++ and I want to subscribe to the temperature data. What should my message structure look like? Python wants to use a tuple and C++ wants a vector. Does anyone have a code example that illustrates how to approach this? I also have little experience working with vectors in C++, so hopefully you could give me a sketch of what the subscriber looks like as well. \nSorry for the long-winded question", "Cheers", "Gideon"], "answer": [" ", " ", " ", " ", "This is similar to ", ". There is a good answer there that has some example code and points to ", ". The tutorial has example code for publishers and subscribers.", "For ", " you can do something like the following (someone correct my C++ if I'm wrong please):", "or follow ", "."], "answer_code": ["std::vector<double>::iterator my_iterator;\nint i = 0;\nfor (my_iterator = msg.vector_field.begin(); msg.vector_field.end(); my_iterator++)\n{\n    my_local_battery_value[i] = msg.vector_field[my_iterator];\n    i++;\n}\n"], "url": "https://answers.ros.org/question/11105/how-to-pass-arrays-in-custom-messages/"},
{"title": "openni_camera driver problem...", "time": "2011-10-12 07:48:50 -0600", "post_content": [" ", " ", " ", " ", "Based on many questions ihere, it looks like there is a problem with the openni driver that makes it not publish reliably.   I'm having this same problem.", "I'm running ubuntu 11.04 and ros diamondback.", "I've tried both installing the driver directly as well as building it from the source as described here: ", " ", "\nI have ps-engine and openni-dev packages installed. ", "When launching:  roslaunch openni_camera openni_node.launch", "I get something that looks like this:...", "but the red laser never turns on (although the green LED continues to blink when it is plugged into one of my USB 2.0 ports). ", "Any ideas on what could be the problem?  The strange thing is, I've viewed point clouds through rviz recently so I've had it working. I didn't think I've done anything to break it, but not sure what happened.  I've seen some others talk about the driver only coming up 1/5 times or 1/10 times, but in my case it isn't coming up at all.", ".... If I can get past these driver problems, I would have some interesting point cloud work to start doing!", "thanks in advance for any advice,", "Walt "], "answer": [" ", " ", "The answer here worked for me:"], "question_code": ["SUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /openni_node1/use_indices\n * /openni_node1/depth_registration\n * /openni_node1/image_time_offset\n * /openni_node1/depth_frame_id\n * /openni_node1/depth_mode\n * /openni_node1/debayering\n * /rosversion\n * /openni_node1/projector_depth_baseline\n * /openni_node1/rgb_frame_id\n * /openni_node1/depth_rgb_translation\n * /openni_node1/depth_time_offset\n * /openni_node1/image_mode\n * /openni_node1/shift_offset\n * /openni_node1/device_id\n * /openni_node1/depth_rgb_rotation\n\nNODES\n  /\n    openni_node1 (openni_camera/openni_node)\n    kinect_base_link (tf/static_transform_publisher)\n    kinect_base_link1 (tf/static_transform_publisher)\n    kinect_base_link2 (tf/static_transform_publisher)\n    kinect_base_link3 (tf/static_transform_publisher)\n\nauto-starting new master\nprocess[master]: started with pid [17226]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to ad069ada-f504-11e0-b3d7-b05ba57903\nprocess[rosout-1]: started with pid [17239]\nstarted core service [/rosout]\nprocess[openni_node1-2]: started with pid [17248]\nprocess[kinect_base_link-3]: started with pid [17252]\nprocess[kinect_base_link1-4]: started with pid [17253]\nprocess[kinect_base_link2-5]: started with pid [17254]\nprocess[kinect_base_link3-6]: started with pid [17260]\n[ INFO] [1318446127.209918971]: [/openni_node1] Number devices connected: 1\n[ INFO] [1318446127.210083741]: [/openni_node1] 1. device on bus 001:12 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'B00363247819047B'\n[ INFO] [1318446127.211848426]: [/openni_node1] searching for device with index = 1\n[ INFO] [1318446127.250457248]: [/openni_node1] Opened 'Xbox NUI Camera' on bus 1:12 with serial number 'B00363247819457B'\n[ INFO] [1318446127.259633975]: rgb_frame_id = '/openni_rgb_optical_frame'\n[ INFO] [1318446127.260773433]: depth_frame_id = '/openni_depth_optical_frame'\n"], "url": "https://answers.ros.org/question/11518/openni_camera-driver-problem/"},
{"title": "erratic referenced robot not moving in gazebo [closed]", "time": "2011-11-07 10:46:07 -0600", "post_content": [" ", " ", "we create our own robot named as pbot referenced as erratic robot, and all the urdf files are highly similar as erratic which has base and wheels. Our probot can be roslaunched in gazebo, and we trying to use ", "$ rosrun erratic_teleop erratic_keyboard_teleop", "to control it,but the base is barely moving, only when push shit+W it moves suddenly a little bit sometimes.", "I attached our pbot base urdf file and I'm wondering if anyone can give some clue how to fix it.", " ", "\n  <include filename=\"$(find pbot_description)/urdf/materials.urdf.xacro\"/>\n  <include filename=\"$(find pbot_description)/urdf/powerbot_wheel.xacro\"/>"], "answer": [], "question_code": ["<robot name=\"powerbot\" \n   xmlns:sensor=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor\"\n   xmlns:controller=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\"\n   xmlns:interface=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\"\n   xmlns:xacro=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\">\n", "    <!-- base_footprint is a fictitious link(frame) that is on the ground right below base_link origin,\n         navigation stack depends on this frame -->\n    <link name=\"base_footprint\">\n        <inertial>\n            <mass value=\"0.0001\" />\n            <origin xyz=\"0 0 0\" />\n            <inertia ixx=\"0.0001\" ixy=\"0.0\" ixz=\"0.0\"\n                     iyy=\"0.0001\" iyz=\"0.0\" \n                     izz=\"0.0001\" />\n        </inertial>\n\n        <visual>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <geometry>\n                <box size=\"0.001 0.001 0.001\" />\n            </geometry>\n            <material name=\"Green\" />\n        </visual>\n\n        <collision>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <geometry>\n                 <box size=\"0.001 0.001 0.001\" />\n            </geometry>\n        </collision>\n    </link>\n\n\n    <joint name=\"base_footprint_joint\" type=\"fixed\">\n        <origin xyz=\"0 0 0.148\" rpy=\"0 0 0\" />        \n        <parent link=\"base_footprint\"/>\n        <child link=\"base_link\" />\n    </joint>\n\n    <!-- Base Link -->\n    <link name=\"base_link\">\n        <inertial>\n            <mass value=\"3.5\" />\n            <origin xyz=\"0 0 0\" />\n            <inertia ixx=\"1.0\" ixy=\"0.0\" ixz=\"0.0\"\n                     iyy=\"1.0\" iyz=\"0.0\" \n                     izz=\"1.0\" />\n        </inertial>\n\n        <visual>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <geometry>\n                <mesh filename=\"package://p2os_urdf/meshes/p3dx_meshes/chassis.stl\"/>\n            </geometry>\n            <material name=\"ChassisRed\" />\n        </visual>\n\n        <collision>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <geometry>\n                <box size=\"0.40 0.27 0.1725\"/>\n            </geometry>\n        </collision>\n    </link>\n\n    <joint name=\"base_top_joint\" type=\"fixed\">\n    <origin xyz=\"0 0 0.08625\" rpy=\"0 0 0\"/>\n    <parent link=\"base_link\"/>\n    <child link=\"top_plate\"/>\n</joint>\n\n    <!-- Top -->\n<link name=\"top_plate\">\n    <inertial>\n    <mass value=\"0.1\"/> \n    <origin xyz=\"0 0 0\"/>\n    <inertia ixx=\"1\" ixy=\"0\" ixz=\"0\"\n         iyy=\"1\" iyz=\"0\"\n         izz=\"1\"/>\n    </inertial>\n\n    <visual>\n    <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n    <geometry>\n        <mesh filename=\"package://p2os_urdf/meshes/p3dx_meshes/top.stl\"/>\n    </geometry>\n            <material name=\"TopBlack\"/>\n    </visual>\n\n    <collision>\n    <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n    <geometry>\n        <box size=\"0.45 0.38 0.01\"/>\n    </geometry>\n    </collision>\n</link>\n\n    <joint name=\"base_caster_support_joint\" type=\"continuous\">\n    <origin xyz=\"-0.14 0 -0.093\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <anchor xyz=\"0.01 0 0\"/>\n        <limit effort=\"100\" velocity=\"100\" k_velocity=\"0\" />\n        <joint_properties damping=\"0.0\" friction=\"0.0\" />\n    <parent link=\"base_link\"/>\n    <child link=\"base_caster_support_link\"/>\n</joint>\n    <transmission type=\"pr2_mechanism_model/SimpleTransmission\" name=\"base_caster_support_trans\">\n        <actuator name=\"base_caster_support_motor\" />\n        <joint name=\"base_caster_support_joint\" />\n        <mechanicalReduction>1.0 ..."], "url": "https://answers.ros.org/question/11831/erratic-referenced-robot-not-moving-in-gazebo/"},
{"title": "Rosserial with the Maple IDE", "time": "2011-09-30 07:18:42 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to set up the new rosserial package to work with the leaflabs Maple board.  I tried using the supplied rosserial_client but it seems that its still trying to refer to the ArduinoHardware.h file which confuses me since it is supposed to be the general implementation of the rosserial communication.  I have a Arduino board as well and I tested it using the rosserial_arduino package and it works wonderfully but unfortunately the arduino processor isn't powerful enough to handle the wheel encoders I'm using. Has anybody already ported over to the maple board using rosserial? If so can somebody point me to the code.  Or maybe shed some light on how to integrate the rosserial_client with the Maple IDE like what they did for the Arduino?", "This is my current set of code for the rosserial_client, but I think there is an error with the read/write because the terminal just pops out the message \"Lost sync with device, restarting...\n\"  after I initialize the node using \"rosrun rosserial_python serial_node.py /dev/ttyUSB0\".  ", "I have also attempted the Serial2.write() function inplace of the print that I have now and get a different error, which is \"Failed to parse subscriber. unpack requires a string argument of length 4\"\nAny help/insight would be appreciated."], "answer": [" ", " ", "I found the ", " which is what I was needing to understand. It was not under the tutorials for the overall package, but instead just under the rosserial_client package.", " ", " ", "When did you checkout the code? There was a ros.h accidently checked in until Sep 18th -- but there shouldn't now be any arduino-related code in rosserial_client, if you find something, please file a ticket to have it removed. "], "question_code": ["*#ifndef MAPLEHARDWARE_H_\n#define MAPLEHARDWARE_H_\n\n#include \"WProgram.h\"\n#include <HardwareSerial.h>\n\nclass MapleHardware {\npublic:\n\nMapleHardware(){\n    baud_ =  57600; \n    }\n\nvoid init(){\nSerial2.begin(baud_);\n}\n\nint read(){\n  numUnread = Serial2.available();\n\n  if (numUnread > 0) {\n    return Serial2.read();\n  }\n  else{return -1;}\n}\n\nvoid write(uint8_t* data, int length){\nfor(int i=0; i<length; i++) Serial2.write(data[i]);\n}\n\nunsigned long time(){return millis();}\n\nprotected:\nuint32 baud_;\nint numUnread;\n};\n\n#endif\n"], "url": "https://answers.ros.org/question/11383/rosserial-with-the-maple-ide/"},
{"title": "proliferation of dynamic_reconfigure servers due to image_proc or image_transport", "time": "2011-09-16 13:44:06 -0600", "post_content": [" ", " ", "Hi-", "I've noticed recently that it has become increasingly difficult to make use of the dynamic reconfigure gui because there are too many servers in the dropdown list. It seems now every image transport is adding two servers to the list, one for \"compressed\" and one for \"theora\".", "When the PR2 is running there is more than a whole screen's worth of dynamic reconfigure servers just for the image processing stuff. Trying to find or quickly switch between servers in the drop down menu is really annoying... I've resorted to making my servers of interest start with 'a_<name>' so that I can find them first, but this seems hacky/unsustainable.", "Is there any way to set a flag that tells some of those image transport nodes that I don't want them to advertise their servers? Or to tell the gui to ignore a set of nodes? (Maybe the servers could advertise a priority, like the different ROS_INFO/ROS_WARN levels, or they could belong to groups that are easier to manage.)", "Ideas?"], "answer": [" ", " ", "You could write a Trac enhancement ticket for ", " for this. Maybe something along the lines of Eric's suggestion. We are currently planning Fuerte changes, maybe this could be included.", " ", " ", "Seems to me that a good solution would be to make the dropdown list a tree structure based on namespace. This would be analogous to, say, how Nautilus displays a folder hierarchy in \"List\" view, replacing folders with namespaces and files with actual dynamic_reconfigure servers.", "I think this would keep the number of nodes available to select manageable as long as your ROS nodes are namespaced well."], "answer_code": ["dynamic_reconfigure"], "url": "https://answers.ros.org/question/11236/proliferation-of-dynamic_reconfigure-servers-due-to-image_proc-or-image_transport/"},
{"title": "Turtlebot doesn't start rotating on calibration", "time": "2011-11-15 14:04:06 -0600", "post_content": [" ", " ", " ", " ", "It's almost the same with ", " but I open as new question since that question is not closed and ", " software must have been updated since then. ", "Calibration movement doesn't start and keeps saying \"Still waiting for imu\". Stdout result is below (long, sorry). On other turtlebots calibration go properly. I found that the green power light on ", " goes off with the beep sound when I start running calibration ", " file, even though ", " still shows the battery amount and breakers in green.", "Env) Ubuntu 10.04, ", " as well as "], "answer": [" ", " ", "Today calibration is successful. What I did before trying calibration is to re-do ", " (I remember I did it even before I asked this question). Anyway thank ", " and all!"], "question_code": ["Turtlebot", "iRobot Create", "launch", "dashboard", "ROS electric", "turtlebot-robot", "turtlebot@turtlebot-3:~$ roslaunch turtlebot_calibration calibrate.launch \n... logging to /home/turtlebot/.ros/log/eff803fe-1004-11e1-bc0b-485d6032f3ac/roslaunch-turtlebot-3-9308.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://turtlebot-ip:59709/\n\nSUMMARY\n========\n\nPARAMETERS\n * /openni_camera/depth_time_offset\n * /openni_camera/image_mode\n * /kinect_laser/output_frame_id\n * /openni_camera/depth_rgb_translation\n * /pointcloud_throttle/max_rate\n * /openni_camera/depth_mode\n * /openni_camera/shift_offset\n * /scan_to_angle/min_angle\n * /kinect_laser/min_height\n * /openni_camera/depth_rgb_rotation\n * /kinect_laser_narrow/max_height\n * /scan_to_angle/max_angle\n * /kinect_laser/max_height\n * /rosdistro\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /rosversion\n * /openni_camera/debayering\n * /openni_camera/depth_frame_id\n * /openni_camera/image_time_offset\n * /openni_camera/depth_registration\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n    scan_to_angle (turtlebot_calibration/scan_to_angle.py)\n    turtlebot_calibration (turtlebot_calibration/calibrate.py)\n\nROS_MASTER_URI=http://turtlebot-ip:11311\n\ncore service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [9329]\nprocess[openni_manager-2]: started with pid [9330]\nprocess[openni_camera-3]: started with pid [9331]\nprocess[pointcloud_throttle-4]: started with pid [9332]\nprocess[kinect_laser-5]: started with pid [9333]\nprocess[kinect_laser_narrow-6]: started with pid [9345]\nprocess[scan_to_angle-7]: started with pid [9350]\nprocess[turtlebot_calibration-8]: started with pid [9362]\n[INFO] [WallTime: 1321415783.246879] has_gyro True\n[INFO] [WallTime: 1321415783.368377] Estimating imu drift\n[INFO] [WallTime: 1321415783.670546] Still waiting for imu\n[INFO] [WallTime: 1321415783.972455] Still waiting for scan\n[INFO] [WallTime: 1321415784.274242] Still waiting for scan\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/eff803fe-1004-11e1-bc0b-485d6032f3ac/kinect_breaker_enabler-1*.log\n[INFO] [WallTime: 1321415784.575880] Still waiting for scan\n[INFO] [WallTime: 1321415784.877857] Still waiting for scan\n[INFO] [WallTime: 1321415785.179778] Still waiting for scan\n[INFO] [WallTime: 1321415785.481708] Still waiting for scan\n[INFO] [WallTime: 1321415785.783952] Still waiting for scan\n[INFO] [WallTime: 1321415786.086041] Still waiting for scan\n[INFO] [WallTime: 1321415786.387849] Still waiting for scan\n[INFO] [WallTime: 1321415786.690298] Still waiting for scan\n[INFO] [WallTime: 1321415786.992970] Still waiting for scan\n[INFO] [WallTime: 1321415787.295427] Still waiting for scan\n[INFO] [WallTime: 1321415787.597576] Still waiting for scan\n[ INFO] [1321415787.648486655]: [/openni_camera] Number devices connected: 1\n[ INFO] [1321415787.649516784]: [/openni_camera] 1. device on bus 001:07 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00364856520048A'\n[ WARN] [1321415787.653199988]: [/openni_camera] device_id is not set! Using first device.\n[ INFO] [1321415787.733836961]: [/openni_camera] Opened 'Xbox NUI Camera' on bus 1:7 with serial number 'A00364856520048A'\n[ INFO] [1321415787.780257377]: rgb_frame_id = 'kinect_rgb_optical_frame' \n[ INFO] [1321415787.787665082]: depth_frame_id = 'kinect_depth_optical_frame' \n[INFO] [WallTime: 1321415787.899734] Still waiting for scan\n[INFO] [WallTime: 1321415788.202021] Still waiting for scan\n[INFO] [WallTime: 1321415788.504294] Still ..."], "url": "https://answers.ros.org/question/11956/turtlebot-doesnt-start-rotating-on-calibration/"},
{"title": "Turtlebot on Roomba 564, many different problems.", "time": "2011-10-04 21:12:57 -0600", "post_content": [" ", " ", "Hi, I received a turtlebot pack from turtlebot.eu and I have been having problems with it. During the explanation of the problems I will refer to pictures and texts, you can download them from this link:\n", "The Roomba seems to work normally as it can clean and also go back to charge automatically while using it for its normal purpose. I hope you can help solving the problems.", "When we run the dashboard it looks like the first picture attached (1.png) if it is connected to the power grid. If we don't have the Roomba connected to the power grid it is impossible to run any command by pressing the breakers or the button to change mode. If we try to push some breaker we get a error message:\nService call failed with error: service [/turtlebot_node/set_digital_outputs] unavailable\nYou can see the window that appears on 2.png. This problem doesn't exist if the Roomba is connected to the power grid.\nIf we try to chang the robot to full mode we get two different error messages depending on :\nFailed to put the turtlebot in full mode: service call failed with error: transport error completing service call: unable to receive data from sender, check sender's logs for details (you can see the window on 3.png)\nFailed to put the turtlebot in full mode: service call failed with error: service [/turtlebot_node/set_operation_mode] unavailable (you can see the window on 4.png)\nIf the Roomba is connected to the power grid we can change to full mode and also activate breakers, and also the diagnostics change. You can see the diagnostics when the Roomba is connected to the power grid in pictures 5 and 6, and when disconected on 7 and 8.", "A part of that when we try to move the robot by using 'roslaunch turtlebot_teleop keyboard_teleop.launch' we get to see that there is something published on the topic '/turtlebot_node/cmd_vel', but the robot doesn't seems to reed that as it is not moving. What type of problem can it be? Is it possible that the we need to do something to the Roomba so it starts reading the topic? We have tried it both ways, with the Roomba connected and disconnected of the power grid.", "The next thing it is happening is that when we try to run the calibration with 'roslaunch turtlebot_calibration calibrate.launch' we get the same message all the time:\n[INFO] [WallTime: 1317739604.242895] Still waiting for imu (picture 9, text 1)\nAnd if we don't have the kinect connected directly to the power grid we also have this other message:\nHow can we calibrate/run the turtlebot if we can not receive data from imu?", "The last thing, quite related to the second part of last problem, is that on your last e-mail you tell me that the power board is supposed to do the same as the USA one, but ..."], "answer": [" ", " ", "Hi Albert, as you might have read in the provided pdf and in the email we send before, we use a different low level driver, cause the system api of the roomba is different from this from the create. \nI'm currently at Willow Garage and one the things I'll work on here is to integrate the low level interface in the default turtlebot stack."], "url": "https://answers.ros.org/question/11412/turtlebot-on-roomba-564-many-different-problems/"},
{"title": "Routerstation Pro + OpenWRT or DebWRT + ROS + Arduino [closed]", "time": "2011-10-07 06:43:30 -0600", "post_content": [" ", " ", " ", " ", "I was wondering if its even possible to compile a bare bones ROS Arduino node that will work with this great little board. The main reason is to have something cheap with up to 3 radios at different freq with gigabit ethernet, wifi, usb, GPIO, and serial connections. I might not even need arduino for drive just use gpio and step direction some stepper motors. If you get a ham lic evidently you can run on freq that no one else is on. In competitions and the like everyone has wifi and its a nightmare to get any good connection so going 700mhz-5Ghz is great. At any rate there is also a DebWRT version that might work. Is there any issues with using a mips 24kc proc? I want to use this as a removeable base with a diff drive and minimal sensors controlled by arduino. Think of this as a low power mobile mesh station (kind of like a turtlebot) sending video etc to remote computer for heavy processing if needed. Just start out maybe use poor mans lidar and sonar. i think at 800mhz it might do pretty well on on its own, it doesn't have to move very fast. Lets say its driving along gets out of range from base station then it needs to be smart enough to turn around avoid humans etc and get back to a good spot. Before someone says yes i know one of the radios does hold world record at 186miles, how fraekn far do i need to go:)", " ", "/ "], "answer": [], "url": "https://answers.ros.org/question/11465/routerstation-pro-openwrt-or-debwrt-ros-arduino/"},
{"title": "Why does pr2_plugs fail to locate my wall outlet?", "time": "2011-10-07 05:25:32 -0600", "post_content": [" ", " ", " ", " ", "I am trying to get our PR2 to use the autonomous plugging action, and was able to get pr2_plugs_actions to run (using  app_plugin.py) but the /vision_outlet_detection node would consistenty fail to detect our wall outlets.  I looked at textured stereo images from Rviz and can verify that the outlet is in the images that the robot receives.  Is there anyway to look into what's going on with the detector more closely to see what's happening with it?  I tried changing the template from green_2x1white to white2x1hall, and fake_wall_2x1white_plug but none of these seems to work.", "Has anyone else run into this problem?  We are running Electric on Ubuntu Lucid and our outlet is a 2x1 with the third prong on the bottom (shadowy side of outlet)."], "answer": [" ", " ", "The outlet detection code is only tested with the default template on outlets that look like the ones used here: ", " Does your outlet look the same? The detector only looks at the position of the holes, not the outlet cover. ", "Did you look at the output of rxconsole to see if there's any more feedback? Could you describe the behavior of the robot in more detail, that might give a better understanding of where in the state machine the robot got (did the projector turn on, did the robot align with the wall, did the robot succeed in finding the outlet from far away, ...)."], "url": "https://answers.ros.org/question/11464/why-does-pr2_plugs-fail-to-locate-my-wall-outlet/"},
{"title": "Brushless Motor Control", "time": "2011-11-18 09:10:39 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to design the motor control system for an autonomous quad-rotor. We're building it from scratch, against my personal desire to use an existing platform like the pelican, and we're using brushless motors to power the propellers. My group just purchased the Mystery 12A Brushless Speed Controller (Blue Series), and I'm trying to figure out how to get ROS to talk to it. I really have no idea how to do this. The tutorials do not mention anything about how to control motors. Will I have to write my own system? Is there a package that can already do this?"], "answer": [" ", " ", " ", " ", "You'll generally want to have microcontroller doing this very low level control and interfacing stuff with hard realtime constraints. If you look at basically any quadrotor you'll notice that they all have a microcontroller for interfacing the motor controllers, IMU, ultrasonic sensors, barometer etc. (and most have the basic attitude stabilization loop running at 100-1000Hz running there too). Even if you'd have the attitude estimation and control running on a PC board, you'd still need a microntoller for interfacing everything.\nNote that ROS in it's basic form does not satisfy hard real-time constraints (there is the OROCOS integration and a realtime_tools package that could be used for that though). "], "url": "https://answers.ros.org/question/11996/brushless-motor-control/"},
{"title": "Cant get data from Create robot - Error reading from SCI port ttyUSB0", "time": "2011-11-28 11:20:44 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "in the turtlebot-dashboard all the sensors are Stale apart from the Laptop battery one, and when I do ", " I get the code below. Weird thing is it worked perfectly yesterday; then I played with Kinect drivers and then it stopped working. So I did a fresh ubuntu install on the turtlebot laptop but I still get the same! ", "Turtlebot is on and light is green by the way so there is plenty of power. I've also checked ", " and it has read and write rules (666). I also checked dmesg and the turtlebot is on ttyUSB0. I can also see the leds on the irobot cable light up in both directions so there is traffic."], "answer": [" ", " ", "turned out my cable was faulty...", "How did you discover that your cable was faulty?"], "question_code": ["roslaunch turtlebot_bringup minimal.launch", "/etc/udev/rules.d/52-turtlebot.conf", "Checking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://192.168.1.15:49038/\n\nSUMMARY\n========\n\nPARAMETERS\n * /use_sim_time\n * /robot_pose_ekf/sensor_timeout\n * /diagnostic_aggregator/analyzers/sensors/path\n * /robot_pose_ekf/imu_used\n * /robot_pose_ekf/odom_used\n * /robot/name\n * /diagnostic_aggregator/analyzers/mode/timeout\n * /diagnostic_aggregator/analyzers/sensors/timeout\n * /diagnostic_aggregator/analyzers/power/type\n * /turtlebot_node/update_rate\n * /diagnostic_aggregator/analyzers/power/timeout\n * /diagnostic_aggregator/analyzers/mode/type\n * /diagnostic_aggregator/analyzers/digital_io/path\n * /diagnostic_aggregator/analyzers/digital_io/timeout\n * /rosdistro\n * /robot_description\n * /diagnostic_aggregator/base_path\n * /robot_pose_ekf/freq\n * /app_manager/interface_master\n * /robot_pose_ekf/vo_used\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/analyzers/digital_io/startswith\n * /diagnostic_aggregator/analyzers/power/path\n * /robot_pose_ekf/output_frame\n * /diagnostic_aggregator/analyzers/mode/path\n * /diagnostic_aggregator/analyzers/digital_io/type\n * /diagnostic_aggregator/analyzers/mode/startswith\n * /rosversion\n * /diagnostic_aggregator/pub_rate\n * /robot_state_publisher/publish_frequency\n * /diagnostic_aggregator/analyzers/sensors/startswith\n * /diagnostic_aggregator/analyzers/power/startswith\n * /turtlebot_node/bonus\n * /robot/type\n * /robot_pose_ekf/publish_tf\n\nNODES\n  /\n    appmaster (app_manager/appmaster)\n    app_manager (app_manager/app_manager)\n    turtlebot_node (turtlebot_node/turtlebot_node.py)\n    turtlebot_laptop_battery (turtlebot_node/laptop_battery.py)\n    robot_state_publisher (robot_state_publisher/state_publisher)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n\nROS_MASTER_URI=http://192.168.1.15:11311\n\ncore service [/rosout] found\nprocess[appmaster-1]: started with pid [14445]\nprocess[app_manager-2]: started with pid [14446]\nprocess[turtlebot_node-3]: started with pid [14447]\nprocess[turtlebot_laptop_battery-4]: started with pid [14448]\nprocess[robot_state_publisher-5]: started with pid [14449]\nprocess[diagnostic_aggregator-6]: started with pid [14450]\nprocess[robot_pose_ekf-7]: started with pid [14451]\nUnhandled exception in thread started by <bound method XmlRpcNode.run of <roslib.xmlrpc.XmlRpcNode object at 0xf851d0>>\nTraceback (most recent call last):\n  File \"/opt/ros/electric/ros/core/roslib/src/roslib/xmlrpc.py\", line 194, in run\n    self._run()\n  File \"/opt/ros/electric/ros/core/roslib/src/roslib/xmlrpc.py\", line 217, in _run\n    self.server = ThreadingXMLRPCServer((bind_address, port), log_requests)\n  File \"/opt/ros/electric/ros/core/roslib/src/roslib/xmlrpc.py\", line 97, in __init__\n    SimpleXMLRPCServer.__init__(self, addr, SilenceableXMLRPCRequestHandler, log_requests)\n  File \"/usr/lib/python2.6/SimpleXMLRPCServer.py\", line 539, in __init__\n    SocketServer.TCPServer.__init__(self, addr, requestHandler, bind_and_activate)\n  File \"/usr/lib/python2.6/SocketServer.py\", line 400, in __init__\n    self.server_bind()\n  File \"/usr/lib/python2.6/SocketServer.py\", line 411, in server_bind\n    self.socket.bind(self.server_address)\n  File \"<string>\", line 1, in bind\nsocket.error: [Errno 98] Address already in use\nturtlebot_apps.installed\nloading installation data for [turtlebot_apps.installed]\n[INFO] [WallTime: 1322529211.211220] Starting app manager for turtlebot\n[INFO] [WallTime: 1322529211.301508] Waiting ..."], "url": "https://answers.ros.org/question/12127/cant-get-data-from-create-robot-error-reading-from-sci-port-ttyusb0/"},
{"title": "Kinect content missing on the Turtlebot", "time": "2011-11-23 13:31:28 -0600", "post_content": [" ", " ", " ", " ", "I'm coming back to Turtlebot after a couple of months on other things. Wow, things are looking great! It's really coming together.", "I'm having a curious problem and I hope someone can provide some guidance. This started out when I started running through the gmapping tutorial. Teleoperation was working fine, but I wasn't seeing anything coming up in rviz. So I tried subscribing to the image topics, using image_view, etc. Still no video. So now I've backed up and am trying to resolve this problem.", "After figuring out that the stuff in kinect.launch wasn't referenced in turtlebot.launch and is now brought up separately (I am lazy and created a custom launch file that includes kinect.launch), I got the openni_kinect topics to show up. The Kinect is powered on, it shows up in lsusb, etc., and the laser is merrily red. Here's my rostopic list:", "... except nothing seems to be actually publishing to the camera topics. For example, a \"rostopic hz /camera/rgb/image_color\" doesn't give me anything, but a rostopic hz /odom gives me the rate info I expect. None of the other /camera topics get anything, either; I get no response from /camera/depth/image,", "Sooo, back to rviz. If I add a display in rviz (Camera, topic /camera/rgb/image_color), I get a Status: warning message that says \"No CameraInfo received on [/camera/rgb/camera_info]. Topic may not exist. But it does exist (see above), it's just that nothing is happening on it. if I rostopic echo or rostopic hz on /camera/rgb/camera_info, I don't see anything.", "I tried running the gmapping stuff, which launches the kinect stuff ..."], "answer": [" ", " ", "You might have a look at ", " to another similar kinect related question. I dont know if the new binaries are already out, but i doubt it. Maybe you could try to build the most recent version from source like suggested and give feedback to Patrick if it helped.", "I was fighting with this issue for weeks with natty and electric and for me a clean reinstall and switch vom 32 to 64 bit helped a lot, but didnt solve the problem completely.", " ", " ", "Patrick's work, as linked in his answer above, fixed the problem."], "question_code": ["/camera/depth/camera_info\n/camera/depth/disparity\n/camera/depth/image\n/camera/depth/image/compressed\n/camera/depth/image/compressed/parameter_descriptions\n/camera/depth/image/compressed/parameter_updates\n/camera/depth/image/theora\n/camera/depth/image/theora/parameter_descriptions\n/camera/depth/image/theora/parameter_updates\n/camera/depth/image_raw\n/camera/depth/image_raw/compressed\n/camera/depth/image_raw/compressed/parameter_descriptions\n/camera/depth/image_raw/compressed/parameter_updates\n/camera/depth/image_raw/theora\n/camera/depth/image_raw/theora/parameter_descriptions\n/camera/depth/image_raw/theora/parameter_updates\n/camera/depth/points\n/camera/rgb/camera_info\n/camera/rgb/image_color\n/camera/rgb/image_color/compressed\n/camera/rgb/image_color/compressed/parameter_descriptions\n/camera/rgb/image_color/compressed/parameter_updates\n/camera/rgb/image_color/theora\n/camera/rgb/image_color/theora/parameter_descriptions\n/camera/rgb/image_color/theora/parameter_updates\n/camera/rgb/image_mono\n/camera/rgb/image_mono/compressed\n/camera/rgb/image_mono/compressed/parameter_descriptions\n/camera/rgb/image_mono/compressed/parameter_updates\n/camera/rgb/image_mono/theora\n/camera/rgb/image_mono/theora/parameter_descriptions\n/camera/rgb/image_mono/theora/parameter_updates\n/camera/rgb/image_raw\n/camera/rgb/image_raw/compressed\n/camera/rgb/image_raw/compressed/parameter_descriptions\n/camera/rgb/image_raw/compressed/parameter_updates\n/camera/rgb/image_raw/theora\n/camera/rgb/image_raw/theora/parameter_descriptions\n/camera/rgb/image_raw/theora/parameter_updates\n/camera/rgb/points\n/cloud_throttled\n/cmd_vel\n/diagnostics\n/diagnostics_agg\n/imu/data\n/imu/raw\n/joint_states\n/kinect_laser/parameter_descriptions\n/kinect_laser/parameter_updates\n/kinect_laser_narrow/parameter_descriptions\n/kinect_laser_narrow/parameter_updates\n/map\n/map_metadata\n/narrow_scan\n/odom\n/openni_camera/parameter_descriptions\n/openni_camera/parameter_updates\n/openni_manager/bond\n/robot_pose_ekf/odom\n/rosout\n/rosout_agg\n/scan\n/slam_gmapping/entropy\n/tf\n/turtlebot/app_list\n/turtlebot/application/app_status\n/turtlebot_node/parameter_descriptions\n/turtlebot_node/parameter_updates\n/turtlebot_node/sensor_state\n"], "url": "https://answers.ros.org/question/12074/kinect-content-missing-on-the-turtlebot/"},
{"title": "device_id is not set on kinect.launch for turtlebot", "time": "2011-12-02 09:22:57 -0600", "post_content": [" ", " ", " ", " ", "Turtlebot dashboard is showing no errors, the first four squares are green as they should be, and it is in full mode.  I started it up using ", " and then in a separate terminal window I issued the command ", ", only to receive a barrage of what seem to be errors:"], "answer": [" ", " ", "When you see nodes starting and dying immediately it usually means that there is another instance of the node already running. do a ps aux |grep ros and look for the process running in the background.", "Sometimes the openni_nodes don't go down cleanly just do a kill -9 on those processes. ", "Also given the above output it looks like you might not have your ROS_IP set in your environments. To check you can always open a terminal and type ", "this shows the values for all the ROS environment variables. If you ROS_IP is not set or set properly on one or more of the computers this should help you find the problem. Also you could review the networking tutorial here: "], "question_code": ["roslaunch turtlebot_bringup minimal.launch", "roslaunch turtlebot_bringup kinect.launch", "    roslaunch turtlebot_bringup kinect.launch \n... logging to /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/roslaunch-ubuntu-9087.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://172.20.110.90:59699/\n\nSUMMARY\n========\n\nPARAMETERS\n * /kinect_laser/max_height\n\n * /openni_camera/depth_rgb_rotation\n * /rosdistro\n * /openni_camera/image_mode\n * /openni_camera/image_time_offset\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /openni_camera/depth_mode\n * /kinect_laser_narrow/max_height\n * /openni_camera/depth_frame_id\n * /openni_camera/depth_time_offset\n * /kinect_laser/output_frame_id\n * /rosversion\n * /openni_camera/depth_registration\n * /openni_camera/debayering\n * /openni_camera/depth_rgb_translation\n * /kinect_laser/min_height\n * /pointcloud_throttle/max_rate\n * /openni_camera/shift_offset\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n\nROS_MASTER_URI=http://172.20.110.90:11311\n\ncore service [/rosout] found\n\nprocess[kinect_breaker_enabler-1]: started with pid [9107]\n\nprocess[openni_manager-2]: started with pid [9108]\n\nprocess[openni_camera-3]: started with pid [9109]\n\nprocess[pointcloud_throttle-4]: started with pid [9110]\n\nprocess[kinect_laser-5]: started with pid [9111]\n\nprocess[kinect_laser_narrow-6]: started with pid [9112]\n\n[rospack] opendir error [No such file or directory] while crawling /home/turtlebot/ros_workspace\n\n[kinect_breaker_enabler-1] process has finished cleanly.\n\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/kinect_breaker_enabler-1*.log\n\n[ INFO] [1322866769.985481656]: [/openni_camera] Number devices connected: 1\n\n[ INFO] [1322866769.987326721]: [/openni_camera] 1. device on bus 001:13 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'B00365619884052B'\n\n[ WARN] [1322866769.993239659]: [/openni_camera] device_id is not set! Using first device.\n\n[openni_manager-2] process has died [pid 9108, exit code -4].\nlog files: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/openni_manager-2*.log\n\nrespawning...\n\n[openni_manager-2] restarting process\nprocess[openni_manager-2]: started with pid [9354]\n\n[kinect_laser-5] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/kinect_laser-5*.log\nrespawning...\n\n[kinect_laser-5] restarting process\nprocess[kinect_laser-5]: started with pid [9423]\n[pointcloud_throttle-4] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/pointcloud_throttle-4*.log\n\nrespawning...\n\n[pointcloud_throttle-4] restarting process\nprocess[pointcloud_throttle-4]: started with pid [9575]\n\n[openni_camera-3] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/openni_camera-3*.log\n\nrespawning...\n\n[kinect_laser_narrow-6] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/kinect_laser_narrow-6*.log\n\nrespawning...\n\n[openni_camera-3] restarting process\nprocess[openni_camera-3]: started with pid [9596]\n\n[kinect_laser_narrow-6] restarting process\nprocess[kinect_laser_narrow-6]: started with pid [9597]\n\n[ INFO] [1322866793.702252097]: [/openni_camera] Number devices connected: 1\n\n[ INFO] [1322866793.703281897]: [/openni_camera] 1. device on bus 001:13 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'B00365619884052B'\n\n[ WARN] [1322866793.705789075]: [/openni_camera] device_id is not set! Using first device.\n\n[openni_manager-2] process has died [pid 9354, exit code -4].\nlog files: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/openni_manager-2*.log\n\nrespawning...\n\n[openni_manager-2] restarting process\nprocess[openni_manager-2]: started with pid [9648]\n\n[kinect_laser-5] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/62dbe61c-1d28-11e1-bb3f-0012f0704d87/kinect_laser-5*.log\n\nrespawning...\n\n[kinect_laser-5] restarting process\nprocess[kinect_laser-5]: started with pid [9704]\n\n[pointcloud_throttle-4] process ..."], "answer_code": ["env|grep ROS\n"], "url": "https://answers.ros.org/question/12195/device_id-is-not-set-on-kinectlaunch-for-turtlebot/"},
{"title": "arm_navigation package (lack of) scaling issue from urdf files", "time": "2011-12-13 02:18:12 -0600", "post_content": [" ", " ", "Hi,\nI tried getting the arm_navigation tool running with my custom (non-PR2) robot .urdf file and had some issues.  The biggest (literally) issue was that my .urdf file uses .stl mesh files that are in inches that are manually scaled to meters. For example:", "<mesh filename=\"package://left_arm.stl\" scale=\".0254 .0254 .0254\"/>", "0.0254, being the inches-to-meters scale factor.", "Now this all works fine in rviz and gazebo and whatever, but when I run the arm_navigation stack, things go a little crazy---at least in the display.  I think the motion planner pieces are working fine, but if you run the planning_components_visualizer tool or the initial wizard to set things up, the green and pink overlays are at the original scale (and thus gigantic).   ", "Anybody seen, or aware of this?", "Thanks,\nSteve"], "answer": [" ", " ", " ", " ", "Yeah, I encountered the same problem. ", " ", " ", "I am having this same issue. I had to convert my model from m to mm using scale:", "Any progress on this bug fix? Thanks!", " ", " ", "The scale is being correctly applied for collision checking purposes - and thus all your arm navigation components should work correctly.  The scale is not being applied to the markers that are sent to rviz, thus making things seem giant.  We'll release a bug fix into electric soon."], "answer_code": ["scale=\".001 .001 .001\"\n"], "url": "https://answers.ros.org/question/12324/arm_navigation-package-lack-of-scaling-issue-from-urdf-files/"},
{"title": "ardrone not responding to the image tags.", "time": "2012-01-14 08:58:30 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I ran some tests on the ardrone to identify the tags .The camera of the ardrone quit well recognizes the tags.This was evident since there was a tag identified on the image window outlining the tag with a green line.I have some pictures of tag identification,since I have only one karma,I am not able to upload.", "These are the commands which are executed for the tag identification and the flight of the ardrone.", "rosrun ardrone_brown ardrone_driver  //synching the computer with the ardrone wifi and acquiring connection", "rosrun ar_recog ar_recog image:=/ardrone/image_raw   //directs the camera feed of the ardrone      to the ar_ecog", "rosrun image_view image_view image:=/ar/image    //this windows shows the image window of the ardrone which is monitored by ar_recog .It is able to recognize image tags and outline it in green outline", "rosrun nolan3d nolan3d.py  //this is the target follower which indeed communicates with the ardrone driver.here is the topic info ", "rosrun drone_teleop drone_teleop.py(using this I set the ardrone on flight)", "This is where I discovered the problem.I held the tag in an optimal distance in-front of the drone.The tag I held was a big one ( 8.5 inches x 11 inches).But the drone does not seem to fly to it and neither align itself to the tag when held close it .", "any suggestions please.. hope the question is more informative now..", "Here is what I did. I made sure the drone was on air. Then I started the node Nolan3D package  by running the python program in its bin.Then I held the image tag.The drone did not fly to me and even when the image tag was close , it was not able to align its movement according to the tag. ", "Any suggestions please."], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "It looks like the drone_teleop node is continuously publishing to cmd_vel, which is probably overriding any commands sent by the nolan3d node. Try stopping the drone_teleop node once your AR drone is in the air.", " ", " ", "When you hold up the ar-tag, try to not place your fingers on the surface of the tag. I've noticed that the ar-recog sometimes fails to recognize the tag if there are fingers on the surface. This is just a tip in addition to the solution proposed by \"tjay\" above."], "question_code": ["metallo@ubuntu:~$ rosnode info /nolan_3014_1326654517968 \n--------------------------------------------------------------------------------\nNode [/nolan_3014_1326654517968]\nPublications: \n * /cmd_vel [geometry_msgs/Twist]\n * /rosout [rosgraph_msgs/Log]\n\nSubscriptions: \n * /tags [ar_recog/Tags]\n\nServices: None\n\ncontacting node http://ubuntu:60922/ ...\nPid: 3014\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /cmd_vel\n    * to: /ardrone_driver            //note here it communicates with the ardrone driver\n    * direction: outbound\n    * transport: TCPROS\n * topic: /tags\n    * to: http://ubuntu:58952/\n    * direction: inbound\n    * transport: TCPROS\n"], "url": "https://answers.ros.org/question/12637/ardrone-not-responding-to-the-image-tags/"},
{"title": "What am I doing wrong? Gmapping on Turtlebot Clone. I've pictures!", "time": "2012-02-06 22:26:41 -0600", "post_content": [" ", " ", "Hello all,", "I've been scratching my head over this for a while now, and I would appreciate an explanation. If some of the angels on top high (like Melonee) could just give me a straightforward answer, it would be awesome.", "Basically, I have been putting together my Turtlebot Clone over the past few weeks. Gotten around to doing some mapping recently, I am just using the standard demo as per suggested by the tutorials. However, I get errors, and mutated maps that even their own mothers couldn't love.", "So, I have taken some screenshots to give you a step by step of what I am doing.", "I start off with the above. I have my robot model, laser, map and camera all good to go. I have ran the gmapping launch file. You can see that the first parts of the occupancy grid is filling in. I have not moved the robot yet. There are no errors and we are all green on the dashboard.", "Now, I have moved the turtlebot slightly to the left. The occupancy grid expands, however errors start to appear from the dashboard. What do those errors look like you might ask? I'll show you.", "It is damn ugly I know. If I continue going around the environment, I will keep getting the error where the transform from base_link to odom fails. The only thing I touch in relation to launch files etc, is the minimal.launch. I don't use startup because teleop would not run when I did. I did the usual gyro config beforehand, and instead of putting the parameters into turtlebot.launch, I place them in minimal.launch. From what I can figure this should work fine. Correct me if I am wrong.", "Now, if I continue around the map. I get another flavor of error. This time this is to do with a loop rate that isn't being met. It is usually just shy of it. ", "Now, if I continue on my merry way around the simple rectangular arena you see that obviously something is wrong. The two differences I can see from anyone else is how I do the bringup with the calibration values in the minimal.launch instead of the turtlebot.launch and I have a weaker netbook than what is suggest. If I had to put my money on it, I would say that it is the netbook not being fast enough.", "You can look at the specs of the netbook I bought here.", "My hands were a little tied with the choice, so this is why I didn't get something more powerful.", "Again, I would really and truly like some feedback on what I might be doing wrong here, so I can zero in on the problem faster.", "Thank you in advance."], "answer": [" ", " ", " ", " ", "To me it looks like something is wrong with your odometry or with the laser data that is coming in You could check the following:"], "answer_details": ["Is odometry running and publishing sane values (check with ", "). Move around and verify that odometry is changing.", "Check if the odom transform is coming in (", "). Try setting the fixed frame in rviz to odom and move the robot around. Verify that the robot is moving correctly in rviz, i.e. when you move one meter forward, it should also be approximately one meter in rviz (the grid display helps here).", " ", " ", " ", " "], "answer_code": ["rostopic echo /odom", "rosrun tf tf_echo odom base_footprint"], "url": "https://answers.ros.org/question/12915/what-am-i-doing-wrong-gmapping-on-turtlebot-clone-ive-pictures/"},
{"title": "How to use the robot_mechanism_controller for manipulator other than PR2 [closed]", "time": "2012-02-21 14:06:41 -0600", "post_content": [" ", " ", " ", " ", "Hi, all,", "I'm trying to make use of the ", " for the schunk powercube manipulator, because I see it has pretty good functionality like spline controller and velocity controller. What I have is the ROS node of the driver of the manipulator ", ", subscribing to the velocity command. I also have set up the motion planner and follow_joint_trajectory action interface using the planning description configuration wizard. I want to make some functionalities like trajectory following and find out that the ", " has a lot of attractive features. But I didn't find out any tutorials about how to use this package. Could anyone me any hint how to use that package? Thanks a lot."], "answer": [], "url": "https://answers.ros.org/question/28116/how-to-use-the-robot_mechanism_controller-for-manipulator-other-than-pr2/"},
{"title": "Odometry Information Source", "time": "2012-01-24 00:14:35 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm trying to generate odometry information following ", ". I am using a robot like ", " - build with phidgets and with a Kinect mounted.", "My problem is that I don't have a speed-source for the robot. It does not have a stepper motor or encoder and the actual speed depends on many factors (laptop weight, floor type, battery state, ...). Seems like ", " is my best shot for now (Obtaining nav_msgs/Odometry from a laser_scan (eg. with laser_scan_matcher)). Is ", " worth trying with kinect? Is there any other possibility? Am I missing something?", ":\nit's quite pointless. I can say that the robot moves with fixed speed x and it kind of works but the next issue is the yaw angle - I don't have a gyro and laser_scan_matcher estimations are not very accurate.", "For me it means: gmapping won't work without a good odometry source."], "answer": [" ", " ", "I believe that ", " is what you're looking for. You can then use laser_scan_matcher to compute odometry information. ", "The scan matcher algorithm is inherently discrete.  Whatever you do you will need to therefore differentiate the signal.  Thus you could simply write a node which computes the velocity based on the sequence of pose messages.  Then you don't have to modify the node. ", "yes but in my case it does not work too good (movement in a corridor is not being recognized with laser_scan_matcher only). if anyone is interested, tom did some modifications already, links are here: ", "If you still need it we have a node to transform several kind of messages to odometry ", " In this case would be \"pose2d_to_odom\""], "url": "https://answers.ros.org/question/12732/odometry-information-source/"},
{"title": "Loading the ROS Sketch onto the Arbotix board [closed]", "time": "2012-02-07 05:34:34 -0600", "post_content": [" ", " ", " ", " ", "I am trying to load the ros.pde sketch onto the ArbotiX board for use with the turtlebot arm, and I keep getting an error when trying to upload the sketch.  The error that I am getting is:", "avrdude: stk500_recv(): programmer is not responding", "My boards.txt file contains this segment:", "The board is powered on and connected to the computer using the FTDI USB-TTY cable listed here:  ", "I tried the FTDI Sparkfun breakout board with a normal mini usb cable and it crashes my arduino ide with the error /dev/USBTTY1 No such file or folder.", "I am running arduino 0018 (as per the suggestion on the arbotix site), which I downloaded directly off of the arduino site (not through apt-get).", "Does anyone have an idea of why I am having problems uploading this sketch?  Thank you very much!"], "answer": [" ", " ", "You are using an FTDI adapter, not an ISP, so you should not have uncommented the .upload.using= part.", "If you have an error about a port not being present, you need to change the port selection within the Arduino IDE (found in the Menu under Tools).", " ", " ", "If too many interrupts are present in the system, you could get this error. I usually got them when I \"overused\" the microcontroller on my Arduino with complicated code."], "question_code": ["# To use an ISP: uncomment the next line, comment out the protocol line\n#arbotix.upload.using=avrispmkii\narbotix.upload.protocol=stk500\n#arbotix.upload.protocol=arduino\n"], "url": "https://answers.ros.org/question/12887/loading-the-ros-sketch-onto-the-arbotix-board/"},
{"title": "turtlebot_calibration isn't working as expected", "time": "2012-04-01 23:44:35 -0600", "post_content": [" ", " ", " ", " ", "My robot isn't a turtlebot, but it's very close / uses many turtlebot parts.", "I'm trying to run the turtlebot calibration routine. On the first step, it rotates 360 degrees. On the second, it only makes it half way, 180 degrees.", "Alternativly, I can set odom_angular_scale_correction so that it goes 2 full circles the first step and the other speeds only go one.", "Any idea what would cause the lower speed s the higher speeds to be completely off?", "[edit] I replaced the create and the power/gyro board without and changes, so it's probably sotware [/edit]", "Dropping the update_rate of turtlebot_node to 10 instead of 30 made it perform a lot better in the calibration, but the rotation still seems way off when I actually drive the robot around.. "], "answer": [" ", " ", "It is supposed to turn 720 the first time then 360 the following times.  "], "url": "https://answers.ros.org/question/30990/turtlebot_calibration-isnt-working-as-expected/"},
{"title": "Starting p3dx using p2os", "time": "2011-12-09 05:43:31 -0600", "post_content": [" ", " ", "Hi,\nI am following the tutorials in ", "I have only one laptop mounted on the robot (p3dx) hence i am not following the 7th and 8th section of the above mentioned tutorial.", "When i run the ", "i see the dashboard getting displayed but except the Rosout (in green) ui others are not colored (red/green) and the battery is 0% even though the robot is fully charged. I am not able to enable or disable the motors.\nI see the problem is here itself.", "And when i run the ", "i get the following errors.", "I have given the read and write permissions to ttyS* and ttyUSB*.\nI am able to use ROSARIA and get the robot moving using keyboard operations.\nI am sure something is being missed. Pls let me know", "Thanks,\nKarthik"], "answer": [" ", " ", " ", " ", "Ok found the issue.\nHad to remap the param name port to /dev/ttyUSB0\nIts working now", "Thanks,\nKarthik", "You may update the param at ./p2os_driver/include/p2os_driver/robot_params.h but beware that the sick laser usage might have a conflict at this port. You may have it as a param in the launch file.", "Hi Karthik,\nI can connect my robot with laptop by using serial-usb cable. I'm trying to control robot using keyboard. \nwhen i run $rosrun p2os_dashboard p2os_dashboard i get ImportError: No module named msg. Can you please help me out.", "I am not sure of it! Can you post it as a new question!", "Thanks. i managed to get it done using rosAria"], "question_code": ["rosrun p2os_dashboard p2os_dashboard\n", "rosrun p2os_drivers p2os\n", "[ INFO] [1323459143.327099680]: using serial port: [/dev/ttyS0]\n[ INFO] [1323459143.360479690]: P2OS connection opening serial port /dev/ttyS0...\n[ERROR] [1323459143.360617485]: P2OS::Setup():tcgetattr():\n[ERROR] [1323459143.360707540]: p2os setup failed...\n"], "url": "https://answers.ros.org/question/12294/starting-p3dx-using-p2os/"},
{"title": "Changing where logging occurs from within roslaunch", "time": "2012-02-23 05:45:54 -0600", "post_content": [" ", " ", "Hello,", "I currently have a robot that, when it powers on, runs a .launch file that launches all of my nodes. It also logs any errors or warnings that occur through rosout, as it likely should. Unfortunately, if an error does occur, or the robot has to shutdown, it is difficult to find the log files pertaining to those errors. My question is twofold: ", ". Is it possible to change the logging directory from within roslaunch. I have tried changing the environment variable as so:", "< launch> ", "< env name=\"ROS_LOG_DIR\" value=\"$(find OryxManager)/logs\" />", "....NODES HERE.... ", "< /launch>", "But it still logs them to .ros/log. Any suggestions?", ". Is it possible to change which sub-folder the logs are stored in? Currently, it is based off of the run_id. Unfortunately, the run_id is a random string, and it is difficult to tell what the previous run_id if robot powered off. Would it be possible to either change the run_id, or change the logging folder name, to be something else. Bonus points if there is a way to name it based off of something more intuitive like the system time.", "If there isn't any way to do this, we may have to write our own method of logging. This would be possible, but not ideal. I would greatly appreciate any advice you can provide.", "Thank you,", "-Joe"], "answer": [" ", " ", "roslaunch starts logging before it interprets the launch file, so setting ROS_LOG_DIR in the file won't do anything.  You have to set the variable prior to launching.  If you want to change how the folders are organized, simply write a wrapper script that sets ROS_LOG_DIR to your preferred organizational scheme prior to invoking roslaunch."], "url": "https://answers.ros.org/question/28304/changing-where-logging-occurs-from-within-roslaunch/"},
{"title": "ros cluster", "time": "2012-02-27 01:11:10 -0600", "post_content": [" ", " ", " ", " ", "At the fundamental level, can it be said that ROS is an efficient message passing system, since essentially, that is what it is doing and managing for the most part.. If that is the case, is it possible to use ROS for creating a powerful computing cluster instead of using it only for robotics?"], "answer": [" ", " ", "There are other, non-robotics-focused, ways of doing clusters; if all you need is message passing, you might want ", "; if you want to think in terms of computational units, not messages, you want ", ".", "So yes, it will work, but you should start with high-level thinking about the problem you're solving. ", "I am merely looking for a generic cluster that allows me to model problems as computational units that can run in parallel. They may or may not have huge data sets. Do you think hadoop would help in this case? I always thought it was meant for processing huge data sets in parallel.", "I would say that small datasets are a subset of large datasets. Some of Hadoop's stuff may be overkill, but it could be the right way to start. ", " ", " ", "I would say this is definitively possible to parallelize algorithms using ROS. In particular, the fact each node runs into a separate process allows to run several times the same node to realize the same treatment on different piece of data.", "On the opposite, depending on your needs, your cluster architecture, etc. there is open source alternatives that may be more efficient and easier to use. You may also hit bottleneck that affects nobody else or reveal new bugs, etc.", "In short: yes you can. Depending on what you want to achieve at the end, it may be a good idea or a very bad one :)"], "url": "https://answers.ros.org/question/28603/ros-cluster/"},
{"title": "Simulated turtlebot dashboard problem", "time": "2012-02-26 08:41:23 -0600", "post_content": [" ", " ", "I'm new with ROS and Turtlebot. So I have very basic question. Should turtlebot dashboard work in simulated turtlebot. Because when I run:", "Dashboard opens, but only the middle button under diagnostic is green. Others are gray. Also when I click on the \"wrench\" under Diagnostic I get info that everything is in Stale. Is this normal in simulated environment?", "Thank you for your help!"], "answer": [" ", " ", "The dashboard is not relevant for the simulation because we do not full simulate all of the physical hardware. You should not need the dashboard to use the TurtleBot simulator. ", "Thank you for the tip. May I ask you something about the simulated kinect? Is it possible to get simulated raw image or create Point Cloud in Rviz with simulated Kinect \"out of the box\"? Because I can only get laser data posted on scan topic, but can't build Point Cloud."], "question_code": ["rosrun turtlebot_dashboard turtlebot_dashboard\n"], "url": "https://answers.ros.org/question/28556/simulated-turtlebot-dashboard-problem/"},
{"title": "Turtlebot cannot running .. [closed]", "time": "2012-03-21 18:37:58 -0600", "post_content": [" ", " ", " ", " ", "please help me..", "when i enter the command \"sudo service turtlebot start\", i can see the message \"turtlebot start/running, process 3198\"", "but!!!", "when i enter the command \"sudo service turtlebot start\" again, i see the message \"turtlebot start/running, process 4125\"", "i think, it's wrong.. and..", "when i enter the command \"rostopic list\", i can see just \"/rosout\" and \"/rosout_agg\".", "why i cannot see turtlebot list?", "ROS_MASTER_URI and ROS_HOSTNAME is all right..", "please help me.. ", "did you press the power button? Please start the dashboard on your workstation computer and see if any errors are showing", "What is the output of 'ps auxf'?"], "answer": [], "url": "https://answers.ros.org/question/30198/turtlebot-cannot-running/"},
{"title": "how to stop the robot if something goes wrong", "time": "2012-03-29 23:00:56 -0600", "post_content": [" ", " ", "I am trying to setup a diagnostic mechanism on my robot so that it will stop if something goes wrong: a sensor stops working, the TF tree becomes too old, some critical node dies, etc.", "I am wondering what's the best practice for that?", "I am considering using the diagnostics tools. The monitors seem to be written to inform the operator only, and not take action. Maybe I could write my own node, based on robot_monitor, that would command the motors to stop whenever a diagnostic becomes ERROR...", "Also, how to detect that a node died? In some cases, subscribers can report that a publisher is dead if no data was received for some time. But there are other scenarios (A node that does not publish periodically, etc.)", "Finally, how to monitor the TF tree? My robot has 2 computers that tend to get out of sync (although I am using NTP to keep their clock synchronized ...) and at times this causes problems that lead the robot to the wall. How could I monitor that, besides periodically testing transforms and catching exceptions (ExtrapolationException ?) ?", "For the action part, I am first going to have a node that will stop the motors if something goes wrong, but later I will include a hardware controller that will cut the power and brake if a heart beat signal stops (watchdog)."], "answer": [" ", " ", "Regarding old data, tf, etc. I would say the usually nobody should work on old data. E.g. if you laser data is 5s old, you won't declare the space free and move there.", "I think you are aiming for something more generic, where some other node watches such critical things. I agree with your understanding of diagnostics to be only for monitoring.", "For what you are planning to do I would probably use a simple node that can detect different failure conditions that would need to be defined.", "Diagnostics being an error is quite simple. Regarding tf and nodes sending data in general there is no real generic way as it depends on what is supposed to happen. Maybe some tf is only sent if available and thus it is absolutely normal to not receive tf. Others should always come with 10Hz. Such frequency things can also be monitored using diagnostics in the sender.", "Checking a node died should be possible using rosnode list or the master api or in the simplest case by checking the binary is running. Unfortunately those are two different things. You can have a binary running that is not connected to ROS and you can have a ros node connected to others that the master doesn't know (e.g. by restarting the master). Both aren't desired."], "url": "https://answers.ros.org/question/30846/how-to-stop-the-robot-if-something-goes-wrong/"},
{"title": "Possible bug in rviz", "time": "2012-02-25 13:48:13 -0600", "post_content": [" ", " ", " ", " ", "I got the following message when I try to run rviz through urdf_tutorials:", "The log says", "The most strange of all that is that it runs perfectly when I first run it (sometimes it works when I wait a long time before running again or lock the screen after the error but the ruslaunch command still executing). If I close and try to reopen, it gives that error. I checked and there is no program running in background that could cause some problem. I'm using Ubuntu 11.10. I'll keep searching for the source of the problem, as it seems related to the X library itself.", "Please post what type of GPU you have and which drivers you are using (e.g. a restricted driver?).", "I'm in a notebook without any GPU. I'm using a default Ubuntu 11.10 installation.", "@Conrado: Even if you have no dedicated powerful graphics card your GPU is from either Intel, Nvidia or AMD/ATI. Check the \"Additional Drivers\" in the settings if you have alternatives available and try them."], "answer": [" ", " ", "This BadDrawable error is generally related to issues with either your GPU being too old or buggy drivers. See the answers to ", " and ", ". Also see the ", ".", "Since you are only having problems after closing it, I would guess something is leaving your graphics driver in a bad state. Try changing up the RTT mode as referenced in the above answers to see if that fixes things."], "question_code": ["(rviz:3447): Gtk-WARNING **: Unable to locate theme engine in module_path: \"pixmap\",\n\n(rviz:3447): Gtk-WARNING **: Unable to locate theme engine in module_path: \"pixmap\",\n\n(rviz:3447): Gtk-WARNING **: Unable to locate theme engine in module_path: \"pixmap\",\n\n(rviz:3447): Gtk-WARNING **: Unable to locate theme engine in module_path: \"pixmap\",\nThe program 'rviz' received an X Window System error.\nThis probably reflects a bug in the program.\nThe error was 'BadDrawable (invalid Pixmap or Window parameter)'.\n  (Details: serial 22 error_code 9 request_code 137 minor_code 3)\n  (Note to programmers: normally, X errors are reported asynchronously;\n   that is, you will receive the error a while after causing it.\n   To debug your program, run it with the --sync command line\n   option to change this behavior. You can then get a meaningful\n   backtrace from your debugger if you break on the gdk_x_error() function.)\n", "[ INFO] [1330220460.687401287]: rviz revision number 1.6.7\n[ INFO] [1330220460.687490754]: ogre_tools revision number 1.6.2\n[ INFO] [1330220460.687512474]: compiled against OGRE version 1.7.3 (Cthugha)\n[ INFO] [1330220460.790543031]: Loading general config from [/home/conrado/.rviz/config]\n[ INFO] [1330220460.791031850]: Loading display config from [/home/conrado/.rviz/display_config]\n[ INFO] [1330220460.859120679]: RTT Preferred Mode is PBuffer.\n"], "url": "https://answers.ros.org/question/28507/possible-bug-in-rviz/"},
{"title": "split openni_launch for resource-constrained systems [closed]", "time": "2012-05-07 06:38:49 -0600", "post_content": [" ", " ", " ", " ", "What is the best way to split openni_launch into some modules running on a (resource-constrained) robot with an Asus Xtion Pro Live sensor providing raw data, and processing modules running on a (powerful) remote PC, performing registration and point cloud construction?"], "answer": [], "url": "https://answers.ros.org/question/33450/split-openni_launch-for-resource-constrained-systems/"},
{"title": "Rviz Turtlebot Roomba SLAM error [closed]", "time": "2012-05-09 06:40:52 -0600", "post_content": [" ", " ", " ", " ", "Dear all, ", "I am using a roomba 500 series model with kinect (Turtlebot.eu version),in order to perform SLAM with ROS-electric and Linux 11.10 oneiric on both workstation-pc and turtlebot-laptop.", "While I follow the tutorial in ROS.org, I get the following error in rviz under laser scanner parameter:", "Transform [sender=/openni_manager] For frame [/camera_depth_frame]: Frame [/camera_depth_frame] does not exist.  ", "I tried to change base and fixed frame values to something else, but again no luck.", "Can you please let me know if anyone here faced the same error before?", "thanks, ", "cg", "P.S.: My camera node runs with no problem, and Kinect is powered up and normally.\nI have no problem at all to run the turtlebot follower tutorial, which makes use of Kinect and Roomba rover, runs smoothly.", "Tell us about the state of your TF tree; "], "answer": [], "question_code": ["rosrun tf view_frames"], "url": "https://answers.ros.org/question/33623/rviz-turtlebot-roomba-slam-error/"},
{"title": "Errors in Minimal.launch after installing Fuerte Turtlebot Upgrade", "time": "2012-05-07 18:19:26 -0600", "post_content": [" ", " ", " ", " ", "On Ubuntu Oneiric, following upgrade instructions to Fuerte Turtlebot, ", " roscore started Ok  with \"minimal.launch\" though with several ROS error msgs in addition to a Lenovo x120e \"battery\" issue I reported in an earlier post:", "core service [/rosout] found\nprocess[appmaster-1]: started with pid [14537]\nprocess[app_manager-2]: started with pid [14538]\nprocess[turtlebot_node-3]: started with pid [14539]\nprocess[turtlebot_laptop_battery-4]: started with pid [14541]\nprocess[robot_state_publisher-5]: started with pid [14543]\nprocess[diagnostic_aggregator-6]: started with pid [14548]", "/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/nodes/turtlebot_node.py:54: UserWarning: roslib.rosenv is deprecated, please use rospkg or rosgraph.rosenv\n  import roslib.rosenv", "process[robot_pose_ekf-7]: started with pid [14581]", "turtlebot_apps.installed\nloading installation data for [turtlebot_apps.installed]", "[WARN] [WallTime: 1336438419.701618] Unable to check laptop battery info. Exception: [Errno 2] No such file or directory: '/proc/acpi/battery/BAT0/info'\n[WARN] [WallTime: 1336438419.702619] Unable to check laptop battery state. Exception: [Errno 2] No such file or directory: '/proc/acpi/battery/BAT0/state'", "ERROR: Cannot locate app file for turtlebot_teleop/android_teleop: package is not installed.", "[INFO] [WallTime: 1336438419.739866] Starting app manager for turtlebot\nTraceback (most recent call last):\n  File \"/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/nodes/turtlebot_node.py\", line 68, in <module>\n    import turtlebot_node.robot_types as robot_types", "File \"/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/src/turtlebot_node/robot_types.py\", line 29, in <module>\n    import create_sensor_handler", "File \"/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/src/turtlebot_node/create_sensor_handler.py\", line 42, in <module>\n    _struct_I = roslib.message.struct_I", "AttributeError: 'module' object has no attribute 'struct_I'", "[INFO] [WallTime: 1336438419.766782] Waiting for foreign master [", "] to come up...\n[INFO] [WallTime: 1336438419.772081] Foreign master is available", "[INFO] [WallTime: 1336438419.809062] Registering (/turtlebot/app_list,", "/) on master ", "[INFO] [WallTime: 1336438419.821318] Registering (/turtlebot/application/app_status,", "/) on master ", "\nturtlebot_apps.installed\nloading installation data for [turtlebot_apps.installed]", "Suggestions to troubleshoot?", "I'm seeing similar problems after updating to Fuerte and at the same time updating to ubuntu precise."], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "This it is a defect (see ticket for patch or better wait till its merged into the trunk):", "p.s.: the fixes I applied:", "-> add /opt/ros/fuerte/share to ROS_PACKAGE_PATH ", "-> get fix from ", "\n   in /opt/ros/fuerte/stacks/turtlebot execute sudo patch -p1 < PathToDiffFix", "I also applied ", " to fix the install.bash and ", " to fix kinect.launch", "For turtlebot.eu fellows: Read \n", "You have to change the turtlebot_node parameters robot_type publish_tf has_gyro", " ", " ", "suggests this might have to do with your python path (same struct_I issue). ", "\nYou could try \necho $PYTHONPATH\nand see if there are old references to electric in there"], "answer_code": ["rospkg.common.ResourceNotFound: rostest\nROS path [0]=/opt/ros/fuerte/share/ros\n", "  File \"/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/src/turtlebot_node/create_sensor_handler.py\", line 42, in <module>\n    _struct_I = roslib.message.struct_I\nAttributeError: 'module' object has no attribute 'struct_I'\n"], "url": "https://answers.ros.org/question/33494/errors-in-minimallaunch-after-installing-fuerte-turtlebot-upgrade/"},
{"title": "TF handling frame uncertainty?", "time": "2012-03-26 07:54:16 -0600", "post_content": [" ", " ", "TF currently do not handles uncertainty. It would be interesting to have a TF-like system with uncertainty handling capacities. Sensors imprecision and moving coordinate frames can end up in an incoherent TF-Tree. This can be problematic specially when the coordinate errors frames are chained. It is planned to develop a future system like this?", "A TF system with uncertainty handling capacities would be much more powerful system an even would allow a transformation graph instead of a transformation tree. Bayesian fusion methods would make possible to get the most probable frame given several estimations. ", "From this discussion (", ") I see that some tools like the MRPT (Mobile Robot Development Toolkit) provide interesting functionality to propagate uncertainty in a chain of coordinate frames. Look at this ", "Merging multiple estimation of the state of a frame, the time and some dynamical information of moving frame looks promising. "], "answer": [" ", " ", "You can find my first prototype for a tf extension allowing to define uncertainty in tf frames can be found at ", ". So far it supports sampling through the tf chain where each frame might or might not have a covariance attached. Additionally one can sample over time (such as when the precise time at which sensor data was captured is not known and one wants to know how this might affect the data).\nOne can also calculate a covariance for a set of transforms, either to get a different look at generated sample sets, or simply to know what to best put into the covariance matrices to represent the uncertainty experienced from empiric data.", "I plan to add using the possibility to use sample set to start from (such as when you have data and want to transform it to another frame, including uncertainty on the chain) and also to add another set of interfaces that would use the unscented transformation instead of sampling (might be less accurate but faster).", "I stepped away from the idea to directly use child frames as sigma points as covariances should be fine to define the uncertainty locally in a frame. The usage of euler angles might have some mathematical downsides but it seems to be widely used in practice, such as in tolerance calculations in mechanical engineering etc. I am currently looking into some alternatives but most people tell me it's fine like it is for practical purposes.", "Would be great to get some feedback, both on the usability of the package and the documentation. ", "Regarding fusion of data, i guess i will not get to that too soon, so if somebody has something to bring in and share, i would love to help with the integration. As a first step, we might want to address filtering over time. ", "Nice. A particle-based representation is a good approach, specially for complex pose distribution when the error in covariances is high. In any case I think it would be nice to have a \"more efficient\" uncertainty pose representation (using the sigma points or a cov matrix).", " ", " ", " ", " ", "I have some approach but didn't get to implement it yet.\nThe idea is to store the pose uncertainty similar to a Sigma-Point Approximation in tf child-frames of the frame that is affected by uncertainty.\nOne could then still use the normal tf listeners etc, giving him the most probable coordinates, as the main frame would represent the mean of the pose distribution.\nI would then implement a new class inheriting from the tf listener that when queried checks for the respective 'uncertainty' child frames along the chain connecting the queried frames and handles the uncertainty propagation.\nThe listener would then return the transforms as usual (the mean) and additionally the uncertainty as a covariance matrix or such.\nI guess we could do this based on the pose_cov_ops package?\nLeaving out loop closures, it shouldn't be too hard. TF was not meant to handle circular frame connections afaik.", "Great idea. This is IHMO a very interesting first approach. It'd maintain backwards compatibility with the implemented software. In any case I still think that a tf unified tfMessage with uncertainty information would be better. I also agree on leaving out the loop clousures as future improvements."], "url": "https://answers.ros.org/question/30498/tf-handling-frame-uncertainty/"},
{"title": "Stereo Camera syncronization", "time": "2012-04-29 03:38:50 -0600", "post_content": [" ", " ", " ", " ", "Hi all,\nI'm using the uvc_camera package and its stereo_node to grab frames from my 2 webcams.\nNow I'm trying to have a measure of the sync error of the 2 frames doing the following:\n1) write informations about left and right camera in 2 files", "2) compare timestamps field inside left.csv and right.csv", "With my big surprise, timestamps are always the same for all frames couples (30000+ couples, 15fps), saying that there's no sync error. I think that this result is quite impossible because timestamps are taken with a precision of 1 nanosecond.", "Now the question is: does uvc_camera has some magical power that I don't know or something else happened?", "Is there another way to have a precise mesure of the sync error?", "Thanks,\nFabio.", "Interesting question. Although timestamps are represented using nanosecond precision, actual clocks are not that accurate. I would not expect USB cameras to be synchronized.", "On a 1394 bus, some camera models support \"bus synchronization\" with accuracy of about 125 microseconds. Does USB have any comparable feature?", "I think that \"bus synchronization\" is a special feature of IEEE1394 and probably it's not available with USB.\nThe uvc_camera \"includes a two-camera node that provides rough synchronization for stereo vision\" so the frame synchronization is done through software.", "Looking at the frames I can't say that they aren't synchronized... by the way I'm expecting a sync error about some microseconds.", " USB Video Class doesn't have that feature, nor does it have triggering."], "answer": [" ", " ", "stereo_node grabs the latest image from each camera and then stamps both images with the current ROS time. That timestamp won't be all that close to the true capture time for either camera, as it's generated after the camera-to-computer transfer and the images may be delayed due to buffering or other processes.", "I don't think there was any way to extract better timestamps from the Video4Linux driver; I'd like to at least get a timestamp that indicates when the computer finished receiving the image. The USB camera protocol does define some timing information, though, so I believe the information is there on some level.", " ", " ", " ", " ", "I've looked at the code and I've seen that frames are grabbed sequentially but timestamps are taken only after the overall grab operation. I think that should be better to set left timestamp just after the left frame grab and the right one after the right grab.\nThis can cause a small overhead because timestams are set also for discarded frames.", "UPDATE:\nI'm going on studying the synchronization error. I've modified the code of the stereo_node in order to take and publish real timestamps for the left and the right frame grabbing.\nLet's look at the code of /uvc_camera/src/stereo.cpp", "I've modified it setting two different timestamps getting very strange results. I'm using the following code:", "\"Now I'm going to organize data and tomorrow I'll write them here.\" Do you have some results already?"], "question_code": ["rostopic echo -p /left/camera_info > left.csv\nrostopic echo -p /right/camera_info > right.csv\n"], "answer_code": ["void StereoCamera::sendInfo(ros::Time time) {\n  CameraInfoPtr info_left(new CameraInfo(left_info_mgr.getCameraInfo()));\n  CameraInfoPtr info_right(new CameraInfo(right_info_mgr.getCameraInfo()));\n\n  info_left->header.stamp = time;\n  info_right->header.stamp = time;\n  info_left->header.frame_id = frame;\n  info_right->header.frame_id = frame;\n\n  left_info_pub.publish(info_left);\n  right_info_pub.publish(info_right);\n}\n\n\nvoid StereoCamera::feedImages() {\n  unsigned int pair_id = 0;\n  while (ok) {\n    unsigned char *frame_left = NULL, *frame_right = NULL;\n    uint32_t bytes_used_left, bytes_used_right;\n\n    ros::Time capture_time = ros::Time::now();\n\n    int left_idx = cam_left->grab(&frame_left, bytes_used_left);\n    int right_idx = cam_right->grab(&frame_right, bytes_used_right);\n\n    /* Read in every frame the camera generates, but only send each\n     * (skip_frames + 1)th frame. This reduces effective frame\n     * rate, processing time and network usage while keeping the\n     * images synchronized within the true framerate.\n     */\n    if (skip_frames == 0 || frames_to_skip == 0) {\n      if (frame_left && frame_right) {\n    ImagePtr image_left(new Image);\n    ImagePtr image_right(new Image);\n\n    image_left->height = height;\n    image_left->width = width;\n    image_left->step = 3 * width;\n    image_left->encoding = image_encodings::RGB8;\n    image_left->header.stamp = capture_time;\n    image_left->header.seq = pair_id;\n\n    image_right->height = height;\n    image_right->width = width;\n    image_right->step = 3 * width;\n    image_right->encoding = image_encodings::RGB8;\n    image_right->header.stamp = capture_time;\n    image_right->header.seq = pair_id;\n\n    image_left->header.frame_id = frame;\n    image_right->header.frame_id = frame;\n\n    image_left->data.resize(image_left->step * image_left->height);\n    image_right->data.resize(image_right->step * image_right->height);\n\n    if (rotate_left)\n      rotate(&image_left->data[0], frame_left, width * height);\n    else\n      memcpy(&image_left->data[0], frame_left, width * height * 3);\n\n    if (rotate_right)\n      rotate(&image_right->data[0], frame_right, width * height);\n    else\n      memcpy(&image_right->data[0], frame_right, width * height * 3);\n\n    left_pub.publish(image_left);\n    right_pub.publish(image_right);\n\n    sendInfo(capture_time);\n\n    ++pair_id;\n      }\n\n      frames_to_skip = skip_frames;\n    } else {\n      frames_to_skip--;\n    }\n\n    if (frame_left)\n      cam_left->release(left_idx);\n    if (frame_right)\n      cam_right->release(right_idx);\n  }\n}\n", "void StereoCamera::sendInfo(ros::Time time_left, ros::Time time_right) {\n  CameraInfoPtr info_left(new CameraInfo(left_info_mgr.getCameraInfo()));\n  CameraInfoPtr info_right(new CameraInfo(right_info_mgr.getCameraInfo()));\n\n  info_left->header.stamp = time_left;\n  info_right->header.stamp = time_right;\n  info_left->header.frame_id = frame;\n  info_right->header.frame_id = frame;\n\n  left_info_pub.publish(info_left);\n  right_info_pub.publish(info_right);\n}\n\n\nvoid StereoCamera::feedImages() {\n  unsigned int pair_id = 0;\n  while (ok) {\n    unsigned char *frame_left = NULL, *frame_right = NULL;\n    uint32_t bytes_used_left, bytes_used_right;\n\n\n    ros::Time capture_time_left = ros::Time::now();\n    int left_idx = cam_left->grab(&frame_left, bytes_used_left);\n\n\n    ros::Time capture_time_right = ros::Time::now();\n    int right_idx = cam_right->grab(&frame_right, bytes_used_right);\n\n    /* Read in every frame the camera generates, but only send each\n     * (skip_frames + 1)th frame. This reduces effective frame\n     * rate, processing time and network usage while keeping the\n     * images synchronized within the true framerate.\n     */\n    if (skip_frames == 0 || frames_to_skip == 0 ..."], "url": "https://answers.ros.org/question/32912/stereo-camera-syncronization/"},
{"title": "Asctec Pelican white screen ? no video signal [closed]", "time": "2012-05-10 16:31:56 -0600", "post_content": [" ", " ", "Hi all,", "yesterday I tried to set up the wireless for the Pelican and suddenly the system went down.\nAfter rebooting the connected display is just plain white. But the System boots correctly (can use system via ssh). Is the Display-port broken or is it a configuration failure? \nI can't get where the failure is. I just wanted to start this script and then the battery died.   ", "face=eth1 ", "\nifconfig $iface down ", "\niwconfig $iface mode Managed ", "\nifconfig $iface up ", "\nkillall wpa_supplicant ", "\nwpa_supplicant -B -Dwext -i $iface -c ./wireless-wpa.conf -dd ", "\ndhclient $iface ", "which should not have any influence on the display/screen. ", "Sorry for posting a non ROS question in this forum but I'm really desperate and hope to get some help from the Asctec Users in this forum.", "Cheers"], "answer": [], "url": "https://answers.ros.org/question/33750/asctec-pelican-white-screen-no-video-signal/"},
{"title": "tf from one month ago", "time": "2012-06-11 05:20:39 -0600", "post_content": [" ", " ", "Hi,", "I'm trying to use tf between my Pandaboard, which controls the robot, and my laptop, which does some additional calculations. The stamp my Pandaboard gives to the transforms seems to be way off however, so the laptop claims it can't extrapolate the transforms. If I run 'rosrun tf tf_monitor' on my Pandaboard, I get the following :", "(notice the average delay and max delay)", "If I run the same thing on my laptop, it gives me :", "(notice again the delays)", "Running 'date' on the Pandaboard gives :\nMon May 21 12:26:38 CEST 2012", "And on the laptop:\nMon Jun 11 17:12:06 CEST 2012", "So I assume this is the issue? I don't want to update the time on my Pandaboard, I don't think that will help much as when the power is removed from the Pandaboard, the time will start to mismatch again.", "Is there a way for my laptop to tell it to use the ros::Time::now() from the roscore ? Which is ran on the Pandaboard.", "Thanks in advance!"], "answer": [" ", " ", "Using TF requires synchronized clocks between all computers. You should set the clock on your pandaboard correctly and probably run something like ", " to keep the clocks synchronized.", "It is possible to publish time as if you were in a simulator, but on running systems it is not recommended.  It will either be very low resolution or eat up a lot of bandwidth. And the performance will never match syncronized clocks. ", "Thank you, this does appear to be what I'm looking for, but I'm having some problems getting it to work. I installed chrony (using sudo apt-get) on both systems and followed this 'guide' ", " , however my time does not appear to be synced...", "Also when I run chronyc and try settime, sources, clients and probably more, I get the message \"506 Cannot talk to daemon\", a google search did not help me.. Any ideas? Both my chronyc and chronyd have the same version number. Should I make this a new question ?", " ", " ", "Another solution might be to organize your system layout. If the laptop only does computations, i.e. does not do any direct interfacing, the laptop should not generate its own timestamps at all. For publishing, the timestamp should be that of the originating message. For transforming data, timestamps from the incoming messages should be used.", "I thought about that, but that won't work. The laptop is also generating timestamps on the images it receives from the kinect, as it is too heavy for the Pandaboard. If I want to transform positions retrieved from the kinect to base_link, then that would still cause issues.", "In that case you'll have to use synchronous time as mentioned by ", "."], "question_code": ["All Broadcasters:\nNode: /HeadMotorHandler 50.2458 Hz, Average Delay: 0.00213946 Max Delay: 0.00610352\nNode: /base_to_head 10.1741 Hz, Average Delay: -0.0980132 Max Delay: 0\nNode: /head_to_arm 10.1743 Hz, Average Delay: -0.0976757 Max Delay: 0\nNode: /kinect_normal_axis_to_kinect 10.1766 Hz, Average Delay: -0.097822 Max Delay: 0\n", "All Broadcasters:\nNode: /HeadMotorHandler 50.2697 Hz, Average Delay: 1.83153e+06 Max Delay: 1.83153e+06\nNode: /base_to_head 10.1984 Hz, Average Delay: 1.83153e+06 Max Delay: 1.83153e+06\nNode: /head_to_arm 10.1998 Hz, Average Delay: 1.83153e+06 Max Delay: 1.83153e+06\nNode: /kinect_normal_axis_to_kinect 10.1955 Hz, Average Delay: 1.83153e+06 Max Delay: 1.83153e+06\n"], "url": "https://answers.ros.org/question/36222/tf-from-one-month-ago/"},
{"title": "no devices connected [closed]", "time": "2012-05-28 00:14:05 -0600", "post_content": [" ", " ", " ", " ", "I'm using ros fuerte, ubuntu 12.04, when I try roslaunch openni_launch openni.launch\n", "\nthis is what I get: ", "d", "how is it possibile?? I have my kinect connected and It works on sdk on win.", "How is your kinect powered?"], "answer": [" ", " ", " ", " ", "ubuntu tries to access the kinect when you plug it in in newer versions using a mod. Go to terminal and type,", "and you should be good (it does this every time you connect the device). There are other troubleshooting methods on the ", "EDIT:\nIf this is your problem, blacklist the gspca_kinect module on your system,", " ", " ", " ", " ", "No it doesn't work, \nERROR: Module gspca_kinect does not exist in /proc/modules ", "any other ideas?? ", "After plugging in the kinect? Is the XnSensorServer running after connection? (try 'killall XnSensorServer' into terminal, it's another prime sense executable attempting to access the kinect). If the gspca_* are not mod's then I don't think the kinect is transferring data to your port", "already tried killall XnSensorServer and no process found. ", "XnSensorServer and the module gspca_kinect should run when you connect the device if all packages were installed correctly. I would double-check the hardware by finding another method to ensure the connection is correct,", "I just know that on windows 7 with the official sdk, this kinect works. anyway I've just noticed that when kinect is connect to win has the little light always on, when is on ubuntu the light is blinking. so maybe you're right. ", "is it possible that I ve not installed correctly openni?? ", " I was checking here, sudo apt-get install ros-fuerte-openni-kinect ..I think there is just this to do or not? cause I've tried to do the lines hg clone ", " but I get err", "You tried 1. 'sudo apt-get install ros-fuerte-openni-launch' 2. 'sudo apt-get install ros-fuerte-openni-camera' 3. 'rosmake openni_launch' 4. 'rosmake openni_camera'? If connection did not work after that I've got nothing. Blinking light doesn't matter - mine does too", "possibly try 'sudo apt-get install --reinstall <package>' both those packages, and then rosmake them again, you might have some more luck", "after all day trying don't know why but at the end it works, I reinstalling package,trying in my pc, other pc, other kinect (and hey xbox kinect works, pc vers no), but it works just once then I have to restart ubuntu to make it works again. better than nothing =) ", " ", " ", "thank you, I'll try tomorrow and I tell you if it works, I really hope :D ", " ", " ", " ", " ", "now I receive another error too!! ", "[ERROR] [1338277737.235478636]: Tried\n  to advertise a service that is already\n  advertised in this node\n  [/camera/depth_registered/image_rect_raw/theora/set_parameters]\n  [ INFO] [1338277739.671942244]: No\n  devices connected.... waiting for\n  devices to be connected", "Please use the \"comment\" button for commenting instead of posting new answers, or alternatively edit your question. Answers will get reordered over time, so this whole thread won't be a thread anymore, but a garble of sentences.", "ok sorry   "], "question_code": ["[ INFO] [1307492242.533491680]:[/openni_node1] No devices connected.... waiting for devices to be connecte"], "answer_code": ["sudo rmmod gspca_kinect\n", "sudo echo \"blacklist gspca_kinect\" >> /etc/modprobe.d/blacklist.conf\n"], "url": "https://answers.ros.org/question/35020/no-devices-connected/"},
{"title": "Turtlebot_Teleop not Working Properly", "time": "2012-03-19 09:17:41 -0600", "post_content": [" ", " ", " ", " ", "We are having trouble getting turtlebot_teleop node to work.", "After we ssh into the turtlebot and ", " the program begins working, but when we press the keys, the turtlebot does not respond.  It displays changes in velocity, so we know that it is registering our key presses, but the robot remains stationary.  ", "We even borrowed another turtlebot from someone else working in the lab to make sure it wasn't our robot, but could not get it to work on his either.", "Log File:", "When I echo /cmd_vel it displays linear and angular coordinates, so the information is definitely being published, but when I look at the rxgraph of everything, the /turtlebot_teleop_keyboard is only linked to /rosout.", "Shouldn't it also be communicating with something else in order to send its messages to the robot?", "Karthik, ", "I will try this out later today and let you know how it goes.  Thank you again for your help!", "March 26 Update", "I tested all the network connections and everything is fine, but turtlesim is not installed on the laptop, so I could not test that.", "So I believe my problem stems from the nodes not correctly publishing or subscribing.  In my rxgraph, the turtlebot_teleop_keyboard node is publishing a cmd_vel topic but the turtlebot_node is not subscribing to it.  When I click on the turtlebot_node it says \"Subscriptions: none\".  When I do ", " it says ", "so it looks like there is conflicting info.  I am unsure what to do to fix this.", "March 28", "So I found this post which might explain some of the problem: ", "but I followed the proposed solutions and it still does not work.", "I downloaded Brown university's teleop package - teleop_twist_keyboard and it almost mimics the ...", "Please provide more information.  We cannot help you unless you provide information on how to reproduce your error. ", "One thing to check is that the TurtleBot dashboard is working and that you are communicating with the Create. ", ".", "There is no real \"error\", the robot just does not respond to keystrokes.  The output on the screen shows that the program is responding to the keystrokes, but the robot itself does not move.  It led us to believe it was a problem with our robot, but we tried another robot it would not move either.", "The dashboard is working perfectly (everything is green), and we can even visualize things with the kinect in rviz.  We were hoping to have some fun with gmapping, but without teleop we have no way to really drive it around.  Thank you again for any suggestions.", "The /turtlebot_teleop_keyboard is connected to /cmd_vel\n  The details are Type: geometry_msgs/Twist", "Publishers: \n * /turtlebot_teleop_keyboard (", "/)", "Subscribers: None", "Type: geometry_msgs/Twist", "Publishers: \n * /turtlebot_teleop_keyboard (", "/)", "Subscribers: None", "Ok i have edited my answer witha diagnosing method. Let me know if you find anything", "if you run 'rostopic info /cmd_vel' you should see turtlebot_node listed as a subscriber. I would try running minimal.launch manually to see if there is a problem there."], "answer": [" ", " ", "The default topic for the cmd_vel changed as you discovered.  It looks like you might be running code of different versions of the release on your different machines.  You can fix it by using ", " until you update one or both machines.  ", " ", " ", " ", " ", "Hi,\nIf you are able to view the pointclouds in rviz, then you might have just overlooked something. ", "Check rxgraph if the nodes are connected by correct topics. ", "Try to echo the topic /cmd_vel to see if the keystrokes are giving the msgs. ", "Check the launch file if the topics are correct.", "You should be able to diagnose something with all these checks. Hope it helps", "Thanks,\nKarthik", "Following are the checks that you can do", "Check the .bashrc of the both workstation and turtlebot_lappy if they are concurrent with the ", " ", "You may check the network setup by running the turtlesim on turtlebot_lappy and teleop it from the workstation. This should work fine if network is perfectly setup. I explain the steps as follows", "a)In your turtlebot_lappy", "and", "b)From your workstation ", "Check if the turtle is moving in turtlebot_lappy when u press the arrow keys in workstation.", "Once this is working. Then i guess you may test the turtlebot with similar procedure. ", "The rxgraph should look like below for the turtlebot teleoperated from workstation\n", "Hope it helps", "Karthik", " ", " ", "I have had this issue myself. For some reason, the turtlebot_teleop only likes to work when the keyboard is connected to the actual turtlebot. I would try using a program called ", " to share your keyboard with the turtlebot. Once you share it, you should open a terminal on the turtlebot laptop and run the teleop. Use Synergy to share the keyboard at your workstation with the robot. Hope it works! ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "April 16 - PROBLEM FIXED", "Sorry for the late response, but we managed to fix this a few weeks ago by doing the following:", "roscd turtlebot_bringup/upstart\nsudo ./install.bash\nFor some reason both diamondback and electric were installed and this fixed the issue of it trying to use old stuff."], "question_code": ["roslaunch turtlebot_teleop keyboard_teleop.launch", "[rospy.client][INFO] 2012-03-21 11:15:15,572: init_node, name[/turtlebot_teleop_keyboard], pid[9624]\n[xmlrpc][INFO] 2012-03-21 11:15:15,573: XML-RPC server binding to 0.0.0.0\n[xmlrpc][INFO] 2012-03-21 11:15:15,575: Started XML-RPC server [http://192.168.1.101:42373/]\n[rospy.init][INFO] 2012-03-21 11:15:15,575: ROS Slave URI: [http://192.168.1.101:42373/]\n[rospy.impl.masterslave][INFO] 2012-03-21 11:15:15,576: _ready: http://192.168.1.101:42373/\n[rospy.registration][INFO] 2012-03-21 11:15:15,580: Registering with master node http://192.168.1.101:11311\n[xmlrpc][INFO] 2012-03-21 11:15:15,580: xml rpc node: starting XML-RPC server\n[rospy.registration][INFO] 2012-03-21 11:15:15,581: Registering publisher topic [/cmd_vel] type [geometry_msgs/Twist] with master\n[rospy.init][INFO] 2012-03-21 11:15:15,677: registered with master\n[rospy.rosout][INFO] 2012-03-21 11:15:15,808: initializing /rosout core topic\n[rospy.rosout][INFO] 2012-03-21 11:15:15,814: connected to core topic /rosout\n[rospy.simtime][INFO] 2012-03-21 11:15:15,821: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic\n[rospy.core][INFO] 2012-03-21 11:15:55,983: signal_shutdown [atexit]\n[rospy.impl.masterslave][INFO] 2012-03-21 11:15:55,995: atexit\n", "rosnode info turtlebot_node", "Subscriptions: \n * /turtlebot_node/cmd_vel [unknown type]\n"], "answer_code": ["roscore\n", "rosrun turtlesim turtlesim_node\n", "rosrun turtlesim turtle_teleop_key\n"], "url": "https://answers.ros.org/question/29983/turtlebot_teleop-not-working-properly/"},
{"title": "How long can i run a usb web cam cord from my desk top? [closed]", "time": "2012-07-05 06:48:52 -0600", "post_content": [" ", " ", " ", " ", "How long can i run a usb web cam cord from my desk top? Is there some kind of power or signal booster that can lengthen the amount of cable signal strength from the desk top on a really long cord to a web cam.", "Closing as off-topic. It's a much better question for something like ", " (and a super quick search because I was curious did find ", " )"], "answer": [" ", " ", "This is really not a ROS question. I'll allow it because someone here might know the answer."], "url": "https://answers.ros.org/question/38020/how-long-can-i-run-a-usb-web-cam-cord-from-my-desk-top/"},
{"title": "IMU for Turtlebot [closed]", "time": "2012-06-18 07:24:32 -0600", "post_content": [" ", " ", " ", " ", "Just wondering if anyone has any experience with adding an IMU to a turtlebot. Was looking at this one specifically ", " Has anyone had luck with any others or are there more popular choices? Also, would it be as simple as plugging it in and running the driver node? I'm guessing there would be problems with the turtlebot's powerboard gyro as that also publishes to imu/data."], "answer": [], "url": "https://answers.ros.org/question/36732/imu-for-turtlebot/"},
{"title": "ROS Backwards Compatibility in Distributed System", "time": "2012-06-27 06:32:18 -0600", "post_content": [" ", " ", "I'm helping to develop an industrial robotics system that uses ROS.  The system will be distributed amongst several embedded devices, perhaps some more powerful machines for the complicated algorithms, and a few consoles. The roscore will be running on one of the more powerful machines/consoles.", "After we get our system working, we plan to continue improving it, adding more devices as necessary. On the new devices we'll probably want to use the new awesomeFeatureX from the new version of ROS that'll be out then.  I don't want to have to update ", " node on each embedded machine in service because something might break with all those updates, and there's no reason to fix something that isn't broken. It would also be a pain to have two different ROS systems running side by side, and doing so would defeat one of the reasons for using ROS in the first place.  On the other hand, the system will have been working and will be depended upon, so we can't break anything by just updating the master, either.", "So my question is: what would happen to a working, distributed system if a new ROS version were installed on the machine running the master?", "Or basically, what it boils down to is: In the foreseeable future, will communication between a master of the future and nodes of the past still be the same format?\nOr perhaps devices programmed with new code can communicate with an old master?", "Thanks in advance."], "answer": [" ", " ", "The incompatibility is not just about a new version of the master. (So far most releases haven't changed the master). The primary reason for incompatibility of communication between versions of ROS is that the message definitions may have changed.  Which is not something which can be trivially fixed.  There are bag migration rules which could be applied in a python process and do translations, but maintaining support for arbitrary combinations of versions is a very large burden which we cannot support at the moment.  ", "I would suggest that if you really need newFeatureX in an older platform that you backport newFeatureX to the older platform instead of trying to get both versions to be compatible.  If you compile it against the older version of ROS that's great. The farther you diverge for the tested and integrated system the more effort it will require on your part. ", "Another solution which might be possible would be to create a translation node which can talk to both systems and republish communications from one to another.  This might be possible in python with a lot of work, but I wouldn't recommend even trying. ", " ", " ", "I would never recommend combining nodes compiled using different ROS versions in the same run-time execution graph.", "While it may occasionally work, it does not in general."], "url": "https://answers.ros.org/question/37403/ros-backwards-compatibility-in-distributed-system/"},
{"title": "Bad Gyro Callibration", "time": "2012-06-21 08:57:28 -0600", "post_content": [" ", " ", " ", " ", "I am receiving the following error while running the Turtlebot dashboard:", "Level: Error\nMessage: Bad Gyro Calibration Offset: The gyro average is outside the standard deviation.", "Gyro Enabled: True\nRaw Gyro Rate: 311\nCalibration Offset: 311\nCalibration Buffer: [311, 310, 310, ..., 310] (Values range from 310-311.)", "I saw the other two questions regarding this and tried the solutions suggested to no avail. I have cycled the power, unplugged and replugged the power board several times and nothing has worked. I am also receiving this error with two different Turtlebots. Both are assembled using kits from Clearpath Robotics.", "Possible solutions?", "Did you ever get this issue resolved? I have basically the same problem.  I also noticed the raw gyro value would change if the robot was turning, which seems to indicate the gyro is NOT malfunctioning.", "I seem to have found the right combination of calibration numbers to make the gyro report correct values.  I still get an error message on the dashboard saying bad gyro calibration, but the robot model in rviz is turning almost exactly as much as the physical turtlebot."], "answer": [" ", " ", "Contact support@clearpathrobotics.com; we'll hook you up with replacements. :)"], "url": "https://answers.ros.org/question/37009/bad-gyro-callibration/"},
{"title": "Turtlebot obstacles", "time": "2012-05-06 09:40:55 -0600", "post_content": [" ", " ", "I would like to make my turtlebot navigate (by itself) through a room (where i give him the destination). The current solution builds a map using gmapping and then I use rviz to give it a destination on the map.", "It works but now i would like to add sticky notes (of different colors) on obstacles.", "Turtlebot has to detect the sticky note (from point cloud) and move itself in the right direction (based on color. For example if the sticky note is red it should go to the right, if it is green it should go to the left).", "Which packages can we use? Any recommendations?", "I'm a beginner with ROS and I would appreciate a point in the right direction.", "Thanks.", "do you need to navigate by color or would some other form of identifier be okay? like ", "We need to navigate by color.", "I suggest you to go with the ar_pose as indicated by mwise_wg. Doing it with color may have many distortions because of the environment and it must be ideal environment. "], "answer": [" ", " ", " ", " ", " would be a worthy tool for this job. It packages a lot of image processing algorithms into one library and supports a wide variety of cameras. I haven't personally used it with ROS but my experience on other projects has been good.", " ", " ", "Im trying to do the same thing. Use gmapping to build the map and then set a destination for the turtlebot to navigate to.  But were having problems with our kinect.  It doesnt seem to be outputting any data on rviz.  Can you shed some light on your process of getting the gmapping working and giving the turtlebot a destination?  Thank you!!"], "url": "https://answers.ros.org/question/33380/turtlebot-obstacles/"},
{"title": "simulated turtlebot mapping itself", "time": "2012-06-07 05:56:03 -0600", "post_content": [" ", " ", "Hi,", "I am trying to simulate a turtlebot gmapping with gazebo, a la this tutorial\n", "My issue is that although I have gmapping up and running, I can't get anything close to an accurate map because the laserscan appears to be picking up the bot itself. it places an object where the robot was, and then fails to localize due to the object no longer being there once the bot moves.", "Is there any way that I can exempt the robot from the scan?", "Will you please post a bag file? I'd like to have a look at this situation... Record the tf, scan, odom, imu topics. ", "Please post the image of the map in rviz so that i could analyse the issue.", "I don't know how to post a bag file, but I will if you tell me how.", "Here is a screenshot of rviz. ", "As you can see, the room is well mapped, but the robot has left little bits of wall along the path that I drove it. For now it is localized properly, but the more I drive, the more likely these artifacts are to mess with localization. ", "also, note that the little green boxes that represent an object being picked up by the laserscan are also lighting up around the base of the robot. After close analysis the scan appears to be picking up on the four support bars in the robot model"], "answer": [" ", " ", "One common solution to this problem is to filter out scans that are less than some minimum distance from the sensor.", "Do you know of any good method of doing this? I can't seem to find a parameter or file to change to create this filter.", "You can check the fix that I made: ", "/", " ", " ", "Take a look at the launch file ", ". You can narrow your laser with this. This can adjust the F.O.V. for the laser to avoid scanning the robot in your scan. "], "url": "https://answers.ros.org/question/35962/simulated-turtlebot-mapping-itself/"},
{"title": "Creating an RVIZ plugin for a new display type", "time": "2012-08-02 23:56:31 -0600", "post_content": [" ", " ", "Hello,", "So for my project I created my own message type and was hoping to have it displayed in RVIZ. It is a slight adaptation of GridCells, but instead of containing a list of Points, it contains a list of a Point message that I created. The custom Point message contains two more pieces of information.", "My problem is that the User Guide for RVIZ has told me that there is no API documentation. Is this still true? Is there any way for me to write a plugin? I think using RVIZ to display custom types of messages would make it even more powerful of a tool. Thanks for the help,", "Blair"], "answer": [" ", " ", "As far as I know the interface is still under development, so the API might not be stable, but there is a tutorial:", "Depending on what you want to display, there is always the option to send everything as a marker array. That should work all the time, if applicable.", " ", " ", "Creating new ", " plugins is difficult and not encouraged.", "There is a new ", " that provides a more general plugin interface. It should be better for new visualization work.", "Two years passed... so is still this the situation? We plan to create a graphical editor to add semantic information to 2D/3D maps, and a RViz plugin could be the ideal solution. Rqt plugins look great for simple interfaces, but we need the fully-fledged 3D interface that RViz provides. Thanks!!!", "Another year passed. Seems like there is no relief for plugin makers. I guess now they have these random tutorials but cannot seem to get the configuration correctly with catkin. The tutorials use ros_make. Anyone with experience???", " the visualization tutorial package is a catkin package, you can use that as a template:  "], "answer_code": ["rviz", "rqt"], "url": "https://answers.ros.org/question/40468/creating-an-rviz-plugin-for-a-new-display-type/"},
{"title": "Prosilica Hardware Synchronization [closed]", "time": "2012-08-15 08:39:05 -0600", "post_content": [" ", " ", " ", " ", "Goal: I am trying to create a stereo set up with two Prosilica GC750C cameras. I would like to collect the stereo data and store it into a bag file to post process later. I am limited by the quality of cabling and processing power. I am running an Intel Atom processor and have underwater cabling.", "Problem: My problem is that I cannot get synchronized image capture. I have my cameras hardwired so that the syncin2 of one camera is connected to the syncout2 of the other camera ", ". I have tried various configurations in ROS and I feel that the most promising is to run one camera in the \"streaming\" mode and have the other in \"syncin2\" and receive triggers from the streaming camera. ", "Using the Sample Viewer I currently have the \"syncout\" set to strobe1 which is set up to reflect when the camera is exposing(I have read that the FrameTrigger pulse is too short to trigger the camera and have had no luck getting the FrameTrigger to work with the strobe feature). The Strobe is used to limit the pulse length of the exposing trigger. While I have played around with the settings of SyncOut I am not sure that \"exposing\" is what I am looking for.", "Under this setup I frequently get a scenario where the \"streaming\" camera logs frames twice as fast as the synced camera. I believe that this is because the \"synced\" camera is still capturing when the \"streaming\" camera sends its next trigger. Significantly reducing the max exposure time of the synced camera eliminates this problem, yet I feel that this is a poor hack to this problem. Also, using rxbag I can see that the frames coming from the \"synced\" camera are logged a little bit later. I am not sure if this is a result of triggering the camera with \"exposing\" and not \"FrameTrigger\" or a result of how the frames are time stamped, as described in ticket ", "I feel that the best thing for me to do would be to try to have the streaming camera set to a fixed rate. That way I could insure an adequate amount of time between the triggers of the \"syncin2\" triggered camera so that camera can finish exposing before the next trigger is received.  I understand that the ROS drivers do not have this functionality yet but that it is something that prosilica API does. Has anyone played with this?", "Has anyone achieved hardware synchronization with prosilica cameras? Is your setup(hardware/software) similar to what I described or am I on the wrong track? Any suggestions on what I should try?", "Thanks! "], "answer": [], "url": "https://answers.ros.org/question/41510/prosilica-hardware-synchronization/"},
{"title": "Problems with comunicate Turtlebot and Netbook", "time": "2012-08-01 01:57:46 -0600", "post_content": [" ", " ", "Hi,\nI am an engineering student who is working with the turtlebot, this one was purchased from the official website.", "I  follow all the tutorials from the ROS website but, I think that ROS can't access my Create... Is not possible see informations about the robot in turtlebot_dashboard, like battery and sensors status, but is possible to see netbook battery status, then I can conclude that the connection Workstation-netbook is ok, the problem could be between the netbook and Create. The status of all sensors in turtlebot_dashboard is Stale, except the netbook battery.", "Using  \"lsusb\" command, I can see that my converter is properly connected at ttyUSB0. ", "I read in another post that adding in the /etc/udev/rules.d/52-turtlebot.rules a line with my idProduct and idVendor the problem is solve. This line is: ", "ATTRS{idProduct}==\"6001\",ATTRS{idVendor}==\"0403\",MODE=\"666\",GROUP=\"turtlebot\"", "But this don't work for me, can anyone help me? ", "Thanks, iscaram", "Did you actually start the turtlebot?", "Yes, i have the turtlebot here, i did all the tutorials for install ROS (in both computers, turtlebot ones and in my laptop), but  Is not possible see informations about the robot in turtlebot_dashboard"], "answer": [" ", " ", "I had similar problems. When you run lsusb on TurtleBot laptop do you see a list of three microsoft corporation devices referring to the Kinect ?. If no, the TurtleBot is not fully up. Bring-up the TurtleBot either using minimal launch or service start and wait for the second beep from create. Now the dashboard should work properly. Put the TurtleBot on full mode and check lsusb again. This ", " of mine might help."], "url": "https://answers.ros.org/question/40307/problems-with-comunicate-turtlebot-and-netbook/"},
{"title": "rviz vs vtk", "time": "2012-08-01 02:32:54 -0600", "post_content": [" ", " ", " ", " ", "I have a to create a new vizualizer for a robot related project. While I have solid experience with OpenGL, that experience points to there being a productivity gain from using a toolkit. The decision comes down to rviz, or VTK. ", "The vizualizer will need to be:\n- embed-able in other applications\n- middleware agnostic\n- displays will renders, there seems to be more to leverage from VTK", "To be honest, from what I see there is very little I can leverage from rviz. What I may want to leverage have ties to the \"ros way\" of doing things which conflict with being middleware agnostic. VTK also seems to have a larger set of components, and potential power.", "Thoughts?", "Related, can ogre and VTK play together in the same app? (i.e. if someone wanted to make an rviz plugin for a VTK based component)"], "answer": [" ", " ", "VTK is a better approach for ROS-agnostic visualization.", "I believe VTK uses OGRE, itself.", "If you later want to integrate your tool into a ROS system, you might consider the ", ", which is designed to be easier to plug into than rviz."], "url": "https://answers.ros.org/question/40310/rviz-vs-vtk/"},
{"title": "Hokuyo Laser not appearing under /dev [closed]", "time": "2012-07-25 10:00:55 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to follow the instructions for the hokuyo node here: ", "Nothing is coming up when I \"ls /dev/ttyACM*\"  My laser is connected to a usb port and the green light on top of the laser is on.", "Does anyone have an idea of why this might happen?", "Thanks!", "Can you edit your post with the output of 'dmesg'?", "Also the output of \"lsusb\"?"], "answer": [], "url": "https://answers.ros.org/question/39807/hokuyo-laser-not-appearing-under-dev/"},
{"title": "Problem adding rosserial node to turtlebot bringup", "time": "2012-07-13 18:27:49 -0600", "post_content": [" ", " ", "Hello All,", "This is my first question to the list so please advise any etiquette that I may have overlooked regards to posts. Sorry about the length, but I figure more detail is better!", " I have created and tested my rosserial nodes and Arduino code, and everything is working. I have also added these nodes to a separate launch file, and when I call roslaunch on it, ", ".", "The problem occurs when I try to get the rosserial node to start with all the other Turtlebot nodes by including my launch file in minimal.launch.", " ", "Ubuntu 11.10 Oneiric\nROS Electric\nros-electric-rosserial, 0.3.0-s1336605681~oneiric\nArduino UNO (Rev 2 board)\nFor reference, I had installed Ubuntu first, then ROS electric, then the turtlebot stacks.", " ", "This is from the error log in /home/turtlebot/.ros/log/....../\nAs you can see, ", ", however I am not touching anything and all other nodes live on.", "[rospy.client][INFO] 2012-07-13 19:53:18,918: init_node, name[/rosserial_lipo], pid[17829] ", "\n[xmlrpc][INFO] 2012-07-13 19:53:18,919: XML-RPC server binding to 0.0.0.0 ", "\n[rospy.init][INFO] 2012-07-13 19:53:18,920: ROS Slave URI: [", "/] ", "\n[xmlrpc][INFO] 2012-07-13 19:53:18,920: Started XML-RPC server [", "/] ", "\n[rospy.impl.masterslave][INFO] 2012-07-13 19:53:18,921: _ready: ", "/ ", "\n[xmlrpc][INFO] 2012-07-13 19:53:18,922: xml rpc node: starting XML-RPC server ", "\n[rospy.registration][INFO] 2012-07-13 19:53:18,925: Registering with master node ", " ", "\n[rospy.init][INFO] 2012-07-13 19:53:19,021: registered with master ", "\n[rospy.rosout][INFO] 2012-07-13 19:53:19,109: initializing /rosout core topic ", "\n[rospy.rosout][INFO] 2012-07-13 19:53:19,121: connected to core topic /rosout ", "\n[rospy.simtime][INFO] 2012-07-13 19:53:19,125: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic ", "\n[rosout][INFO] 2012-07-13 19:53:19,126: ROS Serial Python Node ", "\n[rosout][INFO] 2012-07-13 19:53:19,133: Connected on /dev/ttyACM0 at 57600 baud ", " ", " ", " ", " (in /opt/ros/electric/stacks/turtlebot/turtlebot_bringup)", "In turtlebot_bringup package, I've edited minimal.launch to have the following. The \"laptop battery\" stuff has been pulled out since I am using a mini-PC instead of a netbook (I'm using Arduino as a LiPo voltage monitor).", " ", "This is the complete file. The launch file, as well as ", ", live under /home/doug/ros_workspace/turtlebot_lipo/."], "answer": [" ", " ", " ", " ", "Well, as much as I dislike answering my own questions on such a site, since having other people's input gives a fresh perspective, I was able to eventually figure things out.", "The main issue was that the user \"turtlebot\" did not have appropriate permissions to open /dev/ttyACM0. One part of the solution involved creating a file for my USB connection, as outlined in a previous post here:\n", "\n/etc/udev/rules.d/81-ftdi.rules", "\nSUBSYSTEMS==\"usb\", KERNEL==\"ttyACM[0-9]*\", ATTRS{idVendor}==\"2341\", ATTRS{idProduct}==\"0001\", ATTRS{serial}==\"649383233313511011A0\", SYMLINK+=\"uno_rev2\", MODE=\"666\", GROUP=\"turtlebot\"", "The critical parts that made it work for me are at the end, especially the part ", ". This mounts the device as being owned by the turtlebot group, so the startup script has no problem connecting to it. Explanation of the remaining parts can be found in the link, except the \"ARRTS{serial}\" part that I picked up ", ".", "Regards to how I figured out that it was a permission issue, when everything other resource said I should be fine, required diving into the rosserial_python code. I have some code enhancements and will file a bug report / feature request on the update. I has assumed that when ROS outputs this...", "...that it was actually connected. It turns out this statement is printed even before an attempt to open the port is made.", "Another thing I had to do was make a small change to the launch file. Since the Arduino is apparently not ready when the launch comes up, I added ", " to the rosserial_python node so it will keep trying until it is ready. I must find a way to set respawn frequency though since it currently is about 1Hz.", "I created a ticket for enhancement here, along with a recommended patch : "], "url": "https://answers.ros.org/question/38786/problem-adding-rosserial-node-to-turtlebot-bringup/"},
{"title": "cob_script_server always results in aborted", "time": "2012-08-01 12:42:38 -0600", "post_content": [" ", " ", " ", " ", "I've been trying to interface with the COB_SCRIPT_SERVER but all of my attempts are generally resulting in an aborted state, although in Gazebo it's clear that the robot is completing the action.  Even the included scripts are resulting in the same behaviour.  For simplicity, I'm going to reference a run of one of the included scripts ros_script.py.  I say generally, because about 10% of the time, the individual action will result in a success state.  For example, the tray will reach down, but the arm will abort, or vice versa.  I've experienced this on both machines I've used for development.  I'm not sure if I'm missing a step during the installation, or if there is a bug that's preventing the script server from running properly.", "Details below:", "\nVersion: Electric", "\nOutput:", "\nnathan@Desktop:~$ rosrun cob_script_server ros_script.py", "\n[INFO] [WallTime: 1343860413.922051] [0.000000] Start parsing...", "\n[INFO] [WallTime: 1343860415.098590] [1813.232000] ...parsing finished", "\n[INFO] [WallTime: 1343860416.166140] [1814.234000] Starting <<ros_script>> script...", "\n[INFO] [WallTime: 1343860416.173912] [1814.238000] <<init>> <<tray>>", "\n[INFO] [WallTime: 1343860416.177135] [1814.244000] Wait for <<tray>> to <<init>>...", "\n[INFO] [WallTime: 1343860416.179207] [1814.247000] ...<<tray>> is <<init>>", "\n[INFO] [WallTime: 1343860416.181657] [1814.247000] <<init>> <<torso>>", "\n[INFO] [WallTime: 1343860416.184228] [1814.247000] Wait for <<torso>> to <<init>>...", "\n[INFO] [WallTime: 1343860416.186081] [1814.247000] ...<<torso>> is <<init>>", "\n[INFO] [WallTime: 1343860416.188504] [1814.250000] <<init>> <<arm>>", "\n[INFO] [WallTime: 1343860416.197476] [1814.258000] Wait for <<arm>> to <<init>>...", "\n[INFO] [WallTime: 1343860416.199323] [1814.260000] ...<<arm>> is <<init>>", "\n[INFO] [WallTime: 1343860416.202449] [1814.265000] <<init>> <<sdh>>", "\n[INFO] [WallTime: 1343860416.205368] [1814.270000] Wait for <<sdh>> to <<init>>...", "\n[INFO] [WallTime: 1343860416.207662] [1814.275000] ...<<sdh>> is <<init>>", "\n[INFO] [WallTime: 1343860416.216444] [1814.280000] Set light to <<red>>", "\n[INFO] [WallTime: 1343860416.220778] [1814.284000] Move <<arm>> to <<folded>>", "\n[INFO] [WallTime: 1343860416.493836] [1814.510000] Move <<torso>> to <<home>>", "\n[INFO] [WallTime: 1343860416.732325] [1814.705000] Move <<sdh>> to <<home>>", "\n[INFO] [WallTime: 1343860417.005013] [1814.980000] Move <<tray>> to <<down>>", "\n[INFO] [WallTime: 1343860417.278968] [1815.216000] Wait for <<tray>> reaching <<down>>...", "\n[ERROR] [WallTime: 1343860420.619813] [1818.026000] Got a result when we were already in the DONE state", "\n[ERROR] [WallTime: 1343860420.910384] [1818.226000] Got a result when we were already in the DONE state", "\n[ERROR] [WallTime: 1343860421.142359] [1818.403000] Got a result when we were already in the DONE state", " ", " ", "\n[INFO] [WallTime: 1343860421.281221] [1818.501000] Move <<base>> to <<home>>", "\n[ERROR] [WallTime: 1343860421.585754] [1818.728000] Got a result when we were already in the DONE state", "\n[ERROR] [WallTime: 1343860427.967003] [1823.516000] /move_base action server not ready within timeout, aborting...", "\n[INFO] [WallTime: 1343860427.975447] [1823.519000] Set light to <<green>>", "\n[INFO] [WallTime: 1343860427.978389] [1823.524000] Wait for script to finish...", "\n[INFO] [WallTime: 1343860427.978713] [1823.524000] ...script finished.", " "], "answer": [" ", " ", "The problem you describe is not related to the cob_script_server but to the trajectory controller which is moving the components in gazebo. The script_server just forwards the action result which is returned by the actionlib interface to the gazebo controllers.\nSo far I am not aware of a sulution solving this. If you want to investigate furhter you'd probably dive into ", ".", "Thanks for the help, at least now I know it's not something that I've done.  I'll just work around it until I get a chance to run it on the physical robot"], "url": "https://answers.ros.org/question/40351/cob_script_server-always-results-in-aborted/"},
{"title": "RGBD SLAM Display [closed]", "time": "2012-07-26 07:05:43 -0600", "post_content": [" ", " ", " ", " ", "I am using Ubuntu 11.10 with Ros ELECTRIC. Kinect is connected. ", "The command ", " results in acquiring RGB  and depth data. But in the upper window where i suppose a map should be displayed. Nothing comes, only a black screen with coordinate lines in the center (red and green). ", "Can anyone help??", "This sounds like a duplicate of ", "/", "Sometimes it's just the simplest solution. Did you press space bar to start processing? ", "I am working with Tahir. Yes, we pressed space bar. No output.", "did u try launchin openni.launch and rgbdslam.launch separately?"], "answer": [], "url": "https://answers.ros.org/question/39919/rgbd-slam-display/"},
{"title": "Visualization of the maximum distance point in rviz", "time": "2012-08-07 16:39:15 -0600", "post_content": [" ", " ", " ", " ", "Hello", "I create and build own node that calculate the maximal distance of the obstacle (based on laser data). I would like to visualize that point in rviz and distinguished with some colour(for example green). This is my launch file", " ", "This is the source code of the max_range_node", "What is max_range_node doing and how does it provide its data? What's the output of ", "? By just seeing the launch file, it is hard to infer what all your nodes are doing without even having the documentation/sourcecode of these nodes.", "Ok. Just add the source code of the node. Max range just calculate the max distance of the obstacle of the laser range scan data,. ", "I just want to mark (with some colour, for example green) this Maximum range, which is max_range calculated in the node I just post it. ", "I just the max_range_node visualize. The other nodes are at the moment not important. Ok?", "Your code seems quite obscure. Are you sure it does what you want? You walk over the laser in nested loops. If you just want to get the max/min that is wrong.", "Ok. yes than its wrong. What should I do to get max/min?", "Well, usually it's: max=0; for(uint i ... laser.range) if(laser.range[i] > max: max=laser.range[i]"], "answer": [" ", " ", "Given your purpose I'd suggest to use a visualization_msgs/Marker instead of a LaserScan. A LaserScan can only display complete data, not single beams. With a marker, you can choose specific points, select a color, and also increase the size of the marker.", "Ok. Thanks. How to use a visualization_msgs/Marker??Where to specify that?Do I need some extra code for the marker???", "any example of  visualization_msgs/Marker and single beam like maximum range??"], "question_code": ["<launch>\n<param name=\"/use_sim_time\" value=\"true\"/>\n\n<node name=\"rosplay\" pkg=\"rosbag\" type=\"play\" args=\" /Data/p1-20.bag --loop  --clock\"/>\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"baselink_laser\" args=\"0 0 0 0 0 0 /base_link /laser 10\"/>\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"laser_imu\" args=\"0 0 0 0 0 0 /laser /base_imu 10\"/>\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"baselink_camera\" args=\"0 0 0 0 0 0 /base_link /camera 10\"/>\n\n<!-- Start the map server node and specify the map file (*.pgm) and the map resolution in metres/pixel -->\n<node name=\"map_server\" pkg=\"map_server\" type=\"map_server\" args=\"$(find amcl_listener)/maps/pow_real_time.yaml\"         output=\"screen\"/>\n\n<!--Start the Laser_scan_matcher package, to provide odometry from laser data (ICP)-->\n<node pkg=\"laser_scan_matcher\" type=\"laser_scan_matcher_node\" \n    name=\"laser_scan_matcher_node\" output=\"screen\">\n    <param name=\"use_alpha_beta\" value=\"true\"/>\n    <param name=\"max_iterations\" value=\"10\"/>\n</node>\n", "<node pkg=\"amcl\" type=\"amcl\" name=\"amcl\" respawn=\"true\" output=\"screen\">\n    <param name=\"base_frame_id\" value=\"base_link\"/>\n    <param name=\"odom_frame_id\" value=\"world\"/>\n    <param name=\"global_frame_id\" value=\"map\"/>\n    <param name=\"update_min_d\" value=\"0.09\"/>   \n    <param name=\"update_min_a\" value=\"0.09\"/> \n    <param name=\"initial_pose_x\" value=\"-1\"/>\n    <param name=\"initial_pose_y\" value=\"0\"/>\n    <param name=\"initial_pose_a\" value=\"-0.1\"/>\n    <param name=\"min_particles\" value=\"6000\"/> \n    <param name=\"max_particles\" value=\"8000\"/>\n    <param name=\"odom_model_type\" value=\"diff\"/>\n    <param name=\"kld_err\" value=\"0.1\"/> \n    <param name=\"resample_interval\" value=\"1\"/>\n    <param name=\"odom_alpha1\" value=\"0.2\"/>   \n    <param name=\"odom_alpha2\" value=\"0.2\"/>\n    <param name=\"odom_alpha3\" value=\"0.5\"/>\n    <param name=\"odom_alpha4\" value=\"0.5\"/>\n    <param name=\"laser_max_beams\" value=\"30\"/>\n</node>\n\n<!--Start the \"analyzer_packages\"-->\n<!-- acml_listener broadcasts markers for the position of the wheelchair, to visualize in Rviz-->\n<node pkg=\"amcl_listener\" type=\"amcl_pose_listener\" name=\"amcl_listener\" output=\"screen\"/>\n<!-- Object node records minimal ranges from laser, speed_node will record travel speed of wheelchair -->\n<!--node pkg=\"pow_analyzer\" type=\"object_node\" name=\"object_node\" output=\"screen\"/> -->\n<node pkg=\"pow_analyzer\" type=\"max_range_node\" name=\"max_range_node\" output=\"screen\"/>\n<!--node pkg=\"pow_analyzer\" type=\"speed_node\" name=\"speed_node\" output=\"screen\"/> -->\n\n<!-- Start an rviz node with a custom configuration for the viewpoint, map_server, trajectory, laser scans, etc -->\n<node pkg=\"rviz\" type=\"rviz\" output=\"screen\" name=\"rviz\" args=\"-d $(find pow_analyzer)/launch/pow_rviz.vcg\"/> \n\n</launch>\n", "#include \"ros/ros.h\"\n#include \"std_msgs/String.h\"\n#include \"sensor_msgs/LaserScan.h\"\n#include <vector>\n\nsensor_msgs::LaserScan laser_scan;\nfloat min_range;\nfloat max_range;\n\nvoid scanCallback(const sensor_msgs::LaserScan::ConstPtr& msg)\n{\n\n    std::vector<float> laser;\n    laser = msg->ranges;\n\n    int size_laser = laser.size();\n    for (int i=0;i<size_laser;i++){\n        if (laser[i] < 0.01){\n            laser[i] = 99999;\n        }\n        if (laser[i] > 45){\n            laser[i] = 99999;\n        }\n    }\n\n    min_range = 2; //Init closest point \n    max_range = 1; // Init maximal distance point\n    int index_min; \n    int index_max; \n\n//find the closest point to the obstacle and the maximal distance to the //obstacle\n\n\n    for (int i=0;i<size_laser;i++){\n        for (int j=0;j<size_laser;j++){\n\n        if (laser[i] < min_range ...", "rosnode info max_range_node"], "url": "https://answers.ros.org/question/40806/visualization-of-the-maximum-distance-point-in-rviz/"},
{"title": "Using cvGetSize and cvSplit", "time": "2012-08-22 14:00:35 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to find centroids of four green circles in image_transport video. I'm planning to convert to OpenCV image with cv_bridge. Then, with OpenCV, extract green channel (with cv::Split) and apply a threshold.", "For this I need cvCreateImage, cvGetSize and cvSplit, like this:", "cv_ptr = cv_bridge::toCvCopy(original_image, enc::BGR8); ", "cv_ptr_red = cvCreateImage( cvGetSize(cv_ptr), 8, 1); ", "cv_ptr_green = cvCreateImage( cvGetSize(cv_ptr), 8, 1); ", "cv_ptr_blue = cvCreateImage( cvGetSize(cv_ptr), 8, 1); ", "These lines that doesn't work give me the following error:", "cannot convert \u2018cv_bridge::CvImage\u2019 to\n  \u2018const CvArr* {aka const void*}\u2019 for\n  argument \u20181\u2019 to \u2018CvSize\n  cvGetSize(const CvArr*)\u2019", "What am I missing?", "I'm sorry for so many basic questions, but this C++ is way more complicated then Matlab.", "It does compile using the cv_ptr_green->image but the it gives me an error when running it. I reduced the code to try to find out where it was happening and now my code only has:", "And the error I get when running (compiling is OK):", "ardrone_visualservo: /usr/include/boost/smart_ptr/shared_ptr.hpp:418: T* boost::shared_ptr<t>::operator->() const [with T = cv_bridge::CvImage]: Assertion `px != 0' failed.", "Aborted (core dumped)", "If I understood what it tried to say, it's complaining about the -> operator usage with cv_bridge::CvImage (which is the type of my cv_ptr_green object).", "Any ideas how to fix this?", "I found that maybe I can use cv::mixChannels() to extract only the green channel (as I actually need) with", "but it also gives me an error", "path/to/cpp/file.cpp: In function \u2018void imageCallback(const ImageConstPtr&)\u2019:", "path/to/cpp/file.cpp:62:58: error: no matching function for call to \u2018mixChannels(cv_bridge::CvImagePtr&, int, cv_bridge::CvImagePtr&, int, int [2], int)\u2019", "path/to/cpp/file.cpp:62:58: note: candidates are:\n  /opt/ros/fuerte/include/opencv2/core/core.hpp:2137:17: note: void cv::mixChannels(const cv::Mat", ", size_t, const int*, size_t)", "/opt/ros/fuerte/include/opencv2/core/core.hpp:2137:17: note:   no known conversion for argument 1 from \u2018cv_bridge::CvImagePtr {aka boost::shared_ptr<cv_bridge::cvimage>}\u2019 to \u2018const cv::Mat*\u2019", "/opt/ros/fuerte/include/opencv2/core/core.hpp:2139:17: note: void cv::mixChannels(const std::vector<cv::mat>&, std::vector<cv::mat>&, const int*, size_t)", "/opt/ros/fuerte/include/opencv2/core/core.hpp:2139:17: note:   candidate expects 4 arguments, 6 provided", "/opt/ros/fuerte/include/opencv2/core/core.hpp:2141:19: note: void cv::mixChannels(cv::InputArrayOfArrays, cv::InputArrayOfArrays, const std::vector<int>&)", "Are you still using the older version of openCV? In that case are your dependencies also set to that version?", "What do you mean by older version of openCV? I installed everything, both ROS and openCV, about four days ago. My includes are: #include <opencv2/opencv.hpp>, ", "SivamPillai, thank you very much for your help. Unfortunately, it did not work for me though. It seems it can't find cv:create and cv:size member functions. Yes, I believe my OpenCV is the new one which creates cv:Mat types form cv_bridge (I had this include, but it was not listed previously).", "I will add more of the code and maybe it makes it easier to help me.", "The error I get with your suggestions is: \u2018cv_bridge::CvImagePtr\u2019 has no member named \u2018create\u2019, \u2018size\u2019 is not a member of \u2018cv\u2019 and suggested alternative:\n/usr/include/boost/mpl/size_fwd.hpp:20:38: note:   \u2018boost::mpl::size\u2019", "I missed to realize that you created all the images using cv_bridge... In that case you will always have to use the structure member image (cv_ptr->image). The proper use of size is as a member of the class... I assume the change now should work... sorry for the previous mistake. (answer is updated)", "SivamPillai, thanks again for the help. As you can see, I'm not used to C++ programming and it is far more difficult than Matlab. :/ Could you please take a look at the \"Update on error\" I added to the question?", "That's a common error and it is related to your image not being properly initialized and pointing to NULL. ", "/ this link maybe of some help. Try to initialize your image before using it."], "answer": [" ", " ", " ", " ", "Hi,", "From your comment it appears that you are using the newer version of openCV where the images are of the cv::Mat type and not IplImage. In that case, where you are using C++ functions of OpenCV there are following replacements for the functions you have used:", "cvCreateImage() = cv::create()", "cvGetSize() = cv::size()", "In general I might have written one of your above lines as:", "cv_ptr_red->image.create(cv_ptr->image.size(), CV_8UC1);", "Further, I believe you may have to include the following header file since you are using CVbridge:", "include \"cv_bridge/cv_bridge.h\"", "You may have to do relevant changes in your manifest file for this change to be accommodated. But before going into details I would want to know if you believe, I took you in the right direction!", "For the complete details of the functions I have provided above you may want to check the following link by searching for relevant functions:", "Whenever you use cv_bridge to create an opencv image it is important to note that image is a member element of the class you create.", "For example, if you create an image like this:", "cv_bridge::CvImagePtr cv_ptr;", "Then the image and all its members could only be accessed by,", "cv_ptr->image, cv_ptr->image.size(), cv_ptr->image.rows, etc.", "Hope this helps.", "Regards,", " ", " ", " ", " ", "SivamPillai already pointed out that there is a difference between the ", " functions that still use ", " and are part of the C interface of OpenCV and the ", " functions that use ", " as data type and are part of the C++ interface. Whenever you get an error where a function needs a ", " argument, you are probably giving it a ", " instead of an ", ". I would recommend looking for the C++ equivalent of that function and completely avoiding ", " as ", " is much more flexible.", "\nYou seem to be confusing the ", " types (defined in cv_bridge) and ", " (defined in OpenCV). If you want to process the image contained in the message after you have it copied and encoded in ", ", you should assign the data to an actual ", " doing something like", " is the member of ", " that contains the actual image data ", ". ", " also contains the ", " of the image message and the encoding. You don't have to worry about that since the encoding (without the channel order though) is already covered in the ", " of the ", ".", "If you use the above line, you should be able to do something like", "This should give you the separate channels. If you display them with ", " you will see three gray images of course.", "You can then apply the threshold function as it is explained ", ". "], "question_code": ["cv_bridge::CvImagePtr cv_ptr_green;\ncv_ptr = cv_bridge::toCvCopy(original_image, enc::BGR8);\ncv_ptr_green->image.create(cv_ptr->image.size(), CV_8UC1);\n", "    // cv_ptr[1] -> cv_ptr_green[1]\n    int from_to[] = {1,1};\n    cv::mixChannels( cv_ptr, 1, cv_ptr_green, 1, from_to, 1);\n", " ", "    #include <ros/ros.h>\n    //Use image_transport for publishing and subscribing to images in ROS\n    #include <image_transport/image_transport.h>\n    //Use cv_bridge to convert between ROS and OpenCV Image formats\n    #include <cv_bridge ..."], "answer_code": ["cvXXX", "IplImage", "cv::XXX", "cv::Mat", "const CvArr*", "cv::Mat", "IplImage", "IplImage", "cv::Mat", "CvImagePtr", "cv::Mat", "cv_ptr", "cv::Mat", "cv::Mat imageColor = cv_ptr->image;\n", "image", "CvImagePtr", "CvImagePtr", "type", "cv::Mat", "std::vector<cv::Mat> channels;\ncv::split(imageColor, channels);\n\n// And then if you like\ncv::Mat imageB = channels[0];\ncv::Mat imageG = channels[1];\ncv::Mat imageR = channels[2];\n", "cv::imshow"], "url": "https://answers.ros.org/question/42061/using-cvgetsize-and-cvsplit/"},
{"title": "iRobot Create and BeagleBoard \"Failed to open port /dev/ttyUSB0\" [closed]", "time": "2012-08-07 23:06:16 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am trying to launch iRobot Create using BeagleBoard xM revC with Ubuntu ARM installed. I downloaded sources of turtlebot stack, fulfilled all needed dependencies for turtlebot_node package and rosmade it. After I run command rosrun turtlebot_node turtlebot_node.py i get following output:", "My USB-serial cable seems to work fine. Here's output of lsusb:", "I tried solution presented ", ". My rule line was:", "I created group turtlebot, added user ubuntu to it and restarted udev. After ls -la /dev/ttyU* i get following:", "So the permissions should be fine. I tried minicom to check if I get anything from Create, and some trash pops out on the screen, so data is transfered through the USB-Serial port (what's more, the fact that I can use minicom without sudo prefix indicates that permission to use ttyUSB0 port is granted to user ubuntu). Cables seems to be fine as well, because i can easily connect to Create from my laptop (after proper permissions change). ", "I would greatly appreciate any help.", ": I properly set serial port properties (baud rate 57600 8N1, no flow control) and was able to receive clear output from my Create. So the system properties (permissions and stuff) are set correctly. I suppose there has to be something wrong with the package itself.", "You say you get trash through the serial port in minicom?  During charging (in default, start-up Passive mode), you should be getting clear, regularly repeating battery status messages - something like: \"bat:   min 124  sec 24  mV 15864  mA -85  deg-C 43\".  Do you get that in minicom?", "I can't do that, because i don't have batteries in my Create. I am powering it from DC power supplier (for now at least). Beside that, the minicom example was brought up to show, that there is a communication between Create and BeagleBoard and there still isn't any when turtlebot_node is launched."], "answer": [], "question_code": ["ubuntu@omap:~$ rosrun turtlebot_node turtlebot_node.py \n/home/ubuntu/ros/turtlebot/turtlebot_node/nodes/turtlebot_node.py:54: UserWarning: roslib.rosenv is deprecated, please use rospkg or rosgraph.rosenv\n  import roslib.rosenv\n[INFO] [WallTime: 1344413441.546115] serial port: /dev/ttyUSB0\n[INFO] [WallTime: 1344413441.554904] update_rate: 30.0\n[INFO] [WallTime: 1344413441.560733] drive mode: twist\n[INFO] [WallTime: 1344413441.566837] has gyro: True\n[INFO] [WallTime: 1344413441.918765] self.gyro_measurement_range 150.000000\n[INFO] [WallTime: 1344413441.924991] self.gyro_scale_correction 1.350000\n[ERROR] [WallTime: 1344413445.109409] Failed to open port /dev/ttyUSB0.  Please make sure the Create cable is plugged into the computer.\n", "ubuntu@omap:~$ lsusb\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. \nBus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. \nBus 001 Device 004: ID 1a86:7523 QinHeng Electronics HL-340 USB-Serial adapter\nBus 001 Device 005: ID 0cf3:1006 Atheros Communications, Inc. TP-Link TL-WN322G v3 / TL-WN422G v2 802.11g [Atheros AR9271]\n", "ATTRS{idProduct}==\"7523\",ATTRS{idVendor}==\"1a86\",MODE=\"666\",GROUP=\"turtlebot\"\n", "ubuntu@omap:~$ ls -la /dev/ttyU*\ncrw-rw-rw- 1 root turtlebot 188, 0 Dec 31  1969 /dev/ttyUSB0\n"], "url": "https://answers.ros.org/question/40830/irobot-create-and-beagleboard-failed-to-open-port-devttyusb0/"},
{"title": "Raspberry Pi optimization [closed]", "time": "2012-08-19 10:24:24 -0600", "post_content": [" ", " ", " ", " ", "Trying to get ROS working on Raspberry Pi.  Encountered problem here: ", "/", "Rather than leave well enough alone I found ", "typedef qreal\nTypedef for double on all platforms except for those using CPUs with ARM architectures. On ARM-based platforms, qreal is a typedef for float for performance reasons.", "That got me wondering how to optimize ROS for ARM platforms in general.  Is the solution mentioned in the first link the best approach for this specific type of problem?  Is there an automatic way to replace doubles with floats in areas where the extra precision is not needed, but the extra performance is; based on the target platform?  Would it be OK to define floats and doubles both equal to the same length for ARM targets?", "I'm a noob with ROS and Rpi and there will soon be huge numbers of us looking to combine the two in the most braindead fashions possible.  The sooner ARM optimizations get pushed upstream the less pain everyone will feel when the wave of new users breaks.", "I'm curious about this as well, especially with using a Raspberry Pi with the Turtlebot to save on power consumption and simplify charging."], "answer": [], "url": "https://answers.ros.org/question/41815/raspberry-pi-optimization/"},
{"title": "Driver For LMS 511 [closed]", "time": "2012-09-20 01:36:48 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello Everyone,\nWe've recently purchased a new LMS 511 but they have only provided a windows driver version whilst we need the Linux so I'm here to ask if there's a driver for that device on Linux or either way guide through building my won driver using Ethernet RS232 or even USB\nAS I have seen there's no driver for the 5xx and yet it's the most interesting and it's more powerful and give better results\nThanks  ", "Is there anything new on this topic? Is current driver for LMS2xx compatible with LMS5xx in anyway?", " After trying a number of driver and packages for our SICK LMS511 the best one I found to work with my ROS Indigo was from\n ", "All I had to do was initialize a workspace, copy the folder laser_node into the src folder and then catkin_make.\nHope this helps..."], "answer": [], "url": "https://answers.ros.org/question/44206/driver-for-lms-511/"},
{"title": "Pioneer Gripper is stuck [closed]", "time": "2012-09-10 14:56:32 -0600", "post_content": [" ", " ", "Hello,", "We have 2 pioneers 3-DX, both of them have grippers. Everything is same on these two robots. Both of them have on-board Computers with Linux operating system. The robot firmware is P2OS for both of them. ", "The problem is that the gripper of one of the robots does not work. The gripper is in the lower position and it is close. But its state does not change in respond to control commands. ", "One symptom is that when we turn the robot on, its gripper does not go to origin state. The other robot (with working gripper) has another behaviour, if gripper is open or is in lower position, it goes to origin after turning on the robot.\nThe problem continue when we try to send control commands for gripper. ", "We checked the power voltage for gripper (yellow-black cable). It shows it has ~12 volts. The data cable is plugged in correctly. ", "I wonder if there is anything else that I should check at this level. ", "thank you in advance.", "This is not ROS releated. You need a Pioneer forum. "], "answer": [], "url": "https://answers.ros.org/question/43566/pioneer-gripper-is-stuck/"},
{"title": "Neobotix navigation rotation behavior [closed]", "time": "2012-09-18 04:11:30 -0600", "post_content": [" ", " ", "I am trying to get the Neobotix LP-655 platform (not omni-directional) to move to a goal in rviz. The global plan seems to go directly to the goal location. I would expect a curved plan, as the platform only can rotate and not move sideways. Furthermore it does not create a new plan if there is something unknown obstacle blocking the path. Instead it just rotates in front of the obstacle! ", "The platform sometime manage to go to the desired goal, but it shakes a lot, and it often rotate before moving towards the goal and when it is very close to the goal. I have uploaded two videos of this behavior: ", "I have tested my odomotry using this guide: ", " and the scans seems pretty good. ", "Because I am a new use to the site, I cannot upload my config files, so I am sorry that I have to post them here:", "obstacle_range: 12.5", "raytrace_range: 30.0", "footprint: [[0.50,0.45],[0.50,-0.45],[-0.50,-0.45],[-0.50,0.45]]", "inflation_radius: 0.55", "observation_sources: laser_scanner", "laser_scanner: {sensor_frame: /base_link, expected_update_rate: 0.0, observation_persistence: 0.0, data_type: LaserScan, topic: scan, marking: true, clearing: true}", "global_costmap:", "global_frame: /map", "robot_base_frame: /base_link", "update_frequency: 1.0", "static_map: true", "rolling_window: false", "transform_tolerance: 2.5", "cost_scaling_factor: 10.0", "unknown_cost_value: 0", "publish_voxel_map: false", "lethal_cost_threshold: 100", "map_topic: map", "local_costmap:", "global_frame: /map", "robot_base_frame: /base_link", "update_frequency: 1.0", "publish_frequency: 0.0", "static_map: false", "rolling_window: true", "width: 20.0", "height: 20.0", "resolution: 0.1", "transform_tolerance: 2.5", "origin_x: 0.0", "origin_y: 0.0", "map_type: costmap", "track_unknown_space: false", "TrajectoryPlannerROS:", "max_vel_x: 0.35", "min_vel_x: 0.001", "max_rotational_vel: 0.3", "min_in_place_rotational_vel: 0.001", "escape_vel: -0.1", "acc_lim_th: 0.05", "acc_lim_x: 0.05", "acc_lim_y: 0.", "holonomic_robot: false", "path_distance_bias: 0.6", "goal_distance_bias: 0.8", "occdist_scale: 0.01", "heading_lookahead: 0.325", "heading_scoring: false", "heading_scoring_timestep: 0.8", "sim_time: 3.0", "sim_granularity: 0.025", "dwa: false", "xy_goal_tolerance: 0.2", "yaw_goal_tolerance: 0.1", "latch_xy_goal_tolerance: true", "If I set e.g. the resolution of the map to 0.05, my CPU has not got enough power to calculate the path and it misses its desired loop. Could this be a problem? Or that my update_frequency is set to only 1.0?", "I dont know what to do, to get the platform to navigate smoothly to a goal without any shaking or rotating?", "/Christian"], "answer": [], "url": "https://answers.ros.org/question/44047/neobotix-navigation-rotation-behavior/"},
{"title": "ROS failure after system crash", "time": "2012-10-07 21:24:27 -0600", "post_content": [" ", " ", "Hi. I got a problem after my computer crashed. A few days ago I shut off the power of my computer by accident and caused a problem like \"\nmount: mounting /dev on /root/dev failed: No such file or directory\nmount: mounting / sys/ on root/sys failed: No such file or directory\nmount: mounting /proc on /root/proc failed: No such file or dirctory\"\nas a result I couldn't get into the ubuntu operating system. I fixed it following the guidance found on someone's blog, with the command \"sudo fdisk -l\".", "However, there comes an other problem which is quite likely to be caused by the former accident. That is my ROS no longer works, as all ROS related commands are now invalid, including the command \"roscore\", because they are \"not found\". But I checked the file folders, the ROS folder seems intact, and I checked the bashrc, there was a line \"source /opt/ros/electric/setup.bash\", I am completely muddled....", "I wonder if you could help me solve this with out reinstalling ROS, because I am afraid reinstalling ROS may erase some important files or settings. Thanks. "], "answer": [" ", " ", "Can you check the following items:", "Is there any error when running \"source /opt/ros/electric/setup.bash\"?", "What is the output given by \"which roscore\"? It should be \"/opt/ros/electric/ros/bin/roscore\".", "Do a less \"/opt/ros/electric/ros/bin/roscore\" and see if you can open the file.", "(do not do this as root)", "It depends on your ROS setup but there is very high chances that removing the ROS packages, re-installing them and then rebuilding all the packages compiled from source will fix your problem without losing any data. Always make a backup first if you are unsure."], "url": "https://answers.ros.org/question/45282/ros-failure-after-system-crash/"},
{"title": "environment_server doesn't subscribe to collision_map", "time": "2012-10-09 21:11:43 -0600", "post_content": [" ", " ", " ", " ", "I want to generate a collision map from kinect's pointcloud and use this info to do a collision-free arm navigation. so i write a node to receive pointcloud from kinect and publish collison_map with the topic \"collision_map_occ\". i think i've done this correctly because i can see these collision map green boxes in rviz. but it seems that environment_server doesn't subscribe to the collision_map topic. i'm sure that the param use_collision_map is set to true. does anybody know why? ", "besides, according to this ", ", \"If the use_collision_map parameter for the environment_server is set to true there must be a supplier of the CollisionMap or the environment_server will not start\", but after i stop the collision_map publisher, the environment_server can still start successfully.", "btw, i'm using Fuerte on Ubuntu12.04", "Can you post your edit as an answer to your own question, and then accept it? That way it will show up as \"answered\" in the list."], "answer": [" ", " ", "After digging into the src of environment_server, i find that to make environemnt_server subscribe to collision_map, one must set both use_monitor and use_collision_map to true. so i just leave this question here in the hope that it can help other people."], "url": "https://answers.ros.org/question/45467/environment_server-doesnt-subscribe-to-collision_map/"},
{"title": "iRobot Open Interface available for Roomba7xx? [closed]", "time": "2012-09-03 05:54:28 -0600", "post_content": [" ", " ", " ", " ", "I am currently using a couple of Roomba5xx powered Turtlebot-like robots, which are doing fine.\nBut as we are looking towards future developments, wondering if Roomba7xx still supports the Roomba Open Interface specification (which is the successor of the previous SCI).\nSo far I was not able to find a list of supported Roombas...", "Has anyone been using the serial interface of Roomba7xx with OI spec so far?", "Thanks in advance.\nCheers"], "answer": [], "url": "https://answers.ros.org/question/42900/irobot-open-interface-available-for-roomba7xx/"},
{"title": "Turtlebot with roomba 780. Error message: [Distance, angle displacement too big, invalid readings from robot. Distance: 1.84, Angle: -21.02].", "time": "2012-10-16 22:19:56 -0600", "post_content": [" ", " ", " ", " ", "I am trying to run a roomba 780 but get an error message. ", "[Distance, angle displacement too big, invalid readings from robot. Distance: 1.84, Angle: -21.02]. Please check that the Create is powered on and that the connector is plugged into the Create.", "After running the roomba launch file i get the following error message below. Also when I run this launch file I can see that I actually am communicating with my roomba as the roomba turns of, and will not start again until i lift it of the floor. It is as it goes into some kind of protection mode?! I use a irobot usb to din cable.", "I know that the 780 should use  a baud rate of 115200 which is the same as for the 500 models.", "Anyone seen this error with the angle message before?", "Thanks", "Well i am not sure if i understand that answer. Or is that a spam answer?", "Well i am not sure if i understand that answer. Or is that a spam answer?", "Probably it was spam. Deleting it...", "Yes I will try to do that. I have from some other forums that the communication should be similar, but then still similar is not exactly as being the same! The information is not really open as I have seen it but maybe Irobot will want to shear it. I will post it then here. I also have made a kinect mod so you can use it from one usb cable withouth a power adapter (found from another forum) so hope i can contribute something back also : -)"], "answer": [" ", " ", "The problem is probably that the communication protocol of the roomba 780 is slightly different from the 500 series. You will have to debug the problems manually. I would first try to get the protocol specifications of the 500 series and the 780 from iRobot and compare them. Then you might need to patch the turtlebot node to add support for your robot. Patches are of course welcome. "], "question_code": ["The launch file I am using is:\n<launch>\n  <node name=\"turtlebot_node\" type=\"turtlebot_node.py\" pkg=\"turtlebot_node\">\n    <rosparam>\n      port: /dev/ttyUSB0\n      publish_tf: True\n      robot_type: roomba\n      has_gyro: False\n    </rosparam>\n  </node>\n</launch>\n", "turtlebot@turtlebot:~$ roslaunch rob.launch\n... logging to /home/turtlebot/.ros/log/bc774c0e-15f0-11e2-99a6-00215c5e4e7b/roslaunch-turtlebot-4549.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://turtlebot:53600/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /rosversion\n * /turtlebot_node/has_gyro\n * /turtlebot_node/port\n * /turtlebot_node/publish_tf\n * /turtlebot_node/robot_type\n\nNODES\n  /\n    turtlebot_node (turtlebot_node/turtlebot_node.py)\n\nauto-starting new master\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[master]: started with pid [4566]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to bc774c0e-15f0-11e2-99a6-00215c5e4e7b\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[rosout-1]: started with pid [4580]\nstarted core service [/rosout]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[turtlebot_node-2]: started with pid [4592]\n/opt/ros/fuerte/stacks/turtlebot/turtlebot_node/nodes/turtlebot_node.py:54: UserWarning: roslib.rosenv is deprecated, please use rospkg or rosgraph.rosenv\n  import roslib.rosenv\n[ERROR] [WallTime: 1350213455.602533] Failed to contact device with error: [Distance, angle displacement too big, invalid readings from robot. Distance: 1.84, Angle: -21.02]. Please check that the Create is powered on and that the connector is plugged into the Create.\n[ERROR] [WallTime: 1350213462.009986] Failed to contact device with error: [Distance, angle displacement too big, invalid readings from robot. Distance: 1.84, Angle: -21.02]. Please check that the Create is powered on and that the connector is plugged into the Create.\n^C[turtlebot_node-2] killing on exit\n[rosout-1] killing on exit\n[master] killing on exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\n"], "url": "https://answers.ros.org/question/46058/turtlebot-with-roomba-780-error-message-distance-angle-displacement-too-big-invalid-readings-from-robot-distance-184-angle-2102/"},
{"title": "How to add statements through json_prolog?", "time": "2012-11-01 20:15:22 -0600", "post_content": [" ", " ", "I ran tutorial.launch in knowrob_tutorial to start services that accept knowrob queries", "And now I can query knowrob from my program like this:", "However, I want to make use of the power of knowrob, that is, I want to add statements to knowrob knowledge base and make use of the reasoning mechanisms in knowrob. But I found there's only query I can use in ", ".", "If I want to use pizza.owl I generated somewhere:", "The solution I got is\n    {'A': '", "}", "But I want to get ", "since the subclasses are defined in pizza.owl\n", "How can I achieve that?", "Thanks in advance. "], "answer": [" ", " ", " ", " ", "Are you sure that PizzaTopping is in the KnowRob namespace?", "You query for knowrob:'PizzaTopping', but probably want to ask for  ", "Edit:\nIn your query, you need to set the correct IRI of the OWL class you are asking for. You find the base IRI of your ontology in the head of your OWL file in the line that looks like  xmlns=\"", "\".", "Assuming you used the standard pizza.owl, you therefore need to query for:", "If you would like to abbreviate the long IRIs, you can define a namespace, e.g. in one of your init.pl using the rdf_db:rdf_register_ns directive. Have a look at the init.pl in ias_knowledge_base for an example.", "The pizza.owl is generated by Protege by myself, in this situation how can I add this owl file to knowrob knowledge base?", "If you ask for OWL information, you need to specify the correct IRI, i.e. the same that is used in your OWL file. The KnowRob core concepts have the base IRI ", ", abbreviated as knowrob: Your pizza.owl probably uses a different namespace that needs to be used here.", "Yeah I think the problem is on prolog.query(\"owl_subclass_of(A, knowrob:'PizzaTopping')\"), I should change knowrob to another namespace. But since I don't know what should I use to replace knowrob, is there any functions I can check namespaces? Or is there default namespace for the parsed pizza.owl?"], "question_code": ["<launch>\n    <node name=\"json_prolog\" pkg=\"rosprolog\" type=\"run_with_prolog_env\"\n    args=\"ias_semantic_map $(find json_prolog)/bin/json_prolog knowrob_tutorial\" />\n</launch>\n", "#!/usr/bin/env python\n\nimport roslib\nroslib.load_manifest('json_prolog')\nimport rospy\nimport json_prolog\n\nrospy.init_node('query_json_prolog')\n\nprolog = json_prolog.Prolog()\nquery = prolog.query(\"owl_has(knowrob:'Drawer1',P,O)\")\n\nfor sol in query.solutions():\n    print sol \n\nquery.finish()\nrospy.spin()\n", "#!/usr/bin/env python\n\nimport roslib\nroslib.load_manifest('json_prolog')\nimport rospy\nimport json_prolog\n\nrospy.init_node('test_json_prolog')\nprolog = json_prolog.Prolog()\n\nquery = prolog.query(\"owl_parser:owl_parse('/rosfuerte/home/project/test/owl/pizza.owl', false, false, true)\")\nquery2 = prolog.query(\"owl_subclass_of(A, knowrob:'PizzaTopping')\")\n\nfor sol in query2.solutions():\n    print sol \n\nquery.finish()\n", "{'A': 'xxx#PizzaTopping'}\n{'A': 'xxx#CheeseTopping'}\n{'A': 'xxx#MeatTopping'}\n{'A': 'xxx#SeafoodTopping'}\n{'A': 'xxx#VegetableTopping'}\n"], "url": "https://answers.ros.org/question/47289/how-to-add-statements-through-json_prolog/"},
{"title": "What's the most highest ampere when PR2 comsume the most when charging?", "time": "2012-08-07 00:00:07 -0600", "post_content": [" ", " ", " ", " ", "How to calculus ampere which PR2 need the highest?", "What action will cause PR2 consume the most power?", "Thank you~"], "answer": [" ", " ", "You can find some information here in paragraph 1.2.4.\n", " \nAnd here the current per joint:\n"], "url": "https://answers.ros.org/question/40734/whats-the-most-highest-ampere-when-pr2-comsume-the-most-when-charging/"},
{"title": "does kinect for windows work on turtlebot?", "time": "2012-10-18 01:37:44 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I have been trying to install kinect for windows (", ") on the turtlebot (and not xbox kinect)", "I have been eating out my brains for last 2 days trying the openni driver to work, and still the device is not detected. The kinect is blinking and getting power from the turtlebot sensor board. But after multiple trial I am unable to get it work. ", "Does anyone has experience of getting it worked on turtlebot? Please reply , else I will have to get a Xbox kinect as a last resort. ", "PS- Turtlebot page should mention this point to new users. "], "answer": [" ", " ", "I don't believe it does.  Check out my answer to your other question."], "url": "https://answers.ros.org/question/46173/does-kinect-for-windows-work-on-turtlebot/"},
{"title": "How do you exit back to safe mode after running a roslaunch file?", "time": "2012-10-21 14:06:29 -0600", "post_content": [" ", " ", " ", " ", "Hi", "I am using a roomba to run turtlebot on, I have a specific launch file that I run, but I still haven't figured out how to stop it?! To end the session i typically kill the running processes but then robot is still in receiving mode and I have to do a hard reset to make the roomba work again as normal (we are using it as a vacuum cleaner so I don't like resetting it every time i use it)", "Is there any real turning of command so to say?", "Thanks", "Thanks tfoote, not really sure how I would use the ~set_operation_mode command, atleast did not manage to execute it. I have no powerboard, and connecting to my roomba 780 directly with the irobot serial usb cable but havent really gotten the dashboard to show me any status from the robot, is there something special detail there to take notice to make it to work?", "Aha, that was a clever way. Will try that out today, because still have to do alot of manual resets! Thanks", "Jbuesch, what kind of launch file do you use then for your roomba? I tried your trick/idea but i then just get my roomba to go in a loop into passive mode all the time... when you say put it as the last line of spin() that means to put it just below the r.sleep() command right?"], "answer": [" ", " ", "First I think you need to be in passive mode for it to drive itself.  And secondly there are two options, one is to put it into passive mode using the ", " before you shutdown the TurtleBot software.  The second is to call the rosservice directly with the passive command  ", " (Note the turtlebot_driver needs to still be running for either of these solutions to work.)", " ", " ", "I had the same problem and altered the turtlebot_node.py by adding self._robot_reboot() in the last line of spin(). By doing so, roomba is soft-rebooted in the end (after killing the process from terminal).", "This way you cannot forget to put it back to passive mode my rosservice."], "url": "https://answers.ros.org/question/46411/how-do-you-exit-back-to-safe-mode-after-running-a-roslaunch-file/"},
{"title": "TurtleBot ROS Fuerte RC1 & Roomba Model 533", "time": "2012-11-01 01:28:34 -0600", "post_content": [" ", " ", " ", " ", "I finally got my hands on a Roomba Model 533, best I can do right now.\nI have been experimenting with electric and so far I have been able to\npartially get the roomba_500_series to work, intermittent kinect and camera, mostly\nbecause I have only found 'partial tutorials'.", "I have also been able to teleop from keyboard using the tutorials but it keeps moving and won't stop. I had to modify the roomba_teleop_key python executable to make it stop.", "My question is: Is it possible to get the 533 Roomba to work with TurtleBot ROS Fuerte RC1 iso or is it just for those lucky enough to have the fundage to purchase the Create??", "Anyone have any experience along these lines that would have to patience to assist?", "Ok I have fuerte installed to the hard drive and the minimal.launch file is :", "<launch>\n  ", "<arg name=\"urdf_file\" default=\"$(find xacro)/xacro.py '$(find turtlebot_description)/robots/turtlebot.urdf.xacro'\"/>\n  ", " ", "<include file=\"$(find turtlebot_bringup)/app_manager.launch\"/>", "\n  <node pkg=\"turtlebot_node\" type=\"turtlebot_node.py\" name=\"turtlebot_node\" output=\"screen\" respawn=\"true\" args=\"--respawnable\">\n    <rosparam>\n      port: /dev/ttyUSB0\n      publish_tf: True\n      robot:type: roomba\n      has_gyro: False\n    </rosparam>\n  </node>", "\n  <node pkg=\"turtlebot_node\" type=\"laptop_battery.py\" name=\"turtlebot_laptop_battery\">\n        ", "\n  </node>", "<node pkg=\"robot_state_publisher\" type=\"state_publisher\" name=\"robot_state_publisher\" output=\"screen\">\n    ", "\n    \n  </node>", "<node pkg=\"diagnostic_aggregator\" type=\"aggregator_node\" name=\"diagnostic_aggregator\" &gt;=\"\" <!--=\"\" load=\"\" the=\"\" file=\"\" you=\"\" made=\"\" above=\"\" --=\"\">\n    <rosparam command=\"load\" file=\"$(find turtlebot_bringup)/config/diagnostics.yaml\"/>\n  </node>\n</launch>", "I am now getting the following error in rxconsole -  Failed to contact device with error: [Error reading from SCI port. No data.]. \nPlease check that the Create is powered on and that the connector is plugged \ninto the Create", "I purchased the cable from irobot and the roomba is turned on...", "I also checked the following", "turtlebot@turtlebot-Presario-CQ56-Notebook-PC:/dev$ ls /dev/ttyUSB*\n/dev/ttyUSB0", "turtlebot@turtlebot-Presario-CQ56-Notebook-PC:/dev$ dmesg | grep ttyUSB\n[   57.669696] usb 5-1: FTDI USB Serial Device converter now attached to ttyUSB0"], "answer": [" ", " ", "SOLVED!!  I was copying the minimal.launch example file the you suggested above and after 3 reimages I was able to fine another webpage where someone else had same issue and one person in the forum stated - you have a typo in statement robot:type: roomba\nIt should be robot_type: roomba   !!!", "a single underscore was the reason for the non-communication with SCI port!!!", "show link - ", "example in middle of page of minimal.launch should be   robot_type: roomba", "Thanks Lorenz for you help!!", " ", " ", "I would not suggest to use the roomba_500_series stack because of several implementation related problems. You were facing the most severe one I guess, it does not implement a watchdog that stops the robot in case it does not receive any velocity commands anymore. ", "You can use the turtlebot driver with the roomba 500 series so I expect the turtlebot iso to work as well. You just need to change the main turtlebot launch file to disable the robot_pose_ekf (I guess you don't have an IMU), to disable the IMU in turtlebot_node, to enable tf publication directly in the turtlebot_node and to change the robot type to roomba. You can find an example launch file ", "."], "url": "https://answers.ros.org/question/47236/turtlebot-ros-fuerte-rc1-roomba-model-533/"},
{"title": "problem with gmapping [closed]", "time": "2012-11-21 13:07:21 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to use gmapping stack to do some slam research. But it seemed like gmapping is using the laser scans believing that the robot is still from the map built. I've checked odometry, which is accurate. I've checked the tf(odom->base_link base_link->base_laser) via not launching gmapping, only view laser and tf in rviz, which seemed also correct when I move the robot by hand. But if I choose \"/base_link\", \"/odom\" or \"/map\" as the fixed frame in the \"global option tag\", the \"LaserScan\" tag will change red between green from time to time. I check the error message in LaserScan tag, and be told \"unknown reason for transform failure\". I mean, sometimes it is correct(green), but also incorrect sometimes.\nHow can I fix that problem?", "How does your tf tree look like? How do you publish the transforms?", "the tree is just like this: map->odom->base_link->base_laser"], "answer": [], "url": "https://answers.ros.org/question/48786/problem-with-gmapping/"},
{"title": "Activating shading for a .stl based environment in gazebo", "time": "2012-01-20 01:23:40 -0600", "post_content": [" ", " ", " ", " ", "I built a simple rolling landscape terrain using the sculpt tool in Blender and exported this as STL. Using the terrain in a launch file works ok, but I can't figure out how to activate proper shading so visualization looks nice. I tried many of the available gazebo materials (PR2/Grey1, Gazebo/PioneerBody, Gazebo/White, Gazebo/GrassFloor) but all those resulted in a monochromatic landscape.", "The relevant parts of the .world file are:", "Screenshot (using PR2/Grey1 material, Gazebo/GrassFloor looks the same, only green)", "The monochromatic visualization obviously makes judging distances of terrain features impossible. Shadows for the quadrotor UAV work (but shading apparently does not work).", "The .stl, world and launch file are also available in the ", " package and can be started using", "/edit: So I found out that terrain as well as quadrotor do not receive illumination at all, but if I spawn spheres or other objects via the gazebo toolbar, those receive proper illumination.", "Update: I tried playing around with materials and found that there is a difference like night and day (literally ;) ) between built-in primitives and mesh files.", "Consider this extract from a URDF file:", "Looks like this in gazebo:\n", "The cylinder apparently receives correct illumination.", "If I now use the following line instead of the cylinder (everything else stays the same, most notably the material!)", "the same scene looks like this:\n", "Can someone explain this? The mesh looks perfectly fine in Blender, Meshlab etc., so I doubt the normals are wrong. It seems very wrong that materials show such differences between being used for built-in primitives and external meshes."], "answer": [" ", " ", "I define my own materials and set them to image files that are non-monochromatic. It helps a bit with this problem. Take a look at the Gazebo.material file in ", " for an example. In your package, you can set it up to export materials to Gazebo (because you don't want to modify the simulator_gazebo folder if you didn't install from source). You can find that tutorial ", ". ", " ", " ", "I had the exact same problem today. STL files exported by Blender somehow do not receive shading when spawned inside Gazebo. Tried reversing/recalculating normals to no avail. Just import the STL into Meshlab and export it without doing anything. This enabled the shading for the mesh. ", "Worked for me, thanks! I thought it was the \"Unify Duplicated Vertices\" option that appears when an STL is loaded in MeshLab that fixes it, but I tested without and it's not."], "question_code": ["  <rendering:ogre>\n    <ambient>0.5 0.5 0.5 0.5</ambient>\n    <sky>\n      <material>Gazebo/CloudySky</material>\n    </sky>\n    <grid>false</grid>\n    <maxUpdateRate>10.</maxUpdateRate>\n    <!--<shadowTechnique>none</shadowTechnique>-->\n    <shadows>true</shadows>\n  </rendering:ogre>\n\n   <model:physical name=\"landscape\">\n   <xyz>0 0 0</xyz> \n   <rpy>0 0 0</rpy>\n   <static>true</static>\n   <body:trimesh name=\"landscape_body\">\n     <geom:trimesh name=\"landscape_geom\">\n       <scale>1.0 1.0 1.0</scale>\n       <mesh>rolling_landscape_120m.stl</mesh>\n       <visual>\n         <scale>1.0 1.0 1.0</scale>\n         <mesh>rolling_landscape_120m.stl</mesh>\n     <genTexCoord>true</genTexCoord>\n     <material>Gazebo/GrassFloor</material>\n       </visual>\n     </geom:trimesh>\n       </body:trimesh>\n  </model:physical>\n", "roslaunch hector_gazebo_worlds rolling_landscape_120m.launch\n", "  <link name=\"base_link\">\n    <inertial>\n      <mass value=\"1.316\" />\n      <origin xyz=\"0 0 0\" />\n      <inertia ixx=\"0.0128\" ixy=\"0.0\" ixz=\"0.0\"\n      iyy=\"0.0128\" iyz=\"0.0\" \n        izz=\"0.0218\" />\n      </inertial>\n\n      <visual>\n        <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n        <geometry>        \n           <cylinder radius=\"0.4\" length=\"0.2\"/>\n        </geometry>\n        <material name=\"DarkGrey\" >\n          <color rgba=\"0.3 0.3 0.3 1\"/>\n        </material>\n      </visual>\n\n      <collision>\n        <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n        <geometry>\n          <mesh filename=\"package://hector_quadrotor_urdf/meshes/quadrotor/quadrotor_base.stl\"/>\n        </geometry>\n      </c![image description](http://img842.imageshack.us/img842/6112/gazebodisk.png)ollision>\n\n    </link>\n\n    <gazebo reference=\"base_link\">\n      <material>Gazebo/White</material>\n      <turnGravityOff>false</turnGravityOff>\n    </gazebo>\n", " <mesh filename=\"package://hector_quadrotor_urdf/meshes/quadrotor/quadrotor_base.stl\"/>\n"], "answer_code": ["simulator_gazebo/gazebo/gazebo/share/gazebo/Media/materials/scripts"], "url": "https://answers.ros.org/question/12704/activating-shading-for-a-stl-based-environment-in-gazebo/"},
{"title": "interactive markers not fully pitching", "time": "2012-11-05 06:57:12 -0600", "post_content": [" ", " ", " ", " ", "Maybe it's my lack of a fundamental understanding of quaternions but I can't seem to pitch an interactive marker beyond 1.57rad. I'm building my own robot model using interactive markers but have also run through the tutorial on basic control of interactive markers and none of them seem to want to pitch past PI/2. ", "Is this a bug or am I missing something in the mathematics of a quaternion calculation? I'm using tf to convert from a quaternion to euler.", "Edit 1: I'm running Fuerte on Precise. I don't generate the quaternion, rather, I receive the pose via callbacks that are registered to the interactive markers which I then convert using tf:", "The 'Y' value of the quaternion seems to be valid, so I must be missing something with the math behind the 'W' (orientation) value. Basically when I pitch the interactive marker beyond 1.57rad, the pitch starts going back down to 0 as if the quaternion to euler conversion doesn't utilize the orientation?", "Edit 2: The following are some images captured and smashed together, overlook the bad quality. I'm using the green ring to pitch the interactive marker down.", "Image 1: pitching hand to show result is as expected. example case.", "Image 2: pitched hand ~1.57rad. this seems to be the limit for correct pitch values. Y and W from the quaternion still look good.", "Image 3: shows the hand (clearly) pitched beyond 1.57rad but the pitch value seems to go back to 0. Y from the quaternion still seems valid (range is +/- 1).", "Unfortunately, your question does not provide enough information to help. How is your setup? How are you generating the quaternion? How are you converting it? Which ros distro are you using? Code snippets would also help. Have a look at ", "To me, your code looks good. Can you also add the quaternion and the values of rpy for a case that is failing? Maybe also a screenshot of the interactive marker?"], "answer": [" ", " ", "I do think the values you get are correct. The roll and yaw values are close to PI. Thus the yaw value does make sense. If you convert the quaternion of your last example to an axis angle representation, you will see that it is essentially a rotation along the y axis with either a negative axis and an angle of about 2.72 or a positive axis and an angle of about -2.72.", "Have a look at ", " to find out more about the different representations of rotations. I would not suggest to use euler angles because of several problems including the ", " and ambiguities (e.g. the order in which roll pitch and yaw rotations are applied matters).", "thanks for the pointers, those are both very interesting reads and gimbal lock isn't anything I've heard of before so thanks for pointing that out. unfortunately the IK plugin I have requires euler to solve. I found a workaround (hack, really) for the time being until we can alter the IK. "], "question_code": ["processInteractiveObjectFeedback(const visualization_msgs::InteractiveMarkerFeedbackConstPtr &feedback) \n{\n      tf::Quaternion q;\n      tf::quaternionMsgToTF(feedback->pose.orientation, q);\n      double r, p, y;\n      tf::Matrix3x3(q).getRPY(r, p, y);\n}\n"], "url": "https://answers.ros.org/question/47511/interactive-markers-not-fully-pitching/"},
{"title": "Is ROS for me?", "time": "2012-11-28 13:09:10 -0600", "post_content": [" ", " ", " ", " ", "I am building a hobby robot from a old powered wheelchair. For the brains I have built a mini PC (intel core i3). For the motor controllers I am using two 25amp pololu simple motor controllers that can controlled over USB. I have a USB video camera that I'll use as well as a high power wifi radio.  I consider myself to be a beginner at the software side of robotics. I have programmed some with the arduino compiler and visual basic. For right now I would like to be able to control the robot remotely and see what it sees. Just something to play around with until I can program more complex actions. "], "answer": [" ", " ", " ", " ", "ROS is almost certainly a ", " choice. Many hobbyists use it effectively.", "Whether it's your ", " option is for you to decide. Most of us here use it heavily and like it, so our responses will tend to favor ROS.", "You should understand that ROS is complex. It provides many useful features, but learning it will take a while. ", "As ", " said: work through the ", ". They will give you a clear understanding of the basics. If you like what you see, you'll be well on your way. If you don't, you'll know to look elsewhere.", " ", " ", "w.w.w.pololu.com/catalog/product/1383. \nW.w.w.pololu.com/docs/0J44/7\nI have only used the serial TTL communication side of these in connection with an arduino. For my new build of this robot I wanted to use the USB interface. You said I should be able to use ros. I believe I have installed ros under Ubuntu 12.04, but I'm not really sure where to start. \nThanks for any help you can give me:-) ", "Please do not create answers for discussion or comments. This is not a forum. Instead, either edit your original post or use the comment functionality.", "If you have arduino code running, there is rosserial, which allows you to easily connect an arduino with ROS. You should be able to control your robot via the arduino then. Camera drivers and generic teleop nodes are also available. Start with the beginner's tutorials on ", ".", " ", " ", "I don't know about your motors; assuming there are ROS drivers (or that you can write your own; you say they're \"controllable over USB\", which means the answer is almost certainly yes), then the answer is \"yes, absolutely\"."], "url": "https://answers.ros.org/question/49348/is-ros-for-me/"},
{"title": "Dynamixel motors not found", "time": "2012-10-23 06:33:09 -0600", "post_content": [" ", " ", " ", " ", "I'm going through ", " about connecting/ running Dynamixel servos through ROS (these are AX-12 servos).\nIn step 3.Step 2: Create a launch file for the manager node, I created a .launch file as instructed by copy-pasting the supplied code in to a new file named controller_manager.launch.\nAfter executing the command \"roslaunch dynamixel_tutorials controller_manager.launch\", I receive an error. The error states \n[FATAL] [WAllTime: 13151009139.949289] pan_tilt_port: No motors found", "The error log file referenced states the same information, the motors can not be \"pinged\" and therefore the process is killed.", "Running the command \nls -la /dev/ttyUSB*", "Gives me crw-rw-r-- 1 turtlebot 188, 0 Oct 23 11:18 /dev/ttyUSB0", "lsusb gives me\nBus 002 Device 003: ID 0403:6001 Future Technology Devices...", "So it looks like the computer \"sees\" the USB connector. I've even tried a different motor and different power supply... ", "Any ideas?", "What version of ROS are you using?"], "answer": [" ", " ", "I have updated the tutorial to work with Electric and Fuerte version of the dynamixel_motor stack.", " ", " ", "The USB port isn't set to writable. Run ", " to make it writable by all users. ", "You can add a udev rule to make USB ports be writable by all users by default. Make a file called ", " in ", " and add the following to it:"], "answer_code": ["sudo chmod a+rw /dev/ttyUSB0", "75-usb-rw.rules", "/etc/udev/rules.d"], "url": "https://answers.ros.org/question/46612/dynamixel-motors-not-found/"},
{"title": "Global costmap disappears [closed]", "time": "2012-11-19 22:47:27 -0600", "post_content": [" ", " ", " ", " ", "Hi", "I'm using the navigation stack for moving a robot. I use the AMCL for localizing the robot in the map, and the map_server for loading a reference map.", "My problem is that the global costmap goes disappearing as the robot moves. So, sometimes, when I set a goal the robot cant find an optimal path because it has no reference map.", "Here you have a picture of what it's happening:\n", "The dark green pixels are the global costmap, and the light ones the local costmap. Look at the bottom of the image: when the robot moves from the center to the top, this part of the map dissapears. And if the robot had moved from the center to the right, the left part of the map would have disappeared.", "Here you have my global costmap configuration:", "I don't know what can be happening. Can anyone help me?", "I'm using Ubuntu 12.04 and ROS Fuerte"], "answer": [], "question_code": ["global_costmap:\n  global_frame: /map\n  robot_base_frame: /base_footprint\n  update_frequency: 5.0\n  static_map: true\n  resolution: 0.20\n"], "url": "https://answers.ros.org/question/48643/global-costmap-disappears/"},
{"title": "where do I need to start", "time": "2012-12-13 13:20:07 -0600", "post_content": [" ", " ", " ", " ", "I am very new to ros and i am building a robot from an old powered wheelchair containing a full PC, two USB high power motor controllers, USB camera on a RC pan and tilt, high power WiFi radio. my goal for this robot for now is remote control over the WiFi.  I have installed ros and have run through most of the beginner tutorials. my question is, where do i start. what are the steps that i need to follow to make it remote controlled. i have no software written for this robot. ", "the motor controllers that i am using are simple motor controllers 24v23 from pololu. They do have drivers for linux.", "Ok, I maybe missing something very simple, how do i install gscam and gstreamer?"], "answer": [" ", " ", "For simple remote control you only need to be able to make your motors move from within a ROS node by sending commands to your motor controllers. I don't know what type of motor controllers you have, but you can ask around to see if a node already exists to talk to them. If no node exists, then you will have to write that yourself.", "You can use the ", " package to send commands from a joystick or keyboard to your motor controller node. Install that with", "since turtlebot_teleop is a package in the turtlebot_apps stack. Modify that install command to replace ", " if you are using a different version of ROS.", "So the order of your nodes will look like the following (with the topics that connect your nodes inside the arrows):", "Note that you will have to set up your ", " so that nodes running on two different computers can talk to each other.", "To do teleoperation (remote control with cameras) you will have to try out the different camera driver packages (probably ", " since it is USB) to get images out and use ", " to view the image stream.", "The pan/tilt mechanism would have to be another node, probably one that you write."], "answer_code": ["sudo apt-get install ros-fuerte-turtlebot-apps\n", "fuerte", "turtlebot_teleop_joy (computer 1) --cmd_vel--> motor_controller_node (computer 2) --motor_controller_outputs--> motors\n"], "url": "https://answers.ros.org/question/50427/where-do-i-need-to-start/"},
{"title": "Kinetic Robot hardware setup", "time": "2012-12-14 01:09:28 -0600", "post_content": [" ", " ", " ", " ", "Would this be a good setup? Im mostly interested on the MB/CPU/PSU and battery", "MB: www.newegg.com/Product/Product.aspx?Item=N82E16813186211", "CPU: www.newegg.com/Product/Product.aspx?Item=N82E16819115078", "PSU: www.logicsupply.com/products/picopsu_160", "Battery: www.batterymart.com/p-12v-7ah-sealed-lead-acid-battery.html", "Case: www.dfrobot.com/index.php?route=product/product&path=37_111&product_id=63#.UMlFRbT3BT5", "Also i'll be using a Arduino/Arbotix and a motor controller for controlling motors, and servos. ", "Anything i should change? or look into?", "Edit: I want to build a robot, that will create a map of its location using the kinetic, also i want to add 1-2 hands to it, that he can use to grab objects and open doors... Hands will use servos.", "Please tag your question and be more specific. What do you want to achieve. Make sure you have a look at the ", ".", "Updated question", "Updated question again "], "answer": [" ", " ", "Your question is still vague. But you are missing a way to do some I/O and a motor-controller. Look in to the ros serial package and get an arduino (for instance) and a big motor controller."], "url": "https://answers.ros.org/question/50459/kinetic-robot-hardware-setup/"},
{"title": "Connecting Kinect on USB 1.0", "time": "2013-01-08 00:02:06 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi, ", "I am trying to connect Kinect on USB 1.0 port, but it is not working. Is it possible to use Kinect on USB 1.0. ", "regards", "It's quite possible that the kinect doesn't work on USB 1.0, as I know there are often issues with USB 3.0. Sometimes the issue is that the USB port does not supply power."], "answer": [" ", " ", "At full bandwidth, USB 1 can do 12Mbps. There is no way a kinect can run with this limited bandwidth. "], "url": "https://answers.ros.org/question/51660/connecting-kinect-on-usb-10/"},
{"title": "No camera info and image recieved [closed]", "time": "2012-12-24 00:05:40 -0600", "post_content": [" ", " ", " ", " ", "I have followed all the tutorials step by step.. Everything was fine and I was able to run rviz as well but seems it does not receive any image . My camera status is error and saying no camera info received and no image recieved.\nI check the kinect ... It got the flash green color light \nAny idea how to deal with this?? ", "Is the kinect publishing data to a topic? Do you see the kinect in the list of rostopic list? Perhaps add some more info to you question."], "answer": [], "url": "https://answers.ros.org/question/51026/no-camera-info-and-image-recieved/"},
{"title": "Query Regarding Microsoft Kinect X-box 360 [closed]", "time": "2012-12-27 16:11:57 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi ALL,\n      We  are working on pr2 robot and we are trying to configure Microsoft Xbox 360 on our  pr2 robot ,We followed link below and installed  ros-electric-openni-kinect, when we launch roslaunch openni_launch openni.launch we get the following error", "INFO] [1356520739.620137369]: Number devices connected: 1\nKilling XnSensorServer and removing module gspca_kinect did not work for us, our lsusb looks like this", "pr2admin@c1:~$ lsusb\nBus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 002 Device 008: ID 0408:0003 Quanta Computer, Inc.\nBus 002 Device 004: ID 0408:0907 Quanta Computer, Inc.\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 007 Device 005: ID 045e:02ae Microsoft Corp.\nBus 007 Device 004: ID 045e:02ad Microsoft Corp.\nBus 007 Device 003: ID 045e:02b0 Microsoft Corp.\nBus 007 Device 002: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 006 Device 003: ID 10c4:ea60 Cygnal Integrated Products, Inc. CP210x Composite Device\nBus 006 Device 002: ID 046d:0a04 Logitech, Inc. V20 portable speakers (USB powered)\nBus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub", "is there any configurations or any other steps we are missing in order to bring up our kinect? please let us know\nThanks in advance.", "I think you forgot to post the actual error. The line given just says that 1 openni device is connected.", "The actual error\n INFO] [1356520739.620137369]: Number devices connected: 1", "There are 5 ", " in the ros logging system. The actual error will say 'ERROR' unless it's a lower level error (like an assertion failing).  Maybe you could edit your question with the entire output of the program?", "When openni.launch is launched we get the following error message lines", "pr2admin@c1:~$ roslaunch openni_launch  openni.launch\n... logging to /u/pr2admin/.ros/log/95f1b46c-5adc-11e2-aede-001517ebc515/roslaunch-c1-16275.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server http://c1:41914/", "PARAMETERS\n * /rosdistro\n * /camera/driver/rgb_frame_id\n * /camera/disparity_depth_registered/min_range\n * /camera/driver/rgb_camera_info_url\n * /camera/depth_registered/rectify_depth/interpolation\n * /camera/driver/depth_frame_id\n * /camera/depth/rectify_depth/interpolation\n * /camera/disparity_depth/max_range\n * /rosversion\n * /camera/driver/device_id\n * /camera/driver/depth_camera_info_url\n * /camera/disparity_depth/min_range\n * /camera/disparity_depth_registered/max_range", "NODES\n  /camera/depth/\n    rectify_depth (nodelet/nodelet)\n    metric_rect (nodelet/nodelet)\n    metric (nodelet/nodelet)\n    points (nodelet/nodelet)\n  /camera/rgb/\n    debayer (nodelet/nodelet)\n    rectify_mono (nodelet/nodelet)\n    rectify_color (nodelet/nodelet)\n  /\n    camera_nodelet_manager (nodelet/nodelet)\n    camera_base_link (tf/static_transform_publisher)\n    camera_base_link1 (tf/static_transform_publisher)\n    camera_base_link2 (tf/static_transform_publisher)\n    camera_base_link3 (tf/static_transform_publisher)\n  /camera/\n    driver (nodelet/nodelet)\n    register_depth_rgb (nodelet/nodelet)\n    points_xyzrgb_depth_rgb (nodelet/nodelet)\n    disparity_depth (nodelet/nodelet)\n    disparity_depth_registered ..."], "answer": [], "url": "https://answers.ros.org/question/51145/query-regarding-microsoft-kinect-x-box-360/"},
{"title": "Serial Load Cell is slow in ROS", "time": "2013-01-08 12:07:47 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "So I have these load cells (4 of them) for which I have written a nice C++ class wrapper to interface with the sensors in a ROS environment. I have 4 topics, called \"loadSensor", "\" where ", " = 0,1,2,3. These topics publish the force felt by these sensors in millipounds.", "Now the load cells are actually ", " capacitive sensors with serial USB output. Here is the link to it", "To the left of the page, you can see that default data rate for a standard sensor is 150 Hz. I have 4 of these sensors connected to my PC with a USB hub. Now all the topics and everything work fine. But when I do ", "instead of getting 150 Hz, I get only about 10-15 Hz. Also, when I connect just a single load cell instead of all 4 and try ", "I get about 50-55 Hz.", "In both cases, I get much less data output rate compared to the promised 150 Hz. Which brings me to my question. Why does this happen? Is it because I am using a USB hub? Or is it my laptop specifications? Or is it ROS using up a lot of CPU power for just publishing 4 topics? Also, why do multiple load cells being connected with the hub affect my data rate? ", "My laptop specifications are", "Intel Pentium CPU B960 Processor @ 2.20 GHz", "64-bit Windows 8 OS with 32-bit Ubuntu 12.04 on VMWare Player.", "4 GB RAM for host OS, 1 GB RAM for Ubuntu 12.04.", "Thanks and any help is greatly appreciated.", "Can you try on Ubuntu installed on an actual machine? I'm curious if maybe there is some odd interaction in the VMWare USB stack that could be affecting this. Try ", "'s suggestion first, but if it doesn't look like your code that is the problem, try removing the VM.", "Yes, I had that feeling too. Plus, the computer I am using has only 1 processor. On an different computer that I used earlier, with 4 processors (pentium core i7), I got 16-20 Hz when I used 4 sensors and 60-65 Hz with 1 sensor. In any case, I will first try dornheges solution and post updates here."], "answer": [" ", " ", "I suspect that this is not a ROS problem. This can be simply tested: Just take your driver code and don't do anything ROS related. You'll need to check the frequency yourself manually and see the result. Depending on the result you can look further.", "The data rate with one sensor connected seems 4x data rate with 4 sensors, so there seems to be some general bottleneck.", "I was planning on doing exactly that. Isolate the serial reading and writing parts, and see how many times I can write to a file in say 1 second. However, I need to use the force logging feature in the background while I am doing something else. Do you suggest I use threads instead of topics then?", "Not sure what you mean by \"force logging feature\". I wouldn't do anything with the data first, but get it, discard it and count the frequency. Once that is fine, you can go to processing (or check why it is too slow).", "By force logging feature, I basically meant reading the force data from the sensor and writing it to a text file. I shall update this post as soon as I get results from the manual frequency tests.", " : I know this is a bit late, but I did try out your suggestion. Without the ROS stuff and manually measuring the frequency, I get about 55 Hz frequency per sensor. This means that I am losing out some data somewhere. What do you suggest I do now?", "Make sure your send and receive buffers are large enough to receive messages from all senders simultaneously "], "question_code": ["rostopic hz /loadSensor0\n", "rostopic hz /loadSensor0\n"], "url": "https://answers.ros.org/question/51734/serial-load-cell-is-slow-in-ros/"},
{"title": "Gazebo 1.0.x and CUDA in Furete", "time": "2012-07-17 04:33:24 -0600", "post_content": [" ", " ", " ", " ", "We are currently running a relatively intense simulation for some work we are doing, and we noticed that the physics engine seems to be slowing us down the most. We don't want to decrease the precision of the physics engine too much (i.e., changing parameters in the <solver> tag in the .world file) as this can cause models to \"explode\" -- as discussed ", " and ", ".", "I know in previous version of ROS/Gazebo, parallel quick step could be used in conjunction with CUDA to help the physics engine. However, I cannot seem to find out how to get this to work in Fuerte. Previous posts (e.g., ", " have discussed how to configure this in Electric. With Gazebo 1.0.x used in Fuerte and the new ", ", I see no place to force Gazebo to use CUDA to speed itself up. Even the ", " show that \"world\" and \"quick\" are the only two valid solver types (i.e., no parallel_quick/cuda). Furthermore, the example launch files that use CUDA in the Ubuntu Fuerte parallel_quickstep package use out-of-date world-file syntax/don't run. Is it no longer possible to use CUDA to speed up simulations? We have a relatively powerful Nvida GPU in our Ubuntu 12.04 machine and it would be a shame not to utilize it in the simulations."], "answer": [" ", " ", "I'm going to hazard a guess that it is no longer supported/maintained. In $(rosfind gazebo)/build/gazebo-r22f33a2ed71a/deps/parallel_quickstep/ there is some code that references CUDA, but I can't find anywhere where it connects to the main gazebo program (renaming the folder seems to have no ill effects on compilation).  The patch files in the main directory don't look like they have been updated since before fuerte. ", " ", " ", "if you want to try and get something going, I suggest starting with the code that was pulled into gazebo ", ", note the code base is there, but we have not done any work to start any real integration.", " ", " ", "Were you able to use Gazebo 1.0.x and CUDA in Fuerte?\nAnd how please\nThanks"], "url": "https://answers.ros.org/question/39024/gazebo-10x-and-cuda-in-furete/"},
{"title": "what hardware should I use for wheel encoders.", "time": "2013-01-13 13:28:22 -0600", "post_content": [" ", " ", "I am building a differential drive robot from an old powered wheelchair. The wheels are attached to the motor by a ~10:1 gearbox. I would like to add encoders to the motors.  what kind of encoders should I use. I am running a full PC onboard. I have a chip kit max32 that I could use as an interface. What is the best way to add encoders. Is there a useful usb interface? What kind resolution should I look at. The motor drivers I am using do not have encoder feedback. Thanks for any feedback.", "At full power the robot can move about town miles an hour. Is it typical to use a dedicated device just to read the encoders?  A device like\"http://www.robotshop.com/phidgets-phidgetencoder-highspeed-usb-4-input-2.html\"", "My other question is, what hardware do you guys recommend. 200 CPR encoder connected to an arduino?"], "answer": [" ", " ", "The encoder resolution depends on ur system demands,i.e, how fast do u want ur robot to go. If u need excellent low speed characteristics, it's better to have high resolution encoders. From the situation u've explained, I suggest u can use an ARM development board with high speed IOs to read the pulses outputted by the encoders, and communicate with ur PC via serial port like RS232.  "], "url": "https://answers.ros.org/question/52131/what-hardware-should-i-use-for-wheel-encoders/"},
{"title": "Kinect data publishing strange behaviour [closed]", "time": "2013-01-10 20:22:34 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "After release of groovy I did a clean install of Kubuntu 12.10 and groovy. I also have a windows 7 installed with Microsoft Kinect SDK, which can work with my Kinect for Xbox very well, so there is no hardware issue. When I run rviz or image viewers, upon moving my kinect sometime the images freeze. This is the output of ", "Does anybody know why this happens? and why frequency is around 5hz? I have a powerful CPU and GPU, and in windows point cloud is generated at around 30fps.", "What resolution are you using? What is the exact command line you executed to launch?"], "answer": [], "question_code": ["rostopic hz /camera/depth_registered/points", "average rate: 5.390\n    min: 0.023s max: 13.525s std dev: 0.81439s window: 410\naverage rate: 5.382\n    min: 0.023s max: 13.525s std dev: 0.81341s window: 411\nno new messages\nno new messages\nno new messages\nno new messages\naverage rate: 5.143\n    min: 0.023s max: 13.525s std dev: 0.83635s window: 423\n"], "url": "https://answers.ros.org/question/51998/kinect-data-publishing-strange-behaviour/"},
{"title": "is there any benchmarks for comparing the methods  of visual slam", "time": "2013-01-20 18:44:39 -0600", "post_content": [" ", " ", " ", " ", "I have studied the matlab code of Javier Civera's 1-point RANSAC for EKF MonoSLAM, but I find I am lack of some datasets to evaluate the visual slam system, because I don't have the powerful robot to collect good data sets.\n  does any one know some image dataset to test the visual SLAM methods ? or else, tell me some good simulator  to implement the visual SLAM methods and finally evaluate the methods "], "answer": [" ", " ", "There is a dataset designed to \"establish a novel benchmark for the evaluation of visual odometry and visual SLAM systems\" by J\u00fcrgen Sturm at TUM: ", "Hope that helps...", "Here's also the paper about that dataset: ", " ", " ", "The RGB-D-SLAM package Multiresolution Surfel Maps ", "code.google.com/p/mrsmap", "has been tested on the TUM/Freiburg data set with good results.", "It runs on CPU.", "See this paper for details:", "www.ais.uni-bonn.de/papers/JVCI_13_RGB-D-SLAM.pdf", " ", " ", "Besides, is there any  community websites focus on the slam problem, so that we can discuss or study with each other?"], "url": "https://answers.ros.org/question/52861/is-there-any-benchmarks-for-comparing-the-methods-of-visual-slam/"},
{"title": "How can i know my gyro part number?& can i perform navigation without gyro sensor ?", "time": "2013-01-31 03:23:48 -0600", "post_content": [" ", " ", " ", " ", "hi..we are a graduation project team works on the turtlebot ..we already bought icreate robot and we designed the the rest plates of the turtlebot and we had the sdk as it is available and on our way to start calibration we found that we have to know our gyro part number which as the following tutorial ", " how can we know our gyro part number through our icreate robot base ??", "answer was as Mr Ryan said :", "\"The iCreate robot does not have a gyro built in. You'll need to purchase a power board separately\"", "Editing :-", "Do you have access to the gyro board? You probably have to look at the actual sensor, it's definitely written there. In ", ", for instance, the gyro would be the silver square on the bottom right corner of the board.", "that what i am talking about at the tutorial he is showing the board which i thought it is within the base and i don't have the access to it  but MR.Ryan said below that i have to buy it separately and i just noticed that \"Only necessary if you have a Create base which is using an external gyro.\"", "Question edited"], "answer": [" ", " ", "The iCreate robot does not have a gyro built in. You'll need to purchase a ", "thank you i just noticed this statement on the tutorial \"Only necessary if you have a Create base which is using an external gyro. \" ..am i still able to navigate without calibration ?"], "question_code": ["**Am i still able to perform navigation and VSLAM without the gyro sensor ?**\n"], "url": "https://answers.ros.org/question/53815/how-can-i-know-my-gyro-part-number-can-i-perform-navigation-without-gyro-sensor/"},
{"title": "turtlebot navigation", "time": "2013-01-31 21:21:43 -0600", "post_content": [" ", " ", "hello,", "can i build my map without doing calibration because i have only irobot create ? ", "thanks", "what kind of calibration are you talking about??", "this one\nhttp://www.ros.org/wiki/turtlebot_calibration/Tutorials/Calibrate%20Odometry%20and%20Gyro", "I have a turtlebot which has a create base. I did not do any calibration for my bot as mentioned in the link..", "Can you tell me how to built your map without using calibration ? as my irobot create dosen't contain this part http://store.iheartengineering.com/ihe-0200-0000-fa00.html", ":- you bought the full turtle bot with (power/sensor gyro board )..or only icreate robot ?", "Since u dont have the gyroscope sensor, wat exact set of readings are you missing??", "How can i do it without having gyro sensor?", "what is turtlebot calibration ?! and why need this ?!\nsee this my help !\n", "/"], "answer": [" ", " ", "Considering the comments I guess by \"without calibration\" you mean to say that you don't have an IMU?", "In that case, try ", ". It doesn't require odometry and might work for you.", " :  Does that technique works without \"board/sensor gyro board\".. can perform mapping task with the normal icreate robot base ?", "Yes, it doesn't require odometry in principle. Just a laser or similar. How good it works only with a kinect and thus very limited data is another question.", "that's sounds great...i will start work on this technique ... and hope it leads to a good results ...i love this community ^_^...thank you very much,sir. and thanks to the question asker ^_^", " ", " ", " I am not sure of the turtlebot specification that I have in my lab. ", " if ur using gmapping algorithm then all you need is laser scan and position of laser sensor w.r.t to world. Laser position is defined with respect to base_link of robot which in defined in odometry frame. So if u have laser readings and sensor position then mapping can easily be done. ", "Thank u ... is there any tutorial explains how i can use this method ?", "look for gmapping tutorial,that should help...."], "url": "https://answers.ros.org/question/53912/turtlebot-navigation/"},
{"title": "ROS AMCL based navigation error [closed]", "time": "2013-02-06 18:05:07 -0600", "post_content": [" ", " ", " ", " ", "I got an error when running AMCL based navigation.  It runs in loop asked me to give transformation from /map to /base_link. I believe that should be given by AMCL right? I see through the previous solution on the ROS_answers. Non of the previous suggestion helps solve the problem", "here I", "m going to runn this node on quadcopter. So I don`t have a odom", "My launch file consist of standard configuration in ROS_NAVIGATION tutorial.", "hokuyo_node", "laser_scan_matcher", "tf  base_link to laser", "tf base_link to odom ", "\nSo I set odom frame always correct and it is precisely at base_link.", "map server", "AMCL\nhere I set odom frame to be base_link or world", "move_base\nand rviz", "the move_base and all other node works when using together with Gmapping node. But it is too much computational power for small uav system to handle. \nOnly difference is I replaced the Gmapping with the map server and AMCL", "tf in the rviz shows /laser  /base_link /odom is shown\nBut /map and /world is missing"], "answer": [], "question_code": ["m not running on actual robot. Just using laser to do localization by giving a fake odom. All I need to see is the correct planed path and correct cmd_vel. I"], "url": "https://answers.ros.org/question/54548/ros-amcl-based-navigation-error/"},
{"title": "Cannot launch openni_camera: No devices connected (ROS electric)", "time": "2012-03-30 09:55:26 -0600", "post_content": [" ", " ", " ", " ", "I'm using Linux Mint 12 x64 (based on Ubuntu Oneiric) and ROS electric.", "I installed the openni_kinect stack using the instructions here: ", "I then restarted, made sure the Kinect had external power (got a blinking green light), saw the following from lsusb:", "But when I try to start up the openni_camera stack, it does not detect the device:", "I know there is a related question ", ". That question was answered, but it's from a while ago, and the answer (basically, \"I re-installed ubuntu and it just worked\") is not very satisfactory. So I thought it couldn't hurt to bring it up again.", "Does anyone have any ideas on things ..."], "answer": [" ", " ", "Figured it out. ", "I bought a ", " instead of ", ". Apparently this new version, while it has better hardware, is ", "Is it supported yet? Have you tried libfreenect?", " ", " ", "I get this error when I install Groovy on Ubuntu 12.10 and do the following:", "1) Go to \"www.openni.org/openni-sdk/openni-sdk-history-2\" and download NiTE v1.5.2.21 for Linux-x64\n2) Unzip it and go inside the top-level directory\n3) Type: sudo ./install.sh", "Before I do these 3 lines above, \"roslaunch openni_launch openni.launch\" works fine but after I do them I get:", "No devices connected.... waiting for devices to be connected", "Dave", "I'm also getting this error on fuerte and ubuntu 12.04 32bits ( i install the 32bits NITE)", " ", " ", "Can you try it out with:", "and not with", "and tell me which output you get.", "BTW: Are you using the \"Kinect for Winows\"?", " ", " ", " ", " ", "This might be the same issue as this one:\n", "I also had problems with the kinect and the solution was to plug it into a 2.0 slot."], "url": "https://answers.ros.org/question/30889/cannot-launch-openni_camera-no-devices-connected-ros-electric/"},
{"title": "Access to willow garage dataset", "time": "2013-02-20 06:06:29 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI am trying to access the datasets at ", ".\nCould someone check if the link is available. Because when I try to download for ex the first 08-09-19_10party-100scenes-greenroom/ at \n", "/ \nI don't see existence of that page.", "Any assistance is much appreciated.", "Thanks,\nKarthik"], "answer": [" ", " ", "/u/prdata/... is an internal network folder, not a web address. ", "Hey so how do I get access to that?", "That's not possible.  There's lots of released datasets.  Where are you getting that url from? It looks like a combination of multiple ones. "], "url": "https://answers.ros.org/question/55799/access-to-willow-garage-dataset/"},
{"title": "How to change the color of interactive_marker [closed]", "time": "2013-02-17 20:24:17 -0600", "post_content": [" ", " ", " ", " ", "I am using interactive_markers and I don't know how to change the color of interactive_markers.", "The default color for x-axis is red, for y-axis is green, and for z-axis is blue.\nBut I want to change the color.\nCan I change it easily?"], "answer": [], "url": "https://answers.ros.org/question/55471/how-to-change-the-color-of-interactive_marker/"},
{"title": "kobuki_ftdi create_udev_rules  not found", "time": "2013-02-18 21:16:33 -0600", "post_content": [" ", " ", " ", " ", "Hello", "I received mu turtlebot with Kobuki and I followed the ROS (Groovy) tutorial for installing Turtlebot and Kobuki\nThere is a problem with this command", "rosrun kobuki_ftdi create_udev_rules ", "Actually there is no \"create_udev_rules\" node in the package kabuki_ftdi.\nSo the command does not work. There are other nodes such as find_device, flasher, ftdi_kobuki, \u2026 But not create_udev_rules.\nIs there any new process that makes the rules?", "I did the following command \nroscd kobuki_ftdi\nmake udev\nThat I found in installation tutorial\nAnd it make Kobuki appears as /dev/kobuki. But still when I run get_serial_number, I get this output", "1 device found\nSomething went wrong. Did you run with sudo?", "When I try rosrun with sudo, it does not accept to execute rosrun with sudo.", "When I run minimal launch, it works but gives a problem with the battery.", "I hope you can help in fixing this issue.", "Thanks\nAnis", "create_udev_rules  is still in ros-shadow-fixed awaiting a push into the ros repo. Just running 'make udev' will work in the interim."], "answer": [" ", " ", "Sudo does not work with rosrun (or any ros command) because it does not carry across all the ros environment variables. Bad idea anyway, as it saves all your logs and other temporaries as root.", "Just try calling it directly with sudo (i.e. without ros) should work.", "Having said that, we just catkinized kobuki, so once that deb reaches ros public repo, that binary will be in ", ". We'll get that moved into ", " so it's easier to find and run and update the wiki instructions this week."], "answer_code": ["/opt/ros/groovy/lib/kobuki_ftdi", "/opt/ros/groovy/bin"], "url": "https://answers.ros.org/question/55599/kobuki_ftdi-create_udev_rules-not-found/"},
{"title": "navfn stability", "time": "2013-02-06 20:40:07 -0600", "post_content": [" ", " ", " ", " ", "Hi. ", "\nI encountered with the problem that global_planner in navfn package makes invalid plans. It means that they go through walls on a map like on this picture.\n", "There is ROS Electric on board. ", "\nCannot find out why it happens, so I'd appreciate any suggestions.  ", "UPD. I tried with original navigation stack (as well as with forked base_local_planner and move_base) and it still makes plans through the walls sometimes. \nIs it possible that it's a bug in navfn package?\nIs there any others global planners? ", "\nHere are my configs: ", "\n1. base_local_planner_params", "2. costmap_common_params", "3 global_costmap_params", "4 local_costmap_params", "UPD2.  ", "5 BaseLocalPlanner.cfg", "This should never happen and I've never seen it happen. I suspect it's probably more of a setup issue. Could you add your configuration?", "Can't see anything obviously wrong, besides maybe the publish_frequency of 0.0. I can't look it up right now as ", " is down. One other thing: Your rviz display seem not to correspond to each other at all. Can you explain that? Maybe something is wrong with tf/frames?", "Sorry, did not get what is not corresponded? And one more clarification. As far as I understood navfn algorithm works standalone and my base_local_planner hacks will not affect global_planner. For example I've changed the weights for local trajectories.", "The pink cells you display are the plan, etc. right? It doesn't seem to match anything (the map, the current robot pose). That seems odd.  Yes, you understood this correctly. Changing things in the local planner is irrelevant for the global planner.", "Yes, the pink cells are inflated obstacles. The green one are lethal_ostacles recognized by kinect. The red are laser scans. I think that they are dismatched with the static map a little because of errors in the navigation system. ", "OK, so this might only be for the local planner. What is the green plan then? Is that maybe from the local planner? This would explain everything. Given the obstacles you display the green plan is correct. Unfortunately this local planner isn't bound to the global plan and makes its own decisions.", "If it doesn't consider the map the observations are consistent. Can you check which plan you display? Usually there are three different topics.", "The green plan is the global plan computed by navfn. One interesting thing: the first plan is always correct. This problem popped up when I tried cope with oscillations of my robot. So I hacked move_base a little to remake the global plan once a minute. "], "answer": [" ", " ", "It looks like your range sensor possibly has seen through the wall.  In that case the map will be updated to reflect that there's no obstacles seen in that location and it's valid to plan there. ", "And in this case navfn disregards the static map?", "Yes. Problematic as it may be, the costmap will overwrite the static map to trust its local sensors (just in case there's noise in the map)", "Same happened to us, not sure whether tullys explanation holds: ", " In any case setting /move_base_node/planner_frequency to non-zero can help reduce the problem. hydro seems to be okay for us, with stage anyway.", "In the updated hydro nav, the static map is NOT overwritten. It should probably be configurable, but I haven't made that edit yet."], "question_code": ["    controller_frequency: 5.0\nTrajectoryPlannerROS:\n  max_vel_x: 0.70\n  min_vel_x: 0.1\n  max_rotational_vel: 1.5\n  min_in_place_rotational_vel: 1.0\n  acc_lim_th: 0.75\n  acc_lim_x: 0.50\n  acc_lim_y: 0.35\n\n  holonomic_robot: false\n  yaw_goal_tolerance: 1.1\n  xy_goal_tolerance: 0.15\n  goal_distance_bias: 0.8\n  path_distance_bias: 0.6\n  sim_time: 1.5\n  heading_lookahead: 0.325\n  oscillation_reset_dist: 0.05\n\n  vx_samples: 6\n  vtheta_samples: 20\n  dwa: false\n", "obstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[-0.35, -0.245], [0.35,-0.245], [0.35,0.245], [-0.35, 0.245]]\nfootprint_padding: 0.01\ninflation_radius: 1.0\nobservation_sources: filter\nfilter: {data_type: LaserScan, topic: /filter, marking: true, clearing: true}\n", "global_costmap:\n   global_frame: /map\n   robot_base_frame: /base_link\n   update_frequency: 3.0\n   publish_frequency: 0.0\n   static_map: true\n   transform_tolerance: 0.5\n", "local_costmap:\n   global_frame: /odom\n   robot_base_frame: /base_link\n   update_frequency: 5.0\n   publish_frequency: 5.0\n   static_map: false\n   rolling_window: true\n   width: 4.0\n   height: 4.0\n   resolution: 0.1\n   transform_tolerance: 0.5\n", "# gen.add(\"inscribed_radius\", double_t, 0, \"The radius of the inscribed circle of the robot\", 1, 0)\n# gen.add(\"circumscribed_radius\", double_t, 0, \"The radius of the circumscribed circle of the robot\", 1, 0)\n\ngen.add(\"acc_lim_x\", double_t, 0, \"The acceleration limit of the robot in the x direction\", 1.0, 0, 20.0)\ngen.add(\"acc_lim_y\", double_t, 0, \"The acceleration limit of the robot in the y direction\", 1.0, 0, 20.0)\ngen.add(\"acc_lim_theta\", double_t, 0, \"The acceleration limit of the robot in the theta direction\", 1.0, 0, 20.0)\n\ngen.add(\"max_vel_x\", double_t, 0, \"The maximum x velocity for the robot in m/s\", 0.55, 0, 20.0)\ngen.add(\"min_vel_x\", double_t, 0, \"The minimum x velocity for the robot in m/s\", 0.0, 0, 20.0)\n\ngen.add(\"max_vel_theta\", double_t, 0, \"The absolute value of the maximum rotational velocity for the robot in rad/s\",  1.0, 0, 20.0)\ngen.add(\"min_vel_theta\", double_t, 0, \"The absolute value of the minimum rotational velocity for the robot in rad/s\", -1.0, 0, 20.0)\ngen.add(\"min_in_place_vel_theta\", double_t, 0, \"The absolute value of the minimum in-place rotational velocity the controller will explore\", 0.4, 0, 20.0)\n\ngen.add(\"sim_time\", double_t, 0, \"The amount of time to roll trajectories out for in seconds\", 3, 0, 10)\ngen.add(\"sim_granularity\", double_t, 0, \"The granularity with which to check for collisions along each trajectory in meters\", 0.025, 0, 5)\n\ngen.add(\"pdist_scale\", double_t, 0, \"The weight for the path distance part of the cost function ..."], "url": "https://answers.ros.org/question/54559/navfn-stability/"},
{"title": "Speech recognition packages", "time": "2013-04-09 07:25:43 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am looking for speech recognition packages. Someone can help-me with suggestions.", "This is complex problem, it is required that applications are all the time listening and processing the information. This is not the core of my work but i need something that translates voice to text. ", "I have found (", "/ and ", ") which uses pocketsphinx, but from what I have seen is not accurate and is required to build a dictionary of words.  ", "Another solution is using the google framework (", "), but it is required internet connection and can take more the 3 seconds to get an answer.", "Another option is use Julian (", "/) from video looks great. ", "The Hark seems to be powerful but they are using multiples microphones for localization also. (", " ", ") ", "I need something light then HARK and more efficient then pocketsphinx. I think that I will start with HARK. Can you give-me others suggestions?", "Thanks", "interesting question", "From my point of view PocketSphinx is a powerfull tool for Speech Recognition. However, it need some tuning to get the most of it. Also, HARK is a good think, I do not like the way the implemented but are good algorithms for localisation and separation in quiet environments. "], "answer": [" ", " ", "The most important here, is that  you do not need to choose between HARK and Sphinx. HARK is a signal processing toollkit for source localisation and separation and Sphinx is a speech recognition software. However, it is very very likely that to combine them you will need to retrain or adapt the acoustic models."], "url": "https://answers.ros.org/question/60323/speech-recognition-packages/"},
{"title": "Cancelling a ROS action within an executive_teer node", "time": "2013-04-04 04:58:28 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am trying out the ", " package and my test setup is a simulated TurtleBot running two main tasks: a navigation task that moves the TurtleBot through a series of waypoints in a map using MoveBase actions and a battery_check task that monitors a simulated battery level and sends the robot to a charging base (also using MoveBase) when the level falls too low.", "The trouble I am having is interrupting the current MoveBaseAction from the battery_check task when the battery level falls below threshold and while the robot is still on the way to a waypoint.  Even though I send a move_base.cancel_goal() from the battery_check task, the robot always continues to the current waypoint before then heading to the charging station.  I've also tried killing or pausing the navigation task but I get the same result--the MoveBaseAction appears to be outside the teer environment's control.", "I can post the code details if necessary but perhaps there is some general trick when using teer with standard ROS actions?", "Thanks!", "\npatrick"], "answer": [" ", " ", "The answer given by Patrick works, but misses the point of teer.  A better solution would be to have ", " as a ConditionVariable, so that the co-routine is automatically un-paused when this variable becomes true. Please see ", " for more informations about condition variables.", "Thanks for the clarification ", ".", " ", " ", "I figured out an answer to my own question.  It appears that using the move_base wait_for_result() function is what is blocking teer from cancelling a current goal.  But it turns out that you can override the move_base callbacks for \"done\", \"feedback\" and \"active\".  So I now call move_base.send_goal() like this:", "where the self.move_base_cb callback is defined as:", "And this now allows me to cancel a current goal from another teer task.", "--patrick"], "answer_code": ["self.move_base_done", "self.move_base_done = False\nself.move_base.send_goal(self.goal, done_cb=self.move_base_done_cb)\nwhile not self.move_base_done:\n    yield WaitDuration(0.5)\n", "def move_base_done_cb(self, status, result):\n    self.move_base_done = True\n"], "url": "https://answers.ros.org/question/59996/cancelling-a-ros-action-within-an-executive_teer-node/"},
{"title": "how to calculate covariance", "time": "2013-03-22 01:41:02 -0600", "post_content": [" ", " ", "Hey!", "I am looking for the error covariance of two sensors (gyro & accelerometer). Since I do have the data sheets I was wondering if you could tell me how to obtain the value. I do know that the covariance might change but is there a way to estimate it from noise specifications or something similar?", "In both cases I have the power spectral density of the noise. Is this the value I need? The information I am referring to is on page 12 and page 13 in this http://www.invensense.com/mems/gyro/documents/PS-MPU-6000A.pdf (document).", "Thanks for any suggestions :)"], "answer": [" ", " ", "Your link does not work. In general, if you have the data, you could just use the sample covariance as an estimate ("], "url": "https://answers.ros.org/question/58921/how-to-calculate-covariance/"},
{"title": "use Hokuyo LIDAR with ethernet in ROS", "time": "2013-03-21 01:47:26 -0600", "post_content": [" ", " ", " ", " ", "Hi!\nI got a Hokuyo UTM-30LX-EW which I would like to use with my Turtlebot. The sensor has a power and an ethernet connector. I connected the power to the turtlebot, it seems to be working. I connected the ethernet cable to the wifi router. I can ping the sensors IP from the turtlebot, it seems to be up and running.", "I then installed the ros_groovy_laser_drivers package. Then I wanted to try: ", "I get the following warning and error: ", "I wasn't very surprised, since I haven't set the ip of the sensor neither in ROS nor in Ubuntu, so probably that's why I don't have the missing port. I don't know how to set this up. The pages regarding the ros hokuyo node don't mention any IP or ethernet related topics.\nHowever, I found a page with the iri_hokuyo driver (http://mediabox.grasp.upenn.edu/roswiki/doc/api/iri_hokuyo_laser/html/), which referst to ethernet and USB ports. Unfortunately, I can't find the iri_hokuyo driver in the ubuntu/ros repositories.", "Could someone tell me how should I set up the Hokuyo sensor?", "Are you aware of any differences (pros/conns) of using an ethernet interface over the usb interface?\nI am currently evaluating Hokuyo sensors for my application and would like to know if there is any advantage in either interface.\nApologies for commenting on an old topic.\nRegards,\nChris."], "answer": [" ", " ", " ", " ", "There's an actively maintained wrapper for urgwidget here:\n", "Eventually, this may replace hokuyo_node as the recommended hokuyo driver - even for PR2.", "To install:", "To run (for ethernet):", "Thanks Chad! It is working!", "Are there any options for Fuerte? I need to use an ethernet UTM-30LX-EW and it would be convient if it worked with Fuerte.", "I'm not aware of any options for Fuerte.  You could make a fork and convert it back to rosbuild.  :P", "Thanks for the reply. I ended up using this http://devel.iri.upc.edu/docs/labrobotica/hokuyo_laser_2d/index.html But it was quite involved", " ", " ", "As of 3/21/2013 there is no support for the Multiecho/Ethernet version of the Hokuyo in the hokuyo_node.  It is expected to be added in the next version of ROS, which would be Hydro.  Here is the issue tracker in GIT: ", "They would probably be able to give you a better answer of their status, if you contact them directly.", "Thank you Raptor!", "This was just posted also: ", " ", " ", "Finally, I got it working with the help of Hokuyo customer service.\nI used the urgwidget_driver, that is a wrapper for ROS  (http://sourceforge.net/projects/urgwidget).\nAccording to Hokuyo, it might work with the iri_hokuyo_laser driver too in Fuerte, but in Groovy not sure.\nIf I get their consent, I am going to post their step by step guide later here.", "Hi Zoltan, \nCan you please post the step-by-step guide for how you set up the Hokuyo LiDAR using the urgwidget_driver? I'm facing the same issue.\nThanks!", "Hi redarean! I didn't get Hokuyo's consent to forward the info, but the solution Chad Rockey posted above works perfectly, try that one, or write to Hokuyo an email, they answered quickly.", "Hi ZoltanS,\nThanks for the prompt reply!\nI have tried Chad Rockey's solution, but keep getting the following error: \n\"[FATAL] [1372183207.157433441]: Could not open network Hokuyo:\n192.168.0.10:10940\ncould not open ethernet port.\"\nDid you also change any IP settings to run urg_node?\nThanks again!", "Hi redarean, I didn't change the IP of the sensor. I got mine from someone who already used it in windows and it was already configured to 192.168.0.10. There is a windows tool to change the ip if necessary.", "Hi ZoltanS, I finally figured it out! I needed to assign a static IP to my machine (auto eth0\niface eth0 inet static\naddress 192.168.0.14\nnetmask 255.255.255.0\ngateway 192.168.0.1)\nThanks again for all your help! :-)", " ", " ", "Zoltan,", "/dev/ttyACM0 is a serial port. ", "Look here for details in how to set it up ", "Cheers", "Mark", "Hi Mark, thanks, I found this out. Unfortunately, our sensor has only an ethernet port. Zoltan", "You could consider writing yourself a bit of code to read in off the ethernet port and write out to a virtual serial port - or buy something like a raspberry pi and write a python script - i guess it depends if you can wait for the update Raptor mentioned :)"], "question_code": ["roslaunch hokuyo_node hokuyo_test.launch\n", "[ WARN] [1363863375.680619490]: The use_rep_117 parameter has not been specified and has been automatically set to true.  This parameter will be removed in Hydromedusa.  Please see: http://ros.org/wiki/rep_117/migration\n[ERROR] [1363863375.690137879]: Exception thrown while opening Hokuyo. Failed to open port: /dev/ttyACM0. No such file or directory (errno = 2). The requested port does not exist. Is the hokuyo connected? Was the port name misspelled? (in hokuyo::laser::open) You may find further details at http://www.ros.org/wiki/hokuyo_node/Troubleshooting\n"], "answer_code": ["sudo apt-get install ros-groovy-urg-node\n", "rosrun urg_node urg_node _ip_address:=\"192.168.0.10\"\n"], "url": "https://answers.ros.org/question/58822/use-hokuyo-lidar-with-ethernet-in-ros/"},
{"title": "How to represent my end-effector in Quaternions [closed]", "time": "2013-03-26 14:10:05 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "I am having some confusion with the representation of the end-effector of my robot. I have a point in space where I want the end-effector to go but I am unsure how to produce the quaternions. ", "I have the \"base_link\" as the fixed frame (0,0,0 @ the center of the horizontal grey cylinder, axes: z-up, x-right, y-back), and the point in space (0.15,-0.60,-0.20) w.r.t that fixed frame. ", ". ", "My end-effector as defined by the urdf file is pointing straight down with the initial orientation of (w=1,x=0,y=0,z=0). ", "My goal is to reach for the green cylinder in the picture. I believe I need a rotation of ~ -90 degrees about the x-axis (pitch), ~0 degrees about the y-axis (roll) and a range of values ~(-15,+15) about the z-axis corresponding to different (yaw) angles to pick up the green cyclinder with, depending on it's location on the table.", "If this is correct, are my quaternions defined by the conversion below, where phi,theta, and psy represent x,y,z rotations I described above?", " (from ", ")", "I've been looking at other posts but I just can't quite wrap my head around this concept and I really appreciate any help! ", " I worked it out over matlab and this is the correct way to determine what orientation you want your end-effector link via RPY coordinates w.r.t. some fixed frame. Cheers!", "Kind Regards,\nMartin"], "answer": [], "url": "https://answers.ros.org/question/59299/how-to-represent-my-end-effector-in-quaternions/"},
{"title": "Using navigation stack with outdated on-board PC", "time": "2013-03-26 02:08:06 -0600", "post_content": [" ", " ", "Hi,", "We have a Pioneer P3AT robot with a rather outdated on-board PC - Pentium III 700 MHz with 256 MB RAM. The robot communicates with a powerful PC through Wi-Fi. In our current setup, the on-board PC runs a driver package (RosAria), a node for SICK laser, transformation and robot_state_publisher stuff. The following components run on the external PC: move_base, amcl, map_server, rviz.", "This setup does perform pretty well.", "We were thinking about an optimization - move every node to the robot's on-board PC except for rviz. This should make information flow more efficient, but the question is that will the robot's hardware handle this task.", "Note: the on-board PC runs Ubuntu 12.04.2 LTS in text mode."], "answer": [" ", " ", "The best way to find out is to try it and see.", "You should be able to get some idea of how well it may work by looking at the current CPU usage on the robot and the desktop; if the robot is mostly idle (<25%), it's definitely worth a try. If the robot is already pretty busy (>50%), I probably wouldn't bother.", "When you're running everything on the robot, be sure to watch the CPU usage and ROS console carefully; the navigation stack behaves pretty poorly if it doesn't get enough CPU time. You'll see lots of messages about various internal loops running too slowly if this is the case. Reducing the control loop frequencies and the costmap sizes or resolutions should help with this."], "url": "https://answers.ros.org/question/59213/using-navigation-stack-with-outdated-on-board-pc/"},
{"title": "Sudden break down of Kinect???", "time": "2013-04-09 23:20:10 -0600", "post_content": [" ", " ", " ", " ", "I was using rviz and roboearth when suddenly the image view went black.", "The terminal of openni.launch showed a red sentence \"depth image frame id does not match rgb image frame id\".", "I closed the terminal and opened a new one and then roslaunch openni_launch openni.launch but it said \"No devices connected... waiting for devices to be connected\". ", "I reboot the computer a few times, sudo apt-get install ros-electric-openni-kinect, but the problem persists. ", "Up till this morning the Kinect has been running normally. I really have no idea what happened. Is it due to auto-update? Anyone please help?", "Turtlebot laptop: Ubuntu 10.04; ROS Electric", "Workstation: Ubuntu 12.04; ROS Fuerte (I have another one in Ubuntu 10.04 and ROS Electric. I tried but the problem remains unsolved.)", "The Kinect shows a flashing green light. But when I type lsusb, I saw only one device from Microsoft and one from Chronicity. Is it normal or not?", "Remove USB and restart and connect back only when you do rviz. I had the same problem. ", "Thank you very much. I tried that but it still says No devices connected."], "answer": [" ", " ", "Solved. Power supply problem from the iRobot Create base.", " ", " ", " ", " ", "As ", " said, unplugging and replugging the kinect can help. ", "Also, sometimes the XnSensorServer process hangs around after the driver has been killed. You can check if this is happening by running ", ". If the process is still there, kill it.", "The openni drivers were also updated a while ago to fix a lot of problems like this. On the turtlebot laptop, run ", " to grab all the new packages and see if that helps. ", "Probably not related to your problem, but it is not a good idea to mix ROS versions.", "I tried killall XnSensorServer but it does not help. I re-installed ROS but it does not help. The No devices connected problem still persists. When I type lsusb, I saw one device from Microsoft and one from Chronicity. Is it normal or not?"], "answer_code": ["ps aux | grep XnSensorServer", "sudo apt-get update && sudo apt-get dist-upgrade"], "url": "https://answers.ros.org/question/60372/sudden-break-down-of-kinect/"},
{"title": "Tag Recognition with Kinect", "time": "2013-03-17 19:00:21 -0600", "post_content": [" ", " ", " ", " ", "Today, it is still very challenging to do object recognition in real time while the robot and the object are both moving. Hence, I am thinking of putting some special red/green tags on the target objects or labels. Do you think it will make things better or worse? Also, which ROS package is the most suitable for recognizing the tags? I would like to implement this in TurtleBot with Kinect. Thank you very much for your suggestions."], "answer": [" ", " ", "Ar markers are the best to detect probably, use the ", " package", "So it is okay even if things are moving?", "I my research the markers are static and the camera is moving, that works. The system is reasonably robust but you should try it in your own application."], "url": "https://answers.ros.org/question/58398/tag-recognition-with-kinect/"},
{"title": "Multi-tag bundles detection of ar_track_alvar", "time": "2013-04-10 08:21:14 -0600", "post_content": [" ", " ", " ", " ", "I have some problem to detect Multi-tag bundles through ar_track_alvar.\nAccording to the wiki, ", "To create a bundle, first choose which tag you want to be the master tag. Treat the center of the master tag as (0,0,0). Then, after placing the rest of the tags, measure the x, y, and z coordinate for each of the 4 corners of all of the tags, relative to the master tag origin. Enter these measurements for each tag into the XML file starting with the lower left corner and prograssing clockwise around the tag.", "So I think I should add points in XML file as this sequences: lower left -> upper left -> upper right -> lower right.", "However, an example XML file included in ar_track_alvar seems like not to follow that rules. (It progresses counter-clockiwise around tag. e.g. lower left -> lower right -> upper right -> upper left)", "I also tried to make XML file through createMarker. However, it is also weird. 4 points generated by it are center -> lower right -> upper right -> upper left. ", "I tried three of them. When I check it through rviz, all three cases cannot detect master tag (red square marker appears different position and orientation) However, other markers except for master one are well detected as green marker.", "So what was my fault or what I have to do more?", "Thank you."], "answer": [" ", " ", "Yes, you have to consider the corners counter-clockwise. But I have never used createMarker."], "url": "https://answers.ros.org/question/60422/multi-tag-bundles-detection-of-ar_track_alvar/"},
{"title": "UDPROS Protocol and Lost Packet/Sequencing Algorithms", "time": "2013-04-22 17:00:04 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Dear ROS Community,", "My recent adventure into development of a Java based UDPROS protocol left me with some unanswered questions that will be beneficial for the ROS Community to discuss, and create a specification of some sort.", " Timeout before dropping message entirely, or passing on received data with missing packets.", " Drop entire message immediately if a received datagram contains a new message id, and the preceding multi-packet message is incomplete. ", "Both options have advantages depending on the topic type, and robot requirements. Option 2 requires more processing power and energy usage to maintain timeouts, and Option 2 handles out of sequence/out of order messages/packets. Option 1 performs more quicker/efficiently, at the cost of more lost messages. Both options seem like they could be valuable if they are implemented in the UDPROS API."], "answer": [" ", " ", "In the current UDPROS I believe Option 1 is implemented as it is the simplest and provides the low latency performance for which most people select UDP.  If you want to start a discussion I suggest you restart this on ros-users@code.ros.org", "Thanks, I emailed ", "."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " Out of Order message handling. ", " Dropped message handling.", " Dropped Datagram Packets in a single message. ", " Out of sequence datagram packet handling.", " Wait for messages to complete in any/every order until a timeout is reached regardless of order until the entire message is received, and pass message into a topic queue. Drop message if message times out."], "url": "https://answers.ros.org/question/61318/udpros-protocol-and-lost-packetsequencing-algorithms/"},
{"title": "Getting failed to open device error for swissranger even after udev rules change", "time": "2012-12-22 12:59:57 -0600", "post_content": [" ", " ", "I'm attempting to set up a swissranger TOF camera in ROS to be used in RVIZ. I'm fairly new to ROS and have been working through the tutorials. I have installed ROS using the tutorials on the site on a clean Ubuntu installation, single user machine, running 12.04. The ROS installation is Fuerte. ", "I have followed the instructions on the ros.org tutorial", "as well as here some instructions I found on youBot.", "And have done some digging but, I gotta admit, I am totally lost. I am able to run the libMesaSRTester if I run 'sudo /usr/bin/libMesaSRTester' but it does not run properly if it is not run with sudo. I changed the udev rules as recommended. Here's the current version of my 46-mesa.rules file:", "I have also added myself to the usb group as recommended at the youBot page. ", "On running rosrun swissranger_camera swissranger_camera I get the following error:", "Did you finally solve the problem?  I ran into similar problems as you had.   Udev rules(46-mesa.rules) were set as recommended.  Added my self to usb group.  Got the error \"Exception thrown while connecting to the camera: [SR::open]: Failed to open device!\".   Did \"sudo chmod -R a+rwx /dev/bus/usb\" as suggested by rasmusan, then everything seemed to be ok as long as SR-3000 was on and the USB cable was connected.  However, if SR-3000 was powered off then powered on or the USB cable was disconnected then reconnected, then the error appeared, and \"sudo chmod -R a+rwx /dev/bus/usb\" was needed to get things going.  I am wondering if there is any permanent solution to fix the permission issue.  Thanks, CC", "My permanent solution is pretty gross to be honest. I gave the user who is running the pipeline god rights. Given the extremely limited environment our system is running in it works ok, but it may not be a viable solution for anyone else.", "Can you be more specific on \"god' rights?  I added myself to usb group, it didn't work without doing \"sudo chmod -R a+rwx /dev/bus/usb\" whenever SR-3000 was powered on or a USB cable was reconnected\", Then I added myself to root group, and it didn't work, either.\n\nThanks,"], "answer": [" ", " ", "I got this error too. The problem for me was, that the usb devices could only be accessed by root. The fix was:", "sudo chmod -R a+rwx /dev/bus/usb", "I'm running Kubuntu 12.04. Hope this can help.", "This worked for me as well, I had the same problem. Thank you!", "@ the users of swissranger_camera, I have some questions. Are you guys running the \"swissranger_camera\" ROS package and using \"PCL\" from ROS as well? I noticed that the pointcloud message publish by \"swissranger_camera\" no longer supports by PCL. How did you guys solve the problem?", "I got the same error, and the error was \"temporarily\" fixed by adding  sudo chmod -R a+rwx /dev/bus/usb as suggested.  However, I need to do it every time SR-3000 is turned back on or the USB cable is re-plugged in.  Is there a permanent fix?  I am running Ubuntu 12.04 + ROS fuerte on a 32-bit PC.\n\nThanks,\nCC"], "question_code": ["BUS!=\"usb\", SUBSYSTEM!==\"usb_device\", ACTION!=\"add\", GOTO=\"kcontrol_rules_end\"\n\n#Swissranger SR3k(old),SR3k,SR4k\nSYSFS{idVendor}==\"0852\", SYSFS{idProduct}==\"0074\", MODE=\"666\", GROUP=\"usb\"\nSYSFS{idVendor}==\"1ad2\", SYSFS{idProduct}==\"0074\", MODE=\"666\", GROUP=\"usb\"\nSYSFS{idVendor}==\"1ad2\", SYSFS{idProduct}==\"0075\", MODE=\"666\", GROUP=\"usb\"\nSUBSYSTEM==\"usb\", ACTION==\"add\", SYSFS{idVendor}==\"0852\", SYSFS{idProduct}==\"0074\", MODE=\"660\", GROUP=\"plugdev\"\nSUBSYSTEM==\"usb\", ACTION==\"add\", SYSFS{idVendor}==\"1ad2\", SYSFS{idProduct}==\"0074\", MODE=\"660\", GROUP=\"plugdev\"\nSUBSYSTEM==\"usb\", ACTION==\"add\", SYSFS{idVendor}==\"1ad2\", SYSFS{idProduct}==\"0075\", MODE=\"660\", GROUP=\"plugdev\"\n\nLABEL=\"kcontrol_rules_end\"\n", "usb_set_debug: Setting debugging level to 255 (on)\nusb_os_find_busses: Found 004\nusb_os_find_busses: Found 003\nusb_os_find_busses: Found 002\nusb_os_find_busses: Found 001\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 001 on 004\nskipping descriptor 0x30\nskipped 1 class/vendor specific endpoint descriptors\nerror obtaining child information: Operation not permitted\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 001 on 003\nerror obtaining child information: Operation not permitted\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 008 on 002\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 007 on 002\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 006 on 002\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 002 on 002\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 001 on 002\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 004 on 001\nskipping descriptor 0xB\nskipped 1 class/vendor specific endpoint descriptors\nskipped 6 class/vendor specific interface descriptors\nskipping descriptor 0x25\nskipped 1 class/vendor specific endpoint descriptors\nskipped 10 class/vendor specific interface descriptors\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 003 on 001\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 002 on 001\nusb_os_find_devices: couldn't get connect info\nusb_os_find_devices: Found 001 on 001\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nerror obtaining child information: Operation not permitted\nerror ..."], "url": "https://answers.ros.org/question/50976/getting-failed-to-open-device-error-for-swissranger-even-after-udev-rules-change/"},
{"title": "SLAM loop closure [closed]", "time": "2013-04-19 03:55:42 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "Firstly thanks for your time.", "I'm working on a two wheeled robot SLAM project in Matlab and am having trouble with the loop closure stage.", "The dropbox files linked below are the Matlab code and the data file it uses. The data has been created from a simulated robot which travels around a square.", "On the output simulation from the Matlab code, the green line is the robots path and the red dots are ultrasonic sensor measurements. As you will be able to see, the line is not a loop at the moment and this is my problem. ", "The mathematical basis of the code is from the following document (section 3).\ntinyphoon.com/rainbow/_tinyphoon/Documents/SimplePathPlanning.pdf", "Code Variables:\nLeftMotor and RightMotor are the angular velocities of each wheel. Time is the timestamp associated with each measurement.", "Matlab Code - dropbox.com/s/wodoytlejz46dfm/PathPlan.m\nData File - dropbox.com/s/z0bgc7im937hklq/18thApril2.mat", "Any help would be much appreciated.", "Thanks,\nSteve", "sorry ", ", but although your question is related to robots, it is not related to ROS. maybe you should try asking it somewhere else. cheers."], "answer": [], "url": "https://answers.ros.org/question/61152/slam-loop-closure/"},
{"title": "Global path not changing despite newly discovered obstacle", "time": "2013-05-06 23:13:06 -0600", "post_content": [" ", " ", "Hi,", "I'm using move_base to generate paths based on a map & odom generated by hector_mapping. The below rviz screenshot sums up my problem quite well: essentially as the new (middle) obstacle appeared in my LiDAR's view, the global path (in green) doesn't update. It seems properly inflated (blue). ", "So what happens is that move_base keeps going onto the global path until it arrives too close to the obstacle, and then it backs up (recovery mode). ", "Any ideas why my global path isn't changing?", " ", "My yamls:"], "answer": [" ", " ", "I was able to fix this problem by setting a smaller path_distance_bias for my base_local_planner. Now move_base veers away from this \"wrong\" global path & creates a new better global path.", " ", " ", " ", " ", ": I am not sure if the solution you have provided will work in every case. Here's my take on it.", "Take a look at the planner_frequency parameter at ", " I believe your navigation instance is not re-planning as the local_planner is not giving up control despite the obstacle. ", "On a differential drive robot, it can sometimes get difficult to balance the path and goal bias parameters to produce good behavior in all cases (in this particular instance you have found a solution by tweaking these parameters). An easier solution is to set planner_frequency to something other than 0, which forces replanning giving the local_planner a plan with a better chance of success.", "Hi, thanks a lot for your response, it made me discover a bunch of useful params that I had left as defaults! I've tried setting controller_frequency to non-zero values but my global path isn't recomputed - it still goes through the obstacle. I suppose it's not a big deal if the local planner avoids", "the obstacle successfully. Can you think of particular situations in which this \"hack\" would not work? Thanks again", ": I can't remember, and I use a different local planner these days. You should try changing the planner_frequency instead of controller_frequency.", "Oops yes I meant planner_frequency. I've changed it and I still have the same global paths..", "hi dear Ernest , i have exactly the same problem . is your problem solved ? , if yes , how can i force the trajectory planner to generate dynamic paths when an obstacle comes in front of robot ... ?! \n\nif any body have any idea its my pleasure to hear them ."], "question_code": ["   global_costmap:\n\n  global_frame: /map\n  robot_base_frame: /base_link \n  update_frequency: 1.0\n\n  static_map: false\n  rolling_window: true\n\n  width: 26\n  height: 26\n  resolution: 0.025\n\n  map_type: costmap\n\n\nlocal_costmap:\n  global_frame: /odom\n  robot_base_frame: /base_link \n  update_frequency: 1.5\n  publish_frequency: 1.5\n  static_map: false\n  rolling_window: true\n  width: 20.0 \n  height: 20.0 \n  resolution: 0.05\n\n  map_type: costmap\n"], "answer_code": ["path_distance_bias: 0.15 # weighting for how much the controller should stay close to the path it was given\n"], "url": "https://answers.ros.org/question/62295/global-path-not-changing-despite-newly-discovered-obstacle/"},
{"title": "rostopic pub", "time": "2013-05-16 05:14:59 -0600", "post_content": [" ", " ", "whenever I type in terminal:", "my robot smiles and has a green nose LED lit. ", "But, when I type it in a rospy script, I get a syntax error.", "HOW can I do this correctly. I have tried almost everything.\nThanks in Advance@@\n:-)"], "answer": [" ", " ", "You cannot just type commands into a python script. You'll need to use Python syntax. Check out the ROS beginner tutorials for the basics.", "If you just want a script that lists commands, use a shell script."], "question_code": ["rostopic pub call (/cmd_nose qbo_arduqbo/Nose [0,0,'test'] 2)\nrostopic pub call (-1 /cmd_mouth qbo_arduqbo/Mouth [0,0,'test'] [0,0,0,0,0,1,0,0,0,1,0,1,1,1,0,0,0,0,0,0])\n"], "url": "https://answers.ros.org/question/62895/rostopic-pub/"},
{"title": "Data Droping at transport layer", "time": "2013-04-15 21:19:26 -0600", "post_content": [" ", " ", " ", " ", "Can we do data droping and filtering at transport layer(TCP).", "I want to drop useless packets at Transport layer. To save the bandwidth,energy of nodes and reduce the congestion. So is it possible to drop packets at transport layer.", "Please elaborate.", "I want to drop useless packets at Transport layer. To save the bandwidth,energy of nodes and reduce the congestion. So is it possible to drop packets at transport layer."], "answer": [" ", " ", "If you want to do filtering at the TCP level there are lots of standard tools for doing that such as iptables.  "], "url": "https://answers.ros.org/question/60765/data-droping-at-transport-layer/"},
{"title": "Pioneer 3DX strange sonar behavior", "time": "2013-05-31 05:46:54 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm doing experiments with the frontal sonar ring present in the Pioneer 3DX using ROSARIA with ROS Fuerte. However, the output from the array wasn't what I expected. The sonar readings often return the maximum range of the sonar, even though the wall is clearly not so far away. I made a video where is possible to compare the sonar readings with an LFR in my campus corridors. Also, I have uploaded a photo of the corridor to give a general overview of the scenario.", "Yellow dots are the sonar readings (pointcloud published by ROSARIA), red dots are LFR scans and the green dots can be ignored. ", "I've tried both ROSARIA and the p2os drivers. Both gave me the same output. I also tried it with two more pioneers available in my lab, but they gave me the same results. Is there an hardware issue with the p3dx sonars? Does anyone know if changing the frequency of the sonar readings solves this issue?", "Best regards,", "Jo\u00e3o Santos", "I have worked with ARIA libraries, and have no problems with sonars... so I dont think this could be a hardware issue.", "Check the max range of sonar in the code. Just to confirm whether sonar is working fine  try getting some obstacle very close to sonar and see whether it detects it and slowly move it away from robot to get some rough estimate of range of sonar. "], "answer": [" ", " ", "That looks pretty much like what I would expect from sonar sensors. There's a reason why they are cheap and why LIDARs are generally used when one wants to have (more) accurate data.", "It clearly looks like the sensors don't like it when their emitted sound hits walls at oblique angles (as this means it is reflected away from the sensor, thus giving no return). Observe that the two sensors to the sides work reliably in your video and that these are the only ones that consistently emit at a right angle against the walls, thus retrieving strong returns. The material the environment is made of of course also plays a significant role in overall performance."], "url": "https://answers.ros.org/question/64065/pioneer-3dx-strange-sonar-behavior/"},
{"title": "rosbridge rostopic and javascript [closed]", "time": "2013-06-19 19:58:10 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I have ROS electric installed on a summit xl robot. I also installed rosbridge 1.0 and what I am trying to do is to get the output from rostopic echo ,i.e. rostopic echo /battery, to show on a webpage. How can I do that?"], "answer": [], "url": "https://answers.ros.org/question/65561/rosbridge-rostopic-and-javascript/"},
{"title": "OpenNI and RGBDSLAM Launch Files", "time": "2013-06-10 13:53:00 -0600", "post_content": [" ", " ", " ", " ", "I'm having a issue with the ROS RGBDSLAM installation in Fuerte. Whenever I run \"roslaunch rgbdslam kinect+rgbdslam.launch\" an empty RGBDSLAM GUI pops up. The terminal shows the error: ", "even though I have the Kinect Xbox 360 connected and LibFreenect works fine. What am I doing wrong?", "It appears to be an issue with OpenNI in particular, since when I run \"roslaunch openni_launch openni.launch\" the same error occurs. ", "Glancing at the source code in openni.launch and device.launch (in home/ros_workspace/rgbdslam_freiburg/rgbdslam/launch), I noticed that \"depth_registered.launch\" and \"depth.launch\" are referenced, but do not exist. Maybe that's the issue?", "Ex, see code section:", "Thank you!", "Your device is not found. I have seen reports that some drivers have problems with newer versions of the Xtion pro, maybe that holds true for your Kinect? Otherwise, have you tried a different USB port, preferrably a USB 2.0 port?", "This issue arises for Asus xtion pro live devices and new version of microsoft kinect \"kinect 4 windows\"", "If you have a Turtlebot and if your camera is draining power from your robot, make sure it's set to full mode on the dashboard. I have to do that every time I want to use openni + Kinect."], "answer": [" ", " ", " ", " ", "Hi,", "You can find the installation steps to get the new kinect 4 windows working from these links", "!msg/openni-dev/h0F6kYCNigs/BR6iqqFhSJ8J", "Before these installation,", "remove the old library files from ubuntu software center searching for libopenni.", "after installation,", "dont do rosdep install openni_launch.", "directly do rosmake openni_launch and it should work."], "question_code": ["[ INFO] [1370907710.759761259]: No devices connected.... waiting for devices to be connected\n", "<!-- Unregistered depth processing --> <include file=\"$(find openni_launch)/launch/includes/depth.launch\"\n       ns=\"$(arg depth)\">\n    <arg name=\"manager\" value=\"$(arg manager)\" />\n    <arg name=\"bond\" value=\"$(arg bond)\" />   </include>\n\n  <!-- Depth-to-RGB registration and processing -->   <include file=\"$(find openni_launch)/launch/includes/depth_registered.launch\">\n    <arg name=\"manager\" value=\"$(arg manager)\" />\n    <arg name=\"rgb\" value=\"$(arg rgb)\" />\n    <arg name=\"depth\" value=\"$(arg depth)\" />\n    <arg name=\"depth_registered\" value=\"$(arg depth_registered)\" />\n    <arg name=\"bond\" value=\"$(arg bond)\" />   </include>\n"], "url": "https://answers.ros.org/question/64731/openni-and-rgbdslam-launch-files/"},
{"title": "Errors using sicktoolbox_wrapper on Powerbot", "time": "2011-12-07 17:37:05 -0600", "post_content": [" ", " ", " ", " ", "I'm having an issue using sicktoolbox_wrapper on the MobileRobots Powerbot.  I've looked through the related threads but my issue appears to be a bit different.  For reference, these are the threads I'm referring to:", "The third seems most promising since it appears to be an issue related to MobileRobots and a solution for the Pioneer was found by adding a delay into the source code.  I also read the information in the ", " Abhijit provided regarding the laser integration board.  ", "Since I'm working with the Powerbot, there is in fact a switch on the side of the robot that must be turned on prior to using the laser.  I did so and gave it quite a bit of time to warm up, with all the lights green etc.  I have experienced the same issue others have addressed in the threads above, however, I'm not always observing the same output.  The weird thing is I'm not doing anything different each time.  For instance, one attempt will result in an output similar to thread 1) and 3):\n"], "answer": [" ", " ", " ", " ", "Errors related to \"missed scans\" in the 0.5 degree mode may be related to ", ". Try setting your laser to 1 degree mode and see if the missed scan errors go away.", "UPDATE:\nSee the resolution to the ticket. This \"missed scans\" functionality was removed, as it was not an accurate indication of scans being missed. Diagnostics were added to monitor the publishing rate of the driver.", "Thanks for the feedback.  How would I go about changing the degree mode?", "Set the ~resolution parameter (detailed here: ", " ) to 1.0 . You'd use a command like rosrun sicktoolbox_wrapper sicklms _resolution:=1.0 .", "Thanks.  I did in fact try that but alas, no luck on either platform.  Still getting the missed scans.", "See the update to my answer regarding the solution to this problem.", " ", " ", " ", " ", "I've edited the original question and have determined this isn't a PowerBot specific problem.  I've tried implementing this on another platform and have observed the exact same results, which are evidently missed scans.   ", "In all attempts on both platforms, the results are the same.  I'm wondering if this thread should be closed / reopened into a different thread since this is not only an issue with the PowerBot?", "You can edit the title too.  ", " ", " ", "Hey, were you able to solve the problem? I am having the same issue.", " ", " ", "I have the same problem!\nDid anyone come up with a solution?"], "question_code": ["usr@powerbot:~$ rosrun sicktoolbox_wrapper sicklms", " ", "*** Attempting to initialize the Sick LMS...\nAttempting to open device @ /dev/ttyUSB0\n", " ", "SickLMS::_setTermSpeed: ioctl() failed while trying to get serial port info!\n    NOTE: This is normal when connected via USB!\nSickLMS::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\n        Device opened!\n    Attempting to start buffer monitor...\n        Buffer monitor started!\n    Attempting to set requested baud rate...\nA Timeout Occurred!  2 tries remaining\nA Timeout Occurred!  1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n    Failed to set requested baud rate...\n    Attempting to detect LMS baud rate...\n        Checking 19200bps...\nSickLMS::_setTermSpeed: ioctl() failed while trying to get serial port info!\n    NOTE: This is normal when connected via USB!\nSickLMS::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\nA Timeout Occurred!  2 tries remaining\nA Timeout Occurred!  1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n        Checking 38400bps...\nSickLMS::_setTermSpeed: ioctl() fThe other platform is simply a laptop running Natty and ROS Diamondback.  I used an identical laser to that connected to the PowerBot.  I attempted this with two different serial-to-USB converters: FTDI FT232R and Prolific PL2303 both with and without a null modem cable. ailed while trying to get serial port info!\n    NOTE: This is normal when connected via USB!\nSickLMS::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\nA Timeout Occurred!  2 tries remaining\nA Timeout Occurred!  1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n        Checking 500Kbps...\nERROR: I/O exception - SickLMS::_setTerminalBaud: ioctl() failed!\nERROR ..."], "url": "https://answers.ros.org/question/12264/errors-using-sicktoolbox_wrapper-on-powerbot/"},
{"title": "Can I home modules using ipa_canopen_ros [closed]", "time": "2013-06-13 15:04:33 -0600", "post_content": [" ", " ", "I would like to home my modules using the ros portion of ipa_canopen.", "I have a powerball arm and I am using peak usb can device.  I am communicating fine and controlling arms fine.  I was just wondering if there was a function or process for homing the modules?  I would like to do this from my python scripts.", "I have tried the provided ipa_canopen_core tools for homing and I am not having success."], "answer": [], "url": "https://answers.ros.org/question/65099/can-i-home-modules-using-ipa_canopen_ros/"},
{"title": "What are prerequisites for ROS?", "time": "2013-08-01 22:28:53 -0600", "post_content": [" ", " ", " ", " ", "Hi, I am a newbie to ROS. I am an Electronics undergrad and I have worked in the field of robotics for two years. But most of my experience is related to electronics. I have designed power supplies, motor drivers for robots; I have done some work on sensors, sensor interfacing and calibration. I have worked on ATmega series microcontrollers, Arduino, PIC & MSP430 . I have also done few projects using MATLAB and LabVIEW. I familiar with C/CPP programming but that is mostly in windows- visual studio and eclipse IDE. I also know basics of python. I have no experience with programming in Linux. ", "I am currently working on opencv.", "I know my experience related to programming is very less, but I'm interested in implementing ROS on some custom robots. To do so, what are other things that I'll have to learn? Also what would you say about time that I will require to get started and complete a small project like simulating a simple robot using ROS?", "You should learn some of the basics of linux/BASH before diving into the ROS tutorials (e.g. ", "). There's quite a learning curve for Linux+ROS if you're completely new to it, but with your background you should be up and running soon!"], "answer": [" ", " ", "You have mentioned every thing that is needed and in fact you know it.\nAll you need is to attempt to work on it.\nto get confidence at initial stage you need to have an ROS supported robot and further you can manage it all for your own robot too."], "url": "https://answers.ros.org/question/70651/what-are-prerequisites-for-ros/"},
{"title": "Asctec_Proc - Timeout: 0 bytes available 3 bytes requested", "time": "2013-07-12 02:40:19 -0600", "post_content": [" ", " ", " ", " ", "Hi, i try to run ros on the Asctec Pelican, but when i run", "i get the following error:", "[ERROR] [1230811515.060357725]:     Read (-1): [ERROR] [1230811515.061434957]:   Read failed[ERROR] [1230811515.122562563]: Timeout: 0 bytes available 3 bytes requested[ERROR] [1230811515.123636163]:Error Reading Packet Header: Resource temporarily unavailable", "On the Atomboard i have installed Ubuntu 11.10 and ROS-Fuerte.", "The ros-node also sais it connects succsesfully  to /dev/ttyUSB0, Baudrate 57600.", "I have tried it with extern current and with the battery. ", "Does anyone know how to fix this problem ? ", "Please tell me if further information is needed"], "answer": [" ", " ", " is the Atomboard connected properly to the low level processor of the pelican? This might help:  ", "thanks...we was using a different cable, which of course do not work.\nnow all starts without any erros :)"], "question_code": ["roslaunch asctec_proc asctec_driver_nodelets.launch\n"], "url": "https://answers.ros.org/question/67184/asctec_proc-timeout-0-bytes-available-3-bytes-requested/"},
{"title": "Turtlebot unable to check laptop battery/BAT0/state", "time": "2013-07-17 20:31:17 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Ros version is Fuerte. Turtlebot unable to check laptop battery/BAT0/state.\nI think there is laptop battery information at battery/BAT1/state. How can I change it ?"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "How do you start it? The launch file in turtlebot_bringup has this path parametrized, so you can just pass another one in.", "thank you ^^"], "url": "https://answers.ros.org/question/67616/turtlebot-unable-to-check-laptop-batterybat0state/"},
{"title": "Kinect not getting power from Irobot create base?", "time": "2013-07-22 14:40:49 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base. ", "I read to a tutorial by Prof. Jason O'Kane he says that : \n", "I tried it but this is not working. The create base is connected to ttyACM0 port as the serial to USB connector definition."], "answer": [" ", " ", "Hey If you are using Kinect with turtlebot then you have to move the bot to start kinect. If you move it couple of inches then you'll start receiving data from kinect.  "], "question_code": ["rosrun create_node kinect_breaker_enabler.py\n"], "url": "https://answers.ros.org/question/68155/kinect-not-getting-power-from-irobot-create-base/"},
{"title": "Kinect power problem while connecting to the irobot create base? [closed]", "time": "2013-07-17 20:53:54 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base.  ", "I read to a tutorial by Prof. Jason O'Kane he says that : ", " ", "this command working well as I am getting power at pin 10-11-12 of DB25 but not getting power out of circuit board designed by Clear Path robotics. Actually this circuit board came with a turtlebot (with irobot create base)and I am using it with another independent I robot create base to connect the kinect but it is not working. ", "I tried this circuit back to the turtlebot with which it came and its working fine. \n", "What is the output when you run 'rosrun create_node kinect_breaker_enabler.py'. Do you have a multimeter you can use to measure the voltage output on the kinect power connector?", "Yeah, I got to check that and found that Irobot create base working fine and giving 14V on pin 10-11-12 with respect to GND at pin 10. ", "\nso its problem of the DB25 circuit board which I am willing to change now.", "What is the output of `rosrun create_node kinect_breaker_enabler.py` ?", "14V on pin 10-11-12 with respect to GND at pin 10 of the DB25 connector of Irobot create base.", "What is the console output of `rosrun create_node kinect_breaker_enabler.py` ?\n\nIs there any voltage on DB25 pin 19 (Digital output 0, which enables the 12V regulator)?\n\nIs there any voltage on the connector (NOT the DB25) that goes to the kinect?", "This problem is now got modified to a new form ...  I am modifying the question with a more detail at the end of the question ......... Please read it and reply.", "Dear Ahendrix ....... could you get the modified form of question ? please reply", "You still haven't provided the console output from running `rosrun create_node kinect_breaker_enabler.py` or indicated if there's any voltage on pin 19 of the DB25 connector."], "answer": [], "question_code": ["**rosrun create_node kinect_breaker_enabler.py**\n"], "url": "https://answers.ros.org/question/67618/kinect-power-problem-while-connecting-to-the-irobot-create-base/"},
{"title": "AR.Drone loses connection every-time I land it", "time": "2013-09-12 10:33:50 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hey guys, I have the AR.Drone 2.0 with the ardrone_autonomy using ROS. I followed this great tutorial: \nlink:", " and managed to get my joystick to fly the AR.Drone well. ", "When I take off, fly and land the drone, I can no longer take off again. I have to disconnect the battery, wait for the drone to recalibrate and run the .launch file again. I found this to happen sometimes when I used the keyboard from this tutorial also: ", "link:", "I wanted to know if anyone has experienced this problem. I have tried to just terminate the .launch file and re run in the command prompt but it seems like the only way is to physically restart the AR.Drone by pulling out the battery. Is there perhaps a command that can turn off and then on the drone? ", "Thanks so much!"], "answer": [], "url": "https://answers.ros.org/question/79011/ardrone-loses-connection-every-time-i-land-it/"},
{"title": "Different color for each cube in CUBE_LIST using std_msgs/ColorRGBA[]", "time": "2013-08-29 04:40:08 -0600", "post_content": [" ", " ", " ", " ", "Hey there,", "I have a problem creating a CUBE_LIST with different colors for each cube. Below is my code. The strange thing is, that when setColor is set to true, all cubes are drawn in the same color (as expected in green), but when it is set to false, nothing is shown at all. How exactly is the use of the markerMsg attribute colors? Do you see the mistake I made?\nIn the documentation it states \"In visualization 1.1+ will also optionally use the colors member for per-vertex color.\". How can I find out which version of visualization I am using? I have ros fuerte installed.", "Thank you so much for your help!\nJens", "edit: solved! added the line \"markerMsg->colors.clear();\" now the number of color values and points is the same and it works.\nThanks everyone!", "Since i is in the range of zero to nine, could the value you are setting in the red channel be rounding down to zero?", "Good point, but that's not it. Changed it to a switch case statement with 5 \"hard coded\" colors, still not working. Shouldn't the boxes still be drawn but black then?\nI just added clearing the colors vector in the markerMsg, as I forget that earlier. That was it!!\nThanks everybody for the helpt!"], "answer": [" ", " ", "One problem is that you are not using a ", ". For more than one Marker always use ", ". If you are not using Marker Array then push ", " inside the for loop. If you change to MarkerArray then use publisher of type  ", "  Change this and tell me if you still have some problem ", "are you really getting more than one marker on your screen in rviz?", "Yes, I am getting more than one. Why shouldn't I use CUBE_LIST, if I am ok with all cubes having the same size and attributes except for color ans position? This should be working with CUBE_LIST, or not?\n", "By the way, I am having the same issue using LINE_LIST.\nWhy should the publish method be called within the for loop? That really makes no sense to me... The msg is filled with 10 cubes and then published at the end. It is working if setColor is set to true!"], "question_code": ["ros::Publisher pubTrackMarkers;\npubTrackMarkers = n.advertise<visualization_msgs::Marker>(\"/track_markers\",queue_size, true);\nvisualization_msgs::Marker markerMsg;\n\nmarkerMsg.header.frame_id  = \"/trackMarkers_frame\";\nmarkerMsg.ns = \"trackMarkers\";\nmarkerMsg.action = visualization_msgs::Marker::ADD;\nmarkerMsg.pose.orientation.w = 1.0;\nmarkerMsg.type = visualization_msgs::Marker::CUBE_LIST;\nmarkerMsg.scale.x = 0.8f;\nmarkerMsg.scale.y = 0.8f;\nmarkerMsg.scale.z = 0.8f;\nif (setColor){\n    markerMsg.color.r = 0;\n    markerMsg.color.g = 1;\n    markerMsg.color.b = 0;\n    markerMsg.color.a = 1;\n}\n\nfor (int i=0; i<10; i++){\n    geometry_msgs::Point temp;\n    temp.x = i;\n    temp.y = 1;\n    temp.z = 0.5;\n    markerMsg.points.push_back(temp);\n    std_msgs::ColorRGBA c;\n    c.r = (float)i/10.0;\n    c.g = 0;\n    c.b = 0;\n    c.a = 1;\n    markerMsg.colors.push_back(c);\n}\n\npubTrackMarkers.publish(markerMsg);\nmarkerMsg->points.clear();\nmarkerMsg->colors.clear();\n"], "answer_code": ["Marker Array", "Marker Array", "pubTrackMarkers.publish(markerMsg);", "<visualization_msgs::MarkerArray>"], "url": "https://answers.ros.org/question/75502/different-color-for-each-cube-in-cube_list-using-std_msgscolorrgba/"},
{"title": "questions about irobot create", "time": "2013-11-10 12:36:54 -0600", "post_content": [" ", " ", "Hello, I'm interested in getting a Create for a project I'll be working on, and wanted some information about it from somebody that already has one:", "How much weight can it safely carry? I talked with Irobot's tech support and they told me the maximum is 5lb, but searching on the internet it seems like this limit is actually not as strict as it appears to be. I'm asking because I'd need to put a 3kg laptop on top of it, which would mean ~3.5-4kg if you also consider the kinect and eventual supports for both. I guess I could use a netbook and send the data I need to another computer, but I wanted to avoid the additional overhead of the wireless link.", "For how long does it run using AA batteries? I'm inclined on not getting the battery pack, since I'd be using the robot in europe, so I'd also need a transformer if I went with the battery pack option. ", "Thanks!"], "answer": [" ", " ", "Since this question isn't ROS related, it's probably better suited for a different venue such as the ", "."], "url": "https://answers.ros.org/question/99576/questions-about-irobot-create/"},
{"title": "Select published topics", "time": "2013-10-23 07:08:19 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I'm using the  openni_camera and openni_launch stacks with ROS fuerte on a platform with low processing power.", "I only need two topics: /rgb/image_rect and /depth_registered/image_rect. Is there a way to only publish those topics in order to speed up the processing time?", "Thanks,"], "answer": [" ", " ", "You do not have to do that in ROS. Data will (should) be computed only if there is at least one subscriber listening to your topic.\nThe whole goal here is to reduce the configuration burden of each node. Think of the topic list as a list of capabilities and not a list of produced data.", "Is there a way to get topics from more than one Kinect?", "I guess if you launch the nodes twice in different namespaces it should work. You may need to connect your two kinects on two different USB controllers though.", "Well as I explained in my question here: ", " I'm not sure exactly how to modify the launch file to launch separate nodes. Any ideas?\n\nEdit: Thanks for the suggestion Thomas, it helped me figure it out."], "url": "https://answers.ros.org/question/93641/select-published-topics/"},
{"title": "Raspberry Pi openni USB interface not supported", "time": "2013-05-15 21:16:39 -0600", "post_content": [" ", " ", " ", " ", "I have managed to successfully install ros groovy on my raspberry pi and openni. Unfortunately when I connect my Asus Xtion to the raspberry pi (with a powered hub), it does detect the sensor, but I receive and error of ", ". Is there any way that I can fix this error?"], "answer": [" ", " ", " ", " ", "I did not get it to work with OpenNI either but OpenNI2 works with some minor changes. It should compile out of the box from my repo using make:\n", "I also wrote an openni2_camera package which runs on the raspberry and can stream the depth image as a ROS message with 30fps (when overclocked)\n", "After building OpenNI2 successfully and catkin_make ran through, navigate into OpenNI2/Bin/ReleaseX and call rosrun openni2_camera openni2_camera_node from here. Otherwise it will not find the Openni2 libraries", "I will definitely try this out soon. Thanks", "Could it work only with the Asus Xtion? or could also with the Kinect?", "I did not get it to work with the Kinect", ": maybe your openni2 have a problem with calibration, I can not set using my calibration, it still use defaut calibration, but I am working with Linux Arm and the resolution of depth in ARM just 160*120, but with rgb it is 320*240. Do you know how to I fix it?", " ", " ", "I have generated a remote point cloud. Thanks to piggy backing Kalectro's work I was able to stream the depth image to a laptop (as ros master) that I simply copied and reduced all openni_launch files to just depth processing in the processing include launch file changing the camera parameter to openni2_camera. My spin was transporting the stream off my Mike Ferguson Arduino based robot with a second Raspberry Pi acting as a wireless bridge with a cat5 crossover cable to the XTION connected Pi (because its USB is to bogged to transmit 802.11 usb). Thanks Mike and Kalectro for a cool Raspberry Pi Turtlebot Navigation capable robot! ", " ", " ", "so kalectro i tried to run your openni2_camera_node and i get the following errors. where do you think Im going wrong here?", "I updated my answer but this information with a little more explanation should also be in my README file in the openni2_camera repo", "I did read it, it was a bit confusing but now I understand and it works!!! :) thank you kalectro. Ive been spending weeks trying to fix this!!", "I was confused as well which might be the reason why the documentation does not make that much sense. I am glad that it now works for you. Would you be so kind and edit the Readme to make it clearer? I would then merge the changes", "Alright I did that. I hope its alright. So kalectro can you tell me how you can produce and view pointcloud on rviz from the depth images your openni2_camera_node streams. I am not very familiar with what it takes in the background to achieve that. With openni.launch, getting pointclouds was simple ", "I did not produce point clouds yet but this should be possible with the use of PCL. Feel free to try to adapt the code from openni_camera to also output a point cloud. I think this could be useful for other people as well", "I managed to run openni_launch successfully on the raspberry pi now. follow the steps on this link to install the Xtion drivers on the pi. http://mewgen.com/Ge107_files/20120921%20Setting%20up%20Rasberry%20pi%20for%20the%20Xtion%20and%20kinect.html ", "@cryus are you able to help me install openni_launch on the rpi please?", "hi boog. Unfornunately I have installed openni_launch months ago on ros groovy so i barely remember what I did. what I can tell you about openni_launch is that i manage to get the depth images from the xtion but with a lower frame rate. Unfortunately openni_launch crashes when I try to get rgb images from the camera. So I suggest you mabye follow what Kalectro has done and use that. and generating point clouds on the pi itself is extremly slow. I let that process happen on my computer rather than the pi."], "answer_code": ["[ INFO] [1368968043.171416263]: creating image_transport... this might take a while...\n[ERROR] [1368968049.678261385]: Tried to advertise a service that is already advertised in this node [/openni2_camera/set_camera_info]\n[ INFO] [1368968049.696679863]: using default calibration URL\n[ INFO] [1368968049.716394302]: camera calibration URL: file:///home/pi/.ros/camera_info/rgb.yaml\n[ERROR] [1368968049.727926974]: Unable to open camera calibration file [/home/pi/.ros/camera_info/rgb.yaml]\n[ WARN] [1368968049.743592530]: Camera calibration file /home/pi/.ros/camera_info/rgb.yaml not found.\n[ WARN] [1368968049.755016205]: Using default parameters for RGB camera calibration.\n[ INFO] [1368968049.771172746]: using default calibration URL\n[ INFO] [1368968049.786680305]: camera calibration URL: file:///home/pi/.ros/camera_info/depth.yaml\n[ERROR] [1368968049.790892185]: Unable to open camera calibration file [/home/pi/.ros/camera_info/depth.yaml]\n[ WARN] [1368968049.802418858]: Camera calibration file /home/pi/.ros/camera_info/depth.yaml not found.\n[ WARN] [1368968049.825350207]: Using default parameters for IR camera calibration.\n[ERROR] [1368968049.832703998]: Device could not be initialized because     Found no files matching './OpenNI2/Drivers/lib*.so'\n"], "url": "https://answers.ros.org/question/62867/raspberry-pi-openni-usb-interface-not-supported/"},
{"title": "Hokuyo laser configuration", "time": "2013-10-16 21:25:05 -0600", "post_content": [" ", " ", " ", " ", "Hi guys,", "I am currently using URG-04LX hokuyo laser and there is one problem that every time I power on this laser I need to reconfigure it all over again. Whenever I \"ls -l /dev/ttyACM0\", the feedback will always be ", "I was thinking could I possibly save the configuration(", ") or is there any launch code that would possibly do this job?"], "answer": [" ", " ", "Here is the more fancy udev solution that we use (needs the hokuyo_node package) and /etc/ros/run.sh:", "Besides access rights this uses the getID util in hokuyo_node to provide device symlinks as /dev/sensors/hokuyo_SERIAL to enable identifying specific sensors neverminds which ttyACM they land at.", "/etc/ros/run.sh enables ROS", "/etc/ros/setup.sh is a symlink to the setup.sh in /opt/ros/DISTRO/setup.sh", "What is the different between your solution and the solution provided by ", " ?", "This one also provides device ids based on the serial number. Also it sets access rights for any user (other), not only dialout, but you can configure that.", " ", " ", "I would recommend adding your username to the group ", ":", "How does this make a difference? What is suppose to happen after I enter this command?", "That is what you were asking for. It gives your user permanent access rights to this device so you don't have to \"chmod\" any more.", "The idea behind the command is to add your user to the group, which has read/write access for `/dev/ttyACM0` by default instead of setting read/write access for all users every time again.", "I can confirm that this has been THE  way to solve access right issues for many sensors over here. It should be the preferred (simplest) solution for any /dev/tty* access issues."], "question_code": ["crw-rw---- 1 root dialout 166, 0 Oct 17 15:10 /dev/ttyACM0"], "answer_code": ["SUBSYSTEMS==\"usb\", KERNEL==\"ttyACM[0-9]*\", ATTRS{manufacturer}==\"Hokuyo Data Flex for USB\", ATTRS{product}==\"URG-Series USB Driver\", MODE=\"0666\", GROUP=\"dialout\", PROGRAM==\"/etc/ros/run.sh hokuyo_node getID %N q\", SYMLINK+=\"sensors/hokuyo_%c\"\n", "#!/bin/sh\n. /etc/ros/setup.sh \nrosrun $@\n", "dialout", "sudo adduser $USER dialout\n"], "url": "https://answers.ros.org/question/91721/hokuyo-laser-configuration/"},
{"title": "Ballpark miminum CPU/RAM requirements for moveit packages", "time": "2013-10-31 07:30:22 -0600", "post_content": [" ", " ", " ", " ", "I am currently using ROS fuerte (and Ubuntu 12.04 LTS) and running about 10 nodes in an embedded system (PC104 Atom 1.2 GHz).  I would like to use MoveIt's manipulator kinematic packages (yes I realize MoveIt uses ROS Groovy and later) to calculate forward and inverse kinematics, as well as path planning for the end-effector.", "I am planning to change processor boards and would like to know what kind of processor and RAM requirements are needed to handle kinematic calculations and path planning.  Would you have recommendations on processor type (Haswell/Atom/etc?) and RAM needs to add MoveIt's packages to my application?  ", "I don't need to run a visualizer in the embedded hardware, just the calculations portion.\nAnd I'm looking for a low power solution.\nIn addition, I would like to have some headroom in the new processor to handle other kinds of calculations in the future.", "Thank you for sharing your experiences.", "What kind of application do you target? For what kind of robot do you want to generate motion? Obviously depending on the number of DOF your robot have and the difficult of your problem, the required CPU will differ."], "answer": [], "url": "https://answers.ros.org/question/96521/ballpark-miminum-cpuram-requirements-for-moveit-packages/"},
{"title": "hector_quadrotor color settings for laserscan", "time": "2013-10-04 01:53:39 -0600", "post_content": [" ", " ", "Hi,", "I am trying to reproduce the same color behaviour from the \"hector_quadrotor outdoor scenario demo\" youtube clip. In this clip in the top down view from rviz the hector team gets a nice color visualization of the laserscan.", "What settings do you choose in rviz under the LaserScan category to achieve the same?", "I tried various and I'm currently using:", "With this setting I get a very colorful image but to me the colors don't make sense at all. There is no consisting color change as it can be observed in the hector youtube clip. For example I  get green lines after blue ones and then blue again.", "I use Ubuntu 12.04 and ROS fuerte and run the outdoor_flight_gazebo.launch file from the hector_quadrotor_demo package. ", "Thanks a lot."], "answer": [" ", " ", "The culprit is the \"autocompute value bounds\" option. This means that for every scan the min/max values for coloration are computed in isolation, so coloring is not consistent across multiple scans. Switching off the autocompute, you can set the bounds yourself manually. It\u00b4s of course advisable that you are using some reasonable fixed frame for this to work as intended.", "Perfect, thanks! I tried it without \"autocompute value bounds\" but due to wrong min/max values I never got the desired result. Now it works!"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Topic: /scan ", "Queue Size: 10", "List item", "Selectable: yes", "Style: Points", "Alpha:10", "Decay Time: 10", "Position Transformer: XYZ", "Color Transformer: AxisColor Axis: Z", "Autocompute Value Bounds: yes ", "Use Fixed Frame: yes"], "url": "https://answers.ros.org/question/87231/hector_quadrotor-color-settings-for-laserscan/"},
{"title": "Using SMACH to monitor robot battery level", "time": "2013-10-21 07:00:52 -0600", "post_content": [" ", " ", "I'm wondering if anyone knows how to program the following simple scenario using SMACH.", "The robot monitors its battery level on a topic called /battery_level using a SMACH MonitorState which is part of the state machine SM_MONITOR_BATTERY.  In the meantime, the robot navigates around a series of waypoints in succession using a state machine SM_NAV_WAYPOINTS.", "I want the overall state machine to behave as follows:  If the battery level falls below a threshold (SM_MONITOR_BATTERY returns 'invalid'), SM_MONITOR_BATTERY transitions to another state called RECHARGE that moves the robot to the docking station (NAV_DOCKING_STATION) and recharges the robot.  In the meantime, the SM_NAV_WAYPOINT machine is paused or terminated until the battery is recharged (SM_MONITOR_BATTERY returns 'valid').", "I can make both the recharging behavior and the waypoint navigation behavior work on their own but I have only a vague idea how they can be combined using the Concurrence container.  I've tried for days to get the right syntax and I have failed.", "Any help would be greatly appreciated.", "--patrick"], "answer": [" ", " ", "This sounds similar to something I had to do last year - I posted a simplified version of my final state machine ", ".", "However, I assume that you already found that, and it hasn't helped. You might get better answers if you post what you've tried so far about and ask what's wrong with it. ", "Hi Linzey--I think this is exactly what I need so many thanks!  You're right--I did read your Wiki page some time ago but forgot where it was until now."], "url": "https://answers.ros.org/question/92891/using-smach-to-monitor-robot-battery-level/"},
{"title": "Not All Serial Communication Captured with Neato XV11", "time": "2013-11-22 20:06:49 -0600", "post_content": [" ", " ", " ", " ", "I've been trying to get a Neato XV11 up and running following the first part of Fergy's tutorial (1) and using the drivers updated for firmware 3.1 (2) by Vegelius (mine is running 3.2, but API looks the same). I was running into a problem I've seen a couple people have had \"Waiting on transform from base_link to map to become available before running costmap, tf error: \" that I thought was just my own misunderstanding of tf /ROS but have since figured out is related to topics not being published, I think because of a serial communications issue. ", "If I run the Neato node (roslaunch neato_node bringup.launch) immediately after I restart the base by holding down the power button, the LDS (lidar scanner) doesn't power up/spin, but messages are published to /odom, /base_scan (all ranges are 0) and other topics as expected. Navigation loads as well upon roslaunch neato_2dnav move_base.launch, and I can see the map distributed with the code in RViz. If I have picocom open before connecting, when I connect I see the commands getMotors and getScanRanges echoed back. All's well, expect my laser scanner isn't spinning. If I try to start a new connection from picocom to manually start the LDS rotation, neato_node errors out.", "Fergy's tutorial (first link) notes this failure to spin as a common problem, and recommends restarting the node. However if I re-run the bringup.launch file, the LDS spins up but nothing is published on the aforementioned topics. There are no errors, except the aforementioned \"Waiting on transform from base_link...\" and \"no laser scans published on base_scan.\" I suspect this has something to do with how Python (specifically in the driver code, neato_driver.py) is handling the serial port. If I run picocom on /dev/ttyUSB0, it flushes responses from the buffer ", " that the python code is supposed to handle (scan ranges, sensor data), and continues to capture them thereafter. That doesn't seem right.", "I'm not sure what approach to take: either 1) figure out how to get the LDS running while everything is publishing properly or 2) figure out why things aren't publishing upon restarting the node.", "In case it's useful, I am running Hydro on a 2012 Macbook Air running Ubuntu 12.10. API command of the XV-11 work fine through picocom.", "1) xv11hacking.wikispaces.com/Connecting+to+ROS", "2) github.com/vegelius/neato_robot/commit/c8ad29d9c861f82d27729c569cedf62db42d3ace", "Hi,\n\nI have the same issu with the serial port. Did you manage to solve it?\nFYI, I'm using  neato ros package wrote by Mike Ferguson, on Groovy, Ubuntu 12.10.", "Not as of mid March, though I haven't had much time to work on it."], "answer": [], "url": "https://answers.ros.org/question/104216/not-all-serial-communication-captured-with-neato-xv11/"},
{"title": "Is there a Catkin unit test report XMLT (for pretty printing)?", "time": "2013-11-15 14:40:33 -0600", "post_content": [" ", " ", "Has anybody come up with XMLT files to convert gtest/nosetest XML reports to a more human-readable form? I find it hard to tell at a glance if all tests passed, so something condensed and uniform would be nice. (E.g. if all lines are green, then I don't need any more detail.)"], "answer": [" ", " ", "For a (ascii) summary of (failing) tests you can call ", ".", " ", " ", "Thanks, that's perfect. Is this documented anywhere?", "It was not covered by the tutorials yet. I just created a page which describes the usage: "], "answer_code": ["catkin_test_results path/to/test/results"], "url": "https://answers.ros.org/question/101841/is-there-a-catkin-unit-test-report-xmlt-for-pretty-printing/"},
{"title": "Turtlebot2 Kobuki usb error", "time": "2013-12-04 02:40:56 -0600", "post_content": [" ", " ", " ", " ", "Hi guys I need a little help we have 5 turtlebots in our lab, 2 work fine but we're having trouble with the other 3. They power up fine but do not show up as a device (either /dev/kobuki or /dev/ttyUSB*) ", "dmesg outputs the following error message:", "hub 2-1:1.0: connect-debounce failed, port 4 disabled", "We've tried different cables, laptops, fully charging the robots and switching to firmware update mode and get the same error. ", "Any help would be appreciated.", "Did that ever work? If the same procedure/laptop works on some and not others, this might be a hardware problem and might be something for the support.", "Yes, contact us directly on the address jihoonl posted above. We'd like to investigate the problem as well. Thanks."], "answer": [" ", " ", " It might be a hardware issue. I would recommend to contact  "], "url": "https://answers.ros.org/question/106891/turtlebot2-kobuki-usb-error/"},
{"title": "DiagnosedPublisher window size seems wrong. [closed]", "time": "2013-11-26 04:40:55 -0600", "post_content": [" ", " ", "I'm not sure what's going on here. I have a DiagnosedPublisher (on Groovy) created like this:", "Here, expectedFrequency_ is 15.0. When I check the diagnostics in rqt_robot_monitor I get the following:", "According to the ", " (and actually also from what I can see in  the code that is linked there) the last parameter to FrequencyStatusParam should be the window size in number of events. So with 15Hz frequency, I'd expect roughly a window length of 0.66s with 10 events. However, I'm getting a duration of the window of 10s and 10 * 15Hz = 150 events in a window.", "I can't really explain what's going on here. Am I setting this up somewhere wrongly or is this functioning as intended? The behavior seems to match those observations. If I change the window size to e.g. 7, I get a 7s window. When I unplug and re-plug the device it also takes about that long until diagnostics goes back to green."], "answer": [], "question_code": ["diagnosticPub_ = new diagnostic_updater::DiagnosedPublisher<sensor_msgs::LaserScan>(pub_, diagnostics_,\n      // frequency should be target +- 10%.\n      diagnostic_updater::FrequencyStatusParam(&expectedFrequency_, &expectedFrequency_, 0.1, 10),\n      // timestamp delta can be from 0.0 to 1.3x what it ideally is.\n      diagnostic_updater::TimeStampStatusParam(-1, 1.3 * 1.0/expectedFrequency_));\n"], "url": "https://answers.ros.org/question/104906/diagnosedpublisher-window-size-seems-wrong/"},
{"title": "Brand new Roomba 790 Malfunctioning", "time": "2013-12-19 18:28:40 -0600", "post_content": [" ", " ", "Hi,", "Hoping someone can help with what seems to be a defective ", " Roomba 790.\nI've videoed the issue for ease.\nyoutube.com/watch?v=ns9IxxboVSs", "Thanks in advance.", "It could need a software reset.   First make sure the clean button is not lit.  If it is lit, press and hold the clean button until it goes off. Press and hold the dock and the spot buttons at the same time for 30 seconds.  Charge the battery for a good 16 hours after the reset.  "], "answer": [], "url": "https://answers.ros.org/question/112261/brand-new-roomba-790-malfunctioning/"},
{"title": "ardrone not detecting tags [closed]", "time": "2013-12-29 00:50:07 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello,", "i hope someone could help with ardrone tag recognition. I am subscribing ardrone/navdata in my code and i receive all values except for the tag detection values, which are always 0 or empty. When i echo this topic from my terminal it seems as if my drone is not recognizing them at all. I have these orange/green stripes as tags and an ardrone 2.0.\nDo i need to calibrate the camera or something like this? (I am using ardrone_autonomy)", "I'd be glad if someone could help me! (And sorry for my english ;))", "(Oh i have marked this post as a wiki.. how do i delete this? XD)", "Has anyone got an idea of what is wrong?", "I changed the enemy_color in the launch-file and tried it this way, but nothing happened. I really need your help :( Is my drone broken maybe? Anyone help me plz :/", "I hope we can get an answer for this problem. I am also having the same problem. All the parameters for tag detection seems correct. The drone detects tags on iphone app. But not on ROS. Also tried the ready package available on robohub but no chance. Detected tag number and tag positions are always 0 never updated."], "answer": [], "url": "https://answers.ros.org/question/113651/ardrone-not-detecting-tags/"},
{"title": "Rotating Hokuyo on the Atlas?", "time": "2013-12-20 06:27:58 -0600", "post_content": [" ", " ", " ", " ", "This isn't quite ROS-related, but I figure someone around here would know - from watching the DARPA challenge and the Atlas robot, I am very curious to know what the teams use to rotate the Hokuyo lidar 360 degrees. Anyone know what the mechanism/hardware is? How are they powering/getting the data from a unit that is rotating 360deg?"], "answer": [" ", " ", "The hardware is the Carnegie Robotics MultisenseSL sensor head, which has the Hokuyo spinning on a slip ring, allowing for continuous rotation. See also ", " for some info.", "Awesome, thanks!", " ", " ", " ", " ", "To make a 3D point cloud to sense the environement.", "This is realy precise.", "Some explaination and results here:\n"], "url": "https://answers.ros.org/question/112376/rotating-hokuyo-on-the-atlas/"},
{"title": "Local Planner does not plan through inflated costs?", "time": "2013-11-18 23:25:16 -0600", "post_content": [" ", " ", "Hi,", "I have a problem with my local planner (base_local_planner). ", "\nAs you can see on the screenshot, the costmap of the local planner does't take inflated obstacles into account. ", "\nBut why? Because hitting this inflated area with the side of my robot is possible without having a collision. ", "\nIs there some parameter to quickly adjust this behaviour?  ", "Btw: These yellow points in the middle of the grey inflated area are obstacles.", "Regards ", "\nMax ", " ", "Hi Max. A) What distro are you running? B) What is the point cloud/color map in the screenshot showing? (specific topic please)", "That's a point cloud from the base_local_planner: /move_base/TrajectoryPlannerROS/cost_cloud", "What is the red polygon and what is the green polygon?", "green polygon is the static footprint, red one is dynamic footprint"], "answer": [" ", " ", " ", " ", "There is ROS package called ", ". \nThe costmap_2d package provides a configurable structure that maintains complete info about all position in the map in the form of a occupancy grid, which helps robot navigate.", "Particularly for your problem, there is a parameter called \"inflation_radius\": default set as 0.55m.\nI am using ROS fuerte. In my system this parameter is found at:\n", " in ", "For more info, you should refer to the ", ".", "In detail it is the cost_inscribed parameter.", " ", " ", "It looks to me like the planner ", " taking inflated obstacles into account. The lack of colored points around the yellow points indicates that the planner is not even considering driving there. ", "If you are having problems not indicated in the screenshot (like collisions in planning) then there is a separate problem with the planner. "], "answer_code": ["costmap_2d", "/opt/ros/fuerte/stacks/navigation/costmap_2d/cfg/cpp/costmap_2d/", "Costmap2DConfig.h"], "url": "https://answers.ros.org/question/102621/local-planner-does-not-plan-through-inflated-costs/"},
{"title": "Fatal - pan_tilt_port: No motors found", "time": "2014-01-10 20:01:50 -0600", "post_content": [" ", " ", " ", " ", "Hello Everyone, I'm following the tutorial dynamixel_controllers/Tutorials/ConnectingToDynamixelBus and I cannot found my motor AX12+. I'm testing just one AX12+ on Linux 12.04 and 13.04 and both do not work.", "My ", " output is:", "When I ran the roslaunch command my last lines before the error are:", "I already tried with 3 different AX12+ motors and always the same error. I'm using a 9V power supply and seems perfectly working, the motor blink once when the power source is connected. ", "Thank you very much for the attention.", "You should post your launch and parameter files."], "answer": [], "question_code": ["ls -la /dev/ttyUSB", "crw-rw-rw- 1 root dialout 188, 0  1\uc6d4 11 16:52 /dev/ttyUSB0\n", "setting /run_id to 4e5d5690-7a95-11e3-bc0f-e0cb4e79fce2\nprocess[rosout-1]: started with pid [28760]\nstarted core service [/rosout]\nprocess[dynamixel_manager-2]: started with pid [28772]\n[INFO] [WallTime: 1389426744.355062] pan_tilt_port: Pinging motor IDs 1 through 25...\n[FATAL] [WallTime: 1389426746.482201] pan_tilt_port: No motors found.\n"], "url": "https://answers.ros.org/question/117006/fatal-pan_tilt_port-no-motors-found/"},
{"title": "Freenect_launch does not detect Kinect, BeagleBoard-xM, Ubuntu [closed]", "time": "2014-01-14 04:10:02 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm trying to run the freenect_launch package on a BeagleBoard-xM (Rev C) running Ubuntu 13.04. The aim is to record a short rosbag of kinect data and store it, to be later transmitted for offboard processing.", "The prebuilt demo image of Ubuntu was installed following the instructions on the elinux page for BeagleBoardUbuntu.", "I installed ros-hydro-freenect-launch using apt-get.", " successfully launches roscore and starts the kinect-related processes, but does not detect the kinect. It repeatedly displays ", "I've tried blacklisting the gspca_kinect module, but to no avail.", " shows that the Kinect is recognized: ", " ", " ", " ", " ", "I'd really appreciate if someone could help getting freenect to recognize the kinect, or suggest alternatives.", "EDIT 1:", "We are no longer facing this issue with the latest release of Ros-hydro for Ubuntu-ARM (the hydro/Installation/UbuntuARM page has been updated on 18th Jan 2014) with the namniart repositories. We have also switched over to a Pandaboard (Rev A3) for greater computing power, so we cannot say for sure that the issue has been resolved on the Beagleboard-xM.", "After installing ros-hydro-freenect-stack on Ubuntu 13.04 (OMAP4 Desktop Image) on the Pandaboard, freenect.launch detects the Kinect and publishes topics successfully.", "The last time I tried this, it didn't work.", "Thank you for the response. Freenect-launch worked with the Kinect on a Pandaboard after the recent repository update.", "Awesome! Can you post that as an answer? (Yes, you're allowed to answer your own question)"], "answer": [" ", " ", "On Pandaboard Ubuntu 13.04, hydro  freenect launch says:", "process[camera/disparity_registered_sw-17]: started with pid [3840]\nprocess[camera/disparity_registered_hw-18]: started with pid [3884]\nprocess[camera_base_link-19]: started with pid [3902]\nprocess[camera_base_link1-20]: started with pid [3927]\nprocess[camera_base_link2-21]: started with pid [3948]\nprocess[camera_base_link3-22]: started with pid [3963]\nrostopic list  shows lots of topics, but echo shows they are all empty of data.", "This is the same for openni launch.", "I am guessing something to do with USB 3 vs 2?  -or- bad calibration file?", "This is not an answer. Please ask it as a new question."], "url": "https://answers.ros.org/question/117716/freenect_launch-does-not-detect-kinect-beagleboard-xm-ubuntu/"},
{"title": "OpenNI - no device connected (kinect) [closed]", "time": "2014-01-23 00:57:08 -0600", "post_content": [" ", " ", " ", " ", "Hey everyone,", "I'm running ros hydro, ubuntu 12.04 and installed openni with \"sudo apt-get install ros-hydro-openni-launch\". after that i run \"roslaunch openni_launch openni.launch\". openni starts, but in the end it says", "\"\n\" ", "lsusb lists 3 microsoft devices,- as it should. my kinect led is blinking green. the output of \"rostopic list\" is", "\" eddi@Eddi:~$ rostopic list /rosout /rosout_agg \"", "When i try \"killall XnSensorServer\" it says \"XnSensorServer: Kein Prozess gefunden\" (no process found) \"XnSensorServer\" --> \"XnSensorServer: Befehl nicht gefunden\" (command not found)", "what am i doing wrong? (i am a complete beginner)", "thanks for your help!!!", "Robin", "Hi Robin, apparently there is problems with OpenNI devices on ROS Hydro, which sensor model are you using?"], "answer": [" ", " ", " ", " ", "Alright, I know that you've already installed openni-camera but have you installed all openni related packages?  I.e. sudo apt-get install ros-hydro-openni*?  Sometimes packages fall through the cracks.", "There are a couple other tools you can use to help us determine what's wrong.  Aside from tutorials on the website (", ") it would be useful to use the ", " command, which will sometimes list errors you've missed.  Also, you may use", "This will show some of the important ROS paths in your system and people may be able to point out what looks strange. Post the output by editing your original question here and when you have pasted it, highlight and click the \"101010\" button in the window to turn it to text that's easier to read.", "I would highly suggest you take a read through the wiki first though.  If you google \"ros wiki\" along with whatever you're looking for, 9 times out of 10 there is something in the wiki which can help.", " ", " ", "Hi Rolias, try this command:", "sudo apt-get install ros-hydro-openni-camera", "and launch the opeeni launcher again. The same problem occurs? ", "Regards. ", "yes, unfortunatly. it says that i already have installed the latest version of openni-camera. according to that, nothing changed. any other ideas? \ni have to say that i reinstalled my whole system totday, cause the same error occured before, but this changed nothing at all.", "You are using a kinect for windows yes? I have the same problem but I have a kinect xbox which is known to have problems. However there may be other issues.", " ", " ", "Mh I couldn't edit my former post. I'll try it later again, but first I wanted to show you the output of:", "Do you see any wrong paths in there?", "Thanks again for your help!", "Robin"], "answer_code": ["roswtf", "export | grep ros\n", "eddi@Eddi:~$ export | grep ros\ndeclare -x CMAKE_PREFIX_PATH=\"/opt/ros/hydro\"\ndeclare -x CPATH=\"/opt/ros/hydro/include\"\ndeclare -x LD_LIBRARY_PATH=\"/opt/ros/hydro/lib\"\ndeclare -x PATH=\"/opt/ros/hydro/bin:/usr/lib/lightdm/lightdm:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\"\ndeclare -x PKG_CONFIG_PATH=\"/opt/ros/hydro/lib/pkgconfig\"\ndeclare -x PYTHONPATH=\"/opt/ros/hydro/lib/python2.7/dist-packages\"\ndeclare -x ROS_ETC_DIR=\"/opt/ros/hydro/etc/ros\"\ndeclare -x ROS_PACKAGE_PATH=\"/opt/ros/hydro/share:/opt/ros/hydro/stacks\"\ndeclare -x ROS_ROOT=\"/opt/ros/hydro/share/ros\"\n"], "url": "https://answers.ros.org/question/121326/openni-no-device-connected-kinect/"},
{"title": "urg_node single echo scan error", "time": "2014-01-14 10:30:31 -0600", "post_content": [" ", " ", " ", " ", "I'm using the urg_node to run the hokuyo utm 30lx ew. I have the lidar set up through a wireless router, I only need it for a single scan(not a multiscan).", "Occasionally, the node shoots me the warnings, ", "I get 5 warnings, then it tries to reconnect. After the reconnect fails, the sequence happens again. Then finally, it successfully reconnects.", "This would happen pretty maybe once every 1-3 minutes.", "Is there is a way to get rid the scan retrieving failure?", "Thank you!", "Edit: It was because of the wireless set up that I was receiving these errors, should have figured, with the publish rates, it was bound to lose some thing. When connected to the network with Ethernet, I did not receive 1 warning. Should have done more testing before posting this question up.", "Appreciate anyone who took time to read this.", "Having the same problem - using the urg node to run the 04LX Classic and I'm not getting any data.", "Fixed it. Downloaded \"upgrade\" and \"reset\" attachments from this website (directions on website):", "Ran the programs.", "Then I cycled the power on the laser and everything seemed to work fine.", "Are the \"upgrade\" and \"reset\" commands 64bit programs? I am trying to run them on an Odroid X-U4(3 bit) and I get the error \" cannot execute binary file: Exec format error\" which apparently means the files are not 32bit like the system is.", ": answered in ", "."], "answer": [], "question_code": ["Could not grab single echo scan.\n"], "url": "https://answers.ros.org/question/117841/urg_node-single-echo-scan-error/"},
{"title": "Unable to check laptop battery [closed]", "time": "2012-08-22 12:41:59 -0600", "post_content": [" ", " ", " ", " ", "I have come across the issue: Turtlebot unable to check laptop battery state, as explained in greater detail in ", " ", "I tried to follow the directions by downloading the suggested python file from the post ", " \nEven after replacing the old file with the new version, I still have the same problem despite change. Please help. ", "I am running a fuerte distribution. \nThank you.  "], "answer": [], "url": "https://answers.ros.org/question/42059/unable-to-check-laptop-battery/"},
{"title": "Kinect navigation with color detection [closed]", "time": "2014-02-04 17:19:03 -0600", "post_content": [" ", " ", " ", " ", "Hello, \nFirst off, I am new to ROS and amazed at all the awesome things people have done with the software. I am working with a Pioneer 3-AT and would like to build an autonomous lawn mower using a kinect and ultrasonic sesnor (maxbotix MB1010) which I might not need anymore. This is for a senior design project and I am now working on it alone because my partner became very ill and needed brain surgery. I hope he gets better soon and want to continue to work on the project. I am using Ubuntu 13.04 and have installed ROS, ROSARIA, and the freenect_stack. I have gone through most of the main ROS tutorials (such as creating a package, etc.) and will finish the rest tonight. For the project, there will be a laptop on top of the robot which will connect to the Pioneer 3-AT with a serial to usb cable and the kinect sensor along with a custom made trailer hitch with an electric motor and battery to cut the grass. (The frame is built but I need to order the electric motor and blade along with an additional battery.) I would like to have the robot navigate a lawn while staying inside the lawn by detecting the color of the boundary which will either be dirt or cement. Also, I would like the robot to avoid obstacles such as a human or a football for example. Can the kinect track color and also track objects at the same time? I have been searching and reading many tutorials and am a little daunted when it comes to which package/ library I should use, and which tutorials I should be reading. Rviz seems to be very useful and I imagine I will need to install the Point Cloud Library interface stack and learn how that works along with the navigation stack. Could anyone point me to some tutorials and reading for what I would like to achieve? I understand that the kinect does not work in direct sunlight, but it would still work on cloudy days right?", "Thanks,\nMatthew", "I wouldn't pin all of your hopes in the Kinect working outdoors. Even on a cloudy day, outdoor lighting is still much brighter and contains a great deal more IR than you will find indoors. At least the very least, you should spend some time observing the depth information from the Kinect outdoors.", "Ahh, I see, thank you for the reply ahendrix. Do you have any suggested tutorials for material on how to extract and interpret the depth information from the kinect? So far I have only ran the demo glview and not sure where to go for now."], "answer": [], "url": "https://answers.ros.org/question/126286/kinect-navigation-with-color-detection/"},
{"title": "Omnidirectional platform master controller [closed]", "time": "2014-02-10 03:41:41 -0600", "post_content": [" ", " ", "is there any kind of comercial master controller for an omnidirectional platform with mecanum wheels?", "Please be more specific. Are you looking for hardware, i.e. a motor control board to power and control the wheel or software to talk to the board?"], "answer": [], "url": "https://answers.ros.org/question/127811/omnidirectional-platform-master-controller/"},
{"title": "RViz memory leak?", "time": "2013-11-11 23:53:02 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "I noticed that after a long time (hours) my memory was stuffed full when working with our robot. When closing RViz this memory is released. I suspect that there is a memory leak in RViz. ", "To replicate this error, I recorded a bag file, which is rather large (350MB) and can thus be found we.tl/Qba0dJdPqc, or you can use we.tl/2x6yPrGhBH smaller bag file.", "The used config file for RViz is included.", "To replicate:\nLaunch rviz with the provided config file and play the bag file. You will see your memory usage increasing over time. This bag file is not long enough to actually fill my  memory(6GB) but you get the idea. Now if you remove the green Gridcells display, the one that displays the '/move_base/local_costmap/inflated_obstacles' topic, there is no memory leak (or it is happening much slower).", "Is this a problem on my end and/or did anyone else notice this problem?\nDoes anyone have a solution?", "The robot is running electric, RViz is ran on a pc running hydro, could this be causing the problem? I noticed that the new move_base/local_costmap in hydro uses a occupancy grid instead of the girdcells. ", "I know electric is quite old already and therefore we are upgrading the robot to hydro shortly, but just wanted to share this observation with you.", "With kind regards,", "Okke Hendriks", " Rose B.V. -  ", "+1 for sharing .bag file. During the entire last week, I kept running RViz to exhibit a robot all day long and once (but only once) I saw memory leaked to 15GB. Though I haven't replicated it, a customer keeps mentioning about that single occurrence and thus I'd like to know more about this issue.", "See ", " for another instance of a severe memory leak in rviz on hydro. It appears the maintainers are very busy atm, as there has been no reaction to posted issues in quite some time."], "answer": [" ", " ", " This memory leak has been fixed, see  "], "url": "https://answers.ros.org/question/100221/rviz-memory-leak/"},
{"title": "How to call a service .srv from Terminal [closed]", "time": "2014-02-21 04:00:20 -0600", "post_content": [" ", " ", "A short question:", "I have a service defined like this: (", ")", "I would like to call it by a terminal with a command like:", "I have tried with: ", "\n... \"path1, path2, path3\" ", "\n... 'path1, path2, path3' ", "\n... \"path1\", \"path2\", \"path3\" ", "\n... 'path1', 'path2', 'path3' ", "\n... path1 path2 path3 ", "\n... path1, path2, path3 ", "\n... ", "\nBut i don't find the correct text chain(argument) to the service works.\nCould someone help me?", "It's a list, so something along '{path: []}. 'Can you try the \"cheater\" way and try tab completion?", "Thank you very much. The \"cheater\" way was the solution. I had underestimated the power of tab ;)"], "answer": [" ", " ", " Does the this help:  ", "I know that this is an old thread, but this is helpful too: "], "url": "https://answers.ros.org/question/131796/how-to-call-a-service-srv-from-terminal/"},
{"title": "Turtlebot - Unable to acquire data image from camera", "time": "2014-02-12 01:23:39 -0600", "post_content": [" ", " ", " ", " ", "Hello everybody,", "I looked for a solution in the forum, analyzing the different answers/questions but I still don't find a solution to my problem.", "Some basic information:\n- Ubuntu 12.04 LTS 32 bit\n- Groovy\n- Turtlebot with Kobuki base", "I followed the guide and there are no problems for communication, teleoperation and so on.\nHowever, I am complitely not able to use the camera: basically, I don't acquire data information.", "At this step, I have problem (reference: turtlebot_bringup/Tutorials/groovy/3D Visualisation )", "In netbook:", "Workstation:", "Everything seems ok but when I go in rviz, I have a warn in Image: No Image Received.", "I tried different solutions but no ones worked. I am going to explain you.", "First of all, kinect seems connected to my netbook. The light of kinect flashes green.\nI tried all the possible USB port (to avoid USB 3.0 problems) but the result is the same.", "So...This is the list of rosnode list from workstation", "Then I tried to get the image directly from kinect using the command", "I obtain a white image.", "Then, the last try. I killed 3dsensor.launch and I runned", "but in turtlebot-pc and workstation, the answer is the same:", "What can I do to solve the problem?", "I am beginner and maybe I skip some \"basic\" step about kinect but I followed everything in the Tutorial.\nThanks a lot"], "answer": [" ", " ", "You can use ", " to see what topics you can subscribe to. You will see a lot of image related topics. Particularly, you can", "rosrun image_view image_view image:=/camera/rgb/image_color compressed", "And that should work. Notice that the ", " is optional. ", " is also working for me. I cannot see depth images using image_view yet ( ", " ) but they do show in RVIZ.  ", "In RVIZ, expand the IMAGE line and try subscribing to some topic."], "question_code": ["roslaunch turtlebot_bringup 3dsensor.launch\n", "roslaunch turtlebot_rviz_launchers view_robot.launch\n", "turtlebot@turtlebot-W110ER:/opt/ros/groovy$ lsusb\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 002 Device 003: ID 04f2:b34c Chicony Electronics Co., Ltd \nBus 003 Device 014: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\nBus 003 Device 032: ID 045e:02c2 Microsoft Corp. \nBus 003 Device 035: ID 045e:02ad Microsoft Corp. Xbox NUI Audio\nBus 003 Device 038: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\n", "cvassall@hagurosan:groovy$ rosnode list\n/app_manager\n/bumper2pointcloud\n/camera/camera_nodelet_manager\n/camera/depth/metric\n/camera/depth/metric_rect\n/camera/depth/points\n/camera/depth/rectify_depth\n/camera/depth_registered/metric\n/camera/depth_registered/metric_rect\n/camera/depth_registered/rectify_depth\n/camera/depthimage_to_laserscan_loader\n/camera/disparity_depth\n/camera/disparity_depth_registered\n/camera/driver\n/camera/ir/rectify_ir\n/camera/openni_camera_loader\n/camera/points_xyzrgb\n/camera/points_xyzrgb_depth_rgb\n/camera/register_depth_rgb\n/camera/rgb/debayer\n/camera/rgb/rectify_color\n/camera/rgb/rectify_mono\n/camera_nodelet_manager\n/cmd_vel_mux\n/diagnostic_aggregator\n/master_sync_turtlebot_W110ER_7065_566021383\n/mobile_base\n/mobile_base_nodelet_manager\n/robot_pose_ekf\n/robot_state_publisher\n/rosout\n/rqt_gui_py_node_11279\n/rviz\n/zeroconf/zeroconf_avahi\n", "rosrun image_view image_view image:=/camera/rgb/image\n", "cvassall@hagurosan:groovy$ rosrun openni_camera openni_node\n", "[ INFO] [1392211004.943802894]: Initializing nodelet with 4 worker threads.\n[ INFO] [1392211005.029530557]: No devices connected.... waiting for devices to be connected\n[ INFO] [1392211008.034147743]: No devices connected.... waiting for devices to be connected\n"], "answer_code": ["rostopic list", "/camera/rgb/image_mono"], "url": "https://answers.ros.org/question/128571/turtlebot-unable-to-acquire-data-image-from-camera/"},
{"title": "Advice To Help You Complete The Master Cleanse Diet [closed]", "time": "2014-03-19 18:18:02 -0600", "post_content": [" ", " ", "For a few people it's going to function as the most difficult thing they actually do. As a result of this many folks never finish the entire cleanse.", "But I'm-not planning to let you are doing that, because I'm here to make sure you succeed.", "So take note of this, and do not try to start the Master Cleanse without following these steps...", "You first must flog yourself in to a white-hot frenzy", "This will start at least 3 days before going on the cleanse.", "You may think of this step as the pre game warm up. Set yourself in a peek mental state, and you need to psych yourself up. In case you observe this advice you're going to be at the point where you can't wait to begin the cleanse.", "This is an absolute must, most folks starting the lemonade diet skip this component - They roll out of bed each morning and determine that this would have been a goodbye to take up a 10 day detox.", "Here's what you should do, beginning at least 3 days before you truly go to the lemonade diet.", "Work with a big red marker. This may also work as a count-down -- some thing to enjoy.", "That Is for your eyes only so write down all of the ways your lifestyle will be better by finishing this cleanse. It could be bodily things like weight reduction, or ample energy. It may also be mental matters like assurance, or maybe bringing love into your own life. It could be anything so long as it's positive.", "It's important to keep yourself occupied so that you do not think about breaking your diet plan and eating so be sure you have the full calendar of low-impact tasks like reading, pictures, spa appointments, or anything else thatis not overly strenuous to keep your brain occupied.", "Another tip: if you're likely to read or view movies strive make certain they're uplifting narratives about people who were victorious under dire conditions. That's if you are in to that things, it is possible to view slasher movies or professional wresting if that's what is needed to keep your mind off food.", "Okay it's the nighttime before the wedding day, you should be itching to start this diet and make huge transitions in your life. But you first need to go throughout your house and get rid of anything that might sabotage your diet plan. I'm discussing food.", "You can contribute it, give it away, or drop it in the junk, but don't keep it your home. If ..."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Mark your calendar for the day you would like to begin the diet.", "Get yourself a brand new laptop, and jot down all the advantages you'll receive by going with this diet. Don't Censor Yourself.", "Programme activities for the spare time you will have while to the Master Cleanse.", "Rid your house of possible traps."], "url": "https://answers.ros.org/question/142071/advice-to-help-you-complete-the-master-cleanse-diet/"},
{"title": "How Long Can 5mW Blue Laser Pointer Be Visible [closed]", "time": "2014-04-01 01:42:22 -0600", "post_content": [" ", " ", "I am in need of a blue laser pointer for indoor presentation test. However, I have no experience with blue laser, but only the most visible green laser. \nIs there any body can help me with this problem? Its wavelength is 460nm.", "I think there is no connection between your question and the forum's topic, ROS..."], "answer": [], "url": "https://answers.ros.org/question/146881/how-long-can-5mw-blue-laser-pointer-be-visible/"},
{"title": "libfreenect \"Kinect for windows\" compatility", "time": "2014-04-26 23:05:16 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nWe have a problem with a \"kinect for windows\" under ros. The Kinect for windows works with \"freenect-glview\" but in ros with libfreenect, we are able to work with a kinect xbox 360 but it doesn't work with a \"kinect for windows\".", "With dmesg, we were able to see on what port the kinect is connected. We use different laptops and it seems that for some of them, it's needed to modify some parameters for tlp (power parameters), otherwise kinect is always disconnected. And after this, it's ok to work with freenect-glview with kinect Xbox 360 and kinect for windows. But with ros, it seems that only kinect Xbox 360 works.", "We tried with roslaunch openni2_launch openni2.launch, roslaunch freenect_launch freenect.launch. May be we need to use another command ?", "Thanks for help", "System: Ubuntu 13.04, hydro-desktop-full, libfreenect, openni2"], "answer": [], "url": "https://answers.ros.org/question/157851/libfreenect-kinect-for-windows-compatility/"},
{"title": "could not find executable file", "time": "2014-04-21 22:15:46 -0600", "post_content": [" ", " ", " Hi\nI am trying to use Voxelized Shape and Color Histograms (VOSCH) in order to recognize an object,  I follow this link  ", "  ,but when I run :  ", "rosrun vosch example_vosch ", "/data/sample_cone_green.pcd", "I got the Error that ", "Couldn't find executable named example_vosch below   /opt/ros/fuerte/stacks/mapping/vosch", "can anyone help me to fix it?"], "answer": [" ", " ", "I think that you did not compile it after downloading the package.", "Please check that all dependencies are installed and it compiles without errors. Then it should find the executable.", "You can also try ", " ", "to remove the previous builds and compile it right now in your system.", "Good luck!"], "question_code": ["rospack find vosch"], "answer_code": ["rosmake --pre-clean <package_name>"], "url": "https://answers.ros.org/question/155991/could-not-find-executable-file/"},
{"title": "Navigation stack on an Atom CPU: possible?", "time": "2014-05-07 20:14:25 -0600", "post_content": [" ", " ", "Hello,", "We're working on getting the Navigation ", " up-and-running on our ", ". We're making headway, but we have a question about the horsepower needed to run the nav stack. We're working under the assumption that ", " computer will do the trick. The DE2i-150 has an Atom N2600 at 1.6 GHz and 2GB of DDR3 RAM. Is this a reasonable assumption?", "Thanks in advance,", "Rick Armstrong"], "answer": [" ", " ", "It appears the ", " originally used with the netbooks sold with the Turtlebot (eeePC 1215n). It thus should be possible to run the navigation stack on the machine you mentioned.", "Thanks, Stefan.", "I would definitely look at the turtlebot nav configuration -- they turned down certain update rates to make things work smoothly on the Atom (you can't update quite as fast as a PR2, from which the defaults exist but with the proper update rates, you should be able to run navigation on the Atom)."], "url": "https://answers.ros.org/question/162681/navigation-stack-on-an-atom-cpu-possible/"},
{"title": "kinect driver problems [closed]", "time": "2014-04-23 21:00:28 -0600", "post_content": [" ", " ", " ", " ", "Hey everyone,", "i think i have s similar problem, which could not be solved by killing the xnsensorserver. let me explain: i'm running ros hydro, ubuntu 12.04 and installed openni by \"sudo apt-get install ros-hydro-openni-launch\". after running \"roslaunch openni_launch openni.launch\". Openni starts, but in the end it says", "\" No devices connected.... waiting for devices to be connected \"", "lsusb lists 3 microsoft devices,- as it should. my kinect led is blinking green. the output of \"rostopic list\" is", "\" eddi@Eddi:~$ rostopic list /rosout /rosout_agg \"", "When i try \"killall XnSensorServer\" it says \"XnSensorServer: Kein Prozess gefunden\" (no process found) \"XnSensorServer\" --> \"XnSensorServer: Befehl nicht gefunden\" (command not found)", "thanks for your help!"], "answer": [], "url": "https://answers.ros.org/question/156951/kinect-driver-problems/"},
{"title": "PR2 Moveit Tutorial - Kinematics Plugin failed to load [closed]", "time": "2014-05-04 23:27:17 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am trying to follow the PR2 MoveIt Tutorial here: docs.ros.org/api/moveit_ros_visualization/html/doc/tutorial.html. But after entering ", " I get various errors for each group:", "Rviz then continues to launch and shows the PR2. When I then continue the tutorial, I notice 3 things:", "At the end of step 1 in the tutorial, the screenshot shows that the current group is highlighted in green. This doesn't happen for me", "Even when I'm in the interact mode, I can only move the camera and nothing regarding the roboter", "No markers show up to interact with the roboter (As shown in the screenshot in step 3).", "The missing library descriped in the error is in the path it is supposed to be, and I don't know what to do with the ", ".", "I am using ROS Hydro on Ubuntu 12.04 64bit, I uploaded the complete output here: gist.github.com/rabenimmermehr/0b094c1ad032cb90ac0d \n(Sorry for the link formatting, I have insufficient karma)", "Thanks in advance,", "Rabe"], "answer": [" ", " ", "Seems like some packages failed to install. After a complete reinstall, everything workes as expected."], "question_code": ["roslaunch pr2_moveit_config demo.launch", "[ERROR] [1399281344.376185685]: The kinematics plugin (left_arm) failed to load. Error: Failed to load library /home/kuka/Programs/moveit/devel/lib/libpr2_moveit_arm_kinematics.so. Make sure that you are calling the PLUGINLIB_EXPORT_CLASS macro in the library code, and that names are consistent between this macro and your XML. Error string: Could not load library (Poco exception = /home/kuka/Programs/moveit/devel/lib/libpr2_moveit_arm_kinematics.so: undefined symbol: _ZN10kinematics14KinematicsBase9setValuesERKSsS2_S2_RKSt6vectorISsSaISsEEd)\n[ERROR] [1399281344.470927551]: The kinematics plugin (left_arm_and_torso) failed to load. Error: Failed to load library /home/kuka/Programs/moveit/devel/lib//libmoveit_kdl_kinematics_plugin.so. Make sure that you are calling the PLUGINLIB_EXPORT_CLASS macro in the library code, and that names are consistent between this macro and your XML. Error string: Could not load library (Poco exception = /home/kuka/Programs/moveit/devel/lib//libmoveit_kdl_kinematics_plugin.so: undefined symbol: _ZN10kinematics14KinematicsBase9setValuesERKSsS2_S2_RKSt6vectorISsSaISsEEd)\n[ERROR] [1399281344.558265962]: The kinematics plugin (right_arm) failed to load. Error: Failed to load library /home/kuka/Programs/moveit/devel/lib/libpr2_moveit_arm_kinematics.so. Make sure that you are calling the PLUGINLIB_EXPORT_CLASS macro in the library code, and that names are consistent between this macro and your XML. Error string: Could not load library (Poco exception = /home/kuka/Programs/moveit/devel/lib/libpr2_moveit_arm_kinematics.so: undefined symbol: _ZN10kinematics14KinematicsBase9setValuesERKSsS2_S2_RKSt6vectorISsSaISsEEd)\n[ERROR] [1399281344.652086203]: The kinematics plugin (right_arm_and_torso) failed to load. Error: Failed to load library /home/kuka/Programs/moveit/devel/lib//libmoveit_kdl_kinematics_plugin.so. Make sure that you are calling the PLUGINLIB_EXPORT_CLASS macro in the library code, and that names are consistent between this macro and your XML. Error string: Could not load library (Poco exception = /home/kuka/Programs/moveit/devel/lib//libmoveit_kdl_kinematics_plugin.so: undefined symbol: _ZN10kinematics14KinematicsBase9setValuesERKSsS2_S2_RKSt6vectorISsSaISsEEd)\n", "undefined symbol"], "url": "https://answers.ros.org/question/161176/pr2-moveit-tutorial-kinematics-plugin-failed-to-load/"},
{"title": "How to read a value from a ros message?", "time": "2014-04-29 23:22:57 -0600", "post_content": [" ", " ", "Dear friends,", "I am new in ROS, I am usinig CMVISION package, and the tos topic which I need is \" /blobs \", with type \"cmvision/blobs\"", "$ rosmsg show cmvision/Blob                    ", "uint32 red\nuint32 green\nuint32 blue\nuint32 area\nuint32 x\nuint32 y\nuint32 left\nuint32 right\nuint32 top\nuint32 bottom", "I want to just read the \"x\", and \"y\" from this message,", "As far as I know the \"/\" can help me, but I tried \"rostopic echo /blobs/x\" and it did not work....", "Any body knows how I could do this?", "Thanks", "Do you want to read the value in terminal or use it in your code?", "I want to use that in another code to be the input,"], "answer": [" ", " ", "The message that is being send over the topic is of type Blob.", "You need to receive the message type Blob by subscribing to the topic. \nThen in your code you can write something like ", "So in summary you need to receive the complete message.", "Thanks a lot"], "answer_code": ["int x = blob_message.x;\nint y = blob_message.y;\n"], "url": "https://answers.ros.org/question/158976/how-to-read-a-value-from-a-ros-message/"},
{"title": "How to achieve a value of a multi-type message mesage ?", "time": "2014-05-09 00:07:07 -0600", "post_content": [" ", " ", " ", " ", "Dear friends,", "I am trying to Subscribe from a topic, this topic has been published by CMVISION (/blobs) and has this type : ", "Header header\nuint32 image_width\nuint32 image_height\nuint32 blob_count\nBlob[] blobs", "and in this type, \"Blob\" is another message type itself :\nuint32 red\nuint32 green\nuint32 blue\nuint32 area\nuint32 x\nuint32 y\nuint32 left\nuint32 right\nuint32 top\nuint32 bottom", "I want to subscribe the \"x\" and \"y\" from this message, but I dont know hoe to call that, ", "For example I want to write them on the terminal  and in my \"callback\" function I wrote : ", "void callback1(const cmvision::Blobs::ConstPtr& msg1)\n{", "} ", "I tried to call that like an array, it didn't work.... can any body help me in this problem?", "Thanks \nHAmed"], "answer": [" ", " ", "blobs should be a std::vector, so ", "or", "should work"], "question_code": ["ROS_INFO(\"I heard: [%d]\", msg1->blobs ??????);\n"], "answer_code": ["msgl->blobs.at(i)\n", "msgl->blobs[i]\n"], "url": "https://answers.ros.org/question/163061/how-to-achieve-a-value-of-a-multi-type-message-mesage/"},
{"title": "Why can't youbot_battery_monitor connect to serial port? [closed]", "time": "2014-05-19 03:45:34 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "i am trying to get youbot_battery_monitor running on my youbot. The README (cannot post link, please check yourself on google) says i could run youbot_battery_monitor as standalone program. But when i try to run it with ", "$ ./home/youbotuser/Software/catkin_ws/devel/lib/youbot_battery_monitor/youbot_battery_monitor\n  /dev/youbot/lcd_display eth1 wlan0 &", "it says \"could not to connect to serial port: /dev/youbot/lcd_display\"", "I wrote a udev rule 110-battery.rule:", "SUBSYSTEM==\"usb\",\n  ATTRS{idVendor}==\"16d0\",\n  ATTRS{idProduct}==\"059d\",\n  SYMLINK+=\"youbot/lcd_display\",\n  RUN+=\"/home/youbotuser/Software/catkin_ws/src/youbot_diagnostics/youbot_battery_monitor/youbot_battery_monitor.sh\"", "My youbot_battery_monitor.sh looks like this:", "#!/bin/bash", "export ROS_MASTER_URI=http: //localhost:11311", "/home/youbotuser/Software/catkin_ws/devel/lib/youbot_battery_monitor/youbot_battery_monitor /dev/youbot/lcd_display eth1 wlan0 &", "The udev rule seems to be working. It at least generates the /dev/youbot/lcd_display file. But i am not really sure about using SUBSYSTEM==\"usb\"?!", "I don't use the youbot that long yet. I'd really much appreciate any help.\nThanks a lot.", "Edited after ahendrix 2nd comment:", "I think i finally solved the problem why it cannot connect to the serial port. But it is still not running properly yet. I can execute youbot_battery_monitor.sh by typing \"sudo bash youbot_battery_monitor.sh\". I obviously needed the root rights i got by adding sudo.", "This is the output i get then:", "sudo bash youbot_battery_monitor.sh", "youbotuser@Jakobs-youBot:~/Software/catkin_ws/src/youbot_diagnostics/youbot_battery_monitor$ try to connect to serial port: /dev/youbot/lcd_display", "serial port /dev/youbot/lcd_displayis already open", "connected to serial port: /dev/youbot/lcd_display", "[W              ]", "[L               ]", "[W              ]", "[L               ]\n  ....", "Every time it prints [W      ] and [L    ] it is beeping.", "Are the permissions on /dev/youbot/lcd_display correct? It looks like that udev rule should already be starting the battery monitor; perhaps you should check to see if it's already running?", "the lcd_display file has these permissions: lrwxrwxrwx 1 root root\nThe udev rule does start youbot_battery_monitor.sh, which starts the exe. The process is running. But it is beeping the whole time and i think it is beeping because it doesn't get a signal because cannot connect to the serial port??", "I suspect the process that's running has a lock on the serial port so that other programs can't open it. I would look for a log from that process to see if and how it's failing, or just kill it and run it again from the terminal, where you can actually see the output.", "Check edited question...", "Solved it. I changed SUBSYSTEM==\"usb\" to SUBSYSTEM==\"tty\". This fixed it. Now it is running how it is supposed to do. Tanks a lot for your help."], "answer": [], "url": "https://answers.ros.org/question/166861/why-cant-youbot_battery_monitor-connect-to-serial-port/"},
{"title": "Canopen ROS infinite loop at Init [closed]", "time": "2014-05-13 02:42:50 -0600", "post_content": [" ", " ", " ", " ", "Hello everybody,", "I encountered some kind of really big trouble trying to initialize the schunk powerball lwa using the ipa_canopen_ros node after updating to its new version. (I did this to take advance of the new ", "-service.)\nFirst of all, I want to mention that before the update everything worked perfectly.\nBut then, it brought me a strange problem.", "This is what I do as a startup:", "Up until this point everything works fine, no errors. So let's initialize the arm, using the ", "-service of the canopen_ros node (e.g. clicking the Init-Button of the dashboard). This makes the canopen_ros node check for availability of the robot's nodes, finds all of them, starts some PDO mapping, runs through it and stops responding afterwards. Neither there are click-noises (as it should be while initializing the robot) to hear from the robot nor the robot's joints were initialized. It just steps into an infinite loop it, that's it.\nWhen contacting the canopen_ros  node using my own program, it gets no response from the canopen_ros node (as it is caught in an infinite loop).", "So far, so bad.\nWhat's happening? - As far as I could understand the canopen_ros and canopen_core program, after the PDO mapping, it tries to set motor states for all joints. I found out by running the canopen_ros node in QtCreator in debug mode, setting some breakpoints and randomly halting the program after it gets caught in its infinite loop. Funny thing is, that using breakpoints and stepping through it, sometimes the program gets to initialize some joints or even all, sometimes there is no difference to running it normally in its behaviour. Some kind of randomness.", "This problem occurs in the ", " routine, which is somehow called while initializing the arm. It contains a loop in which for all devices ( =joints) the current motor state is compared to a desired motor state, trying to change the motor state if this is not the case ( = if the current state does not equal the desired state). The program gets caught in this loop as it fails to match the current state and the desired state of the robot's first joint.", "So my question is, why does the program fail to change the motor state? How can I resolve this?\nIs there some kind of communication fail? - I do not think so, as I can easily initialize and move the arm using a schunk software tool.\nI did not edit any launch- or yaml-file and I reinstalled the ipa_canopen, schunk_robots and schunk_modular_robotics package. This error occurs everytime I try to initialize the arm using the ros service.", "I really hope, someone can help me with this one. Thank you in advance and best regards,", "Marcel Usai", "P.S.: I use ROS groovy."], "answer": [], "question_code": ["roscore\nroslaunch schunk_bringup lwa4p_solo.launch\nroslaunch schunk_bringup dashboard_lwa4p.launch\n"], "url": "https://answers.ros.org/question/164206/canopen-ros-infinite-loop-at-init/"},
{"title": "(How) can a rospy node be reinitialized after ctrl+c?", "time": "2014-05-28 07:11:16 -0600", "post_content": [" ", " ", " ", " ", "TL:DR ", "Since interacting with ros from python is much more powerful than interacting via the command line interface, I've been attempting to use the interactive python shell to do many of the things I might otherwise do at the command line, like echoing and filtering messages.", "The problem is that once I hit ", " to ", " ", ", from then on out ", " is useless as it is in shutdown state. I've tried re-calling ", " again, but that fails to solve the problem.", "So, is there a correct way to use rospy in the interactive shell?", "Is there a way to kick rospy out of the shutdown state?"], "answer": [" ", " ", "In an interactive shell, there's no need for rospy.spin(). Unlike in C++ where it deals with processing the event queue, rospy.spin()'s only purpose is to keep your program from exiting.", "That was the insight I needed. Thanks!", " ", " ", "rospy is not designed to be used in an interactive shell. It is possible to restart a rospy process but it's not part of the public documented API. ", "Ken developed ", " as a prototype of an python based interactive environment for interacting with ROS. Dan seems to be maintaining a fork but it's not actively developed anymore. ", "rosh is great for interactive testing/prototyping - it deals with some of the paradigms ROS relies on that don't work well in an interactive context (like callbacks). The fork I maintain has been released into hydro, though I haven't had much time for new features."], "question_code": ["ctrl+c", "SIGINT", "rospy.spin()", "rospy.spin()", "rospy.init_node"], "url": "https://answers.ros.org/question/171121/how-can-a-rospy-node-be-reinitialized-after-ctrlc/"},
{"title": "kinect not detected", "time": "2013-11-01 13:13:38 -0600", "post_content": [" ", " ", " ", " ", "Hi guys.", "I'm using ros hydro \nI read in the ros answers site that openni2 and freenect can be used with windows kinect, so i installed using ", " and installed freenect with ", "But there is a problem, the windows kinect is not detected when i type ", " There is a message that says ", " and this message keeps iterating", "When i type ", " in the console, it detects the windows kinect.", "I'm a ros newbie, i have no clue what to do next. I'm doing something wrong?", "I'll appreciate your help.", "Thanks in advance. :)", "Can you see three entries in the result for ' lsusb ' command for Kinect ? Also, your kinect should be powered along with being connected to netbook/ laptop. \n\nAlso, did you try with simple install ?\nsudo apt-get install ros-hydro-openni-launch  ros-hydro-openni-tracker ros-hydro-openni-camera", "Thanks for answering .. hmm, didn't knew that kinect use 3 entries, i will check it. But, it really needs to detect the 3 entries?", "Yes, the entries that come up when you use 'lsusb' command as below:\n\nBus 001 Device 010: ID 045e:02ad Microsoft Corp. Xbox NUI Audio\nBus 001 Device 009: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nBus 001 Device 011: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\n\nDid you try with above simple install ?"], "answer": [" ", " ", "openni2_launch will load openni2_camera -- this package/driver does NOT support any version of the Kinect. If you are wanting to use freenect,  you should be using freenect_launch, not openni2_launch", "unless you install libFreenectDriver, which is a libfreenect-based driver for OpenNI2\n\n", "Have you actually tested that against a debian install of openni2_camera?", "Nope, I tested it on Ubuntu 12.04 with openni2 2.2.0.33 (", ")\nalso I recompiled the openni2_* packages. and It worked.", " ", " ", "Hi,", "Kinect for windows was discussed here"], "question_code": ["sudo apt-get install ros-hydro-openni2-*", "sudo apt-get install ros-hydro-freenect-stack", "roslaunch openni2_launch openni2.launch", "[ INFO] [1383342884.120184670]: No matching device found.... waiting for devices. Reason: std::string openni2_wrapper::OpenNI2Driver::resolveDeviceURI(const string&) @ /tmp/buildd/ros-hydro-openni2-camera-0.1.0-0raring-20131015-2254/src/openni2_driver.cpp @ 623 : Invalid device number 1, there are 0 devices connected.\n", "lsusb"], "url": "https://answers.ros.org/question/97081/kinect-not-detected/"},
{"title": "pocketsphinx microphone connection is unreliable", "time": "2014-07-07 09:58:40 -0600", "post_content": [" ", " ", "Hi,  I have pocketsphinx installed with hydro and raring ubutu (13.04).  I've done the standard installation with gstreamer0.10-pocketsphinx, ros-hydro-pocketsphinx, ros-hydro-audio-common and libasound2.", "When it works, it works quite well and will run for hours.  However, often when I start it up, it does not respond to the microphone at all.  The microphone is still working according to the Sound Settings input page, but pocketsphinx just doesn't seem to receive the input.  After it has been running properly, I can just stop it with control C and start it again and it may or may not work properly.  No changes are made between stopping and starting it.", "I assume that the mike data is getting lost somewhere in the gstreamer connection; but troubleshooting this connection is out of my experience.  Online search has many examples of problems in this area, but none of the proposed solutions seem to help.  I found gstreamer-properties which seems to allow selection of pulseaudio and my specific microphone; but doesn't make any difference.", "Any suggestions will be appreciated.\nThanks,\nAlex", "A few generic points to help troubleshooting. Compare top results when the bug happens and during normal operation. See if there is a process taking more time. Also, it might be that some process went to sleep mode, like a power save feature.", "Well, what I could see is that PulseAudio %CPU goes up about 50% (10% go 15% roughly) when pocketsphinx is working.  I read up on PulseAudio and added some of the tools which made me think I had it fixed.  But it went back to not working after 8 or 10 restarts.  I'm going to follow up on MIke's answer below."], "answer": [" ", " ", " I've not used 13.04, but on 14.04 I found that pulseaudio+gstreamer was problematic. I found that I could get a more reliable system using ALSA. I've thus added a new parameter \"~source\" which can be used to specify an arbitrary source input, for instance \"alsarsrc\" has worked well for me. This is not in debs yet, you would have to install from source ( ", " ) ", " (for reference, the commit is  ", " ) ", "Hi Mike,\n  The commit link above took me to your indigo release.  I assume you are talking about the recognizer.py program.  As far as I can see it is just like my hydro version which has a \"~source\" parameter.  I had tried before to use the _device_name_param with no luck.  \nBut, my python is pretty basic (or less) and I don't recognize the syntax on the rospy.has_param arguments.   I was assuming \"self._device_name_param\" was a local param.  Is '~source' a global param?\nMy pacmd list-sources doesn't have a 'alsasrc' listing, just much longer names.  Do I just add a\n", " to my global params in my launch file?", "OK, I figured it out.  I'm going to have to learn to work with python programs.\n\nYour solution works, the param is local (s/b 'private') so I just added it to my pocketsphinx launch file.  Pocketsphinx has opened successfully 20 time in a row.  this far exceeds the previous record so I think it is fixed.  This was with the ros-hydro-pocketsphinx version of your program just loaded by apt-get.\nThanks!", "Worked perfectly after modifying the source. Thanks buddy!!!"], "url": "https://answers.ros.org/question/185261/pocketsphinx-microphone-connection-is-unreliable/"},
{"title": "Create powers off after minimal.launch", "time": "2014-06-24 19:08:48 -0600", "post_content": [" ", " ", " ", " ", "I just got an iRobot iCreate base and I've followed the instructions given in  ROS Tutorials to setup the turtlebot pc and the workstation. I could successfully ssh into username @ turtlebot through workstation so I'm assuming that is all good. I had an issue with create not able to detect the usb cable which I solved using the detailed answer given for [question] (http:// answers.ros.org/question/46790/failed-to-open-port-devttyusb0/)  at ros answers. This solved the problem of \"Failed to open port / dev / ttyUSB0\" that I was facing before. ", "Now the next step would be to ssh into the turtlebot (which I've done) and use ", " to do whatever the command does (I've no idea what to expect upon launch). But apparently something's amiss since the create base chirps and then powers down after showing ", " as output and the log file location (see output below), but I dont see a prompt. I checked the battery and that's charged so that's not the problem. ", " is the terminal output.", " is the log file: /home ...", "please be specific on which distro of ROS you are using. previous versions of ROS are discontinued to be updated and seems to give problems. After reading your comments I would only suggest to reinstall Ubuntu and ROS and try again. If the problem persists reply here."], "answer": [], "question_code": ["roslaunch turtlebot_bringup minimal.launch", "[kinect_breaker_enabler-5] process has finished cleanly", "anshul@AnshulsPC:~$ roslaunch turtlebot_bringup minimal.launch\n... logging to /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/roslaunch-AnshulsPC-5038.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http:// 128.110.74.233:48495/\n\nSUMMARY\n========\n\nPARAMETERS\n * /cmd_vel_mux/yaml_cfg_file\n * /diagnostic_aggregator/analyzers/digital_io/path\n * /diagnostic_aggregator/analyzers/digital_io/startswith\n * /diagnostic_aggregator/analyzers/digital_io/timeout\n * /diagnostic_aggregator/analyzers/digital_io/type\n * /diagnostic_aggregator/analyzers/mode/path\n * /diagnostic_aggregator/analyzers/mode/startswith\n * /diagnostic_aggregator/analyzers/mode/timeout\n * /diagnostic_aggregator/analyzers/mode/type\n * /diagnostic_aggregator/analyzers/nodes/contains\n * /diagnostic_aggregator/analyzers/nodes/path\n * /diagnostic_aggregator/analyzers/nodes/timeout\n * /diagnostic_aggregator/analyzers/nodes/type\n * /diagnostic_aggregator/analyzers/power/path\n * /diagnostic_aggregator/analyzers/power/startswith\n * /diagnostic_aggregator/analyzers/power/timeout\n * /diagnostic_aggregator/analyzers/power/type\n * /diagnostic_aggregator/analyzers/sensors/path\n * /diagnostic_aggregator/analyzers/sensors/startswith\n * /diagnostic_aggregator/analyzers/sensors/timeout\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/base_path\n * /diagnostic_aggregator/pub_rate\n * /robot/name\n * /robot/type\n * /robot_description\n * /robot_pose_ekf/freq\n * /robot_pose_ekf/imu_used\n * /robot_pose_ekf/odom_used\n * /robot_pose_ekf/output_frame\n * /robot_pose_ekf/publish_tf\n * /robot_pose_ekf/sensor_timeout\n * /robot_pose_ekf/vo_used\n * /robot_state_publisher/publish_frequency\n * /rosdistro\n * /rosversion\n * /turtlebot_laptop_battery/acpi_path\n * /turtlebot_node/bonus\n * /turtlebot_node/port\n * /turtlebot_node/update_rate\n * /use_sim_time\n\nNODES\n  /\n    cmd_vel_mux (nodelet/nodelet)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    kinect_breaker_enabler (create_node/kinect_breaker_enabler.py)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    turtlebot_laptop_battery (linux_hardware/laptop_battery.py)\n    turtlebot_node (create_node/turtlebot_node.py)\n\nauto-starting new master\nprocess[master]: started with pid [5055]\nROS_MASTER_URI=http:// 128.110.74.233:11311\n\nsetting /run_id to 9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9\nprocess[rosout-1]: started with pid [5068]\nstarted core service [/rosout]\nprocess[robot_state_publisher-2]: started with pid [5081]\nprocess[diagnostic_aggregator-3]: started with pid [5102]\nprocess[turtlebot_node-4]: started with pid [5117]\nprocess[kinect_breaker_enabler-5]: started with pid [5122]\nprocess[robot_pose_ekf-6]: started with pid [5181]\nprocess[mobile_base_nodelet_manager-7]: started with pid [5226]\nprocess[cmd_vel_mux-8]: started with pid [5245]\nprocess[turtlebot_laptop_battery-9]: started with pid [5262]\n[WARN] [WallTime: 1403641073.765412] Create : robot not connected yet, sci not available\n[WARN] [WallTime: 1403641076.772764] Create : robot not connected yet, sci not available\n[kinect_breaker_enabler-5] process has finished cleanly\nlog file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log\n"], "url": "https://answers.ros.org/question/174855/create-powers-off-after-minimallaunch/"},
{"title": "PR2 does not move to follow planned paths", "time": "2014-07-15 06:24:39 -0600", "post_content": [" ", " ", "I am trying to follow '8. PR2 Navigation Stack Demo' in pr2_simulator/Tutorials/BasicPR2Controls.", "After launching pr2 with rviz by typing", "In 'move_base.rviz', I can see a (green) path is planned between the init and goal pos selected by me. \nBut PR2 does not move to follow the path as described in the tutorial. \nThe error messages from the command line are as follows:", "Any idea to fix this problem?"], "answer": [], "question_details": [" ", " ", " ", " ", "roslaunch ~/pr2_nav_tutorial.launch", "roslaunch pr2_navigation_global rviz_move_base.launch"], "url": "https://answers.ros.org/question/186621/pr2-does-not-move-to-follow-planned-paths/"},
{"title": "How to determine the inertial properties of a two link manipulator to perform dynamic analysis of the arm?", "time": "2014-07-18 18:00:07 -0600", "post_content": [" ", " ", "I have a two link manipulator with DC motors powering the links, I have designed the same using solid works but the mass properties are not about the referenced coordinate system but something else. This makes the calculaion even more difficult. I cannot estimate its Mass moment of Inertia and Center of mass. Pls tell any suggestion or alternative.", "Thnx.", "This isn't really a ROS-related question. You may want to try posting it elsewhere."], "answer": [], "url": "https://answers.ros.org/question/187073/how-to-determine-the-inertial-properties-of-a-two-link-manipulator-to-perform-dynamic-analysis-of-the-arm/"},
{"title": "How to deal with an offset in precise GPS data?", "time": "2014-06-24 06:59:36 -0600", "post_content": [" ", " ", "I've got a robot setup with a very precise differential GPS (centimeter level). For technical reasons, I can't attach the GPS directly over the rotational center of the robot, but rather need to make an offset in both x- and y-direction. ", "There are two things I'd like to do: ", "First, infer the orientation of the robot from the measurements of the GPS", "Secondly, infer the position of the robot on the UTM grid from the position of the GPS", "Obviously, the first task is the harder one. Once it is solved, the second one is trivial. ", "I hope the picture illustrates the situation a bit. My robot has a differential drive, so the rotational center is located between the two wheels. My GPS is attached to a rod with offset delta_x and delta_y (in the picture, delta_x is negative). The green dots stand for the three last GPS measurements that were recorded.  What I'd like to infer is angle alpha. ", "From what I've tried so far, I gather that there is no single solution if I take only the last two measurements into account. Therefore, I am thinking about using the last three measurements. They lie on exactly one circle with midpoint M. This means that my robot is currently moving on a circle with the same midpoint. ", "Apparently, my geometry skills simply aren't good enough - I've not been able to compute a solution. The solution is fairly simple if either delta_x or delta_y are equal to zero - but with both values != 0, I don't get anywhere. ", "Can anybody give me a hint whether this problem is solvable and how you'd go about solving it? On first glance it sounds like a fairly common usage of a GPS. "], "answer": [" ", " ", "Have a look at ", ". It appears the author there describes in great detail how to solve your exact problem using an Extended Kalman Filter.", "Thanks a lot for your answer and clarifying that tf is not the way to go. Up to now I'm fairly certain that combining the GPS with a kalman filter is the way to go. In the blog post (as far as I get it) the orientation of the robot is only updated differentially from a given theta_0.", "It's dawning on me that I can't have both: Absolute orientation from the GPS and an offset in the data.. I'll accept your answer because I think combining the GPS with measurements from other sensors in a kalman filter is the closest I can get to answering my problem! Thanks again!", " ", " ", " ", " ", " Have a look at the tf package:  ", "\nIt seems to be right what you need:", "tf is a package that lets the user\n  keep track of multiple coordinate\n  frames over time. tf maintains the\n  relationship between coordinate frames\n  in a tree structure buffered in time,\n  and lets the user transform points,\n  vectors, etc between any two\n  coordinate frames at any desired point\n  in time.", "Good luck with you project. Sounds interesting :)", " With tf you can set up a \"tree\" of \"frames\". You have to set up the relation between the gps position and the car once, as descried here:  ", " . T\n ", " hen you have to publish the gps position to tf  ", "   and listen to its transformation:  ", " . This will contain your angle alpha as a quaternion  ", "  which can be transformed relatively easy to an normal angle. ", "\nAt least that what I think how it works. The upside is, that tf does all the work for you. It is originally designed for bigger numbers of frames. But it should work for your problem, too.", "Thanks, I will look at it again - at first try I didn't manage to get the rotations right. I've written some code myself now which (kind of) works, and I will post it here when I've tested it and cleaned it up. Any help with setting up tf for this problem would still be most appreciated.", "Okay, looking at it again I remember why I didn't manage to set it up:I don't see any way that tf would calculate the angle alpha for me. Of course I can write a transform from gps-->robot_base myself, for which I however still need to know alpha and solve the question above. Am I missing something?", "please see my edit", "tf won\u00b4t help here, as orientation has to be estimated from position only measurements."], "url": "https://answers.ros.org/question/174735/how-to-deal-with-an-offset-in-precise-gps-data/"},
{"title": "obstacle_layer does not clear area", "time": "2014-05-15 21:55:46 -0600", "post_content": [" ", " ", " ", " ", "I'm experiencing a very strange and to me random behavior from the ", ". To visualize my point I recorded a bag file and launched it over and over again with the same settings. The situation is like this: The costmap is getting initialized with a previously recorded pgm image. Without changing the settings of the *_params. files I get different outcomes. I moved the trash can in the lower left corner to the area right-above its original position. Sometimes the original position gets cleared (as called for) and sometimes it does not get cleared. Can anybody help me with that problem?", "I reached the necessary karma level and can provide you with screen shots. So the \"real world situation\" is like that. I place and do not move the robot. Then I move the trash can from its original position (just when you come through the door to your right) to its new position (right wall). The screen shots are taken at the approximately same time but as mentioned above with different outputs from different runs.", "The first image shows the situation when the clearing does not seem to work and the second image shows the situation where the clearings works as expected. The effect can also be seen at the left wall of the entrance door. The green dots should visualize the ", " and the red dots represent the laser scan.", "If relevant: I use Hydro on an Ubuntu 12.04 LTS.", "global_costmap_params.yaml:", "costmap_common_params.yaml:", "move_base.launch:", "launch-file for starting the bag-file:", "I think you have adequate karma now. Please post pictures, because otherwise it is unclear what you are talking about. If you can't post pictures on the forum, please upload them elsewhere and link to them.", "Thanks David. I updated my question with the screen shots.", "Does the laser you're using move relative to the base of the robot? (like the PR2's tilting laser?)", "The tf tree is fixed. So no moving lasers or anything."], "answer": [" ", " ", " ", " ", "I think that the reason you get different outcomes on different runs is that you're maintaining a 3 dimensional costmap when you only need a two dimensional one. Try changing your VoxelLayer to an ObstacleLayer. ", " Edit: The different outcomes I believe are related to this bug.  ", "  since the points that are not getting cleared from the costmap are from the static map ", "Sorry for the late reply. It took me some time to get a hand on the robot again. It does not make a difference. Neither layer clears the area...", "Can you share the bag file?", "Sure. Let me check if I can organize some server space for that.", "Thanks a lot. The thing is that I wanted to start simple with just the laser. Other sources will come into play in a later step.", " ", " ", "When you visualize your laserScan with \"rostopic echo /scan\" does it show a lot of zeros (or Infs)? ", "If so, create a filter to change these values to your laser max range values. ", "Maybe you are not clearing because your lasers do not tell that \"they see staff beyond\" the place where there used to be an obstacle.", "Thanks for the reply. The ranges values are all above 0 and below inf. Only the intensities are all 0.0 but that should not be a problem. I checked the laser output and it actually shows the wall behind the obstacle. If my karma would be higher I would post a screen shot."], "question_code": ["obstacle_layer", "clearing_endpoints", "global_costmap:\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: true\n  rolling_window: false\n  plugins:  \n    - {name: static_layer, type: \"costmap_2d::StaticLayer\"}\n    - {name: obstacle_layer, type: \"costmap_2d::VoxelLayer\"}\n", "global_frame: /map\nrobot_base_frame: /base_link\nmap_type: voxel\npublish_voxel_map: true\nfootprint: [[0.235, 0.31], [-0.515, 0.31], [-0.515, -0.31], [0.235, -0.31]]\n\nstatic_layer:\n  map_topic: /map\n\nobstacle_layer:\n  max_obstacle_height: 2.0\n  obstacle_range: 2.5\n  raytrace_range: 3.0\n\n  origin_z: -0.08\n  z_resolution: 0.2\n  z_voxels: 6\n  unknown_threshold: 6\n  mark_threshold: 0\n\n  track_unknown_space: true\n  combination_method: 0\n\n  observation_sources: laser\n\n  laser: {sensor_frame: base_laser_link, data_type: LaserScan, topic: scan, marking: true, clearing: true}\n", "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\n<launch>\n\n    <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\" clear_params=\"true\">\n    <!-- default:20.0. with this value dwa planner fails to find a valid plan a lot more -->\n    <param name=\"controller_frequency\" value=\"10.0\" />\n    <param name=\"controller_patience\" value=\"15.0\" />\n    <param name=\"planner_frequency\" value=\"2.0\" />\n    <param name=\"clearing_rotation_allowed\" value=\"false\" />\n    <rosparam file=\"$(find scitos_2d_navigation)/scitos_move_base_params/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n<!--\n    <rosparam file=\"$(find scitos_2d_navigation)/scitos_move_base_params/local_costmap_params.yaml\" command=\"load\" />\n-->\n    <rosparam file=\"$(find scitos_2d_navigation)/scitos_move_base_params/global_costmap_params.yaml\" command=\"load\" />\n\n    <param name=\"base_local_planner\" value=\"dwa_local_planner/DWAPlannerROS\" />\n    <rosparam file=\"$(find scitos_2d_navigation)/scitos_move_base_params/dwa_planner_ros.yaml\" command=\"load\" />\n    </node>\n\n</launch>\n", "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\n<launch>\n    <!-- launch map server -->\n<!--\n    <node name=\"map_server\" pkg=\"map_server\" type=\"map_server\" args=\"$(find scitos_2d_navigation)/maps/floorsix.yaml\"/>\n-->\n\n    <!--\n    set up time nicely so there is no problem with old, outdated timestamps; rosparam set use_sim_time true\n    -->\n    <param name=\"/use_sim_time\" value=\"true\"/>\n    <node pkg=\"rosbag\" type ..."], "url": "https://answers.ros.org/question/165571/obstacle_layer-does-not-clear-area/"},
{"title": "How to create soft objects (wheel tires) in gazebo?", "time": "2014-04-09 04:13:53 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am currently trying to model a wheeled robot (pioneer3at) in Gazebo. The problem I face is that it strangely bounces of its wheels whenever it gets an impulse. So I thought the problem might be that the wheels are too hard and do not absorb any energy.", "Following the link above I added a kp-tag to the according gazebo-tag in my URDF, but whatever value I use, it just causes the wheel to be ignored completely for collisions. (It will just fall through the floor)", "What is the correct way to describe collision properties in Gazebo?", "I now had a closer look at the Husky-Bot, which seems quite similar to the Pioneer. In Gazebo, it shows none of the mentioned problems. The definition of the Husky-Wheels seems pretty similar to what I did:", "So how comes it is not bouncing around like my Pioneer?"], "answer": [" ", " ", " ", " ", "We had good success on our robots with not really modeling softness, but using the default parameters. You should be able to find a few skidsteering robots for inspiration regarding parameters (the ", " package should provide a working example). I found that some parameters do not appear to work correctly though (see ", ").", "There also is a  ", ". I\u00b4d be surprised if there is not AT3 available for simulation, as this robot is in pretty widespread use still.", "/edit: Regarding parameters of the husky: Performance is a function of the complete model, so also the masses, inertias and ratios between them for the full model. It might be the case that the husky has better properties there. Note that issues could be reduced by altering some simulation parameters (timestep etc.) or model parameters (changing masses, getting rid of details etc.).", "Thank you, I will have a look at these packages! I am currently using the p3at-model from p2os_urdf, which works in Gazebo besides the mentioned problems.", " ", " ", "I had the same problem with a pioneer3at package I found on the web. The problem was that the inertia information for the wheels was set to be the identity matrix:", "When ever I would move the robot through teleopt, I would get some bouncing behavior.\nI replaced it with the proper formula for a the interia matrix of a cylinder, and the runs smoothly:", "When you run Gazebo, go to the visualusation tabe and check  the inertia view and you will see pink boxes. These\nshould be more or less within the meshes of your vehicle. I now have fully working version of the pioneer3at for indigo under ubuntu 14.04", " ", " ", "I had the same problem. My Pioneer-3AT couldn't run much far without a bit of bouncing... making some useless reads with the laser scan, and move_base creating false collisions predictions.\nBut the Pioneer-3AT model is very widespread and quite complete now, so I suspected this could be generated by other issues.\nI had ROS-Indigo on Ubuntu 14.04 LTS, and for me the solution was to run ROS-Hydro on a VM with a previous version of Ubuntu .\nI'm still not sure if the problem was the version of Gazebo which is shipped with ROS itself that had problems... or it was something on ROS-Indigo. I don't know.\nI'm on a rush to get results and this is what worked for me."], "question_code": ["<gazebo reference=\"${fb}_${lr}_wheel\">\n    <mu1 value=\"1.0\"/>\n    <mu2 value=\"1.0\"/>\n    <kp  value=\"10000000.0\" />\n    <kd  value=\"1.0\" />\n    <fdir1 value=\"1 0 0\"/>\n    <material>Gazebo/Grey</material>\n    <turnGravityOff>false</turnGravityOff>\n</gazebo>\n"], "answer_code": ["  <inertial>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n  <mass value=\"1\"/>\n  <inertia  ixx=\"1\" ixy = \"0\" ixz = \"0\"  iyy=\"1\"  iyz = \"0\"  izz=\"1\" /> \n  </inertial>\n", "<macro name=\"cylinder_inertia\" params=\"m r h\">\n          <inertia  ixx=\"${m*(3*r*r+h*h)/12}\" ixy = \"0\" ixz = \"0\"\n                    iyy=\"${m*(3*r*r+h*h)/12}\" iyz = \"0\"\n                    izz=\"${m*r*r/2}\" /> \n</macro>\n\n\n    <inertial>\n        <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n            <mass value=\"1\"/>\n            <cylinder_inertia  m=\"1\" r=\"0.1\" h=\"0.11\" />\n   </inertial>\n"], "url": "https://answers.ros.org/question/151121/how-to-create-soft-objects-wheel-tires-in-gazebo/"},
{"title": "pi_face_tracker parameters with ros groovy and kinect", "time": "2014-07-17 11:31:25 -0600", "post_content": [" ", " ", "Hello everybody. I have a problem with ros groovy and pi_face_tracker. I'd like to know where are described the full list of the parameters of video stream. I'm trying to store the distance of the face from the sensor and the coordinates of the green points for each instant of time, but I don't know where to look for them.\nCan you help me?"], "answer": [], "url": "https://answers.ros.org/question/186953/pi_face_tracker-parameters-with-ros-groovy-and-kinect/"},
{"title": "turtlebot2 teleop problem", "time": "2014-07-15 06:06:40 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI'm new to ROS and I'm having some trouble when running the teleop application in the turtlebot2. I'm trying to run it directly on the netbook that came along with the turtlebot. Therefore, I'm not using SSH. ROS Hydro and the turtlebot_teleop package were already installed in the netbook. I'm running \"turtlebot_bringup minimal.launch\" in one terminal and \"turtlebot_teleop keyboard_teleop.launch\" in another one. When I press one of the assigned letters I can see that the velocities are being changed, however the robot doesn't move. ", "the output of 'roswtf' was 1 warning:", " Furthermore, I used rqt_graph to take a look in the nodes connection.\nThe rqt_graph can be seen here:  ", "   and  ", "This graph looks weird to me. Why are there so many topics named /mobile_base_nodelet_manager/bond?", "I've read other threads of people with a similar problem and it seems that the problem was either with the turtlebot_teleop package or that the person was running codes of different release versions. However, I'm not sure if this is my case too because I didn't have to install anything. Everything I'm using came already installed in the netbook that came along with the robot.", "EDIT: the log for 'roslaunch turtlebot_bringup minimal.launch --screen' is:", "Can you attach the log of minimal.launch terminal? You should see some logs if you start turtlebot using roslaunch turtlebot_bringup minimal.launch --screen", "Hi jihoonl, I've added the log you requested in the question. Thanks", "I don't see any problem here. Try rostopic echo on /mobile_base/commands/velocity and /cmd_vel_mux/input/teleop to see if there is any data stream. /cmd_vel_mux/input/teleop is a command from teleop_keyboard node and /mobile_base/commands/velocity is actual topic which kobuki_driver listens.", "Both topics aren't receiving the commands. When I use the keyboard to try to move the robot, the teleop terminal displays the 'speed' and 'turn' parameters changing, however both topics that you mentioned display only zeroes. I also ran the kobuki dashboard and everything is fine (green mark)", "What keys did you press to move the robot?", "W,E,Q,C,X,Z. But I don't think this is the problem, since I could see the velocity parameters changing in the terminal as I pressed the keys. Anyway, I managed to make it work using a different command (see comments on the other answer)", "Yeah it was the problem.. You should have used u,i,o,j,k,l,m ... keys to move robot. weqcxz are to configure speed of robots."], "answer": [" ", " ", " ", " ", "Instead brining up the turtlebot with roslaunch turtlebot_bringup minimal.launch, I suggest that you simply start up the turtlebot from the top right corner if you are going to be testing out some of the basic stuff with out ssh. ", "Just simply click on the turtle icon on the top right corner and start turtlebot. Hopefully you will see this.\nAlso, wait for the jingling noise to finish before you try to run anything else.", "They simply launch the turtlebot_teleop package. ", "I think that should help you for the most part. Let me know if this suggestion is useful. ", "Hi choog, I'm sorry but I'm new to this and I didn't understand what you mean. Where can I find this turtle icon that you mentioned? Is it in the top right corner of what?", "I guess if your ROS_HOSTNAME is different from your ROS_MASTER_URI. That means that setup is wrong. Could you check your .bashrc? In your case, I think that both has to be same because you intend to make a turtlebot move with the netbook on turtlebot.", "Both of them are the same, the only difference is that the ROS_MASTER_URI is pointed to port 11311 in the turtlebot IP. I did exactly as instructed in section 3 of the Turtlebot Networking Setup tutorial (", "Just an update, I've tried the commands 'roslaunch kobuki_node minimal.launch' and 'roslaunch kobuki_keyop keyop.launch' and they worked. I've found them in the kobuki manual that came along with the robot. Why these commands work and not the 'turtlebot_teleop' ones?"], "question_code": ["WARNING The following node subscriptions are unconnected:\n * /mobile_base_nodelet_manager:\n   * /mobile_base/commands/controller_info\n   * /mobile_base/commands/external_power\n   * /mobile_base/commands/reset_odometry\n   * /mobile_base/commands/sound\n   * /cmd_vel_mux/input/safety_controller\n   * /cmd_vel_mux/input/navi\n   * /mobile_base/commands/digital_output\n   * /mobile_base/commands/led1\n   * /mobile_base/commands/led2\n   * /mobile_base/commands/motor_power\n", "turtlebot@turtlebot-X200CA:~$ roslaunch turtlebot_bringup minimal.launch --screen... logging to /home/turtlebot/.ros/log/917cdfc6-0cc2-11e4-9663-48d22479773d/roslaunch-turtlebot-X200CA-2196.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://10.85.254.209:54459/\n\nSUMMARY\n========\n\nPARAMETERS\n * /bumper2pointcloud/pointcloud_radius\n * /cmd_vel_mux/yaml_cfg_file\n * /diagnostic_aggregator/analyzers/input_ports/contains\n * /diagnostic_aggregator/analyzers/input_ports/path\n * /diagnostic_aggregator/analyzers/input_ports/remove_prefix\n * /diagnostic_aggregator/analyzers/input_ports/timeout\n * /diagnostic_aggregator/analyzers/input_ports/type\n * /diagnostic_aggregator/analyzers/kobuki/contains\n * /diagnostic_aggregator/analyzers/kobuki/path\n * /diagnostic_aggregator/analyzers/kobuki/remove_prefix\n * /diagnostic_aggregator/analyzers/kobuki/timeout\n * /diagnostic_aggregator/analyzers/kobuki/type\n * /diagnostic_aggregator/analyzers/power/contains\n * /diagnostic_aggregator/analyzers/power/path\n * /diagnostic_aggregator/analyzers/power/remove_prefix\n * /diagnostic_aggregator/analyzers/power/timeout\n * /diagnostic_aggregator/analyzers/power/type\n * /diagnostic_aggregator/analyzers/sensors/contains\n * /diagnostic_aggregator/analyzers/sensors/path\n * /diagnostic_aggregator/analyzers/sensors/remove_prefix\n * /diagnostic_aggregator/analyzers/sensors/timeout\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/base_path\n * /diagnostic_aggregator/pub_rate\n * /mobile_base/base_frame\n * /mobile_base/battery_capacity\n * /mobile_base/battery_dangerous\n * /mobile_base/battery_low\n * /mobile_base/cmd_vel_timeout\n * /mobile_base/device_port\n * /mobile_base/odom_frame\n * /mobile_base/publish_tf\n * /mobile_base/use_imu_heading\n * /mobile_base/wheel_left_joint_name\n * /mobile_base/wheel_right_joint_name\n * /robot/name\n * /robot/type\n * /robot_description\n * /robot_state_publisher/publish_frequency\n * /rosdistro\n * /rosversion\n * /turtlebot_laptop_battery/acpi_path\n * /use_sim_time\n\nNODES\n  /\n    bumper2pointcloud (nodelet/nodelet)\n    cmd_vel_mux (nodelet/nodelet)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    mobile_base (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    turtlebot_laptop_battery (linux_hardware/laptop_battery.py)\n\nauto-starting new master\nprocess[master]: started with pid [2213]\nROS_MASTER_URI=http://10.85.254.209:11311\n\nsetting /run_id to 917cdfc6-0cc2-11e4-9663-48d22479773d\nprocess[rosout-1]: started with pid [2226]\nstarted core service [/rosout]\nprocess[robot_state_publisher-2]: started with pid [2229]\nprocess[diagnostic_aggregator-3]: started with pid [2230 ..."], "url": "https://answers.ros.org/question/186613/turtlebot2-teleop-problem/"},
{"title": "Limit linear velocity [closed]", "time": "2014-07-23 16:46:57 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "I wrote a node that subscribes to the topic sensor_msgs/LaserScan and publishes in the topic geometry_msgs/Twist. I want to limit the maximum linear velocity that a powered wheelchair can reach. How can I do that?", "Here's the code of how I publish the velocity: (this is just an example, I don't want to set the linear velocity to 1.4m/s, I want that the maximum velocity reachable is 1.4 m/s)", "can you describe more", "+bvbdort Hi want to limit the max linear velocity. I am new to ROS and I don't know which topic should I publish or with what parameter can I limit the linear velocity.", "Divide your 'linear.x' by a factor in the Twist message that you send to /command_velocity or cmd_vel.\nOr limit your linear.x", "please post your code how you are publishing your linear velocity.", "+McMurdo divide linear.c by what factor?? I want to limit my linear velocity (linear.x) just like you said, but I don't know how, please explain it to me...\n+bvbdort I'll post the code now!", "`linear.x` is zero in the code you posted. I don't know how you want to limit that more.", "If you just want to enforce a max velocity, check the value of vel.linear.x and change it if it is too large. If you want to scale the velocity to another range, that can be done in one line -  see ", "+dornhege This was just an example of how I publish the velocity. I want to limit the maximum linear velocity of the wheelchair. I want that, no matter which part of my code is executing, the maximum velocity that the power wheelchair user can reach (joystick is the HMI) is for example 1.4m/s!"], "answer": [" ", " ", "Write another node the subscribes to the Twist message, enforces any limits you want to set (such as maximum speed), and then publishes the new, modified Twist.  Have this node subscribe to the topics published by your original node, and publish to the lower-level drivers for the wheelchair."], "question_code": ["ros::Publisher vel_pub_=n.advertise<geometry_msgs::Twist>(\"cmd_vel\", 1);\n(...)\nvel.linear.x=1.4;\nvel.angular.z+=M_PI/8;\nvel_pub_.publish(vel);\n"], "url": "https://answers.ros.org/question/187593/limit-linear-velocity/"},
{"title": "Is there an async universal robot control language?", "time": "2014-07-30 20:35:58 -0600", "post_content": [" ", " ", "I'm thinking about how to link certain components together in a network, rather than the FPGA model all on a single board. If I were to say have numerous independent components (arms, legs etc) the obvious thing is fieldbus comms, but is there not a generic asynchronous API or scripting language (DSL) for controlling things?", "Here is some odd psuedo language that's a mix of javascript, lisp and ruby:\nISO 10218. TS 15066 ", "There must be a finite set of conditions around the robot mechanisms that would lend themselves to such a scripting language ... any ideas?", "-Paul", "So if I understood well, every single component will have its own networking abilities and own board and not connected directly to a main controlling board? What about using the CAN bus? I'm sure there is many implementations out there.", "I think any bus or protocol can work, being just a layer for communication. I see a lot of C code written at a very low level for controlling motion and the bus or protocol is embedded into the code. It just seems wrong as it creates tightly coupled code that becomes harder to maintain over time as new features are added.\n\nMy thought is that we need a DSL/API or language to abstract the control away from the actual purpose and separate the various layers of interaction in the robot control space. The current models seem to me to be very centralised. For example incremental encoder counters being on the central processing board. Maybe each limb needs it's own controller and simply broadcasts it's position in space, accepts commands. If say the left hand needs to know what the right hand is doing, there's no ...", "... arm1 could be holding(red_ball) and the status of blue_ball could be on(floor). To pick up the blue_ball, arm1 would need to drop or carefully set down red_ball and so on depending both on rules and also on knowledge of the object (can red_ball be dropped and would that be good or bad). I'm looking for this layer of abstraction, before essentially building it in myself.", "I don't see why a central board is a problem in this case, ROS itself is asynchronous but runs quite well on one single board. And for the knowledge about objects, check knowrob.", "I would like to distribute the processing, no single point of failure, more processing power using a cheap interconnect. Leverage cheap dev boards rather than custom built. Will check out knowrob"], "answer": [], "question_code": ["    require \"iso10218\"\n    arm1 = Arm.new(dof: 5, limits: TS15066, joint_follow: J_LOOSE, url: \"tcprobot://192.168.10.5\")\n    arm1.alert = lambda (signal) {\n    if (signal == TORQUE_EXCEEDED) do\n       emergency_stop()\n       do_something()\n   end\n\n   arm1.follow_sequence([\n      \"G01 X 100 Y 100 Z 100 F 1600\",\n      \"G01 A 355 B 10\",\n      \"G01 A 210 Y 120 X 0\"\n   ])\n"], "url": "https://answers.ros.org/question/188393/is-there-an-async-universal-robot-control-language/"},
{"title": "Multithreaded RViz?", "time": "2014-07-27 14:50:27 -0600", "post_content": [" ", " ", " ", " ", "When running Movit on my desktop, RViz cannot show smoothly arm trajectories. Well, it looks cool at the beginning, but after 2 or 3 extra plannings/executions, CPU load reaches 100% and arm trajectories become jumpy.", "The point is that my laptop, with a less powerful CPU (same RAM) does a much better job, with RViz taking ~80% of CPU, because in a thread-by-thread basis, is much faster:", "So... weird as it can sound, I would love a multi-threaded RViz, or recompile it better fitted for my CPU.", "Any idea of what can I do? (I tried overclocking but always make the PC unstable)", "Hope this gets useful for ROS users thinking to buy a new PC: carefully check per-thread performance!!!", "I made some progresses; compiling RViz for my platform improves things, so now, if I disconnect the 3D sensor, RViz never reaches 100% CPU and so the arm moves smoothly:", "However, performance still degrades when RViz shows the octomap (but takes longer). Someone knows any additional compilation tweak to try? Btw, how can I override the predefined compiler flags? It uses -O2, while I want to try -Os instead (I saw in a benchmark assuring it's better for my CPU)", "Another solution could be to simplify the arm. Yes, I use the tiny turtlebot arm, but the URDF is surprisingly complex, as it adds a link for every frame element, totaling 25 links! (with 23 meshes). Makes this any sense? Or it will be a waste of time?", "Thanks!!!", "Are you sure this is related to CPU performance, and not due to poor graphic performance?", "I'm inclined to say the same: on my -- even slower -- machine (2.8Ghz, c2d) -- I can run RViz and MoveIt for hours on end, without it eating up my entire CPU. Could you add some more info on the rest of your hardware?", "Well, I'm pretty sure it's not the graphic card because I have a one that should be more than enough for RViz (AMD HD 7770), while I have none in the laptop. And even like this, the laptop works better. Also, I changed to proprietary AMD drivers without noticing any improve on RViz", "Any parameter starting with 'D' is passed to cmake.\ncatkin_make -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=-Os  \ncatkin_make -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-Os -march=native\"", "Yes, I have no problem passing arguments to gcc. But I don't know how to remove the default ones, e.g. -O2.", "When you build in ros, the -O's will be applied by cmake according to the CMAKE_BUILD_TYPE setting.", "Run `top` on the rviz process, use H (threads) and f (fields, select the last used cpu). Are you seeing all the computation in one thread? Mine is, but I'm not testing on anything significant. Being a qt program, I wonder if all the hard yakka is getting done in the main thread.", "Such a large program with alot going on could farm out some jobs to threads or hang widgets on the main one from external threads, but that starts to require some thinking/planning."], "answer": [" ", " ", " ", " ", "After edit: you never mentioned that you were doing things with Octomaps, which is why I assumed (and I think ", " as well) that the performance bottleneck was within any of the subsystems RViz uses, and the performance problems were always there, even when just doing simple path planning with MoveIt.", "If performance degrades over time, perhaps you've ran into a memory leak issue. RViz has had some of those in the past (see ", " and ", " fi). You could use any of the standard tools to catch those.", "To get a feeling for which nodes / processes are taking up all the CPU time, try to run ", " or just normal top and see what is going on.", "Edit: just realised that it could very well be something else (ie: ", " RViz) that is your performance bottleneck.", "If this really is a memory leak, it's entirely possible that rviz is bottlenecking on memory speed (and not CPU speed) on you desktop vs laptop.", "If something is constantly malloc-ing/new-ing, then yes, I agree. I can see that becoming a problem with pointclouds/octomap usage.", "OK, my mistake: I shouldn't have mentioned the octomap, as this can blurry the discussion. The performance degradation happens without visualizing octomaps. It just gets worse if I show them, but this means that RViz gets more loaded showing them, not that the CPU struggles creating them.", "I'm pretty sure on this because I have monitored the CPU usage of all processes, and the only making heavy use of the CPU is RViz. That said, I have also observed that RViz keeps taking more and more memory after executing a plan, so looks like it has important memory leaks. I will try with valgrind"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Desktop: AMD FX Series FX-8350 4.0Ghz 8 cores", "Laptop: Intel i7-4600U 3.00 GHz dualcore"], "question_code": ["set(PLATFORM_FAMILY \"amd\" CACHE STRING \"Platform family, usually referring to intel/arm etc.\")\nset(PLATFORM_NAME \"native\" CACHE STRING \"Platform name, usually referring to the cpu architecture.\")\n...\nset(PLATFORM_CXX_FLAGS \"-march=native\" CACHE STRING \"Compile flags specific to this platform.\")\n...\nset(CMAKE_BUILD_TYPE Release CACHE STRING \"Build mode type.\")\n"], "answer_code": ["rqt_top"], "url": "https://answers.ros.org/question/187957/multithreaded-rviz/"},
{"title": "cannot view image topic/calibration window with integrated webcam", "time": "2014-08-23 05:05:59 -0600", "post_content": [" ", " ", " ", " ", "Hello All,", "I'm new to the ROS environment, currently running ROS hydro on Ubuntu 12.04 in virtualbox. I am trying to run camera calibration as per instructions on the ROS tutorials, using the integrated facetime HD camera. It works perfectly fine using cheese/openCV. But when i run the camera calibration, all i get is a gray image\nI tried to subscribe to the image topic and view the published image in a seperate window. I get a green screen", "I run the following commands:", "-roscore", "-rosrun uvc_camera uvc_camera_node frame:=camera device:=/dev/video0", "opening /dev/video0", "pixfmt 0 = 'MJPG' desc = 'MJPEG'", "discrete: 1280x720:   1/1 ", "discrete: 640x480:   1/1 ", "discrete: 320x240:   1/1 ", "int (Brightness, 0, id = 980900): 0 to 100 (1)", "-rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/image_raw :=/", "output of rostopic list is:", "/camera_info", "/image_raw", "/rosout", "/rosout_agg", "Can you please let me know where i am going wrong with this? ", "Can you view the camera output in rqt_image_view or rosrun image_view image_view image:=/image_raw?", "yes i tried rosrun image_view image_view image:/image_raw\nI got a window, filled with green color along with a noisy band on top", ": did you find the error? Because I am experiencing the same symptom: a green area with a noisy band", "I could find a solution to this problem..webcams and virtualbox don't go hand in hand i guess :P  I just moved to a standalone Ubuntu machine and everything fell in place."], "answer": [], "url": "https://answers.ros.org/question/191058/cannot-view-image-topiccalibration-window-with-integrated-webcam/"},
{"title": "Bias move_base to move forward and not turn? [closed]", "time": "2014-08-03 10:46:23 -0600", "post_content": [" ", " ", " ", " ", "Hi, ", "I'm running the navigation stack on the Seekur Jr. robot. After a lot of parameter tuning, I have managed to get the robot to move satisfactorily to the goal.", "However, when the goal is mostly straight in front of the robot (like at the end of a straight corridor) I find that the robot oscillates about (right and left) the path given to it by move_base. Is there a way to bias the robot to select more linear trajectories and less of angular trajectories? ", "In my application, I do not care much about the lateral displacement (and so I have set the xy goal tolerance to be high) but that doesn't solve the problem.", "Thank you.  ", "EDIT: ", " Video link:  ", "I am navigating a corridor. I have a wall following algorithm that is computing a goal and sending it to move base. The goal can be seen by the big red marker and the global path (green) and local path (pink) are also visible. Ignore the green points on the floor in front of the robot (those are the computed ground plane). ", "As you can see, the planner always selects paths that curve a bit to the right. Although it keeps correcting and turning back to the left, it keep going ever so slightly to the right. And finally it gets too close to the obstacle. ", "So I am guessing that for whatever reason it is choosing trajectories that have negative v_theta. Any help you could proved on this would be great.", "Thank you. "], "answer": [" ", " ", "There are a couple of things that could be causing this:", "Without more information, it's hard to say which of these is actually the problem. Screenshots or video of rviz visualizing the global and local plans would be a good first step.", "Thanks for the help. I'll do that ASAP.", " I have uploaded the rviz video (link added to question). I also included details on what you are visualizing. (Sorry it will be uploaded in 40 minutes acc. to Vimeo)", "It looks like the global plan is consistently a little to the right of the robot, so it generates a local plan that goes right to take it quickly back to the global plan. You may want to try increasing the lookahead distance or the angular scoring weight.", " Thank you\nI increased the lookahead distance. That reduces the error a bit. But any ideas on what could be causing the global path to be slightly to the right every time.", "It's hard to say. My best guess would be that the global planner is planning in a discrete space, and the nearest discretization happens to be to the right for most of the video you showed. I wouldn't worry about it unless you're seeing consistent errors over many runs."], "answer_details": ["The global planner chooses a path that isn't very straight, and the local planner follows it", "The global planner chooses a straight path, but the local planner overshoots and oscillates when trying to follow it", "The robot itself has some momentum, and the base controller that's driving it is under-damped, causing the system to oscillate.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/188739/bias-move_base-to-move-forward-and-not-turn/"},
{"title": "dc maxon motor control", "time": "2014-08-06 11:24:51 -0600", "post_content": [" ", " ", "hello \nI am a new user in ROS and i want to build my robot. I have two DC maxon motors with 150 w power and two arduino uno boards. Is it possible for me to control my base with ros_arduino_python package??\nI think i need some extra controller like Pololu but i am not sure. Does anyone do this before???\nI really appreciate anyone who could help me:))"], "answer": [" ", " ", "Yes. As noted on the ", ", you need the recommended Pololu motor shield and the encoder shield.", "The ", " for Hydro goes into more detail about the hardware and wiring that is required."], "url": "https://answers.ros.org/question/189183/dc-maxon-motor-control/"},
{"title": "why some empty areas are with high cost in costmap? [closed]", "time": "2014-09-12 04:23:55 -0600", "post_content": [" ", " ", " ", " ", "Hi, all ,I am using turtlebot2( i.e. kobuki) to test my local planner. The global_planner being used is the default navfn of move_base node (i.e. navfn). But some strange path planning happen due to the incorrect costmap (global costmap ?). \nPlease see following two images.  The black and grey areas are with high cost from global costmap.\nThe red ellipses are the areas that should be with white. Because there are nothing on these area. Beforehand some moving objects such as persons were moving through these areas. But when a turtlebot robot is ordered to go from start points ( blue squares) to goal points (orange squares), there are nothing in these areas. That is why the planned path(green lines) get round these areas like there exists obstacles.  ", " ", "Here I give the yaml files that the move_base node is using.", "local_costmap_params.yaml:", "global_costmap_params.yaml:", "costmap_common_params:", "How to change these grey areas into white area in time? ", "One more thing, I think the grey areas attached to static object (black bold lines from wall and desk) in my map are thicker than I request like the grey area around the desk, how to make them thin? Thank you", "Edit\uff1a one picture is replaced.", "Dear (@Fernando\\ Herrero)\uff0c Thank you for your attention. So I should change the inflation radius smaller to be 0.20, i.e. 0.20m. ", "However, please note the second picture. The white lines are the laser scan, so the area in red ellipse is scanned but still in grey. Why?  Thank you\uff01", "The global costmap is updated with the scan if it has an obstacle layer. \nIf these areas are result of mobile obstacles, they should be cleared by the scan when you go there again and it's free. The thick areas are result of the obstacle inflation, to prevent the robot to get too close to obstacles", "However, please note the second picture. The white lines are the laser scan, so the area in red ellipse is scanned but still in grey. Why?  Thank you\uff01"], "answer": [" ", " ", "I got the solution: the parameter in common_costmap.yaml for laserscan , that is \"Clearing\" should be \"true\". Right? Thank you!"], "question_code": ["local_costmap:\n   global_frame: odom #was /odom\n   robot_base_frame: base_link #was /base_footprint\n   update_frequency: 5.0\n   publish_frequency: 5.0\n   static_map: false\n   rolling_window: true\n   width: 4.0\n   height: 4.0\n   resolution: 0.1\n   transform_tolerance: 0.5\n", "global_costmap:\n   global_frame: /map\n   robot_base_frame: base_link # was /base_footprint\n   update_frequency: 5.0\n   publish_frequency: 1.0\n   static_map: true\n   transform_tolerance: 0.5\n   cost_scaling_factor: 10.0\n   lethal_cost_threshold: 100\n", "max_obstacle_height: 0.60  # assume something like an arm is mounted on top of the robot\nobstacle_range: 2.5\nraytrace_range: 3.0\nrobot_radius: 0.18\ninflation_radius: 0.50\nobservation_sources: scan bump\nscan: {data_type: LaserScan, topic: /scan, marking: true, clearing: false}\nbump: {data_type: PointCloud2, topic: mobile_base/sensors/bumper_pointcloud, marking: true, clearing: false}\n"], "url": "https://answers.ros.org/question/192551/why-some-empty-areas-are-with-high-cost-in-costmap/"},
{"title": "Is it possible to publish messages between 2 Raspberry Pis running ros using a serial type connection (i2c, etc)?", "time": "2014-08-30 10:14:05 -0600", "post_content": [" ", " ", "Hello all, first of all let me say how amazing ROS is, I just started with it a couple of days ago and have been extremely impressed with its capabilities and speed.", "I would like to off load some of my computational power by paralleling multiple Raspberry Pis (2+) on the I2C bus, I've seen a lot of tutorials on hooking up an Arduino to a Pi over the I2C bus / other serial connections but I haven't seen how you would hook 2 Pis together.", "I know we have the technology, because all the Arduino is doing is posting to Serial and the Raspberry Pi is picking up the messages published (with the command: ", ").  But I don't see a similar library to publish these serial messages via python or C.", "Any help in pointing me in the right direction would be greatly appreciated, Thank you!", "Dustin"], "answer": [], "question_code": ["rosrun rosserial_python serial_node.py _port:=/dev/ttyUSB0"], "url": "https://answers.ros.org/question/191557/is-it-possible-to-publish-messages-between-2-raspberry-pis-running-ros-using-a-serial-type-connection-i2c-etc/"},
{"title": "Kinect video Buffer is wrong size.", "time": "2014-03-20 08:53:01 -0600", "post_content": [" ", " ", " ", " ", "I am using the Turtlebot2 with ROS and matlab. I have the plugin for control with Matlab. I have tried Mathwork's Turtlebot ROS demo and everything works very well on that end. ", "I am trying to make a simple script that will just take the RGB input from the on board kinect in the exact same way that the demo uses it. I have copied the code verbatim and I am running into an issue. The demo scripts (within TurtlebotCommunicator.m) grabs message.getData().array() which is a vector of length widthxheightx3 or the red, green and blue channels collated. For a 640x480 image, this vector is approximately  921600 points long, and indeed when I run the demo program in debug mode that is the buffer size that it receives. We I try to run my own version I receive a vector that is one third as long. I am not sure what the disconnect it. ", "Below is the code that I am running, and like I said, it is near verbatim what is written in the demo that I have proven to work. Thanks", "function  getimage", "end", "It sounds like you may be receiving a grayscale image instead of color. I don't know enough about matlab or matlab ROS to suggest more."], "answer": [" ", " ", " ", " ", "Hi,", "For your problem I think you should try: /camera/rgb/image_color", "Am trying to do the same thing but struggling to get data from kinect. Since it is question regarding same topic..I did not create a new post. Here is what am doing, if you see something missing please let me know:", "In terminal (using ros-groovy): ", "In Matlab: ", "I also tried the way you have mentioned but still no progress. Code does not enter in grabimage function.:(", "It can be seen with rostopic info  that NODE is subscribed to topic /camera/rb/image_rect but imgMsg.getData() is always zero.", "Could you please tell if am missing any steps.", "Thanks..:)", "are you running matlab and the openni driver on the same computer or over a network?", "at the moment it is on same computer but does it matter? bcoz. later it will be over a network"], "question_code": ["node = rosmatlab.node('NODE', '10.0.1.4', 11311); \nImgSub = node.addSubscriber('/camera/rgb/image_raw', 'sensor_msgs/Image', 5);\nImgSub.setOnNewMessageListeners({@grabimage});\n\nfunction grabimage(message)\n\nwidth = message.getWidth();\nheight = message.getHeight();\noffset = message.getData().arrayOffset()-2; \n\nif strcmp(message.getEncoding, 'bgr8')\n    indexB = offset:3:width*height*3+offset-1;\n    indexG = indexB+1;\n    indexR = indexG+1;            \nelse                \n    indexR = offset:3:width*height*3+offset-1;\n    indexG = indexR+1;\n    indexB = indexG+1;\nend\n\nimgCol = typecast(message.getData().array(), 'uint8');\nimg = reshape([imgCol(indexR); imgCol(indexG); imgCol(indexB)], width, height, 3);\n\nimg = permute(img, [2 1 3]);\nimshow(img);\n\nend\n"], "answer_code": ["roslaunch openni_launch openi.launch\n", "node = rosmatlab.node('NODE',master_uri)\nsubscriber = rosmatlab.subscriber('/camera/rgb/image_rect','sensor_msgs/Image',10,node);\nimgMsg = rosmatlab.message('sensor_msgs/Image',node);\n", "ImgSub.setOnNewMessageListeners({@grabimage});\n"], "url": "https://answers.ros.org/question/142456/kinect-video-buffer-is-wrong-size/"},
{"title": "My screen is messed up after uninstalling ros. What is the problem? Please refer to the details for the full question", "time": "2014-08-31 19:31:57 -0600", "post_content": [" ", " ", "Hi. I'm new to Linux. I have installed Kubuntu on an acer Aspire 5040 for a project at school. I had also installed ROS on it and had been using the laptop to go through ROS tutorials. I had originally installed ROS fuerte on it but at some point I installed ROS groovy alongside fuerte. Due to some problems I decided to uninstall ROS all together. This is the command I used:", "sudo apt-get remove ros-*", "After it was completely uninstalled I turned off my laptop. The next day when I powered it back on, it would bring up the login window, but after loggin in I would see a black and white striped screen. Did I uninstall some drivers while I was trying to uninstall ROS? How can I make it work again? I would really appreciate your help. "], "answer": [" ", " ", "It's hard to say exactly what else was uninstalled when you removed ROS. You may want to review your dpkg log ( /var/log/dpkg.log ), and check the login type."], "url": "https://answers.ros.org/question/191603/my-screen-is-messed-up-after-uninstalling-ros-what-is-the-problem-please-refer-to-the-details-for-the-full-question/"},
{"title": "local costmap empty using move_base_node", "time": "2014-01-17 06:34:41 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm using move_base_node in stage, and I've found that local costmap is empty. In this picture from rviz I can see that global costmap is fine but local costmap (the small square) is empty. The red dots are the laser scans from robot (green footprint) with which the local costmap should be generated. Is something wrong with this? Because although of this I can set goals and the robot can reach them.", "Although the robot can reach the goals, it does it at a very low speed, even if I increase the value of ", " parameter of local_planner. I don't know if this happens due to the lack of local_costmap.", "If I change ", "for", "where ", " is another LaserScan topic, some black areas start to appear in local_costmap until origin of sensor its outside it.", "This is my ", "; base_* are \"robots\" on each corner of the intersection; I say \"robots\" because I use them as static lasers. And car_* are robots  navigating as vehicles, in this case car_0 is the green polygon.", "Here are the parameters I use:", "costmap_common file:", "global_costmap file:", "What is the output of move_base? Any errors? Also, I'm not sure what the defined behavior is when you don't have a footprint defined (it's commented out in your file). Is it possible that your robot has no footprint, hence it can go anywhere?", "Hi @Tim-sweet, parameter in costmap_common file are common to global and local costmaps, there is the footprint: \n'footprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.3, 0.0], [0.25, -0.25]]'", "Oh I missed that, my bad. Does move_base produce any errors? It might be helpful to set the logger level to debug (you can do that in rqt_console).", "move_base doesn't produce errors, just warnings about control rate.", "What does your TF tree look like? Also are the /base_*/base_scan lasers not on the robot? I'm confused to your setup.", "HI @David_lu, I updated my question with a link to my TF tree and a brief explanation about my setup. Thanks", "When you run the non-working configuration, rostopic info /car_0/base_scan show the proper connections?", "Yes, /car_0/base_scan is connected to /car_0/move_base_node. If you need to test the code is on ", " and "], "answer": [" ", " ", "I faced a similar issue in a much simpler setup. I found out ", " of ", " will not put any obstacles from a sensor in the costmap, if there is negative z-offset between the ", " plane and ", " plane (assuming a planar lidar as sensor). I could get around this by setting a negative value for  ", " (less than the z offset between two planes).", "You saved me! Ran into similar problems and it was exactly this negative z-offset! Thx"], "question_code": ["max_vel_x", "local_costmap:\n      observation_sources: base_scan\n      base_scan: {\n        topic: /car_0/base_scan\n...\n", "local_costmap:\n          observation_sources: base_scan\n          base_scan: {\n            topic: /base_0/base_scan\n    ...\n", "/base_0/base_scan", "#This file contains common configuration options for the two costmaps used in the navigation stack for more details on the parameters in this file, and a full list of the parameters used by the costmaps, please see http://www.ros.org/wiki/costmap_2d\n\n#For this example we'll configure the costmap in voxel-grid mode\nmap_type: voxel\n\n#Voxel grid specific parameters\norigin_z: 0.0\nz_resolution: 0.2\nz_voxels: 10\nunknown_threshold: 9\nmark_threshold: 0\n\n#Set the tolerance we're willing to have for tf transforms\ntransform_tolerance: 0.3\n\n#Obstacle marking parameters\nobstacle_range: 2.5\nmax_obstacle_height: 2.0\nraytrace_range: 3.0\n\n#The footprint of the robot and associated padding\n# included in both global and local costmaps params\n#footprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.3, 0.0], [0.25, -0.25]]\nfootprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.3, 0.0], [0.25, -0.25]]\nfootprint_padding: 0.05\n\n#Cost function parameters\n# included in both global and local costmaps params\ninflation_radius: 0.35\ncost_scaling_factor: 10.0\n\n#The cost at which a cell is considered an obstacle when a map is read from the map_server\nlethal_cost_threshold: 100\n\n#Configuration for the sensors that the costmap will use to update a map\n# included in both global and local costmaps params\n#observation_sources: base_scan\n#base_scan: {data_type: LaserScan, expected_update_rate: 0.4,\n#  observation_persistence: 0.0, marking: true, clearing: true, max_obstacle_height: 0.4, min_obstacle_height: 0.08}\n", "#Independent settings for the global planner's costmap. Detailed descriptions of these parameters can be found at http://www.ros.org/wiki/costmap_2d\n\nglobal_costmap:\n  observation_sources: \"base_0 base_1 base_2 base_3\"\n  base_0: {\n    topic: /base_0/base_scan,\n    data_type: LaserScan, \n    expected_update_rate: 0.4,\n    observation_persistence: 0.0, \n    marking: true, \n    clearing: true, \n    max_obstacle_height: 0.4, \n    min_obstacle_height: 0.08\n    }\n\n  base_1: {\n    topic ..."], "answer_code": ["ObstacleLayer", "Costmap2D", "sensor_frame", "map", "min_obstacle_height"], "url": "https://answers.ros.org/question/119051/local-costmap-empty-using-move_base_node/"},
{"title": "mobile_base topics don't apper on turtlebot_gazebo [closed]", "time": "2014-11-03 03:49:00 -0600", "post_content": [" ", " ", " ", " ", "When I tried to run turtlebot_playground.launch in turtlebot_gazebo,\ngazebo window with turtlebot with some objects appeard, but turtlebot kept rotating and\nsome topics to control turtlebot couldn't be found such as /mobile_base/commands/motor_power and /mobile_base/commands/velocity. How to fix it?", "On ros-hydro", " started roslaunch server  ", "PARAMETERS\n * /cmd_vel_mux/yaml_cfg_file\n * /depthimage_to_laserscan/output_frame_id\n * /depthimage_to_laserscan/range_min\n * /depthimage_to_laserscan/scan_height\n * /robot_description\n * /robot_pose_ekf/freq\n * /robot_pose_ekf/imu_used\n * /robot_pose_ekf/odom_used\n * /robot_pose_ekf/output_frame\n * /robot_pose_ekf/sensor_timeout\n * /robot_pose_ekf/vo_used\n * /robot_state_publisher/publish_frequency\n * /rosdistro\n * /rosversion\n * /use_sim_time", "NODES\n  /\n    cmd_vel_mux (nodelet/nodelet)\n    depthimage_to_laserscan (nodelet/nodelet)\n    gazebo (gazebo_ros/gzserver)\n    gazebo_gui (gazebo_ros/gzclient)\n    laserscan_nodelet_manager (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    spawn_turtlebot_model (gazebo_ros/spawn_model)", "ROS_MASTER_URI=http://localhost:11311", " core service [/rosout] found\nprocess[gazebo-1]: started with pid [19032]\nprocess[gazebo_gui-2]: started with pid [19038]\nprocess[spawn_turtlebot_model-3]: started with pid [19043]\nGazebo multi-robot simulator, version 1.9.6\nCopyright (C) 2014 Open Source Robotics Foundation.\nReleased under the Apache 2 License.\n ", " Gazebo multi-robot simulator, version 1.9.6\nCopyright (C) 2014 Open Source Robotics Foundation.\nReleased under the Apache 2 License.\n ", " process[robot_pose_ekf-4]: started with pid [19049]\nprocess[mobile_base_nodelet_manager-5]: started with pid [19086]\nprocess[cmd_vel_mux-6]: started with pid [19089]\nMsg Waiting for master.process[robot_state_publisher-7]: started with pid [19124]\nprocess[laserscan_nodelet_manager-8]: started with pid [19147]\nMsg Waiting for master\nMsg Connected to gazebo master @  ", " \nMsg Publicized address: 192.168.2.103\nprocess[depthimage_to_laserscan-9]: started with pid [19179] ", " Msg Connected to gazebo master @  ", " \nMsg Publicized address: 192.168.2.103\nDbg plugin model name: turtlebot\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized)\nWarning [RaySensor.cc:283] ranges not constructed yet (zero sized ...", "Can you run ", " of the ", "?"], "answer": [], "question_code": ["kobuki_playground.launch"], "url": "https://answers.ros.org/question/196552/mobile_base-topics-dont-apper-on-turtlebot_gazebo/"},
{"title": "High level design involving a network of robots and a server", "time": "2014-03-19 22:33:36 -0600", "post_content": [" ", " ", "I'm having some trouble trying to come up with a design for a network of robots, and a server. I'm trying to have a set up such that:", "For example, say I have two robots running Ubuntu, and I want to monitor and control them through a web server. Say the server is running Tomcat, and has a public IP, so it can be assessed from anywhere. For security reasons, let's say I'll have to log in with a valid username and password to assess my robots. In this case, I would code up perhaps a .jsp file and serve that, or write some sort of webapp, to have a user interface to interact with the robots. To monitor the robots could mean like, I could select robot 1, and view what the camera mounted on robot 1 is currently capturing (say its running mjpeg_server). It could also mean that, the robot is constantly sending information back to the server about its status, such as whether it is online or offline, its current speed, its battery levels, etc, and the server is displaying the information through the user interface. For control, it could mean that I can toggle between exploration mode, and remote control. In remote control mode, I'd have buttons that I can press to move the robot.", "I'm looking to build something like this, but don't really have an idea on how to start. Could I just use ROS's publisher/subscriber and service system for such communications? The issue is that my server is running on Windows, and as far as I know, ROS is only experimental on Windows, so would it be better for the server to be independent of ROS? If yes, how would I link the server with the robots? Would I create some kind of TCP connection, where the server sends some commands, and I write a ROS package for the robot to interpret these commands and carry them out? ", "I'm not looking for one exact answer, but more of a discussion on how to achieve such a set up, and to explore the options I have to do this. Any ideas, opinions, advice, warnings and reading resources would be appreciated! ", "Did you find and answer for that? Perhaps the best place to start a design discussion is in ros-user mailing list, or look at multimaster SIG group list?"], "answer": [" ", " ", "I managed to come up with a simple solution using OpenVPN. With OpenVPN, I created a private network so that I could directly access the robots through my webserver. From my webserver, I used rosbridge_suite to connect to the robot via a websocket. From there, I could publish messages to topics, subscribe to topics, and call services. I also ran mjpeg_server on the robots to be able to view them from the webserver!", "Hello  Andrew\nI would like to know more about your work on the web controlled robot\nwith ROS .I am also working on somewhat  same thing . Can you help \nme."], "question_details": [" ", " ", " ", " ", " ", " ", "I have a central command center (like a server), where I can monitor and control these robots ", "I can use the server to give commands to each robot to do different things", "Have robots send back responses to commands, back to the server"], "url": "https://answers.ros.org/question/142166/high-level-design-involving-a-network-of-robots-and-a-server/"},
{"title": "Map topic data not published on remote computer", "time": "2014-11-01 05:35:55 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I'm using ROS to run my own differential drive robot. Since my robot is powered by a UDOO board I'm using a minimal Ubuntu installation without GUI so I cannot run rviz on this device. For this reason I use a second laptop with ubuntu in order to control my robot. Roscore and the complete navigation stack (AMCL, move base, gmapping...) runs on the UDOO board and only rviz is running on remote laptop.", "On rviz I'm able to see the laser scan, the robot pose and another topics data but I'm not able to see the map data (the topic is there and also listed by ", ").", "In order to understand what is going I try to show topic data from the remote system: ", "show me scan data. Instead typing", "does not shown any data.\nThe strange thing is that the same test, performed on the system running Roscore, show me topic data for map topic too. It seems that map data only is not sent/received on the remote system!", "I'm using ROS hydro and already try to update all packages on two systems and I made a succesfull network test.", "Any idea about this behavior?", "Thanks\nAle", "UPDATE", "After some tests I notice that some map topic data are published on remote PC but not every one sent by my navigation stack. Some idea about possible reason? ", "Network bandwidth? CPU/memory used out?"], "answer": [" ", " ", "Maybe the message type of the /map topic is not built on the remote PC because the package containing it is not installed there!?", "Dear Wolf", "I checked and map topic exist on the remote PC too.", "After some test I notice that some map data are send to the remote PC but nit every data published by gmapping node. Today I will make some test using another WiFi router just to be sure the problem is not caused by bad networking.", "Tha"], "question_code": ["rostopic list", "rostopic echo scan\n", "rostopic echo map\n"], "url": "https://answers.ros.org/question/196445/map-topic-data-not-published-on-remote-computer/"},
{"title": "Does anyone have an extra PR2 replacement power board? (PR2 replacement parts)", "time": "2014-10-23 13:33:58 -0600", "post_content": [" ", " ", " ", " ", "Dear PR2 users,", "I am maintaining a PR2 at the Institute for Artificial Intelligence of the University of Bremen.", "We have been diagnosing an issue with our robot since September 15th together with the help of Clearpath Robotics, but sadly they don't have any new Power-Boards on stock, and they are working under pressure to repair another board to send to us. Since we depend on the PR2 for the work of a lot of researchers, this has been affecting us heavily.", "The issue started with under-voltage warnings on the right arm, which were halting the robot, happening once or twice a day, and it got worse until the robot can not even complete self-calibration without halting. We now disconnected the right arm, but we need both for some experiments.", "It is a long shot, but who knows?:  Does any other PR2 team have a working power board that they are not using currently?", "Did anyone else have problems with the power board or other components of the PR2 that needed replacement? We had very few issues with our PR2 in TUM (Munich), but the one here has had many more. I'm interested in your experiences with the hardware.", " Is there a mailing list of PR2 users/owners?  I would gladly move this question there if  ", "  is not an appropriate place. ", "Thanks in advance!", "Alexis"], "answer": [" ", " ", "Alexis,", " For the mailing lists, you can request access to the PR2 Google Groups which is a replacement for the old  ", "  pr2-users mailing list.  ", " ", "Thanks a lot for the link to the new mailing list. Will try there.", " ", " ", "The best place to reach PR2 users is the pr2-users mailing list or administrators on pr2-admin  The ", " links to  ", "  as the home for those lists. But that site appears to be down. I suspect the mailing lists were migrated, but I don't know where to.  ", " Hi Tully, we've moved to a google group now for the PR2 users since the WG virtual server that was hosting them was taken offline. They can now be found here  ", "It would be great to update the links on the pr2s site homepage.", "I agree, will do."], "url": "https://answers.ros.org/question/195763/does-anyone-have-an-extra-pr2-replacement-power-board-pr2-replacement-parts/"},
{"title": "Task scheduler recommendation/advice", "time": "2014-10-24 04:19:26 -0600", "post_content": [" ", " ", "Hi all,", "I am looking for some advise on choosing a way/tool to achieve high level control (or say tasks scheduling) with ros. I am naturally more inclined to using scripting (e.g. in python as naturally it integrates with ros) because I have been programming for years and I am convinced of the versatility of languages.", "Though I was wondering whether there were other solutions that you would recommend based on experience (e.g. solutions that improve workflow, are more something-friendly... and do not limit the expressive power)", "Thanks,", "Antoine."], "answer": [" ", " ", "Straight-forward scripting with actionlib interfaces works. If that is sufficient for your goals depends on the task at hand.", "The standard ROS way is probably to use ", ", which builds on the concept of state machines ", "There is a ", " developed by our group, although I consider that experimental. There might also be other solutions in ROS.", "Hum, if Smach is as standard as I expect it to be that may well be a no brainer... Thanks ;)", "Could actionlib be used also for different tasks such as navigation and manipulation? For example, patrolling around in a room and once you spot small objects on the ground, pick them up, place them on a table and continue patrolling. Can a behavior like this be implemented with actionlib?"], "url": "https://answers.ros.org/question/195818/task-scheduler-recommendationadvice/"},
{"title": "Is PR2 also follow DH Convention?", "time": "2014-11-04 02:50:25 -0600", "post_content": [" ", " ", " ", " ", "I found weird things.", "I open rviz for PR2.", "We can see that in /base_link, z axis is blue.", "But why left arm joint axis is not blue but green?", "Is PR2 follow DH convention?", "If not, how to understand the coordinate design of PR2?", "Thank you~"], "answer": [" ", " ", "Robots using URDF rarely follow DH conventions -- and a number or ROS packages (calibration, for sure) don't necessarily work with all the extra fixed rotations that are needed to make URDF follow DH.", "For the PR2, and a number of other robots, the convention has been that \"when the joints are at their 0.0 position, all of the TF frames will line up parallel to the base_link orientation\". Thus, if you put the PR2 arm out forward and level with the ground, with all the roll joints zeroed, all the links will be red (X) forward, blue (Z) up.", " ", " ", "The URDF allows you to freely set the axis of rotation for a joint.", "The coordinate frames are probably layout so that coordinates on a link are as intuitive as possible."], "url": "https://answers.ros.org/question/196669/is-pr2-also-follow-dh-convention/"},
{"title": "Disparity image reconstruction", "time": "2014-11-17 03:00:23 -0600", "post_content": [" ", " ", " ", " ", "I am using two prosilica cameras in order to generate the disparity image. by the help of stereo_image_proc I can generate the disparity image and is available in /stereo/disparity_image topic. Now in another node I am reconstructing the disparity image using raw data[]. ", "for each set of pixel data I am ignoring one data as it provides the depth..\nduring viewing the reconstructed image only the colors get swaped. that menas the portion with green switched to some other color and so on.. ", "in the DosparityImage format there is a fuled name Image. In Image field there is data[]. is that data contains RGB and Depth info raw data of the iamge or something else.. thanks in advance\n????"], "answer": [" ", " ", " ", " ", "I solved the problem. The data available in the disparity image is not the RGB values. Those are encoded in TYPE_32FC1\nso inorder to reconstruct the image we have to use some colormap done in stereo_view node in the image_view package. just subscribe the topic where the disparity image is available."], "question_code": ["int count = 0;\nsensor_msgs::Image img = disp.image;\nint h = (img.height);\nint w = (img.width);\ncv::WImageBuffer3_b pic(h,w);\nROS_INFO(\"height width and step [%d]  [%d]  [%d]\" ,img.height,img.width,img.step);\ncount = 0; \nfor(int i=(h-1);i>=0;i--) {\n   for(int j=(w-1);j>=0;j--)  { \n      count++;\n      *(pic(i,j)) = (img.data[count]);\n      count++;\n      *(pic(i,j)+1) = (img.data[count]);\n      count++;   \n      *(pic(i,j)+2) = (img.data[count]);\n      count++;\n  }\n}\n"], "answer_code": ["void imageCallback(const stereo_msgs::DisparityImageConstPtr& disp) {\n  Mat_ Vec3b disparity_color_;\n  float min_disparity = disp->min_disparity;\n  float max_disparity = disp->max_disparity;\n  float multiplier = 255.0f / (max_disparity - min_disparity);\n  assert(disp->image.encoding == sensor_msgs::image_encodings::TYPE_32FC1);\n  const cv::Mat_<float> dmat(disp->image.height, disp->image.width,\n                             (float*)&disp->image.data[0], disp->image.step);\n  disparity_color_.create(disp->image.height, disp->image.width);\n\n  for (int row = 0; row < disparity_color_.rows; ++row) {\n    const float* d = dmat[row];\n    for (int col = 0; col < disparity_color_.cols; ++col) {\n      int index = (d[col] - min_disparity) * multiplier + 0.5;\n      index = std::min(255, std::max(0, index));\n      // Fill as BGR\n      disparity_color_(row, col)[2] = colormap[3*index + 0];\n      disparity_color_(row, col)[1] = colormap[3*index + 1];\n      disparity_color_(row, col)[0] = colormap[3*index + 2];\n    }\n  }\n  imshow( \"view\", disparity_color_ );\n}\n"], "url": "https://answers.ros.org/question/197576/disparity-image-reconstruction/"},
{"title": "RQT graph line colour", "time": "2014-12-13 18:08:24 -0600", "post_content": [" ", " ", "Hi there.\nIm running the rqt graph code.as you know there is lot of line, and when you move the mouse at the line, there will be colour on the line like red, green, orange,blue and others.\nSo what are the meaning for each colour on the rqt graph?\nregards."], "answer": [" ", " ", "Each line represents a single field (from a topic) which is plotted over time.", "The color of the line indicates which value it represents; the ", " on the graph tells you which field in the topic is represented by each color.", "If you're still having trouble, maybe you can edit you question to include a screenshot, and I can explain by example."], "url": "https://answers.ros.org/question/199438/rqt-graph-line-colour/"},
{"title": "cmd_vel linear and angular relationship", "time": "2014-11-17 15:28:10 -0600", "post_content": [" ", " ", "Hi All,", "I'm using rqt robot steering to output cmd_vel message which I subscribe to and they control the power of the motors.", "Linear is fine and +/-100% is controlling the motors fed and reverse okay. How should I combine that with the rotational input such that the effect is to increase or decrease the rate of turn? Just something simple is fine, this is a 2 wheeled robot. ", "The examples I keep finding on the internet seem to be simple key based designs. i.e. F=Fwd/L=Left etc... ", "Many Thanks", "Mark"], "answer": [" ", " ", " ", " ", "I think the following is what you're looking for. I'm not familiar with rqt's robot steering, so I'm assuming you're using ", " messages. If so, and assuming a ", ", the usual method requires knowledge of the wheel separation (i.e., axle length) and wheel diameters.", "Thanks Kramer -  steering passes through rad/s so I can convert that m/s (v = \u03c9r)", " ", " ", "Actually if I add of subtract (left or right) cos of the angle that should work yes?"], "answer_code": ["geometry_msgs/Twist", "void twistCb(geometry_msgs::TwistConstPtr &msg) {\n  transVelocity = msg->linear.x;\n  rotVelocity = msg->angular.z;\n  double velDiff = (wheelSep * rotVelocity) / 2.0;\n  double leftPower = (transVelocity + velDiff) / wheelRadius;\n  double rightPower = (transVelocity - velDiff) / wheelRadius;\n}\n"], "url": "https://answers.ros.org/question/197626/cmd_vel-linear-and-angular-relationship/"},
{"title": "Kinect not connected ubuntu 12.04, 32bit processor, ros fuerte [closed]", "time": "2015-01-28 04:15:39 -0600", "post_content": [" ", " ", "Hi,", "I am using ubuntu 12.04, 32 bit processor, ros fuerte.\nI am trying to use gmapping using turtlebot with kinect sensor. Turtlebot running fine. But when given command for gmapping, it showing", "I checked usb list and there i found that its showing only motor and not video and audio", "vishu@vishu-pc:~$ lsusb\nBus 001 Device 011: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 002 Device 003: ID 04f2:b008 Chicony Electronics Co., Ltd USB 2.0 Camera\nBus 005 Device 006: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\nBus 005 Device 007: ID 0461:4e22 Primax Electronics, Ltd \nBus 007 Device 002: ID 0930:0508 Toshiba Corp. Integrated Bluetooth HCI\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 001 Device 012: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nvishu@vishu-pc:~$ ", "kinect even getting power and the light too glowing, I tried several things  to test like using \"glview\", in response i got no devices connected", "so am I facing usb driver problem?\nor, whether kinect works with 32bit processor?\nDo I need to install any other dependencies?", "please help me.. "], "answer": [], "url": "https://answers.ros.org/question/201925/kinect-not-connected-ubuntu-1204-32bit-processor-ros-fuerte/"},
{"title": "kinect not working with ubuntu 12.04, 32 bit processor, ros fuerte [closed]", "time": "2015-01-28 06:12:30 -0600", "post_content": [" ", " ", "Hi,", "I am using ubuntu 12.04, 32 bit processor, ros fuerte.\nI am trying to use gmapping using turtlebot with kinect sensor. Turtlebot running fine. But when given command for gmapping, it showing", "[ INFO]    [1422458174.703395280]: ", "\n[/openni_launch] No devices ", "\nconnected.... waiting for devices to \nbe connected ", "[ INFO]       [1422458175.471462783]:\nStill waiting    on map...", "I checked usb list and there i found that its showing only motor and not video and audio", "Bus 006 Device 001: ID 1d6b:0001\nLinux Foundation 1.1 root hub", "Bus 007 Device 001: ID 1d6b:0001\nLinux Foundation 1.1 root hub", "Bus 001 Device 012: ID 045e:02b0\nMicrosoft Corp. Xbox NUI Motor", "vishu@vishu-pc:~$", "kinect even getting power and the light too glowing, I tried several things  to test like using \"glview\", in response i got no devices connected", "so am I facing usb driver problem?\nor, whether kinect works with 32bit processor?\nDo I need to install any other dependencies?", "please help me.. ", "when I connect kinect (I am sure power is ON) to PC and typed command lsusb, I found \nusb0 : microsoft ", " motor \nand not able to view audio and camera option"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "[ INFO] [1422458174.471326802]: Still\nwaiting on map...", "[ INFO]    [1422458175.716978152]: ", "\n[/openni_launch] No devices ", "\nconnected.... waiting for devices to \nbe connected", "vishu@vishu-pc:~$ lsusb", "Bus 001 Device 011: ID 0409:005a NEC\nCorp. HighSpeed Hub", "Bus 002 Device 003: ID 04f2:b008\nChicony Electronics Co., Ltd USB 2.0\nCamera", "Bus 005 Device 006: ID 0403:6001\nFuture Technology Devices\nInternational, Ltd FT232 USB-Serial\n(UART) IC", "Bus 005 Device 007: ID 0461:4e22\nPrimax Electronics, Ltd", "Bus 007 Device 002: ID 0930:0508\nToshiba Corp. Integrated Bluetooth\nHCI", "Bus 001 Device 001: ID 1d6b:0002\nLinux Foundation 2.0 root hub", "Bus 002 Device 001: ID 1d6b:0002\nLinux Foundation 2.0 root hub", "Bus 003 Device 001: ID 1d6b:0001\nLinux Foundation 1.1 root hub", "Bus 004 Device 001: ID 1d6b:0001", "List item Linux Foundation 1.1 root hub", "Bus 005 Device 001: ID 1d6b:0001\nLinux Foundation 1.1 root hub"], "url": "https://answers.ros.org/question/201937/kinect-not-working-with-ubuntu-1204-32-bit-processor-ros-fuerte/"},
{"title": "Kinect not connected ubuntu 12, 32bit processor, ros fuerte [closed]", "time": "2015-01-28 04:17:09 -0600", "post_content": [" ", " ", "Hi,", "I am using ubuntu 12.04, 32 bit processor, ros fuerte.\nI am trying to use gmapping using turtlebot with kinect sensor. Turtlebot running fine. But when given command for gmapping, it showing", "I checked usb list and there i found that its showing only motor and not video and audio", "vishu@vishu-pc:~$ lsusb\nBus 001 Device 011: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 002 Device 003: ID 04f2:b008 Chicony Electronics Co., Ltd USB 2.0 Camera\nBus 005 Device 006: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\nBus 005 Device 007: ID 0461:4e22 Primax Electronics, Ltd \nBus 007 Device 002: ID 0930:0508 Toshiba Corp. Integrated Bluetooth HCI\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 001 Device 012: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nvishu@vishu-pc:~$ ", "kinect even getting power and the light too glowing, I tried several things  to test like using \"glview\", in response i got no devices connected", "so am I facing usb driver problem?\nor, whether kinect works with 32bit processor?\nDo I need to install any other dependencies?", "please help me.. "], "answer": [], "url": "https://answers.ros.org/question/201926/kinect-not-connected-ubuntu-12-32bit-processor-ros-fuerte/"},
{"title": "Commanding simple paths / curves to move_base", "time": "2014-12-19 20:40:03 -0600", "post_content": [" ", " ", "Hi", "I want to measure the power consumption of my robot when it moves on some simple paths.", "\nFor example I would like to command ", ". Is there any way to save and modify  certain paths? \nWhat I tried so far:", "Thanks for your help.", "have you found the solution to this?"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Just giving goals to the robot leads to a very unspecific behavior which is not easily repeatable. ", "Also simply using ", " is too low level for this."], "url": "https://answers.ros.org/question/199865/commanding-simple-paths-curves-to-move_base/"},
{"title": "Integrate an IMU sensor with robot_localization", "time": "2014-11-27 07:42:00 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I'm trying to integrate an IMU sensor to my mobile robot no holonomic.", "I follow the robot_localization tutorial to do that, but I'm a little confused with some questions.", "First, how should be my resulting tf tree? I think the frame \"odom_ekf\" provided from ekf_localization node would be at the top of the tree. The base_link frame would be down. So when I called the set_pose service provided from ekf_localization node, this service could changed the values of transformed between \"odom_ekf\" frame and \"base_link\" frame. Is that correct?", "Second, only starting to fuse the IMU sensor with the odometry, how the launch file of robot_localization should be?", "With this launch, the tf between odom_ekf and base_link is published and the topic /odometry/filtered shows correctly the robot position. But:", "- The topic /odometry/filtered doesn't take into a count the changes at orientation. The position is more or less fine, but the orientation doesn't change.\n- The remap to /odometry/odom_imu is not working.\n- When the robot_localization is running these warnings appears constantly:", "\"[ WARN] [1417095211.082070235]: MessageFilter [target=odom_ekf ]: Dropped 100.00% of messages so far. Please turn the [ros.robot_localization.message_notifier] rosconsole logger to DEBUG for more information.\nI think first I have to solve this problems and then integrate the GPS sensor.", "I'm not sure what I'm doing wrong. Some help would be greatly appreciated.", "Here is my ", "I've corrected some errors with the tf and now the robot_localization node doesn't show any errors.", "The problem now is that the odometry/filetered isn't given the correct position and orientation. This is my launch file:", "The odometry of odom0 is ...", "Question before I update the answer: is this robot holonomic? It doesn't look it, but I know the Summit XLs can have omnidirectional wheels.", "Comment in response to update 3: I'll take a look at this. Just so I'm clear, for the path that you drove, the robot's real-world start and end locations were exactly the same, correct?", "One more thing: I keep getting errors when I attempt to filter your ROS bag files. This happened for the last file as well. The error is long, but here's the last line of it:", "UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 4080: ordinal not in range(128)", "The path that I drove finished more or less  at the same location that started. Just the odometry (red) is so reliable, the odometry_filtered (green) should be very similiar to odometry(red).", "I don't have any problem playing my  bagfiles. I'm using hydro, maybe you're using indigo and something at bagfiles changed?", "Great! I'm glad it's working for you. Do you mind if I use your bag file for generating new integration tests? I ask because it would likely become part of the package, at least until I locate some web space for storing them.", "Of course. Feel free to use my bag file.", "ASoriano, Do you have the file where you have fill the imu msg? I use rosserial_arduino to do that but I want to compare other methods. \nThank you."], "answer": [" ", " ", " ", " ", "Before I start, it would be super helpful if you would also post sample messages from your odometry and IMU. Also, if you can, maybe post your tf tree?", "To your questions:", "Does your IMU provide only absolute orientation, or does it provide rotational velocity as well? Right now, you have it set for absolute orientation and linear acceleration only. If you wanted rotational velocity as well, do this:", "Unless you have good reason, set the ", " setting to ", ". Will you be starting on an incline? ", "The warning is very telling. The issue is that the filter can't transform your IMU messages into the target ", " frame. If you post your ", " tree and a sample IMU message, that will help.", "Side note 1: gravitational acceleration only applies to IMU topics. You can remove", "Side note 2: Your odometry configuration needs some tweaking. Is your robot operating in 2D or 3D? If it's 3D (or if you want to include subtle variations in the ground plane for a robot operating in 2D), then does your odometry data actually fill out roll, pitch, roll velocity, and pitch velocity? Also, your odometry data doesn't contain linear acceleration, so the last three ", " values are unnecessary. I'd have to see your odometry data to give you a more definitive answer,  but for now, try this:", "Fix those issues and post your messages and I can help more. ", "EDIT after update:", "Two things:", "- Looking at your rviz output, it seems as though one of your orientation data sources is producing headings (or heading velocities) that are the wrong sign. As per ", ", it's critical that you look at the output of each of your sensors, one at a time, and make sure that the signs are correct and in adherence with REP-103. If your robot turns left, that is positive yaw. You'll have to covert from quaternions in the messages to Euler angles (unless you're good at deciphering quaternions). ", "Fix those, and then let me know how it goes. If it's still ..."], "answer_details": ["If your ", " is the same as your ", " (which is the case for you), then the node outputs a transform from your ", " to your ", ". This transform is the same as the position that is output by the filter, so yes, if you call the ", " service, the transform should also change.", " ", " ", " ", " ", "For your odometry data, use the velocity or use the absolute position. Don't use both. If your odometry source is generating body-frame velocities (and make sure they're in the body frame), then I'd go with those, and set X, Y, and yaw to false.", " ", " ", " ", " "], "question_code": ["<launch>\n<node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization\" clear_params=\"true\" output=\"screen\">\n\n  <param name=\"frequency\" value=\"30\"/>\n  <param name=\"sensor_timeout\" value=\"0.1\"/>\n\n  <param name=\"odom_frame\" value=\"odom_ekf\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom_ekf\"/>\n\n  <param name=\"odom0\" value=\"/myOdomTopic\"/>\n  <param name=\"imu0\" value=\"/myImuTopic\"/>\n  <rosparam param=\"odom0_config\">[false, false, true,\n                                  true, true, true,\n                                  true, true,true,\n                                  true, true, true,\n                                  true, true, true]</rosparam>\n\n  <rosparam param=\"imu0_config\">[false, false, false,\n                                 true,  true,  true,\n                                 false, false, false,\n                                 false, false, false,\n                                 true,  true,  true]</rosparam>\n\n  <param name=\"imu0_remove_gravitational_acceleration\" value=\"true\"/>\n  <param name=\"odom0_remove_gravitational_acceleration\" value=\"true\"/>\n\n  <param name=\"odom0_differential\" value=\"true\"/>\n  <param name=\"imu0_differential\" value=\"false\"/>\n  <remap from=\"/odometry/filtered\" to=\"/odometry/odom_imu\" />\n\n </node>\n</launch>\n", "<launch>\n  <node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization\" clear_params=\"true\" output=\"screen\">\n<param name=\"frequency\" value=\"30\"/>\n<param name=\"sensor_timeout\" value=\"0.1\"/>\n  <param name=\"odom_frame\" value=\"summit_a/odom\"/>\n      <param name=\"base_link_frame\" value=\"summit_a/base_footprint\"/>\n<param name=\"world_frame\" value=\"summit_a/odom\"/>\n<param name=\"odom0\" value=\"summit_xl_controller/odom\"/>\n <rosparam param=\"odom0_config\">[true, true, false,\n                                      false, false, true,\n                                      true, true, false,\n                                      false, false, true,\n                                      false, false, false]\n                                      </rosparam>\n<rosparam param=\"imu0_config\">[false, false, false,\n                                     false, false, false,\n                                     false, false, false,\n                                     false, false, true,\n                                     false, false, false]</rosparam>\n  <param name=\"two_d_mode\" value=\"true\"/>\n      <param name=\"odom0_differential\" value=\"true\"/>\n<param name=\"imu0_differential\" value=\"true\"/>\n   </node>\n</launch>\n"], "answer_code": ["world_frame", "odom_frame", "world_frame", "odom_frame", "set_pose", "<rosparam param=\"imu0_config\">[false, false, false,\n                             true,  true,  true,\n                             false, false, false,\n                             true, true, true,\n                             true,  true,  true]</rosparam>  \n\n<param name=\"imu0_differential\" value=\"true\"/>\n", "imu0_differential", "tf", "tf", "<param name=\"odom0_remove_gravitational_acceleration\" value=\"true\"/>\n", "<rosparam param=\"odom0_config\">[false, false, true,\n                                true, true, true, \n                                true, true, true,\n                                true, true, true,\n                                false, false, false]</rosparam>\n"], "url": "https://answers.ros.org/question/198451/integrate-an-imu-sensor-with-robot_localization/"},
{"title": "Raspian / armhf package repository plans?", "time": "2015-01-21 16:03:13 -0600", "post_content": [" ", " ", " ", " ", "Are there any plans for building and providing a standard repository for armhf packages compatible with the likes of Rasbian?", "Raspberry Pi seem like a great gateway drug for getting into ROS given how cheap, relatively powerful and well supported platform it is.  Seem like a very sensible/obvious first choice when upgrading a robot to include a computer capable of running a full blown OS.  The rather convoluted, shaky and time consuming process of building the environment and trying to get different packages is pretty intimidating if your not already a ROS veteran and a competent  linux buff. I can claim neither.", "I have come across a willow garage effort by Paul Mathieu in mid 2013 for groovy (including buildfarm instructions) which looks great but I don't see any indication that it was ever taken further or updated since.", "Having prowled through the ROS tutorials (via ROS in a VM) and being thoroughly impressed by it's capabilities I only imagine the satisfaction of being able to start operating ROS on a fresh $40 RPi with in minutes of powering it on for the first time.", "I myself am one of those amateur robot builder trying to upgrade from a arduino based arbotix include a RPi and I know it has all the tools I need but getting them to work remains a pretty convoluted process still.", "Spend the extra $10-15 for a better board with an ARMv7 processor. You'll get something significantly more powerful and better supported by Ubuntu and ROS. I have no plans to support the Raspberry Pi.", "Raspberry Pi 2 is being announced these days with a quad 900Mhz ARMv7 and 1GB RAM so maybe I will have the entire cake and eat it in the end. I'll be looking for your packages when I get my hands on one :)"], "answer": [" ", " ", "The current build farm does not build ARM packages on a regular base.", " built ARMhf packages some time ago and they are being imported into the official repositories in the next days (?). But they are already several months old.", " The upcoming build farm will be able to build ARMhf at the same time as the other architectures. If you would like to try these packages (which are still in a temporary apt repository) you can get them from  ", "Hopefully the new build farm can replace the current one (for Indigo and Jade) in the near future.", "Note that these packages (old and new) won't work on the Raspberry Pi; it's a ARMv6 instruction set, while all of the builds are done for the ARMv7 instruction set.", "Will they work on the new Raspberry Pi 2 B? I am playing around with one now but there was some problem following the Raspbian instructions. I suppose the Raspbian instructions when refrencing binary debs will give you the wrong kind (for ARMv6).", " The UbuntuARM debs probably won't work on Raspbian. They might work if you install Ubuntu. See also:  "], "url": "https://answers.ros.org/question/201469/raspian-armhf-package-repository-plans/"},
{"title": "Can not find ROS folder on my directories", "time": "2015-02-28 13:12:18 -0600", "post_content": [" ", " ", "I have instaled ROS and i got this output while instaling ii: ", "... A instalar ros-indigo-ecl-math", "(0.61.0-0trusty-20141230-0416-+0000)", "... A instalar ros-indigo-ecl-geometry", "(0.61.0-0trusty-20141230-1212-+0000)", "... A instalar", "ros-indigo-ecl-mobile-robot", "(0.60.0-1trusty-20141230-1216-+0000)", "... A instalar ros-indigo-ecl-sigslots", "(0.61.0-0trusty-20141230-1208-+0000)", "... A instalar ros-indigo-ecl-streams", "(0.61.0-0trusty-20141230-1206-+0000)", "... A instalar", "ros-indigo-forward-command-controller", "(0.9.1-0trusty-20141230-0438-+0000)", "... A instalar", "ros-indigo-effort-controllers", "(0.9.1-0trusty-20141230-0554-+0000)", "... A instalar", "ros-indigo-fake-localization", "(1.11.14-0trusty-20141230-0937-+0000)", "... A instalar ros-indigo-gateway-msgs", "(0.7.8-0trusty-20141230-0034-+0000)", "... A instalar ros-indigo-navfn", "(1.11.14-0trusty-20150120-2137-+0000)", "... A instalar", "ros-indigo-global-planner", "(1.11.14-0trusty-20150120-2142-+0000)", "... A instalar", "ros-indigo-openslam-gmapping", "(0.1.0-2trusty-20141229-2204-+0000)", "... A instalar ros-indigo-gmapping", "(1.3.5-0trusty-20141230-0937-+0000)", "... A instalar", "ros-indigo-hector-uav-msgs", "(0.3.3-0trusty-20141230-0521-+0000)", "... A instalar", "ros-indigo-hector-quadrotor-controller", "(0.3.3-0trusty-20141230-0524-+0000)\n  ... A instalar", "ros-indigo-hector-sensors-description", "(0.3.2-0trusty-20150114-1920-+0000)", "... A instalar", "ros-indigo-hector-quadrotor-description", "(0.3.3-0trusty-20150114-2126-+0000)", "... A instalar", "ros-indigo-hector-quadrotor-model", "(0.3.3-0trusty-20141230-0524-+0000)", "... A instalar ros-indigo-joy", "(1.10.0-0trusty-20150110-2152-+0000)", "... A instalar", "ros-indigo-hector-quadrotor-teleop", "(0.3.3-0trusty-20150111-0521-+0000)", "... A instalar", "ros-indigo-hector-quadrotor", "(0.3.3-0trusty-20150114-2326-+0000)", "... A instalar", "ros-indigo-joint-limits-interface", "(0.9.1-0trusty-20141230-0500-+0000)", "... A instalar", "ros-indigo-joint-state-controller", "(0.9.1-0trusty-20141230-0445-+0000)", "... A instalar", "ros-indigo-kobuki-dock-drive", "(0.6.0-0trusty-20141230-1216-+0000)", "... A instalar ros-indigo-kobuki-msgs", "(0.6.1-0trusty-20141230-0035-+0000)", "... A instalar", "ros-indigo-yocs-cmd-vel-mux", "(0.6.3-0trusty-20141230-0533-+0000)", "... A instalar", "ros-indigo-kobuki-auto-docking", "(0.6.5-0trusty-20141230-1219-+0000)", "... A instalar", "ros-indigo-kobuki-bumper2pc", "(0.6.5-0trusty-20141230-0437-+0000)", "... A instalar", "ros-indigo-kobuki-driver", "(0.6.0-0trusty-20141230-1218-+0000)", "... A instalar ros-indigo-kobuki-ftdi", "(0.6.0-0trusty-20141230-0103-+0000)", "... A instalar", "ros-indigo-yocs-velocity-smoother", "(0.6.3-0trusty-20141230-1209-+0000)", "... A instalar ros-indigo-kobuki-keyop", "(0.6.5-0trusty-20141230-1213-+0000)", "... A instalar", "ros-indigo-yocs-controllers", "(0.6.3-0trusty-20141230-0433-+0000)", "... A instalar", "ros-indigo-kobuki-random-walker", "(0.6.5-0trusty-20141230-1208-+0000)", "... A instalar ros-indigo-kobuki-rapps", "(0.6.5-0trusty-20141230-1222-+0000)", "... A instalar", "ros-indigo-kobuki-safety-controller", "(0.6.5-0trusty-20141230-1208-+0000)", "... A instalar ros-indigo-kobuki-node", "(0.6.5-0trusty-20150110-0807-+0000)", "... A instalar ros-indigo-uuid-msgs", "(1.0.4-0trusty-20141230-0052-+0000)", "... A instalar", "ros-indigo-rocon-service-pair-msgs", "(0.7.8-0trusty-20141230-0434-+0000)", "... A instalar", "ros-indigo-rocon-std-msgs", "(0.7.8-0trusty-20141230-0436-+0000)", "... A instalar", "ros-indigo-rocon-app-manager-msgs", "(0.7.8-0trusty-20141230-0439-+0000)", "... A instalar", "ros-indigo-rocon-python-utils", "(0.1.13-0trusty-20150112-1220-+0000)", "... A instalar", "ros-indigo-rocon-console", "(0.1.13-0trusty-20150112-1201-+0000)", "... A instalar ros-indigo-rocon-ebnf", "(0.1.13-0trusty-20150112-1206-+0000)", "... A instalar ros-indigo-rocon-uri", "(0.1.13-0trusty-20150112-1236-+0000)", "... A instalar", "ros-indigo-rocon-app-utilities", "(0.7.6-0trusty-20150112-1239-+0000)", "... A instalar ros-indigo-rocon-apps", "(0.7.6-0trusty-20150112-1157-+0000)", "... A instalar ros-indigo-unique-id", "(1.0.4-0trusty-20141230-0450-+0000)", "... A instalar", "ros-indigo-rocon-python-comms", "(0.1.13-0trusty-20150112-1212-+0000)", "... A instalar", "ros-indigo-rocon-gateway-utils", "(0.7.6-0trusty-20150112-1254-+0000)", "... A instalar", "ros-indigo-rocon-python-redis", "(0.1.13-0trusty-20150112-1215-+0000)", "... A instalar", "ros-indigo-rocon-hub-client", "(0.7.6-0trusty-20150112-1350-+0000)", "... A instalar", "ros-indigo-rocon-python-wifi", "(0.1.13-0trusty-20150112-1235-+0000)", "... A instalar", "ros-indigo-zeroconf-msgs", "(0.2.1-0trusty-20141230-0053-+0000)", "... A instalar", "ros-indigo-zeroconf-avahi", "(0.2.3-1trusty-20141230-0451-+0000)", "... A instalar", "ros-indigo-rocon-gateway", "(0.7.6-0trusty-20150112-1352-+0000)", "... A instalar", "ros-indigo-rocon-semantic-version", "(0.1.13-0trusty-20150112-1229-+0000)", "... A instalar", "ros-indigo-rocon-bubble-icons", "(0.1.13-0trusty-20150112-1158-+0000)", "... A instalar ros-indigo-rocon-icons\n  (0.1.13-0trusty-20150112-1209-+0000)", "... A instalar", "ros-indigo-rocon-interaction-msgs", "(0.7.8-0trusty-20141230-0439-+0000)", "... A instalar", "ros-indigo-rocon-interactions", "(0.1.13-0trusty-20150112-1243-+0000)", "... A instalar", "ros-indigo-rocon-master-info", "(0.1.13-0trusty-20150112-1225-+0000)", "... A instalar", "ros-indigo-std-capabilities", "(0.1.0-0trusty-20141229-2149-+0000)", "... A instalar", "ros-indigo-kobuki-description", "(0.6.5-0trusty-20150114-1927-+0000)", "... A instalar", "ros-indigo-kobuki-gazebo-plugins", "(0.4.1-0trusty-20150110-2245-+0000)", "... A instalar", "ros-indigo-smart-battery-msgs", "(0.1.0-0trusty-20141230-0052-+0000)", "... A instalar", "ros-indigo-laptop-battery-monitor", "(0.1.2-0trusty-20150106-0523-+0000)", "... A instalar ros-indigo-map-server", "(1.11.14-0trusty-20141230-0942-+0000)", "... A instalar", "ros-indigo-move-base-msgs", "(1.11.14-0trusty-20141230-0421-+0000)", "... A instalar", "ros-indigo-rotate-recovery", "(1.11 ..."], "answer": [" ", " ", "If you installation returned without an error ROS is probably installed. ~/ros is a common place in your home directory to put a workspace where you will install your sources. ", "The most common reason for this sort of error is that you did not follow all the previous steps in the instructions. It looks like you're following a tutorial. If you'd like more help, please link to the tutorial and provide enough information to reproduce the issue. ", " ", " ", "I have looked for the ROS folder on", "Blockquote", "on /opt/ros/indigo", "Blockquote", "and still i could't find it. I have done on the comand line :", "s:/opt/ros/indigo$ cd ~/ros", "and got ", "bash: cd: /home/s/ros: No such file or directory", "This means that ROS is not correctly instaled?", "Thanks", " ", " ", "ROS packages installed through apt are installed into ", " ; for indigo this will be "], "answer_code": ["/opt/ros/<distro>", "/opt/ros/indigo"], "url": "https://answers.ros.org/question/203974/can-not-find-ros-folder-on-my-directories/"},
{"title": "how to separate channels of a topic of the type sensor_msgs/Image", "time": "2015-02-09 03:17:57 -0600", "post_content": [" ", " ", "Hi guys,", "I'm new to ROS and struggling with understanding it so I would really appreciate your help. I have a node which publishes /ladybug_image with type sensor_msgs/Image but I need to extract the green channel of this topic. I looked at the ros documentation about sensor_msgs/Image. To me it sounds the uint8[] data contains the image data but I don't get how can I refer to each channel explicitly. Could you please help me how can I do that?"], "answer": [" ", " ", " ", " ", "hello,", "the uint8[] data is the raw image data. inorder to have individual channel you need to know the image data is structured or unstructured. it means the it is RGB[pix1] RGB[pix2]....... or R[pix1] R[pix2].........G[pix1] G[pix2].........B[pix1] B[pix2].......... once u know which type it is using a incremental loop  (for loop) you can extract whatever you want. i can give you one eg i used for reconstruction of image in the subscriber node. ", "here img is sensor_msgs::Image. \nplease tell me if its not clear.", " ", " ", "You could use ", " to convert the incoming ", " into an OpenCV datatype. This allows you to use OpenCV functions to perform operations such as accessing channels. As an additional note, you can look at the string contained in the ", " field of the ", " to determine what format the image is expressed in."], "answer_code": ["for(int i=(w-1);i>=0;i--)\n{\n    for(int j=(h-1);j>=0;j--)\n    { \n        *(image(i,j)) = *(&img->data[count]);\n        count++;\n        *(image(i,j)+1) = *(&img->data[count]);\n        count++;\n        *(image(i,j)+2) = *(&img->data[count]);\n        count++;\n    }\n}\n", "sensor_msgs/Image", "encoding", "sensor_msgs/Image"], "url": "https://answers.ros.org/question/202728/how-to-separate-channels-of-a-topic-of-the-type-sensor_msgsimage/"},
{"title": "Slam Raspberry Pi 2 * [closed]", "time": "2015-03-02 09:30:27 -0600", "post_content": [" ", " ", "The raspberry pi 2 is a much more powerful version of raspberry pi. I've installed ROS hydro on it because I had problems with ROS Indigo.", "First of all its a quadcopter with a seperate microcontroller for stabilization and position hold.", "I have an 2D RpLidar, down facing optical flow sensor, and a down facing sonar for altitude as well as an IMU.", "I've seen many slam libraries, but only one slam example was similar to my problem.", "I was wondering what slam library I could use for 3D slam?"], "answer": [], "url": "https://answers.ros.org/question/204064/slam-raspberry-pi-2/"},
{"title": "Roomba 560  \"Could not connect to Roomba\" Error in ROS (Ubuntu 12.04)", "time": "2013-05-26 15:57:41 -0600", "post_content": [" ", " ", " ", " ", "I've built an SCI-RS-232-USB cable for connecting to my Roomba using the following tutorial:", "The cable works as I tested it on my Windows machine using the RealTerm Serial Terminal. I was able to make the roomba move and beep by sending raw opcodes. However it should be noted, ", "I've tried to change the hardware and software control options in minicom to try to get a different response; no luck.  Basic troubleshooting - disconnecting, power cycling, etc.", "Any help to remedy the problem is appreciated!", "Here is the necessary info from the USB-Serial interface"], "answer": [" ", " ", "Try enabling hardware flow control. That should raise and lower the RTS pin for you from minicom.\nLet me know your results, I'm working on the same problem and have so far had no luck connecting to the roomba..."], "question_code": ["blog.makezine.com/2008/02/29/how-to-make-a-roomba-seri/\n", "dmesg | grep tty\n\n[    1.113447] tty tty34: hash matches\n[ 1142.333506] usb 2-1: FTDI USB Serial Device converter now attached to ttyUSB0\n\nls -l /dev/ttyUSB0\ncrwxrwxrwx 1 root dialout 188, 0 May 26 18:19 /dev/ttyUSB0\n", "A -    Serial Device      : /dev/ttyUSB0                              |\n    | B - Lockfile Location     : /var/lock                                 |\n    | C -   Callin Program      :                                           |\n    | D -  Callout Program      :                                           |\n    | E -    Bps/Par/Bits       : 115200 8N1                                |\n    | F - Hardware Flow Control : No                                        |\n    | G - Software Flow Control : No\n", "    rosrun roomba_500_series roomba500_light_node\n\n    [ INFO] [1369617555.945517395]: Roomba for ROS 2.01\n    [FATAL] [1369617555.969045803]: Could not connect to Roomba.\n    [FATAL] [1369617555.969251975]: BREAKPOINT HIT\n        file = /home/trevor/catkin_ws/trunk/roomba_500_series/src/roomba500_lightweight.cpp\n        line=90\n\nTrace/breakpoint trap (core dumped)\n"], "url": "https://answers.ros.org/question/63552/roomba-560-could-not-connect-to-roomba-error-in-ros-ubuntu-1204/"},
{"title": "laptop froze while running openni tracker", "time": "2015-03-15 01:45:29 -0600", "post_content": [" ", " ", "everytime i run rviz tf for skeletal tracker my laptop froze. Is it because my laptop is not powerful enough?", "What are the specs of your laptop?"], "answer": [], "url": "https://answers.ros.org/question/205018/laptop-froze-while-running-openni-tracker/"},
{"title": "Why the navdata topic doesnt show Linear z velocity in ardrone_autonomy?", "time": "2015-04-03 13:38:37 -0600", "post_content": [" ", " ", " ", " ", "Hi there,\nI am working with the ardrone_autonomy package, and all seems right, except for the detail that when I echo the /ardrone/navdata topic, all the values seem ok except the Vz, linear Z velocity value. Here is an example of what im getting:", "batteryPercent: 18.0", "state: 2", "magX: -31", "magY: -73", "magZ: 51", "pressure: 75088", "temp: 403", "wind_speed: 0.0", "wind_angle: 45.0000190735", "wind_comp_angle: -0.0", "rotX: -4.86100006104", "rotY: 0.421000003815", "rotZ: 4.75699996948", "altd: 0", "vx: 56.8887748718", "vy: 83.2262802124", "vz: -0.0", "ax: -0.0310622174293", "ay: -0.0128604527563", "az: 1.00342142582", "As you can see, Vz displays 0, and it is constant all the time. I think i saw a person with the same problem, but i couldnt find the web or question again. I hope someone can help me.", "Thanks", "It shows your altitude as 0. Was the drone flying when you captured this?", "No, for the picture it wasnt flying. But i have tried also when the drone is flying, and the altitude shows the correct value, bit the linear z velocity, dont", "You can try checking out the ", " to track down the issue, or you can differentiate the altitude data to get Vz.", "From my own experience with the AR.Drone 2.0, it never publishes data about vertical velocity (vz). I'm guessing here, but it may because it estimates its speed using the vertical camera and to estimate it's vz then it would have to the size of the objects on the ground which it can't."], "answer": [], "url": "https://answers.ros.org/question/206605/why-the-navdata-topic-doesnt-show-linear-z-velocity-in-ardrone_autonomy/"},
{"title": "Issues with RViz and Displaying IMU data", "time": "2015-02-08 12:58:21 -0600", "post_content": [" ", " ", " ", " ", "I have a bunch of bags that were recorded from sensor nodes during an experiment a month ago.  Right now I'm trying to get some sort of visualization working from the IMU data, beyond plotting the values in rqt.  Ideally some sort of visualization of the acceleration vector.", "I've been trying to use RViz to display the data, but haven't had any luck.  When I try to add a visualization by topic it shows my sensor_msgs/imu_msg message on my imu_chatter topic, but when I select it I get an error saying \"Transform [sender=unknown_publisher] For frame []: Fixed Frame [map] does not exist\".  It does have a green checkmark next to the topic.", "This is my first time using RViz, so I really don't know what's going on.  Any ideas as to what is wrong?", "I have the same situation. Did you solve the problem?", "Yes I did.  I had to go in and manually add a frame id to the bagfiles.  I wrote a quick Python script to update the bags.  It's been about a year since I've messed with any of this, so hopefully this still works.", " Here's the link to that script on github (had to shorten)  ", "Thank you very much. I will try it. \nRegards", "Hi, I was trying your code, but I got an error.. ", "Do I have to write an script to make a tf??  or Do I have to write also an script to record such bag files?", "You didn't give the script any input or output files then, input.bag (in directory you ran script from) is what it defaults if you give it no arguments."], "answer": [" ", " ", "Looks like you forgot to set a frame_id when you recorded your IMU data. If you look at the rviz \"Global Options\" (top left) you see the \"Fixed Frame\". This is the fixed frame used by rviz and all data you try to visualize has to be transformable into this fixed frame (i.e. has to be connected in the tf tree).", "I never tried this, but maybe you can make your IMU data display work by setting the fixed frame to be a empty string in rviz. If not, you'll likely want to process your bagfile data to add the missing frame_id. You can then set the the rviz \"Fixed Frame\" to the same frame_id and have your data visualized.", "That's probably the problem.  I've only started using ros recently, so I don't know entirely what I'm doing here.", "Anyway, I did try making the fixed frame field blank, but then I get Transform [sender=unknown_publisher] Unknown reason for transform failure. ", "\nHow would I go about adding a frame_id?", " You're on the right track with  ", "  :) ", "Ok, so I finally did manage to get frame ids written to the bag (wish there was better documentation for the ros cookbook), and now it does display the acceleration vector.  Thanks."], "url": "https://answers.ros.org/question/202693/issues-with-rviz-and-displaying-imu-data/"},
{"title": "dynamixel_control no motor found /dynamixel mx-64AR, usb2dynamixel", "time": "2015-04-01 09:35:04 -0600", "post_content": [" ", " ", " ", " ", "Hi guys,\nI am trying to use the new robotis servo: dynamixel MX-64AR, but I cannot find the servo both in win8 and ubuntu 14.04.\nin win8, I tried to use RoboPlus Wizard to search all the bps, nothing found...\nin Ros, I run: ", ",\nand I got:", "and I am sure of this :", "About the HW, I use usb2dynamixel, RS485 -4p interface, and the switcher is in the middle.", "the power support is a takasago ex-375l, I set output as 12.5 v, 0.4A, the motor led flashed when it's powered  on.", "And I had checked the 4p cable, all the four lines are good. ", "About the ros version;", "about the bps in the launch file, the parameter of the servo is link:", ", the default bps of mx-64ar should be 57142.... I had tried 57142, 57143, 57142.9 and 1000000, no one works.", "I am really screwed here.......................\nHEEEEEEEEEEEEELLLLLLLLLLLPPPPPPP!", "What version of dynamixel package are you using? Add to your original question the following command result for the future refenrence:"], "answer": [" ", " ", "Hi guys, finally, I went to the Robotis Shop, and they test my motor and a new mx-64ar and an AX series motor, however, only the AX motor can work. So it seems like that this batch of mx-64ar productions have a lot defectives. \nWhat's a bad luck, a defective pesters me 5 days....................", "one more thing just for reminding the followers, with the new mx-64ar I exchanged from the shop, everything works well, but the default baud of this motor is 57143.....if you want to ping it  through \"controller_manager.launch\". note that 57142 doesn't work here, although it works in RoboPlus.", " ", " ", " ", " ", " Are you aware that a counterfeit ftdi chip is being used in the usb2dynamixel?\nMaybe you are struck by this problem.\nSee\n   ", " \nfor the description and the solution.\nWorked for me,\n   Sietse ", "Thanks for your reply, but I am using mx-64ar, which works with RS485, so I am using RS485 port of USB2dynamixel, not rs232.... which motor are you using ?", "and  also,  I tried lsusb, mine is 0403:6001, so it should be something else........", "I'm using an arm from crustcrawler with ax18a's. Good luck with finding the problem."], "question_code": ["roslaunch my_dynamixel_tutorial controller_manager.launch", "SUMMARY\n========\n\nPARAMETERS\n * /dynamixel_manager/namespace: dxl_manager\n * /dynamixel_manager/serial_ports/pan_tilt_port/baud_rate: 57142\n * /dynamixel_manager/serial_ports/pan_tilt_port/max_motor_id: 25\n * /dynamixel_manager/serial_ports/pan_tilt_port/min_motor_id: 0\n * /dynamixel_manager/serial_ports/pan_tilt_port/max_motor_id: 25\n * /dynamixel_manager/serial_ports/pan_tilt_port/update_rate: 20\n * /rosdistro: indigo\n\n * /rosversion: 1.11.10\n\nNODES\n  /\n    dynamixel_manager (dynamixel_controllers/controller_manager.py)\n\nauto-starting new master\nprocess[master]: started with pid [23510]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to c01c22cc-d87a-11e4-9915-001aa0aa3777\nprocess[rosout-1]: started with pid [23523]\nstarted core service [/rosout]\nprocess[dynamixel_manager-2]: started with pid [23540]\n/opt/ros/indigo/lib/python2.7/dist-packages/dynamixel_driver/dynamixel_serial_proxy.py:92: SyntaxWarning: The publisher should be created with an explicit keyword argument 'queue_size'. Please see http://wiki.ros.org/rospy/Overview/Publishers%20and%20Subscribers for more information.\n  self.motor_states_pub = rospy.Publisher('motor_states/%s' % self.port_namespace, MotorStateList)\n/opt/ros/indigo/lib/python2.7/dist-packages/dynamixel_driver/dynamixel_serial_proxy.py:93: SyntaxWarning: The publisher should be created with an explicit keyword argument 'queue_size'. Please see http://wiki.ros.org/rospy/Overview/Publishers%20and%20Subscribers for more information.\n  self.diagnostics_pub = rospy.Publisher('/diagnostics', DiagnosticArray)\n\nINFO] [WallTime: 1427898245.997934] pan_tilt_port: Pinging motor IDs 0 through 25...\n\n[FATAL] [WallTime: 1427898248.149759] pan_tilt_port: No motors found.\n", "ls -l /dev/ttyUSB0 \n\ncrw-rw-rw- 1 root dialout 188, 0  4\u6708  1 22:51 /dev/ttyUSB0 ....\n", "rosversion dynamixel_controllers\n\n0.4.0\n\n$ dpkg -p ros-indigo-dynamixel-controllers | grep Ver\n\nVersion: 0.4.0-0trusty-20150305-1931-+0000\n", "$ rosversion dynamixel_controllers\n0.4.0\n$ dpkg -p ros-indigo-dynamixel-controllers | grep Ver\nVersion: 0.4.0-0trusty-20150327-0507-+0000\n"], "url": "https://answers.ros.org/question/206414/dynamixel_control-no-motor-found-dynamixel-mx-64ar-usb2dynamixel/"},
{"title": "unwanted rotation of the markers detected with ar_track_alvar", "time": "2015-04-27 04:18:46 -0600", "post_content": [" ", " ", " ", " ", "hello! \nI discovered something weird with ar_track_alvar but maybe I just forgot something when launching it.\nI put a mark on the head of a robot and another one one the ground, this is when it is working fine:\n", "but if i make the robot turn a little bit mor to the left, a PI rotation relative to the green axe seems to be made :", " ", "I set up a static_transform_publisher between \"/axis_camera\" and \"/map, then I set the \"output_frame\" argument of ar_track_alvar to \"/axis_camera\" and the rest of the launchfile is identical to \"pr2_indiv_no_kinect.launch\".", "The problem is not happening with the mark on the ground if i make it turn.", "If it can give a clue, here is the rotation data of the mark when my robot make a complete turn:\n", "I am grateful for any help!", "I am trying to improve camera calibrations, it might solve the problem, I will let you know !"], "answer": [" ", " ", "I am not sure about that but could it be that your camera distortion on the edges of the images and the strong skew of the markers in your case cause this misdetection? Try it again with markers facing the camera and see if the problem persists."], "url": "https://answers.ros.org/question/207993/unwanted-rotation-of-the-markers-detected-with-ar_track_alvar/"},
{"title": "How to debug odometry error?", "time": "2015-03-22 09:52:22 -0600", "post_content": [" ", " ", "I have a custom 4 wheel robot (only back wheels are powered) and a laser range finder, and I'd like to have it do SLAM and navigation.  However, after following this ", ", I found that my odometry error is way too large in both translation and rotation.  ", "I think there are three measurements that are critical (please correct me if I miss anything): ", ", ", ", and ", ". ", "So, I measured these numbers, and created an URDF for the robot (the robot itself is about 0.5x0.5m).  However, there are probably some measurement error, because even simple translation for 2m showed some error (the laser data of the room is inconsistent before and after the robot translation).  When testing the rotation, the error is a lot higher... So, I wonder if anyone could provide their experience in debugging the odometry error, so that I can get it right! Thanks in advance!"], "answer": [" ", " ", "As measuring the transform may be tricky sometimes (e.g., wide wheels, laser at locations hard to measure), the way I debug odometry in the end is to use hector_slam to get the laser pose, get the encoder counts from both wheels, and then minimize the base_link's odometry differences obtained from both sources (laser pose and encoder counts).  In this way, I can optimize several parameters at once (e.g., left wheel radius, right wheel radius, base_link width, pose of the laser relative to the base_link), and use my rough measurements as the initial values.  ", "I collect the laser pose and encoder counts by driving the robot in rectangular routes and turning both clockwise and counter-clockwise.  Below is my matlab program for such calibration optimization.", " ", " ", "Open up rviz and set your laserscan visualization to high persistence, so that you see subsequent scans overlaid on each other.", "Make sure that the transform between base_laser and base_link is as good as you can get it, as a poor transform will poison subsequent tuning.", "Then, tune your wheel circumference measurement until translational (forward/backward) odometry is as good as it's going to get, by driving the robot back and forth and making sure that the scans line up.", "Then, tune your wheel track (distance between the back wheels) by spinning the robot in place until the scans line up.", "Thanks for the suggestions! I'll do that. Also, do you know any tests that can debug each of the 3 measurements independently? I wonder how accurate it needs to be (e.g., <1mm)? Thanks again."], "answer_code": ["function calibrate_odometry(filepath, filePrefix)\n% calibrate_odometry(filepath, filePrefix)\n%\n% This program calibrates odometry and the pose of the laser relative to the\n% base_link. It expects 2 csv files parsed from your bag file.  One csv files\n% stores the pose of your laser (retrieved from hector_slam), and the other\n% stores the encoder counts of both your wheels (see initialize_index() for\n% which columns corresponds to which data).  This program will minimize the\n% differences in odometry created by both sources.  Hence, the variables it\n% will optimize are left wheel radius, right wheel radius, base_link width,\n% pose of the laser relative to the base_link (see initialize_params() and put\n% your default values there).\n%\n% filepath: path to the csv files.  \n% filePrefix: prefix of the csv files.  One csv file should be named as\n%             [prefix]_pose2D_laser.csv, and the other is \n%             [prefix]_encoder_num.csv.\n\n    global index\n\n    % default value\n    same_wheel_circ = false;\n    initialize_index();\n    x0 = initialize_params();\n\n    if same_wheel_circ\n        index.wheel_circ_right = index.wheel_circ_left;\n    end\n\n    % load data\n    [pose2D_laser, encoder_num] = load_data(filepath, filePrefix);\n\n    % make theta continuous for optimization\n    pose2D_laser(:,index.theta) =...\n        make_theta_continuous(pose2D_laser(:,index.theta));\n\n    % plot how it looks like first\n    traj_laser = create_baselink_traj_from_laser(pose2D_laser, x0);        \n    traj_encoder =...\n        create_baselink_traj_from_encoder(encoder_num, traj_laser(1,:), x0);     \n    plot_traj(traj_laser, traj_encoder, {'baselink\\_from\\_laser', 'reconstruct\\_odom'});\n    title('before calibration')\n\n    % calibrate it    \n    [x, traj_encoder, traj_laser] = calibrate_full(pose2D_laser, encoder_num, x0);  \n    plot_traj(traj_laser, traj_encoder, {'baselink\\_from\\_laser', 'reconstruct\\_odom'});\n    title('after calibration')\n\n    % print optimized number\n    print_optimized(x)\n    print_optimized_for_urdf(x)\nend\n\n\nfunction print_optimized_for_urdf(x)    \n    global index\n    param = {'wheel_base',...\n        'wheel_radius_left',...\n        'wheel_radius_right',...\n        'rplidar_pos_x',...\n        'rplidar_pos_y',...\n        'rplidar_yaw'};\n\n    v = [x(index.wheel_base)...\n        x(index.wheel_circ_left)/(2*pi)...\n        x(index.wheel_circ_right)/(2*pi)...\n        x(index.laser_dist)*cos(x(index.laser_shift_theta))...\n        x(index.laser_dist)*sin(x(index.laser_shift_theta))...\n        x(index.laser_yaw)];\n\n    value = num2cell(v);\n\n    [param' value']   \nend\n\nfunction print_optimized(x)    \n    global index\n    param = {'wheel_base',...\n        'wheel_circ_left',...\n        'wheel_circ_right',...\n        'laser_dist',...\n        'laser_shift_theta',...\n        'laser_yaw'};\n\n    v = [x(index.wheel_base)...\n        x(index.wheel_circ_left)...\n        x(index.wheel_circ_right)...\n        x(index.laser_dist)...\n        x(index.laser_shift_theta)...\n        x(index.laser_yaw)];    \n\n    value = num2cell(v);\n\n    [param' value']\nend\n\nfunction initialize_index()\n    global index\n    index = struct(...\n        ... % this row is the index for Pose2D\n        'time', 1, 'x', 2, 'y', 3, 'theta', 4, ... \n        ... % this row is the index for encoder counts\n        'leftwheel', 2, 'rightwheel', 3,... \n        ... % these two rows are the index for x, the parameters to be optimized\n        'wheel_base', 1, 'wheel_circ_left', 2, 'wheel_circ_right', 3, ...\n        'laser_dist', 4, 'laser_shift_theta', 5, 'laser_yaw', 6);    \nend\n\nfunction x0 = initialize_params()\n    wheel_base ..."], "url": "https://answers.ros.org/question/205580/how-to-debug-odometry-error/"},
{"title": "No devices connected... Waiting for devices to be connected", "time": "2014-10-27 12:36:29 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "When I start 3dsensor.launch I get:", "When I rosrun ", " by itself I get the same error.", "I've trawled rosanswers looking for a solution, but nothing's worked yet. : C\n", " lists the kinect; I've followed the suggestions ", ", and the suggestions ", ", and the suggestions ", ", and the suggestions ", ", and ", " to no avail.", "This is being run on a turtlebot2, and both the turtlebot and the workstation are on 14.04 trusty and indigo igloo. The kinect is plugged into the usb 2.0 port.", "lsusb yields: ", "Turtlebot specs are:\nUbuntu 14.04 Trusty\nMemory 1.8 G\nProcessor Intel Celeron @ 1.50Ghz\nGraphics intel ivybridge mobile\nOS type 64 bit", "I'm at a loss. does anyone have suggestions?", "I'm not entirely certain if it's related, but when I start up my minimal launch I get this:", "What do you get from lsusb? and how did you start kinect? what's your pc spec?", "I've updated the question with the contents of lsusb. ", "The kinect is listed there, as it should. I started the kinect by making sure everything is connected. All the lights are green.", "Which computer do you want the specs for, the workstation or the turtlebot?", "turtlebot's spec would be helpful"], "answer": [" ", " ", " ", " ", "It turned out the solution was to get an ", ". This resolved the connection error.", "However, I now have a minor issue with RVIZ.", "From the 3d Launch I get:", " produces numbers, and I get a point-cloud visualization.", "i've the same problem but im getting the openni drivers from apt-get install ros-indigo-openni-camera and ..-launch and executing roslaunch openni_launch openni.launch. What do you mean with \"getting an openni patch\"? Are you installing those drivers from git before indigo's openni packages?", ", That's where you should be getting them. As far as I know, openni actually doesn't support the Kinect. The patch which I linked to fixes this. I didn't have to remove and re-install anything, I just downloaded it alongside my existing stack.", "hey can you plz tell me how can i download and use \"https://github.com/avin2/SensorKinect\" or ", "\" on ros? i am new to linux and ros and i am facing the same above mentioned problem for kinect.", " ", " ", "disabling USB3.0 on BIOS should work on openni_launch, ", " ", " ", " ", " ", " ", "Experienced a very similar (or the same) issue. I've worked it around by ", ". This may not work on different environment (see a ", " for more info) though."], "question_code": ["[ INFO] [1414430559.252574176]: No devices connected.... waiting for devices to be connected\n", "Bus 002 Device 004: ID 0bda:5605 Realtek Semiconductor Corp. \nBus 002 Device 009: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\nBus 002 Device 008: ID 045e:02ad Microsoft Corp. Xbox NUI Audio\nBus 002 Device 007: ID 045e:02c2 Microsoft Corp. \nBus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 003: ID 03eb:883e Atmel Corp. \nBus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 003 Device 003: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n", "ROS_MASTER_URI=http://172.20.11.86:11311\ncore service [/rosout] found\nprocess[robot_state_publisher-1]: started with pid [12734]\nprocess[diagnostic_aggregator-2]: started with pid [12760]\nprocess[mobile_base_nodelet_manager-3]: started with pid [12835]\nprocess[mobile_base-4]: started with pid [12854]\nprocess[cmd_vel_mux-5]: started with pid [12976]\nprocess[bumper2pointcloud-6]: started with pid [13040]\nprocess[turtlebot_laptop_battery-7]: started with pid [13081]\nprocess[capability_server-8]: started with pid [13098]\nprocess[app_manager-9]: started with pid [13105]\nprocess[master-10]: started with pid [13145]\nprocess[interactions-11]: started with pid [13187]\nprocess[zeroconf/zeroconf-12]: started with pid [13232]\n[ INFO] [1414430339.909748537]: Zeroconf: service successfully established [turtlebot][_ros-master._tcp][11311]\n/opt/ros/indigo/lib/python2.7/dist-packages/bondpy/bondpy.py:114: SyntaxWarning: The publisher should be created with an explicit keyword argument 'queue_size'. Please see http://wiki.ros.org/rospy/Overview/Publishers%20and%20Subscribers for more information.\n  self.pub = rospy.Publisher(self.topic, Status)\n/opt/ros/indigo/lib/python2.7/dist-packages/bondpy/bondpy.py:114: SyntaxWarning: The publisher should be created with an explicit keyword argument 'queue_size'. Please see http://wiki.ros.org/rospy/Overview/Publishers%20and%20Subscribers for more information.\n  self.pub = rospy.Publisher(self.topic, Status)\n[WARN] [WallTime: 1414430344.195425] Rapp Manager : No preferred rapp for 'turtlebot_rapps ..."], "answer_code": ["[ WARN] [1414934636.270421799]: Camera calibration file /home/turtlebot/.ros/camera_info/depth_0000000000000000.yaml not found. \n[ WARN] [1414934636.270594235]: Using default parameters for IR camera calibration.\n", "roslaunch freenect_launch freenect.launch", "dpkg -p ros-indigo-openni-launch ros-indigo-freenect-launch | grep Vers\nVersion: 1.9.5-0trusty-20141231-0013-+0000\nVersion: 0.4.0-0trusty-20150131-0944-+0000\n"], "url": "https://answers.ros.org/question/196035/no-devices-connected-waiting-for-devices-to-be-connected/"},
{"title": "robot_localization with GPS + IMU", "time": "2015-04-14 06:11:57 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to setup a localization system for my aerial vehicle, however I never used robot_localization or the TF package, so everything seems a little overwhelming, setup wise. I have a couple of questions:", "1) Is it possible to use robot_localization with only GPS and IMU? Would the output be any good?", "2) I have a bottom camera that is used from ground target detection. Should I use it for some kind of visual odometry? I'm afraid that this will put the processor under too much strain (odroid X2).", "Can I get some tips on how to setup all of this? I find myself a little lost.", "EDIT1: \nJust for some context, my system is an aerial vehicle supported by a balloon that uses two rotors in order to adjust its position:", "(1-baloon; 5-rotors; 4-bottom camera (webcam); 3-main \"box\" with GPS, IMU, odroid X2, etc)", "I had already changed the IMU output to be as specified in REP-103. \nInitially I was using \"udm_odometry_node\" from gps_common, but switched to the \"navsat_transforme_node\".\nFor the altitude I'm using the GPS, since the IMU I'm using doesn't have altitude information.", "The bottom camera (simple webcam) is meant for ground target detection/tracking, however I could use it for some kind of visual odometry if the odroid is able to handle it.", "I think I was able to setup everything correctly since I'm able to \"echo\" the output on odometry/filtered. However it seems like it has a lot of noise. I still have to do some real tests, since I only tested this with my laptop and the sensors were stationary. (Note: Are the velocities in Twist set as m/s?)", "However I got an error when trying to set the process_noise_covariance (type 7 if I recall correctly). Since I was using the covariance from the template launch file, I just deleted it in order to check if everything was working (if I was able to get an output).", "EDIT2: (For some reason I'm unable to comment your answer. Whenever I write a comment it disappears when I refresh the page)", "One of the problems might be connected to the fact that all covariances on the IMU message are set to 0, since I wasn't able to find information about those parameters for the mpu9150. Besides that, the IMU might need some new calibration.", "Sample of the IMU msg:", "Thanks, I'll check out the bags when I get a chance.", "Question: as this is a UAV, does the robot often fly in a direction other than the direction it's facing? In other words, is its course over ground different from its heading/yaw? Even though the direction of travel in your GPS data is west-to-east, the IMU reads about -0.99 radians.", "Also, can you add a cheap barometer to this thing? You should really have an altitude estimate of some kind. I have a launch file that worked for me (with the latest source), and I will post it when you answer these questions.", "The bagfile is from a test I did with only the GPS and IMU. That is, it was not the UAV that performed the path. I performed the path with the laptop, GPS and IMU while mantaining the imu in a position similar to the one it would be in the system. Notice that this IMU has no magnometer information.", "The GPS altitude information is really bad, so I will have to add a barometer to the system. By the end of this month I might have a new IMU with barometer, which should be enough, right?\nAnother question, is there a way to get a better estimate of linear velocity? with the GPS alone it's too noisy", "If you don't have a world-referenced yaw/heading estimate, ", " will not perform correctly. You need a magnetometer. You could always try pointing a camera downward and using optical flow + altitude to determine linear velocity.", "The IMU I'm using has a magnetometer, however I decided not to use it since it seemed to be sensible to electrical noise (from other hardware around the IMU). The use of the downward camera and optical flow was my first idea, however I'm not sure if it will be too demanding for the odroid.", "Can you calibrate the IMU for hard and soft iron distortion? Also, if you downsampled the image data enough, you could probably get by with optical flow on an Odroid. I'm not sure how powerful the X2 is (I have a U3)."], "answer": [" ", " ", " ", " ", "It is possible, certainly. The quality of the output will be heavily dependent on the input, however.", "Have you read through the ", "? I realize it's a lot to take in, but if you had some feedback as to which parts could use some clarification, I'd be happy to update them. ", "One thing to be aware of with your IMU is that the state estimation nodes in ", " assume that the IMU data is in the ENU frame, and IMUs commonly report data in NED. What you really want is for the signs of your rotation angles to ", ". ", "Just an FYI: if you integrate GPS data, your position estimate will be subject to discrete jumps. The accelerometers will smooth that out a little bit, but they won't eliminate it. ", "Do you have an altimeter, or are you going to use the GPS altitude?", "Finally, if you can find a lightweight ARM-friendly visual odometry node, then yes, having more data to fuse is always better. I haven't had much luck on that front, however, as many of the packages are optimized for x86/x86-64 CPUs.", "EDIT 1 (in response to EDIT 1 above): Which version of the software are you using? I'll look into the template covariance issue. It used to be that all the values of the covariance matrix had to have decimal points, but in the latest version (not yet released), it can take values without decimal points.", "When you say \"it seems like it has a lot of noise,\" can you quantify the error/noise you're seeing? If you can, it would be very helpful if you could post (1) a sample message for each input to ", ", and (2) your launch file for ", ". ", "EDIT 2 (in reponse to edits 2 and 3 above): ", "First, your rectangular plot looks like it has 90-degree angles in it to me. Make sure you do ", " when you plot things in MATLAB. Remember: the ", " data will not align perfectly with your X and Y axes when you plot it. It's going to be based on whatever yaw value the IMU had when it received the GPS data. In other words, the first MATLAB plot looks solid to me.", "Second, I'm a bit confused as to your setup. What is your ", " topic (the ", " input source)? I don't see you feeding the ", " topic back into the ", ", which is how it's intended to be used.", "Also, can you post either a bag file of your run, or perhaps a printout of the ", " topic?", "That might actually be the problem! I don't have an odometry measure and I should be feeding the /odometry/gps back to the ekf_localization_node in odom0. For some reason I didn't save the changes and didn't noticed it was like this until you pointed it out. Just to make sure is the config correct?", "You have ", " set to true, but you are trying to fuze Z position from ", ", as well as roll and pitch (and their velocities) and Z acceleration from your IMU. If you want to use 3D data, set ", " to false. Also, I would just advise that you double-check your IMU data's signs.", "Thanks for all the help Tom. I have been doing some more tests and one doubt that I have is connected to the \"imuN_remove_gravitational_acceleration\". When should it be 'true'? If I want the acceleration due to gravity to be removed by filter or if that acceleration is already removed?", "I already found the answer after some small tests. We need to set the parameter to true if we want the acceleration to be removed. On a different note, do you want me to send you a bagfile of the run? Do you have any advice in order to reduce the noisy linear velocity I obtain?", "Right, ", " means it will remove acceleration due to gravity.", "Be careful using linear acceleration, though: when you only have a GPS and IMU, if your IMU has biases in the acceleration data, it's going to start to run away. Sure, post a bag, and I'll look at it when I can.", "Posted the bag as an edit to the main question. Another thing I noticed is that i'm only feeding the gps to the navsat_transform_node, however from the tutorials it seems like the node requires 3 inputs (gps, imu, odometry).", " have all of those inputs. However, if you are getting output on your ", " topic, then it's working. The IMU topic defaults to ", ", and the odometry topic defaults to ", ", so perhaps you just happen to have those same topic names?", "That must be it."], "question_code": ["---\nheader: \n  seq: 14983\n  stamp: \n    secs: 1429104187\n    nsecs: 410562312\n  frame_id: imu\n  orientation: \n   x: 0.00824941202457\n   y: 0.0174910227596\n   z: 0.0350623021512\n   w: 0.997508466348\n orientation_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n angular_velocity: \n   x: -0.0017719756579\n   y: 0.00648836186156\n   z: 0.000426339596743\n angular_velocity_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n linear_acceleration: \n   x: -0.210282728076\n   y: 0.174357444048\n   z: 9.848320961\n linear_acceleration_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0 ...", "navsat_transform_node"], "answer_code": ["robot_localization", "ekf_localization_node", "ekf_localization_node", "axis equal", "/odometry/gps", "odom", "odom0", "/odometry/gps", "ekf_localization_node", "/odometry/filtered", "two_d_mode", "odom0", "two_d_mode", "navsat_transform_node", "/odometry/gps", "/imu/data", "/odometry/filtered"], "url": "https://answers.ros.org/question/207117/robot_localization-with-gps-imu/"},
{"title": "How to hide joint axis in Rviz", "time": "2015-04-26 22:03:22 -0600", "post_content": [" ", " ", "How do you hide the joint axis in Rviz, or at least make them translucent or smaller so they don't hide your model?", "I have a model consisting of a box with a single thin \"leg\", but the joint axis indicators for the leg are so big, they're obscuring the leg.", "The default leg position puts it right where the green cylinder is, which, as you can see, completely obscures it."], "answer": [" ", " ", "The axes you're trying to hide are part of the TF display.", "You can either disable the TF display completely, or expand the TF display properties and adjust the settings. It has a number of different options for changing the axes size, hiding and showing axes and the connecting arrows, adjusting the transparency, and and hiding and showing axes for individual frames.", "Thanks. Digging into the urdf_tutorial's code, apparently that's all saved in a *.rviz file, so I can just copy that into my own package and customize the settings so I don't have to manually change them every time I load Rviz.", "Hi,\nI'm using RViz 1.11.10 from Jade and I'm editing the config file .rviz to hide some of the axes, however everytime I restart RViz all the axes are visible. All the other settings are maintained.\nAny idea?", "Francesco"], "question_code": ["<?xml version=\"1.0\"?>\n<robot name=\"myrobot\"\n    xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n\n    <!-- Included URDF/XACRO Files -->\n    <xacro:include filename=\"$(find myrobot_description)/urdf/materials.urdf.xacro\" />\n\n    <!-- width in meters -->\n    <property name=\"torso_x_size\" value=\"0.1\" />\n\n    <!-- length in meters -->\n    <property name=\"torso_y_size\" value=\"0.205\" />\n\n    <!-- height in meters -->\n    <property name=\"torso_z_size\" value=\"0.03\" />\n\n    <!-- torso mass (not including legs) in kg -->\n    <property name=\"torso_mass\" value=\"0.920\" />\n\n    <!-- mass of a single upper leg piece in kg -->\n    <property name=\"upper_leg_mass\" value=\"0.010\" />\n\n    <link name=\"base_link\">\n        <visual>\n            <geometry>\n                <box size=\"${torso_x_size} ${torso_y_size} ${torso_z_size}\" />\n            </geometry>\n            <material name=\"red\"/>\n        </visual>\n        <xacro:default_inertial mass=\"${torso_mass}\"/>\n    </link>\n    <xacro:macro name=\"leg\" params=\"name reflect_x reflect_y\">\n        <link name=\"${name}\">\n            <visual>\n                <geometry>\n                    <box size=\"0.003 0.04 0.005\" />\n                </geometry>\n                <origin xyz=\"0 0.04 0\" rpy=\"0 0 0\"/>\n                <material name=\"blue\" />\n            </visual>\n            <collision>\n                <geometry>\n                    <box size=\"0.003 0.04 0.005\" />\n                </geometry>\n                <origin xyz=\"0 0.04 0\" rpy=\"0 0 0\"/>\n            </collision>\n            <xacro:default_inertial mass=\"${upper_leg_mass}\"/>\n        </link>\n        <joint name=\"torso_to_${name}\" type=\"fixed\">\n            <parent link=\"base_link\"/>\n            <child link=\"${name}\"/>\n            <origin xyz=\"${torso_x_size/2*reflect_x} ${torso_y_size/2*reflect_y} 0\" />\n        </joint>\n    </xacro:macro>\n    <xacro:leg name=\"front_right_leg\" reflect_x=\"1\" reflect_y=\"1\" />\n</robot>\n"], "url": "https://answers.ros.org/question/207964/how-to-hide-joint-axis-in-rviz/"},
{"title": "Cannot run the mapping tutorial [closed]", "time": "2015-04-25 05:29:13 -0600", "post_content": [" ", " ", "Hi,", "Maybe my question is a bit dump because I am very new to hector slam and even to ROS. I have installed ROS and hector slam (indigo). When I was trying to do the tutorial, after I typed \n\"roslaunch hector_slam_launch tutorial.launch\"\nand press enter, Rviz started fine with no error. I can rotate the grid and saw Global Status is in red text (no data feeding into Rviz).", "Then I typed \n\"rosbag play Team_Hector_MappingBox_RoboCup_2011_Rescue_Arena.bag  --clock\"\nin another console and there is no error shown. I could see the [RUNNING] line being showed and I guess the data is being fed to Rviz?", "However, there is nothing shown in Rviz, I can still rotate the grid but there is no map being created. Also, I saw the Global Status became green and there was a check sign. I could also see the \"ROS Elapsed\" at the bottom is running.", "When I added a TF, I could see two coordinates moving but there is just no map.", "Can anyone help me on this please?\nThanks", "If this is a duplicate of ", ", please close this question.", "Closed. Thanks for the reminder."], "answer": [], "url": "https://answers.ros.org/question/207871/cannot-run-the-mapping-tutorial/"},
{"title": "Unable to view camera output while using cv_bridge", "time": "2015-04-09 13:06:17 -0600", "post_content": [" ", " ", " ", " ", "I am using ROS Hydro in Ubuntu 12.04 which is installed in Virtualbox. I am trying to use cv_bridge to convert my ros images to opencv images captured by Logitech C270.", "Being a novice to this field, I am following the tutorial explained on this blog : ", ".", "All works fine but still I am not able to see the camera output, rather the complete output screen appears green.", "Same is the case while following the ROS tutorial, given on the link : ", "Guys I am in need of help...please help me...!!!", "some more or better info, like, what commands are you running to see an image? can you normaly see the image?etc. etc.", "Firstly thanks for your reply. Yes sure.\nInitially I am using the command  - roslaunch uvcCameraLaunch.launch  ... this works fine and my camera gets light up.\nAfter that i am running the command  - rosrun image_view image_view image:=/camera/image_raw.. this pops up a window named /camera/image_raw", "This window /camera/image_raw has the screen which is completely green rather than seeing the display of camera.", "check with \"rostopic list\" after uvcCameraLaunch.launch which image topics also exist and try another with \"rosrun image_viewer...\"  ", "else could you see with a simply webcam app a normal picture or also a green one ?", "Yes i am able to view properly the webcam output with webcam app.\nThe output of rostopic list is:\n/camera/camera_info ", "\n/camera/image_raw ", "\n/rosout ", "\n/rosout_agg", " the page  ", "  says that one should use  ", " , tried that? ", "OH thank you very much for giving me an idea that there might be some issue with package. The uvc_camera package was having some problem. So i used the package usb_cam and it worked."], "answer": [" ", " ", "Use package usb_cam instead of uvc_camera. "], "url": "https://answers.ros.org/question/206971/unable-to-view-camera-output-while-using-cv_bridge/"},
{"title": "Nothing shown in rviz when doing hector_slam mapping tutorial", "time": "2015-04-25 05:31:52 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "Maybe my question is a bit dump because I am very new to hector slam and even to ROS. I have installed ROS and hector slam (indigo). When I was trying to do the tutorial, after I typed \n", "\nand press enter, Rviz started fine with no error. I can rotate the grid and saw Global Status is in red text (no data feeding into Rviz).", "Then I typed ", "\nin another console and there is no error shown. I could see the [RUNNING] line being showed and I guess the data is being fed to Rviz?", "However, there is nothing shown in Rviz, I can still rotate the grid but there is no map being created. Also, I saw the Global Status became green and there was a check sign. I could also see the \"ROS Elapsed\" at the bottom is running.", "When I added a TF, I could see two coordinates moving but there is just no map.", "I saved the file to an alternative path (when I clicked save it said the file cannot be open, so I have to save a copy to another path). The content of the file is shown below:"], "answer": [" ", " ", " ", " ", "Have you checked all the Displays are correctly setup or subscribed to the proper topic??", "Global Options -- map (according to your file, you already have this one)", "Map -- /map (add using Topic and add /map)", "Those should be the more important ones for map I think.", "Best", "Hi, thanks for replying. ", "The default setting in Global Options is 'map' which I assume is the correct one? However there is nothing shown on the grid. ", "Sorry I don't understand what 'Map -- /map (add using Topic and add /map)' mean (I am a newbie in ROS). Could you elaborate a bit more?", "Thanks.", "In the rviz window, in the left part you see Displays. Below of that you find the Global options, which you already have correctly. Below of that, you find an Add button, click it. In the new window, choose by topic and choose map. \nHope it works. Let us know.", "Ok it worked! Thanks a lot! But when I added a map by types, there is no map showing. What is the difference between topics and types?", "When you add by topic, it automatically selects the information to be displayed. When you add by types, in the Display menu, you have to choose among the different topic that publishes to map (which in your case should only be 1). If it works now, mark the question as answered please."], "question_code": ["roslaunch hector_slam_launch tutorial.launch", "rosbag play Team_Hector_MappingBox_RoboCup_2011_Rescue_Arena.bag  --clock", "Panels:\n  - Class: rviz/Displays\n    Help Height: 78\n    Name: Displays\n    Property Tree Widget:\n      Expanded:\n        - /Global Options1\n        - /Status1\n      Splitter Ratio: 0.5\n    Tree Height: 565\n  - Class: rviz/Selection\n    Name: Selection\n  - Class: rviz/Tool Properties\n    Expanded:\n      - /2D Pose Estimate1\n      - /2D Nav Goal1\n      - /Publish Point1\n    Name: Tool Properties\n    Splitter Ratio: 0.588679\n  - Class: rviz/Views\n    Expanded:\n      - /Current View1\n    Name: Views\n    Splitter Ratio: 0.5\n  - Class: rviz/Time\n    Experimental: false\n    Name: Time\n    SyncMode: 0\n    SyncSource: \"\"\nVisualization Manager:\n  Class: \"\"\n  Displays:\n    - Alpha: 0.5\n      Cell Size: 1\n      Class: rviz/Grid\n      Color: 160; 160; 164\n      Enabled: true\n      Line Style:\n        Line Width: 0.03\n        Value: Lines\n      Name: Grid\n      Normal Cell Count: 0\n      Offset:\n        X: 0\n        Y: 0\n        Z: 0\n      Plane: XY\n      Plane Cell Count: 10\n      Reference Frame: <Fixed Frame>\n      Value: true\n    - Angle Tolerance: 0.1\n      Class: rviz/Odometry\n      Color: 255; 25; 0\n      Enabled: true\n      Keep: 100\n      Length: 1\n      Name: Odometry\n      Position Tolerance: 0.1\n      Topic: \"\"\n      Value: true\n    - Class: rviz/TF\n      Enabled: false\n      Frame Timeout: 15\n      Frames:\n        All Enabled: true\n      Marker Scale: 1\n      Name: TF\n      Show Arrows: true\n      Show Axes: true\n      Show Names: true\n      Tree:\n        {}\n      Update Interval: 0\n      Value: false\n  Enabled: true\n  Global Options:\n    Background Color: 48; 48; 48\n    Fixed Frame: map\n    Frame Rate: 30\n  Name: root\n  Tools:\n    - Class: rviz/Interact\n      Hide Inactive Objects: true\n    - Class: rviz/MoveCamera\n    - Class: rviz/Select\n    - Class: rviz/FocusCamera\n    - Class: rviz/Measure\n    - Class: rviz/SetInitialPose\n      Topic: /initialpose\n    - Class: rviz/SetGoal\n      Topic: /move_base_simple/goal\n    - Class: rviz/PublishPoint\n      Single click: true\n      Topic: /clicked_point\n  Value: true\n  Views:\n    Current:\n      Class: rviz/Orbit\n      Distance: 16.2326\n      Enable Stereo Rendering:\n        Stereo Eye Separation: 0.06\n        Stereo Focal Distance: 1\n        Swap Stereo Eyes: false\n        Value: false\n      Focal Point:\n        X: 0\n        Y: 0\n        Z: 0\n      Name: Current View\n      Near Clip Distance: 0.01\n      Pitch: 0.350399\n      Target Frame: <Fixed Frame>\n      Value: Orbit (rviz)\n      Yaw: 0.425397\n    Saved: ~\nWindow ..."], "url": "https://answers.ros.org/question/207872/nothing-shown-in-rviz-when-doing-hector_slam-mapping-tutorial/"},
{"title": "Kinect stops when moving motor", "time": "2015-04-23 11:25:07 -0600", "post_content": [" ", " ", "Hello,", "I'm using Ubuntu 14.04 and ROS indigo.", "Also, I'm using pointcloud_to_laserscan package for the kinect and it's working fine.", "The problem is whenever I send a command to run the motors, the kinect stops and the laptop cannot detect that the kinect is connected. ", "NOTE: The kinect and the motor controller are connected through 2 USB ports.", "I think the problem is in the laptop, because I tried another laptop and they work fine.", "Unfortunately, I can't use the good laptop due to RAM problem. ", "Any idea??", "Sounds like an EMI problem. Where does the motor and where does Kinect get the power from? Is the Laptop connected to the same power source (battery). Whats the whole setup?", " The motors get the power from 24 volt battery, where the kinect is connected to AC. The laptop uses its own battery."], "answer": [" ", " ", "Actually I think it could really be some EMI thing.\nThat the motor interferes with the Kinect or the USB communication.\nI once did a very simple microcontroller board for a fellow student,\nit should just turn on the motor depending on input states, but the motor created\nhuge emissions so that when you put on an oscilloscope it showed a huge spike without\nconnection to the motor when the motor was running. Also when it was running, it never stopped, \nexcept someone touched the microcontroller package with his finger.", "To get rid of that, an laboratory assistant told us to solder capacitors (100nF) to the motor pins and connect them with the case. After that it was gone. The motor was a simple brushed DC motor. Maybe you could try something similar, if it is a DC motor."], "url": "https://answers.ros.org/question/207710/kinect-stops-when-moving-motor/"},
{"title": "Difference between Gazebo & Moveit!!!", "time": "2015-05-08 08:39:35 -0600", "post_content": [" ", " ", "I'm planning to do do motion planning for a quadcopter but I'm still confused concerning the software that I should use.\nCould someone explain to me the difference between gazebo and ROS?  The only explanation I found was:", "\"Gazebo is a 3D simulator, while ROS serves as the interface for the robot. Combining both results in a powerful robot simulator.\"\nBut it is still not clear for me!", "I would like also to know which one is more suitable for motion planning of a quad among the following list: (Gazebo, Ros moveit, or V-rep or may be matlab ), I'm sorry, but I'm really confused, and there were no enough information to be able to distinguish the difference between each one of them. ", "So I would like to know what is the difference between the ones mentioned above and for what reasons would one want to use gazebo over ROS or V-rep and vice versa?", "I would really appreciate any clarification about the subject!\nThank you"], "answer": [" ", " ", "These are NOT replacements for each other. Gazebo, ROS, and MoveIt serve completely different purposes.", "Gazebo is a simulator, it doesn't do any motion planning.\nROS is (in this context) the middleware that allows gazebo to talk to other software.\nMoveIt is a motion planning framework that uses ROS to talk to Gazebo.", "If you want to do motion planning, use moveit.", "If you want to test these motion plans without an actual robot, replace that robot with a simulated one in gazebo. ", "Thank you very much for your answer that was helpful!", "which will be better simulator gazebo or V Rep for manipulators ?", " please don't use ", " section to ask a new question (questions in comment section don't even get much attention)."], "answer_code": ["comment"], "url": "https://answers.ros.org/question/208851/difference-between-gazebo-moveit/"},
{"title": "KeyError:'laptop_battery_state'", "time": "2015-06-04 08:19:00 -0600", "post_content": [" ", " ", "Hey guys. I'm working on the migration of ROS hydro in turtlebot 2 to ROS Indigo. I have a problem with the kobuki_dashboard.I can see the kobuki battery but not the laptop one. and when i do a \"rostopic echo /turtlebot_../diagnostics_agg\" , i can't see the LapTop Battery key in the /Power System . \nin the logfile it gave this error: ", "if\n  power_state['laptop_battery_state'] !=\n  'OK': KeyError: 'laptop_battery_state'", "Any help,suggestions would be welcomed.", "Thank You !"], "answer": [], "url": "https://answers.ros.org/question/210592/keyerrorlaptop_battery_state/"},
{"title": "Timing problems in rviz in case of message frequency mismatch", "time": "2015-06-29 04:33:27 -0600", "post_content": [" ", " ", "Hi all,", "I have a problem with visualizing data in rviz in case there is frequency mismatch between LaserScans and the corresponding transfrom.", "My LaserScans arrive with a high frequency (ca. 50 Hz) while the frame transform to the global frame needs to be computed first and have a lower frequency. The rviz shows the information that the transform from the LaserScan frame to global frame is missing.\nWith a static transform I discovered that everything is working fine if the transform has higher frequency then my LaserScans. In case it has the same frequency the rviz output flickers from green to red and in case of a lower frequency it stays red with the \"no transform\" error output.", "Is there any way to tell rviz to take an \"older\" transform?", "Thanks a lot!", "Can you create a ", " file for the both successful and non-successful situations and share it somewhere?", " Not sure if it's the same issue but may seem related  "], "answer": [], "question_code": [".bag"], "url": "https://answers.ros.org/question/212426/timing-problems-in-rviz-in-case-of-message-frequency-mismatch/"},
{"title": "Best practices for organizing a project [closed]", "time": "2015-06-10 19:09:30 -0600", "post_content": [" ", " ", "Hello all,", "Sorry for the long winded explanation but my thoughts are a bit scattered on this.", "I'm fairly new to ROS. I've been working with it for about a month and I feel I'm beginning to understand most of the basics (packages, topics, subscriptions, services, etc), but when it comes to how to put together a practical project I'm struggling. I'm using ROS for my senior design project, which is a mobile autonomous robot with some mapping and computer vision capabilities. The robot is going to have several well-defined modes of operation where some subsystems should be active or inactive. ", "We're using a Kinect and converting the depth cloud to a laserscan with the depthcloud_to_laserscan node. I've already got this working with HectorSLAM as a preliminary mapping system since the robot isn't built yet and I don't have access to physical odometry. Here are examples of a few of the operation modes:", "What's the best way to initialize/start this system and switch between these states? I don't want all the subsystems to be active at the same time because power and processing will be important. For prototyping, I want to be able to type commands into the terminal to switch between each of these systems, launching all of the relevant nodes and closing them cleanly when the procedures are complete or are interrupted.", "I suppose this is more of a design philosophy question, I'm sorry if it's not specific enough. I've read through most of the tutorial content and so far I haven't found anything that quite answers my question. ", "All replies welcome, thanks."], "answer": [" ", " ", "For the first revision of your project, I think launch files are appropriate; have a look at ", "For my robot, I have a base launch file which runs the driver for my base and my sensors, and the kalman filter that I use to estimate position. All of my modes use this functionality.", "For each mode, I have an additional launch file which runs the nodes for that mode.", "Launch files can include one another as well, so if you have some functionality that is used by some modes, but not all, you can separate out those nodes into their own launch file, and include that launch file within each mode that needs it.", "For even more things that you can do with roslaunch, have a look at the ", "Thanks for the helpful suggestions. Do you have any advice for how to stop or shut down nodes cleanly? Would it be sufficient to just put a system call in the nodes to kill themselves when a particular condition is met?", "So I started delving deeper into the rospy API and found this: ", "If anyone else has the same question your answer is at the bottom: rospy.signal_shutdown(reason)", "When you stop a roslaunch, it will stop all of the nodes that it started; I usually use that instead of an explicit shutdown within the node.", "The ", " is a more advanced system which can manage dependencies and startup and shutdown of launch files, but it also requires more setup."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Autonomous Mapping Mode: Sonar/tactile based obstacle avoidance system + SLAM system", "Manual Mapping Mode: Teleoperation system + SLAM system", "Autonomous Patrol Mode: Sonar/tactile based obstacle avoidance system + Map-based navigation system + Motion    detection system"], "url": "https://answers.ros.org/question/211045/best-practices-for-organizing-a-project/"},
{"title": "Problems with costmap and the inflation layer [closed]", "time": "2015-06-15 05:08:12 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "since english isn't my first language, I hope I can explain my problem well enough and I am thanking you in advance for slogging through this mess of a text.", "I'm kind of new to ROS and was setting up the navigation stack as a part of my bachelor thesis. I followed this ", ". I'm using v-rep to simulate and rviz to visualize. I properly configured all necessary publishers (odom, tf, etc.).", "In my understanding, the ", " exists for keeping the robot out of range of obstacles. I adjusted the inflation_radius and the cost_scaling_factor parameters to my liking, but here lies the problem. My robot seems to ignore the gobal path (green line), and tries to make a very sharp turn around the inscribed area, causing the center of the robot to enter the inscribed area as soon as the costmap moves itself a little bit (for accuracity reasons I believe) and stopping the navigation. What am I doing wrong? Why is the robot ignoring the global plan that acknowledges the costmap values and doing something different?", "Thanks in advance for your answers.", "Best Regards,\nGeorg Schuppe", "EDIT: Solved. I learned about the parameters pdist_scale and  occdist_scale. With proper configuration, I could solve my problem.", "Ubuntu 14.04\nROS indigo\nnewest versions of rviz and vrep (updated 15.06.2015)", "costmap_common_params.yaml", "base_local_planner_params.yaml", "local_costmap_params.yaml", "global_costmap_params.yaml", "What does the local costmap look like?", "Sorry for answering so late, I solved the problem by myself. The local planner was simply not impressed enough about the local costmap, there is a parameter in base_local_planner_params.yaml that solves exactly that: occdist_scale. In summary: global plan, was good, local plan messed it up."], "answer": [], "question_code": ["obstacle_range: 4.5\nraytrace_range: 5.0\n\nfootprint: [[0.52, 0.32],\n            [0.52, 0.028],\n            [0.62, 0.028],\n            [0.62, -0.028],\n            [0.52, -0.028],\n            [0.52, -0.32],\n            [-0.54, -0.32],\n            [-0.54, 0.32]]\n\ninflation_radius: 1.0\ncost_scaling_factor: 0.5\n\nobservation_sources: base_scan\n\nbase_scan: {sensor_frame: base_front_link,\n            data_type: LaserScan,\n            topic: /vrep/front_scan,\n            expected_update_rate: 0.2,\n            observation_persistence: 0.0,\n            marking: true,\n            clearing: true,\n            min_obstacle_height: -0.10,\n            max_obstacle_height: 2.0}\n", "TrajectoryPlannerROS:\n  max_vel_x: 0.45\n  min_vel_x: 0.1\n  max_rotational_vel: 1.0\n  min_in_place_rotational_vel: 0.4\n\n  acc_lim_th: 3.2\n  acc_lim_x: 2.5\n  acc_lim_y: 2.5\n\n  holonomic_robot: true\n\n  xy_goal_tolerance: 0.3\n  yaw_goal_tolerance: 0.3\n", "local_costmap:\n  global_frame: odom\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 20.0\n  height: 20.0\n  resolution: 0.05\n", "global_costmap:\n  global_frame: /map\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  static_map: true\n  width: 40.0\n  height: 40.0\n"], "url": "https://answers.ros.org/question/211322/problems-with-costmap-and-the-inflation-layer/"},
{"title": "ROS2: content-based topic subscriptions?", "time": "2015-05-26 10:39:44 -0600", "post_content": [" ", " ", " ", " ", "Seeing as ROS2 will be using DDS as its middleware of choice, I was wondering if it would also support content filtered topic subscriptions next to the normal (ROS1) topic subscriptions.", "Being able to filter messages on topics by content is a very powerful capability (especially with writer-side filtering), making bus-like topologies much more feasible in ROS applications without requiring each node to implement some kind of filtering mechanism itself.", "Is this something that will be supported through the 'standard' ROS2-DDS API (I did not find anything in the ", ", but perhaps I've missed it), or will this require node implementors to bypass the API and reach into the underlying DDS implementation (if possible)?", "The RTI community site has an example ", ".", "I haven't heard any discussion about this. You may want to bring it up on the ros 2 sig mailing list."], "answer": [" ", " ", "For future reference, this moved to the ", " list, specifically here: ", ".", "Following was the answer by Dirk Thomas:", "content-based subscriptions are a\n  really interesting feature.", "The current ROS 2 prototype only uses it internally to implement request/reply on top of publish/subscribe in OpenSplice (", ").", "The current RMW interface does not offer an API for specifying a filter criteria though. But since the feature would be pretty useful it might be added in the future to allow custom filters as well.", "We would need to ensure that it is possible to specify the filter criteria in a vendor agnostic way."], "answer_code": ["ros-sig-ng-ros"], "url": "https://answers.ros.org/question/209935/ros2-content-based-topic-subscriptions/"},
{"title": "Does ROS Jade support Turtlebot? [closed]", "time": "2015-06-03 07:48:17 -0600", "post_content": [" ", " ", "I'd like to upgrade to ROS Jade to beef up my simulation power with the new form of Gazebo, but I'd also like to be able to control my ", ".  I know the last officially supported release for Turtlebot is Indigo, but is there any way I can get the best of both worlds without having to install both ROS Jade and ROS Indigo?", "Alternatively, I guess just ", " would work, but I'd rather keep things simple!"], "answer": [], "url": "https://answers.ros.org/question/210502/does-ros-jade-support-turtlebot/"},
{"title": "GPS based Autonomous car", "time": "2015-06-09 23:11:40 -0600", "post_content": [" ", " ", "Hi,", "I am very new to this field. My first target is to make a self driven car which only drives itself based on GPS.", "I have seen some nodes in ROS which can help in moving from one point to another point in a straight line if the source and destination co-ordinates are given. To add turns, one way is to give detailed way-points and smooth out the turns. Is there any better way?", "My another questions is, map provider like Google provides maps as images and uses client libraries to know and draw your location on it. How can I get the route from one source to destination, in form of a series of continuous lat-lang pair, so that I can feed it to ROS node to move the vehicle?", "=> I believe, there is some better approach to do it. Please help me by giving some pointers to existing methods.", "Hi, I am also working on autonomous car. As I am new to ROS, I am finding it difficult to find any means to move the car from point A to point B using GPS. As you mentioned in your first point, I guess you have atleast figured out how to move the car from one point to another. Pls let me know...", "I know this question was post over a year ago but I am curious to find out why you would not use something like PolySync to do this , isn't that specifically designed for Autonomous cars?", "What is PolySync?", "Take a look at polysync.io , they have powerful APIs and tools for Autonomous cars , let me know if that is the case. Thx"], "answer": [" ", " ", "There might be simpler first projects than self driving cars :-)", "I'd recommend starting by reading the journal papers written by the DARPA Urban Challenge competitors. A good one to start with is ", "Then just work backwards from there. A small matter of learning state of the art planning, controls, and perception.", "Thank you for your reply. Can you please point me to some material where I can find the solution of my problem?"], "url": "https://answers.ros.org/question/210946/gps-based-autonomous-car/"},
{"title": "detect qrcode with zbar_ros", "time": "2015-05-16 04:19:53 -0600", "post_content": [" ", " ", "I could use zbar_ros with a webcamera to detect QRcode successfully.\nBut when I change the camera ,and add ", ".", "I can't start zbar_ros successfully.", "The detail indormation is here.", "Hi, wsAndy. I wonder how can I use these codes. Do I need another QR code reader to help detect QR code? I am almost a green hand here. Any suggestion will be appreciated. Thanks in advance.", "Best regards, Peter", "Well, you don't need another QR reader. zbar_ros is a good package. I think the reason why I couldn't use another camera to detect QR is that I use a wrong driver. As zbar_ros package just need to subscribe an image ."], "answer": [" ", " ", "The ", " tag is for ROS graph objects (topics, services, etc) rather than for ROS parameters. If you want to change the input device, you should use the ", " tag.", "Well, what you is really great.Thanks.\nBut when I changed as you said, some error occur too."], "question_code": ["<remap from=\"device\" to=\"/dev/video1\"/>", " exbot@ubuntu:~/zbar_ros$ roslaunch zbar_ros example.launch \n... logging to /home/exbot/.ros/log/693c9cd0-fbab-11e4-b783-485ab6072ea3/roslaunch-ubuntu-9697.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://ubuntu:45860/\n\nSUMMARY\n========\n\nPARAMETERS\n * /barcode_reader/throttle_repeated_barcodes: 2.0\n * /rosdistro: indigo\n * /rosversion: 1.11.10\n * /uvc_camera/device: /dev/video1\n\nNODES\n  /\n    barcode_reader (zbar_ros/barcode_reader_node)\n    uvc_camera (nodelet/nodelet)\n    zbar_manager (nodelet/nodelet)\n\nauto-starting new master\nprocess[master]: started with pid [9709]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to 693c9cd0-fbab-11e4-b783-485ab6072ea3\nprocess[rosout-1]: started with pid [9722]\nstarted core service [/rosout]\nprocess[zbar_manager-2]: started with pid [9739]\nprocess[uvc_camera-3]: started with pid [9740]\nprocess[barcode_reader-4]: started with pid [9775]\n**[zbar_manager-2] process has died [pid 9739, exit code -11, cmd /opt/ros/indigo/lib/nodelet/nodelet manager __name:=zbar_manager __log:=/home/exbot/.ros/log/693c9cd0-fbab-11e4-b783-485ab6072ea3/zbar_manager-2.log].**\nlog file: /home/exbot/.ros/log/693c9cd0-fbab-11e4-b783-485ab6072ea3/zbar_manager-2*.log\n[uvc_camera-3] process has finished cleanly\nlog file: /home/exbot/.ros/log/693c9cd0-fbab-11e4-b783-485ab6072ea3/uvc_camera-3*.log\n"], "answer_code": ["<remap>", "<param>", "[manager-2] process has died [pid 4655, exit code -11, cmd /opt/ros/indigo/lib/nodelet/nodelet manager __name:=manager __log:=/home/exbot/.ros/log/b6aebed4-fe45-11e4-8e3f-485ab6072ea3/manager-2.log].log file: /home/exbot/.ros/log/b6aebed4-fe45-11e4"], "url": "https://answers.ros.org/question/209322/detect-qrcode-with-zbar_ros/"},
{"title": "nav2d autonomouse exploration", "time": "2015-05-13 18:37:48 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I installed and tried nav2d navigation package exploration feature, with a real robot setup (kobuki base, urg-04lx) as per ros wiki instructions. i modified the tutorial3 launch and param files, as shown below (just copied the launch file and .yaml files into my catkin workspace so i can edit them). \nI received a map and other topics in RVIZ. since i dont have a joy stick i used ", " and ", " as per tutorials instructions i got nothing but a \"response: 0\" on my terminal window.", "LAUNCH File:", "</launch>", "when i tried to launch the SelfLocalizer in my launch file, it gave me an error saying: ", "Another issue, i tried to remap ", " to kobuki's velocity topic but i received an error, [MD5] topic format mismatch.", "any help, suggestions are greately appreciated. Param files (.yaml) are also available.", " ", "Here is the ", " file to download.", "tf_tree:", "rqt_graph:", "it also takes like 20 seconds or so before the topics in RVIZ receive information and go green.", "\nParameters list: \nPARAMETERS", " Please add a picture from \"rqt_graph\" and the dot-export result from \"rosrun tf view_frames\". These are usually the best starting point to search for problems in your setup.", " really appreciate it, i updated the question with the info that you asked.", "The transformations look fine, but in your ROS-Graph you see that the Operator in not connected to \"base_scan\". As the remapping is correct, it probably doesn't get to the point where the Costmap is initialized. Does it give any warnings or errors during start?", " i only can get 2 errors in different start up setups: (1. when i try to remap /cmd_vel to topic of my robot. (it says content is mismatch, MD5 xxx) (2. when i include SelfLocalizer in the launch file, its says the node can not be found int the package folder.", ": 1) Please do a \"rostopic type\" on your robots cmd-topic. If its not geometry_msgs/Twist, you have to do a conversion. 2) Have you build the package from source or installed the binary packages?", " the topic is ", " and rostopic shows that type is geometry_msgs/Twist. tried to remap that topic to /cmd_vel and got the MD5 error. I installed the pkg from the binary as per in the ros-wiki instructions using apt-get install.", ": This is really strange. Do you have different ROS-Versions on your machine? Or does any of your packages bring an own version of \"geometry_msgs\"? Before you start your launch file, do a \"roscd geometry_msgs\" and check where it takes you.", "My machine's config is: native \"Ubuntu 14,04\" with ROS Indigo (full desktop). i just use this 1 machine, and it runs my robot. roscd takes me to ", "\nyes it looks very strange. i thought it should be pretty straightforward. other packages work Ok."], "answer": [" ", " ", "The operator is not connected to the laser scanner. It needs it to update the local costmap for obstacle avoidance. In your costmap.yaml, the observation_source is defined incorrectly. Should be: \"observation_sources: scan\". You should visualize the costmap in RVIZ to see if the local costmap (not the global map from the mapper) is built correctly.", "Before you start any navigation task (setting a goal via RVIZ or explore) you should move the robot a bit forward, (with a joystick or by calling \"rosservice call /StartMapping 3\")", "In your mapper.yaml you have \"range_threshold: 4.0\". This is VERY little for a Hokuyo-Scanner! This is the reason your global map is so small and has a lot of holes in it. Use at least 10 meter. (Hokuyo can easily go up to 30 meter)", "Changed ", " and ran a quick test and it worked...! my Hokuyo scanner is URG-04LX-UG01 and according to its data sheet Max range is 4m , so the only way to increase maps size is to increase the ", "? "], "question_code": ["rosservice call /StartMapping 3", "rosservice call /StartExploration 2", "<launch>\n\n<!-- Some general parameters -->\n<param name=\"use_sim_time\" value=\"false\" />\n<rosparam file=\"$(find navigation)/param/ros.yaml\"/>\n\n<!-- Start Stage simulator with a given environment\n<node name=\"Stage\" pkg=\"stage_ros\" type=\"stageros\" args=\"$(find nav2d_tutorials)/world/tutorial.world\">\n    <param name=\"base_watchdog_timeout\" value=\"0\" />\n</node> -->\n\n<!-- Start the Operator to control the simulated robot -->\n<node name=\"Operator\" pkg=\"nav2d_operator\" type=\"operator\" >\n    <remap from=\"scan\" to=\"base_scan\"/>\n    <rosparam file=\"$(find navigation)/param/operator.yaml\"/>\n    <rosparam file=\"$(find navigation)/param/costmap.yaml\" ns=\"local_map\" />\n</node>\n\n<!-- Start Mapper to genreate map from laser scans -->\n<node name=\"Mapper\" pkg=\"nav2d_karto\" type=\"mapper\">\n    <remap from=\"scan\" to=\"base_scan\"/>\n    <rosparam file=\"$(find navigation)/param/mapper.yaml\"/>\n</node>\n\n<!-- Start the Navigator to move the robot autonomously -->\n<node name=\"Navigator\" pkg=\"nav2d_navigator\" type=\"navigator\">\n    <rosparam file=\"$(find navigation)/param/navigator.yaml\"/>\n            <!-- <remap from=\"/cmd_vel\" to=\"/mobile_base/commands/velocity\" /> -->\n</node>\n\n<node name=\"GetMap\" pkg=\"nav2d_navigator\" type=\"get_map_client\" />\n<node name=\"Explore\" pkg=\"nav2d_navigator\" type=\"explore_client\" />\n<node name=\"SetGoal\" pkg=\"nav2d_navigator\" type=\"set_goal_client\" />\n\n<!-- Start particle filter to track the robot's position \n<node name=\"SelfLocalizer\" pkg=\"nav2d_localizer\" type=\"localizer\">\n    <remap from=\"scan\" to=\"base_scan\"/>\n    <param name=\"min_particles\" type=\"int\" value=\"5000\"/>\n    <param name=\"max_particles\" type=\"int\" value=\"20000\"/>\n</node> -->\n\n<!-- Start the joystick-driver and remote-controller for operation\n<node name=\"Joystick\" pkg=\"joy\" type=\"joy_node\" />\n<node name=\"Remote\" pkg=\"nav2d_remote\" type=\"remote_joy\" /> -->\n\n<!-- RVIZ to view the visualization -->\n<node name=\"RVIZ\" pkg=\"rviz\" type=\"rviz\" args=\" -d $(find nav2d_tutorials)/param/tutorial3.rviz\" />\n", "ERROR: cannot launch node of type [nav2d_localizer/localizer]: can't locate node [localizer] in package [nav2d_localizer]", "/cmd_vel", " * /Mapper/LoopSearchMaximumDistance: 10.0\n * /Mapper/MinimumTravelDistance: 1.0\n * /Mapper/MinimumTravelHeading: 0.52\n * /Mapper/grid_resolution: 0.05\n * /Mapper/map_update_rate: 5\n * /Mapper/max_covariance: 0.01\n * /Mapper/max_particles: 10000\n * /Mapper/min_map_size: 20\n * /Mapper/min_particles: 2500\n * /Mapper/publish_pose_graph: True\n * /Mapper/range_threshold: 4.0\n * /Mapper/transform_publish_period: 0.1\n * /Navigator/exploration_goal_distance: 0.5\n * /Navigator/map_inflation_radius: 0.25\n * /Navigator/min_target_area_size: 1.0\n * /Navigator/navigation_goal_angle: 1.0\n * /Navigator/navigation_goal_distance: 0.5\n * /Navigator/navigation_homing_distance: 0.5\n * /Navigator/robot_radius: 0 ..."], "answer_code": ["observation_sources", "range_threshold"], "url": "https://answers.ros.org/question/209177/nav2d-autonomouse-exploration/"},
{"title": "Migration to ROS Indigo [ laptop Battery] ?", "time": "2015-06-03 02:36:53 -0600", "post_content": [" ", " ", "Hey guys. I'm working on the migration of ROS hydro in turtlebot 2 to ROS Indigo. I have a problem with the kobuki_dashboard.I can see the kobuki battery but not the laptop one. and when i do a \"rostopic echo /turtlebot_../diagnostics_agg\" , i can't see the LapTop Battery key in the /Power System . Any help,suggestions would be welcomed.", "Thank You !"], "answer": [], "url": "https://answers.ros.org/question/210482/migration-to-ros-indigo-laptop-battery/"},
{"title": "Goal reached, incorrect map", "time": "2015-05-25 05:58:24 -0600", "post_content": [" ", " ", "Hello,", "I am using ", " to make my robot move a certain distance (x,y) ", ". I am using the following code in order to send my goal to ", ":", "The issue is that in the physical world, the goal is being reached successfully, as I'm expecting it to be. So, I can see that the robot has moved by 1m forward (x-axis) and 0.3m towards its left (y-axis). The trajectory following is also perfect. My only problem is that the map being built at the same time (by ", ") is totally incorrect and the orientation of the robot in this map is also a mess:", "Now, according to my code, the robot should have moved by 1m forward, 0.3m towards the left and then the final orientation would be at 0 degrees to itself, i.e. it should remain facing forwards. Physically, I can see the robot reaching this goal appropriately and perfectly. But what I see in RVIZ is a completely different story. Here are the parameter files that I am using:", "common_costmap_params.yaml:", "global_costmap_params.yaml:", "local_costmap_params.yaml:", "base_local_planner_params.yaml:", "Here's the launch file for move_base:", "What is the orientation of your robot when it starts? And where does it get its orientation from?", "the orientation of the robot when it starts is (x, y, z) = (0,0,0) and w = 1 (in quaternion form). So it is oriented face forwards and the axis set to base_link show the red axis forward, green axis left. What do you mean where it gets its orientation from? I have gmapping and RosAria."], "answer": [], "question_code": ["move_base", "move_base", "goal.target_pose.header.frame_id = \"/base_link\";\n    goal.target_pose.header.stamp = ros::Time::now();\n\n    double theta;  // target robot orientation\n    theta = 0;\n\n    goal.target_pose.pose.position.x = 1.0; //Send robot to goal +1m along x, and 0.3m along y, z remains the same\n    goal.target_pose.pose.position.y = 0.3;\n    goal.target_pose.pose.position.z = 0;\n    goal.target_pose.pose.orientation.x = 0.000;\n    goal.target_pose.pose.orientation.y = 0.000;\n    goal.target_pose.pose.orientation.z = sin(theta/2); \n    goal.target_pose.pose.orientation.w = cos(theta/2); \n\n    ROS_INFO(\"Sending goal\");\n    ac.sendGoal(goal);\n", "gmapping", "map_type: costmap\ntransform_tolerance: 0.7\nobstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[-0.5, -0.33], [-0.5, 0.33], [0.5, 0.33], [0.5, -0.33]]\n\ninflation_radius: 0.55\n\nobservation_sources: laser_scan_sensor\n\nlaser_scan_sensor: {sensor_frame: laser, \n                    data_type: LaserScan, \n                    topic: /scan, \n                    marking: true, \n                    clearing: true,\n                    inf_is_valid: true}\n", "global_costmap:\n    global_frame: /odom\n    robot_base_frame: base_link\n    update_frequency: 3.0\n    publish_frequency: 10.0\n    static_map: true\n    #width: 30.0\n    #height: 30.0\n    #resolution: 0.025\n    #origin_x: 0.0\n    #origin_y: 0.0\n\nplugins:\n  - {name: static,                  type: \"costmap_2d::StaticLayer\"}\n  - {name: inflation,               type: \"costmap_2d::InflationLayer\"}\n", "local_costmap:\n    global_frame: /odom\n    robot_base_frame: base_link\n    update_frequency: 3.0\n    publish_frequency: 10.0\n    static_map: false\n    rolling_window: true\n    width: 10.0\n    height: 10.0\n    resolution: 0.05\n    origin_x: 0.0\n    origin_y: 0.0\n\nplugins:\n  - {name: obstacles_laser,           type: \"costmap_2d::ObstacleLayer\"}\n  - {name: inflation,                 type: \"costmap_2d::InflationLayer\"}\n", "TrajectoryPlannerROS:\n    # Robot Configuration Parameters\n    acc_lim_x: 0.5\n    acc_lim_y: 0.5\n    acc_lim_theta: 2.0\n    max_vel_x: 0.2\n    min_vel_x: 0.02\n    max_rotational_vel: 1.0\n    min_in_place_rotational_vel: 0.4\n    escape_vel: -0.1\n    holonomic_robot: false\n    meter_scoring: true\n\n    # Goal Tolerance Parameters\n    xy_goal_tolerance: 0.1\n    yaw_goal_tolerance: 0.2\n#new addition\n    latch_xy_goal_tolerance: false\n\n    # Forward Simulation Parameters\n    sim_time: 1.0\n    sim_granularity: 0.025\n    vx_samples: 3\n    vtheta_samples: 20\n", "<launch>\n  <master auto=\"start\"/>\n\n  <arg name=\"no_static_map\" default=\"false\"/>\n\n  <arg name=\"base_global_planner\" default=\"navfn/NavfnROS\"/>\n  <!-- <arg name=\"base_global_planner\" default=\"carrot_planner/CarrotPlanner\"/> -->\n  <arg name=\"base_local_planner\" default=\"base_local_planner/TrajectoryPlannerROS\"/>\n\n  <node pkg=\"move_base\" type ..."], "url": "https://answers.ros.org/question/209882/goal-reached-incorrect-map/"},
{"title": "MY RPlidar stopped spinning and sending data", "time": "2015-07-07 10:36:49 -0600", "post_content": [" ", " ", " ", " ", "i have an Arduino Leonardo and the RPLidar. I also have a RGB shield attached to it. I got the liar to spin on windows but have recently just changed to macintosh and it isn't spinning anymore. The code loads in fine through arduino but there is no output just 0's. /Users/williamphoenix/Desktop/Screen Shot 2015-07-07 at 11.23.37 AM.png Here is a screen shot of my error on my program. ", "I also have loaded many others onto the board and none of them seem to get the liar going and spitting back data.", "Thank you ", "Sorry Just Starting out on this and trying new things", "Idk how else toe the image into the blog it says that i need 5 of something.", "The error is /Users/williamphoenix/Documents/Arduino/libraries/RPLidarDriver/RPLidar.h:56:10: note: no known conversion for argument 1 from 'Serial_' to 'HardwareSerial&' no matching function for call to 'RPLidar::begin(Serial_&)'", "As well as i have loaded a few of the Arduino written libraries into my Arduino leonardo which is connected to my RPlidar and it doesn't spin anymore but the green LED is lighting up along with the board is responding but the liar will not spin", "can you please host the image somewhere else? also, please provide a more thorough explanation of the error, so that we might be able to help.", "Sorry Just Starting out on this and trying new things", "Idk how else toe the image into the blog it says that i need 5 of something.", "The error is /Users/williamphoenix/Documents/Arduino/libraries/RPLidarDriver/RPLidar.h:56:10: note: no known conversion for argument 1 from 'Serial_' to 'HardwareSeria"], "answer": [" ", " ", " ", " ", "Every time I open the serial of my rplidar here without the ROS node it does the same, it stops spinning. ", "\nI guess there the serial adapter somehow controls the reset line using the hardware signals ", ".", "You should maybe check to not interfere with them, maybe the reset line of the LIDAR is pulled to \nreset or not connected or has a lose contact...", "If you use them seperately (e.g. USB serial interfaces):", "Maybe you opened the wrong serial I guess Arduino and RPLidar register to the same kind of device names\n", "0 (maybe your Arduino Software opened the one of the RPLidar)", "Replug both and check with ", " which one is Arduino and which one is RPLidar", "Regards", "Christian"], "answer_code": ["RTS/CTS", "dmesg"], "url": "https://answers.ros.org/question/213056/my-rplidar-stopped-spinning-and-sending-data/"},
{"title": "Getting Started with ROS", "time": "2015-07-17 17:55:14 -0600", "post_content": [" ", " ", " ", " ", "I've built a couple of small robots using the Arduino for 2 servo control in addition to a distance sensor.  I've also gone through the ROS tutorial using VM VirtualBox to get Ubuntu to run ROS.  The reason I want to develop using ROS is given the extensive libraries that exist for ROS. However, I've read that in order to run ROS reasonably, the Raspberry PI or BeagleBone may run a bit slow if one is using multiple processor-intensive libraries like machine computer vision.  Is there a microcontroller that is able to run intensive libraries or do you really need to use at minimum a laptop for a robot that needs a number of libraries like computer vision, learning, motor control, path planning, etc.?", "Also, I'd like to know what level of difficulty exists in trying to run libraries like computer vision, machine learning, etc.  Would I be better off 1) first building robots with the Raspberry PI to get more used to interfacing motor controllers and servos with the PI and to also get more experience with Ubuntu (I'm familiar with it enough to follow the tutorial and build an image)?  I'm pretty familiar with C++ but not Python.  Or, would you recommend 2) diving into ROS and trying to build simple robots with either the Raspberry PI or a VirtualBox?  ", "Edit: The TK1 looks very intriguing - what sounds like very high performance and very low power consumption.  Would the TK1 be used only for computer vision or as a standalone microcontroller?  Or, would you recommend adding the TK1 to another microcontroller that would handle other processing?  If you recommend adding the TK1 to another microcontroller, which one would you recommend and how would you join the 2 together to communicate?", "Also, are there any rules of thumb for how much processing and RAM various libraries typically use - like computer vision, path planning, learning, etc.? ", ": I've moved your answer (which was more a comment) to your OP. Here on ROS Answers we typically only use answers to answer the question, not to post follow-up questions, or comment on other answers. Not a big deal, but would be nice to keep in mind."], "answer": [" ", " ", "Hi", "We are using a raspberry pi for our robot for running ROS, but the motor control stuff we are executing externally on a second microcontroller and controlling it as slave over SPI because of the realtime aspects of motor control, otherwise you would need to add realtime kernel patches and control the motor directly from the raspberry pie. When looking at its quite weak performance, I would definitely not recommend this. And as you already guessed the raspberry pie is not really good for machine computer vision.", "I guess there are some ARMs of manufacturers like used in mobile phones, but the probability to get them for your project, are quite low, well if you do not offer a perspective for selling a lot of chips with your product.", "Doing both, machine vision algorithms together with motor control on the raspberrypi ... well ... even with really optimized software ... I guess it will not work out. ", "And to your question about recommendation... \nGetting ROS run on the rasperrypi is also not so nice then just installing it in a VM. Also it takes a long time to startup nodes on the rasperrypi. Some code also does not work on ARM ...", "I would recommend starting to use functions of ROS in your VM or better directly on your computer because of RViz, and Gazebo (if you want to simulate robots).", "Regards,", "Christian", "so you think the custom robots like turtle bot or jakal or any others.... do you think they put motor control in a separate mcu than the ros one? ", "btw, do they use raspberry pi or their own hardware? I didn't see a source for designing custom hardware to run ubuntu and ros on the internet.", " ", " ", "You can check out the Nvidia Jetson development kit:", " "], "url": "https://answers.ros.org/question/214026/getting-started-with-ros/"},
{"title": "Problem connect turtlebot and workstation", "time": "2015-08-03 04:43:54 -0600", "post_content": [" ", " ", "Hello,\nAfter I have followed the tutorials , the connection between my turtlebot2 and the workstation still doesn't work, \nI have set ROS_MASTER_URI , ROS_HOSTNAME in both the workstation and the turtlebot netbook.\nwhen I'm trying to do :\nroslaunch turtlebot_bringup minimal.launch ", "\n (after I've ssh'd to the turtlebot netbook)", "I get these errors:", "core service [/rosout] found\nprocess[robot_state_publisher-1]: started with pid [2978]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\nprocess[diagnostic_aggregator-2]: started with pid [2980]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\nprocess[mobile_base_nodelet_manager-3]: started with pid [2982]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\nprocess[mobile_base-4]: started with pid [2984]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\n[robot_state_publisher-1] process has died [pid 2978, exit code -6, cmd /opt/ros/hydro/lib/robot_state_publisher/robot_state_publisher __name:=robot_state_publisher __log:=/home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/robot_state_publisher-1.log].\nlog file: /home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/robot_state_publisher-1", ".log\nprocess[cmd_vel_mux-5]: started with pid [2986]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\nprocess[bumper2pointcloud-6]: started with pid [2988]\n[mobile_base_nodelet_manager-3] process has died [pid 2982, exit code -6, cmd /opt/ros/hydro/lib/nodelet/nodelet manager __name:=mobile_base_nodelet_manager __log:=/home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/mobile_base_nodelet_manager-3.log].\nlog file: /home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/mobile_base_nodelet_manager-3", ".log\n[cmd_vel_mux-5] process has died [pid 2986, exit code -6, cmd /opt/ros/hydro/lib/nodelet/nodelet load yocs_cmd_vel_mux/CmdVelMuxNodelet mobile_base_nodelet_manager cmd_vel_mux/output:=mobile_base/commands/velocity __name:=cmd_vel_mux __log:=/home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/cmd_vel_mux-5.log].\nlog file: /home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/cmd_vel_mux-5", ".log\n[WARN] [WallTime: 1438594577.419304] Battery : unable to check laptop battery info [global name 'exceptions' is not defined]\n[WARN] [WallTime: 1438594577.422830] Battery : unable to check laptop battery state [global name 'exceptions' is not defined]\n[turtlebot_laptop_battery-7] process has finished cleanly\nlog file: /home/turtlebot/.ros/log/ed2507e6-39c0-11e5-9712-c4d987c9f05e/turtlebot_laptop_battery-7*.log\nall processes on machine have ..."], "answer": [" ", " ", "So I've found a way to make it work,\napprently the problew was in \n \"  what(): locale::facet::_S_create_c_locale name not valid process\"\nthe solution is by :\n\" export LC_ALL=\"C\" \"\nin my understanding it means that the code is written in C (language).\nif anybody can tell otherwise i will be happy to learn."], "url": "https://answers.ros.org/question/215130/problem-connect-turtlebot-and-workstation/"},
{"title": "Shutdown system (computer) with a ROS node", "time": "2015-07-25 09:10:11 -0600", "post_content": [" ", " ", "Is it possible to shutdown the system from within a ROS node? ", "In my case, I would like to shutdown the system once the battery level reaches a certain point, however I don't know how to do it or if it is even possible."], "answer": [" ", " ", "Just to add to the answer by ", " (ROS nodes are really just regular Linux programs that have chosen to communicate with each other via the ROS middleware): shutting down a Linux system with a desktop environment is often possible without using something like ", " (which requires special privileges). See \"", "\" on stackoverflow for an example where they use ConsoleKit / DBus:", "You could invoke ", " using the ", " function call in C/C++, or execute it from a Python node.", "For Python there are better (more integrated) ways to interface with ConsoleKit / DBus (copied from \"", "\" on stackoverflow):", "You could fi wrap the call to ", " (which is just an example) in a ROS service call (I wouldn't use parameters for this, it is clearly a synchronous action).", "This raised another question: Is it possible to close all nodes from within another node? Since this system is controlling some motors I would like to send a command to stop the motors and close all nodes before shutting down.", "As you already said: this is another question. Please open a new one for that.", "You are totally right. I have already posted a different ", ". Thanks for the help!", " ", " ", " ", " ", "A ROS-Node is a normal program that uses the ROS-Stuff. So you can (and have to) do whatever you want in your node, but there is nothing special in ROS that shuts down your computer. ", "You could create a service (the ubuntu-version, not a ROS-Service) that will be launched with root-rights (you'll need them to shutdown the system), that e.g. checks a ros-param. If you set this param, the service could shutdown the machine. \nOtherwise, you'd have to start you node with root-rights. "], "answer_code": ["shutdown", "dbus-send \\\n    --system \\\n    ---dest=org.freedesktop.ConsoleKit \\\n    --type=method_call \\\n    --print-reply \\\n    --reply-timeout=2000 \\\n    /org/freedesktop/ConsoleKit/Manager \\\n    org.freedesktop.ConsoleKit.Manager.Stop\n", "dbus-send", "system(..)", "import dbus\nsys_bus = dbus.SystemBus()\nck_srv = sys_bus.get_object('org.freedesktop.ConsoleKit',\n                            '/org/freedesktop/ConsoleKit/Manager')\nck_iface = dbus.Interface(ck_srv, 'org.freedesktop.ConsoleKit.Manager')\nstop_method = ck_iface.get_dbus_method(\"Stop\")\nstop_method()\n", "stop_method()"], "url": "https://answers.ros.org/question/214554/shutdown-system-computer-with-a-ros-node/"},
{"title": "ekf odom and compass robot_localization", "time": "2015-07-14 11:51:04 -0600", "post_content": [" ", " ", " ", " ", "Hi! \nI have a compass sensor and a odom sensor, this start in the position 0,0,0. The compass it's in the correct position but when i echo the odometry filtered It is delaying it will respect the GPS signal. ", "Any answer?", " EDIT 1:\nThis is a rosbag with the sensor topics  ", " \nThis is a tree graph, nodes graph and rviz capture.\n ", " ", "Red color /odometry/filtered and green color position/GPS\n", " ", "The position GPS is in absolute coordinates.\nThe ekf launch is this, with the base link compass transformation:", "Sorry, to clarify: you have wheel encoder odometry, a magnetometer, and a GPS? Are you using ", "? Perhaps you can give me some more information about your setup.", " Better post the solution you found as an answer and mark it as an answer so that others can tell the question is solved."], "answer": [" ", " ", " ", " ", "Hi!!\nI already solved my problem , the error was that i wasnt including the time stamp in the topics of sensors and these become crazy the filter, now the system is working well!! Regards"], "question_code": ["<!-- EKF -->\n<launch>\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"link1_broadcaster\" args=\"0.0 0.0 0.0 0.0 0.0\n0.0 1.0 base_link compass 20\" />\n\n<node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization\" clear_params=\"true\">\n  <param name=\"frequency\" value=\"30\"/>  \n  <param name=\"sensor_timeout\" value=\"0.1\"/>  \n  <param name=\"two_d_mode\" value=\"true\"/>\n\n  <param name=\"map_frame\" value=\"map\"/>\n  <param name=\"odom_frame\" value=\"odom\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom\"/>\n\n  <param name=\"odom0\" value=\"position/gps\"/>\n  <param name=\"imu0\" value=\"compass/data\"/> \n\n\n  <rosparam param=\"odom0_config\">[true,  true,  false, \n                                  false, false, false, \n                                  false, false, false, \n                                  false, false, false,\n                                  false, false, false]</rosparam>\n\n  <rosparam param=\"imu0_config\">[false, false, false, \n                                 false, false, true, \n                                 false, false, false, \n                                 false, false, false,\n                                 false, false, false]</rosparam>\n\n  <param name=\"odom0_differential\" value=\"false\"/>\n  <param name=\"imu0_differential\" value=\"false\"/>\n\n  <param name=\"imu0_remove_gravitational_acceleration\" value=\"true\"/>\n\n  <param name=\"print_diagnostics\" value=\"false\"/>\n\n  <!-- ======== ADVANCED PARAMETERS ======== -->\n\n  <param name=\"odom0_queue_size\" value=\"2\"/>\n  <param name=\"imu0_queue_size\" value=\"10\"/>\n\n  <param name=\"debug\"           value=\"false\"/>\n  <param name=\"debug_out_file\"  value=\"/home/tmoore/Desktop/debug_ekf_localization.txt\"/>\n\n  <rosparam param=\"process_noise_covariance\">[0.05, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0 ...", "navsat_transform_node"], "url": "https://answers.ros.org/question/213717/ekf-odom-and-compass-robot_localization/"},
{"title": "When ever I try to put my turtlebot2(simulated in gazebo) into a namespace the fake laser stops publishing messages on /scan", "time": "2015-08-27 02:16:53 -0600", "post_content": [" ", " ", "I am trying to put multiple turtlebots in my simulation and need to declare group namespaces for this purpose. When defined within the namespace the robot_state_publisher published all the transforms under the new namespace but the fake laser first of all does not follow the namespace the /scan topic remains /scan and 2nd it even stops getting any message at /scan which is working fine without the group namespace declared. Below is my launch file", "<launch>\n  <arg name=\"world_file\" default=\"$(env TURTLEBOT_GAZEBO_WORLD_FILE)\"/>\n  <arg name=\"base\" value=\"$(optenv TURTLEBOT_BASE kobuki)\"/> \n  <arg name=\"battery\" value=\"$(optenv TURTLEBOT_BATTERY /proc/acpi/battery/BAT0)\"/>   \n  <arg name=\"stacks\" value=\"$(optenv TURTLEBOT_STACKS hexagons)\"/>   \n  <arg name=\"3d_sensor\" value=\"$(optenv TURTLEBOT_3D_SENSOR kinect)\"/>   ", "<include file=\"$(find gazebo_ros)/launch/empty_world.launch\">\n    <arg name=\"use_sim_time\" value=\"true\"/>\n    <arg name=\"debug\" value=\"false\"/>\n    <arg name=\"world_name\" value=\"$(arg world_file)\"/>\n  </include>", "<group ns=\"robot1\"> \n", "\n  <include file=\"$(find turtlebot_gazebo)/launch/includes/$(arg base).launch.xml\">\n    <arg name=\"base\" value=\"$(arg base)\"/>\n    <arg name=\"stacks\" value=\"$(arg stacks)\"/>\n    <arg name=\"3d_sensor\" value=\"$(arg 3d_sensor)\"/>\n  </include>", "<node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" name=\"robot_state_publisher\">\n    ", "\n  </node>", "\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"laserscan_nodelet_manager\" args=\"manager\"/>\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"depthimage_to_laserscan\" args=\"load depthimage_to_laserscan/DepthImageToLaserScanNodelet laserscan_nodelet_manager\">\n    ", " ", " ", "\n    <remap from=\"image\" to=\"/camera/depth/image_raw\"/>\n    <remap from=\"scan\" to=\"/scan\"/>\n  </node>\n</group>\n</launch>", "I want my /scan topic to be published under /robot1/scan so that I could add in multiple robots all publishing their own scans. "], "answer": [" ", " ", "There is remapping rule in laserscan_nodelet_manager.", "<remap from=\"scan\" to=\"/scan\"/> ", "You may want to remove this."], "url": "https://answers.ros.org/question/216611/when-ever-i-try-to-put-my-turtlebot2simulated-in-gazebo-into-a-namespace-the-fake-laser-stops-publishing-messages-on-scan/"},
{"title": "Robot position Rviz is jumping", "time": "2015-08-12 08:38:44 -0600", "post_content": [" ", " ", "I'm using ", " on Ubuntu 14.04. I use MoveIT to control a robot arm (cyton gamma 1500) and visualize it in Rviz.\nI have a camera mounted on the gripper of the arm, and I'm detecting a chess board in front of the arm. ", "My problem that the current state of the is switching between the initial state and the real current sate.\nSo, if I send  a ", " or ", " command, than in ", " the current and goal states are showed well, but with a high frequency it appears the initial state (upright) of the arm.", "This is not only a simulation issue, because in the ", " topic also can be detected. The relative position of the chessboard is showed in the two places as the figure shows:", "The green object is the detected chessboard at the first time published its position, while the ", " arrow and axes are showing the ", " position which is a view from  the initial position of the arm.", "\nHere you can see a frame where the current state (green) and the initial position (gray) is showed. The real position of the hardware is the green one.", "Why does the the inital position always show up while I plane and execute ? \nDo I have to set something in a ", " launch file?"], "answer": [" ", " ", "The problem was found in the ", " I had a state publisher which published the initial state of the robot in lack of the ", "."], "question_code": ["group.move()", "group.execute(plan)", "tf", "TF"], "answer_code": ["move_group.launch", "fake_controller", "<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"odom_broadcaster\" args=\"0 0 0 0 0 0 odom_combined base_footprint 100\" /> \n<node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\">\n <param name=\"/use_gui\" value=\"false\"/>\n</node>\n"], "url": "https://answers.ros.org/question/215742/robot-position-rviz-is-jumping/"},
{"title": "depthimage_to_laserscan outputs laserscan upside down", "time": "2015-09-02 14:25:18 -0600", "post_content": [" ", " ", "Hi everyone, Im having trouble with depthimage_to_laserscan, its essentially upside down, if i try rotate the TF by 180 degrees the map is offset from the laserscan? is there anyway to flip the depthimage_to_laserscan? ", "Did you try to flip your depth image?", "How is that done?", "Use OpenCV function ", " :  ", " See this post for details  ", "I know there is a way with OpenCV but this would consume much more processing power flipping each depth image?", "This will not affect performance at all if you are working on a desktop or on a laptop.", "The other way to do image processing is by means of ", " but I did not find a method for image flipping. Or you can try ", " package  ", "  but it does the same opencv operations. ", "The depth camera is on a Raspberry Pi.I will try that, thanks for the help."], "answer": [], "question_code": ["flip", "image_transport", "image_rotate"], "url": "https://answers.ros.org/question/216983/depthimage_to_laserscan-outputs-laserscan-upside-down/"},
{"title": "Battery Monitor", "time": "2015-11-01 08:27:39 -0600", "post_content": [" ", " ", "I have a home built robot using a pc board, and not using a laptop computer.  I have lithium battery running everything.  I would like to add a battery monitor hardware and use ROS to monitor it.  Has anyone  accomplished something like this?\nThank you!", "Thanks Christian I will give it a try and let you know how works out.\nThanks again!"], "answer": [" ", " ", " ", " ", "Hi,", "I've added this weekend such feature for my robot, using an Arduino NANO.\nThe Arduino uses internal ADC and writes current voltages via USB to the ROS computer.\nA simple ROS-ArduPowerDaemon collects all messages from the device and publishes it as topics.\nAdditional, I write all values to /tmp/ups-dummy, which can be used be a standard UPS-Deamon to shutdown the system.", "This week I will update the docs for my robot.", " Please see:  ", "Cheers\nChristian"], "url": "https://answers.ros.org/question/220233/battery-monitor/"},
{"title": "Custom message_filters implementation. How?", "time": "2015-10-17 07:37:05 -0600", "post_content": [" ", " ", ":", "After studying the ", " documenation, ", " appear to me as a powerful tool for topic synchronization. I see how this is indeed realized in the elegant interfaces of the stated ", ", ", ", ", " and Policy-Based Synchronizers.", "But those listed implementations are quite limited considering that there apperar to be only two implemented policies for Policy-Based Synchronizers, one of which is ", " and the other is ", " which mimics ", ".", "So I see no easy way to synchronize topics in any other way than ", " or ", ". I tried eyballing ", " and ", " but there is no documentation that would help me figure out how to write my own sync policy.", ": ", "Is there some documentation on the sync policy API or are there more examples of sync policy implementations? How can I avoid reverse engineering non-documented code that makes heavy use of metaprogramming?", "I'd like to have a synchronizer for multiple topics, say ", " and I want my ", " to be triggered only each new ", " where for ", " and ", " just he most recent messages are passed to the callback. All of the data must be available before any callback. I see no way of doing this with the existing ", " implementation.", "What I do now is to handle that logic in my node by heaving ", " members which are updated in their own callback plus checking in the ", " callback whether ", " were already received. But this is such a common pattern and begs for encapsulation in a message filter! "], "answer": [], "question_code": ["message_filters", "TimeSynchronizer", "TimeSequencer", "Cache", "ApproximateTime", "ExactTime", "TimeSynchronizer", "ExactTime", "ApproximateTime", "<A, B, C, D>", "callback(A, B, C, D)", "A", "B, C", "D", "message_filter", "B, C, D", "A", "B, C, D"], "url": "https://answers.ros.org/question/219292/custom-message_filters-implementation-how/"},
{"title": "What is the specification pioneer3-DX motors? [closed]", "time": "2015-10-27 07:42:30 -0600", "post_content": [" ", " ", "What is the torque, power and gear ratio of the motors used in ", " robot.", " This is not a ROS question. Please stay on topic:  "], "answer": [], "url": "https://answers.ros.org/question/219982/what-is-the-specification-pioneer3-dx-motors/"},
{"title": "Shutdown nodes from within a ROS node", "time": "2015-09-29 10:17:32 -0600", "post_content": [" ", " ", " ", " ", "This is a follow up question to ", ".", "Since the system in which the ROS is running is connected to some motors I would like to know if it is possible to close nodes from within a different ROS node.", "Since the system is powered by some batteries, the shutdown procedure I would like to achieve is something like this:\nOnce battery level is below a certain threshold:", "However I have no clue how to \"Close/stop all nodes\" and \"Stop rosbag recording\" from within a certain node (The same node used to check the battery level).", "Is this possible?"], "answer": [" ", " ", "You can use ", " to shutdown nodes. I'm not sure this is necessary though - the system shutdown command (", ") should send ", " to all processes so they can end gracefully.", "Would that stop the rosbag recording properly so that I can use the bag-file after rebooting? And is there a way to select which nodes to \"kill\" or the only way is to shutdown every node with rosnode.kill_nodes?", "That's a good question: I tried ", " with rosbag which seems like it may not properly close the file. Your best bet would just be to just try rebooting with an active ", " to see what happens. ", "ing rosbag does seem to work fine though."], "question_details": [" ", " ", " ", " ", " ", " ", "Close/stop all nodes", "Stop rosbag recording", "Shutdown system "], "answer_code": ["shutdown", "SIGTERM", "kill -15", "rosbag record", "rosnode kill"], "url": "https://answers.ros.org/question/218412/shutdown-nodes-from-within-a-ros-node/"},
{"title": "Roomba 531 - Power For Kinect", "time": "2015-10-19 21:34:45 -0600", "post_content": [" ", " ", "This is a mostly-salvage project, and I've got an old Roomba 531 working really well. I have not yet got my Kinect wired to the thing. I'm using a genuine iRobot SCI/DIN-to-USB serial cable, so to get to the unregulated power from the DIN connector I'll need to do some slicing. My current idea is to cut up a separate DIN cable to use as an extender and just splice into the 12V and GND wires there. Are there any known/better ideas? ", "Take a look at Asus Xtion, it is USB powered only.", "I appreciate the advice, but a Xtion really isn't in the budget for this project unless I could find one for under $25."], "answer": [], "url": "https://answers.ros.org/question/219438/roomba-531-power-for-kinect/"},
{"title": "RViz Performance on a Raspberry Pi 2", "time": "2015-09-23 03:43:26 -0600", "post_content": [" ", " ", " ", " ", "I am currently running Ubuntu 14.04 LTS in a VirtualBox. When I start RViz, the overall performance of the system becomes super crappy (and when using the 3D Acceleration Feature in the VirtualBox settings, RViz always crashes, see ", "). Since I have a spare Raspberry Pi 2, I thought about setting up ROS on it and use its RViz remotely for visualization by publishing the relevant topics over the network, but I have some concerns regarding whether the pi has enough processing power to run Rviz nicely.", "So my question is: ", " ", "I would assume running RViz on a RPi2 would be a bad idea since it doesn't have very much processing power and doesn't have a GPU.", " The Raspberry Pi do have GPUs; they all are OpenGL ES 2.0 capable.  ", " . Make sure GPU hardware acceleration is available in your distribution (eg: Raspbian has it) "], "answer": [], "url": "https://answers.ros.org/question/218101/rviz-performance-on-a-raspberry-pi-2/"},
{"title": "Problem creating a ROS package with the presence of external headers", "time": "2015-09-17 04:11:33 -0600", "post_content": [" ", " ", "Hi I am a PhD student and quite new to ROS trying to make a ROS package to use my Robot. The company who built the robot has also provided us some C++ libraries and codes, i.e. libRobot.h and libRobot.cc. Inside these codes there some functions from different classes that help us to read status of the robot and also send commands to it. \nFor example: ", "Now what I am trying to do is to create a package that executes some of this functions, for example to read the voltage of the battery, and publish the data as ROS messages. \nI put the ", ".cc files in my_package/ directories. I also set up my CMakeLists.txt as follows:", "When I put my header in my cpp file, (i.e. #include libRobot.h) the catkin_make process fails at the very last second:", "Do you have any idea how can I possibly solve the problem?", "Thank you very much in advance"], "answer": [" ", " ", "The solution to this problem can be find in this ", ". ", " ", " ", " ", " ", "For the next time, please try to find the relevant lines where the error is detailed.\nFrom your post, we only see there is an error.", "However, there are several problems with your ", ".", "You need to tell the compiler where to find the headers you include.\nSo adapt the following lines in your ", " from: ", "to ", "But I guess you will also need to add the library source file to the ", " call, so:"], "question_code": ["int Rover::getSpeed ( Time & _timestamp,\nfloat & _left,\nfloat & _right \n) \n\nGet the speed of the tracks [in rad/s].\n\nParameters\n[out] _timestamp the current timestamp [in s].\n[out] _left the average speed of the front and rear left tracks [in rad/s].\n[out] _right the average speed of the front and rear right tracks [in rad/s].\nReturns\n0 if success, an error code otherwise.\n", "cmake_minimum_required(VERSION 2.8.3)\nproject(donkey_rover)\nfind_package(catkin REQUIRED COMPONENTS\nroscpp\nrospy\nstd_msgs\n)\ngenerate_messages(\nDEPENDENCIES\nstd_msgs\n)\ncatkin_package(\nINCLUDE_DIRS include LIBRARIES donkey_rover \nCATKIN_DEPENDS roscpp rospy std_msgs\nDEPENDS system_lib\n)\ninclude_directories(\n${catkin_INCLUDE_DIRS}\n)\nadd_executable(rovstate src/rovstate.cpp)\ntarget_link_libraries(rovstate\n${catkin_LIBRARIES}\n)\n\nadd_dependencies(rovstate donkey_rover_generate_messages_cpp)\nadd_dependencies(rovstate ${catkin_EXPORTED_TARGETS})\ninstall(DIRECTORY include/${PROJECT_NAME}/\nDESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}\nFILES_MATCHING PATTERN \"*.h\"\nPATTERN \".svn\" EXCLUDE\n)\n", "[ 98%] Built target navfn_node\n[ 98%] Built target navtest\n[100%] Built target global_planner\n[100%] Built target move_base_node\n[100%] Built target planner\nmake[2]: *** [donkey_rover/CMakeFiles/rovstate.dir/src/rovstate.cpp.o] Error 1\nmake[1]: *** [donkey_rover/CMakeFiles/rovstate.dir/all] Error 2\nmake: *** [all] Error 2\nInvoking \"make -j2 -l2\" failed\n"], "answer_code": ["CMakeLists.txt", "CMakeLists.txt", "include_directories(\n  ${catkin_INCLUDE_DIRS}\n)\n", "include_directories(\n  include\n  ${catkin_INCLUDE_DIRS}\n)\n", "add_executable", "add_executable(rovstate src/rovstate.cpp src/libRobot.cc)\n"], "url": "https://answers.ros.org/question/217804/problem-creating-a-ros-package-with-the-presence-of-external-headers/"},
{"title": "Possible to shut off obstacle avoidance (husky)?", "time": "2015-10-28 05:21:48 -0600", "post_content": [" ", " ", "Hello ROS-community,", "I have a problem you might help me figure out. I am using a husky to pick up certain objects, let's say these objects are boxes. I have a breadcrumb following navigation node that communicates with move_base to maneuver the environment and approach the box. The problem is that if i want to approach the box the husky will treat the box as an obstacle, thus trying to avoid it. Is it possible to \"shut off\" the obstacle avoidance, pickup the box, and power it on? I was reading this thread: ", "And thinking there might be a parameter i could reconfigure during runtime. Another problem i have is that i want to be able to take the fastest path if in order to align the husky to the box properly. If i give a goal point to move_base that's behind the husky, i want it to go backwards. But at the same time if the goal point is in front of the husky, it should move forward. Is this a possible behavior using dynamic reconfigure?", "Another think with dynamic reconfigure as the thread above: Is there a need to call some sort of update, in order for the new parameters to take place? I made a simple node like the thread above to reconfigure some move_base parameters but when i read them from the terminal they hadn't changed.", "TLDR:\n1) Is it possible to \"shut off\" and \"power on\" obstacle avoidance during runtime?\n2) Is it possible to have a behaviour that takes fastest path possible, which goes backwards if needed, and switch to \"normal\" maneuvering behaviour during runtime?\n3) Do I need to call some sort of update for my dynamically reconfigured parameters to take place?", "Thanks in advance!\nWbr\nMattias"], "answer": [" ", " ", "I might have figured it out, posting my findings here if anyone browses this thread:", "1) I set the obstacle layer of the laser scanner to \"enabled: false\". This allows me to approach objects without planner stopping me.\n2) Setting the min_vel_x to max_vel_x but switching sign solved this.\n3) No, the parameters are updated when dynamically reconfigured. "], "url": "https://answers.ros.org/question/220042/possible-to-shut-off-obstacle-avoidance-husky/"},
{"title": "Global path not changing with new obstacles !", "time": "2015-11-25 09:05:12 -0600", "post_content": [" ", " ", " ", " ", "hello  I'm using move_base to generate paths based on a map.\nThe below rviz screenshot sums up my problem quite well: essentially as the new (middle) obstacle appeared in my LiDAR's view, the global path (in green) doesn't update. It seems properly inflated (blue).", " my config files (DWA) :\n ", "\u0131 tried; ", "planner_frequency: 1.0", "static_map : true", "So what happens is that move_base keeps going onto the global path until it arrives too close to the obstacle, and then it backs up (recovery mode).", "Any ideas why my global path isn't changing?"], "answer": [], "url": "https://answers.ros.org/question/221518/global-path-not-changing-with-new-obstacles/"},
{"title": "controlling 2 dynamixel servos in parallel with arbotix driver", "time": "2015-11-13 08:52:14 -0600", "post_content": [" ", " ", " ", " ", "Hello List,", "We have an arm (from crustcrawler) where 2 servo's are connected in parallel to get more power in the joint.\nIt can be used using the dynamixel_controllers package by using the motor_slave construct in\nthe yaml file.", "But I would like to use the arbotix package. Is something similar possible? Or is it feasible to\njust use 2 controllers with always the same set points? Would that work, or will the motors\nbe fighting each other?\nOr do I need to port the functionality to the arbotix package?", "Thanks in advance,\n     Sietse"], "answer": [], "url": "https://answers.ros.org/question/220927/controlling-2-dynamixel-servos-in-parallel-with-arbotix-driver/"},
{"title": "3d visualizing multiple GPS points on a vehicle", "time": "2015-11-21 07:01:24 -0600", "post_content": [" ", " ", "Hello,\nI am a completely new to ROS and rviz. I wanted to ask a quick question to see if ROS can help me with my requirement. I work on a GPS and IMU logger system which logs position data of multiple points on a car. We do this on multiple cars. We post process this data to gather point to point distance. This is useful for advanced driving assistance systems testing, because it allows developers to have a independent data set where they can see speed, distance etc.  data and tweak their algorithms. As the number of points gathered can be huge(upto 32), visualizing them is difficult. I was wondering if ROS 3d visualization can help me with this. Imagine a situation where I could draw a scale 3D model of multiple vehicles(accurate only in points of consideration), and as the vehicles move, GPS coordinates of points will change and thus I will be able to visualize all points on the system. This gives a more powerful visualization rather than just looking at multiple points and their distance from other points. Sorry if this is completely not related to ROS, but any suggestions will be welcome.\nThanks"], "answer": [], "url": "https://answers.ros.org/question/221318/3d-visualizing-multiple-gps-points-on-a-vehicle/"},
{"title": "Using move_base for navigation, obstacle cannot be cleared timely", "time": "2015-11-17 06:24:15 -0600", "post_content": [" ", " ", " ", " ", "Dear all,", "I am using move_base for navigation with a SICK laser scanner. I have set the local cost map update frequency to 30Hz.\nHowever, the obstacle cannot be cleared timely when the obstacle such as a walking person moved away. The costmap will not be updated until the robot move. ", "For example, in the attached figure. The walking person in the green axis has moved away, but the costmap cannot clear the obstacle timely. Thus, the robot will be stuck in the virtual corner. The red axis is the robot heading direction.", "The parameter settings are:", "move_base parameters:", "Thank you very much,\nJack", "Can you provide more information like your move_base parameters?", "The parameters are posted."], "answer": [" ", " ", "Hi!", "If you have time problems with the obstacle upate in your costmap, i think you should increase the publish frequency in your local_costmap params, since this is the frequency at the map the robot uses is updated. If this doesnt help maybe there is a problem with the sensor.", "Hope this helps!"], "question_code": ["obstacle_range: 2.5\nraytrace_range: 3.0\nrobot_radius: 0.165\ninflation_radius: 0.30\nmax_obstacle_height: 0.6\nmin_obstacle_height: 0.0\nobservation_sources: scan\nscan: {sensor_frame: /laser, expected_update_rate: 30, data_type: LaserScan, topic: /scan, marking: true, clearing: true}   \n\nglobal_costmap:\n  global_frame: /map\n  robot_base_frame: /base_link\n  update_frequency: 5.0\n  publish_frequency: 0.0\n  rolling_window: false\n  static_map: true\n  resolution: 0.05\n  transform_tolerance: 2.0\n  map_type: costmap\n\n\nlocal_costmap:\n  global_frame: /odom\n  robot_base_frame: /base_link\n  update_frequency: 30.0 \n  publish_frequency: 10.0  \n  static_map: false\n  rolling_window: true\n  width: 6.0\n  height: 6.0\n  resolution: 0.05\n  transform_tolerance: 2.0\n  map_type: costmap\nobstacle_layer:\n    observation_sources: laser_scan_sensor\n    laser_scan_sensor: {sensor_frame: laser_link, data_type: LaserScan, topic: /scan, marking: true, clearing: true, observation_persistence: 0.0, expected_update_rate: 0.0, max_obstacle_height: 2.0, min_obstacle_height: -2.0, obstacle_range: 4.0, raytrace_range: 5.0, inf_is_valid: false}\n    max_obstacle_height: 2.0\n    obstacle_range: 4.0\n    raytrace_range: 5.0\n    track_unknown_space: false\n", "controller_frequency: 3.0\nrecovery_behavior_enabled: false\nclearing_rotation_allowed: false\n\nTrajectoryPlannerROS:\n   max_vel_x: 0.5\n   min_vel_x: 0.03\n   max_vel_y: 0.0   \n   min_vel_y: 0.0\n   min_in_place_vel_theta: 0.5\n   max_vel_theta: 0.25\n   min_vel_theta: -0.25\n   escape_vel: -0.1\n   acc_lim_x: 2.5\n   acc_lim_y: 0.0 \n   acc_lim_theta: 3.2\n\n   holonomic_robot: false\n   yaw_goal_tolerance: 0.15  \n   xy_goal_tolerance: 0.15  \n   latch_xy_goal_tolerance: false\n   pdist_scale: 1.2\n   gdist_scale: 0.6\n   meter_scoring: true\n\n   heading_lookahead: 0.5  \n   heading_scoring: false\n   heading_scoring_timestep: 0.8\n   occdist_scale: 0.2 \n   oscillation_reset_dist: 0.05\n   publish_cost_grid_pc: false\n   prune_plan: true\n\n   sim_time: 1.0\n   sim_granularity: 0.025\n   angular_sim_granularity: 0.025\n   vx_samples: 8\n   vy_samples: 0 \n   vtheta_samples: 20\n   dwa: true\n   simple_attractor: false\n"], "url": "https://answers.ros.org/question/221066/using-move_base-for-navigation-obstacle-cannot-be-cleared-timely/"},
{"title": "offboard reject error", "time": "2015-12-11 03:11:55 -0600", "post_content": [" ", " ", " ", " ", "Hi to all,", "I am using Ros Indigo in  Ubuntu 14.04.\nMy mavros version is 0.16.4.0 as for pixhawk version it is latest one which I updated it on 20 Dec 2015.", "For my offboard control project, I can using 3DR 433Mhz radio connected to pixhawk to transfer the code from computer to pixhawk.But I encounter a problem when switching to auto mode in RC controller, it tend to reject the auto mode and switch back to manual mode and back to auto mode again and back to manual mode . As the pixhawk light keep switching from green to blue back to green the back to blue again.  ", "The frequency I publish to setpoint topics is 100Hz .And the topics published are /mavros/setpoint/cmd_vel", "The code I type in is ", "roslaunch mavros px4.launch fcu_url:='/dev/ttyUSB0:57600'", "then I type in \nrostopic echo -n1  /diagnostics", "The code shown are ", "Next I type in rosrun hexrotor offboard\nThe Roslaunch output is ", "It is working fine but after a while the error occur ", "Is there any way to make the copter stay at auto mode ? Thanks"], "answer": [" ", " ", "The flight controller needs a stream of setpoint messages before the commander accepts offboard mode. Make sure the flight controller gets a stream of setpoints (0.5s timeout) before you switch to offboard mode. ", " ", " More details in  ", " ", " ", "Please dump result of that command:", "rostopic echo -n1 /diagnostics", "Please dump full roslaunch output, not only FCU message.", "I think you may use services like pastebin or edit first post. ", "After I type in rostopic echo -n1 /diagnostics", "The code shown are", "Next I type in rosrun hexrotor offboard ", "The Roslaunch output is"], "answer_details": [" ", " ", " ", " ", "With what frequency you publish to setpoint topics? Also please list which topics do you use.", "What versions of px4 firmware and mavros used?", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/222335/offboard-reject-error/"},
{"title": "New Robot with Ubuntu: Snappy or Traditional Ubuntu?", "time": "2015-12-17 21:20:32 -0600", "post_content": [" ", " ", "I'm just getting started building a new robot using a 6WD wild thumper chassis, an Arduino, and a Radxa Rock quad-core ARM single-board computer.", "I've just recently learned about Ubuntu Snappy. The premise sounds awesome: a stripped-down version of Ubuntu designed specifically for running on low-power embedded devices.", "The documentation appears a bit sparse, however, compared with all the abundant documentation for traditional Ubuntu OS's.", "So I'm wondering: would it be a good idea to use Ubuntu Snappy on my new robot? Is it stable and/or mature enough yet, in your opinion?", "Thanks!"], "answer": [" ", " ", " ", " ", "It all depends on your skill level and ability to navigate through documentation and tutorials. \nThe short answer:\nIf you are a beginner, go with a traditional Ubuntu version and use the documentation that's there.\nIf you are willing to \"trailblaze\" a little bit, go with Snappy and be prepared to have little or no documentation/support. \nThe long answer:\nThere are yet no installation instructions on the wiki for snappy, which shows the amount of support. Packages to run ROS with full compatibility on ROS have been made by some people on the side, but I'm not sure if ROS itself is actively working on this. For a beginner, as I am and probably will still be for a while, having tons of documentation is invaluable. With the Ubuntu Trusty Desktop you get just that, tons of documentation and full support. It's really up to you. If you feel like you can handle it without documentation guiding the way, by all means do it. ", "To specifically answer your question: If you are just starting out, don't use Snappy, otherwise go for it.", "Hope that helps,", " Yeah, I'm with ya. I am an Ubuntu veteran, with plenty of experience using it on the desktop and navigating docs/tutorials. But I haven't used it much on embedded devices, and I'm almost a total ROS n00b. I think I'll stick with UbuntuARM for now. Thanks", "I'm glad I could be of service."], "url": "https://answers.ros.org/question/222740/new-robot-with-ubuntu-snappy-or-traditional-ubuntu/"},
{"title": "mavros global_position topic not publishing", "time": "2015-12-10 04:25:32 -0600", "post_content": [" ", " ", " ", " ", "Hi,\n I am having problem getting the message published in global_position.", "I am using Ubuntu 14.04 Ros Indigo and recently just updated the firmware. I use a pixhawk and 3DR Iris in Hexacopter .", "The code I type is", " ", "NODES\n  /\n mavros (mavros/mavros_node)", "auto-starting new master", "process[master]: started with pid [3595]", "ROS_MASTER_URI=http://localhost:11311", "setting /run_id to ed737cc2-9f24-11e5-83d8-605718672f0f", "process[rosout-1]: started with pid [3608]", "started core service [/rosout]", "process[mavros-2]: started with pid [3626]", "After that I type in ", "/diagnostics", "/mavlink/from", "/mavlink/to", "/mavros/actuator_control", "/mavros/battery", "/mavros/cam_imu_sync/cam_imu_stamp", "/mavros/global_position/compass_hdg", "/mavros/global_position/global", "/mavros/global_position/local", "/mavros/global_position/raw/fix", "/mavros/global_position/raw/gps_vel", "/mavros/global_position/rel_alt", "/mavros/imu/atm_pressure", "/mavros/imu/data", "/mavros/imu/data_raw", "/mavros/imu/mag", "/mavros/imu/temperature", "/mavros/local_position/local", "/mavros/manual_control/control", "/mavros/mission/waypoints", "/mavros/mocap/pose", "/mavros/px4flow/ground_distance", "/mavros/px4flow/raw/optical_flow_rad", "/mavros/px4flow/temperature", "/mavros/radio_status", "/mavros/rc/in", "/mavros/rc/out", "/mavros/rc/override", "/mavros/setpoint_accel/accel", "/mavros/setpoint_attitude/att_throttle", "/mavros/setpoint_attitude/attitude", "/mavros/setpoint_attitude/cmd_vel", "/mavros/setpoint_position/local", "/mavros/setpoint_velocity/cmd_vel", "/mavros/state", "/mavros/time_reference", "/mavros/vfr_hud", "/mavros/vision_pose/pose", "/mavros/vision_speed/speed_vector", "/mavros/wind_estimation", "/rosout", "/rosout_agg", "/tf", "/tf_static", "which the code is shown below", "header: ", "seq: 594", "stamp: ", "secs: 1449742645", "nsecs: 599718628", "frame_id: fcu", "status: ", "status: -1", "service: 1", "latitude: 0.0", "longitude: 0.0", "altitude: -17.0", "position_covariance: [-1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "position_covariance_type: 0", "May I know why is there still missing messages publishing in global_position where the  Plugin global_position loaded and initialized has been successful?"], "answer": [" ", " ", "You do not have GPS Fix.", "Tip: see diagnostics first before asking questions!", "Even with GPS fix, I am only able to get message published to global_position/raw/gps_vel and global_position/raw/fix but other global_position topics  such as global_position/compass_hdg,global_position/rel_alt,global_position/global and global_position/local have no messages published out.", "The following is the roslaunch full output", "Sorry you are right, I need to have a GPS fix in order to get all messages published out in global message but the weird thing is through direct usb connection with the use of (roslaunch mavros px4.launch) I cannot get all the messages published in global_position even with GPS fix.", "But through indirect connection with the use of 3DR antenna(roslaunch mavros px4.launch fcu_url:='/dev/ttyUSB0:57600'), I am able to get all the messages published in global_position, thanks a lot.", " I have been working with the 3dr solo as well and I have the same problem. When connected directly with a usb, some of the data is not published. Can you give some more detail on how you did it through the \"indirect connection with 3dr antenna\"? Thanks!"], "answer_details": [" ", " ", " ", " ", "Pixhawk USB only for ground maintenance and tests. Read about SYS_COMPANION parameter and TELEM2.", "Anyway problem on FCU side. Check streams configuration (extras.txt & rc)", " is usual on PX4 with empty mission list."], "answer_code": ["FCU: WPM", "rostopic echo -n1 /diagnostics"], "url": "https://answers.ros.org/question/222290/mavros-global_position-topic-not-publishing/"},
{"title": "Why the increased system usage?", "time": "2015-12-10 08:30:31 -0600", "post_content": [" ", " ", "I have a Raspberry Pi B running Raspbian and ROS Groovy with two nodes. The first node controls the Pi camera and publishes the feed which is subscribed to on a separate PC. The second node publishes data received from a custom daughter board that is subscribed to by the PC, and subscribes to data published by the PC and sends that data to the daughter board. The camera streams fine by itself, and the other node operates great by itself. If both nodes are running, the camera is still fine, but the second node never updates data (the PC and daughter board never receive quality data). I plugged the Pi into a monitor for troubleshooting and noticed the green system usage bar is maxed out when running the second node.", "I figured the Pi could not handle this robust of a node and decided to upgrade to the Pi 2.", "Here's where things get interesting/confusing.", "The Pi 2 is running Ubuntu with ROS Indigo. Both nodes start, but now the camera feed lags by 2-3 seconds. I decided to check the system usage for both Pi's and got the following results:\nPi B:                                          Pi 2: ", "\n      PiCameraNode: 5.5-5.8%             PiCameraNode: 3.3-4.0% ", "\n      ServerNode:      87.4-88.9%         ServerNode:      122.6-124.0%", "Why would the Pi 2 which is significantly more powerful need more resources for the same code?\nAny ideas as to what could cause this, or how to fix the problem?\nAny help would be appreciated.", "As noted by Humpelstilzchen, the %CPU is per core so there is no problem with 125%CPU on the Pi2.\nI was able to decrease the lag of the camera to about 0.5 seconds by adding \"gpu_mem=512\" to \"/boot/config.txt\". I will play around with it some more and see how low I can get the lag."], "answer": [" ", " ", "Sorry, didn't got it. You said the Pi1 was to slow for your task, which also seemed to be confirmed by your values: 99%+6%=95% CPU usage, which is way too close to 100%. This indicates that the PI is throwing all its idle time to ServerNode which is still not enough.", "The Pi2 on the other hand seem to be fast enough with its multiple cores, it is using 125% = 1,25 CPU cores to handle the node.", "Thanks for the rapid response. I did not realize that %CPU was for one core. \nNow I just need to solve the camera latency issue.", " ", " ", "Your description of the ServerNode using a full CPU core is suspicious given that it's only acting as an intermediary between the computer and the daughter card; particularly given that it still uses a full core even when moved to a larger CPU.", "You should inspect the code for your ServerNode and make sure that it isn't busy-waiting or otherwise consuming CPU cycles when it's just waiting for events."], "url": "https://answers.ros.org/question/222311/why-the-increased-system-usage/"},
{"title": "Control 8x8 grid of RGB LEDs like Raspberry Pi Sense Hat or Unicorn Hat?", "time": "2015-12-24 00:30:48 -0600", "post_content": [" ", " ", " ", " ", "Hi--", "I'm completely new to ROS.", "We have a project where battery power is limited but we need some real-time visualization of sensor inputs. We're looking for ways to avoid using rviz on a tiny TFT screen due to the current draw.", "One way that may save us power and give us a few more sensors is to use the Raspberry Pi's Sense Hat, which has an 8x8 grid of RGB LEDs that can be dimmed to very low brightness. It would work perfectly for our project.", " I did a little searching and didn't find any mention of LED grids or arrays. I did find a tutorial on controlling a single RGB LED here:  ", " The Sense Hat and Pimoroni's Unicorn Hat are both easily controlled via very simple Python APIs; the Sense Hat is documented here at  ", " . It's pretty much one-line member function calls to draw the entire grid. ", "My questions are: ", "1) Is there already some code out there for outputting to an RGB LED grid? If so, could you point me to it?", "2) If not, how hard would it be to write the ROS code myself? I am an experienced bash, Perl, and PHP coder but new to Python and ROS. The amount of code necessary to control the one RGB LED in the tutorial seems a bit high. If we're talking over 8 hours of reading documentation and then coding even with the Sense Hat's simple Python API, then it is probably not worth it to try this idea. Keep in mind that the coding part is probably not the bulk of the time needed--I really don't know anything about ROS and so would have to read tons of docs before being able to write good code.", "Thanks in advance for any help you may provide!"], "answer": [], "url": "https://answers.ros.org/question/223046/control-8x8-grid-of-rgb-leds-like-raspberry-pi-sense-hat-or-unicorn-hat/"},
{"title": "Visualization of pruned octomap in RVIZ strange", "time": "2015-12-21 06:50:52 -0600", "post_content": [" ", " ", "Hello,", "these are two questions, however, they are related. ", "I have a height map stored in a .xyz file (each line corresponds to an x,y,z pair, like for a point cloud). The resolution for x,y is 2m each. I would like to put this information in an octomap. This means for each x,y, all that is below z is marked as occupied space, everything that is above z is marked as free space.", "Currently, I iterate twice through all of the space covered by the octomap. The first time for setting all of the space as free space, and the second time for setting all below z as occupied:", "Is there a way to create a free octomap without iterating through all leafs (e.g. assigning free to a node which automatically sets all children as free as well)?", "The method described above works. After I created the octomap, I prune it and save it as map.ot. When I visualize it using octovis, everything is fine. However, when I visualize it using RVIZ (see code below), the visualization is bad (no big voxels are shown, just the small ones, I have some screenshots, however I cannot upload pictures). What do I have to do visualize it correctly in RVIZ? (As a note: If I expand the octree (reverse of prune), the visualization is correct, however needs a lot more of computational power.) ", "ros::Publisher octomap_pub = n.advertise<octomap_msgs::octomap>(\"octomap_msg\",1);", "octomap_msgs::Octomap octomap_msg;", "octomap_msg.binary = 1;", "octomap_msg.id = 1;", "octomap_msg.header.frame_id = \"/base_link\";", "octomap_msg.header.stamp = ros::Time::now();", "bool octomap_ok = octomap_msgs::fullMapToMsg(*(mM->octomap),octomap_msg);"], "answer": [], "question_code": ["for (double x = 0.; x <= len_z; x += s_x) {\n\n    for (double y = 0.; y <= len_y; y += s_y) {\n\n        for(double z = 0.; z <= len_z; z += s_z) {\n            // Create free octomap\n            octomap->updateNode(x,y, z, false);\n        }\n\n   }\n\n}\n\nfor (double x = 0.; x <= len_z; x += s_x) {\n\n    for (double y = 0.; y <= len_y; y += s_y) {\n\n        for(double z = 0.; z <= getZ(x,y); z += s_z) {\n            // Fill in info in octomap.\n            octomap->updateNode(x,y, z, true);\n        }\n\n   }\n\n}\n"], "url": "https://answers.ros.org/question/222889/visualization-of-pruned-octomap-in-rviz-strange/"},
{"title": "what should be my recommended hardware architecture?", "time": "2015-11-30 12:02:26 -0600", "post_content": [" ", " ", "I'm essentially trying to build a robot platform (similar to Turtle) from scratch. part of the reason is that Turtle started many years ago, and after that, hardware technology have progressed a lot (multiple powerful ARM chips appeared , etc).  my understanding is that Turtle basically uses a small desktop computer with a miniATX motherboard, sort of. ", "at first I tried to do all the processing on an Arduino, but then found that Arduino (at least UNO) is way too simple, doesn't have a competent OS, hence possibly can't run the full C++ code in which ROS is written in. \nArduino TRE does run Linux, and is a small form-factor and consumes just 5v power supply to be very light. but I am still not sure if it's powerful enough to run complex algorithms such as video feature extraction and then SLAM.", "another attempt is to run an Arduino board solely for the purpose of interacting with sensor data collection, and sending out actuator commands, and then communicate all the in/out data with another computer . the latter may be an Android phone, or a small desktop. but using a desktop requires me to provide a big power supply (how is the Turtle getting its power?)", "Thanks\nYang"], "answer": [], "url": "https://answers.ros.org/question/221765/what-should-be-my-recommended-hardware-architecture/"},
{"title": "Tf has two or more unconnected trees", "time": "2015-12-16 20:30:03 -0600", "post_content": [" ", " ", "Hi friend. I have a problem. I want to run my bag file with Karto. So I use static transform publisher to connect frames.", "My frames.pdf is here: ", "I set the fixed frame to the odom, My LaserScan sometimes red, sometimes green:", "Like this(Red): ", "But it changes. ", "When I set fixed_frame to the base_link, LaserScan seems good, but at this time, map going to red(Error)sometimes and green sometimes. And it says me \"No transform from [map] to [base_link]\"", "This is an error or normal? Is my TF tree correct?", "Why do you have ", " frame? Is it intentional?", "Does the output of your odometry/localization node say something helpful?", "Hey Akif, thanks for your reply. No it is not intentional and I didn't find, why it is on my tf tree.", "Hey BennyRe, which node? I have these node now: ", "I don't know. You should know which node is responsible for odometry and/or localization but it should be one of ", " node is nav2d_karto, ", " is LaserScanMatcher node, ", " is my package. I take Pose2d message from LaserScanMatcher and I convert it to the  nav_msgs/Odometry."], "answer": [], "question_code": ["world", "/Mapper\n/lsm_node\n/odom_lsm\n/robot_pose_ekf\n/rosout\n/rostopic_7736_1450348973162\n/rviz\n/tf1\n/tf2\n/tf3\n", "/Mapper /lsm_node /odom_lsm /robot_pose_ekf", "/Mapper", "lsm_node", "odom_lsm"], "url": "https://answers.ros.org/question/222661/tf-has-two-or-more-unconnected-trees/"},
{"title": "Adding an IMU to Turtlebot 1", "time": "2016-02-04 14:38:42 -0600", "post_content": [" ", " ", "Hi Everyone,", " I'm trying to put together a Turtlebot 1 with some old iRobot creates and kinects I have had around my lab. So far, I've been able to make the plates, standoffs, power regulation board, and get everything working with the turtlebot package. I had to go in the create launch file and disable the gyro (which I can't buy anymore). I decided to order the sparkfun razer 9dof IMU as a replacement since I already saw it had a ROS package available.  ", "I created a launchfile which would call the turtlebot minimal.launch and 3dsensor.launch with the parameters for the Turtlebot 1. I also added in that launchfile the razor-pub.launch for the IMU. I also modified the robot_pose_ekf node launched by create node to subscribe to the /imu topic published by the razor instead of the /mobile_base/sensors/imu_data which was the gyro data published by the create node.", "Everything seemed to work, robot_pose_ekf was subscribing to /imu, the /imu topic was publishing data, and I could teleop the turtlebot around. However, I noticed a problem when I tried to run gmapping. It appears that I am missing the transform from base_imu_link to base_footprint.", "Now, where I am stuck is: how do I tell the /imu topic to be published as the base_imu_link frame? Does the turtlebot urdf file create the base_imu_link from or does robot_pose_ekf? ", "I've been chasing this issue through many files, but as a relatively new ROS user, the turtlebot's flexible and cryptic method of launching the URDF gets me lost. My ROS console is giving the error \"Could not transform imu message from base_imu_link to base_footprint. Imu will not be activated yet.\" It is also indicated that /robot_pose_ekf is the topic giving the warning.", "Thanks in advance for any pointers or insight."], "answer": [" ", " ", "I guess I'll answer my own question, as I managed to resolve the issue. It appears that the razer 9dof node provides the /base_imu_link frame. I simply added the following line to my launch file, and everything worked perfectly!\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"imu_to_base\" args=\"0 0 0 0 0 0 base_link base_imu_link 10\"/>", "I suppose the URDF file for turtlebot provides all of the frames there, but since it's too complicated to bother editing, it's easier/better to manually provide the frame conversion in tf.", "Thank you for this pointer -- I ran into the same problem. Edited: I had a follow up question about /imu vs /imu_data but you answered that in the original question. Thanks!", "OK -- actual follow-up: After getting this to work, I see that /odom_combined only uses the 9DOF to update the orientation. Evidently it doesn't integrate the accelerometer data to get velocity or change of position. Since the TurtleBot pretty much lives in a 2D world, all I got from the IMU is yaw?"], "url": "https://answers.ros.org/question/225873/adding-an-imu-to-turtlebot-1/"},
{"title": "Ros memory usage", "time": "2016-01-19 06:20:39 -0600", "post_content": [" ", " ", " ", " ", "I i'm trying to use an old c++ library of a task planner in ROS. I've tested the library outside ROS and it's works fine. Using Valgrind I can see a lot of memory leaks, but it's Ok, as I say everything works and pass sucessfully the test battery.", "The problem is when I call the same functions in a ROS node: the program crashes when I delete the memory of a pointer", "Using Valgrind it gives me this last error before the stacktrace", "And literally the only changes between the compilation that works and the other that crashes is that in the first I use Make manually and in the other I compile with catkin.", "I'm missing something about ROS and his memory usage and I need some orientation. ", "Thanks.", "ps. The code of the function that contains the delete"], "answer": [" ", " ", "ROS does not have any special memory usage patterns that would be causing this.", "The presence of memory leaks in your planning library suggests that it may also be overwriting memory elsewhere. You should check your valgrind results for out-of-bouds accesses.", "Running your program in GDB and getting a backtrace may also give you a more precise crash location.", "I've fixed one by one, every memory leak, and It works now."], "question_code": ["*** Error in `./devel/lib/task_executor/task_executor': free(): invalid next size (fast): 0x0000000001a0cf40 ***\n", "valgrind: m_mallocfree.c:304 (get_bszB_as_is): Assertion 'bszB_lo == bszB_hi' failed.\nvalgrind: Heap block lo/hi size mismatch: lo = 160, hi = 0.\nThis is probably caused by your program erroneously writing past the\nend of a heap block and corrupting heap metadata.  If you fix any\ninvalid writes reported by Memcheck, this assertion failure will\nprobably go away.  Please try that before reporting this as a bug.\n", "bool parse(string domain, string problem){\n\n  if(domain.length() == 0){\n    ROS_INFO_STREAM( \"Error: Undefined domain file.\" << endl);\n    return true;\n  }\n\n  if(problem.length() == 0 ){\n    ROS_INFO_STREAM( \"Error: Undefined problem file.\" << endl);\n    return true;\n  }\n\n  parser_api = new PAPI();\n  try{\n    parser_api->parse(domain.c_str(),problem.c_str(),true);\n  }\n  catch(exception &e){\n     ROS_INFO_STREAM( \"Exception caught!: \" << e.what() << endl);\n  }\n  catch(...) {\n    ROS_INFO_STREAM( \"Exception caught!: ??\" << endl);\n  }\n  bool errors = parser_api->errors;\n  delete parser_api;\n  return errors;\n}\n"], "url": "https://answers.ros.org/question/224477/ros-memory-usage/"},
{"title": "XV11 LIDAR on Jade", "time": "2016-02-02 20:54:37 -0600", "post_content": [" ", " ", " ", " ", "I'm following these tutorials on using the xv11 lidar with ROS:", "looking at the raw input on /dev/ttyUSB0", "it looks like I'm getting data, but ", "is empty, as is", "when I ", "I see", "and it's the same for both versions of the firmware", "I'm thinking that the driver might have problems with ROS Jade? I'm not sure what else could be going wrong.", "Has anyone else got this working with ROS Jade?", "are there any other drivers I could try?", "Are you running : rosrun xv_11_laser_driver neato_laser_publisher _port:=/dev/ttyUSB0 _firmware_version:=2", "yeah, the /scan topic only appears after running the driver. I've tried both versions of the firmware too.", "The data dump from your laser almost looks like it's constantly rebooting.", "What do you get when you run rostopic echo /rpms ?", "I don't have the hardware connected now but /rpms is also empty I think. I'll check it when I hook it up again.", "I've also tried powering the lidar and motor from 2.8->3.3 volts on a bench supply. At extreme ranges I stop getting data, the debug output is from 3.0v. I've checked to make sure current isn't being limited, it draws about 100mA", "You probably shouldn't be running the motor and lidar from the same voltage source. I'd recommend that you run the motor from your variable supply, and the lidar from 3.3V", "ok, I'll try that. The diagram on the tutorial had both at 3.0 so I thought that was the best way."], "answer": [], "question_code": ["2016-02-02 18:41:32.959795: /dev/ttyUSB0\n53 43 73 20 6F 72 20 42 52 45 41 4B 20 74 6F 20 SCs or BREAK to \n61 62 6F 72 74 2E 3A 29 0A 0D 50 69 63 63 6F 6C abort.:)..Piccol\n6F 20 4C 61 73 65 72 20 44 69 73 74 61 6E 63 65 o Laser Distance\n20 53 63 61 6E 6E 65 72 0D 43 6F 70 79 72 69 67  Scanner.Copyrig\n68 74 28 63 29 20 32 30 30 39 2D 32 30 31 20 4E ht(c) 2009-201 N\n65 61 74 6F 20 52 6F 2E 0A 0D 41 6C 6C 20 52 69 eato Ro...All Ri\n67 68 74 73 20 52 65 73 65 72 76 65 64 0A 0D 0A ghts Reserved...\n0D 4C 6F 61 64 65 72 09 56 32 2E 35 2E 31 34 30 .Loader.V2.5.140\n31 30 0A 0D 43 50 55 09 46 32 38 30 32 78 2F 63 10..CPU.F2802x/c\n30 30 31 0A 0D 53 65 72 69 61 6C 09 57 54 44 33 001..Serial.WTD3\n34 34 31 32 41 41 2D 31 35 32 36 35 30 0A 0D 4C 4412AA-152650..L\n61 73 74 43 61 6C 09 5B 35 33 37 31 32 36 43 5D astCal.[537126C]\n0A 0D 52 75 6E 74 69 6D 65 09 56 32 2E 36 2E 31 ..Runtime.V2.6.1\n35 32 35 0A 0D 23 53 70 69 6E 2E 2E 2E 33 20 45 525..#Spin...3 E\n53 43 73 20 6F 72 20 42 52 45 41 20 74 6F 20 61 SCs or BREA to a\n62 6F 72 74 2E 3A 29 0A 0D 50 69 63 63 6F 6C 6F bort.:)..Piccolo\n20 4C 61 73 65 72 20 44 69 73 74 61 63 65 20 53  Laser Distace S\n63 61 6E 6E 65 72 0D 43 6F 70 79 72 69 67 68 74 canner.Copyright\n20 63 29 20 32 30 30 39 2D 32 30 31 20 4E 65 61  c) 2009-201 Nea\n74 6F 20 52 6F 6F 74 69 63 73 2C 20 49 6E 63 2E to Rootics, Inc.\n", "rostopic hz /scan\n", "rostopic echo /scan\n", "rostopic list\n", "/rosout\n/rosout_agg\n/rpms\n/scan\n"], "url": "https://answers.ros.org/question/225725/xv11-lidar-on-jade/"},
{"title": "RViz ignores COLLADA mesh colours", "time": "2014-12-03 22:12:26 -0600", "post_content": [" ", " ", "I have a COLLADA (.dae) file that I have exported from blender. Its just a cube that has been shaded green. ", "However, whenever I try to get rviz to display this mesh (using a Marker display) all I get is a white cube.", "I have set the marker colours to RGBA = {0, 0, 0, 0} in the message. I have set /mesh_use_embedded_materials to true. I read somewhere that if there is an ambient colour in the mesh file with one of its components set to 0 that rviz wont render it properly, I checked that and there were no zeros.", " has the mesh file and screenshots from blender and rviz.", "Can someone tell me how to get rviz to render my mesh using the coulours specified in the mesh file? ", "I am using the lastest version of Blender (2.72, downloaded just the other day). I am running Ubuntu 14.04 and ROS Indigo 1.11.9", "Bidski"], "answer": [" ", " ", " ", " ", "Blender export materials with a null ambient lightning, this is probably why your meshes don't look like expected.", "I do not know what to change in the Blender material to set the ambient so I manually edit the exported ", " files; Replace", "with", "Here is a script that copies the diffuse color to the ambient color, first edit a new script file (this is for Nemo, not Nautilus):", "Use the following script:", "Make the script executable:", "See also ", ".", " ", " ", "Possibly related: ", ". Also: ", "."], "answer_code": ["dae", "<ambient>\n  <color sid=\"ambient\">0 0 0 1</color>\n</ambient>\n", "<ambient>\n  <color sid=\"ambient\">0.1 0.1 0.1 1</color>\n</ambient>\n", "gedit ~/.local/share/nemo/scripts/RViz\\ collada\n", "#!/bin/bash\n\n# Check if xml-twig-tools is installed\npkg_ok=$(dpkg-query -W --showformat='${Status}\\n' xml-twig-tools|grep \"install ok installed\")\nif [ \"\" == \"$pkg_ok\" ]; then\n    notify-send \"Missing package\" -t 1 \"Please install xml-twig-tools\"\n    exit 0\nfi\n\ncount=0\n\n# Reads Nautilus / nemo arguments to detect which file have to processed\n# Processes only selected files with .dae extension (case insensitive)\nfor file in $(find ${NEMO_SCRIPT_SELECTED_FILE_PATHS} -iname '*.dae'); do\n    # Fetches all \"diffuse\" properties from materials accross the file into a single string\n    diffuse_properties=`xml_grep 'diffuse' $file --text_only`\n\n    # Splits diffuse_properties string into an array of strings\n    readarray -t properties_array <<<\"$diffuse_properties\"\n\n    # Replaces only the first occurence in the file each time\n    for element in \"${properties_array[@]}\"\n    do\n        sed -i \"0,/ambient\\\">0\\ 0\\ 0\\ 1/s//ambient\\\">$element/\" $file\n    done\n\n    ((count++))\ndone\n\n# Notify user\nnotify-send \"RViz collada\" -t 1 \"$count files tweaked\"\nexit 0\n", "chmod +x ~/.local/share/nemo/scripts/*\n"], "url": "https://answers.ros.org/question/198860/rviz-ignores-collada-mesh-colours/"},
{"title": "Launch files: How to call rosaria and transforms", "time": "2016-02-26 04:02:20 -0600", "post_content": [" ", " ", "Hi all,", "I created a launch file to perform autonomous navigation with the robot. The launch file's contents can be seen below:", "To launch rosaria and transforms, I usually run the following commands succesfully in 2 separate terminals:", "However, when I try to run rosaria using the launch file mentioned, I get the following error:", "ERROR: cannot launch node of type\n  [rosaria/RosAria]: rosaria ROS path\n  [0]=/opt/ros/hydro/share/ros ROS path\n  [1]=/home/powerbot67/ros/eband_local_planner\n  ROS path\n  [2]=/home/powerbot67/ros/sbpl_lattice_planner\n  ROS path\n  [3]=/home/powerbot67/hydro/sandbox ROS\n  path [4]=/opt/ros ROS path\n  [5]=/opt/ros/hydro/share ROS path\n  [6]=/opt/ros/hydro/stacks", "Any ideas as to why this is happening? How should I call transforms in this launch file? I would appreciate any help. ", "Thanks."], "answer": [" ", " ", "Hello, it looks like roslaunch is unable to find the RosAria node in its ROS search path.  Did you source ", " before running roslaunch?  (Same as you do before using rosrun)", "Thanks for your reply. What should I add to the launch file in order to source devel/setup.bash ?", "Just in case, someone has the same problem: Adding the launch file to the catkin_ws folder and then typing the following commands solves the issue:", "cd ~/catkin_ws\nsource ./devel/setup.bash\nroslaunch Navigation.launch", "Thanks all", " That's not the best way to go about it. It's pretty common to source the workspace from your ", " file then use ", " to launch it from the package. If you do things differently from the \"ROS way\" then you start making it harder to share your work and for others to use it."], "question_code": ["###launching rviz###\n  <node name=\"rviz\" pkg=\"rviz\" type=\"rviz\" />\n\n###start laser###\n  <node pkg=\"sicktoolbox_wrapper\" type=\"sicklms\" name=\"sicklms\" output=\"screen\">\n  <param name=\"port\" value=\"/dev/ttyS1\"/>\n  <param name=\"inverted\" value=\"true\"/>\n  </node>\n\n###get map###\n  <node pkg=\"map_server\" type=\"map_server\" name=\"map_server\" args=\"/home/powerbot67/FacultyGNDFloorMapDan.yaml\"/>\n\n###RosAria###\n  <node pkg=\"rosaria\" type=\"RosAria\" name=\"RosAria\" output=\"screen\" args=\"/home/powerbot67/catkin_ws/src\" >\n  <param name=\"port\" value=\"/dev/ttyS0\"/>\n  <param name=\"DriftFactor\" value=\"-20\"/>\n  <param name=\"TicksMM\" value=\"115\"/>\n  </node>\n\n</launch>\n", "cd ~/catkin_ws\nsource ./devel/setup.bash\nrosrun rosaria RosAria _port:=/dev/ttyS0 _DriftFactor:=-20 _TicksMM:=115\n\ncd ~/transform_ws\nsource ./devel/setup.bash\nrosrun transform transform\n"], "answer_code": ["devel/setup.bash", "~/.bashrc", "roslaunch"], "url": "https://answers.ros.org/question/227649/launch-files-how-to-call-rosaria-and-transforms/"},
{"title": "How to install eband_local_planner?", "time": "2016-02-19 05:13:14 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I would appreciate it if someone can guide me on how to install eband_local_planner for use with move_base on Hydro because every approach that I have tried so far, has failed.", "Thanks", "I installed eband_local_planner by using the following commands:", "I changed the move_base.launch file to use eband_local planner as follows:", "However, when I run move_base, I get the following error:", "[FATAL] [1455878479.341080841]: Failed\n  to create the\n  eband_local_planner/EBandPlannerROS\n  planner, are you sure it is properly\n  registered and that the containing\n  library is built? Exception: Could not\n  find library corresponding to plugin\n  eband_local_planner/EBandPlannerROS.\n  Make sure the plugin description XML\n  file has the correct name of the\n  library and that the library actually\n  exists. [move_base-1] process has died\n  [pid 5295, exit code 1, cmd\n  /opt/ros/hydro/lib/move_base/move_base\n  cmd_vel:=/RosAria/cmd_vel\n  __name:=move_base __log:=/home/powerbot67/.ros/log/e58dae7e-d6f1-11e5-a32e-00026f87bbb1/move_base-1.log].\n  log file:\n  /home/powerbot67/.ros/log/e58dae7e-d6f1-11e5-a32e-00026f87bbb1/move_base-1*.log", "Also, running roswtf gives the following:", "What should I do to solve this?", "Thanks", "ros version?", "ROS version is Hydro", "it looks like eband_local_planner is a catkin package. Why are you trying to build it with rosmake?", "I am not sure when to use rosmake but your answer to my other question installed eband_local_planner successfully. Thanks"], "answer": [" ", " ", "eband_local_planner is marked as \"released\" on ", ". This means that you should be able to install it from debs with:", "Thank you!", " ", " ", "Are you passing base_local_planner parameter correctly? \nTry passing base_local_planner parameter inside node tag.  ", "This is my launch file, and eband_local_planner is working properly. ", "I am passing the parameter as follows below and I think that now it being launched correctly. The problem was with installing eband_local_planner correctly. However, thanks for your answer."], "question_code": ["rosws set eband_local_planner https://github.com/utexas-bwi/eband_local_planner --git\nrosws eband_local_planner\nroscd eband_local_planner\nrosmake\n", "powerbot67@powerbot67-desktop:~$ printenv | grep ROS\nROS_ROOT=/opt/ros/hydro/share/ros\nROS_PACKAGE_PATH=/home/powerbot67/ros/sbpl_lattice_planner:/home/powerbot67/ros/eband_local_planner:/home/powerbot67/hydro/sandbox:/opt/ros:/opt/ros/hydro/share:/opt/ros/hydro/stacks\nROS_MASTER_URI=http://localhost:11311\nROS_WORKSPACE=/home/powerbot67/ros\nROSLISP_PACKAGE_DIRECTORIES=\nROS_DISTRO=hydro\nROS_ETC_DIR=/opt/ros/hydro/etc/ros\n", "<arg name=\"base_local_planner\" default=\"eband_local_planner/EBandPlannerROS\"/>\n", "ERROR Not all paths in ROS_PACKAGE_PATH [/home/powerbot67/ros/sbpl_lattice_planner:/home/powerbot67/ros/eband_local_planner:/home/powerbot67/hydro/sandbox:/opt/ros:/opt/ros/hydro/share:/opt/ros/hydro/stacks] point to an existing directory: \n * /home/powerbot67/ros/sbpl_lattice_planner\n"], "answer_code": ["sudo apt-get install ros-hydro-eband-local-planner\n", "<node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n  <rosparam file=\"$(find car_2dnav)/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\"/>\n  <rosparam param=\"base_local_planner\">eband_local_planner/EBandPlannerROS</rosparam>      \n</node>\n", "<arg name=\"base_local_planner\" default=\"eband_local_planner/EBandPlannerROS\"/>\n"], "url": "https://answers.ros.org/question/227098/how-to-install-eband_local_planner/"},
{"title": "Sending hil_gps msg with mavros", "time": "2016-03-14 12:09:20 -0600", "post_content": [" ", " ", "Hello,", "I'm trying to build indoor system with ultrasonic sensors.\nI'm substitute mocap msgs by publishing correct data to /mavros/mocap/pose and /tf , pixhawk accepts it but led won't go in green. \nSo as I understand for flying indoor I also need to give fake gps data? As I undestand I can do it with sending HIL_GPS message. Is it possibly to send this msg with python+mavros? If yes, where I can learn how to do it? Any example?", "P.S. I'm not sure but mby its possible to fly indoor without faking gps? If so, can you help me with that?", "Thanks in advance,\nxFirefly", "Hi ", ",\nDid you manage to create fake gps data?"], "answer": [], "url": "https://answers.ros.org/question/229035/sending-hil_gps-msg-with-mavros/"},
{"title": "Move_base: Why is the robot still using recovery behaviors if they are turned off?", "time": "2016-03-23 16:04:31 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "The current move_base launch file that I am using has the following parameters set as follows:", "The wiki page for move_base ", " states the following:", "These recovery behaviors can be\n  configured using the\n  recovery_behaviors parameter, and\n  disabled using the\n  recovery_behavior_enabled parameter.", "So why does the robot still perform clearing rotations if the related parameters are disabled?", "Thanks", "move_base launch file:", "Do a ", " and then 2x ", " after launching your system please. Just to confirm that the two parameters end up being set properly. Another issue might be that they're set, but in a different namespace.", "I will check it out. Thanks!", "are you running catkin_make or catkin_make install after changing those parameters? It could also be that your launch files are being read from /devel and not from /src", "Thanks for your reply ", " No, I am not running either of catkin_make or catkin_make install. Is this always required after changing a parameter? I was only pressing 'save' after changing parameters in the launch file. I assumed that this is enough to affect a change in parameter. Thanks again", " Could you update your question with the entire section of your launch file that runs and parametrizes ", " please? Also, did you check using ", "? What's the verdict?", "rosparam get shows the parameters set correctly : powerbot67@powerbot67-desktop:~/catkin_ws$ rosparam get /move_base/recovery_behavior_enabled\nfalse\npowerbot67@powerbot67-desktop:~/catkin_ws$ rosparam get /move_base/clearing_rotation_allowed\nfalse", " I edited the question to include move_base.launch", "Hmm, in terms of recovery behaviors, the only difference between your setup and mine seems to be that I'm loading those two params from a YAML file. But that shouldn't matter at all ofc. Another difference is that I do provide the list ", " in addition to setting the params to false."], "answer": [], "question_code": ["<param name=\"recovery_behavior_enabled\" value=\"false\"/>\n<param name=\"clearing_rotation_allowed\" value=\"false\"/>\n", "<launch>\n  <master auto=\"start\"/>\n\n  <arg name=\"no_static_map\" default=\"false\"/>\n\n  <arg name=\"base_global_planner\" default=\"navfn/NavfnROS\"/> \n  <!--<arg name=\"base_global_planner\" default=\"carrot_planner/CarrotPlanner\"/>-->\n\n  <arg name=\"base_local_planner\" default=\"base_local_planner/TrajectoryPlannerROS\"/>\n\n  <!--<arg name=\"base_local_planner\" default=\"dwa_local_planner/DWAPlannerROS\"/>-->\n\n  <!--<arg name=\"base_local_planner\" default=\"eband_local_planner/EBandPlannerROS\"/>-->\n\n\n  <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/local_costmap_params.yaml\" command=\"load\" />\n\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/global_costmap_params.yaml\" command=\"load\" />\n\n    <rosparam file=\"/home/powerbot67/navigation_powerbot/src/powerbot_2dnav/config/base_local_planner_params.yaml\" command=\"load\" />\n\n    <param name=\"base_global_planner\" value=\"$(arg base_global_planner)\"/>\n    <param name=\"base_local_planner\" value=\"$(arg base_local_planner)\"/>  \n    <param name=\"recovery_behavior_enabled\" value=\"false\"/> \n    <param name=\"recovery_behaviour_enabled\" value=\"false\"/> \n    <param name=\"clearing_rotation_allowed\" value=\"false\"/>\n    <param name=\"planner_frequency\" value=\"40.0\"/> \n\n    <!--<param name=\"controller_frequency\" value=\"10.0\"/> --> \n    <!--<param name=\"planner_patience\" value=\"40.0\"/> -->\n    <!--<param name=\"controller_patience\" value=\"40.0\"/> -->\n\n    <!--<param name=\"oscillation_timeout\" value=\"40\"/> -->\n\n    <!-- Remap into namespace for cmd_vel_mux switching-->\n    <remap from=\"cmd_vel\" to=\"/RosAria/cmd_vel\" />\n  </node>\n</launch>\n", "rosparam list", "rosparam get ...", "move_base", "rosparam", "recovery_behaviors"], "url": "https://answers.ros.org/question/230003/move_base-why-is-the-robot-still-using-recovery-behaviors-if-they-are-turned-off/"},
{"title": "Intel J1900 or N3150 for kinect?", "time": "2016-04-17 15:07:33 -0600", "post_content": [" ", " ", "Hi, \nIm building a small mobile robot with a kinect (for visual odometry  and possible SLAM). On the platform i need a suitable computer. My initial thought was to use a raspberry pi 3, but it seems to be a little bit under powered. Now I have found some mini-ITX SoC motherbords with intel celeron J1900 and N3150 processors. Do you guys think they are powerful enough? (i will have a workstation for teleop/visualization that could be used to lower the load of the on board processor). ", "Specs:", "J1900 (Bay Trail from 2013),\nquad core 2.0GHz (2.4Ghz),\ngpu: 688MHz (4 units),\nmem: 1333 SO-DIMM,\n10W", "N3150 (Braswell from 2015),\nquad core 1.6Ghz (2.1Ghz),\ngpu: 320MHz (12 units),\nmem: 1600 DIMM\n6W"], "answer": [], "url": "https://answers.ros.org/question/232109/intel-j1900-or-n3150-for-kinect/"},
{"title": "3D obstacles persist if there's nothing but free space behind them", "time": "2016-04-04 18:06:56 -0600", "post_content": [" ", " ", "What's an algorithm or technique to solve this problem?", "A 3D obstacle persists if there's lots of free space behind it when it moves away. I've observed this problem using costmap_2d's VoxelLayer plugin on ROS Jade. ", "Here's a side view of a ToF camera pointed at a green obstacle. The red triangle is the field of view, and the red dots are points returned by the sensor.", "VoxelLayer will mark voxels as shown below. Green is free, yellow is occupied, and white is unknown. Here it correctly sees the green box as an obstacle.\n", "Now imagine the green box moves out of the way vertically or sideways out of your screen. Behind the box is nothing but free space, so no points beyond it are returned by the ToF camera. Voxels need points behind them to be cleared, so the robot mistakenly believes an obstacle is still there.\n", "The phantom obstacle is a problem, but I can't really fault VoxelLayer's logic. What if some sensor malfunction prevented the ToF camera from getting points on the box? How can I tell the difference between free space and an invalid sensor reading?", "This is a problem I'm struggeling with as well. So I cannot give any hints.", "However: Thank you for this nice question! Very nice put and comprehensive!"], "answer": [" ", " ", "I had a similar issue, but not quite the same as you. I will not offer you a solution, but rather some links that may shed some light on your problems...", " "], "url": "https://answers.ros.org/question/230999/3d-obstacles-persist-if-theres-nothing-but-free-space-behind-them/"},
{"title": "How to send velocity to pixhawk with Mavros?", "time": "2015-04-13 19:15:02 -0600", "post_content": [" ", " ", "Hi guys,", "after searching a lot and don't get a solution I would like a help. I am a new user of Mavros package and I have some doubts about it. My setup for tests is:", "1 Pixhawk with ArduCopter V3.2.1Quad firmware, 1 ESC 20A SkyWalker, 1 motor, 1 GPS+Compass, 1 radio receiver and 1 battery. I installed mavros on Indigo and ubuntu 14.04.", "I want to start sending a simple command of linear velocity to pixhawk, for example vx = 1 m/s. ", "I typed the commands bellow in sequence:", "1) sudo chmod 666 /dev/ttyACM0", "2) roslaunch mavros px4.launch", "3) rosrun mavros mavsafety arm", "4) and my code: rosrun copter velocity", "After that nothing happened. The the motor spun with default velocity of the node mavsafety arm and not with desired velocity.", "What topic should I publish this message?", "I checked the topic /mavros/setpoint_attitude/cmd_vel and the value that I set in the source code was there.", "So, where is the mistake? Should I do some setup before to run?", "I read at some forum people talking about some setup that we have to do...but is not clear for me.", "Can anyone show a piece of code that make what I want or give me a tips?", "Thank you!", "I wrote a code trying to do this but without successful."], "answer": [" ", " ", "I did which you suggest, ", "2) roslaunch mavros apm.launch", "3) rosrun mavros mavsafety arm", "4) rosrun mavros mavsys mode -c GUIDED\n    that resulted: Mode changed", "5) and ran my code (with just yaw rotation -> twist.angular.z)", "but nothing happened. The motor is spinning with default velocity when the pixhawk is armed.", "The topic that am I publishing is correct? (/mavros/setpoint_attitude/cmd_vel)", "Topic correct (you may check by ", "). Also try position sp. Note that SP support is new in APM, not sure that it exist in latest stable release.", " ", " ", "may be you should try: \n/mavros/setpoint_velocity/cmd_vel (geometry_msgs/TwistStamped)", "Velocity setpoint.", " ", " ", "I've been trying to send yaw (radians/degrees) to rotate the UAV Quadcompter using ros + mavros + SITL(to connect fcu with mavproxy), I now can read that apm is not supported yet, however, I used roslaunch mavros px4.launch fcu_url ...  and nothing different append. The version that I use is mavros 0.16.3, I've armed, changed to guided mode and takeoff. The success that I have until today is related with moving the quadcopter in the sitl to go forward, go back, left, etc-- all before is having a publisher in /mavros/setpoint_velocity/cmd_vel.. ", "Is there anyway  to rotate the quadcopter in the desired degrees keeping the same alttitude? ", "Thanks. ", " ", " ", "2) You should use ", " , did you read:   ", "  ? ", "After arming setpoint also require enabling specific mode, ", " for APM and ", " for PX4.", "Next: velocity only supports YAW rotation (", "), read source it's very simple: ", ".", " ", " ", "I am not expert in offboard mode. I have the same problem with this attitude topic   I just want some help please. and I am using ROS indigo.", "As I can see the topic /mavros/setpoint_velocity/cmd_vel   generates a change in velocity in the motors and seem works,\nbut when I publish  /mavros/setpoint_attitude/cmd_vel  I can't observe any change only in the value of the topic.\nThe OFFBOARD mode is not not refusing my command.\nI try using:\nmsg.twist.linear.x = 100   and other different values\nalso I try with msg.twist.angular.z =100  but none works\nAnybody knows what could be the mistake and how to update this in order to publish velocity commands?", "I don't see support for SET_ATTITUDE_TARGET in APM code. Or you used PX4?", "I am using PX4.   I want to move linear using a velocity command and to make a short rotation with a fraction of \\pi. for example pi/6 CCW. I don't know what message could be appropiate", "I am able to send commands with /mavros/setpoint_velocity/cmd_vel but I am not sure of  behaviour flying. \nThanks", "As i know currently velocity sp's broken in PX4 master. Also note that attitude twist only uses angular part.", "Excuse me,  do you know if exist any way of generate linear velocity commands in x and y with mavros and pixhawk with px4. \nIf the velocity sp's are broken, Do I have to change the firmware? or is a problem with mavros?\n How could I get some help with this problem? please", "Try to update to latest firmware. PX4 development is very fast, so probably sp handler are fixed.", "Thanks, I update the firmware, but I would like to take off my quadrotor and after do a linear command.  The first stage is done with set position (it is working ok) and after that I am trying to use a linear velocity command, but I am not sure if the actual sp allow to do that now?", "Don't mix setpoint types, it may cause errors.", "ok I will. Some questions please.\nDo you know the units of linear in this kind of commands msg.twist.linear.y?\nI am trying to taking off by using the sp velocity but nothing any idea?\nDo you know how to reduce the max velocity in position sp's?, because the changes of positions are too strong\nThanks"], "question_code": ["#include <ros/ros.h>\n#include <std_msgs/String.h> \n#include <stdio.h>\n#include \"geometry_msgs/TwistStamped.h\"\n#include \"geometry_msgs/Vector3Stamped.h\"\n\nint main(int argc, char **argv)\n{\n   ros::init(argc, argv, \"comun\");\n   ros::NodeHandle n;\n\n   ros::Publisher chatter_pub = n.advertise<geometry_msgs::TwistStamped>(\"/mavros/setpoint_attitude/cmd_vel\",100);\n   ros::Rate loop_rate(10);\n\n   geometry_msgs::TwistStamped msg;\n\n   while(ros::ok()){\n       msg.header.stamp = ros::Time::now();\n       msg.header.seq=1;\n       msg.twist.linear.x = 1;\n       msg.twist.angular.x = 1;\n\n       chatter_pub.publish(msg);\n       ros::spinOnce();\n       loop_rate.sleep();\n   }    \n   return 0;\n}\n"], "answer_code": ["rqt_graph", "apm.launch", "GUIDED", "OFFBOARD", " rosrun mavros mavsys mode -c GUIDED\n", "twist.angular.z"], "url": "https://answers.ros.org/question/207097/how-to-send-velocity-to-pixhawk-with-mavros/"},
{"title": "Point Grey Grasshopper3 Image looks all Green", "time": "2016-03-15 16:14:16 -0600", "post_content": [" ", " ", "I'm having an issue running ", " from the ", " package. When I launch try to view the color image ", "  in either image_view or rviz the image looks all green. This is due to the published image advertising its encoding as ", " when in reality it appears to be ", ". I wrote a quick republisher that just changes the encoding string to ", " and everything shows up fine when I view the image in ", ". What is going wrong here, and what do I need to change to get it working correctly? "], "answer": [], "question_code": ["camera.launch", "pointgrey_camera_driver", "/camera/image_color", "bgr8", "rgb8", "rgb8", "image_view"], "url": "https://answers.ros.org/question/229167/point-grey-grasshopper3-image-looks-all-green/"},
{"title": "Arbotix \"[Errno 11] Resource temporarily unavailable\"", "time": "2015-06-14 03:16:23 -0600", "post_content": [" ", " ", "Hello all - newbie here", " This weekend I've been trying to put together a turtlebot_arm live on a turtlebot using a PhantomX Pincher. I've been using the latest forks as far as I can tell -  ", "  and  ", "  - as well as some of the advice here  ", "I managed to get the system planning & executing with MoveIt and rviz with the arm floating in space, but as I was going back through the URDFs to clean up and attach the arm to the turtlebot, I've happened upon an error that's blocking comms and I can't find it anywhere online, when I run my modified turtlebot_bringup minimal.launch: \"[Errno 11] Resource temporarily unavailable.\"", "This error is shown right after the \"ArbotiX connected\" output of minimal.launch which would normally signal all OK to roslaunch turtlebot_arm_moveit_config and rviz.", "It sounds like a hardware error but restarting/powering down the turtlebot PC and arbotix don't help, and my USB chmod 777 is fine as the Arbotix does connect before hitting the error.", "Has anyone experienced this before?", "Cheers", "Will", "How are you powering the Pincher?  As a test, you might try using a wall-wart to power it, just to eliminate any possible power issues.  Also, it would be great if you can publish notes on this.  I think you will be the first to mount the Pincher on the Turtlebot.", "Thanks corb. I'm using a wall wart already, still waiting for my connectors to arrive for onboard power out here in Singapore! The problem started when I moved from turtlebot_arm to the corot fork so I will revert and try to trace the problem. Once I have a working solution I'm happy to detail here", "Deleted and remade packages - problem solved.", "Problem remains - swapping USB ports, powering on & off and use of chmod sometimes fixes it until the next session. Could be a driver problem - will continue to investigate. Anybody else having Errno 11 problems with a PhantomX pincher & ROS Indigo?"], "answer": [" ", " ", "Think I've managed to solve this - it has to do with whether the FTDI cable to the Arbotix is detected as USB0 or USB1. Once I started ensuring the Kobuki was plugged in after the FTDI, I stopped having the error.", "Perhaps you can use some suitable ", " to make this easier to manage.", "Good knowledge, thanks gvdhoorn - will do. Sometimes this error also seems to present as \"maximum recursion depth exceeded\" for some reason. If anyone else has this issue the above also works as a fix for me."], "url": "https://answers.ros.org/question/211259/arbotix-errno-11-resource-temporarily-unavailable/"},
{"title": "teb_local_planner not work", "time": "2016-03-30 21:58:22 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "My pc can run movebase (navfn + dwa) ,(voronoi + dwa)", "But when comes to the (navfn + teb_local_planner) on the offline rviz simulation", "the behavior of my virtual turtle bot is quite odd,", "But I guess the root cause is lack of computational power:", "The longest control cycle can reach about 300s with teb O_o", "But with dwa_local_planner, my control frequency can reach 100Hz on the same static map.", "And I am following the [teb tutorial]...(http://wiki.ros.org/teb_local_planner/Tutorials/Configure%20and%20run%20Robot%20Navigation)", "Anyone can help?", "Thanks!", "costmap_common_params.yam:", "local_costmap_params.yaml", "global_costmap_params.yaml", "feb_local_planner_params.yaml", "map.yaml", "Hi,\na control cycle with 300s is way too much ;-) Obviously, there must be some issue.\nEven on our 7-10 years old computers, the planner works within a 10Hz control cycle (for a limited, but reasonable local costmap size).", "Can you please provide me with more information about your setup and configuration?\nDid you try the first tutorial: ", "\nHow is your performance here?", "Hi Croesmann, I tried the 1st tutorial, and my performance looks ok with all default settings.\nand ", " shows the problem I asked. also recorded with all default settings.", "thank you for posting the video. The behavior looks indeed quite ugly. Could you please provide me with your navigation configuration files (you told that you are using the default teb paramaters, but what's about costmap parameters, move_base parameters, ...)?", "Hi Croesmann, ", " shows the comparison among 3 sets of different navigation settings on my PC .And I updated the post with my nav cfgs used in the previous video.Thanks for your help."], "answer": [" ", " ", "Thank you for updating your question with content of your parameter files and  for posting the video.", "After investigating your parameters, I think that your local costmap configuration causes the high computation time.\nYour local costmap consist of a static map (", ") which is not recommended.\nIn that case the ", " and ", " parameters are ignored, leading to a local costmap representing your whole environment.\nThe ", " treats each occupied costmap cell as single point obstacle (if no costmap conversion plugin is activated, which is the default).\nWith that in mind, it is not surprising that the performance is poor, since each obstacle causes a huge amount of distance calculations during optimization.", "My current local costmap configuration contains the following parameters:", "I hope this resolves your issue with huge computation times.\nIf you still have problems with your performance afterwards, there are other parameters that influence the computational resources (number of optimization iterations, area/distance behind the robot that should be checked for obstacles, temporal trajectory resolution, ...).\nPlease let me know in that case. Some hints are also given in the tutorials.", "PS: A local costmap resolution of 0.025 is not feasible for the planner on most computers, but it is also not necessary.\nI am running the planner with a resolution of 0.1 (similar to your 3rd example in the video).", "PPS: You might try out the new version 0.3 of the planner. Until it is available in the official repos, you can checkout the ", " \nof the github repository. Parameters and Tutorials are already updated to be compliant with version 0.3 (some minor parameter changes and a new tutorial).", "With the modification of local costmap yaml file, now my virtual turtle bot is much swifter, Thank you! Crosemann."], "question_code": ["obstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[0.215, 0.215], [-0.215, 0.215], [-0.215, -0.215], [0.215, -0.215]]\nfootprint_inflation: 0.6\n# robot_radius: 0.24\ninflation_radius: 0.90\nmax_obstacle_height: 0.6\nmin_obstacle_height: 0.0\nobservation_sources: scan\nscan: {data_type: LaserScan, topic: /scan, marking: true, clearing: true, expected_update_rate: 0}\n", "   local_costmap:\n   global_frame: /odom\n   robot_base_frame: /base_link\n   update_frequency: 5.0\n   publish_frequency: 0.2\n   static_map: true\n   rolling_window: false\n   width: 6.0\n   height: 6.0\n   resolution: 0.08\n   transform_tolerance: 1.0\n", "global_costmap:\n   global_frame: /map\n   robot_base_frame: /base_footprint\n   update_frequency: 1.0\n   publish_frequency: 1.0\n   static_map: true\n   rolling_window: false\n   resolution: 0.08\n   transform_tolerance: 1.0\n   map_type: costmap\n", "TebLocalPlannerROS:\n\n odom_topic: odom\n map_frame: /map # This parameter will be overridden by local_costmap/global_frame \n\n # Trajectory\n\n teb_autosize: True\n dt_ref: 0.3\n dt_hysteresis: 0.1\n global_plan_overwrite_orientation: True\n\n # Robot\n\n max_vel_x: 0.4\n max_vel_x_backwards: 0.2\n max_vel_theta: 0.3\n acc_lim_x: 0.5\n acc_lim_theta: 0.5\n\n # GoalTolerance\n\n xy_goal_tolerance: 0.2\n yaw_goal_tolerance: 0.1\n free_goal_vel: False\n\n # Obstacles\n\n min_obstacle_dist: 0.5\n costmap_emergency_stop_dist: 0.3\n include_costmap_obstacles: True\n costmap_obstacles_front_only: True\n obstacle_poses_affected: 10\n line_obstacle_poses_affected: 25\n polygon_obstacle_poses_affected: 25\n\n # Optimization\n\n no_inner_iterations: 5\n no_outer_iterations: 4\n optimization_activate: True\n optimization_verbose: False\n penalty_scale: 0.1\n penalty_epsilon: 0.1\n weight_max_vel_x: 2\n weight_max_vel_theta: 1\n weight_acc_lim_x: 1\n weight_acc_lim_theta: 1\n weight_kinematics_nh: 1000\n weight_kinematics_forward_drive: 1\n weight_optimaltime: 1\n weight_point_obstacle: 50\n weight_line_obstacle: 50\n weight_poly_obstacle: 50\n\n # Parallel Planning in distinctive Topologies\n\n enable_homotopy_class_planning: True\n enable_multithreading: True\n simple_exploration: False\n max_number_classes: 5\n roadmap_graph_no_samples: 15\n roadmap_graph_area_width: 6\n h_signature_prescaler: 0.5\n h_signature_threshold: 0.01\n obstacle_keypoint_offset: 0.1\n obstacle_heading_threshold: 0.45\n visualize_hc_graph: False\n", "image: map_corner_v3.pgm\nresolution: 0.025000\n#or resolution: 0.01000\norigin: [-11.000000, -10.000000, 0.000000]\n#or origin: [-5.000000, -5.000000, 0.000000]\nnegate: 0\noccupied_thresh: 0.65\nfree_thresh: 0.196\n"], "answer_code": ["static map: true", "width", "height", "static_map: false\nrolling_window: true\nwidth: 5.5\nheight: 5.5\nresolution: 0.1\n"], "url": "https://answers.ros.org/question/230601/teb_local_planner-not-work/"},
{"title": "gazebo camera frame is inconsistent with rviz + opencv convention", "time": "2016-04-21 13:23:37 -0600", "post_content": [" ", " ", " ", " ", "It looks like the gazebo camera frame convention is not the same as rviz and opencv, which the image below shows", "In opencv, z is pointing into the image (the blue axis), x is right (the red axis), and y is down (green axis), while in the gazebo camera x is pointing into the image and z is up, y is right which is similar to the robot convention of x being forward and z up.", "The image above is using an rviz/Camera to overlay the purple grid on the frame generated from the gazebo camera plugin, instead of the grid overlaying properly on the ground and going to toward the horizon rviz thinks the camera is pointed at the ground.", "This example is running the gazebo_ros_demos rrbot_gazebo and rrbot_control launch files, and using standard Ubuntu 14.04 + Jade packages.", " I cross posted  ", "  - or is it the fault of rviz/Camera and opencv, every node calling opencv camera projection functions should rotate first?  Or every node on either side should have options to support either frame?  (Or do options exist already and I've missed them?) ", "My short term solution is going to be to republish every frame out of gazebo with a rotated camera frame in the header (and the urdf/xacro can create the corrected frame, or it could be sent to tf from the same republishing node)."], "answer": [" ", " ", " ", " ", "The xacro needs to create the optical frame like this, and the sensor uses it for frameName:", "This shows the correctly generated optical frame- the Camera overlay RobotModel arm is seamless with the gazebo camera image:", " There is a PR for gazebo_ros_demos to get this fix in, since that is where the basic gazebo + ros tutorial points it really should be working correctly.\n ", "It's possible the other gazebo ros sensors need to be handled similarly (but maybe the depth sensors had this solved within the plugin?).", "Thanks a lot! This fix works very fine.", " ", " ", "I think ROS (rviz + rqt) are doing what they say they should:", " From  ", " : ", " You might consider posting a link to this and/or the GitHub issue on  ", " . ", "It looks like it is up to the user to supply the optical frame in ", ", and then they can generate the tf from the urdf or with their own tf transform send- and then it is up to the user to generate it correctly, I think instead the plugin should do it itself.", " ", " ", "As ", " pointed out, it is up to the creator of the robot model to create the correct frames according to ROS conventions. Note this can be done within a xacro macro and this macro can then be instantiated easily in a model. For instance, the ", " macro can be used in models easily (", "):"], "answer_code": ["  <!-- generate an optical frame \n      http://www.ros.org/reps/rep-0103.html#suffix-frames\n      so that ros and opencv can operate on the camera frame correctly \n      -->\n  <joint name=\"camera_optical_joint\" type=\"fixed\">\n    <!-- these values have to be these values otherwise the gazebo camera\n        image won't be aligned properly with the frame it is supposedly\n        originating from -->\n    <origin xyz=\"0 0 0\" rpy=\"${-pi/2} 0 ${-pi/2}\"/>\n    <parent link=\"camera_link\"/>\n    <child link=\"camera_link_optical\"/>\n  </joint>\n\n  <link name=\"camera_link_optical\">\n  </link>\n\n\n  <gazebo reference=\"camera_link\">\n    <sensor type=\"camera\" name=\"camera1\">\n      ...\n      <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n        ...\n        <frameName>camera_link_optical</frameName>\n        ...\n", "In the case of cameras, there is often a second frame defined with a \"_optical\" suffix. This uses a slightly different convention:\n\nz forward\nx right\ny down\n", "<frameName>", "<xacro:generic_camera name=\"front_cam\" parent=\"base_link\" ros_topic=\"camera/image\" cam_info_topic=\"camera/camera_info\" update_rate=\"10\" res_x=\"320\" res_y=\"240\" image_format=\"R8G8B8\" hfov=\"90\">\n  <origin xyz=\"0.05 0.0 -0.06\" rpy=\"0 0 0\"/>\n</xacro:generic_camera>\n"], "url": "https://answers.ros.org/question/232534/gazebo-camera-frame-is-inconsistent-with-rviz-opencv-convention/"},
{"title": "ros_canopen did not receive a response message", "time": "2016-05-10 03:02:47 -0600", "post_content": [" ", " ", " ", " ", "Hi, I trying to use ros_canopen to control a robot with canopen motors. I use indigo.", "I run candump in a terminal. When I connect the motor to power,candump receives:", "So I suppose the hardware connection is correct.", "However when I load parameters and then run canopen_motor_node , it gave an error:", "The configuration of CAN bus is simple and is ", ".The eds is provided by Copley.", "I've tried to read the code where produces the \"Did not receive a response message\" but could find out how to solve this problem.", "Any help will be appreciated .Thank you.", "If at all possible, please don't use screenshots of terminal output. It's all text, and can easily be copy/pasted into the question. Images aren't searchable, and if the screenshots ever disappear, this question will be worthless.", "If you decide to update, please ", " your original question.", "Thx for your advice."], "answer": [" ", " ", "The branch of ros_canopen i use is indigo-devel,which i found out very different from indigo_release_candidate.Which one is more recommended?", "indigo-devel is the most recent and recommended version. ", "82 8 30 81 0 0 0 40 0 0", "This is a heatbeat failure. (You're controller waits for a heartbeat message, but you have not configured to send it?)", "boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<std::bad_cast> > std::exception::what: std::bad_cast", "The driver expects another data type (signed char), I guess you're EDS states something else.", "It is really hard to debug if you don't provide your config.\nI guess you have been calling the init service from another shell?", "I have checked your EDS with CANchkEDS, it does not match the standard object types.\nYou should install CANeds (v3.6 works with wine) and resolve all errors.", "Thanks! I will try it. I download this EDS file from ", ". Strange problem that it should not match the standard object types. Do you think this leads to problems that I mentioned above?Thanks.", "i have checked the eds file with CANeds. After I fixed two errors that CANeds gave, I still got the bad_cast. There are some warnings about not clearifying remapping direction of some object. Copley replied that other costumers did well with this eds . And it still \"did not receive response msg\"", "Please update your EDS on github.\nA faulty EDS might work for others, but ", " relies on the type information because of its strict type policy.", "Updated eds file is ", ". The main difference from its original file is the \"EDSVersion=3.0\" is changed to \"EDSVersion=4.0\" and a sub-index data type is modified to conform to it's parent index.", "Besides, AccessType in the eds file is changed into lowercase. (canopen_motor_node doesn't accept uppercase ones)\nE.g. \"AccessType=RW\"  -->  \"AccessType=rw\"\nCANeds gave warnings about not specifying the mapping direction.I think AccessType of mappable object with rw access should change to rwr or rw", "Your EDS still uses wrong types for a number of objects. In CANeds you have to add the 402 database (Ctrl+F2)."], "question_code": ["~/ws $ rosrun socketcan_interface socketcan_dump can0\nERROR: state=1 internal_error=0('OK') asio: system:0\nERROR: state=2 internal_error=0('OK') asio: system:0\ns 702   1 0\ns 82    8 30 81 0 0 0 40 0 0\n", "$ rosrun canopen_motor_node canopen_motor_node \n\n[ WARN] [1462871551.731234857]: Sync overflow was not specified, so overflow is disabled per default\n\nDid not receive a response message\n\n[ERROR] [1462871553.457722242]: \n/home/craig/ws/src/ros_canopen/canopen_master/include/canopen_master/objdict.h(457): Throw in function canopen::ObjectStorage::Entry<T> canopen::ObjectStorage::entry(const canopen::ObjectDict::Key&) [with T = signed char]\nDynamic exception type: boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<std::bad_cast> >\nstd::exception::what: std::bad_cast\n"], "answer_code": ["ros_canopen"], "url": "https://answers.ros.org/question/234019/ros_canopen-did-not-receive-a-response-message/"},
{"title": "rosbridge - communicate with ros using another Laptop", "time": "2016-04-28 06:13:59 -0600", "post_content": [" ", " ", " I am following this example:\n ", "I have two laptops, one is running Ubuntu with ROS; the other is running Windows. What I want to do is to control ROS using a non-ROS platform and I chose to use webpages running on browsers. ", "It works! And green \"connected\" showed up. Actually, both", "and", "works. Successful message of connected shows.", "I tried to use the following code in html. \"192.168.1.109\" is the IP address of the computer which is hosting the rosbridge-server in LAN. The LAN is setup through a TPLINK wireless router.", "I could not get successful connected message. ", " When I typed  ", "  in my browser in Windows, the following info showed up: ", "There is some information showed up! So I guess there is some sort of connection!", "However, I just cannot get the simple.html run right with successful message of \"Connected\".", "Why is that?"], "answer": [" ", " ", "I was using html code from github. It shall be a whole folder but I only used the html file in my PC. ", "The 6th line of simple.html is:", "The solution is to replaced it with", "I found the bug using debug tool provided by Chrome. Apparently I am not familiar with web programming at all."], "question_code": ["ros.connect('ws://192.168.1.109:9090/');\n", "ros.connect('ws://localhost:9090/');\n", "ros.connect('ws://192.168.1.109:9090/');\n", "Can \"Upgrade\" only to \"WebSocket\".\n"], "answer_code": ["<script src=\"../build/roslib.js\"></script>\n", "<script src=\"http://cdn.robotwebtools.org/roslibjs/current/roslib.min.js\"></script>\n"], "url": "https://answers.ros.org/question/233154/rosbridge-communicate-with-ros-using-another-laptop/"},
{"title": "Navigation with Clearpath Husky", "time": "2016-05-19 20:52:38 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "I am currently working on getting a Husky to navigate to GPS coordinates.   I was able to do a simple version by writing a simple script that subscribes to GPS and magnetometer data and publishes velocity commands.  Now I need to take advantage of the navigation stack to implement obstacle avoidance and more robust navigation, but I'm honestly at a loss. I have read through the navigation tutorial, and they lose me when they say:", "catkin_create_pkg my_robot_name_2dnav move_base my_tf_configuration_dep my_odom_configuration_dep my_sensor_configuration_dep", "I understand conceptually what it means. I just don't know what to put there. The same goes for making the launch file.  Currently the husky publishes these topics:", "/diagnostics ", "\n  /diagnostics_agg ", "\n  /diagnostics_toplevel_state", "\n  /gps/fix", "\n  /gps/nmea_sentence", "\n  /gps/nmea_sentence_out", "\n  /gps/time_reference", "\n  /gps/vel", "\n  /imu/data", "\n  /imu/data_raw", "\n  /imu/mag", "\n  /imu_filter/parameter_descriptions", "\n  /imu_filter/parameter_updates", "\n  /imu_manager/bond", "\n  /imu_um6/data", "\n  /imu_um6/mag", "\n  /imu_um6/rpy ", "\n  /imu_um6/temperature", "\n  /joy_teleop/cmd_vel", "\n  /joy_teleop/joy", "\n  /odometry/filtered", "\n  /odometry/gps", "\n  /rosout", "\n  /rosout_agg", "\n  /status", "\n  /tf", "\n  /tf_static", "I have attached a Hokuyo UG01 laser to it, and it publishes  a topic called /scan which I am able to subscribe to with a simple python script.", "From my current understanding, I need to :", "\n1. Get the laser node to launch with the other nodes on startup ", "2. Create the catkin workspace with all the correct dependencies (very lost on that one)", " 3. Create launch files for the required nodes. (Maybe 1 and 3 are overlapping?)", "4. ??? Maybe do something with tf?", "That's where my understanding breaks down, I think.  ", "I would greatly appreciate any help on this. It's due in three weeks and I am stressing out.", "To give a bigger picture, the ultimate goal of the project is to have the husky navigate to GPS coordinates, then, at that location,  read a color sequence using open cv or something similar (haven't done too much with that part yet), then travel to colored cones in the order of the color sequence that was read.  For example, the lights flash red, green, blue, and the husky finds the red cone, travels to it, then the green, and so on.  Thanks again for the help.", "EDIT:", "Hi Icehawk. Thanks for the response. I have a few more questions. How would I go about adapting this demo for use on a real husky? I've been through the catkin tutorials, and basically understand how to create a workspace, but the format, naming, and choice of dependencies is what I don't get. I'm running indigo, so that's good, but the computer on the husky only has a terminal output, so I couldn't use rviz. I've tried running it from the remote pc that I ssh into the husky from, but I've had no luck with that. It seems like the tutorials are really good at teaching you to run demos or creating very simple nodes and whatnot, but it's not very helpful when doing things more complicated than basic publishers and subscribers. Thanks again.", "What do you mean it only has terminal output?", " Try taking a look at the tutorial for running ROS over multiple machines.  ", " . This will allow you to use the machine tag to tell ROS where to run certain nodes in a single launch file. ", "I mean that it has no GUI.  It is the server version of ubuntu, i guess.  I already can do what that tutorial teaches.  I can sub and pub from with computer.  I can even run rviz, but I guess I am doing something else wrong.", "I hate server. Everything is easier with a visual display :P"], "answer": [" ", " ", " ", " ", "OK, let's see. To start with 1 and 3 are completely overlapping. You use a launch file to start multiple nodes at the same time. ", "To set up the catkin workspace follow this ", " ", "What nav demo are you using? There is one specifically for the husky ", ". You may need to change the ROS distro though (this is for indigo, I don't know what version you are using).", "Once you have that working you can write your own node to send a goal to the move_base action server in order to get it to navigate to a specified point.", "[EDIT] ", "To run a physical Husky, in a terminal window run ", ". You should also take a look at the husky_bringup package. It has launch files for optional hardware on the Husky such as the CH Robotics UM6 IMU and LMS100 laser range finder. ", "can the parameter arguments just be listed in any order?  The wording is what is confusing me.", "Yeah, you can list the params in any order", "I'll give it another shot.  If the husky_bringup already has the gps and imu as dependencies, could I just include that package to cover the move_base, gps, and imu all in one?  I think I read that on the navigation tutorial.", "I' know that that is how you start the physical husky.  I don't know how to make the physical husky work from the navigation stack tutorial.", "It's like I said earlier.  It's not that simple stuff that I have an issue with.  I guess I don't see how any of the tutorials are helpful for the navigation stack.  It seems like there is some other knowledge that isn't covered in the tutorials.", "The tutorials do leave a lot to be desired. Have you looked at the tutorials for sending simple goals to the navigation stack?", "Also, pay attention to what each node is subscribing and publishing too. move_base publishes on /cmd_vel, but husky_base subscribes to /husky_velocity_controller/cmd_vel. You may need to remap some topics", "I have looked at sending simple goals, but I haven't tried anything since I don't even have the navigation stack running."], "answer_code": ["roslaunch husky_base base.launch"], "url": "https://answers.ros.org/question/234780/navigation-with-clearpath-husky/"},
{"title": "how to interface a servo motor with an arduino mega2560 using ros ?", "time": "2016-05-24 05:48:46 -0600", "post_content": [" ", " ", " i'm trying to use the code in the follwing link:\n ", " \nhow to implement this program .please help me . ", "Whats' exactly your problem with the tutorial?", "i'm unable to upload the ros code given to my arduino mega2560 board.", "could you send more details about the error message you receive from the Arduino IDE ?", "it doesnt show any error.. but the servo doesnt move even after following the commands to b used after uploading the code", "ah, so you can actually upload the code on your Arduino board? You said you were unable to do it. In the \"Testing\" section I guess one part is missing: after starting roscore, type in a new terminal the command ", " (Make sure to use the correct USB)", "i already tried it. still it not moving.i connected an led to check whether there's any problem with the board,but that it isn't the problem.\nthe servo progrm uploaded and rosrun n rostopic pub.. was also done\neven then its not happening..the ros core was also run", "fixed it .. it was nt getting enough power to the board .. thanx anyway :)", "what do u mean not enough power? how do you powering it before?"], "answer": [], "question_code": ["rosrun rosserial_python serial_node.py /dev/ttyUSBX"], "url": "https://answers.ros.org/question/235091/how-to-interface-a-servo-motor-with-an-arduino-mega2560-using-ros/"},
{"title": "Kinect configuration", "time": "2016-05-23 05:06:03 -0600", "post_content": [" ", " ", "hi,\ni have Kinect 1 and i install the package openni_launch.\nwhen i run: ", ", i receive :", "LED green of kinect flashes\nwhat can i do please?"], "answer": [" ", " ", "Try this "], "question_code": ["rosrun openni_launch openni.launch", "No devices connected .... waiting for devices to be connected\n"], "answer_code": ["roslaunch freenect_launch freenect.launch\n"], "url": "https://answers.ros.org/question/234993/kinect-configuration/"},
{"title": "ROS PS3 Distorted camera feed", "time": "2016-05-23 03:00:08 -0600", "post_content": [" ", " ", "Hello", "I have two PS3  cameras that work well. Only when connected directly to the intel NUC. When connected\nto a extension usb adapter the video signal becomes distorted. I believe its something to do with the \npower supplied to the usb extension adapter its not enough for the cameras.", "Hope you can help.", "Obvious question: have you tried a powered hub?"], "answer": [" ", " ", "The PS3 cams are special in that they use USB bulk mode for data transfer (you can get more info by googling \"PS3 camera bulk mode\"). This has the advantage that it allows to transfer more data (and the reason why the PS3 cam is one of the few cams that can do 60FPS over USB 2.0), but it in my experience can be problematic in less than ideal conditions.", "I know I saw issues when oversaturating the USB on a machine with too many PS3 cams streaming high rate images. In that case, images also got randomly distorted. ", "Have you tried different kinds of USB extensions? Also, have you tried other types of webcams?"], "url": "https://answers.ros.org/question/234982/ros-ps3-distorted-camera-feed/"},
{"title": "How to get the battery status of a robot in gazebo?", "time": "2016-06-14 04:03:45 -0600", "post_content": [" ", " ", "I have 2 Husky robots in Gazebo world moving around. I need to get their battery status like how much battery are they consuming in different tasks? I am using ROS-Indigo with Gazebo-2.2.2 on Ubuntu 14.04.", "I tried with PR2 robot as well from  ", " , but I am not able to understand how to use these. Also, I got PR2 dashboard running but it show \"Battery : stale\"."], "answer": [], "url": "https://answers.ros.org/question/236888/how-to-get-the-battery-status-of-a-robot-in-gazebo/"},
{"title": "Trying to obtain an RGB image", "time": "2016-06-29 18:46:26 -0600", "post_content": [" ", " ", "Hi,", "I am finding trouble in viewing the image on '/camera/image' topic. A blank screen comes up when I use the following command.", "rosrun image_view image_view image:=/camera/image", "But when I try to look on topic /camera/image_raw I get a monochrome image. When I try /camera/image_color topic, I get a green shaded image. I would like to get and normal (rgb )image , I see only the following topics : ", "/camera/camera_info", "/camera/camera_nodelet/parameter_descriptions", "/camera/camera_nodelet/parameter_updates", "/camera/camera_nodelet_manager/bond", "/camera/image", "/camera/image_color", "/camera/image_mono", "/camera/image_proc_debayer/parameter_descriptions", "/camera/image_proc_debayer/parameter_updates", "/camera/image_raw", "/diagnostics", "/image_view_1467243696220883524/parameter_descriptions", "/image_view_1467243696220883524/parameter_updates", "/rosout", "/rosout_agg", "Can I know how can I get normal RGB image ?", "Thanks in advance."], "answer": [], "url": "https://answers.ros.org/question/238469/trying-to-obtain-an-rgb-image/"},
{"title": "Leopard Imaging LI-eSP870-STEREO-M031 with ROS", "time": "2016-06-23 16:19:22 -0600", "post_content": [" ", " ", "Hi all,\nIf there's anyone out there that also picked up one of Leopard Imaging's Etron eSP870-powered stereo depth cameras (", "), have you managed to get a depth stream to come through in Rviz or something similar? I don't seem to be able to find any official support or packages for this camera, so are there any generic packages that would take a left and right image and be able to generate a depth cloud from them? \nAny help is much appreciated,\nJake ", "Farthest I ever got was obtaining an official SDK from Leopard, through email. Their SDK allows you to view all the streams, but still no ROS integration.", "Have you considered writing a wrapper node that bridges the SDK functionality into ROS?", "In truth, I am still very much a novice in terms of ROS, and I think that such a task would be better left to someone with a bit more experience in this field. In addition, there are much better 3D cameras out there, Intel Realsense and Asus Xtion, to name a few, with ROS support already.", "Do you have any update on your integration?\nWe are expecting to do the same...\nThanks for ur feedback.", "Honestly, I gave up on integration with this camera, there are other options that do it better for a similar price point, and for less work too. Stereo 3D on this camera is nice, except when you want to be accurate :) Plus, I needed low-light performance, something very easily done with IR sensors.", "Any ROS solution for leopard imaging cameras?", " From what I can see there's a ros driver for libdc1394 (IEEE 1394 Digital Camera protocol) and some Leopard Imaging cameras are on a list of cameras that support the protocol. Not sure about newer cameras. The page may be out of date.  "], "answer": [], "url": "https://answers.ros.org/question/237939/leopard-imaging-li-esp870-stereo-m031-with-ros/"},
{"title": "Irobot Create 2 installation issues", "time": "2016-06-19 01:36:08 -0600", "post_content": [" ", " ", " I robot create 2 driver installation confusion\nInstalled driver for Irobot Create 2 from here. I am a little stuck though. I was expecting a GUI or a place to input commands and not really sure what to do next.  ", "I run ", "rosrun irobotcreate2 irobotcreate2", "and get: ", "Sending OPCode: \ufffd\nSending OPCode: \ufffd\nSending OPCode: \ufffd\nSending OPCode: \ufffd", "The tutorial says:", "rosrun irobotcreate2 irobotcreate2", "to run the basic software and have access to the following topics:", "/battery\n/bumper\n/buttons\n/cliff\n/cmd_vel\n/digit_leds\n/ir_bumper\n/ir_character\n/leds\n/mode\n/odom\n/play_song\n/rosout\n/rosout_agg\n/song\n/tf\n/wheel_drop\nyou can read sensors (/battery, /buttons, /bumper, ...) and send commands (/cmd_vel, ...).", "No idea what to do with these commands without a UI. Any help would be much obliged. First time ROS user.."], "answer": [" ", " ", "While I agree that the package seems to be not documented enough, it does say you can send velocity commands by ", ". I suggest you go through ", " once again, and replace the topic name appropriately to work with Create2."], "answer_code": ["/cmd_vel"], "url": "https://answers.ros.org/question/237452/irobot-create-2-installation-issues/"},
{"title": "Hi Everyone, i want to get some of my apm data to ROS for doing 3D SLAM. For that i am using MAVROS. i successfully installed the mavros but when i echo my topics, some of them publish data but others not.", "time": "2016-04-30 07:48:33 -0600", "post_content": [" ", " ", " ", " ", "THIS is what it shows when i launch apm2.launch file. ", "Built-in SIMD instructions: SSE, SSE2", "Built-in MAVLink package version: 2016.4.4", "[ INFO]  Built-in MAVLink dialect: ardupilotmega", "[ INFO]  MAVROS started. MY ID 1.240, TARGET ID 1.1", "[ERROR]  FCU: Calibrating barometer", "[ INFO] CON: Got HEARTBEAT, connected. FCU: ArduPilotMega", "[ERROR] FCU: barometer calibration complete", "[ERROR] FCU: GROUND START", "[ WARN]  VER: broadcast request timeout, retries left 4", "[ WARN]  VER: broadcast request timeout, retries left 3", "[ WARN]  VER: unicast request timeout, retries left 2", "[ WARN] VER: unicast request timeout, retries left 1", "[ WARN] VER: unicast request timeout, retries left 0", "[ WARN] VER: your FCU don't support AUTOPILOT_VERSION, switched to default capabilities", "[ERROR] FCU: ArduCopter V3.2.1 (36b405fb)", "[ERROR] FCU: Frame: QUAD", "[ INFO] WP: mission received", "[ INFO] PR: parameters list received", "and these are the topics published", "/diagnostics", "/mavlink/from", "/mavlink/to", "/mavros/altitude", "/mavros/battery", "/mavros/cam_imu_sync/cam_imu_stamp", "/mavros/extended_state", "/mavros/global_position/compass_hdg", "/mavros/global_position/global", "/mavros/global_position/local", "/mavros/global_position/raw/fix", "/mavros/global_position/raw/gps_vel", "/mavros/global_position/rel_alt", "/mavros/imu/atm_pressure", "/mavros/imu/data", "/mavros/imu/data_raw", "/mavros/imu/mag", "/mavros/imu/temperature", "/mavros/local_position/odom", "/mavros/local_position/pose", "/mavros/local_position/velocity", "/mavros/manual_control/control", "/mavros/mission/waypoints", "/mavros/radio_status", "/mavros/rc/in", "/mavros/rc/out", "/mavros/rc/override", "/mavros/setpoint_accel/accel", "/mavros/setpoint_position/local", "/mavros/setpoint_raw/attitude", "/mavros/setpoint_raw/global", "/mavros/setpoint_raw/local", "/mavros/setpoint_raw/target_attitude", "/mavros/setpoint_raw/target_global", "/mavros/setpoint_raw/target_local", "/mavros/setpoint_velocity/cmd_vel", "/mavros/state", "/mavros/time_reference", "/mavros/vfr_hud", "/mavros/wind_estimation", "/rosout", "/rosout_agg", "/tf", "/tf_static", "i do /diagnostics and came to know that its because of the absence of GPS and other compulsory hardware. BUT i do not want to use GPS and compass data ", "THANKS in advance! ", " have you come into any solution?"], "answer": [" ", " ", " ", " ", "rosservice call /mavros/set_stream_rate 0 10 1 ", "use the above command to start publishing other datas as well  ", " ", " ", "i do have the same exact problem. "], "url": "https://answers.ros.org/question/233334/hi-everyone-i-want-to-get-some-of-my-apm-data-to-ros-for-doing-3d-slam-for-that-i-am-using-mavros-i-successfully-installed-the-mavros-but-when-i-echo/"},
{"title": "What is meaning of Operator's topics ?", "time": "2016-06-30 06:51:33 -0600", "post_content": [" ", " ", "What does the blue and the green line means in Rviz while using tutorial2 for autonomous not manually (joystick) ?", " According to  ", "  The blue line is  the input command. The input command  ", " has messeage content of (Velocity, Turn, and Mode), But ", " message is complete different !? "], "answer": [" ", " ", "These are just visualizations of the input and have the type: nav_msgs/GridCells. They show the trajectory the robot would follow given this command. In fact, only the turn is visualized, not the velocity and mode.", "From /cmd topic, turn has only one parameter, but /Operator/desired shows x and y. Is it because operator node convert the value of turns into locations in x and y ?", "Yes, the turn is the angle that the robot will turn around. So -1 is a left turn in place, 0 is a straight move and 1 a right turn in place.", "Thanks a lot for all the help during my semester Sebastian :) My exam is over now and in future I will see if I can buy or make a robot, then can run with ROS then it will be fun to implement nav2d :D"], "question_code": ["/cmd", "/Operator/desired"], "url": "https://answers.ros.org/question/238511/what-is-meaning-of-operators-topics/"},
{"title": "Communicating over serial to a stepper motor", "time": "2016-07-12 02:21:16 -0600", "post_content": [" ", " ", "Hi,", "I want to make a simple node to subscribe to a topic and publish/write these to a serial port. I have managed to send what i want and get the response i need but i can't implement this in a ROS node script. All the rosserial guides i have read is about arduino. However i only want to read and write to this port. Not edit whats on the other side of the ports software.", "So basically i have a python script that send commands which gives me the wanted response. (Positioning of a stepper motor of type Nanotec PD2-n) But how can i implement this into a node?", "Appreciate help in python and ROS (new to both)", "Not an answer, but some clarification: ", " is not meant as a generic interface to a (set of) serial port(s), but rather as a convenience library that provides a minimal implementation of the ROS C++ client library that works on Arduinos (and other embedded systems).", "It's called ", " because most of the times, such systems will be connected to a more powerful 'host' via some sort of serial interface (RS232 or similar). To allow communication, it comes with a comms protocol that supports concepts similar to ", ", allowing pub/sub etc from an Arduino.", "Yes, so rosserial will not be used in my case. Thanks for the clarification. I found this example: ", " which i assume is something i want but in python.", " You want a python serial port library? something like  ", "  ? ", " If your code is already running the serial part you are missing the ROS communication part.\nHave you seen the ROS Python tutorials?  ", "  ? "], "answer": [], "question_code": ["import serial, fcntl, struc, time\n\n#establish connection with serial port\n\nser = serial.Serial(\nport='/dev/ttyUSB0',\nbaudrate=115200,\ntimeout=1,\nparity=serial.PARITY_NONE,\nstopbits=serial.STOPBITS_ONE,\nbytesize=serial.EIGHTBITS\n)\n\n# commands as following: \"absolute mode\" , \"move to position 'angle' \" , \"execute\"\n\ncommands = ['#1p2\\r',\n'#1s' + str(angle) + '\\r',\n'#1A\\r',\n]\n\nif ser.isOpen():\n    ser.flushOutput()\n    ser.flushInput()\n    for data in commands\n        ser.write(data)\n        response = ser.readline()\n        print(\"read data: \" + response)\nser.Close()\nelse:\n    print \"cannot open serial port\"\n", "rosserial", "rosserial", "TCPROS"], "url": "https://answers.ros.org/question/239304/communicating-over-serial-to-a-stepper-motor/"},
{"title": "When move_base goal is set in a certain direction, dwa_local_planner plans less stably and the robot is moving slower", "time": "2016-07-26 09:50:17 -0600", "post_content": [" ", " ", "I found some interesting behavior in dwa_local_planner, not sure if it is a bug, or if it can be resolved by setting parameters differently.", "The behavior is that, when the goal is set in the -x direction with respect to TF origin, dwa local planner plans less stably (the local planned trajectory jumps around) and the robot moves really slowly. But when the goal is set in the +x direction, dwa local planner is much more stable, and the robot can move faster.", "I demonstrated this inconsistency in the following two videos (on youtube):", "In these videos, the local trajectory planned by dwa local planner is colored blue. The red line is the global path used by dwa local planner. The green line (in the second video) is the global path produced by global_planner. The dwa parameters I used are:", "Can anyone reproduce such behavior? If so, should we consider this a bug?", "If any clarification is helpful, please point out.", " I opened a github issue about this question ( ", " ). But since nobody responded yet on github, I though I may get some replies here. Any help is sincerely appreciated! ", "I might have seen this issue (or something very related). I'll follow up next time I experience it."], "answer": [], "question_details": [" ", " ", " ", " ", ":"], "question_code": ["DWAPlannerROS:\n# Robot Configuration Parameters\nmax_trans_vel: 0.7\nmin_trans_vel: 0.1\n\nmax_vel_x: 0.7\nmin_vel_x: -0.05\n\nmax_rot_vel: 1.2\nmin_rot_vel: 0.2\n\nacc_lim_x: 1.0\nacc_lim_theta: 2.0\nacc_limit_trans: 2.0\n\n# Differential-drive robot configuration\nholonomic_robot: false\nacc_lim_y: 0.0\nmin_vel_y: 0.0\nmax_vel_y: 0.0\nvy_samples: 0\n\n# Goal Tolerance Parameters\nyaw_goal_tolerance: 0.2\nxy_goal_tolerance: 0.2\nlatch_xy_goal_tolerance: true\n\n# Minimum velocity for the robot to be considered not stop\ntrans_stopped_vel: 0.1\nrot_stopped_vel: 0.1\n\n# Forward Simulation Parameters\nsim_time: 2.0\nvx_samples: 20\nvy_samples: 0\nvth_samples: 40\npenalize_negative_x: false #no longer used\n\n# Trajectory Scoring Parameters\npath_distance_bias: 32.0\ngoal_distance_bias: 9.0\noccdist_scale: 0.020\nstop_time_buffer: 0.5\n\n# Oscillation Prevention Parameters\noscillation_reset_dist: 0.05\n\n# Global Plan Parameters\nprune_plan: true\n"], "url": "https://answers.ros.org/question/240423/when-move_base-goal-is-set-in-a-certain-direction-dwa_local_planner-plans-less-stably-and-the-robot-is-moving-slower/"},
{"title": "Poor results of loop closurse detection rtabmap_ros.", "time": "2016-08-03 12:38:19 -0600", "post_content": [" ", " ", " ", " ", "Hello,", " I was trying to map hallway and passed it twice, during mapping i got almost zero loop closures and my second pass is seen under another angle. Result is that i got two connected hallways:  ", " . ", "\nI'm using Xtion Live Pro with configuration:", "Do you have any ideas how i can get better results? Also I'm not sure about those Brute Forces. I decided to use GFFT+FREAK because i read they give good results, should I try another algorithm?", "Thanks for answers.", "[EDIT]", "\nI realized that using GFFT+FREAK (which i read are slow) i got 440 frames in 30 minutes which gives only 14,(6) frames per minute which is 0,2(4)FPS, with this i scanned 2 floors + stairs few times.\nNext i decided to use ORB algorithm which gives me 330 frames in 6 minutes ~0,91FPS.", "Can I record with high framerate and them make GFFT+FREAK algorithm work? To make sure, algorithms use most of my CPU not GPU or RAM?\nIn comments there are few questions please answer as much as you can. I will make more algorithm tests but my battery is empty i need to wait for charge ;)", "Did you save the database somewhere and can you share it? (the one created in ", ")", "Clouds are saved in database only if I download them all from cache? I few scans today but my database files are very small... Now I'm in office i looked around, downloaded clouds and my database is much bigger.", "Also i realized that after first pass i got about 100 new pictures (IDs counter), second pass ~80, thid ~60 and so on... After 8 passes through hall i got 367 entries which give me about 45 per walk. Today first floor was ok (4passes) when staris are curved and another floor (4passes) looks bad.", "The database contains all data in the map automatically added to it. Make sure to kill rtabmap before copying the database (to make sure everything is saved in it). You can browse data in the database with ", ".", "My bad. In a while I'm going for another scan. Can i record database with data recorder and then process it? Already i recorded DB but it says there is no graph when i try to make pointcloud from databaseViewer, i generated it but viewer still don't see it. How can i export pcl from Data recorder?", "I want to make it this way because my laptop is slow and I'm not in hurry so my idea is to record big amount of pictures and then process them (already all is made \"live\" which i don't need). In short i have a lot of time and i want to process record with high accuray.", "I'm back. I realized that using GFFT+FREAK (which i read are slow) i got 440 frames in 30 minutes which gives only 14,(6) frames per minute which is 0,2(4)FPS, with this i scanned 2 floors + stairs few times.", "\nNext i decided to use ORB algorithm which gives me 330 frames in 6 minutes ~0,91FPS.", "Can I record with high framerate and them make GFFT+FREAK algorithm work? To make sure, algorithms use most of my CPU not GPU or RAM? ", "\nIn comments there are few questions please answer as much as you can. I will make more algorithm tests but my battery is empty i need to wait for charge ;)"], "answer": [" ", " ", "Don't use BruteForce matching for the vocabulary (", " group), use KDTree (default). I tested both databases (see in comments) and ORB seems to find most of the loop closures. The database with ORB seems to be already ok. Note that I have a better global optimized graph using TORO or GTSAM on this database (well g2o with GaussNewton optimizer seems better too ", "). For features, I personally prefer SURF for loop closure detection but it is just I've never did an exhaustive comparison of ORB and SURF, just did SURF benchmark for loop closure detection and it works well most of the time.", "Here some parameters I changed:", "In the database where you traverse two floors, most loop closures are detected with SURF or ORB. You may want to close the loop between the two floors by taking the stairs at the other side, this may reduce the \"bending\" map effect between the two floors. Well, the visual odometry could be also better, to reduce the drift. Also, be careful when you pass a door so that visual odometry can keep a high number of features to track, otherwise large drifts would happen.", "cheers", "Thanks, I will check Your configuration now. I am able to record data with rtabmap-dataRecorder and then analyze it with one of algorithms? Already i got problem which says there is no graph, it's no odometry fault? How i can add graph and analyze database.", "I'm trying to re-compute my odometry but it's lost almost every step, how should i confiugurate it?", "\nDuring re-computing my clouds are still \"bending\" at the end of hallway (i make U turn).\nCan i save and load parameters from file somehow without reset rtabmap?\nAlso i can't check FLANN KdTree.", "I made it ;) Successfully opened database saved with rtabmap-Datarecorder. But i want to check odometry/depth for each frame, now it's scans at about 1hz when \"movie\" is played at 100% speed, can I slow down that record? In next comment i will put link to download my \"movie\" and my result.", " Ok, here's my record:  ", " And here is my database after running it in Rtabmap stand-alone app:  ", "Propably got it. I changed detection rate and data buffer size to 0 (infinity). Should i change something in odometry? Other thing is that i can't check FLANN KdTree (non-commercial OpenCV without SIFT/SURF i want to use BSD licenses), and last thing is that there is no 2D to 3D but 3D to 2D (PnP).", " To make sure here is my configuration  ", " . I changed FLANN to Linear (KdTree not available) How I am able to load it to standalone app? ", "With rtabmap-dataRecorder, there is no graph saved, only raw data. You may use this kind of database as input source. I fixed kdTree not available bug (as it should be always be available). It was a bug in rtabmapviz only, rtabmap node should be ok.", "2D to 3D or 3D to 2D is the same, just a typo"], "question_code": ["  <param name=\"Kp/DetectorStrategy\" type=\"string\" value=\"5\"/> <!-- GFFT+FREAK -->\n  <param name=\"Kp/MaxDepth\" type=\"string\" value=\"3.5\"/> <!-- Max distance  -->\n  <param name=\"Vis/MaxDepth\" type=\"string\" value=\"3.5\"/>\n  <param name=\"Vis/MinDepth\" type=\"string\" value=\"0.8\"/> <!-- Min distance -->\n  <param name=\"Kp/MinDepth\" type=\"string\" value=\"0.8\"/>\n  <param name=\"Kp/NNStrategy\" type=\"string\" value=\"3\"/> <!-- BruteForce -->\n  <param name=\"Vis/CorNNType\" type=\"string\" value=\"3\"/> <!-- BruteForce -->\n", "~/.ros/rtabmap.db", "rtabmap-databaseViewer"], "answer_code": ["Kp", "g2o/Optimizer=1", "<param name=\"Kp/DetectorStrategy\" type=\"string\" value=\"0\"/> <!-- SURF -->\n<param name=\"Kp/MaxDepth\"  type=\"string\" value=\"0\"/> <!-- unlimited distance for the vocabulary  -->\n<param name=\"Kp/NNStrategy\"       type=\"string\" value=\"1\"/> <!-- KdTree -->\n<param name=\"Vis/CorNNType\"       type=\"string\" value=\"1\"/> <!-- KdTree -->\n<param name=\"Optimizer/Strategy\"  type=\"string\" value=\"2\"/> <!-- GTSAM global optimization -->\n<param name=\"RGBD/LoopClosureReextractFeatures\" type=\"string\" value=\"true\"/> <!-- optional but more loop closures would be accepted -->\n\n<!-- optional for odometry and rtabmap node -->\n<param name=\"Vis/EstimationType\" type=\"string\" value=\"1\"/> <!-- use 2D to 3D estimation -->\n<param name=\"Vis/MaxDepth\"       type=\"string\" value=\"5\"/> <!-- max depth can be higher with 2D to 3D estimation -->\n"], "url": "https://answers.ros.org/question/241032/poor-results-of-loop-closurse-detection-rtabmap_ros/"},
{"title": "odometry message from Arduino", "time": "2016-08-04 16:53:58 -0600", "post_content": [" ", " ", "Hello,", "I am trying to create and publish odometry data for a differential drive robot from an Arduino Due board with the data coming from wheel encoders and a gyro sensor attached to the board.", "my intention is to create and publish the Odom messages from the Arduino Due itself, since it has a fairly powerful Microcontroller. (Is it a good practice?)", "I have a few question though to clarify & make sure about a few things for myself. I check out the ", " and i notice that there is a ", " field, do I have to hard code this to ", " for a differential drive robot?", "Is it alright if I just convert the gyro\u2019s readings (which is in Degree Per Second) to Radian/sec and pass it straight as ", " to ", "?", "In the Arduino it seems like that i can not use the odom_quat as in the tutorial to convert my heading to quats.\nInstead should I add ", " and ", " to the message manually and calculate the value?", "At what rate is the best practice to calculate and publish odometry for a Turtlebot class diff drive robot?", "if you have any heads up or pointers regarding to what i'm trying to do I would appreciate it.", "Thank you."], "answer": [" ", " ", " ", " ", "Yes, you can publish on /odom directly from the Due, and it should work fine. One possibility is to use the ", " package. I've done this from an Arduino Leonardo. (However, I've dropped ", " on the smaller Arduinos because of the memory usage of ", ". You shouldn't have any problems on a Due.)", "You should publish the odometry as ", ". Normally, you want to publish in the ", " frame, which often has its origin midway between the two wheels. By convention, ", " is straight ahead, and ", " is up. Therefore, since we assume there is no side-slip, you'll have the velocities in ", " and ", ". The pose will be in ", ", including a quaternion for orientation. All other velocity values will be zero.", "You should publish odometry at a rate equal to or higher than your planning loop rate. Also take your other sensors into account: you want up-to-date odometry when publishing a point cloud, for example. Depending on your setup, anything from 10-100 Hz might be best. Keep in mind the bandwidth required by ", ", too. By default it's only 57600 baud, and messages are not tiny.", "Another option is to just deal with encoder tick rates on the Arduino and convert between ticks and /cmd_vel and /odom in a node on the host side. The ", " package is set up this way, for example.", "Hi Mark, I am currently using the ", " package ofc. I didnt get the part where you wrote i should publish odom as ", ". Do you happen to have any sample codes of the Arduino project you done? thanks for the reply!", "I don't have any C++ code that does it. Instead, I usually publish encoder ticks from the Arduino and then convert to a ", " in Python on the host. For example, see this code in the ", " :  ", "Also see ", ". And I've edited my answer above, which was wrong: you publish ", ", of course.", "Since the Arduino's loop function (which controls wheels and sensor readings too) loops at a constant low delay, my dt will always be the same amount & messages could overwhelm the link. I think the good practice is to just calc the velocities and send them to the host machine to manage Odom. right?", "That's the way I've done it, but YMMV. As far as the send rate, you'll need to control the motors only periodically on the Arduino, either using ", " at the bottom of the ", " code, or checking the elapsed time during the loop to see if it's time to control the motors and publish.", "Thank you ", ". I'll wait a little longer to see if anyone else wants to chip in with ideas and then mark your answer as correct.", ", another tiny confirmation; Theta must be presented in Radians in all odom calculations, is that correct? for example where it says: ", " \"th\" should be in rad and \"vy = 0\" for a differential drive robot...?"], "question_code": ["odom.twist.twist.linear.y = vy", "0.0", "vth", "odom.twist.twist.angular.z = vth", "odom_trans.transform.rotation.z", "odom_trans.transform.rotation.w"], "answer_code": ["rosserial", "rosserial", "rosserial", "nav_msgs/Odometry", "base_link", "x", "z", "twist.linear.x", "twist.angular.z", "pose", "rosserial", "ros_arduino_bridge", "rosserial", "geometry_msgs/Twist", "Twist", "differential_drive package", "nav_msgs/Odometry", "delay()", "loop()", "delta_x = (vx * cos(th) - vy * sin(th)) * dt"], "url": "https://answers.ros.org/question/241118/odometry-message-from-arduino/"},
{"title": "The callback is updated later", "time": "2016-08-08 03:15:52 -0600", "post_content": [" ", " ", " ", " ", "class Laser:", "When I execute the node, the print in the callback is updated later with the correct data and I don't know if it is for my code (DoSOMETHING) does many things, my computer is not very powerfull or something wrong with ros - No such check - . If I remove DoSOMETHING, the print is updated correct, but I need doing DoSOMETHING.", "My computer is: ", "model name  : Intel(R) Core(TM) i5-4200M CPU @ 2.50GHz", "cpu MHz     : 800.000", "cache size  : 3072 KB", "RAM : 6 G", "I use gazebo simulator with Pioneer 3dx and hayuko laser.", "Example: \nIn Gazebo has:", "robot ------(range:5)----->", "in shell: ", "5\n5\n5\n.....", "You move the robot in gazebo:", "robot ----(range:3)-->", "in shell: ", "5\n5\n5\n(after a few seconds)\n4\n4\n3\n3\n3", "Any Ideas?", "Sorry for my English and thanks for your answers"], "answer": [], "question_code": ["    def __init__(self):\n\n        self.sub = rospy.Subscriber('front_laser/scan', LaserScan, self.laser_callback)\n\n    def laser_callback(self, scan):\n\n        print \"rango 340: \" +str(scan.ranges[340])\n\n        ... DoSOMETHING ...\n\nif __name__ == '__main__':\n    rospy.init_node('laser')\n    Laser()\n    rospy.spin()\n"], "url": "https://answers.ros.org/question/241274/the-callback-is-updated-later/"},
{"title": "Paper related to RTABMAP odometry", "time": "2016-08-13 06:15:19 -0600", "post_content": [" ", " ", " ", " ", "Hi! ", "Is there any paper related to the rtabmap_odometry node, especially presenting the various algorithms used (SURF features and RANSAC matching function according to this ", " ", "I only found papers about the back end solutions... As well as info in the official ", "Thanks a lot! \nQuentin", "edit:", "1) In the case of the ICP refining, is it possible to use a \"known\" 3D point cloud and then to use it as a \"skeleton\" in which the odometry is computed? ICP should align the local point cloud and this known global point cloud. ", "2) As I am not an expert in image processing, could you advice me the most efficient solution for odometry tracking? My system should run on a UAV - computing power limited - with airborn 2D laser scanner and kinect. ", "Thanks for your advice!!", ": please don't post answers, unless you are actually ", " your own question. For everything else, use ", " or ", " your original question. You can use the ", " button/link for that (below your question). Thanks."], "answer": [" ", " ", " ", " ", "At that time, RTAB-Map's related papers use wheel odometry instead of visual odometry. Unfortunately, there is no paper about RTAB-Map's visual odometry approaches. ", "The default one is Frame-to-Map with\nOpenCV's GFTT/BRIEF features,\ncorrespondences by descriptors\nmatching, 3D->3D PCL's RANSAC\nregistration with known\ncorrespondences. ", "Another combination I use sometime\n(with car-like motions):\nFrame-to-KeyFrame with OpenCV's GFTT\nfeatures, correspondences by optical\nflow, 2D->3D OpenCV's\nPerspective-n-Points\n(cv::solvePnPRansac).", "It is also possible to add ICP refining if you have 2D laser scans or 3D point clouds. The choice of features are SIFT, SURF, ORB, BRIEF, FREAK, FAST or BRISK. ORB and FAST are also good fast choices. GFTT is nice because it can detect uniformly distributed features across the image. SIFT/SURF are slow for odometry, but really good for loop closure detection.", "Another ", " related to ", " vs ", " approaches used.", "cheers", "Thanks a lot for these answers! I edited my initial post with two additional questions ", " might be able to answer!", "Hi ", ", is there any paper now (2 years after the original post), explaining how the rgbd_odometry works?. Looking at the source code of 'OdometryROS.cpp' It seems that the map is generated and then the transformations between frames are estimated and converted to odometry?. Thanks!", "OdometryROS is a wrapper of Odometry from rtabmap library. The two standard ones are F2M (frame-to-map) and F2F (frame-to-frame). For F2M, the map is a temporary feature map used only for odometry, it is independent from the map of rtabmap."], "answer_details": ["No, it is not possible.", "You may want to look at Visual Inertial Odometry (VIO) approaches, though you would need hardware synchronized IMU/camera. In RTAB-Map, you cannot use a 2D laser scan to refine 6DoF odometry, only 3DoF (in case of a ground vehicle on flat surface).", " ", " ", " ", " "], "question_code": ["edit"], "url": "https://answers.ros.org/question/241665/paper-related-to-rtabmap-odometry/"},
{"title": "Clearing costmap to unstuck robot (3.000000m)", "time": "2016-08-18 09:15:13 -0600", "post_content": [" ", " ", "I've been banging my head against this navigation problem for days.  It occurs under both ROS Indigo and ROS Hydro (Debian installs).  And it occurs with both a Kobuki and a Segway RMP 210 both in Gazebo and the real world.  At the moment, both robots are using a single planar laser scanner for obstacle detection mounted about 15cm off the ground.  Also, I am ", " using a static map--both the local and global cost maps are using the odom frame for the global frame.", "The issue is that when the robot attempts to pass through a narrow gap, such as a doorway, the global planner (NavfnROS) usually plans a reasonable path through the gap.  The local planner (TrajectoryPlannerROS) then speeds the robot along the path toward the gap and some times the robot makes it through but at least 50% of the time, the robot stops either just before it enters the gap or even ", " it has entered the gap.  The robot then rotates through small angles left and right and might even back up a bit.  After 10 to 15 seconds, I see the warning:", "on the move_base terminal.  I've tried a gazillion combination of parameters including increasing the planner frequency, changing the map resolution, reducing the inflation radius to as small as 0.1 and nothing seems to eliminate the problem.  In addition, I don't know where the number 3.000000m in the message above is coming from.  Looking at the source code for ", " this parameter is defined on line 61 as:", "However, I cannot figure out how to redefine this parameter in my yaml files.  For example, neither:", "nor", "changes the value of the parameter in the warning message above.  Also, the following screen shot shows a typical situation where this stuck behavior occurs:", "Notice how the global plan (thin green line) passes very close to the obstacle on the right.  And this is after the robot has already been stuck for 10 seconds or so even though the planner frequency in this case was 5 Hz.  For some reason, the robot just remains stuck instead of replanning a path to go more centrally through the gap.", "So the more general question is this: what parameter tricks are people using to get their robots to move through narrow gaps like doorways?", "Thanks!", "\npatrick", "You're not going to like this response, but the only thing I found that was effective was to reduce footprint size in the yaml file so it wasn't so worried about hitting the wall. It still shows the behavior, but not as much.", "So go with large inflation radius and small footprint.", "Thanks for the suggestion.  It works great in Gazebo but I can't seem to get it working on a real robot (Kobuki), even if I shrink the footprint (radius) to 1/2 the real size and play with the inflation parameters up and down the scale.", "So I can now get the Kobuki (0.35m diameter) to pass through a 0.54m gap using a robot_radius=0.10, inflation_radius=0.5, cost_scaling_factor=5.0, pdist_scale=0.6, gdist_scale=0.8, occdist_scale=0.1.  I still get the \"Clearing costmap\" warning but the robot gets through the gap most of the time.", "Just 1 suggestion (might or might not solve ur problem though): I saw the green line is pretty closed to the obstacle, maybe you can increase the inflation_radius of inflation_layer of global_costmap so that Navfn produces a better global plan which is far away from obstacles", "(cont.) If the global plan is too closed to some obstacle, local planner will have a hard time finding the valid trajectory.", "Thanks for the suggestion.  It seems that the parameters I now have (listed above) also solves the global planning problem so I don't need separate inflation parameters for local and global costmaps.", ", I take that back.  It looks like setting the global inflation radius larger (e.g. double) and reversing the pdist and gdist scale parameters (pdist_scale=0.8, gdist_scale=0.6) improves the chances of success."], "answer": [], "question_code": ["Clearing costmap to unstuck robot (3.000000m)\n", "private_nh.param(\"reset_distance\", reset_distance_, 3.0);\n", "conservative_clear:\n    reset_distance: 1.0\n", "conservative_reset_dist: 1.0\n"], "url": "https://answers.ros.org/question/241969/clearing-costmap-to-unstuck-robot-3000000m/"},
{"title": "How can I use the teleoperation with iRobot Roomba 645 and ROS Indigo?", "time": "2016-09-04 19:18:10 -0600", "post_content": [" ", " ", " ", " ", "Hi!!!", " I'm new in ROS and I want to build a TurtleBot using iRobot 645 and Kinect with ROS Indigo. I've followed the tutorial  ", "  to do the teleoperation to begin, I've changed the hardware parameters but I get the next error: ", " The robot is power on, and the UST-TTL is working because I've used it to move the robot via serial communication using the iRobot Create 2 Open Interface (OI) Specification based on the iRobot Roomba 600 commands. Also I've tried to use this:  ", "  without results. ", "I'd thank you a lot if you could help me.", "Greetings!!!", "More Information:", "I've trying to connect ROS with this robot executing the next commands:", "Then it appears the error. By that I tried the create_autonomy driver. I followed the instructions to install it and when I try to execute it with this commands:", "The next message appears:", "I've using a PL2303 USB to Serial Converter and an Adafruit FTDI Serial TTL-232 USB Cable. I've tried with a baud rate of 19200 and 115200 and nothing. I want to create a map (SLAM) with this robot using ROS. I'm very new in this and I'd thank you a lot if you could tell me step by step how can I do it.", "Thanks and greetings!!!", "What were the specific issues with ", "?", "The command ", " assumes that you have created the workspace in your home directory.", "Try ", " (the square brackets signify optional parameters and should not be there literally, eg, ", " not ", ")", "Thank you by your answer Jacob!!!", "I tried your suggestion but the same error appears:", "[create2.launch] is neither a launch file in package [ca_driver] nor is [ca_driver] a launch file name", "That error means that you have either not properly sourced the workspace where you built the driver, or the driver was never successfully built. I would go through all the steps in ", " again and make sure you get no errors or suspicious warnings throughout the process.", "Thanks again!!!", "You're right. I solved the problem with the next commands:", "But now, I do not how to do SLAM using Gmapping (for example) with this driver and with a Kinect", "Greets", "I think turtlebot uses the package ", " to convert the Kinect data to a message consumable by ", ". I would read about these packages and if you still have trouble open a new question.", "Thank you!!!", "I'll read them.", "Greetings", "Hello!!!", "Excuse me Jacob, but I have some doubts about the create package:", "Thanks"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Yes, the workspace was created in my home directory.", "How do you obtain the odometry?", "How can I open the models of the ca_description carpet in Gazebo or in Rviz?"], "question_code": ["Failed to contact device with error: [Error reading from SCI port. No data.] Please check that the Create is powered on and the connector is plugged into Create\n", " export TURTLEBOT_BASE=create\n export TURTLEBOT_STACKS=circles\n export TURTLEBOT_SERIAL_PORT=/dev/ttyUSB0\n roslaunch turtlebot_bringup minimal.launch\n", " source ~/create_ws/devel/setup.bash\n roslaunch ca_driver create_2.launch [desc:=true] [publish_tf:=true]\n", "[create_2.launch] is neither a launch file in package [ca_driver] nor is [ca_driver] a launch file name\nThe traceback for the exception was written to the log file.\n", "source ~/create_ws/devel/setup.bash", "roslaunch ca_driver create_2.launch", "desc:=true", "[desc:=true]", "cd ~/create_ws/\nsource ~/create_ws/devel/setup.bash\ncatkin_make --source ~/create_ws/src/create_autonomy/\n"], "url": "https://answers.ros.org/question/243085/how-can-i-use-the-teleoperation-with-irobot-roomba-645-and-ros-indigo/"},
{"title": "Roomba 780 bluetooth ROS", "time": "2016-09-23 08:25:42 -0600", "post_content": [" ", " ", " Is it possible to have my roomba 780 controlled via bluetooth and ROS?\nI found the roomba 500 package but unfortunately the source is no longer available.\n ", " ", "Check out my driver ", " to handle the serial communication.\nI'm not sure about Bluetooth. iRobot has a brief tutorial on making a ", "Unfortunately I am using ROS hydro which will not work.", "Have you tried building ", " from source with Hydro? I'm curious what build errors (if any) arise.", "Yea i was able to build it from source. I was not able to run create_2.launch due to giving errors in the urdf.xacro. After replacing the urdf xacro it works. When running the application the roomba starts to beep and the terminal says serial error - Operation canceled and failed to receive data.", "Bascially this is the sequence:\n/opt/ros/hydro/lib/robot_state_publisher/robot_state_publisher\n[create::Serial] failed to receive data from Create. Check if robot is powered!\n[create::Create] retrying to establish serial connection...\n[create::Serial] serial error - Operation canceled", "Did some more testing it fails here ", "So maybe it is not connecting correctly via bluetooth. Unfortunately i dont have usb cable for roomba.", "It is working now for some reason don't know yet but this confirms your library works with bluetooth and roomba 780 Thank you for your help.\nEdit: Looks like for some reason it isn't working again :P", "I am going to make a cable for it but basically your library works with ros hydro except for the urdf xacro file."], "answer": [], "question_code": ["if (!robot_->connect(dev_, baud_))\n"], "url": "https://answers.ros.org/question/244335/roomba-780-bluetooth-ros/"},
{"title": "computing odometry, creating base controller", "time": "2016-08-30 08:48:51 -0600", "post_content": [" ", " ", "I am building my own custom robot and am finding it difficult to interface it to ROS (standard odometry and cmd_vel motor control). All I need is to publish odometry, based on the encoder ticks, and translate the twist messages (subscribed) into motor control signals, to be used with Navigation stacks.\nI found this tutorial - ", ". But it only explains half of the problem. They insert vx, vy and vth values to generate fake odometry of a robot driving in circles. I am trying to calculate real odometry. Can someone help with the code for calculating vx, vy and vth from actual encoder ticks? I will measure the wheel diameter, the amount of ticks per revolution, and the distance between the wheels. But not sure how to implement that in calculating the vx, vy and vth. Intuitively it seems like a simple task, and there are pieces of code on odometry all over ROS tutorials/documentation. But I just can't seem to put it all together.\nLater, I will also need to find a way to convert geometry/Twist messages to motor power signal.\nI would really appreciate any help, and believe it will help others in the future."], "answer": [" ", " ", "There are a couple of examples you can look at. First, assuming you can publish the encoder ticks in some way, the ", " package has nodes to both publish odometry from encoder ticks and to convert from ", " to motor speeds. All the source is in Python, so it's reasonably easy to read.", "Second, if you are using a microcontroller like an Arduino to talk to the hardware, you could use a package like ", ". It has a ROS node (Python) and an Arduino sketch that communicate to share the work. The Python side does the conversion from ticks to odometry and from ", " to motor control. It also includes a PID controller on the Arduino side. (In my robot I'm using a Pololu A-Star board that fits on top of a Raspberry Pi, for example.)", "I think most commercial robots using ROS (PR2, Husky, etc.) use the C++-based hardware interfaces instead. If you're going to follow the same path, you might look at how one of those commercial robots are handling this problem. The examples on the ClearPath site might be a good starting point.", "Thank you for such a complete answer. ros_arduino_bridge looks pretty good actually. Unfortunately it is not yet released for kinetic. I am still not sure how that affects the installation/usage in new distros. At the very least I can look at the source files, and try to figure it out on my own", "You should not have any problems using ", " for kinetic, but you probably need to get source. I recommend using ", ". I've contributed code to specify a frame for each sensor, if desired, for example.", "Use the ", " branch (even in kinetic).", "One more thing: ", " has support for some distance sensors, publishing as ", ". You will have to write/get another node that subscribes to the ", " values and publishes as a point cloud or laser scan, if you want to use the nav stack, or modify ", ".", "Thanks Mark. I will be using Intel's Realsense depth images for that. But it's good to know there is a library that supports simple sensors as well. \nFor now, I will try to get this to publish odometry and send motor signals to my Teensy 3.1.", "With a Teensy you might also consider ", " and ", ". The Arduino programming model is ROS-like: you publish and subscribe directly from the Arduino side. On smaller microcontrollers memory is an issue, but shouldn't be a problem with the Teensy.", "Mark, I already have rosserial_arduino working. Where I had a challenge, was computing the odometry. It even came with an example of publishing odometry on the Arduino side, but it is publishing fake odometry of the robot driving in a circle. If only I could modify it to compute from actual encoders", "OK, here's what I did previously. ", " from ", ", then used the ", " package to publish odometry on ", "."], "answer_code": ["Twist", "Twist", "ros_arduino_bridge", "indigo-devel", "ros_arduino_bridge", "sensor_msgs/Range", "Range", "ros_arduino_bridge", "rosserial_arduino", "rosserial_python", "rosserial", "/odom"], "url": "https://answers.ros.org/question/242736/computing-odometry-creating-base-controller/"},
{"title": "What board is appropriate for real-time computation of LiDAR output?", "time": "2016-09-21 23:28:47 -0600", "post_content": [" ", " ", "I am trying to use BBB for real-time computation of LiDAR to implement an autonomous RC car. ", "Would BBB be powerful enough to handle LiDAR output? I see in the ROS webpage that \"ARM based CPU is not recommended to run rviz\" which is needed to visualize LiDAR output. The ROS webpage recommends to use a board with a a \"GPU and vendor-supplied OpenGL libraries.\" ", "Do you have any suggestion for which board to use? ", "Thanks in advance.", "RViz is just a visualization tool. You don't have to use it in order to implement an RC car. Even if you do want to use it, you can run it on a laptop/desktop computer instead. In terms of boards, I would recommend the RaspberryPi 3, which is much more powerful than the BBB.", " is right that the RPi3 is more powerful than the BBB. However, for LiDAR output the BBB will probably be powerful enough. Neither the BBB nor the RPi has GPU acceleration for OpenCV, so if you want to do anything with vision you'll probably have to use something even more powerful.", "As ", " suggests, you probably won't run ", " on the robot, rather on your laptop. I have my RPi-based robot running Ubuntu headless; I don't run any visualization software on the robot, and the robot has no GUI capability."], "answer": [], "question_code": ["rviz"], "url": "https://answers.ros.org/question/244217/what-board-is-appropriate-for-real-time-computation-of-lidar-output/"},
{"title": "canopen chain node exit with error code -11 at start", "time": "2016-07-15 21:33:53 -0600", "post_content": [" ", " ", "I am trying to connect to an link[http://www.a-m-c.com/download/datasheet/dzcante-060l080.pdf \"AMC DZCante 60L080\"].", "I know that I am connecting to the can bus as i can run socketcam_dump and see the drive's power on message. When I run the canopen_chain_node's chain the launch with the following yaml file:", "I get the following the in logs:", "The canopen_chain-1.log file does not exist so I am at loss on what to do next. I just want to establish basic communication with my drive. My overall goal is to connect my drive with ros control.", "My help would be greatly appreciated. Thanks", "Have you try with a more basic config file? without the nodes and defaults field."], "answer": [" ", " ", " ", " ", "The released version is outdated, better try the source version.\nPlease check the indention of your config, it looks a little bit shifted."], "question_code": ["bus:\n      device: can0\n     master_allocator: canopen::SimpleMaster::Allocator\nsync:\n     interval_ms: 5 \n     overflow: 0\ndefaults:\n     publish: [\"1001sub0\"]\nnodes:\n     node1: #Torso\n     id: 42 \n     name: Torso\n     eds_file: '/robots/q1/axis1.eds'\n", "[roslaunch][ERROR] 2016-07-15 22:06:38,132: [canopen_chain-1] process has died [pid 3572, exit code -11, cmd /opt/ros/indigo/lib/canopen_chain_node/canopen_chain_node __name:=canopen_chain __log:=/home/ocu/.ros/log/1371681c-4ac4-11e6-956c-001c23b398d6/canopen_chain-1.log].\n"], "url": "https://answers.ros.org/question/239635/canopen-chain-node-exit-with-error-code-11-at-start/"},
{"title": "rviz on PC not reconnecting after robot reboot", "time": "2016-09-13 14:12:16 -0600", "post_content": [" ", " ", "I run rviz on a laptop PC and it communicates with the remote master on a robot. When I replace the robot battery and boot the robot, rviz stops receiving data and needs to be restarted each time. I would like to keep rviz receiving data after the robot is rebooted. How to achieve this?"], "answer": [], "url": "https://answers.ros.org/question/243708/rviz-on-pc-not-reconnecting-after-robot-reboot/"},
{"title": "pointcloud to laserscan slow", "time": "2016-10-26 07:14:09 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "For my project i am using a raspberry Pi which is connected to a kinect.\nI wan to use the data of this kinect for driving around the building. The only problem with it is that the pointcloud_to_laserscan node only gives out a laserscan once every 15 seconds. This is way to low.\nDoes anyone how to easily fix this problem.\nUsing Raspberry Pi 3\nROS kinetic\nKinect v1", "If you're building ", " from sources, be sure to enable optimisations (", "). If not: get a platform with some more processing power? Pi's are quite limited, and if you're not using nodelets, the pointcloud gets copied quite a nr of times.", "Also: note the comment on the ", " wiki page:", "If you're trying to create a virtual laserscan from your RGBD device, and your sensor is forward-facing, you'll find ", " will be much more [..] efficient since it operates on image data instead of bulky pointclouds.", "Thanks for your help when building with -DCMAKE_BUILD_TYPE=Release it improved my performance. Currently it is giving me an average of 4FPS which is quite nice already."], "answer": [], "question_code": ["pointcloud_to_laserscan", "-DCMAKE_BUILD_TYPE=Release", "p2l", "depthimage_to_laserscan"], "url": "https://answers.ros.org/question/246483/pointcloud-to-laserscan-slow/"},
{"title": "Boost shared pointer publishing - zero copy", "time": "2016-11-18 05:19:01 -0600", "post_content": [" ", " ", "Hello Community,", "I have read that the intra process communication can be done by publishing message using boost shared pointer of the message. Then there is zero copy. Apparentely this is what makes nodelets so powerful. ", "My question now, is this valid only\nfor nodelets that we should publish\nusing pointers and there is zero\ncopy ? Is this also true between different\nnodes ? ", "Also is there a requirement that the nodelets be launched from the same nodelet manager for the zero copy to be true ?"], "answer": [" ", " ", "1 . My question now, is this valid only for nodelets that we should publish using pointers and there is zero copy ? Is this also true between different nodes ? ", "No, only nodelets can exchange pointers*, as they share the same address space (they're basically nodes mapped onto threads instead of processes)", "(* this is not entirely true: with a suitable transport (such as ", ") zero-copy msg exchange is also possible between nodes, but that is not out-of-the-box supported and comes with some constraints)", "2 . Also is there a requirement that the nodelets be launched from the same nodelet manager for the zero copy to be true ?", "yes, again because they need to share an address space. Different managers will each have their own address spaces.", "3 . Is there a way to check/test whether there is zero copy happening in my implementation or my code is using network resources ?", "I know of no other way than checking resource usage (ie: CPU / memory). But in most contexts where nodelets make sense, the increase in performance is so noticeable that you'll know when it's not working (this is obviously not a good way to check, but is at least something).", "Thanks for the in-depth answer! ", "I have a follow-up comment/question about question 3: would it not be possible to check that the zero-copy has taken place by checking for equality the addresses to which the message pointers are pointing to (on the side of the publisher and each subscriber)?", "Yes, theoreticall this should be possible. I'm not entirely sure (any more, it's been a while) whether there is no place in the control flow between publisher and subscriber (in ", ") that potentially alters the address.", "But in principle the pointers should be equal.", "I think so too. Thanks.", "Actually, I just checked it. And yes, you get the same address on both sides of the message transport (subscriber and publisher)."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Is there a way to check/test whether there is zero copy happening in my implementation or my code is using network resources ?"], "answer_code": ["nodelet"], "url": "https://answers.ros.org/question/248365/boost-shared-pointer-publishing-zero-copy/"},
{"title": "Precompile catkin workspace or ROS packages", "time": "2016-12-03 20:24:50 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have a Raspberry pi 3 with Raspbian on it. This single-board pc has 1gb of RAM and 100 mb of swap. From the 1 gb of RAM, approximately 700mb are available because of the Raspbian OS. I managed to install ROS Kinetic on it (after 12 hours of installation with RAM's overflows) and I also connected a RPLidar a1 on it without issues regarding the powersupply. After that I tried to ", " the hector_slam stack and as I found out the existing RAM memory is not sufficient enough to finish the compile, so I get \"virtual memory exhausted\" errors or frozen screens.", "I thought of some ways to solve this, like increasing the swap memory (will make the processing too slow and it's not generally advised) or even installing another OS like Ubuntu Mate for example (will take some time), but what I would like to ask is:", "Is there any way I can precompile a catkin workspace I have built on my main pc and then transfer to the raspberry so no issues arise? Is this feasible? If yes, could you provide me with tutorial-like links to do that? Lastly, from the people that have worked with raspberry, would you advise me something better as a solution?", "The idea of the precompiled workspace came to me without knowing if it is something feasible or ways I can achieve that and because I don't have experience regarding that, it would be really helpful if you could provide me some links with your answer. ", "Thank you for your time in advance,\nChris", "Not a complete answer, but what you are looking for is called ", "."], "answer": [], "question_code": ["catkin_make"], "url": "https://answers.ros.org/question/249336/precompile-catkin-workspace-or-ros-packages/"},
{"title": "rosserial mbed with EFM32 Wonder Gecko - no sync", "time": "2016-11-10 15:45:06 -0600", "post_content": [" ", " ", "I've been using rosserial with arduinos effectively, but now I need more horsepower so I am trying out the mbed boards. I have an NXP-LPC1768 and a EFM32 Wonder Gecko. ", "The NXP has worked out of the box without a problem. However I can't get the Gecko to work. It is supposed to be an \"mbed enabled\" board, so it is supported by rosserial. \nThe program I am flashing is the ", ". It is being flashed successfully because I see the LED1 light blinking as the code indicates. However when I run the command ", "rosrun rosserial_python serial_node.py /dev/ttyACM0 _baud:=115200", "the communication is never established. The baudrate should be right since the Gecko itself contains a readme saying the Virtual COM port speed is currently fixed at 115200 bps. ", "I am using the mbed online compiler for this, where I import the Hello World example from ", ". It is worth mentioning that I had to update the firmware in the Gecko such that it would show up as a USB Mass Storage device like all mbed devices do.  Nothing weird here, I used the official Simplicity Studio software to update the Gecko firmware to a newer one. ", "Any ideas what I might be doing wrong? These same steps work perfectly on the NXP-LPC1768", "Thanks in advance,", "Hi! Are you sure the serial port is ttyACM0 for the Gecko? Also, the default speed is 57600 (see line 17 in ", ") or did you change it on the example code?"], "answer": [], "url": "https://answers.ros.org/question/247801/rosserial-mbed-with-efm32-wonder-gecko-no-sync/"},
{"title": "TurtleBot 1 Assembly Guide with Roomba Create 2 Base", "time": "2016-11-10 07:13:22 -0600", "post_content": [" ", " ", "Hey there,", "I would like to build a TurtleBot 1. However my problem is that I have Roomba create 2 base (green one) and not a\nroomba create 1 base(white one). Unfortunately I only find assembly instructions for the TurtleBot1 with a  roomba create 1 base.  That leads me to my question:", "Does anyone of you have an assembly instruction for the roomba create 2 base ?\n[somehow the wholes of my turtlebot one plates do not match with the create 2 base wholes - so I cannot mount the plates on the create 2 base....]", "Thanks for your help."], "answer": [], "url": "https://answers.ros.org/question/247745/turtlebot-1-assembly-guide-with-roomba-create-2-base/"},
{"title": "A problem about the conversion from Euler angle to quaternion", "time": "2016-11-29 17:30:09 -0600", "post_content": [" ", " ", " ", " ", "Hi, all. we are trying to construct a 3D scanning system to implement 3D mapping algorithm. Its hardware system is showed in following figure : ", ".", "A Hokuyo laser(UTM-30LX) is mounted on a plate which is rotated by a Dynamixel(", "). The Dynamixel works in back_and_ forth way. Its rotation angles' range is between -1.57 and 1.57, i.e. [-90\u00b0 90\u00b0] .", "The tf tree is showed in following figure : ", ".", "The frame_id \"Laser\" is the local scanning coordinate of Hokuyo laser. The frame_id \"HN07_N101\" is the local rotating(in back_and_forth way) coordinate of MX-28R. The frame_id \"BaseLink\" is the base_link coordinate of the system. The joint between \"Laser\" and \"HN07_N101\" is fixed. But there is a revolute joint between \"HN07_N101\" and \"BaseLink\". ", "The frame \u201cHN07_N101\u201d is rotated around its own \"z\" axis which is the blue axis in above figure.  In the coordinate of \"BaseLink\", the origin of \"HN07_N101\" is (0.22375, 0, 0.1367), the initial orientation is (0.5, 0.5, 0.5, 0.5), as the left part of above figure shows.", "As the Dynamixel rotating the Hokuyo laser, it is necessary to compute the coordinate transform between parent frame \"BaseLink\" and child frame \"HN07_N101\". The dynamixel motor sends ", " message which contain the current rotating angle around \"Z\" axis in radians.  ", "However, when I run my project, the transform between pararent \"BaseLink\" and \"HN07_N101\" is ", ".  Although there is no jump in the value of Dynamixel's msg, both the frame \"HN07_N101\"(including its child frame \"Laser\") and the 3D models of Hokuyo and Dynamixel alway flicking.  The so called \"flicking\" is, while the frame \"HN07_N101\" is rotating, the green \"y\" axis and the red \"x\" axis suddenly switch positions with each other and recover.", "I think the flicking 3D models displayed in rviz means that there is problem in the transfrom between \"BaseLink\" and \"HN07_N101\". But I don't know where the problem is. ", "Is the problem at the conversion from Euler angle (the current_pos in Dynamixel's msg) to the quaternion ( the variable \"q\" in above code) ?", "Looking forwards any valuable advices.  Thank you very much!", "I haven't read everything, but can you clarify why you publish the ", " transform yourself, instead of using ", "? Just publish the ", "s for the appropriate joints, and ", " will do the rest.", "Thank you very much for you attention!  What I did is from ", " which publishs a ", " and a ", ".", "I know the tutorial, but it's really not necessary in your case to broadcast the transforms yourself."], "answer": [], "question_code": ["BaseLink->HN07_N101", "JointState", "robot_state_publisher"], "url": "https://answers.ros.org/question/249058/a-problem-about-the-conversion-from-euler-angle-to-quaternion/"},
{"title": "How to call service global_localization", "time": "2016-12-15 19:21:20 -0600", "post_content": [" ", " ", "I'm learning navigation ,I meet a question. how to call service global_localization. Because now the robot doesn't know its ", "\nposition when the power switch is on ,I have to put it in the origin of map.if not ,the robot will deem that the position where when the power switch is on is the origin of map .Then the robot can't reach the accurate position .So ,I want to write a  c++ code to  make the robot know its position when the power switch is on .I have found some information ,some argue that in RVIZ  click the position estimated .Some argue that call service global_localization ,but this call in the terminal input \"rosservice  call  /global_localization\".These are not that i want to use .Maybe there are other solutions to my demand. Please help me to analyze it .Thanks .", "Now,I have wrote a node to call service global_localization.However,some  problems appeared .sometime the robot warn \"Unable to get starting pose of robot, unable to create global plan\" ,then the robot can't move .Sometime the service call failed .I don't know why ?"], "answer": [], "url": "https://answers.ros.org/question/250144/how-to-call-service-global_localization/"},
{"title": "RViz Ignoring Collisions Between Attached Object and Environment", "time": "2016-11-14 10:25:00 -0600", "post_content": [" ", " ", " ", " ", "I have, from what I can tell, successfully attached a mesh to my robot via this ", ", however the collision checking in RViz seems wrong.", "After attaching the added object (previously green) turns purple.  By dragging the goal state query around I confirm that the attached object collides correctly with the robot ", " (the robot, however, will collide with said objects).  I tried searching for this issue and found some other related topics but none with my exact situation.  ", "Some notes:", "[edit]\nPlaying some more with RViz I am getting status messages to the effect of:", "but the robot doesn't turn any links red and/or cycle through solutions looking for a \"good\" solution.  It is also more than happy to plan and execute to that goal state with colliding links...", "What am I missing here?"], "answer": [" ", " ", " Did you build MoveIt! from source?\nThis sounds to me like you might hit  ", "  which is not fully released yet. ", "Score; this solved the issue. Thanks for the quick reply v4hn.", "Is this issue fixed and has it been released?", "Yes, this is officially released in all ros-distributions.", " I was having the same issue and as a workaround I tried attaching the object to a link with collision geometry. But I still see collision in between the attached object and the environment. Any hints on what the issue might be?"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "The attached object is not visible on RViz's goal query until I hit the \"Publish Current Scene\" button", "Visualizations via the \"/move_group/display_planned_path\" do not show the attached object"], "question_code": ["Goal state colliding links:\nattached_object - environment_object\n"], "url": "https://answers.ros.org/question/248071/rviz-ignoring-collisions-between-attached-object-and-environment/"},
{"title": "Laser Scanner Merging in one Topic", "time": "2016-06-30 09:21:10 -0600", "post_content": [" ", " ", " ", "Hello,", "I am working on the package for merging multiple laser scanner data in one topic (Ira Laser Tools). I am getting 2 data from different sensors placed one on the front of the robot and one on the rare (scan1 and scan2), Then i apply the multi merger node to merge to get one laser scan data (scan), but it is always empty and giving me the following warning", "I created in my URDF model a virtual laser which is my destination frame_id (in green in the picture), it is placed at the midpoint of the rare and front lasers.", "Note that in the picture the data in red are the data received from scan1 and scan2 respectively.", "Please find in the attached file above a screen shoot that describes the situation. And please note than the scan topic is empty.", "Is there any way to solve this problem please? i would be really thankful.", "Hi,wassimhariri", "I have the same question ,as the warn ", "This problem solved ? If settled, please share to me, thank you!", "I think the warning should nt be the reason why there is no scan in the merged topic. Are you sure the 2 laser scan topic are called \"scan1\" and \"scan2\"? Can you post the output of \"rostopic info /scan1\" and \"rostopic info /scan2\""], "answer": [" ", " ", "actually this problem was solved, we created another package than ira laser tools called scan merger, where i can get 3 different scan data from 3 different laser scanners and then publish them on one scan topic where navigation can accept it and implement SLAM. i will post details as an answer", "Any detail update :) ?", "Yes, details would be welcomed :)", "I have the same problem. Would you please show us how to slove this problem? Thank you!"], "question_code": ["[ WARN] [1467292029.145711135]: Transformer::setExtrapolationLimit is deprecated and does not do anything"], "url": "https://answers.ros.org/question/238531/laser-scanner-merging-in-one-topic/"},
{"title": "Debian Jessie armhf repo", "time": "2016-06-06 09:48:11 -0600", "post_content": [" ", " ", "Hi folks,", "would like to ask... is there anywhere repo with ros-minimal packages for debian jessie with armhf architecture?\nand if there is not, is there a chance that there will be given that debian jessie is now official distro? :)", " I wanted to setup my own jenkins, but realized I don't have machine strong enough ( based on what's recommended on  ", "  , but maybe I can work with something less powerfull? ) ", "P.S: please, don't answer with things like \"use ubuntu\" or \"compile it on rpi\", I know these alterantives and they are not point of the question.", "Considering there may not be a near term build of ROS for Debian armhf:", "Could you elaborate on the difference(s) between Jessie and Xenial which affect your choice?\nHave you considered cross compiling?"], "answer": [" ", " ", " ", " ", "Not sure what you mean though, are you asking if binary packages are built for ", " for ", "? If so then ", " AFAIK.", "is there a chance that there will be given that debian jessie is now official distro? :)", "I'm not sure if Jessie is officially supported. Starting from Kinetic, Jessie seems to put on some restriction on the specification (see ", ") but support for it is \"not required\"."], "answer_code": ["Jessie", "armhf"], "url": "https://answers.ros.org/question/236192/debian-jessie-armhf-repo/"},
{"title": "Polling overall diagnostics state", "time": "2017-01-14 19:05:55 -0600", "post_content": [" ", " ", "How do you find the overall diagnostics state from the diagnostics aggregator?", "My diagnostics are running and viewable in RQT monitor, and now I'd like to make a simple rospy node that sets a status LED to red if there's an error, and green otherwise.", ", I see there's a ", " topic, which I intuit should contain this info, but when I echo it, all I see is:", "do all blank values mean everything is ok, or am I misinterpreting this? Is there a better way to poll the diagnostics aggregator?"], "answer": [" ", " ", "The ", " is the correct way to get a state summary from the diagnostic aggregator.", "The ", " field indicates the state and is set from the OK, WARN, ERROR or STALE enum values defined in the ", ".", "In this case, ", " corresponds to ", "."], "question_code": ["/diagnostics_toplevel_state", "level: 2\nname: toplevel_state\nmessage: ''\nhardware_id: ''\nvalues: []\n"], "answer_code": ["/diagnostics_toplevel_state", "level", "level: 2", "ERROR"], "url": "https://answers.ros.org/question/251971/polling-overall-diagnostics-state/"},
{"title": "sw_urdf_exporter preview and sw crash", "time": "2017-01-10 05:01:24 -0600", "post_content": [" ", " ", " ", " ", "When I have finished to ", " at sw_urdf_exporter plugin, while other settings are set to be ", ", I click ", ". The green process in the bottom loads a while and ", ".", "I check ", " and realize that the ", " and ", " should be specified in the model before export. Am I right?", "But instead, my model file type is ", ", where ", ". So, in order to proceed with export urdf, should I add reference coordinate system and axis ", " for model?", "If so, it is a pretty time-cosuming, in other words, ", "   However, exporter plugin might be only fit to SW2012, original assembly is built in newer version.", "no need to assign reference frame"], "answer": [" ", " ", "So, in order to proceed with export urdf, should I add reference coordinate system and axis mannually for model?", "If you want to have joints in your exported URDF: yes.", "If so, it is a pretty time-cosuming, in other words, using original assembly file which contains assembly relation is the correct way to export urdf. Am I right?", "That would certainly seem to be the most straightforward way of doing it, yes.", "However, exporter plugin might be only fit to SW2012, original assembly is built in newer version.", "I would try the plugin with a newer version of SW. It can work, it's just not supported / recommended.", "I tried the export process on SW2012 and failed, however I succeeded on SW2015 using other's PC. Maybe sth wrong with my SW2012.", "I asked someone, he told me that, he used to turn step file into assembly file, after adding assembly relation. If  assembly relation exists, can both reference coordinate systems and reference axes be automatic detected with plugin, without mannually specifying?"], "url": "https://answers.ros.org/question/251639/sw_urdf_exporter-preview-and-sw-crash/"},
{"title": "not able to start turtlebot, what could be the reason ? [closed]", "time": "2017-01-03 13:41:49 -0600", "post_content": [" ", " ", "after fliping start button on kobuki base I didnt hear the sound of turning on the bot neither i get the green light on status, I checked the battery wire of kinect but it i connected accurately at 12v 1.5A. but still bot is not working . What could be the reason behind this and how can i resolve it ?"], "answer": [], "url": "https://answers.ros.org/question/251212/not-able-to-start-turtlebot-what-could-be-the-reason/"},
{"title": "STM32  Microcontroller for communication with ROS", "time": "2016-12-23 04:21:01 -0600", "post_content": [" ", " ", "I currently intend to deploy an STM32 (Stm32f103c8t6) as the microcontroller to publish the Odom message and message from other sensors also. My questions are as follows:", "Thanks in advance.", "Can you clarify what is confusing about ", "? Perhaps we can do something about that. It would seem like a good fit here, provided you have a regular PC / more powerful SBC for the STM to communicate with.", "As in the library files for the rosserial to work on the stm. How do i put the header files or libraries like the <ros.h> in my eclipse IDE", "Can you tell me how could i link the ros libraries with the stm workspace on linux?", "The idea is basically to generated the ", " files (sources + headers), and then use those directly in your STM project. There are no dependencies on anything 'ROS' in the ", " files, so you could even copy them to your Windows machine.", "I have to just include the ros and std::msgs header file and code my STM32 in C only? I have come upon a c++ library of the STM32 ,the stm32plus. Which one should I go with? With all respect, can I have your email ID ?", "It's all C++, so you'll have to find a compatible runtime. I can't recommend you a particular set of libraries, as I don't have enough experience with them.", "re: email: no, I'd rather we keep the discussion on this forum, so future readers may benefit from it as well.", "This is the stm32 C++ ", " and this is the ", " wrapper for the rosserial. I'm confused  here on how do I configure the hardware wrapper into the C++ implementation as well as fusing the ros_lib"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "Is there any support available for the STM32 for communicating with ROS(client, serial etc.)? I know of the rosserial but it's pretty confusing to me on the tutorials page.", "Has anyone implemented the STM32 in their bots? Would it be better to port to an arduino? If not, is the process of writing an altogether new node for the stm32 a tiresome task?"], "question_code": ["rosserial", "ros_lib", "ros_lib"], "url": "https://answers.ros.org/question/250667/stm32-microcontroller-for-communication-with-ros/"},
{"title": "Raspbian Jessie ROS INDIGO download image [closed]", "time": "2015-01-07 02:04:53 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "during christmas vacation I compiled ROS INDIGO on a Raspbian Jessie image with the following features / packages installed:", "Here is the link for downloading:", "Thanks for the image. I'm using this very successfully to run my RPi powered rover", "Dude you are an absolute lifesaver with this.", "File was corrupted when I extracted it with 7-zip.  Any advice?  I can try to gunzip it instead"], "answer": [" ", " ", " Hi Martin, this isn't the appropriate way to advertise a contribution such as this, if you would like to use ros answers for this then you should ask a question and answer it yourself, for example see:  ", "On another note, any time you are distributing a binary, you need to provide details like how you created the image and how someone else could reproduce it. While I'm certain that your intentions are pure, without some sort of signing it would be very easy for someone to advertise such an image which intentionally or unintentionally contains malicious software. By providing a way for others to reproduce it, you can get independent validation and build trust in your image.", " More importantly, while a binary image is certainly useful, it would be good to have instructions on how to do this so that others can build on what you've done and improve the solution that way. It is very hard for someone to reproduce and iterate on a binary image. I would recommend you just detail the steps you used to create the image on a wiki page, similar to something like the UDOO instructions:  ", " ", " ", "Thanks for the image has saved me a lot of time getting ROS up and running on my Pi.", "I can confirm that the image does not boot in RPi2 but does on B+ ", " ", " ", "Forgive my ineptitude, but I used 7-zip to extract the file, had to append .img to the file so that my image writer could use it.  Raspberry Pi didn't even bring up a display... nothing... I know I'm doing something very dumb so please help.  I'm trying to mount it from Windows.", "Thank you! ", " ", " ", "Hello everyone, this image doesn't work on my ", ". When I try to plug in microSD with this image installed by Win32DiskImager, my Monitor connected with HDMI just shows me palette.", " ", " ", "Hi William,\nI do agree to the approach for writing questions in ROS Answers. The next time I will follow your hints.", "The other thing is, that the Raspbian image is a image for writing a SD card with a Debian Armhf operating system for the Raspberry Pi (->Google). This is not a binary in this sense that I did generate or compile the system. What I did was to compiling ROS Indigo and some kernel drivers for the system. The ROS sources and a description to perform this compilation or any ROS package are \"on board\" in /home/pi. You can unpack the image to a 16GB sdcard using:\nsudo dd if=< place where my image is/image_name.img> of=/dev/sdd", " ", " ", "hi! i try to install the .img and when i try to run on the raspberry it ask me a password. COuld you say me the password?", "Thanks million!", "hi,\nthe password is nothing else than:\nuser: pi\npwd: raspberry", "sorry for that, but when i am booting raspbian with the image that i download from here it tell me to login. and when a enter a password appear \"System is booting up. see pam_nologin(8)\" ", "Yesterday i thougth that maybe could be because i upload wrong the image on my sd card. But today happen too.", "hi,\nthe image is working with my raspberry. May be the SD card is corrupted or the image was not downloaded correctly.", "thanks thanks million!! :) it worked perfectly!", " ", " ", "Is there something weird about the ethernet on this image? I run my pi headless, getting internet through my computer, and other images have grabbed IP addresses through my computer, but this one doesn't seem to be playing nicely.  I can set it to a static IP, and it will take the IP and let me ssh in, but won't get internet.  any ideas?", "Hi, I didn't change anything, even ROS is compiled natively on the PI, so i had to wget the sources via internet. But you can have a look in /etc/network/interfaces.Another thing is, that I have the CAN bus running on PI. For my project I need both, CAN and ROS running on a PI 2.", " ", " ", "Hello! i am working with the raspberry pi2 and this image isn't work. Do you know if there is any possibility to use on rpi2? What change i would do it to could use it on my rpi2? ", "thanks!", " ", " ", "I just want to tank you for this image as it seems to fit my needs very will right now.  I'm a ROS n00b and have been having problems getting a stable ros image setup from the default wheezy image. I shall give it a spin tonight. Not sure what the significance of going for Jessie over Wheezy has especially since it is not yet released."], "url": "https://answers.ros.org/question/200504/raspbian-jessie-ros-indigo-download-image/"},
{"title": "ros_control and rosserial_arduino together", "time": "2017-01-05 13:57:24 -0600", "post_content": [" ", " ", "Hi there,", "I am trying to get working a setup consisting of an Arduino-based mobile robot sending its sensor values to and getting the motors velocity commands from a computer where the heavy computations are performed.", "My initial plan was to use ros_control as a bridge between the high level application and the embedded low-level interface with the motors (seems legit, right ?). Referring to the ", ", I understand that I am supposed to write this new class, which exposes the joint states and interprets the joint commands.", "My major concern is whether I should/am able to directly run this interface on the Arduino board, or rather have it on the standalone computer. The second option seems much more cumbersome to me, as a I should still manually manage the communication with the robot, but I am not sure if the Arduino (Leonardo) will have the power to handle this, or even if the library is compatible with rosserial_arduino.", "Thank you so much for your help"], "answer": [], "url": "https://answers.ros.org/question/251354/ros_control-and-rosserial_arduino-together/"},
{"title": "Possible alternatives to ROS tf2", "time": "2017-01-11 12:23:26 -0600", "post_content": [" ", " ", "I am currently looking for a C++ coordinate transformation library to use. I have read through the ROS tf2 and found it to be a powerful library that can fit my use case well. The only concern is that tf2 seems to be coupled with ROS infra (a ROS core, its pubsub system, catkin build system, etc) pretty tightly, which is a little too heavy to our project. I am wondering if it's possible to use tf2 standalone w/o introducing the whole ROS or anyone happens to know some other library that could somehow reach feature parity with tf2. I have only one moving coordinate frame with a few static coordinate frames, so maybe the tree structure of tf2 is also a little overkill to me. Thanks a ton in advance!", "tf1 was specifically modified to produce tf2 in order to make its core independent of ROS. What is your problem with using it standalone?", "I haven't looked into tf2 core yet. Is there any documentation on how to use it standalone?"], "answer": [" ", " ", " ", " ", " As ", " linked to the design doc:  ", " tf2 is designed to be ROS independent. The dependencies for the tf2 package are specifically limited.  ", "We haven't taken the time to package it up separately as there hasn't been much demand and would potentially add overhead to running within ROS.", "Here are what's needed to pull it out as a pure library. ", "rostime is the primative datatype for time in the above messages and is thus included", "It uses the catkin cmake package which is technically independent of ROS and can be used if installed as a pure cmake package. If catkin is installed you can call (mkdir build && cd build && cmake .. && make)", "As an approach to using it I would recommend installing catkin and libconsole-bridge as dependencies. The quick solution for converting the datatypes is to copy the generated ROS message headers. The slightly longer solution is to use a different data representation in the public API. ", "The other relatively simple solution is to install tf2 with it's existing dependencies. You only need to install the message runtime packages and catkin which is 17 packages taking up a total space of 5,244 kB using indigo. a 5MB size is a pretty small footprint to further optimize. Depending on how you measure it it is ROS independent as it has none of the ROS communication middleware dependencies. It just uses datatypes defined using the ROS msg format. ", "Thanks for your thorough answer! I noticed that there's a ros2 branch of tf2 that cuts ros dependency even further (only depends on geometry_msgs and the new build system). Is that in a stable state that can be used as well?", "That is not a stable branch it was a proof of concept for using a library on top of ros2. It might be a good jumping off point for a minor refactor. But I'd like to reiterate that I'd suggest that you just use this as is. The current dependencies are pretty much nominal.", " Any success making TF2 independent of ROS ?", "Hello All,\nI am new to this. I tried to use TF2 independently by considering dependency with libconsole-bridge, tf2_msgs, geometry_msgs, rostime . But while compiling I am getting below error.", "Any thought on this error ?", "Hi, since the discussion is now few months old, I was wondering if there are any updates on this topic, especially about the testing branch mentioned above, and if some kind of updated tutorials are available about how to compile tf2 as a stand alone library. Thank you!", " ", " ", "Following ", " answer, and some work I found someone at my company did with Indigo, I created a ", "My Indigo branch uses the full ROS build, but this simply stopped working one day, so I built the Melodic branch by setting up an 18.04 VM, installing all the ROS dependencies, and then manually copying them into my checkout of ", " (0.65) and re-writing the ", " file.", "So, the obvious issue is that code in this repo will age, but for now it should help.  I'm still testing this code on various platforms, but it seems to be working.", "Hope this helps.", " ", " ", "Here are some links you might find useful:"], "answer_details": ["libconsole-bridge is a ROS independent console output library. ", "tf2_msgs is only used for error enumerations and can be replaced by copying the generated header file", "geometry_msgs datatypes are used for data storage. A truely ros independent version would need to copy a few message header files or implement their own replacement data storage type.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["root@b2baa96c3317:/# sudo apt-get install ros-indigo-tf2\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  binfmt-support comerr-dev krb5-multidev libapr1-dev libaprutil1-dev libblas3\n  libbz2-dev libgfortran3 libgssrpc4 libkadm5clnt-mit9 libkadm5srv-mit9\n  libkdb5-7 liblapack3 libldap2-dev liblog4cxx10 liblog4cxx10-dev liblz4-1\n  liblz4-dev libmysqlclient-dev libmysqlclient18 libodbc1 libpcre3-dev\n  libpcrecpp0 libpipeline1 libpoco-dev libpococrypto9 libpocodata9\n  libpocofoundation9 libpocomysql9 libpoconet9 libpoconetssl9 libpocoodbc9\n  libpocosqlite9 libpocoutil9 libpocoxml9 libpocozip9 libpq-dev libpq5\n  libsctp-dev libsctp1 libsqlite3-dev libssl-dev libssl-doc libtinyxml-dev\n  libtinyxml2.6.2 lksctp-tools mysql-common python-dev python-netifaces\n  python-numpy python-setuptools python-wstool python2.7-dev sbcl uuid-dev\nUse 'apt-get autoremove' to remove them.\nThe following extra packages will be installed:\n  ros-indigo-actionlib-msgs ros-indigo-catkin ros-indigo-cpp-common\n  ros-indigo-gencpp ros-indigo-genlisp ros-indigo-genmsg ros-indigo-genpy\n  ros-indigo-geometry-msgs ros-indigo-message-generation\n  ros-indigo-message-runtime ros-indigo-roscpp-serialization\n  ros-indigo-roscpp-traits ros-indigo-rostime ros-indigo-std-msgs\n  ros-indigo-tf2-msgs\nThe following NEW packages will be installed:\n  ros-indigo-actionlib-msgs ros-indigo-catkin ros-indigo-cpp-common\n  ros-indigo-gencpp ros-indigo-genlisp ros-indigo-genmsg ros-indigo-genpy\n  ros-indigo-geometry-msgs ros-indigo-message-generation\n  ros-indigo-message-runtime ros-indigo-roscpp-serialization\n  ros-indigo-roscpp-traits ros-indigo-rostime ros-indigo-std-msgs\n  ros-indigo-tf2 ros-indigo-tf2-msgs\n0 upgraded, 16 newly installed, 0 to remove and 8 not upgraded.\nNeed to get 574 kB of archives.\nAfter this operation, 5,244 kB of additional disk space will be used.\nDo you want to continue? [Y/n] \nGet:1 http://packages.ros.org/ros/ubuntu/ trusty/main ros-indigo-catkin amd64 0.6.18-0trusty-20160321-102905-0700 [111 kB]\nGet ...", "CMakeLists.txt"], "url": "https://answers.ros.org/question/251737/possible-alternatives-to-ros-tf2/"},
{"title": "local planner don't follow global plan/goal when passing corners", "time": "2017-01-01 13:05:21 -0600", "post_content": [" ", " ", " ", " ", "Hi!", "Please refer to this screenshot:", "When turning corners, the robot does not follow the path (green line) provided by the global planner. Instead, it heads directly to the ", ", e.g. the plan point nearest to the local cost map boundary. In this situation, I wanted the robot to go through the door and enters the corridor. Seems like a trivial task, but it simply can't make it!", "So, the most possible problem should be in-proper ", ", ", " and ", " settings. It looks like the robot treats ", " and the other two equals ", ".", "However, I didn't try that crazy combination. What I did tried is settings like this:", "but they makes no difference, the screenshot shown is taken right under such settings.", "Are there any possible settings or problems that could cause such issue? I'm using ", " as the local planner plugin. I would be great if I can visualize the local plan in ", ", but I already set ", " for the local cost map, ", " still couldn't show anything about the local plan. Don't know why either.", "Many thanks for any help!", "Sorry for I don't have enough priority to upload image to this site directly", "I've given you some karma, please attach the image to the question.", "Thanks you! ", ", image is uploaded now"], "answer": [], "question_code": ["local goal", "pdist_scale", "gdist_scale", "occdist_scale", "gdist_scale = 1", "0", "pdist_scale = 0.6\ngdist_scale = 0.01\noccdist_scale = 0.01\n", "base_local_planner/TrajectoryPlannerROS", "rviz", "publish_frequency = 1", "rviz"], "url": "https://answers.ros.org/question/251084/local-planner-dont-follow-global-plangoal-when-passing-corners/"},
{"title": "i am trying to start turtlebot , but it is not starting.", "time": "2017-01-03 13:38:50 -0600", "post_content": [" ", " ", "After fliping start button on kobuki base i didnt get the sound of starting bot or green led on status. i checked the battery cable it is connected to 12v 1.5A perfectly, but still it didnt turn on. What could be the reason behind this and how can I start my bot ?", " suggests that the kobuki comes with a 19V power supply; are you sure that you have the right power supply?"], "answer": [], "url": "https://answers.ros.org/question/251211/i-am-trying-to-start-turtlebot-but-it-is-not-starting/"},
{"title": "roslibjs tutorials not working", "time": "2017-01-02 09:37:11 -0600", "post_content": [" ", " ", "Hello,", "I have installed ROS indigo at work and kinetic at home and with both installations the tutorials dont work properly. Here is how I start it:", "1.) roscore", "2.) roslaunch rosbridge_server rosbridge_websocket.launch", "3.) rosrun actionlib_tutorials fibonacci_server", "4.) Browser: localhost:9090", "The output I get is: Can \"Upgrade\" only to \"WebSocket\".", "Is there any working tutorial on how to use this framework? The tutorial seems incomplete to me: \"You can open up the file directly in the browser without running a web server\" - no information given which URL one has to type, so I typed localhost:9090, then tried with /fibonacci_server then other paths, with no success. It is a pain to have such a powerful tool with the tutorials not working.", "Please give me a hint, this will be very appreciated.\nThanks in advance!", "\"open up the file directly in the browser\" means (in Firefox fi): ", ", browse to the ", " file and click 'open'. There is no URL involved (well, ", " is technically a URI, but ..). Can you try to see whether that works?", "rosbridge_server does not provide a web server to serve the HTML files. ", " has given the correct syntax. The requirement is the browser must be running on the same machine as where the file exists. On your ROS machine, you can drag the HTML file onto your browser.", "Thank you for your clarification, I haven't thought about this. Unfortunately I am running it on a headless raspberry pi with Ubuntu Server. To my understanding, it should work, if I install apache on it and run the html remotely within the network. I'm going to try that out now.", "Ok, I can open the files now, after I copied the files to /var/www/html/ and also copied the whole roslibjs folder to it. Connecting to ip/roslibjs/examples/fibonacci.html returns: \"Error in the backend! Connection closed.\" And connecting to ip/fibonacci.html returns: Connecting to rosbridge...", "If you add a web server, you have a lot of flexibility. I did something similar (using lighttpd). The only issue I ran into was needing to set the \"ws:\" URL to use the IP address of the RPi.", " You can see my example here:  ", "Strange thing happened: I did the same thing at my work PC: copy roslibjs folder to /var/www/html and called the file via apache and it works without a problem here. It's on localhost but it should be the same, when accessing it from another PC in the network. Thanks for helping!", "It looks like you've mostly answered your own question. You may want to post your solution as an answer (", ")"], "answer": [], "question_code": ["File -> Open File ...", ".html", "file://"], "url": "https://answers.ros.org/question/251132/roslibjs-tutorials-not-working/"},
{"title": "Moveit! move_group segmentation fault on Ubuntu Mate 16.04 with Raspberry Pi 3 (ROS kinetic)", "time": "2016-12-22 05:08:26 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone, ", "I am currently trying to run ROS kinetic with Moveit! on my Raspberry Pi 3 board.", " I successfully installed Ubuntu Mate 16.04 ( ", " ), ROS kinetic ( ", " ) and Moveit! packages on the Raspberry. ", "I also successfully built all the packages for my custom robot with catkin. And I can launch my nodes, set robot_description param, etc... The problems comes when I launch move_group (with move_group.launch from my moveit config folder)", "Note that everything is working fine on my Ubuntu 16.04 desktop installation, I created and tested all my packages. Here is what is happening on my laptop, while running \"roslaunch my_robot_package_moveit_config move_group.launch\"  :", "However, on the Raspberry Pi 3 with Ubuntu Mate, with the ", " :", "I also launched move_group.launch with debug:=true :", " I also tried with Ubuntu ARM ( ", " ) and I got the same error. ", "Does anyone know how to fix that ? Is it a Raspberry Pi relative problem ? Or Ubuntu version problem ? Or a Moveit! problem ?", "Thanks", "Apologies for a question unrelated to yours, but I'm a ROS beginner. Based on your ROS/Pi experience:\nFor a new project, I can get a Pi 3, or wait until Feb 29, when the Pi 4 may (or may not) come out. Some speculate there will be no update this year. Is a Pi 3 sufficient for ROS, or should I wait?", " I've heard that there will be no Pi 4 for 2017, but many improvement on OSes around RPI3. RPI3 is powerful enough for most ROS applications, so I think you'd better start with Pi3 now.", "Hey, I'm facing the same issue. Did you manage to solve this?"], "answer": [" ", " ", "I finally solved my problem :", "I had to remove all geometry stuff from my URDF file :  visual and collision, including box, cylinder and meshes. That way move_group is not having this segfault and everything is running.", "So I'm guessing I am missing some dependencies or packages on my Raspberry Pi 3 with armhf (reminder : everything works well on my 64 bit laptop and on my 32 bit laptop).", "Note : this is only a partial answer, as now I have another problem : collisions will not be detected because I removed them from the URDF. ", "This is not an answer at all.. As you stated collision testing doesn't work if you remove the collision bodies. :)", "I know there have been problems with the binary package of FCL on non-x86 platforms in the past. Maybe this is still a problem?\nYou could try to build FCL from source.", " thanks for your advice, I will try that when I have time and will let you know about the result"], "question_code": ["...\n...\n[ INFO] [1482395047.420033154]: Loading robot model 'my_robot'...\n[ INFO] [1482395047.511544563]: Publishing maintained planning scene on 'monitored_planning_scene'\n[New Thread 0x7fffcbdf8700 (LWP 5206)]\n...\n...\n", "NODES\n  /\n    move_group (moveit_ros_move_group/move_group)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[move_group-1]: started with pid [17610]\n[ INFO] [1482398385.724625628]: Loading robot model 'my_robot'...\n[ INFO] [1482398386.383339632]: Loading robot model 'my_robot'...\n[move_group-1] process has died [pid 17610, exit code -11, cmd /opt/ros/kinetic/lib/moveit_ros_move_group/move_group __name:=move_group __log:=/home/niryo/.ros/log/d73c13fc-c826-11e6-b1b9-b827eb50f10b/move_group-1.log].\nlog file: /home/my_robot/.ros/log/d73c13fc-c826-11e6-b1b9-b827eb50f10b/move_group-1*.log\n", "...\n... \n[ INFO] [1482398405.376777619]: Loading robot model 'my_robot'...\n\nThread 1 \"move_group\" received signal SIGSEGV, Segmentation fault.\n0x76f73f96 in std::_Rb_tree_iterator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, double> > std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, double>, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, double> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, double> > >::_M_emplace_hint_unique<std::piecewise_construct_t const&, std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&>, std::tuple<> >(std::_Rb_tree_const_iterator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, double> >, std::piecewise_construct_t const&, std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&>&&, std::tuple<>&&) ()\n   from /opt/ros/kinetic/lib/libmoveit_planning_scene_monitor.so.0.9.3\n...\n...\n"], "url": "https://answers.ros.org/question/250606/moveit-move_group-segmentation-fault-on-ubuntu-mate-1604-with-raspberry-pi-3-ros-kinetic/"},
{"title": "Kobuki - Auto Docking - Action server is not connected yet", "time": "2017-02-03 01:52:53 -0600", "post_content": [" ", " ", " ", " ", "Hi All,", "A couple of questions - I've just got a Kobuki bot and managed to get most of the Kobuki stack complied on Pi 3 - both talking to one another now - Huzzar! :)", "Wanted to try out the auto dock but is complains that it's waiting for the \"Action server is not connected yet\". ", "Is this something I need to write specifically or should I be using move_base or something else I've missed?", " On a related note - given a use case that the robot is navigating to some location on a map and the battery drops below safe limits then it needs to return to base does this become a case of writing an action client to check for and deal with this situation (i.e. send goal commands  ", "  to find - on the map - then dock with the docking station?) ", ":", "This is what the mobile_base_nodelet dies with:", "Thanks", "Mark"], "answer": [" ", " ", "I believe you need to first setup the action server with:", "Then you can activate auto docking with:", "There is more info on the ", ", but it sounds like you just missed step one.", "Hi thanks for the reply- I've been trying that. For some reason the mobile_base_nodelet_manager- just dies. No apparent reason that I can see and nothing in the logs.", "Does your error match ", "? Also, just in case, would you happen to be doing ", " instead of ", " when the kobuki has already been launched? Because ", " would be trying to start another nodelet manager.", "Looking at that issue it does look the same. I've made the changes mentioned there but as soon as I bring up the last service (roslaunch kobuki_auto_docking activate.launch --screen) then the mobile_base_nodelet_manager just dies. Either with the error in ", " or the one mentioned in that issu"], "question_code": ["process[diagnostic_aggregator-4]: started with pid [13370] nodelet: /usr/local/include/eigen3/Eigen/src/Core/DenseStorage.h:109: Eigen::internal::plain_array<T, Size, MatrixOrArrayOptions, 16>::plain_array() [with T = double; int Size = 4; int MatrixOrArrayOptions = 0]: Assertion `(internal::UIntPtr(eigen_unaligned_array_assert_workaround_gcc47(array)) & (15)) == 0 && \"this assertion is explained here: \" \"http://eigen.tuxfamily.org/dox-devel/group__TopicUnalignedArrayAssert.html\" \" **** READ THIS WEB PAGE !!! ****\"' failed.\n [mobile_base_nodelet_manager-2] process has died [pid 13368, exit code -6, cmd /opt/ros/kinetic/lib/nodelet/nodelet manager __name:=mobile_base_nodelet_manager __log:=/home/pi/.ros/log/1879dcbe-eae1-11e6-9b24-b827ebd5620d/mobile_base_nodelet_manager-2.log]. log file: /home/pi/.ros/log/1879dcbe-eae1-11e6-9b24-b827ebd5620d/mobile_base_nodelet_manager-2*.log\n"], "answer_code": ["roslaunch kobuki_auto_docking minimal.launch\n", "roslaunch kobuki_auto_docking activate.launch\n", "compact.launch", "minimal.launch", "compact.launch"], "url": "https://answers.ros.org/question/253680/kobuki-auto-docking-action-server-is-not-connected-yet/"},
{"title": "Best RGBD camera for ROS on a Raspberry Pi", "time": "2017-02-06 07:05:48 -0600", "post_content": [" ", " ", "Hi I'm building a robot based on the raspberry pi (Rpi) and was thinking of hooking up an RGB-D (depth) camera.", "I've looking at:\nthe Intel Realsense Developer Kit\nthe Asus Xtion pro\nSoftkinetic cameras, e.g. DepthSense\u00ae 525", "The robot is going to maps rooms so I'll need atleast a 1 to 2 meter range. ", "I'm still with a RPi B+ running ROS Jade on Wheezy, so limited to USB 2.0 I think. I have a battery pack (10w, 5200mAh) which needs to power the RPi with:\n1) USB Wifi dongle \u00b1 1W\n2) A sensor hat  - \u00b10.25W?\n3) A arduino nan0 - \u00b10.25W\n4) RPi  - \u00b1 1 W", "So I should have around 10-(20% due to a regulator) - 3  = \u00b1 5W available. ", "Any recommendations?", "Thanks!", "Hi, I was wondering if you ever bought the RGB-D camera and got it to work with your RPi? I've just started on a pre-master thesis project, where the final goal is to develop a RPi 3-based drone to map rooms, but first I need to develop an indoor positioning system such that the drone knows where it", ", I bought a softkinetic DS525 but never got down to testing it on my RPi3. I should be working on it towards the end of this month. Although in retrospect I think the best option would be the Intel Realsense since it has official drivers for ROS.", "Thanks for the reply.", " However, I don't think the Intel RealSense would be best, as it requires USB 3.0, while the RPi 3 uses USB 2.0 ports.  ", " Indeed! Ignore my comment - I had forgotten why I didnt go for the RealSense. Regarding the softkinetic camera - there was a third party ros package that I've tested on an x84_64 machine which worked\n ", "  I'll test in on my RPi3 and see if it works ", ", I've successfully compiled  ", "  on my RPi3 and it looks like the Softkinetic DS325 camera is publishing data. ", "I've had the following cameras work for ros slam on rpi: Kinect v1, Intel r200"], "answer": [], "url": "https://answers.ros.org/question/253918/best-rgbd-camera-for-ros-on-a-raspberry-pi/"},
{"title": "when I run roslaunch rbx1_bringup turtlebot_minimal_create.launch on turtlebot 2,the following occured,why?", "time": "2017-02-17 01:20:04 -0600", "post_content": [" ", " ", "[ERROR] [WallTime: 1487314684.230445] Failed to contact device with error: [Distance, angle displacement too big, invalid readings from robot. Distance: 8.20, Angle: 147.45]. Please check that the Create is powered on and that the connector is plugged into the Create.", "Hello,", "I have the same problem did you find the solution please.", "Thank you in advance."], "answer": [" ", " ", "I think it has something to do with the kobuki base vs the rhoomba base. I am working with a Turtlebot2 with a kobuki base and I got the same error but when I ran roslaunch turtlebot_bringup minimal.launch instead of roslaunch rbx1_bringup turtlebot_minimal_create.launch, everything worked fine. "], "url": "https://answers.ros.org/question/254912/when-i-run-roslaunch-rbx1_bringup-turtlebot_minimal_createlaunch-on-turtlebot-2the-following-occuredwhy/"},
{"title": "Sonar Sensor Reading on Eddiebot", "time": "2017-02-18 17:27:41 -0600", "post_content": [" ", " ", "I've gotten Eddiebot installed fully, I'm looking at the topics:", "I see the readings for the IR sensors and can validate they're reading in distance measurements, however the readings for both cliff_right_signal and cliff_left_signal go back and forth between 0 and 4. Both sonar sensors have the green LED on them flashing and I've tried moving the connector between a few different pins. Right now they're hooked up to slot 1 and slot 2 in the I/O part of the board. What slot should they be installed in?"], "answer": [], "question_code": ["rostopic echo /eddiebot_node/sensor_state\n"], "url": "https://answers.ros.org/question/255051/sonar-sensor-reading-on-eddiebot/"},
{"title": "How to connect RX-24 F dynamixel servo with PC with simple USB-RS-485 converter? [closed]", "time": "2017-03-09 00:40:13 -0600", "post_content": [" ", " ", " ", " ", " Hi,\nI have purchased a RX-24F dynamixel servo. I have USB-RS485 converter with me and I did not purchase USB2Dynamixel adapter. I got to manage to power up the servo from a regulated supply. The red LED glowed for a moment and then turned off.\nI used other dynamixel port for connecting with PC.\nGND->GND\nTX+(RS-485)->D+(PIN3 of Dynamixel)\nTX-(RS-485)->D-(PIN4 of Dynamixel)\nThen I opened Dynamixel wizard and as provided in  ", "  , I searched for servo. But it was not found. Can anybody help me to identify the problem?\nthanks, ", "This looks like a basic problem with Dynamixels and how to connect them. Not ROS related.", "I'd advise you to post this to either a ROBOTIS support forum or something like the robotics SO site.", "To keep ROS Answers on topic, I'm going to close this one."], "answer": [], "url": "https://answers.ros.org/question/256555/how-to-connect-rx-24-f-dynamixel-servo-with-pc-with-simple-usb-rs-485-converter/"},
{"title": "Which routers support ad-hoc mode for real-time implementation of adhoc_communication package?", "time": "2017-02-02 20:03:38 -0600", "post_content": [" ", " ", "Hi...I am trying to establish ad-hoc network between three routers using ROS package adhoc_communication. Have built the package successfully. But when rosrun am getting \"Set Tx power\" error. So now am bothering about connection between laptop and router. Which router supports ad-hoc mode? Am using Nighthawk X4 AC2350 dual band Wi-Fi Netgear router. What settings are to be done for connecting laptop and router? Please help. Thankyou"], "answer": [], "url": "https://answers.ros.org/question/253664/which-routers-support-ad-hoc-mode-for-real-time-implementation-of-adhoc_communication-package/"},
{"title": "ROS Indigo + Turtlebot +Raspberry Pi 3", "time": "2017-02-24 09:54:01 -0600", "post_content": [" ", " ", "I've installing the same target environment software... But my turtlebot don't work...", "I have this error :", "[ERROR] [WallTime: 1487674200.166287] Failed to contact device with error: [Distance, angle displacement too big, invalid readings from robot. Distance: 0.00, Angle: -420.89]. Please check that the Create is powered on and that the connector is plugged into the Create.", "Do you have a solution for this issue ?", "Thanks !", "J\u00e9r\u00f4me"], "answer": [], "url": "https://answers.ros.org/question/255541/ros-indigo-turtlebot-raspberry-pi-3/"},
{"title": "getting data from telem2 to computer", "time": "2017-02-04 01:18:45 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am just trying to get all sensor data from Pixhawk using PX4 firmware from telem2 port to computer. ", "I set ", " and ", "when I launch, my command is ", ". ", "I don't see any message coming through when I do, for example ", ".", "Am I missing something to make this work? I looked a lot of places online and it seems like other people can do this no problem, but no one really explains the process in detail. I am using ROS kinetic and mav_ros in kinetic as well. I would really appreciate if someone can guide me further !", "UPDATE", "I used to use 5V to telem2 port to power on and use UART at the same time (assuming that UART is tolerant to 5V because I read it from somewhere online). However, I just realized that 5V didn't work for the UART of the telem2 port. It has to be 3.3V connected to telem2 port and power to turn on Pixhawk needs to come to different port. I spent several hours to make this work and hopefully this saves some of your time.", "Try to connect QGC to this port (ttyUSB0). Did you reboot after parameter change?", "Thanks vooon, I figured out what I was doing wrong.", "@majiccjae I do not know if this message will reach you but I am trying to connect Telem2 thru & USB2Serial, and I noticed that there is no communication between the pixhawk2.1 and my computer - and the wiring is good (GND tx rx). Can you explain what you did to make it working? thank you", "\nMy solution was what is on \"UPDATE\" and I am not too sure what is going on with \"pixhawk2.1\"."], "answer": [], "question_code": ["SYS_COMPANION  57600", "roslaunch mavros px4.launch fcu_url:=\"/dev/ttyUSB0:57600\"", "rostopic echo /mavros/imu/data"], "url": "https://answers.ros.org/question/253801/getting-data-from-telem2-to-computer/"},
{"title": "Rosrun executable not found", "time": "2014-05-19 05:28:29 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I'm trying to run a node from rosrun (it works when I manually enter ./program inside build directory). Yet it doesn't work when called by rosrun - it finds the directory by tab completion, but not the executable. How can I help it? I need it to use it in launch files. ", "did you run the setup script for your catkin workspace?", "Yes. The problem lies in the location of the executables. They're in catkin_ws/build/package_name and they should be somewhere in devel/lib I think. I guess it's something with the CMake file inside package source, but no idea what exactly.", "What is the exact command line of your rosrun? What is that package setup and ROS_PACKAGE_PATH?", "rosrun ros_programme ros_programme - doesn't work\n./ros_programme (build/ros_programme directory) works\n\nI'm not sure what you mean by package setup, I'm a little bit green here. After sourcing many things, this is the ROS_PACKAGE_PATH. \n\n/home/user/ros_ws/catkin_ws/install/share:/home/omikron/ros_ws/catkin_ws/install/stacks:/home/user/ros_ws/catkin_ws/src:/opt/ros/hydro/share:/opt/ros/hydro/stacks", "Where is your code and \"sourcing many things\" would be the package setup. Is the binary in any of the paths in your package path? e.g. .../install", "Additionally, the packages I put inside catkin_ws (downloaded from miscellaneous locations) usually allow me to easily use rosrun. This is the CMakeLists.txt of my package (also downloaded, but clearly somehow incomplete) \n\n", "The binary is only in build directory (catkin_ws/build/ros_programme).", "did you type ($source devel/setup.bash) in your catkin_ws before typing rosrun ..."], "answer": [" ", " ", " ", " ", "The final solution to this was to make sure to call ", " in the CMakeLists.txt, and to make sure to call it before any ", " or ", " calls.", "Looking at your CMakeLists.txt, you've named your executables ", " and ", ". The correct way to run them is with:", "Or", "The relevant lines in your CMakeLists.txt that set these names are:", "I would also expect the final binaries to be generated into devel/lib/ros_aruco rather than build/ros_aruco; are you passing any extra flags to catkin_make that might be changing this behavior?", "Nah, I've just modified the names for some clarity. The real package name is ros_aruco and I want to call it by running rosrun ros_aruco ros_aruco. So still nothing. ;/\n\nSorry for confusing things.", "No, just \"catkin_make\".", "catkin_package() did the trick for me! Thanks!", " ", " ", "I ran across the same error when trying to rosrun after doing a ", ", it turned out I had the install TARGETS set to ", " instead of ", " (because I had copied that from somewhere else?), and the executables were going into install/bin rather than install/lib/my_package.", "Thanks! This solved my problem! I mistook adding my executable to the part for library installation. The default RUNTIME DESTINATION for libraries and executable are different.", " ", " ", "The correct answer to this problem is to add ", "at the beginning of CMakeLists.txt. ", "You should NOT do this with catkin; the build and install process expects your executables to be placed in the devel directory, and it violates the concept of out-of-source builds.", "I'm open to any new suggestions that works AND will place the exe in the devel directory. So far I couldn't find any.", "Looking at your ROS_PACKAGE_PATH, it looks like you're sourcing the install directoy, but you don't have any install rules for your executable in your CMakeLists. Have you tried sourcing the setup.bash in the devel directory instead?", "You mean source devel/setup.bash in my catkin_ws? That's the first thing I do, always. I think I should add it to bashrc. :P Anyway, my ugly line of code is the only thing so far that allows me to run my node from rosrun.", "The ROS_PACKAGE_PATH in your comment contains catkin_ws/install/... paths instead of catkin_ws/devel/... paths. Can you confirm that that is still the case? Can you list the exact commands you're using to build and source your workspace?", "http://pastebin.com/rRKEQCXp", "If you do a build without set(EXECUTABLE_OUTPUT_PATH...), where does your binary end up: `find . -name ros_aruco` ?", "catkin_ws/build/ros_aruco"], "question_code": ["[rosrun] Found the following, but they're either not files,\n[rosrun] or not executable:\n[rosrun]   /home/user/ros_ws/catkin_ws/src/ros_programme\n"], "answer_code": ["catkin_package()", "add_executable()", "add_library()", "ros_aruco", "subpose", "rosrun ros_programme ros_aruco\n", "rosrun ros_programme subpose\n", "add_executable(ros_aruco src/ros_aruco.cpp)\nadd_executable(subpose src/subpose.cpp)\n", "catkin_make install", "CATKIN_GLOBAL_BIN_DESTINATION", "CATKIN_PACKAGE_BIN_DESTINATION", "set(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin)\n"], "url": "https://answers.ros.org/question/166926/rosrun-executable-not-found/"},
{"title": "Speed up 'rostopic list'", "time": "2017-03-29 08:31:04 -0600", "post_content": [" ", " ", "Hey there,", "I'm running ROS-Base Indigo (Bare Bones) on a low-power computer and I noticed that when I run ", " it takes ~5 seconds to answer. As expected, also ", " take the same amount of time. ", "Of course, I know the problem is on the computational power of the cpu but my question is if there is any way to speed up ", " commands with, for instance, some kind of cache.", "Thanks in advance", "are you sure the lag is due to computer power and not due to a network issue?", "How can I disambiguate? All the TCP connections seem to be fine.", "Btw, the computer that I'm using has a AMD Geode LX-800 500MHz CPU with 512MB RAM.", "are you physically connected to the low-power computer (keyboard and screen) or is it on your network and you are accessing it from another computer?", "Yes, I'm physically connected to it."], "answer": [], "question_code": ["rostopic list", "rostopic pub", "rostopic"], "url": "https://answers.ros.org/question/258169/speed-up-rostopic-list/"},
{"title": "How to change MoveIt! goal state through code?", "time": "2017-03-20 06:12:04 -0600", "post_content": [" ", " ", " ", " ", "I am working with ROS Indigo and MoveIt! and I already set up my robot through the setup assistant, and In Rivz I can see the start and goal state, in green and orange. I can manually move the orange goal state from Rviz and then plan and execute a trajectory in order to reach it. Otherwise, I have a piece of code that sets a goal trajectory and plans and executes the whole trajectory to it. ", "But I would like to just move the (orange) goal state to a new position through a piece of code, without planning any trajectory. Just changing the goal state. How can I do it?", "Please stop editing your posts over and over again.\nThis sends out notification emails every time to everyone who watches ros-answers.", "Oh damn, I am sorry, I didn't know. I just wanted to have it pop up at the top of the question list in the Home page. I will stop now that I have an answer.", "Hey you said that you had a piece of code that sets a goal trajectory and plans and executes that trajectory?  I am struggling to figure out how to do this, would you mind giving some details?"], "answer": [" ", " ", "MoveIt's RViz display is mostly independent. The orange goal state corresponds to the goal state ", ".\nThere is no such thing as a \"global goal state\".", "So actually you are looking for a way to tell the ", " to update the robot state it maintains as its goal state. Others asked for more general remote-control functionality of the whole motion planning display in the past.\nThere is no satisfying implementation of this at the moment. Someone implemented a small part of it a while ago, but apparently lost interest. Since then, nobody provided patches to improve on the current state...", "So this is what you ", " do out of the box:", "If you check the \"Allow External Comm.\" box in the planning tab, then you can send an ", " message to\nthe ", " topic and the goal state will be set to ", " of the robot.", " This is setup here:  ", " and implemented\n ", "  . ", "It is pretty straight-forward and at most an afternoon of work to add another callback that receives a ", " and updates the start state or goal state with it.", "A patch for this (and a pull-request for the moveit repository) would be awesome!", " I checked that code and indeed it's really a pity that nobody tried to expand it. Anyway, maybe it is enough to call a planning_display_->setQueryGoalState(state); setting the wanted state. Though, trying it, I have the following problem:  ", "Yes and no. You can't do that from outside the rviz display, i.e. from your own node.\nYou can just add another callback and subscriber though, that receives a ", " and calls ", " with that one.", "I see. But I prefer not modifying the source code, since now it's installed with binaries and I didn't want to put my hands on it. Is there a way for doing it with MoveIt! installed from binaries?", "It is quite easy to compile moveit locally (see install section on the website).\nIf you implement the missing function and add a pull-request to the repository, we could have it in the next release and then you don't need your own source build anymore :)\nThis is how open source contributions work...", "If you are not willing to edit MoveIt source code, then no. You can't change the goal start arbitrarily (until someone contributed this functionality).", " A tested patch is available here:  ", "  . A PR will be made soon. "], "answer_code": ["std_msgs/Empty", "/rviz/moveit/update_goal_state", "moveit_msgs/RobotState.msg", "moveit_msgs/RobotState.msg", "planning_display_->setQueryGoalState()"], "url": "https://answers.ros.org/question/257394/how-to-change-moveit-goal-state-through-code/"},
{"title": "Arduino Kalman Filter", "time": "2017-03-26 21:14:57 -0600", "post_content": [" ", " ", " ", " ", "I have an Arduino Mega acting as a low level controller, forwarding motor encoder, IMU and other sensor data to the host computer.", "However, since it's already processing encoder and IMU data, and because my host is a little underpowered processing other sensor data, I'd like the Arduino to use a Kalman filter to produce improved odometry data directly. Is there anything in ROS for this that would run on an Arduino?", "I'm aware of a few projects, like ", ", but they only seem to run on traditional ARM or x86 architectures.", "Not Kalman, but Direction Cosine Matrix: ", "Be aware that AVRs can not to floating point in hardware, they have to emulate this feature in software, which is slow.", "About ROS-package: Most ROS packages need a complete operating system, low level stuff is handled elsewhere."], "answer": [], "url": "https://answers.ros.org/question/257937/arduino-kalman-filter/"},
{"title": "base_footprint not being updated on map", "time": "2017-03-24 12:07:11 -0600", "post_content": [" ", " ", " ", " ", "I have added a prebuilt map on RVIZ( as .yaml file using Map_Server) and have executed AMCL. \nI am able to see Pose Array, Map, LaserScan, TF, Pose, odom, and RobotModel on the side menu bar of RVIZ.", "Problem:base_footprint is not being updated and laser scan is not being matched to the prebuilt map. ", "I am able to find the shortest path (green line) to destination goal, and pose array is being updated as the robot moves to a new random location ", "Any ideas on why this might be happening and how to fix it??\nI am trying to accomplish autonomous navigation using a prebuilt map. ", "Thanks!", "Here is my launch file: ", "<launch> \n    <node name=\"map_server1\" pkg=\"map_server\" type=\"map_server\" args=\"$(find map_server)/office.yaml\"/>", "\n      <node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_link_to_laser\" args=\"0.0 0.0 0.0 0.0 0.0 0.0 /base_link /laser 40\"/>", "<node pkg=\"laser_scan_matcher\" type=\"laser_scan_matcher_node\" name=\"laser_scan_matcher_node\" output=\"screen\">\n        ", " ", " ", "\n      </node>", "</launch>"], "answer": [], "question_code": ["<!--- Run AMCL -->\n<include file=\"$(find amcl)/examples/amcl_diff.launch\" />\n\n<node name=\"movebase\" pkg=\"move_base\" type=\"move_base\" respawn=\"false\" output=\"screen\">\n\n<rosparam file=\"$(find navigation2d_example)/param/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n<rosparam file=\"$(find navigation2d_example)/param/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n<rosparam file=\"$(find navigation2d_example)/param/local_costmap_params.yaml\" command=\"load\" />\n<rosparam file=\"$(find navigation2d_example)/param/global_costmap_params.yaml\" command=\"load\" />\n<rosparam file=\"$(find navigation2d_example)/param/base_local_planner_params.yaml\" command=\"load\"/>\n</node>\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"world_map\" args=\"0 0 0 0 0 0 world map 10\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"map_odom\" args=\"0 0 0 0 0 0 map odom 10\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"odom_base_link\" args=\"0 0 0 0 0 0 odom base_footprint 20\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_foorptint_movebase\" args=\"0 0 0 0 0 0 base_footprint move_base 20\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"movebase_baselink\" args=\"0 0 0 0 0 0 move_base base_link 10\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_link_scanmatcher_frame\" args=\"0 0 0 0 0 0 base_link scanmatcher_frame 25\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_link_nav\" args=\"0 0 0 0 0 0 base_link nav 13\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_link_base_frame\" args=\"0 0 0 0 0 0 base_link base_frame 13\" />\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"baselink_laser\" args=\"0 0 0 0 0 0 base_link laser 10\" />\n"], "url": "https://answers.ros.org/question/257816/base_footprint-not-being-updated-on-map/"},
{"title": "Issues launching ROS on startup", "time": "2014-03-15 21:03:27 -0600", "post_content": [" ", " ", " ", " ", "ROS: groovy\nOS: ubuntu 12.04", "What we are trying to do is have roscore start upon boot (have this working now) and then execute a launch file.\nI am not a linux person, so I feel I am really not approaching this the right way. Currently, this is what I have attempted:", "1) roscore is launched via a script in /etc/init.d. This is fine.\n2) I want to launch my launch file on boot. Manually, I do it like this: roslaunch rover.launch. It works great.\n3) I would like this to start on boot, so I tried the following:\ninside new script (boot.sh) - note i disabled password prompt for sudo: ", "bootRoverLaunch.sh:", "4) I went to \"startup applications\" and added \"bash /home/roboops/groovycat/scripts/boot.sh\"", "5) Tried rebooting. Roscore starts, and the launch files executes. However, the nodes malfunction and don't work correctly (They work fine when started using the launch file directly in a terminal window. Oddly, it also works fine if I log out and back in once)", "More specifically, here is the problem when using the above script to launch:", "I am trying to run two nodes:", "-joystick node (publishes joy topic)\n-motor control node (listens to joy topic and outputs commands to motor controllers via serial)", "Joystick node starts successfully and If I do rostopic echo joy, I can see the values. \nMotor control node is listed as running but if I run roswtf it says:", "The idea is, I need everything to automatically start on power-up. There will be no input devices/monitor connected to it when it is in use."], "answer": [" ", " ", "We recommend using the ", " package we developed. It's simple, seems to do everything you need, and has already been deployed on dozens of robots.", "I notice the package only has a hydro/devel branch. Does this work for groovy?", "Hi Andrew, yes, robot_upstart should work with groovy as well as hydro, as long as you don't mind doing a source install of it in your workspace.", "Hi Mike,", "I'm also trying to use robot_upstart since 3hours without results.... is it possible to use this package with indigo???  ", "Tks", ": I'm not Mike, but I know we use it to run our ", ", which is Indigo-only. Have you tried using", "to install it? Instructions to use it are on both the ROS wiki and its github page.", "Note, this package is no longer supported on Kinetic.", "Support for Kinetic has been added", " ", " ", " I start my roslaunch on start up using cron ( ", " ) and screen ( ", " ). ", "I have a /home/user/autostart/autostart_screens.sh that get executed by cron @reboot (Meaning whenever the PC is booted). You can set these startup files via ", " command. See ", ". My current ", " gives:", "My autostart_screens.sh starts and detaches two screen sessions and logs any thing happening inbetween to a log file. It looks like:", "Note that it is not blocking as it just starts the screens.", "The start_roscore.sh looks like:", "The start_roslaunch.sh looks like:", "All logs go to /home ...", "Why are you running both roscore and roslaunch in separate scripts? roslaunch will automatically run roscore if needed.", " ", " ", " ", " ", "I start my roscore on boot simply by adding this line in \"/etc/rc.local\" before \"exit0\":", "the \"> /dev/null\" means its logs dont get stored anywhere and \"&\" at the end means it starts in the background. You cant start roscore in root mode, that's why \"su pi -c\" is at the start of the command. Replace \"/home/pi/YOURWS\" with your workspace name and destination.", "Works, and it's simple !  I have spent hours trying to use robot_upstart, but it needs too  many files and devices to have their permissions changed. I used a launch file:\nsu ubuntu -c \"source /home/ubuntu/catkin_ws/devel/setup.sh ; roslaunch biped biped_robot.launch  > /dev/null 2>&1 &\""], "question_code": [" sudo su - roboops\n /home/roboops/groovycat/scripts/bootRoverLaunch.sh\n", "cd /home/roboops/groovycat/\nsource devel/setup.sh\nsleep 5\nroslaunch rover.launch > /home/roboops/groovycat/scripts/startup.log\n", "Found 3 error(s).\n\nERROR Could not contact the following nodes:\n * /Drivetrain\n\nERROR The following nodes should be connected but aren't:\n * /Drivetrain->/rosout (/rosout)\n\nERROR Errors connecting to the following services:\n * service [/Drivetrain/get_loggers] appears to be malfunctioning: Unable to communicate with service [/Drivetrain/get_loggers], address [rosrpc://Rover:58631]\n * service [/Drivetrain/set_logger_level] appears to be malfunctioning: Unable to communicate with service [/Drivetrain/set_logger_level], address [rosrpc://Rover:58631]\n"], "answer_code": ["sudo apt-get install ros-indigo-robot-upstart\n", "crontab", "crontab --help", "crontab -l", "# Edit this file to introduce tasks to be run by cron.\n# \n# Each task to run has to be defined through a single line\n# indicating with different fields when the task will be run\n# and what command to run for the task\n# \n# To define the time you can provide concrete values for\n# minute (m), hour (h), day of month (dom), month (mon),\n# and day of week (dow) or use '*' in these fields (for 'any').# \n# Notice that tasks will be started based on the cron's system\n# daemon's notion of time and timezones.\n# \n# Output of the crontab jobs (including errors) is sent through\n# email to the user the crontab file belongs to (unless redirected).\n# \n# For example, you can run a backup of all your user accounts\n# at 5 a.m every week with:\n# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/\n# \n# For more information see the manual pages of crontab(5) and cron(8)\n# \n# m h  dom mon dow   command\n@reboot bash /home/user/autostart/autostart_screens.sh\n", "# !/bin/bash\nLOG_FILE=/home/user/autostart/logs/log_autostart_screens.txt\n\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"Running autostart_screens.sh\" >> ${LOG_FILE}\necho $(date) >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"Logs:\" >> ${LOG_FILE}\n\nset -e\nset -v\n\n{\n\nscreen -d -m bash /home/user/autostart/start_roscore.sh\nscreen -d -m bash /home/user/autostart/start_roslaunch.sh\n\n} &>> ${LOG_FILE}\n", "# !/bin/bash\nLOG_FILE=/home/user/autostart/logs/log_start_roscore.txt\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"Running start_roscore.sh\" >> ${LOG_FILE}\necho $(date) >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"Logs:\" >> ${LOG_FILE}\n\nset -e\n\n{\n\nsource /opt/ros/hydro/setup.bash\nsource /home/user/workspace/ros_hydro/catkin/devel/setup.bash\n\nexport ROS_WORKSPACE=/home/user/workspace/ros_hydro/catkin\n\nexport ROS_MASTER_URI=http://192.168.1.10:11311/ ##e. g. Master\nexport ROS_IP=192.168.1.15                   ##e. g. Own IP\n\nsleep 5\n\n} &>> ${LOG_FILE}\n\nset -v\n\n{\n\nroscore\n\n} &>> ${LOG_FILE}\n", "# !/bin/bash\nLOG_FILE=/home/user/autostart/logs/log_start_roslaunch.txt\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"Running start_roslaunch.sh\" >> ${LOG_FILE}\necho $(date) >> ${LOG_FILE}\necho \"#############################################\" >> ${LOG_FILE}\necho \"\" >> ${LOG_FILE}\necho \"Logs:\" >> ${LOG_FILE}\n\nset -e\n\n{\nsource /opt/ros/hydro/setup.bash\nsource /home/user/workspace/ros_hydro/catkin/devel/setup.bash\n\nexport ROS_WORKSPACE=/home/user/workspace/ros_hydro/catkin\n\nexport ROS_MASTER_URI=http://192.168.1.10:11311/\nexport ROS_IP=192.168.1.15\n\nsleep 8\n\n} &>> ${LOG_FILE}\n\nset -v\n\n{\n\nroslaunch my_pkg my_launch.launch\n\n} &>> ${LOG_FILE}\n", "su pi -c \"source /home/pi/YOURWS/devel/setup.sh ; roscore  > /dev/null 2>&1 &\"\n"], "url": "https://answers.ros.org/question/140426/issues-launching-ros-on-startup/"},
{"title": "how to combine coordinate program and auto docking program", "time": "2017-04-15 04:30:18 -0600", "post_content": [" ", " ", "hi, i have problem, the coordinate program which makes the robot go to specific coordinate already done but now i need to add auto docking program in it. So the robot will automatically perform auto docking when it detects its battery is low. How to make it? below is my program for coordinate", "import rospy\nimport actionlib\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom math import radians, degrees\nfrom actionlib_msgs.msg import *\nfrom geometry_msgs.msg import Point", "class map_navigation():", "def choose(self):", "def __init__(self):", "def shutdown(self):\n        # stop turtlebot\n        rospy.loginfo(\"Quit program\")\n        rospy.sleep()", "def moveToGoal(self,xGoal,yGoal):", "if __name__ == '__main__':\n    try:", "any answers?"], "answer": [], "question_code": ["choice='q'\n\n\nrospy.loginfo(\"|-------------------------------|\")\nrospy.loginfo(\"|PRESSE A KEY:\")\nrospy.loginfo(\"|'0': Tempat Asal \")\nrospy.loginfo(\"|'1': Table 1 \")\nrospy.loginfo(\"|'2': Table 2 \")\nrospy.loginfo(\"|'3': Table 3 \")\nrospy.loginfo(\"|'q': Quit \")\nrospy.loginfo(\"|-------------------------------|\")\nrospy.loginfo(\"|WHERE TO GO?\")\nchoice = input()\nreturn choice\n", "# declare the coordinates of interest\nself.xAsal = 1.63\nself.yAsal = 2.15\nself.xTable1 = 2.41\nself.yTable1 = -0.621\nself.xTable2 = 1.92\nself.yTable2 = -2.62\nself.xTable3 = 2.02\nself.yTable3 = 0.907\nself.goalReached = False\n# initiliaze\nrospy.init_node('map_navigation', anonymous=False)\nchoice = self.choose()\n\nif (choice == 0):\n\n  self.goalReached = self.moveToGoal(self.xAsal, self.yAsal)\n\nelif (choice == 1):\n\n  self.goalReached = self.moveToGoal(self.xTable1, self.yTable1)\n\nelif (choice == 2):\n\n  self.goalReached = self.moveToGoal(self.xTable2, self.yTable2)\n\nelif (choice == 3):\n\n  self.goalReached = self.moveToGoal(self.xTable3, self.yTable3)\n\nif (choice!='q'):\n\n  if (self.goalReached):\n    rospy.loginfo(\"Congratulations!\")\n    #rospy.spin()\n\nelse:\n    rospy.loginfo(\"Hard Luck!\")\n\nwhile choice != 'q':\n  choice = self.choose()\n  if (choice == 0):\n\n    self.goalReached = self.moveToGoal(self.xAsal, self.yAsal)\n\n  elif (choice == 1):\n\n    self.goalReached = self.moveToGoal(self.xTable1, self.yTable1)\n\n  elif (choice == 2):\n\n    self.goalReached = self.moveToGoal(self.xTable2, self.yTable2)\n\n  elif (choice == 3):\n\n    self.goalReached = self.moveToGoal(self.xTable3, self.yTable3)\n\n  if (choice!='q'):\n\n    if (self.goalReached):\n      rospy.loginfo(\"Congratulations!\")\n      #rospy.spin()\n\n\n    else:\n      rospy.loginfo(\"Hard Luck!\")\n", "  #define a client for to send goal requests to the move_base server through a SimpleActionClient\n  ac = actionlib.SimpleActionClient(\"move_base\", MoveBaseAction)\n\n  #wait for the action server to come up\n  while(not ac.wait_for_server(rospy.Duration.from_sec(10.0))):\n          rospy.loginfo(\"Waiting for the move_base action server to come up\")\n\n\n  goal = MoveBaseGoal()\n\n  #set up the frame parameters\n  goal.target_pose.header.frame_id = \"map\"\n  goal.target_pose.header.stamp = rospy.Time.now()\n\n  # moving towards the goal*/\n\n  goal.target_pose.pose.position =  Point(xGoal,yGoal,0)\n  goal.target_pose.pose.orientation.x = 0.0\n  goal.target_pose.pose.orientation.y = 0.0\n  goal.target_pose.pose.orientation.z = 0.0\n  goal.target_pose.pose.orientation.w = 1.0\n\n  rospy.loginfo(\"Sending goal location ...\")\n  ac.send_goal(goal)\n\n  ac.wait_for_result(rospy.Duration(60))\n\n  if(ac.get_state() ==  GoalStatus.SUCCEEDED):\n          rospy.loginfo(\"You have reached the destination\")\n          return True\n\n  else:\n          rospy.loginfo(\"The robot failed to reach the destination\")\n          return False\n", "    rospy.loginfo(\"You have reached the destination\")\n    map_navigation()\n    rospy.spin()\n\nexcept rospy.ROSInterruptException:\n    rospy.loginfo(\"map_navigation node terminated.\")\n"], "url": "https://answers.ros.org/question/259313/how-to-combine-coordinate-program-and-auto-docking-program/"},
{"title": "How to dynamically update mass/center of mass/inertia during Gazebo sim", "time": "2017-04-06 01:47:03 -0600", "post_content": [" ", " ", "Hi, ", "Components in the URDF files can be assigned static mass, center of mass, and inertial matrix. ", "Is there a way to update those values dynamically once the Gazebo simulation environment has started up? ", "For example, say I had a fuel tank on a robot and I wanted to decrease the weight and center of mass of the tank as a function of engine power output over time. Or maybe I have a robot expelling a liquid from a holding tank or sucking up a liquid into a holding tank. ", "I can't seem to find a solution to this in the ROS docs. ", "Thank you, \nRandy "], "answer": [" ", " ", "Yep...just needed to look a little longer. The GetMass() and SetMass functions in the ", " had what I wanted. ", "Thanks!\nRandy", " ", " ", "I think you can find the answer here:"], "url": "https://answers.ros.org/question/258673/how-to-dynamically-update-masscenter-of-massinertia-during-gazebo-sim/"},
{"title": "Kinect Installation and Setup on ROS [Updated]", "time": "2014-11-01 14:32:45 -0600", "post_content": [" ", " ", " ", " ", "Do I need any drivers to run the kinect on ROS? I ran ", " and it downloaded 5mb of files. Then I tried following the tutorial at ", " but when I ran the first command it said \"[ Info] ... No Devices Connected ...\" . lsusb shows up with the xbox nui camera and audio and a microsoft device on bus 1.... I'm using the XBOX 360 kinect and power cord from microsoft...\nWhat am I doing wrong here?\nThanks,", "Update:\nI have a kinect model 1473. Does that make any difference", "When I run hsoltani's answer I still get the ", " error"], "answer": [" ", " ", " ", " ", "I tried a number of things, so not sure what the exact combo that made it work was, but I think this should work:", "A) sudo apt-get install ROS-Indigo-OpenNI-Launch (probably similar to what you installed)", " B) Use  step 3 from this for the drivers:\n ", " C) Play with it:\n ", "If that does work, please comment.", "work for me,\nubuntu 14.04\nros indigo", "work for me too, ubuntu 14.04 ros indigo\nBut I start from step 2, or it will fail.", " ", " ", "Hi!", "Openni_launch does not work anymore under Indigo  for Kinect  , but freenect_launch  does.\nSo install freenect_launch and libfreenect :", "Then call :", "If it still does not work  , disconnect Kinect from usb and plug again.", "One more thing , it didn't work for me  when Kinect was connected through an USB 2.0 hub but directly in USB 3.0 input of pc.", " Source:\n ", " ", "\"", "\"", "It still says no devices connected... A device made by microsoft with code: 045e:02c2 showed up when i executed lsusb.... any other ideas? Also when I run ", " I get ", "Actually it says that openni2 does not support any of Kinects. openni should support. btw. I was encountering with the same problem but freenect_launch solved it. I am able to get rgb and depth data On the other side I have to change lacun files which calls openni_launch as default with freenect", "To solve my problem I also had to install the avin2 drivers. That made it work. Thanks for all your help!", "Hi luketheduke. I am trying to interface kinect to ROS indigo and am facing similar problems. Could you please tell me the entire procedure all over so that i could know where i am going wrong.", "@Namisha I made a blog post detailing how I installed the necessary software. See it here ", ".", "What I get afert luanching freenect is just the following message [INFO]  [1428079238.372035547]: Stopping device RGB and Depth stream flush. And I don't know what does it means,so is still kinect having troubles ? Because I also run rviz, trying to see an image", "I get same log, but it didn't affect anything. I can run RViz without problems.\nNo idea why it comes out.", " I get the same thing as ", ". I don't have any problems with it. Just try restarting rviz if it still doesn't show up.", " ", " ", " ", " ", "What's the model number of your kinect? Kinect 1473 doesn't work with openni2. Remove ros-indigo-openni2-launch and ros-indigo-openni2-camera. Try to install ros-indigo-openni-launch and ros-indigo-openni-camera. Then following the tutorial and launch the OpenNI driver like this: ", "roslaunch openni_launch openni.launch", "I tried that but it still doesn't work... What now?", " ", " ", "just wanted to add to the above- I had to change directory to get it to launch. in a new window enter \"cd /opt/ros/indigo/share/freenect_launch/launch\" then \"roslaunch freenect.launch\" But I'm a total newb.", "Thanks for your input, but next time, please consider putting it as a comment. ", " to the directory the launch file is in is not necessary if you source the ", " file. Then you can just use ", " without ", ". Hope this helps!", "Don't forget to include both the package name and the launch file: roslaunch freenect_launch freenect.launch", " ", " ", " I did everything mentioned in  ", "  but if I run roslaunch freenect_launch freenect.launch i'm not able to connect to Kinect at all...\n...INFO] [1485464131.648351596]: Searching for device with index = 1\nif I run freenect-glview I get: ", "Number of devices found: 1\nCould not claim interface on camera: -5 (libusb_ERROR_NOT_FOUND)\nFailed to open camera subdevice or it is not disabled.Failed to open motor subddevice or it is not disabled.Failed to open audio subdevice or it is not disabled.Could not open device...", "This problem occurs only on my intel nuc, neither on my laptop nor on my raspiIII", "Any ideas?", "Welcome to ROS Answers! This isn't an answer to the question above, so I suggest you post a question and tag it with ", " or ", " Your error message suggests that you are missing ", " ", " ", "Hola, buenas noches!... Yo tambi\u00e9n he intentado con todo hasta ahora y no he podido usar mi kinect... El modelo que yo tengo es 1414 y uso la versi\u00f3n ROS indigo... hice todo lo que han mencionado hasta ahora, ya instale y desinstale varias veces todo y aun as\u00ed no lo consigo... ayuda por favor!", "Welcome to ROS Answers! This isn't an answer to the question above, so I suggest you post a question and tag it with kinect and openni. As to your issue, it would be helpful if you could post the output of ", " and ", " in your question."], "question_code": ["sudo apt-get install ros-indigo-openni*", "no devices connected"], "answer_code": ["sudo apt-get install libfreenect-dev\n\nsudo apt-get install ros-indigo-freenect-launch\n", "roslaunch freenect_launch freenect.launch\n", "roslaunch freenect_launch freenect.launch", "No devices connected.... waiting for devices to connect", "cd", "/opt/ros/indigo/setup.bash", "roslaunch", "cd", "kinect", "openni", "libusb", "lsusb", "roslaunch freenect_launch freenect.launch"], "url": "https://answers.ros.org/question/196455/kinect-installation-and-setup-on-ros-updated/"},
{"title": "network  interfaces configure for Hokuyo ust 10lx", "time": "2017-04-19 23:09:05 -0600", "post_content": [" ", " ", "Hi all, I have a Hokuyo ust 10LX LRF, which use ethernet RJ45 connector, rather than USB.", "Sequence listed below:", "Step 1. Use  10LX's default setting , 192.168.0.10 as LiDAR IP address.", "Step 2. Set network.", "$ sudo vi /etc/network/interfaces", "auto lo", "iface lo inet loopback", "auto eth0", "allow-hotplug eth0", "iface eth0 inet static", "address 192.168.0.15", "netmask 255.255.255.0", "Step 3. Power on LiDAR with 12V@1000mA power supply, ", "Step 4. Plug RJ45 connector into PC's ethernet port.", "Step 5. However get errors saying 'Destination Host Unreachable'.", "$ ping 192.168.0.15", "64 bytes from 192.168.0.15: icmp_seq=1 ttl=64 time=0.032 ms", "64 bytes from 192.168.0.15: icmp_seq=2 ttl=64 time=0.037 ms ...", "ping PC okay, but fail to LiDAR:", "$ ping 192.168.0.10", "PING 192.168.0.10 (192.168.0.10) 56(84) bytes of data.", "From 192.168.0.15 icmp_seq=1 Destination Host Unreachable", "From 192.168.0.15 icmp_seq=2 Destination Host Unreachable", "From 192.168.0.15 icmp_seq=3 Destination Host Unreachable ...", "Thanks very much for your help!"], "answer": [" ", " ", "Check the output of ", " and make sure that there is a route going to the 192.168.0.0/24 block using eth0."], "answer_code": ["ip route"], "url": "https://answers.ros.org/question/259685/network-interfaces-configure-for-hokuyo-ust-10lx/"},
{"title": "Turtlebot minimal.launch not working with Jetson TK1", "time": "2017-04-23 05:09:29 -0600", "post_content": [" ", " ", "Hi all", "After re-flashing my Jetson TK1 to 21.4, installing ROS and Turtlebot per the wiki, minimal.launch no longer brings up the Turtlebot with the expected chime.", "The last few lines I get before the system hangs is", "Looking inside the log file shows:", "Previously I was able to run the Turtlebot with the Jetson with no issue even though there was a warning because the Jetson TK1 obviously doesn't have a battery. It looks like now some kind of shutdown signal is being sent when the BAT0 file isn't found. Has anyone experienced this?", "Cheers", "Will"], "answer": [" ", " ", "Managed to solve this - not sure which step exactly was the fix but as I've been having trouble getting Turtlebot set up with the TK1 lately I've laid out the steps below in case anyone else is going through this process:", "This worked for me - Turtlebot/Jetson TK1 on L4T 21.4/ROS Indigo - not sure if I was having a udev issue, an issue with current draw from using the Jetsonhacks postFlash script which maxes CPU usage, or just made a configuration mistake. If I was braver I might try all this with Jetpack 3.0 and ROS Kinetic as dependencies are starting to become tricky with this setup - if anyone else has had success let me know!", "W", "What is the grinch kernel for exactly? Also, the link you posted is for version 21.3... not sure if it will work right on or need some adjustment.", "Oh... after installing the grinch kernel I lost the UVCvideo module, so you can't really use this method with the r200 camera... I'm lost."], "answer_details": ["Flash with Jetpack 2.2 (this is the 21.4 L4T) - make sure to uncheck the OpenCV installation, it's too new for ROS", " Perform the kernel tweaks needed to run the Realsense camera - even if you're not using it - ros-indigo-turtlebot & associated packages won't install without citing uvcvideo errors  ", " Install ROS using the ROS for ARM scripts provided here  ", "  - make sure to do the network configuration  ", " Then install Turtlebot the normal way  ", " Then install the Grinch kernel  ", " Make sure Kobuki is the only FTDI device plugged in and run the Kobuki udev setup  ", " ", " ", " ", " "], "question_code": ["process[turtlebot_laptop_battery-8]: started with pid [13498]\nprocess[capability_server-9]: started with pid [13506]\nprocess[app_manager-10]: started with pid [13510]\nprocess[master-11]: started with pid [13517]\nprocess[interactions-12]: started with pid [13527]\nprocess[zeroconf/zeroconf-13]: started with pid [13532]\n[WARN] [WallTime: 1492941706.791976] Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]\n[ INFO] [1492941707.253415215]: Zeroconf: service successfully established [turtlebot][_ros-master._tcp][11311]\n[turtlebot_laptop_battery-8] process has finished cleanly\nlog file: /home/ubuntu/.ros/log/da9c7bfa-280b-11e7-ae6c-74da385e01c9/turtlebot_laptop_battery-8*.log\n", "[rospy.client][INFO] 2017-04-23 18:01:46,635: init_node, name[/turtlebot_laptop_battery], pid[13498]\n[xmlrpc][INFO] 2017-04-23 18:01:46,647: XML-RPC server binding to 0.0.0.0:0\n[xmlrpc][INFO] 2017-04-23 18:01:46,647: Started XML-RPC server [http://192.168.0.14:44254/]\n[rospy.init][INFO] 2017-04-23 18:01:46,647: ROS Slave URI: [http://192.168.0.14:44254/]\n[rospy.impl.masterslave][INFO] 2017-04-23 18:01:46,651: _ready: http://192.168.0.14:44254/\n[xmlrpc][INFO] 2017-04-23 18:01:46,658: xml rpc node: starting XML-RPC server\n[rospy.registration][INFO] 2017-04-23 18:01:46,659: Registering with master node http://localhost:11311\n[rospy.init][INFO] 2017-04-23 18:01:46,751: registered with master\n[rospy.rosout][INFO] 2017-04-23 18:01:46,752: initializing /rosout core topic\n[rospy.rosout][INFO] 2017-04-23 18:01:46,757: connected to core topic /rosout\n[rospy.simtime][INFO] 2017-04-23 18:01:46,762: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic\n[rosout][WARNING] 2017-04-23 18:01:46,791: Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]\n[rospy.core][INFO] 2017-04-23 18:01:46,793: signal_shutdown [Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]]\n[rospy.impl.masterslave][INFO] 2017-04-23 18:01:46,803: Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]\n[rospy.core][INFO] 2017-04-23 18:01:47,793: signal_shutdown [atexit]\n"], "url": "https://answers.ros.org/question/259982/turtlebot-minimallaunch-not-working-with-jetson-tk1/"},
{"title": "I couldn't get an ideal path while using RVIZ", "time": "2017-05-17 00:53:51 -0600", "post_content": [" ", " ", " ", " ", "Hi,Here is a image I make during my little car running ROS with move_base.I place two obstacles and make the gap narrow for the car to cross. I want the car to move from position 1 to position 2,and RVIZ shows this green line(I think it's a path).The car then keeps going until it gets to this position the image shows and it begins to rotate.It just rotates and won't go anymore.In my mind,I know this car can't cross this gap,so the car may change its path(maybe just like the yellow arrows I drew on this image) to cover this and get to the goal position.But this didn't happen.Why?", "Here are the parameters I set:", "base_local_planner_params.yaml:", "ROS:Indigo Car:Raspbian Jessie", "The global plan (probably the green line in your photo) is decided by the global planner. The local planner will just try to follow that path. If local planner somehow fails to do so (the gap is too small in this case), it should trigger the global planner to \"replan\".", "So the problem could lie with the global planner itself. What global planner are you using?", "I don't know the meanning of \"What global planner\".I use move_base\uff0cI am new to ROS,I didn't set the kind of global planner.Can I type some commands to know the kind of global planner I am using?"], "answer": [], "question_code": ["TrajectoryPlannerROS:\n    max_vel_x: 0.15\n    min_vel_x: 0.10\n    max_vel_theta: 0.15\n    min_vel_theta: -0.15\n    max_rotational_vel: 0.15\n    min_in_place_vel_theta: 0.10\n    escape_vel: -0.14\n    holonomic_robot: false\n    xy_goal_tolerance: 0.10\n    yaw_goal_tolerance: 0.2\n    transform_tolerance: 2.0\n    controller_frequency: 2.0\n    vx_samples: 50\n    vtheta_samples: 30\n    latch_xy_goal_tolerance: false\n    dwa: false\n    #############\n    occdist_scale:0.1\n    gdis_scale: 1.5\n"], "url": "https://answers.ros.org/question/261905/i-couldnt-get-an-ideal-path-while-using-rviz/"},
{"title": "Where to store calibration data?", "time": "2017-05-28 21:28:03 -0600", "post_content": [" ", " ", "I have a list of integers, returned from my IMU, that represent calibration settings. I need to load these into the IMU on every power-up. Since the settings are unique to my IMU, I don't want to commit them to version control. Is there some standard ROS location for storing small data files, such as calibration values, where I can store these values?"], "answer": [" ", " ", "I typically ", " store these things in a VCS, but then in a special package that only contains that (those) file(s).", "If you then include the serial nr (or something similar) in the name of the file, it's clear that those parameters are for that particular device.", "The only thing you have to do now is add something to your launch file (fi) that loads the file and initialises your IMU with the parameters.", "As an example: in my lab we have a package specifically for Kinect (v2) calibrations, called ", ", which contains the calibration for several of our Kinects. The launch files that tie our setups together then load the appropriate one.", "How do you handle 2d cameras with a lense that require a new calibration after e.g. a refocus?", "We've only very few of those, and if the device supports profiles with such settings we store those with the calibration data. If not, then we don't have a procedure, as manual focus adjustments will never be exact enough.", "I've seen people fork the calibration pkgs then and manage their own data."], "answer_code": ["iai_kinect2_$labname_calibration_data"], "url": "https://answers.ros.org/question/262741/where-to-store-calibration-data/"},
{"title": "Unable to use minimal.launch", "time": "2017-06-29 19:53:14 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I just install Ubuntu 16.04 and then ros kinetic and turtlebot packages on my netbook. Roscore is working fine but when I use minimal.launch its not working.", "Here are the logs:", "How can I resolve this issue?", "-Sakshi", "That looks like it's working. What problem are you having with it?", "Turtlebot is not making a sound that it usually makes when we do minimal.launch.", "Can you show the content of ", " ?", "It seems the turtlebot_laptop_battery node can't run properly."], "answer": [], "question_code": ["turtlebot@turtlebot-Aspire-E3-111:~$ roslaunch turtlebot_bringup minimal.launch ... logging to /home/turtlebot/.ros/log/50225a6a-5d2d-11e7-8174-ec0ec46b500d/roslaunch-turtlebot-Aspire-E3-111-26160.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nxacro: Traditional processing is deprecated. Switch to --inorder processing!\nTo check for compatibility of your document, use option --check-order.\nFor more infos, see http://wiki.ros.org/xacro#Processing_Order\nxacro.py is deprecated; please use xacro instead\nstarted roslaunch server http://192.168.1.102:43727/\n\nSUMMARY\n========\n\nPARAMETERS\n * /bumper2pointcloud/pointcloud_radius: 0.24\n * /cmd_vel_mux/yaml_cfg_file: /opt/ros/kinetic/...\n * /diagnostic_aggregator/analyzers/input_ports/contains: ['Digital Input',...\n * /diagnostic_aggregator/analyzers/input_ports/path: Input Ports\n * /diagnostic_aggregator/analyzers/input_ports/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/input_ports/timeout: 5.0\n * /diagnostic_aggregator/analyzers/input_ports/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/kobuki/contains: ['Watchdog', 'Mot...\n * /diagnostic_aggregator/analyzers/kobuki/path: Kobuki\n * /diagnostic_aggregator/analyzers/kobuki/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/kobuki/timeout: 5.0\n * /diagnostic_aggregator/analyzers/kobuki/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/power/contains: ['Battery', 'Lapt...\n * /diagnostic_aggregator/analyzers/power/path: Power System\n * /diagnostic_aggregator/analyzers/power/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/power/timeout: 5.0\n * /diagnostic_aggregator/analyzers/power/type: diagnostic_aggreg...\n * /diagnostic_aggregator/analyzers/sensors/contains: ['Cliff Sensor', ...\n * /diagnostic_aggregator/analyzers/sensors/path: Sensors\n * /diagnostic_aggregator/analyzers/sensors/remove_prefix: mobile_base_nodel...\n * /diagnostic_aggregator/analyzers/sensors/timeout: 5.0\n * /diagnostic_aggregator/analyzers/sensors/type: diagnostic_aggreg...\n * /diagnostic_aggregator/base_path: \n * /diagnostic_aggregator/pub_rate: 1.0\n * /mobile_base/base_frame: base_footprint\n * /mobile_base/battery_capacity: 16.5\n * /mobile_base/battery_dangerous: 13.2\n * /mobile_base/battery_low: 14.0\n * /mobile_base/cmd_vel_timeout: 0.6\n * /mobile_base/device_port: /dev/ttyUSB0\n * /mobile_base/odom_frame: odom\n * /mobile_base/publish_tf: True\n * /mobile_base/use_imu_heading: True\n * /mobile_base/wheel_left_joint_name: wheel_left_joint\n * /mobile_base/wheel_right_joint_name: wheel_right_joint\n * /robot/name: turtlebot\n * /robot/type: turtlebot\n * /robot_description: <?xml version=\"1....\n * /robot_state_publisher/publish_frequency: 5.0\n * /rosdistro: kinetic\n * /rosversion: 1.12.7\n * /turtlebot_laptop_battery/acpi_path: /sys/class/power_...\n * /use_sim_time: False\n\nNODES\n  /\n    bumper2pointcloud (nodelet/nodelet)\n    cmd_vel_mux (nodelet/nodelet)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    mobile_base (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    turtlebot_laptop_battery (laptop_battery_monitor/laptop_battery.py)\n\nROS_MASTER_URI=http://192.168.1.102:11311\n\ncore service [/rosout] found\nprocess[robot_state_publisher-1]: started with pid [26196]\nprocess[diagnostic_aggregator-2]: started with pid [26197]\nprocess[mobile_base_nodelet_manager-3]: started with pid [26198]\nprocess[mobile_base-4]: started with pid [26205]\nprocess[bumper2pointcloud-5]: started with pid [26214]\nprocess[cmd_vel_mux-6]: started with pid [26221]\nprocess[turtlebot_laptop_battery-7]: started with pid [26228]\n[WARN] [1498783500.858024]: Battery : unable to check laptop battery info [/sys/class/power_supply/BAT0/charge_full_design || /sys/class/power_supply/BAT0/energy_full_design does not exist]\n[turtlebot_laptop_battery-7] process has finished cleanly\nlog file: /home/turtlebot/.ros/log/50225a6a-5d2d-11e7-8174-ec0ec46b500d/turtlebot_laptop_battery-7*.log\n", "/home/turtlebot/.ros/log/50225a6a-5d2d-11e7-8174-ec0ec46b500d/turtlebot_laptop_battery-7*.log"], "url": "https://answers.ros.org/question/265194/unable-to-use-minimallaunch/"},
{"title": "Hardware Interface for Arduino", "time": "2017-06-08 15:05:02 -0600", "post_content": [" ", " ", "Hey,", "Recently i started my own robot project to learn ROS, using Arduino.\nSo far i just have a single Arduino + L289n dual h-bridge motor controller to power 2 DC motors", "The initial setup of my project is based on the Husky project, so i have a base node containing a hardware_interface::RobotHW class that communicates with the Arduino. Using the teleop_twist_keyboard i'm able to move my robot. Currently this is done using a custom message, where the RobotHW is the advertiser and the Arduino the subscriber. But just driving the motors is far from enough. I'm using powerfull 24V DC motors salvaged from broken printers and big (heavy) wheels from an old RC car. Currently i'm running the motors are 12v, which seem to be enough to get the robot rolling, but it takes time to get up to speed cause of the big heavy wheels and they keep spinning once the wheel speed is set to 0.", "To apply brake force to the wheels i need to reverse the motor power till the wheels stopped, so i got myself some motor encoders to measure the speed of the wheels. According to the hardware interface documentation and code i been checking to learn how it works, i need to read from the hardware, update the controller manager, then write to the hardware. So using ROS message system doesn't seem the correct way.", "Been looking into using rosserial for c++ and using the Serial communication in the Arduino firmware instead of message advertisers/subscribers, but this require writing some custom protocol for messages between the hardware and the hardware interface node.", "So before continuing writing a custom serial comm protocol, i wanted to know if this is the correct way to go, or if i should just use advertisers/subscribers to send custom messages between the hardware/ros node?", " Full code of my project in the current state of using messages can be found here:\n ", "the ampru_base package, contains both the firmware and the hardware interface node."], "answer": [" ", " ", " ", " ", "As there were no answers to my questions, i went forward with using a custom serial protocol instead of publish/subscribe topics to communicate between the hardware interface node and Arduino.", "The protocol is based on HDLC and seems to work fine :)\nCode pushed to git repository.", "Hi Randy,\nI'm new to roscontrol.\nCan I know what was the problem if you use public/subscribe to communicate between hardware interface node and Arduino.?\nCan't we subscribe to the joint values from Arduino and update control manager and then publish the new joint values to Arduino.?", "Thank you :)", "Hi Sai & Randy, I also wanted to know if we can work with publisher/subscriber instead of a custom protocol . Just wanted an opinion before  I go forward with my method. Let me know. Thanx!", "Hi, i tested with publisher/subscriber on Arduino and it works fine, but my concern was that using it contains more code, while the programming memory on arduino is limited and there is more data going over the serial port.", "Ah...makes sense....i also got it working anyway....thanx", " Hi Pranavb,\nCan I know your mail id. So that we can share our works.\nMine is  ", " . ", " my email is  ", "Share your code on github, might be useful to others as well ;)", " sure !  "], "url": "https://answers.ros.org/question/263501/hardware-interface-for-arduino/"},
{"title": "Our robot will be  equipped with lidars, sterio camera, SLAM algorithm, machine vision and similar loads. Please suggest us the computer specification which will be ideal and optimal?", "time": "2017-06-12 05:33:13 -0600", "post_content": [" ", " ", "We are team of 4 engineers trying to develop autonomous robot system which can navigate autonomously in farms and other agricultural environment. We are kicking off the project next month and We are looking for ideal computer system specification. ", "We are purchasing stuffs for the development and we are looking for recommendations for hardware requirement. ", "Is this mean to be prototyping? Do you have any constrains about weight, power consumption?", "Weight constrain is not there as for now. Power consumption can be taken care off by using larger batteries if needed.", "But it will be helpful if you can give insights in either situation for us to evaluate further.", "regards", "If so, I would consider multiple small PCs (like Inter NUC or Gigabyte Brix) as a start, because the computational power depends on many factors (number of sensors, your algo, speed of the vehicle, etc.).", "ROS is also best choice to use distributed systems (multiple PCs working together), so you can have 1 PC to handle motion planning + localization, 1 for machine-vision for example. This also enable \"plug-and-play\" if you wanna try different machine vision algo with same motion planning capability.", "Yes, I was thinking of something in those lines. I will dig in more into inter NUC and gigabyte Brix.", "For the software development purpose or simulation work before actual robot testing, do you think we will need high computional requirements?", "What I mean is, I will be distributing the vision, navigation and robotic hand work in the team and I am in process of purchasing the hardware for them. I want to buy the optimal hardwares."], "answer": [], "url": "https://answers.ros.org/question/263682/our-robot-will-be-equipped-with-lidars-sterio-camera-slam-algorithm-machine-vision-and-similar-loads-please-suggest-us-the-computer-specification/"},
{"title": "Wheel Calibration", "time": "2017-07-02 12:22:13 -0600", "post_content": [" ", " ", "I have my first robot prototype build from scratch up and running using ROS + Arduino. :)\nBut when i control the robot straight forward, it goes a bit sideway.", "I'm using a L298N Dual H-Bridge to power the motors, which has more power input as the motors require, cause the shield seem to measure whats the max power to put on the motors.\nI noticed, one side of the motor shield gives more power as the other side. (10v input, left wheel 7V output, right wheel 7.7V output, motors specs 6V)", "I also have an MPU6050 connected, but not fully implemented yet in the hardware interface (not publishing the IMU data yet)\nWill the IMU data update the joint commands received by the hardware interface if it notice the robot don't go straight or should i try to fix this in the firmware code?"], "answer": [" ", " ", "Will the IMU data update the joint commands received by the hardware interface if it notice the robot don't go straight [..]", "The ", " ", " you mean? not automagically, no.", "or should i try to fix this in the firmware code?", "That, or write a controller / node that compensates for the discrepancy.", "If you have encoders I would actually recommend you implement a velocity controlled system: that would make things closed-loop and would automatically take care of any differences between the two motors.", "What u mean by velocity controlled?", "\u0394x/\u0394t: periodically check the nr of encoder counts, convert to radians and divide by sampling period. Then use that with a (pid) controller to expose a velocity controlled interface to each of your motors (ideally: wheels). 1rad/s == 1rad/s, irrespective of what your motor controller does.", "That makes things closed-loop. From your description I got the impression that you're currently using an open-loop approach (ie: % PWM or something) and assuming that both motors behave the same. If you add some sensors (ie: encoders), you can ", " behave the same.", " yes, i convert the joint command in the hardware interface to PWM on the Arduino and update the joint position/velocity from the encoders\n ", " \nwill do some reading about velocity controlled and see what i need to change ", "The velocity control would be done on the arduino-side, probably / preferably. If you don't already have it, you would probably have to run some control loop on the Arduino that checks desired velocity against effective. Then PD control that."], "answer_code": ["ros_control", "diff_drive_controller"], "url": "https://answers.ros.org/question/265299/wheel-calibration/"},
{"title": "robot_ip in industrial_robot_client", "time": "2017-06-15 22:25:56 -0600", "post_content": [" ", " ", "Hey, dear all,\n Recently, I am integrating ros industrial with dual Staubli TX90 arm. Actually, a package for the single arm has been developed at ", ". We have tested and ran well each controller.\nBut my case is: I need to communicate and control dual arm. And each controller has its own ip address. for instant, left_arm[192.168.169.1], right_arm[192.168.169.2]. ", "Here are what I have done:", "And I am here to asking:", ": could I ask you to please mark the question as answered by ticking the checkmark to the left of the answer? It should turn green.", "That would much more clearly mark this question as answered than when you close it.", "Thanks."], "answer": [" ", " ", " ", " ", "Is it right that motoman has only one hardware ethernet ip address to connect two controllers?", "No. SDA* robots use ", " FS100 (or newer) controller that controls all groups (4) simultaneously.", "The driver thus only needs a single TCP/IP connection to the controller.", "So the motoplus or controller app will send all states or receive command in one port? How they can pub all states in only one /joint_states?", "I think this is now clearer from my earlier answer above.", "In more detail: ", " messages contain one ", " message per motion group, for a total of four messages.", "Is it right to go that I use use two connect with two standalone ip address? How to handle two controller states and commands in one topic, /joint_states, and /joint_trajectory_command? For Q2, I am synchronously coding, it is a really heavy work. I hope anyone could give me advice or a yes to go.", "If you have two controllers, then asynchronous motion and joint state feedback could be achieved by simply running two instances of the relevant of ", " at the same time, each instance connecting to its own controller (just supply a different ", " to each launch file). Run each in their own ROS namespace (you could set ", " and ", " on your launch file ", " tags, if you decide to create a single wrapper launch file).", "This would effectively treat your two arms as two independent, separate robots.", "If combined joint states are desired, then an instance of ", " with its ", " parameter (see the ", ") set to include the ", " topics in both namespaces should work.", "Synchronous motion with two separate controllers is going to be difficult -- if not impossible -- to achieve without some form of (mfg supported?) synchronisation between the two controllers. It is also not a use-case that the ", " was designed for."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "I studied ", ", and in its case, the ", " ", " with one robot_ip can connect two controllers.\nSince I have not used motoman sda series ever,  I guess motoman has integrated two controller hardware in one ethernet port.", "I am rewriting the original staubli-ROS-I to support for dual arm connecting. I intend to use two ", " and motoman-like group to manage the states and commands.", " Is it right that motoman has only one hardware ethernet ip address to connect two controllers? So the motoplus or controller app will send all states or receive command in one port? How they can pub all states in only one /joint_states?", " Is it right to go that I use use two ", " with two standalone ip address? How to handle two controller states and commands in one topic, /joint_states, and /joint_trajectory_command?\nFor Q2, I am synchronously coding, it is a really heavy work. I hope anyone could give me advice or a yes to go."], "answer_code": ["JOINT_FEEDBACK_EX", "staubli_val3_driver/launch/robot_interface_streaming.launch", "robot_ip", "group=\"left_arm\"", "group=\"right_arm\"", "include", "source_list", "joint_states", "staubli_val3_driver"], "url": "https://answers.ros.org/question/263999/robot_ip-in-industrial_robot_client/"},
{"title": "Problem with rosserial_arduino topic publishing?", "time": "2017-06-12 15:31:55 -0600", "post_content": [" ", " ", "I use rosserial_arduino to publish the data received from the analog input of Arduino to ROS environment. I'm using Arduino MEGA and I install ROS on Raspberry Pi 3 (Ubuntu 16.04). The problem is that when I \"", "\" the topic, there are many times that all the values of the data are 0.0 , and sometimes there are a warning like this:", "Then, I try to unplug and replug the USB cable between Arduino and Raspberry Pi, restart the Raspberry Pi, reupload the Arduino program or increase the power supply to the Arduino. Sometimes it solves the problems (I can publish the real data from Arduino instead of all 0.0), and sometimes the problem persist, which is very annoying. Has anyone encountered this problem, and what is the reason, how can I solve this? Is this problem common when working with rosserial_arduino or not? Or it's just because I'm using Raspberry Pi 3 instead of laptop?", "Thank you."], "answer": [], "question_code": ["[WARN] [1479567965.827686]: Inbound TCP/IP connection failed: connection from sender terminated before handshake header received. 0 bytes were received. Please check sender for additional details.\n"], "url": "https://answers.ros.org/question/263717/problem-with-rosserial_arduino-topic-publishing/"},
{"title": "The problem about \"?- rdf_triple(knowrob:forCommand,A,'make pancakes').\"", "time": "2017-07-22 03:16:57 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am very interested in Knowrob project.I find it is powerful for the knowledge processing of robots.\n   I install knowrob (groovy version) in the Ubuntu12, I install knowrob through rosbuild command.\n   However,I meet a difficult problem.When I run the command in the terminal:", "Then I run the commands in another terminal:", "But there are many warnings and errors appearing in the treminal:", "Could you help me solve the problem?Thanks a lot!\n   I\u2019m looking forward to your reply.", "Sincerely yours,\n   Munu", "Could you please format your question properly? Use the ", " button (the one with ", " on it) for code and console copy-pastes.", "And please ", " indent every sentence with 4 spaces or more for regular text, as that triggers the code formatter.", "OK! Could you help me solve the problem?Thanks a lot!"], "answer": [], "question_code": ["roslaunch opencyc opencyc.launch\n", "roscd comp_ehow\n\nrosrun rosprolog rosprolog comp_ehow\n\n?- rdf_triple(knowrob:forCommand,A,'make pancakes').\n", "WARNING: [Local Folder Repository] The specified file must be a directory\n(/home/use/catkin_ws/src/stack/knowrob_addons/comp_ehow/plugins/edu.stanford.smi.protegex.owl) -- LocalFolderRepository.update()\n", "101010"], "url": "https://answers.ros.org/question/267113/the-problem-about-rdf_tripleknowrobforcommandamake-pancakes/"},
{"title": "[ERROR] [WallTime: 1504001477.235354] Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create.", "time": "2017-08-29 05:13:52 -0600", "post_content": [" ", " ", "I am new to ros ,\ni followed the tutorial for turtlebot and when i launch the minimal luanch file i get this error ", "[ERROR] [WallTime: 1504001477.235354] Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create. ", "I checked the cable and batteries ,they both are fine .", "kindly help me "], "answer": [], "url": "https://answers.ros.org/question/269833/error-walltime-1504001477235354-failed-to-contact-device-with-error-error-reading-from-sci-port-no-data-please-check-that-the-create-is-powered-on-and/"},
{"title": "Ar drone - camera transparent images [closed]", "time": "2017-07-24 07:00:53 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am using a python script to control the tum ar drone. I have prepared an environment in gazebo, in which the drone flies and captures the image. However, in the image my camera captures, the chairs and other objects are transparent. I have included the images from the camera below: 1- during the experiment, 2 - saved to .png using below callback ", " ", " ", "Does anybody have an idea how can I update the camera not to be transparent? In my gazebo world, everything is normal, as shown in this image:", "They're not transparent, but rendered only as ", ".", "In my gazebo world, everything is normal,", "I would say they look strange in Gazebo as well. Not sure if that is related.", "Is there something that looks strange in Gazebo to you, except the wireframes of the chairs?", "Do you know how one can turn off wireframes in Gazebo? ;)", "or maybe you know any other normal Gazebo chair models? I am having troubles finding them.", "I'm not much of a Gazebo user, so I haven't run into any of this.", "This is either a Gazebo issue, or a problem with the camera plugin (", "). If the former, you should probably ask about it over at ", ". The latter would be a ROS issue.", "I think it might be something with the camera, because in the second picture there should be a green square below the drone (which is possible to see in gazebo)", "Do you have any ideas on where can I try changing the camera output?"], "answer": [], "question_code": ["def front_image_callback(self, msg):\n    try:\n        # Convert your ROS Image message to OpenCV2\n        cv2_img = CvBridge().imgmsg_to_cv2(msg, \"bgr8\")\n    except CvBridgeError, e:\n        print(e)\n    else:\n        # Save OpenCV2 image as a jpeg\n        cv2.imwrite('front_camera/image_{}.jpeg'.format(self.k), cv2_img)\n        self.k += 1\n", "gazebo_ros_pkgs", "answers.gazebosim.org"], "url": "https://answers.ros.org/question/267228/ar-drone-camera-transparent-images/"},
{"title": "Missing compile step?", "time": "2017-07-26 11:03:54 -0600", "post_content": [" ", " ", " ", " ", "I'm learning ROS for Turtlebot3, and am getting an error message that appears to tell me that I missed a compilation or build step. Perhaps I am missing an environment variable. Here are the key steps:", "And looking for the missing geometry_msgs.msg:", "And inside you can see the Twist.msg:", "Here's the full source of red green.py:", "Why is the ", " package outside your ", " space? That won't work (well it can, but let's not complicate things).", "Put it in ", ", run ", " (or ", "), ", " your ", " and try again.", "Also, check what you have on line 6 of ", ".", "Thanks ", "... I made the suggested corrections, and updated the original question above. Then run it with ", " and got the same error. Any other suggestion would be appreciated!", "It looks like you're not finding ", " in ", " (", "). Where is it installed on your system? I'm on Ubuntu 14.04 and using Indigo, and it's installed at ", ".", ", thanks, it seems to be there. Also I've updated the original question with that detail and some more. Thanks for sticking with me!", "Make sure to post the entire contents of your source. I ran the previous edit of your code and didn't get any errors. I ran the current version and got your errorl", "yes. I guess I was trying to unclutter the question before I had any idea that that comment was the culprit!"], "answer": [" ", " ", "I think I found the problem: the first line of the .py file was a #comment... before the shebang line. I think thats what messed everything up!", "If this solved your question you should mark it as correct."], "question_code": ["$ catkin_create_pkg wanderbot rospy geometry_msgs sensor_msgs\n\n$ source ~/catkin_ws/devel/setup.bash \npitosalas@ubuntu:~/catkin_ws/wanderbot/src$ ./redgreen.py \nfrom: can't read /var/mail/geometry_msgs.msg\n./redgreen.py: line 6: syntax error near unexpected token `('\n./redgreen.py: line 6: `cmd_vel_pub = rospy.Publisher('cmd_vel', Twist, queue_size=1) #<1>'\n", "pitosalas@ubuntu:/$ ls -l /opt/ros/kinetic/share/geometry_msgs/\ntotal 12\ndrwxr-xr-x 2 root root 4096 Jul 23 18:39 cmake\ndrwxr-xr-x 2 root root 4096 Jul 23 18:39 msg\n-rw-r--r-- 1 root root  794 Sep 30  2016 package.xml\n", "ls -l /opt/ros/kinetic/share/geometry_msgs/msg/\ntotal 116\n-rw-r--r-- 1 root root 119 Sep 30  2016 Accel.msg\n-rw-r--r-- 1 root root  83 Sep 30  2016 AccelStamped.msg\n-rw-r--r-- 1 root root 330 Sep 30  2016 AccelWithCovariance.msg\n-rw-r--r-- 1 root root 124 Sep 30  2016 AccelWithCovarianceStamped.msg\n-rw-r--r-- 1 root root 235 Sep 30  2016 Inertia.msg\n-rw-r--r-- 1 root root  30 Sep 30  2016 InertiaStamped.msg\n-rw-r--r-- 1 root root 367 Sep 30  2016 Point32.msg\n-rw-r--r-- 1 root root  84 Sep 30  2016 Point.msg\n-rw-r--r-- 1 root root  98 Sep 30  2016 PointStamped.msg\n-rw-r--r-- 1 root root 107 Sep 30  2016 Polygon.msg\n-rw-r--r-- 1 root root 104 Sep 30  2016 PolygonStamped.msg\n-rw-r--r-- 1 root root  96 Sep 30  2016 Pose2D.msg\n-rw-r--r-- 1 root root  85 Sep 30  2016 PoseArray.msg\n-rw-r--r-- 1 root root 119 Sep 30  2016 Pose.msg\n-rw-r--r-- 1 root root  79 Sep 30  2016 PoseStamped.msg\n-rw-r--r-- 1 root root 323 Sep 30  2016 PoseWithCovariance.msg\n-rw-r--r-- 1 root root 122 Sep 30  2016 PoseWithCovarianceStamped.msg\n-rw-r--r-- 1 root root 108 Sep 30  2016 Quaternion.msg\n-rw-r--r-- 1 root root 117 Sep 30  2016 QuaternionStamped.msg\n-rw-r--r-- 1 root root 118 Sep 30  2016 Transform.msg\n-rw-r--r-- 1 root root 337 Sep 30  2016 TransformStamped.msg\n-rw-r--r-- 1 root root 115 Sep 30  2016 Twist.msg\n-rw-r--r-- 1 root root  82 Sep 30  2016 TwistStamped.msg\n-rw-r--r-- 1 root root 326 Sep 30  2016 TwistWithCovariance.msg\n-rw-r--r-- 1 root root 124 Sep 30  2016 TwistWithCovarianceStamped.msg\n-rw-r--r-- 1 root root 382 Sep 30  2016 Vector3.msg\n-rw-r--r-- 1 root root 103 Sep 30  2016 Vector3Stamped.msg\n-rw-r--r-- 1 root root 117 Sep 30  2016 Wrench.msg\n-rw-r--r-- 1 root root  85 Sep 30  2016 WrenchStamped.msg\npitosalas@ubuntu:/$\n", "#example 1\n#!/usr/bin/env python\nimport rospy\nfrom geometry_msgs.msg import Twist\n\ncmd_vel_pub = rospy.Publisher('cmd_vel', Twist, queue_size=1) #<1>\nrospy.init_node('red_light_green_light')\n\nred_light_twist = Twist() #<2>\ngreen_light_twist = Twist()\ngreen_light_twist.linear.x = 0.5 #<3>\n\ndriving_forward = False\nlight_change_time = rospy.Time.now()\nrate = rospy.Rate(10)\n\nwhile not rospy.is_shutdown():\n  if ...", "wanderbot", "src", "catkin_ws/src", "catkin_make", "catkin build", "source", "devel/setup.bash", "redgreen.py", "cd ~./catkin_ws/src/wanderbot/src; ./redgreen.py", "geometry_msgs", "/var/mail", "from: can't read /var/mail/geometry_msgs.msg", "/opt/ros/indigo/share/geometry_msgs"], "url": "https://answers.ros.org/question/267445/missing-compile-step/"},
{"title": "wiimote cannot connect to Turtlebot3", "time": "2017-07-22 21:21:17 -0600", "post_content": [" ", " ", "Hi my friend,", "I am not able to connect wiimote to TB3.", "Here is what I did:", "At first, run:", "sudo apt-get install ros-kinetic-wiimote", "then:", "rosdep install wiimote", "then:", "rosmake wiimote", "Everything is fine. No error found.", "Then I try to pair wiimote with TB3. I get the following error:", "$ rosrun wiimote wiimote_node.py", "Press buttons 1 and 2 together to pair (within 6 seconds.)", "(If no blinking lights, press power button for ~3 seconds.)", "Wiimote read error", "Read error (nunchuk cal)", "/opt/ros/kinetic/lib/python2.7/dist-packages/wiimote/wiistate.py:157: RuntimeWarning: divide by zero encountered in divide\nself.acc = WIIReading((self.accRaw - self._accCalibrationZero) / (self._accCalibrationOne - self.accCalibrationZero), self.time)", "Then I added some logs in wiimote_node.py. and I found that the following line is not working:", "wiimoteDevice = wiimode.WIIMote.WIIMote()", " I went through the source code of wiimote module: WIIMote.py and then found that the following line is causing the problem:\nself._wm = cwiid.Wiimote()\nSee  ", "Anyway, I am able to pair wiimote directly with Ubuntu. But I am not able to pair it with TB3.\nHave you seen similar problem? Any solutions?", "Thank you very much.", "\u2013Kening"], "answer": [], "url": "https://answers.ros.org/question/267149/wiimote-cannot-connect-to-turtlebot3/"},
{"title": "Why dwa planner can't reach the target rotation?", "time": "2017-07-27 07:41:16 -0600", "post_content": [" ", " ", " ", " ", "I've commanded the robot to go to the green arrow position/rotation, so it went there and started rotating CCW until it got near the upper wall, then it got stuck in that position.\nThe question is why it don't go a bit forward so it can complete the rotation? Which parameters can be changed so that it better behave in situations like that?", "Update:\nIt keeps writing in the logs:", "[ INFO] [1501263591.230230037]: Got\n  new plan [ WARN]\n  [1501263591.260831641]: Invalid\n  Trajectory 0.000000, 0.000000,\n  0.150000, cost: -6.000000 [ WARN] [1501263591.260927725]: Rotation cmd\n  in collision [ INFO]\n  [1501263591.260981225]: Error when\n  rotating.", "and after some seconds:", "[ERROR] [1501263591.331646239]:\n  Aborting because a valid control could\n  not be found. Even after executing all\n  recovery behaviors", "Currently we have recovery_behavior_enabled = true and clearing_rotation_allowed = false", "What's the output at the terminal?", " I've updated the question with console output"], "answer": [" ", " ", "I believe it's because you have the ", " parameter set to ", ". It won't rotate to clear the cost map and replan, it'll just give up like you're seeing. ", "From ", ", the ", " parameter", "Determines whether or not the robot\n  will attempt an in-place rotation when\n  attempting to clear out space. Note:\n  This parameter is only used when the\n  default recovery behaviors are in use,\n  meaning the user has not set the\n  recovery_behaviors parameter to\n  anything custom.", "I'll try to enable that, but that behavior has given me problems in the past (like colliding with walls). However, I can't understand why it can't finish the move without resorting to the recovery behaviors, as it would be a simple movement to get out of there"], "answer_code": ["clearing_rotation_allowed", "false", "clearing_rotation_allowed"], "url": "https://answers.ros.org/question/267526/why-dwa-planner-cant-reach-the-target-rotation/"},
{"title": "ROS-Indigo-Turtlebot Android-teleop not working", "time": "2017-08-14 20:20:29 -0600", "post_content": [" ", " ", " ", " ", "With fresh  (August 2017) Ubuntu 14.04 (Trusty), ROS-Indigo-turtlebot (kinect-create) installation on a netbook pc, usual functions keyboard & ps3 joy teleop, RVIZ and image_view (image_raw and registered image) works with clean minimal, 3dsensor.launch. However, installing a fresh Android (4.0.4 Icecream Sandwich) Teleop (Indigo Ver. 1.0.0) \"connects\" but does not drive turtlebot nor show image or battery status. The app appears active with the expected virtual joystick indication on the Android screen and it does not crash.", "The turtlebot ", " and ", " , so the android appears to be properly connected into the turtlebot system.", "Investigating further with ", " shows the appropraite node details, below, but ", ", while showing the android box and nodes, show NO connecting links to any other nodes. ", " show changing liner and angular values, as expected.", "Another curious observation is that the ", " Node Subscribes to ", " which does not exist in the ", " display. The closest Node name might be ", " ? Does the android rosjava app need to be revised?", "Is there an additional command to launch or run or a parameter to set to 'enable\" or launch the android function? There is no text in the ", " screen log referring to android, info, warning or error. ", "Any suggested investigation steps would be appreciated."], "answer": [], "question_code": ["~$ rosnode list shows /android/camera_view", "/android/virtual_joystick", "rosnode info", "rosrun rqt_graph rqt_graph", "rostopic echo /cmd_vel", "/android/camera_view", "/compressed_image [unknown type]", "rosnode list", "/camera/rgb/image_color/compressed", "minimal.launch", "~$ rosnode info /android/virtual_joystick\n--------------------------------------------------------------------------------\nNode [/android/virtual_joystick]\nPublications: \n * /cmd_vel [geometry_msgs/Twist]\n * /rosout [rosgraph_msgs/Log]\n\nSubscriptions: \n * /odom [nav_msgs/Odometry]\n\nServices: None\n\ncontacting node http://192.168.2.122:53246/ ...\nPid: 13755\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n\nturtlebot@herschel:~$ rosnode info /android/camera_view\n--------------------------------------------------------------------------------\nNode [/android/camera_view]\nPublications: \n * /rosout [rosgraph_msgs/Log]\n\nSubscriptions: \n * /compressed_image [unknown type]\n\nServices: None\n\n\ncontacting node http://192.168.2.122:37602/ ...\nPid: 13755\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n\nturtlebot@herschel:~$ rostopic echo /cmd_vel\nlinear: \n  x: -0.20135678895\n  y: -0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: 0.0423908977128\n---\nlinear: \n  x: -0.171266657133\n  y: -0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: 0.065484302429\n"], "url": "https://answers.ros.org/question/268816/ros-indigo-turtlebot-android-teleop-not-working/"},
{"title": "hokuyo UTM-30LX reports error code[00->70] while trying to run hokuyo_node", "time": "2017-09-12 00:45:46 -0600", "post_content": [" ", " ", "Dear community,\nI'm experiencing a weird behaviour of an Hokuyo UTM-30LX using hokuyo_node.The problem has began to appear yesterday after i plugged it to an new power supply, which i tested to be OK. Now the motor inside is not running.\nHere is the BUG REPORT:", "I send \"II\" command via serial port at baudrate 115200, and it returned the same."], "answer": [], "question_code": ["[ERROR] [1505179518.564758030]: Laser returned abnormal status message, aborting: Trouble!! Error No. =[00->70] You may be able to find further information as http://www.ros.org/wiki/hokuyo_node/Troubleshooting/\n"], "url": "https://answers.ros.org/question/270729/hokuyo-utm-30lx-reports-error-code00-70-while-trying-to-run-hokuyo_node/"},
{"title": "How to create a Battery usage plugin", "time": "2017-09-22 17:43:08 -0600", "post_content": [" ", " ", "Hi everybody,\nI have a custom robot model in Gazebo and i want to create a battery plugin and then connect battery with motors. I have seen many different examples and i am a bit confused. I guess, i want the BatteryState topic publish in ROS. I am working with python.\nAny help? Do you have any examples on how to do it?\nThank you!"], "answer": [], "url": "https://answers.ros.org/question/271371/how-to-create-a-battery-usage-plugin/"},
{"title": "How to get and use camera calibration file?", "time": "2011-12-06 04:41:25 -0600", "post_content": [" ", " ", " ", " ", "I am running on Lucid 32-bit from Electric debs. I have been able to run the mono calibration utility with a 1394 camera and get a successful calibration. How do I then get the actual camera calibration file that was saved to disk and use it in a launch file to get rectified images?", "Start ", " and ", " with the following launch file:", "and the following yaml file:", "So far so good, as I get the utility to show up, all the bars go green, I can click 'Save' and 'Commit', and then exit out of the launch file.", "There is a file created at ", " that I am able to open and see 74 images plus a file named ", ". Using the ", " utility I am able to get a yaml file with the following contents:", "I modified the ", " field to be ", " but it made no difference in the errors described below.", "\nThe output of running a launch file similar to above but with ", " and two image viewers is:"], "answer": [" ", " ", " ", " ", "What you are doing looks mostly OK. The error messages you got are misleading and irrelevant. There is an ", " to fix them in Fuerte.", "To answer your specific questions:", "I think so. Clicking the \"COMMIT\" button on the calibration menu should store the new calibration data at the specified URL, which need not have existed before then, although the package directory should already exist.", "No.", "Yes. Electric has a new feature, allowing you to substitute the camera name (GUID) as part of the URL, like ", ". For your device, that would resolve to ", " (reference to ", ")", "Maybe not, depending on how good your lenses are.", "The ", " in the saved calibration file should be ", " for your device. It should get set automatically. If the calibrated name does not agree with the actual device GUID, a warning will be logged.", "In case of other camera drivers like the usb_cam is it the same? How can I find out the camera_name?", "Many camera drivers use camera_info_manager and therefore behave similarly. The usb_cam driver apparently does not, so I don't know how it handles calibration and camera naming."], "question_code": ["<launch>\n  <group ns=\"stereo\">\n    <node pkg=\"camera1394\" type=\"camera1394_node\" name=\"camera_left\">\n      <rosparam file=\"$(find stereo_calibration)/cameras/left_flea2.yaml\"/>\n      <remap from=\"camera\" to=\"left\"/>\n    </node>\n  </group>\n\n  <node pkg=\"camera_calibration\" type=\"cameracalibrator.py\" name=\"mono_calibration\"\n    args=\"-size 9x6 -square 0.054 image:=/stereo/left/image_raw camera:=/stereo/left\">\n  </node>\n</launch>\n", "# ID number of camera. Use Coriander to find it.\nguid: xxxxxxxxxxxxxxxx\n# Mode describes the size and number of channels for the images.\nvideo_mode: 1600x1200_mono8\n# The frame_id MUST be part of your URDF to display stereo data correctly in Rviz.\nframe_id: left_camera\n# The location of the ROS calibration file.\ncamera_info_url: package://stereo_calibration/cameras/xxxxxxxxxxxxxxxx_mono.yaml\n# Frames per second.\nframe_rate: 30\n# Try to reset the camera when opening.\nreset_on_open: true\n# Change from default 400Mbps Firewire.\niso_speed: 800\n", "image_width: 1600\nimage_height: 1200\ncamera_name: narrow_stereo/left\ncamera_matrix:\n  rows: 3\n  cols: 3\n  data: [2379.692159, 0, 810.664016, 0, 2364.724193, 565.047935, 0, 0, 1]\ndistortion_model: plumb_bob\ndistortion_coefficients:\n  rows: 1\n  cols: 5\n  data: [-0.125793, 0.149249, -0.000565, -0.000972, 0]\nrectification_matrix:\n  rows: 3\n  cols: 3\n  data: [1, 0, 0, 0, 1, 0, 0, 0, 1]\nprojection_matrix:\n  rows: 3\n  cols: 4\n  data: [2348.257291, 0, 809.568039, 0, 0, 2344.569053, 563.818346, 0, 0, 0, 1, 0]\n", "camera_name", "stereo/left", "[ERROR] [1323196102.210752328]: Unable to open camera calibration file [~/.ros/camera_info/camera.yaml]\n[ERROR] [1323196102.427796451]: Unable to open camera calibration file [~/.ros/camera_info/00b09d0100a8f71b.yaml]\n... logging to x.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\nstarted roslaunch server http://localhost:58044/\n\nSUMMARY\n========\n\nPARAMETERS\n * /stereo/camera_left/frame_id\n * /rosdistro\n * /stereo/camera_left/frame_rate\n * /stereo/camera_left/camera_info_url\n * /stereo/camera_left/iso_speed\n * /stereo/camera_left/guid\n * /rosversion\n * /stereo/camera_left/reset_on_open\n * /stereo/camera_left/video_mode\n\nNODES\n  /stereo/left/\n    left_image_proc (image_proc/image_proc)\n  /stereo/\n    camera_left (camera1394/camera1394_node)\n    left_raw (image_view/image_view)\n    left_rect (image_view/image_view)\n\nauto-starting new master\nprocess[master]: started with pid [30321]\nROS_MASTER_URI=http://localhost:11311\n~/stereo_calibration/stereo_calibration ..."], "answer_code": ["package://stereo_calibration/cameras/${NAME}.yaml", "package://stereo_calibration/cameras/00b09d0100a8f71b.yaml", "camera_name", "00b09d0100a8f71b"], "url": "https://answers.ros.org/question/12193/how-to-get-and-use-camera-calibration-file/"},
{"title": "Complex nodes (mapping, nav) on embedded, resource constraint platforms (rpi, intel edison): feasibility / performance?", "time": "2017-06-24 20:15:29 -0600", "post_content": [" ", " ", " ", " ", "can I use ros(mapping and path planning) on a robot without a computer, by means using a raspberry pi or intel edison solely? and how much processing power and memory are needed? \nI have read in gmapping alogirithm = improverd RBPF that they needed a proecssor of 2.8 Ghz  for 2.2seconds for one full calculation of one position estimation and map updating cycle ? and 150 MB for 40X40 m2 area. ", "They may not look like it, but even RPis and Intel Edisons are computers. I've updated your question title to -- what I think -- more accurately reflects the contents of your question.", "yes I am aware of that, raspberry pi 3 is a mini computer compared with nowadays computers with 1.2GHz\nbut how much time does it need if it works at all, do I need almost 5 seconds for full loop, and is there a better alternative?", "FYI it was recently announced that the Intel Edison ", "I suggest to test it for your use case.The processing power and memory for gmapping e.g. highly depends on size of your map, resolution, number of scans per second, update counts..", "that is correct, thank you for reminding me. \nthe number of tree particles will be much less for 5x5 m2 area. \nso for such an area, I believe raspberry pi 3 would be more than enough.\nbut for  scanning a whole house 160 m2 for example, it would pull it off ,still need further testing."], "answer": [" ", " ", " ", " ", "I've worked quite a bit with these issues, and I would say that you'd be surprised how extremely efficient ROS is. I must say though, what used to be very clear comparison between scalar numbers has become something that tells me nothing. 2.8Ghz, that's more than the Intel Joule's 2.4Ghz-bursts? RIght? I had one Joule doing SLAM, interface the controller, rgbd with computationally cheap 3D-camera, running high-res 2D laserscan and even running my code (vastly inefficient) for getting some data from another sensor through multiple nodes. I can't say it was a breeze, because the Joule seems to actually have passed right after - but it managed. Keep in mind that I was developing at the same time, starting and stopping things, looking at the nice visualizations (on diff computer ofc) and other stressors for the poor guy. This happened earlier today, and now I've moved the majority of it to an Odroid XU-something (fancy blue fan). ", "I would say you can do the task you describe, given the right parameters (dunno how an exact amount of computational expense can be given without), but it will be a pain to set up and to work on. Would recommend going through everything needed beforehand on workstation, fixing all bugs and setting up a script to do the building and downloading for you. It's really, really painful  to ssh through the error messages and compiler-mysteries hours on end (spoiler: you're probably out of memory)). "], "url": "https://answers.ros.org/question/264681/complex-nodes-mapping-nav-on-embedded-resource-constraint-platforms-rpi-intel-edison-feasibility-performance/"},
{"title": "LIDAR produces \"phantom walls\"", "time": "2017-09-08 11:55:36 -0600", "post_content": [" ", " ", "Hello,", "We have a SICK LMS100 LIDAR attached to the front of our mobile robot and we have been using it for several years.  We suddenly started noticing \"phantom walls\" appearing in the laser scan when viewed in RViz.  You can see ", ".  The small yellow dots in the video are produced by a rotating LIDAR on the head of the robot and these dots line up correctly with the surrounding walls and other obstacles.  The larger colored dots are the LMS LIDAR points shown in intensity view.  Near the beginning of the video you can see how the LMS points line up with the yellow dots along a wall at the far right.  As the robot backs up, the LMS readings \"pull off\" the real wall but continue to produce a phantom wall that moves with the robot.", "We have observed the same phenomenon whenever there is a symmetrical gap that the robot is facing and the phantom wall is always positioned at roughly the same distance from the robot.", "Can anyone explain what is going on here?  Is the LMS faulty at the hardware level?  Or is this some kind of reflection problem that we can filter?", "Thanks!\npatrick", "Is the lidar getting tilted and is pointing at the floor in those instances?  Try putting something on the floor with some height but that should always lie beneath the scan plane but will be noticeable if the robot or sensor angles down.", "Maybe the lidar lens is dirty or the power input to the lidar is drooping? P.S. I recognized your name on the Youtube channel, I read ROS by Example a while back, excellent book!", "I agree. I use the book and recommend it to others.", " - Thanks for the suggestion but the scanner is definitely not tilting down and data further away than the \"phantom\" wall shows up OK.", " - Wouldn't a dirty lens cause a permanent reading rather than the odd intermittent reading we get?  Also, we can reproduce the phantom wall effect at certain locations time after time so I don't think it is a power fluctuation which presumably would occur at other locations as well.", "P.S. Thanks for the book recommendation. :)", "Can you share a bag of what is in that video on botbags or marvhub?", "I'll post a bag file on Monday assuming we haven't figured out the issue by then."], "answer": [" ", " ", "And the answer is...Tilt as suggested by ", ".  It turns out the bracket holding the LIDAR had been very slightly bent downward.  Once we bent it back level, the problem went away.  The weird thing is (and it's what confused us the most), is that the downward tilt did not cause a constant reading from the floor a certain distance out from the robot.  Instead, it only produced the \"phantom wall\" under very specific conditions, namely, when the view ahead included a narrowing at a certain distance, like two pillar supports of a wall jutting into the floor space.  Move the robot a little forward or backward and the fake wall would go away.", "Anyway, problem solved.", " ", " ", "I had the exact same effect when placing the SICK LMS100 too close to the floor in my robot (the infrared light does not travel in a perfect beam, it becomes wider with distance. So placing it too close to the floor will cause the sensor to be really sensitive to tilt and see the floor. You would be surprised how uneven floors really are). In the video I see the effect happening more often when the robot is moving backwards. That suggests me that it is actually tilting a bit (in my experiments less than 1 degree is enough to produce the effect). The problem was solved by placing the sensor higher in my robot's structure."], "url": "https://answers.ros.org/question/270521/lidar-produces-phantom-walls/"},
{"title": "ros compatible SBC or minipc for Turtlebot (Kobuki)", "time": "2017-10-04 10:01:40 -0600", "post_content": [" ", " ", " ", " ", "I'm running Nvidia TK1 with Kobuki base (Indigo with Trusty). Things are running ok but I'm not a big fan of arm based cpus.\nSince the TK1 runs off the Kobuki battery, power usage is a big thing.\nAre the other compatible SBC's (single board computer) or mini PCs that are x86 or x64 based (intel/amd) cpu's that are inline with TK1/TX1 capabilities (cpu, memory, power usage wise) that folks have been running?\nAny links would be helpful too."], "answer": [], "url": "https://answers.ros.org/question/272286/ros-compatible-sbc-or-minipc-for-turtlebot-kobuki/"},
{"title": "Please suggest all packages to achieve SLAM for robot", "time": "2017-09-28 20:22:58 -0600", "post_content": [" ", " ", " ", " ", "Hi. I am building a robot.", "Here is what my robot looks like:\n", "Mechanically and electrically it is ready. Time for programming. I read \u201cA Gentle Introduction to ROS\u201d, and have been learning C++.", "I am using Indigo with Ubuntu 14.04.", " - my lidar system is a Sick LMS200 + Absolute encoder + Servo system. The servo moves the LMS, and the encoder (connected to a Lab Jack U3) is providing the exact location the LMS is pointing at. The encoder has a resolution of 1024, so I get 2.85 lms readings per degree. A low level system call to the Lab Jack U3 will return the encoder position. The servo system is by Applied Motion and is powerful (400watt system), and accurate (70/10,000 is the position error under extreme shaking). To control the servo position I need to send commands via serial. Everything is wired, happy and configured. Manually I can get all 3 main components (LMS, Encoder, Servo) to work (send distance measurements, provide position, and move).", " - The robot has 2 drive wheels (driven by stepper motors with no encoder on them), and 2 caster wheels. The stepper motors are 750 watts, and are driven by a stepper drive which receives commands via Ethernet. ", " - What packages do I need for this robot to drive around and not bump anything while making a map at the same time? Could you please list all the packages needed to:", "I am very new to ROS, and do not know of any packages other than 'tutlesim'. Any package you suggest I will gladly look into. Do not feel shy to recommend any packages that you feel will help me. ", "Thanks."], "answer": [" ", " ", "You have built in impressive system, however, this question is too broad to be answered in one thread. It would be better if you'd open several questions that each solve a single component. Doing SLAM on a moving platform with a rotating sensor is a really hard problem and most likely not what you want to do as your first project. ", "I suggest you only use the scanner with a stationary robot (move, measure, update map, move, ...) In this case, you only need the encoder values of your Lidar-motor. So one way to proceed would be:", "Create a urdf-model ", " of your robot. A urdf describes the mechanical setup of your robot and defines frames for each component (e.g. a frame for the robot and one rotating frame for the scanner) You will need a ", " for your laser. Next steps afterwards are the ", " who reads the current joint values and publishes the updated frames.", "You can then use the TF-Library to get the (interpolated) pose of your Sick for every scan line. (If you rotate the scanner slow enough, you don't have to take into account that the scan line is not measured at one point in time but over several milliseconds). ", "If you have managed that (and please fell free to ask a lot here), you can think about merging the pointclouds (e.g. an ICP implemented in PCL) and pathplanning (MoveBase).", " Thanks for explaining this to me. The answer you provided is exactly what I needed; a starting place for using ROS. Thank you.", "No problem. It's fun to help people who showed that they already invested quite some time into a project.", " ", " ", "Please take a look at this example, and check out the related github pages for a decent first pass at navigation:", "Best wishes!"], "question_details": [" ", " ", " ", " ", "Package to get lidar components (LMS, encoder, servo) to work as a single system", "Package to use lidar data for SLAM", "Package for robot to interpret SLAM results and send commands to the drive motors to move"], "url": "https://answers.ros.org/question/271819/please-suggest-all-packages-to-achieve-slam-for-robot/"},
{"title": "ROS on 32bit system", "time": "2017-10-02 10:08:37 -0600", "post_content": [" ", " ", "Hi all,", "Is it possible to install ROS on a 32 bit system? For the OS i will use tinkerOS v2.0.1. I was thinking about buying the asus tinker board because it is more powerfull than the PI 3, but it has a 32 bit CPU, so I'm not sure it will work.", "Sorry if it's a stuppid question.", "I'm using Nvidia TK1 which is a 32-bit cpu with Indigo on Ubuntu 14.04 32-bit. Seems to be working ok."], "answer": [" ", " ", "For a long time i386 was the most common platform since it was the default install for Ubuntu.", " Almost all ROS packages work find on both I386 and arm32(armhf) packages. We have only recently turned off i386 builds for the latest ROS distro due to lack of use:  ", "  But as it says packages are still expected to build on i386. ", "Thank you for your answer! But where can I find the installation guide for an arm32 running tinkerOS (Debian 9)? I couldn't find it on the Lunar installation page ", ", only for amd64/arm64.", " Yes, as I mentioned we no longer build binaries for i386 as of lunar. If you want to use Lunar on i386 you'll need to build from source:  ", ": the Asus Tinker Board runs on a ", " - that is a Rockchip ARMv7-A derivative. No x86 there :).", "Ahh, sorry I'm not familiar with the board. But the same goes for armhf  (v7) you'll need to compile from source."], "url": "https://answers.ros.org/question/272075/ros-on-32bit-system/"},
{"title": "Turtlebot3 - how to work with various OpenCR sensors and actions", "time": "2017-10-11 08:31:02 -0600", "post_content": [" ", " ", " ", " ", "The OpenCR board has various sensors and actions: User LED x 4 : LD3 (red), LD4 (green), LD5 (blue), Gyroscope 3Axis, Accelerometer 3Axis, Magnetometer 3Axis (MPU9250), User button x 2.", "What's the best way to access them from a ROS node? I would like to be able to read the gyro, accelerometer, magnetometer, and user buttons, and activate the LEDs. ", " I see that Turtlebot3 Burger core source for OpenCR in Arduino IDE, there are 5 publishers and I see their message types  ", "  but I can't find the actual code that publishes. I've downloaded what I think is all the source code but can't find the code that actually publishes. I must be blind! ", "Can someone indicate to me where that code is and/or where it is documented?"], "answer": [" ", " ", " ", " ", "Based on the Turtlebot3 Burger core source for ", " in Arduino IDE, there are 5 publishers. ", " One of them is sensor_state_pub that publishes button status, voltage level, encoder values on \"sensor_state\" topic.\n( ", " )  ", "\nSimilarly, imu_pub publishes IMU readings from OpenCR with \"imu\" topic. ", "\nYou can subscribe these topics in order to receive data from OpenCR.", "However, there is no subscriber in the OpenCR firmware nor publisher in RPi 3 Turtlebot3 source for LED control. ", "\nThe latest OpenCR firmware already utilizes USER1~4 LEDs for TXD, RXD, Low Battery and ROS Connect indicator. ", "\nIf you want to use these LEDs for other purpose from RPi3, you need to create publisher for LED topic in the RPi 3 and subscriber for the same in the OpenCR(with Arduino IDE) to control LED.", "========Appended Answer========== ", "\nI'm sorry for the unclear explanation about the OpenCR firmware. ", "\nIn Arduino IDE, if you opened turtlebot3_core from \"File > Examples > turtlebot3 > turtlebot3_burger > turtlebot3_core\", the \"turtlebot3_core\" tab will be opened in default. ", "\nIn the same file, go to line #313 where you can find \"void publishSensorStateMsg(void)\" function. ", "\nThis function is called within the main loop (line # 203) of the code.", "I've edited the original question with some additional details."], "url": "https://answers.ros.org/question/272779/turtlebot3-how-to-work-with-various-opencr-sensors-and-actions/"},
{"title": "Problems with 2D Nav Goal RViz", "time": "2017-10-26 12:59:27 -0600", "post_content": [" ", " ", "I am working with the TurtleBot 3.  I went through all the proper steps for bring up and teleop on the Turtlebot website.  When I get to the Navigation section I am all set with the TurtleBot Burger running in the simulator.  I can teleop the robot with my keyboard, and the robot moves, updates, and the green arrows update on my map file I created and saved with the SLAM function.  However! When I go to set a 2D Nav Goal the red arrow is dropped on the software but my robot does not move at all.  ", "On the terminal where I ran the line of code from the website: roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=$HOME/hall_map.yaml", "I GET THE ERROR:\n", "repeatedly", "Also on the nav.RViz software on the left side bar there are status warnings for Costmap and Map and the message says No map received.", "I am running Ubuntu 16 and ROS Kinetic", "Please let me know if anyone can help", " Have you tried to kill the teleoperation?\nHave you tried this ?  ", " \nI still have the exact same problem, have you solved it? ", "Have you fixed this issue? I have the same problem with the same message however, the map that I send has recived. I've changed a lot of parameter but nothing works. Have you got the solution?"], "answer": [], "url": "https://answers.ros.org/question/274222/problems-with-2d-nav-goal-rviz/"},
{"title": "ROS Indigo Source Build Fail - Ubuntu 14.04", "time": "2016-05-06 11:05:40 -0600", "post_content": [" ", " ", " ", " ", "Trying to install ROS Indigo from source on Ubuntu 14.04.", " I followed the instructions listed here exactly with no issues until the build:  ", "When invoking the following command, ROS fails to compile:", " I looked at a few other topics for help, ( ", " ) however my dependencies are fine: ", "No rule to make target ", "/home/astaauser/ros_catkin_ws/devel_isolated/class_loader/lib/libclass_loader.so'.  Stop.", "I also tried to install libpoco from apt-get but no change in result since it was already installed fine.", "Reading package lists... Done\nBuilding dependency tree ", "\nReading state information... Done\nlibpoco-dev is already the newest version.\nThe following packages were automatically installed and are no longer required:\n  gnome-power-manager libjpeg-progs libjpeg-turbo-progs\nUse 'apt-get autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 542 not upgraded.", "Were you able to solve this issue?"], "answer": [" ", " ", "I ran into the same problem both when compiling indigo on 14.04 and kinetic on 16.04 . When you specify the 'Debug' build type, it needs to link against the debug version of the library. Running ", " revealed that the symlink for libPocoFoundationd.so was broken: the library did not actually exist. Running ", " resolved the problem both times, where ", " is whatever version is available to install.", " ", " ", "I ran into the same issue.  I solved it by using ", " on all installed libpoco packages, installing libpoco ", " and adding -DCMAKE_INCLUDE_PATH with the path for the newly-installed libpoco libraries to the ", " command.", " ", " ", "I think you can solve if you use ln. ", "this is a list when you have installed this program. When you have this, ln in the direction where you need it ", "Example: If when you installed, it have the name libPocoFoundationd.so.x.x", "SThanks so much for your help, but I think the appropriate links are already setup, I did an ls -all where the library was located and got:", "Still get the same error."], "question_code": [" ./src/catkin/bin/catkin_make_isolated --install -DCMAKE_BUILD_TYPE=Debug\n", "~/ros_catkin_ws$ rosdep install --from-paths src --ignore-src --rosdistro indigo -y\n#All required rosdeps installed successfully\n", "/usr/lib/libPocoFoundationd.so', needed by", "~/ros_catkin_ws$ sudo apt-get install libpoco-dev\n"], "answer_code": ["ls -al /usr/lib/libPocoFoundation*", "sudo apt-get install libpocofoundation{version}-dbg", "{version}", "sudo apt-get remove", "catkin_make_isolated", "locate libPocoFoundationd.so", "sudo ln \"direction in your list\" \"direction when you need\"", "sudo ln /usr/lib/libPocoFoundationd.so.x.x /usr/lib/libPocoFoundationd.so", "/usr/lib/libPocoFoundationd.so -> libPocoFoundationd.so.9\n/usr/lib/libPocoFoundation.so -> libPocoFoundation.so.9\n"], "url": "https://answers.ros.org/question/233787/ros-indigo-source-build-fail-ubuntu-1404/"},
{"title": "Multiple issues while attempting to create a simple robot for localization in simulation using amcl", "time": "2017-11-06 23:50:35 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am following a basic tutorial and I created my own robot in Gazebo and Rviz, and added laser and camera sensors to it. I then utilized a map from Clearpath to try and localize this robot using AMCL. ", "There are a few issues I am running across that I am unable to fix as of now.", "The robot when I launch in RViz needs to be set to the \"odom\" fixed frame, however the map requires the \"map\" to function. If I select \"odom\" then the actual map in RViz shuffles about a point (jittery motion). If I select \"map\" then the robot seems to be jittering about. I am not entirely sure how to fix that. This doesn't happen in Gazebo so I am assuming this doesn't contribute to any errors. But any suggestions on what I am missing or understanding incorrectly?", "As soon as I run the launch file for the amcl (which loads the map, and starts amcl node, and the move_base node with the config params), I load it up in RViz and then select \"2D pose estimate\" and point the arrow towards a particular path in the map from the robot. I am attaching an image below. The green circle is where the robot starts and the red arrow underneath is the \"2D pose estimate I set\". As you can see, as soon as I did that, the costmap depicts it as if the area is blocked off. I select the \"2D Nav Goal\" and set it as per the Red arrow you can see on the left side. Since as per the costmap the area in front of the robot is blocked, it decides to take the path opposite of it to try to reach the goal. I am trying to understand why this is happening and how to rectify it. Is it any specific parameter somewhere that I need to tweak or I am initializing something incorrectly? ", "And I get the following errors/warnings -", "[ERROR] [1510033498.355228432, 2137.605000000]: None of the points of the global plan were in the local costmap, global plan points too far from robot", "[ WARN] [1510033498.369760487, 2137.609000000]: The origin for the sensor at (-0.49, -0.43) is out of map bounds. So, the costmap cannot raytrace for it.", "[ WARN] [1510033500.647048940, 2138.619000000]: The origin for the sensor at (-0.49, -0.46) is out of map bounds. So, the costmap cannot raytrace for it.", "[ WARN] [1510033507.102539142, 2141.625000000]: The origin for the sensor at (4 ...", "I tried several times but I can't add more images to my post above. Anyone has any suggestions on how to solve this problem/bug? For the time being I added the images on imgur instead and provided the link here. It didn't allow me to add images using imgur link either."], "answer": [], "question_details": [" ", " ", "After the robot decided to take the other route (much longer) I get the following result after a while (I am running this off of a virtual machine unfortunately so the robot movement is slow, can't speed that up somehow). I have marked areas similar to the image above to give an idea. The robot is now located here and is still trying to find its way around."], "url": "https://answers.ros.org/question/275166/multiple-issues-while-attempting-to-create-a-simple-robot-for-localization-in-simulation-using-amcl/"},
{"title": "can,t read rplidar data on rasberry pi.", "time": "2017-11-12 11:32:11 -0600", "post_content": [" ", " ", "i am using rasberry pi 2 model B.i want to do slam with rplidar and i have done it by connecting rplidar with pc but when i did same thing by connecting lidar with rasberry pi its rotation speed became too slow and it is not showing scan data in rplidar sdk on rasberry pi.I used ubuntu mate in pi. Maybe it is power issue.\n     laptop usb port current rating is 400 to 500 mA ,and  one solution i found on this website is that to increase current rating from 0.6 to 1.2 ampere     by max_usb_current=1 in /boot/config.txt ..\n    But i think this current will damage the lidar as it is more than lidar power requirement.\n  can any body help me please?\nThanks in advance...", "Please tell us how this is related to ROS?"], "answer": [], "url": "https://answers.ros.org/question/275667/cant-read-rplidar-data-on-rasberry-pi/"},
{"title": "Robot rotates around a point instead of navigating to Goal Position when working with amcl", "time": "2017-11-20 19:46:37 -0600", "post_content": [" ", " ", "Hello,", "I have a custom robot with 2 wheels, a camera, and a laser rangefinder. I am trying to localize the robot using amcl. ", "I had everything set up. When rviz was launched I would define the initial 2D pose estimate and the used to define the 2d Nav Goal in RViz as well. ", "However, I implemented a basic node to publish to the topic so that I could have the robot move to the goal directly.   When I run that node, RViz displays the correct path to the goal. But the robot starts moving forward and after a while (usually around the same spot) it starts to rotate around a single point for some reason. ", "I am not sure why that's the case. It works fine when I try to run it via RViz but not via the node. The code I am working with for the node -", "I run the above using ", "This is what it looks like right now. The robot started to rotate instead of following the green path.", "This is not related to ", ", which provides localization. I guess this has something to do with how you setup ", " and the respective planners. I don't see anything wrong in particular with you code, so IMO this should work. You could check the return value of ", " to see if ...", "... you are actually connected, but if the Robot starts to drive, all should be fine.", "Maybe Elaborate on the Problem, does the localization get lost (i.e. real/simulated Position vs. Position in rviz), what is your ", " configuration...", " Thanks for the reply. As of now, it was because of either an additional planner or a planner parameter added by mistake that was causing this. It's been working since I removed that. I will go through some tests to confirm and then add an answer.", " Sir Kindly help me, i'm also facing the same problem since a month.Kindly provide me the solution. it would be a great help. Thanks"], "answer": [], "question_code": ["    #include <ros/ros.h>\n#include <move_base_msgs/MoveBaseAction.h>\n#include <actionlib/client/simple_action_client.h>\n#include <tf/transform_datatypes.h>\n\ntypedef actionlib::SimpleActionClient<move_base_msgs::MoveBaseAction> MoveBaseClient;\n\nint main(int argc, char** argv) {\n    ros::init(argc, argv, node_name);\n\n\n    // create the action client\n    // true causes the client to spin its own thread\n    MoveBaseClient ac(\"move_base\", true);\n\n    // Wait 60 seconds for the action server to become available\n    ROS_INFO(\"Waiting for the move_base action server\");\n    ac.waitForServer(ros::Duration(5));\n\n    ROS_INFO(\"Connected to move base server\");\n\n    // Send a goal to move_base\n    move_base_msgs::MoveBaseGoal goal;\n    goal.target_pose.header.frame_id = \"map\";\n    goal.target_pose.header.stamp = ros::Time::now();\n\n    goal.target_pose.pose.position.x = 0.995;\n    goal.target_pose.pose.position.y = -2.996;\n    goal.target_pose.pose.orientation.w = 1;\n\n    ac.sendGoal(goal);\n\n    // Wait for the action to return\n    ac.waitForResult();\n\n    if (ac.getState() == actionlib::SimpleClientGoalState::SUCCEEDED)\n        ROS_INFO(\"You have reached the goal!\");\n    else\n        ROS_INFO(\"The base failed for some reason\");\n\n    return 0;\n}\n", "rosrun package_name node_name", "amcl", "move_base", "waitForServer", "move_base"], "url": "https://answers.ros.org/question/276372/robot-rotates-around-a-point-instead-of-navigating-to-goal-position-when-working-with-amcl/"},
{"title": "Issues with custom robot amcl in Rviz/Gazebo", "time": "2017-11-13 16:47:30 -0600", "post_content": [" ", " ", "Hello,", "I am following a basic tutorial and I created my own robot in Gazebo and Rviz, and added laser and camera sensors to it. I then utilized a map from Clearpath to try and localize this robot using AMCL. ", "There are a few issues I am running across that I am unable to fix as of now.", "The robot when I launch in RViz needs to be set to the \"odom\" fixed frame, however the map requires the \"map\" to function. If I select \"odom\" then the actual map in RViz shuffles about a point (jittery motion). If I select \"map\" then the robot seems to be jittering about. I am not entirely sure how to fix that. This doesn't happen in Gazebo so I am assuming this doesn't contribute to any errors. But any suggestions on what I am missing or understanding incorrectly?", "As soon as I run the launch file for the amcl (which loads the map, and starts amcl node, and the move_base node with the config params), I load it up in RViz and then select \"2D pose estimate\" and point the arrow towards a particular path in the map from the robot. I am attaching an image below. The green circle is where the robot starts and the red arrow underneath is the \"2D pose estimate I set\". As you can see, as soon as I did that, the costmap depicts it as if the area is blocked off. I select the \"2D Nav Goal\" and set it as per the Red arrow you can see on the left side. Since as per the costmap the area in front of the robot is blocked, it decides to take the path opposite of it to try to reach the goal. I am trying to understand why this is happening and how to rectify it. Is it any specific parameter somewhere that I need to tweak or I am initializing something incorrectly? ", "And I get the following errors/warnings -", "[ERROR] [1510033498.355228432, 2137.605000000]: None of the points of the global plan were in the local costmap, global plan points too far from robot", "[ WARN] [1510033498.369760487, 2137.609000000]: The origin for the sensor at (-0.49, -0.43) is out of map bounds. So, the costmap cannot raytrace for it.", "[ WARN] [1510033500.647048940, 2138.619000000]: The origin for the sensor at (-0.49, -0.46) is out of map bounds. So, the costmap cannot raytrace for it.", "[ WARN] [1510033507.102539142, 2141.625000000]: The origin for the sensor at (4 ...", "I think you're going to need to post your launch/config files in order for anyone to comment."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "After the robot decided to take the other route (much longer) I get the following result after a while (I am running this off of a virtual machine unfortunately so the robot movement is slow, can't speed that up somehow). I have marked areas similar to the image above to give an idea. The robot is now located here and is still trying to find its way around."], "url": "https://answers.ros.org/question/275808/issues-with-custom-robot-amcl-in-rvizgazebo/"},
{"title": "Robot rotates around a point instead of navigating to Goal Position when using the navigation stack", "time": "2017-11-13 16:32:30 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have a custom robot with 2 wheels, a camera, and a laser rangefinder. I am trying to localize the robot using amcl. ", "I had everything set up. When rviz was launched I would define the initial 2D pose estimate and the used to define the 2d Nav Goal in RViz as well. ", "However, I implemented a basic node to publish to the topic so that I could have the robot move to the goal directly.   When I run that node, RViz displays the correct path to the goal. But the robot starts moving forward and after a while (usually around the same spot) it starts to rotate around a single point for some reason. ", "I am not sure why that's the case. It works fine when I try to run it via RViz but not via the node. The code I am working with for the node -", "I run the above using ", "This is what it looks like right now. The robot started to rotate instead of following the green path."], "answer": [], "question_code": ["    #include <ros/ros.h>\n#include <move_base_msgs/MoveBaseAction.h>\n#include <actionlib/client/simple_action_client.h>\n#include <tf/transform_datatypes.h>\n\ntypedef actionlib::SimpleActionClient<move_base_msgs::MoveBaseAction> MoveBaseClient;\n\nint main(int argc, char** argv) {\n    ros::init(argc, argv, node_name);\n\n\n    // create the action client\n    // true causes the client to spin its own thread\n    MoveBaseClient ac(\"move_base\", true);\n\n    // Wait 60 seconds for the action server to become available\n    ROS_INFO(\"Waiting for the move_base action server\");\n    ac.waitForServer(ros::Duration(5));\n\n    ROS_INFO(\"Connected to move base server\");\n\n    // Send a goal to move_base\n    move_base_msgs::MoveBaseGoal goal;\n    goal.target_pose.header.frame_id = \"map\";\n    goal.target_pose.header.stamp = ros::Time::now();\n\n    goal.target_pose.pose.position.x = 0.995;\n    goal.target_pose.pose.position.y = -2.996;\n    goal.target_pose.pose.orientation.w = 1;\n\n    ac.sendGoal(goal);\n\n    // Wait for the action to return\n    ac.waitForResult();\n\n    if (ac.getState() == actionlib::SimpleClientGoalState::SUCCEEDED)\n        ROS_INFO(\"You have reached the goal!\");\n    else\n        ROS_INFO(\"The base failed for some reason\");\n\n    return 0;\n}\n", "rosrun package_name node_name"], "url": "https://answers.ros.org/question/275806/robot-rotates-around-a-point-instead-of-navigating-to-goal-position-when-using-the-navigation-stack/"},
{"title": "When working with amcl why does the robot try to move closer to the wall in an open area rather than navigating more towards the goal?", "time": "2017-11-20 20:28:06 -0600", "post_content": [" ", " ", " ", " ", "For example, the following", "The robot is closer to the wall instead of following the path to the goal (green line).", "I have noticed that happen all the time. Why is that so? Is there a particular parameter that influences this? As soon as the robot gets close to the wall it gets better at navigating straight to the goal I think. Or is it something related to amcl algorithm(s)?", "Here is the robot shortly after reaching close to wall and then moving towards the goal again.", "What causes such a pattern/behavior?", "Here are some of the config/params I am working with.", "move_base node", "costmap_common_params.yaml", "map_type: costmap", "global_costmap_params.yaml", "local_costmap_params.yaml", "base_local_planner_params.yaml", "This is not related to AMCL, which is a localization node. This is related to the navigation package and the planners you are using. Please post the config files you are using...", "@Procopio Updated my question. Thanks!"], "answer": [" ", " ", "I am in no way an expert about this but had a similar issue and was able to solve it. I'll share my experience. \nI had an issue that when the robot came close to a wall or corner it would stop, and then very slowly turn more towards the wall and then get hopelessly stuck trying not to hit the wall while turning towards it. Sometimes it would actually hit the wall, but not always. This was using the base navigation stack on Jade. If this sounds like the problem you're having, then read on.", "To solve it I ended up increasing the inflation radius a bit more than should have been needed and then set the footprint radius to a value smaller than reality. With this, the robot would get closer to the wall than the nav stack would realize, but it also meant the nav stack would not try to avoid it unnecessarily. With those changes things clicked and started working. Maybe try setting the radius of your robot to 0.2 or the inflation radius to 0.7 and see if it improves.", "smaller the local_costmap width and height should be help to this,if it is odom or wheel deviation.And up pdist_scale maybe also useful.", "  Thanks for the response! But shouldn't increasing the inflation radius be a problem in narrower spaces? I have somewhat narrow hallways on my map, and increasing the inflation radius causes issues with that. Unless I am mistaken here. ", " Thanks! I will try those as well. I haven't messed around with any of the height/width/resolution parameters because I am unsure what kind of effects those changes will have. I have kind of patched things together, and there are too many things to tune here.", " ", " ", "Try to set the ", " of your local costmap to ", " ", "Thank you. This hasn't helped, unfortunately."], "question_code": ["<node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"$(find robot)/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"$(find robot)/config/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"$(find robot)/config/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find robot)/config/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find robot)/config/base_local_planner_params.yaml\" command=\"load\" />\n\n    <param name=\"controller_frequency\" value=\"10.0\"/>\n    <param name=\"planner_frequency\" value=\"10.0\"/>\n\n    <remap from=\"cmd_vel\" to=\"cmd_vel\"/>\n    <remap from=\"odom\" to=\"odom\"/>\n    <remap from=\"scan\" to=\"robot/laser/scan\"/>   \n</node>\n", "origin_z: 0.0 \nz_resolution: 1\nz_voxels: 2\n\n\nobstacle_range: 2.5 \nraytrace_range: 3.0 \n\npublish_voxel_map: false\ntransform_tolerance: 0.5 \nmeter_scoring: true\n\nrobot_radius: 0.3 \ninflation_radius: .6\n\nobservation_sources: laser_scan_sensor\n\nlaser_scan_sensor: {sensor_frame: hokuyo, data_type: LaserScan, topic: /robot/laser/scan, marking: true, clearing: true}\n", "global_costmap:\n   global_frame: map \n   robot_base_frame: chassis\n   update_frequency: 10.0\n   publish_frequency: 5.0 \n   width: 40.0\n   height: 40.0\n   resolution: 0.05\n   origin_x: -20.0\n   origin_y: -20.0\n   static_map: true\n   rolling_window: false\n", "local_costmap:\n   global_frame: map \n   robot_base_frame: chassis\n   update_frequency: 10.0\n   publish_frequency: 5.0 \n   width: 10.0\n   height: 10.0\n   resolution: 0.05\n   static_map: false\n   rolling_window: true\n", " TrajectoryPlannerROS:\n  max_vel_x: 0.5 \n  min_vel_x: 0.1 \n\n  max_vel_theta: 1.5 \n  min_in_place_vel_theta: 0.314\n\n  acc_lim_theta: 20.0\n  acc_lim_x: 10.0\n  acc_lim_y: 5.0 \n\n\n  holonomic_robot: false\n"], "answer_code": ["global_frame"], "url": "https://answers.ros.org/question/276373/when-working-with-amcl-why-does-the-robot-try-to-move-closer-to-the-wall-in-an-open-area-rather-than-navigating-more-towards-the-goal/"},
{"title": "how to move my robot from a init pose to a target pose?", "time": "2017-12-22 03:23:28 -0600", "post_content": [" ", " ", "hi,I wanna to make auto docking to my robot ,I get the power station pose by detect the marker . now I test the method Dubins path and R-S path but the result path not so well .I want to get a smooth path form init pose to power station ,just one curve not two or three curves like Dubins or R-S path, can you give me some ideas??", " Since you asked this question twice ( ", " ), please delete one of them. "], "answer": [], "url": "https://answers.ros.org/question/278043/how-to-move-my-robot-from-a-init-pose-to-a-target-pose/"},
{"title": "use librviz to add 2dNavGoal tool, 2dNavGoal not work", "time": "2017-11-29 20:11:55 -0600", "post_content": [" ", " ", " ", " ", "(1) I create a toolbar ", "(2) add  2dNavGoal and Interact ", "(3) add Tool function", "But 2dNavGoal tool doesn't work when I press Button to send Goal, the arrow also doesn't change to green arrow."], "answer": [], "question_code": ["toolbar_ = new QToolBar( \"Tools\" );\n", "myToolmanagr_ = manager_->getToolManager();\n\nmyInteractionTool_ = myToolmanagr_->addTool(\"rviz/Interact\");\n\n addTool(myInteractionTool_);  \n\n myNav_goal_tool_ = myToolmanagr_->addTool(\"rviz/SetGoal\"); \n\n addTool(myNav_goal_tool_);\n", "void MyViz::addTool( rviz::Tool* tool )\n{\n\n  QAction* action = new QAction( tool->getName(), toolbar_actions_ );\n\n  action->setIcon( tool->getIcon() );\n\n  action->setIconText( tool->getName() );\n\n  action->setCheckable( true );\n\n  toolbar_->addAction( action );\n\n  action_to_tool_map_[ action ] = tool;\n\n  tool_to_action_map_[ tool ] = action; \n\n}\n"], "url": "https://answers.ros.org/question/276934/use-librviz-to-add-2dnavgoal-tool-2dnavgoal-not-work/"},
{"title": "joint_trajectory_controller with non zero final velocity", "time": "2017-12-27 16:33:37 -0600", "post_content": [" ", " ", " ", " ", "Is it possible to provide the joint_trajectory_controller with a trajectory where the last point in the trajectory has a velocity that is non zero?", "Imagine i have a robot arm that i want to punch a wall that does not break or cannot be penetrated. I want to provide a trajectory of arbitrary path heading towards the wall (straight punch, uppercut, hook, whatever), but once it gets to the wall, i dont want it to slow down and stop there, i want it to hit the wall as hard as possible. Now once this happen there will be a force from the wall back through the joints of the robot, but i am relying on powerful torque motors to sustain the backlash.", "What kind of trajectory would i provide the joint_trajectory_controller, or what other scheme, may i use to obtain this action?", "Thanks.", "I think the fastest way to figure this out is to just try out what you want - in simulation fi. Or read the code and see what ", " does internally."], "answer": [], "question_code": ["joint_trajectory_controller"], "url": "https://answers.ros.org/question/278279/joint_trajectory_controller-with-non-zero-final-velocity/"},
{"title": "Can not build sick_tim package on raspberry pi 3", "time": "2017-11-29 06:53:11 -0600", "post_content": [" ", " ", "Hi there,\nI'm using Ubuntu mate on Raspberry pi 3 as the core of my robot project. I need to connect it to sick tim laser scanner. However when I'm running catkin_make after cloning ros-kinect package for sick_tim everything just freezes and I must disconnect my RPi from power and boot it again.\nEach time the terminal freezes at this line:"], "answer": [], "question_code": ["[21%] Building CXX object sick_tim/CMakeFiles/sick_tim_3xx.dir/src/sick_tim_common_usb.cpp.o\n"], "url": "https://answers.ros.org/question/276893/can-not-build-sick_tim-package-on-raspberry-pi-3/"},
{"title": "Error at kobuki_node minimal.launch: \"could not contact master\"", "time": "2017-11-28 21:22:12 -0600", "post_content": [" ", " ", "When I run ", "roslaunch kobuki_node minimal.launch", "I get the following output", "... logging to\n  /home/ubuntu/.ros/log/8e8d5c0c-d4b3-11e7-836c-902e1cfcc74f/roslaunch-tegra-ubuntu-2175.log\n  Checking log directory for disk usage.\n  This may take awhile. Press Ctrl-C to\n  interrupt Done checking log file disk\n  usage. Usage is <1GB.", " started roslaunch server\n   ", "PARAMETERS  *\n  /diagnostic_aggregator/analyzers/input_ports/contains:\n  ['Digital Input',...  *\n  /diagnostic_aggregator/analyzers/input_ports/path:\n  Input Ports  *\n  /diagnostic_aggregator/analyzers/input_ports/remove_prefix:\n  mobile_base_nodel...  *\n  /diagnostic_aggregator/analyzers/input_ports/timeout:\n  5.0  * /diagnostic_aggregator/analyzers/input_ports/type:\n  diagnostic_aggreg...  *\n  /diagnostic_aggregator/analyzers/kobuki/contains: ['Watchdog', 'Mot...  *\n  /diagnostic_aggregator/analyzers/kobuki/path:\n  Kobuki  *\n  /diagnostic_aggregator/analyzers/kobuki/remove_prefix:\n  mobile_base_nodel...  *\n  /diagnostic_aggregator/analyzers/kobuki/timeout:\n  5.0  * /diagnostic_aggregator/analyzers/kobuki/type:\n  diagnostic_aggreg...  *\n  /diagnostic_aggregator/analyzers/power/contains:\n  ['Battery']  *\n  /diagnostic_aggregator/analyzers/power/path:\n  Power System  *\n  /diagnostic_aggregator/analyzers/power/remove_prefix:\n  mobile_base_nodel...  *\n  /diagnostic_aggregator/analyzers/power/timeout:\n  5.0  * /diagnostic_aggregator/analyzers/power/type:\n  diagnostic_aggreg...  *\n  /diagnostic_aggregator/analyzers/sensors/contains:\n  ['Cliff Sensor', ...  *\n  /diagnostic_aggregator/analyzers/sensors/path:\n  Sensors  *\n  /diagnostic_aggregator/analyzers/sensors/remove_prefix:\n  mobile_base_nodel...  *\n  /diagnostic_aggregator/analyzers/sensors/timeout: 5.0  * /diagnostic_aggregator/analyzers/sensors/type:\n  diagnostic_aggreg...  *\n  /diagnostic_aggregator/base_path:   *\n  /diagnostic_aggregator/pub_rate: 1.0 \n  * /mobile_base/base_frame: base_footprint  *\n  /mobile_base/battery_capacity: 16.5  *\n  /mobile_base/battery_dangerous: 13.2 \n  * /mobile_base/battery_low: 14.0  * /mobile_base/cmd_vel_timeout: 0.6  *\n  /mobile_base/device_port: /dev/kobuki \n  * /mobile_base/odom_frame: odom  * /mobile_base/publish_tf: True  *\n  /mobile_base/use_imu_heading: True  *\n  /mobile_base/wheel_left_joint_name:\n  wheel_left_joint  *\n  /mobile_base/wheel_right_joint_name:\n  wheel_right_joint  * /rosdistro:\n  indigo  * /rosversion: 1.11.19", "NODES   /\n      diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n      mobile_base (nodelet/nodelet)\n      mobile_base_nodelet_manager (nodelet/nodelet)", " auto-starting new master\n  process[master]: started with pid\n  [2186] ERROR: could not contact master\n  [ ", " ] The\n  traceback for the exception was\n  written to the log file [master]\n  killing on exit ", "and when I look at the logs it appears that it does connect to the master for a short time but then for some reason looses the connection.  What's weird is that I tried this a few days ago and it worked successfully.  I really hope this isn't a hardware failure.  Any suggestions?", "bueller???"], "answer": [], "url": "https://answers.ros.org/question/276860/error-at-kobuki_node-minimallaunch-could-not-contact-master/"},
{"title": "Building a ROS based quadrotor", "time": "2017-12-24 06:08:40 -0600", "post_content": [" ", " ", "Hello ROS Community,", "I'm an engineering undergraduate and hobbyist trying to enter the world of aerial robotics. I figured learning ROS was an essential part of any project involving UAVs and so for the past 4-5 months I have read a few books, searched around a lot and used RTAB-Map with a Kinect sensor on a Raspberry Pi to wirelessly create 3D maps of indoor environments.The idea now is to build a quadrotor and mount the Kinect sensor along with some powerful onboard computer (better than the RPi)  on it to do 3D mapping and navigation with it.", "Now, I'm not sure if that's exactly what I want to do because I want to learn the actual mathematical modelling and control of a quadrotor (I'm taking a course on Coursera), and the truth is that I'm confused as to what project exactly would be best for someone at my stage. Any thoughts?", "The problem I think is that there is no such robotic culture or projects at my institute, so I basically have to figure out everything on my own. Usually, for all the UAV labs and projects that I see, I almost never see the specifications of the quadrotor, onboard computer, exact flight control flow, etc, which keeps me wondering how I would be able to replicate it. I guess for the majority of the people working on such projects, the knowledge is passed on from professor to students, and there is never a need to share it online. Also, the quadrotors are mostly research drones such as the AscTec Pelican, which is expensive for a student to get, and I couldn't simply find a project with a DIY quadrotor.", "So based on what I know, I'll be building a quadrotor capable of handling the required payload, use the Naze32/Flip32 FC on it along with some onboard computer like the ODROID XU4 or probably the Intel NUC, and then put ROS along with the ", " firmware on the FC. Is that how things are done? Would such a system be flexible enough to handle a variety of projects?", "I might be terribly off, please post your thoughts. Thank you for your time.", "As an engineer and hobbyist, I've built drones and ROS-based robots, but not (yet) a ROS-based quad. From my experience, the best way to start is just to start building. You've got some ideas, go and test them out. But do it one-by-one.", "Thank you for your response Arif. I've bought the Naze32 and I plan to initially install and test things out on it first, and then finalize on the quadrotor specifications. Everything being funded in my individual capacity, it is an expensive build, so I don't want to go in unplanned.", "Any news on this one? I'm looking into using a naze32 with usb from raspberry pi"], "answer": [], "url": "https://answers.ros.org/question/278133/building-a-ros-based-quadrotor/"},
{"title": "How to connect VLP-16 velodyne to indigo?", "time": "2017-05-08 12:43:05 -0600", "post_content": [" ", " ", "Under Ubuntu 14.04, indigo,\n          My situation now is: ", "(in new ternimal)", "(in new terminal)", "in new terminal:", "Whether I powered and connected the lidar to my computer or not, there seems to be no difference in the results above.", "I think this means the velodyne lidar wasn't connected to linux OS. Is it true?", "Therefore, how to connect VLP-16 velodyne to indigo?  How to connect the lidar with my computer through the ethernet? \n I only need to power the lidar and connect the lidar to my computer by the Ethernet line, and I don't need to do anything else? For example, I need to turn off wifi or change some network settings in my linux system?"], "answer": [" ", " ", "Your error message has nothing to do with connectivity to the velodyne. It means that rostopic echo does not know about the ", " datatype. And this is likely because it's in your workspace. However in your 2nd terminal you have not source the setup file. Try running", "before you try to echo. ", " I'd recommend reviewing the catkin tutorials especially:  ", "It doesn't make any difference. Still no data is published", " ", " ", "I think first you need to figure out if the LiDAR is connected to the computer or not, if the IP address is 192.168.1.77 as mentioned, try pinging it. If it doesn't return anything then that's not the IP address, by default the VLP-16 IP address is 192.168.1.201. Also, before you use ", " you should first run ", " to display the topics being published. "], "question_code": ["      sudo ifconfig eth0 192.168.1.77\n", "      roscore\n", "      cd ./catkin_ws  \n  source devel/setup.bash\n  roslaunch velodyne_pontcloud VLP16_points.launch\n", " rostopic echo /velodyne_points     However, there is no data printed.\n rostopic echo /velodyne_packets   It is said:\"Cannot load message class for [velodyne_msgs/VelodyneScan]. Are your mess ages bulit?\"\n"], "answer_code": ["velodyne_msgs/VelodyneScan", "cd ./catkin_ws  \nsource devel/setup.bash\n", "rostopic echo \\topic", "rostopic list"], "url": "https://answers.ros.org/question/261330/how-to-connect-vlp-16-velodyne-to-indigo/"},
{"title": "Does ros depend on package?", "time": "2018-01-16 10:05:41 -0600", "post_content": [" ", " ", "Hello guys,\nit might be a weird question because im new in ros :))", "Here it is, once i read AGITR (A gentle introduction to ROS). I learnt that the turtle moves by using node subscribing or publishing topic (turtle1/...).", "or the way drone contolled by mavros package (mavros/...)", "so.. the question is, how can I make a mobile robot (turtlebot) moving each wheel, read from odometry, know battery capacity, etc. anyway I use Raspy 3 and arduino (the processor and the controller).", "Is there any Idea? Thank in advance"], "answer": [], "url": "https://answers.ros.org/question/279984/does-ros-depend-on-package/"},
{"title": "ROS Kinetic Opencv3 build issues", "time": "2018-01-18 18:28:42 -0600", "post_content": [" ", " ", "Hi, I'm having issues installing opencv3 package on my Debian(Stretch) armhf system. I spent more than a week trying various things, but have been unable to get it to build. I wanted to see if anyone has experienced the same errors on their system and were able to find a solution? All ROS dependencies were successfully install prior.", "I'm getting the following errors:", "[ 67%] Automatic moc for target opencv_cvv\n[ 67%] Built target opencv_cvv_automoc\n[ 67%] Building CXX object modules/cvv/CMakeFiles/opencv_cvv.dir/src/view/dual_filter_view.cpp.o\nIn file included from /usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglextrafunctions.h:47:0,\n                 from /usr/include/arm-linux-gnueabihf/qt5/QtGui/QtGui:55,\n                 from /home/linaro/ros/src/opencv3/opencv_contrib/cvv/src/view/dual_filter_view.cpp:15:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glBindTexture(GLenum, GLuint)\u2019:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h:595:5: error: \u2018::glBindTexture\u2019 has not been declared\n     ::glBindTexture(target, texture);\n     ^~\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glBlendFunc(GLenum, GLenum)\u2019:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h:606:5: error: \u2018::glBlendFunc\u2019 has not been declared\n     ::glBlendFunc(sfactor, dfactor);\n     ^~\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glClear(GLbitfield)\u2019:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h:617:5: error: \u2018::glClear\u2019 has not been declared\n     ::glClear(mask);\n     ^~\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glClearColor(GLclampf, GLclampf, GLclampf, GLclampf)\u2019:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h:628:5: error: \u2018::glClearColor\u2019 has not been declared\n     ::glClearColor(red, green, blue, alpha);\n     ^~\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glClearStencil(GLint)\u2019:\n/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h:639:5: error: \u2018::glClearStencil\u2019 has not been declared\n     ::glClearStencil(s);\n     ^~", "..........keeps going for many other functions", "/usr/include/arm-linux-gnueabihf/qt5/QtGui/qopenglfunctions.h: In member function \u2018void QOpenGLFunctions::glVertexAttribPointer(GLuint, GLint, GLenum, GLboolean, GLsizei, const void", "* [all] Error 2\n<== Failed to process package 'opencv3': \n  Command '['/opt/ros/kinetic/env.sh', 'make', '-j1']' returned non-zero exit status 2", "Appreciate any help in advance,", "G"], "answer": [], "url": "https://answers.ros.org/question/280238/ros-kinetic-opencv3-build-issues/"},
{"title": "How to read read the video data from Vport P16-1MP-M12-IR camera", "time": "2018-01-29 05:22:23 -0600", "post_content": [" ", " ", "I have rugged vPort  P16-1MP-M112-IR  camera which provides an HD (720P,1280X720)  video image and feature an H.264/MJPEG IP dome. In addition,the cameras feature EN 50155 compliance ,vandal proofing , rugged M12 ethernet port,built in microphone , digital input PoE power input and an IR illuminator for day and night image capability.", "Can anybody  suggest ROS driver package for the same in order to read the video image from camera.", "Hey,\nWere you able to solve the issue ?\nIf yes , Could you please share the solution here."], "answer": [], "url": "https://answers.ros.org/question/281134/how-to-read-read-the-video-data-from-vport-p16-1mp-m12-ir-camera/"},
{"title": "Localizing on a vertical plane", "time": "2017-12-07 03:14:41 -0600", "post_content": [" ", " ", " ", " ", "I am currently working on a mobile robot that is able to drive on a vertical surface. As it is a mobile robot i would like to be able to locate the robot continuously. From what i have gathered the two most common ways are either ", " or ", ". I haven't been able to find much material on ", " so for now I am looking at ", ". In the ", " it states that:", "\"The Roll and Pitch angles are interpreted as absolute angles (because an IMU sensor has a gravity reference), and the Yaw angle is interpreted as a relative angle.", "Because I am trying to locate on a vertical plane this is exactly the condition i would like to exploit. Say that my map is represented in the XY-plane in Rviz (even though it is the physical XZ-plane) my absolute angle will be the Z-angle and this would enable me to have a absolute (non-drift) heading of my robot. Depending on the configuration of the robot the X or Y angles might line up with the gravity vector, I don't know if this could cause any trouble. ", "I have tried to see if there is a way to bend ", " into compliance, be it rotating the odometry of my robot into another plane to match up with the IMU or vice versa. But the problem I keep running into is that ", " has one relative angle and two absolute where i have at worst the same but depending on configuration a might have three absolute. \nfurthermore as far as I have read ", " neglects the ", " and ", " which inhibits me in rotating the whole system into the physical plane.", "It might also be cause for trouble that the gravity vector no longer is on a specified axis as it can rotate as the robot rotates around the Z-axis.", ": Is there a way to make ", " compliant with my configuration, that I just can't seem to figure out? Maybe it can be modified to neglect X and Y instead of Z and then just use Z alone?", "OR", "Maybe this is a job for ", ", I just haven't been able to find much information on this.", "Any suggestions are MUCH appreciated, as I have set this as a (significant) milestone i my Bachelors project and I am running low on time.", ": This definitely seems like a job for ", ", although in ", " talk it states that it is required to have compass readings, but i can't see why this is necessary, and it is definitely not an option as the robot I am working on relies on (very) powerful magnets to 'stick' to the vertical surface. Shouldn't the fused orientation the IMU is sending out be enough to give a heading? I am using a ", " IMU.", "Even though I am closer to a solution I am by no means there ..."], "answer": [" ", " ", "You don't need a compass for ", ". You just need a compass for integrating GPS data. If you are just driving around on a wall, there's no need for that.", "I don't really see that your use case is that different. It's no different than, say, an aerial vehicle flying straight up. ", " is a 3D state estimation package, so moving around the Z axis is fine. I'm not sure I would \"pretend\" to be in the X-Y plane for this problem. If your IMU says you're climbing a wall, then just fuse that into the EKF. Your EKF yaw angle will remain fixed the entire time, because the wall has a fixed world-frame Z axis orientation. Roll and pitch will move around as the robot drives, but those are absolute angles, so none of your angles should drift, provided that you fuse roll, pitch, and yaw, and not just their velocities.", "The only caveat is that ", " uses Euler angles internally, so you may have some trouble if the noise in the measurements sends you through +/- 90 degrees on pitch (or if the wall isn't perfectly flat). YMMV.", "An alternative, though I haven't though through if this would work, would be to define a transform for your ", " frame to the ", " frame that captures the vertical orientation of the IMU. Then, turn on ", " and see what happens. I honestly haven't thought about this hard enough, so I'm not positive it'll work. This would be effectively pretending you are in the X-Y plane."], "question_code": ["robot_pose_ekf", "robot_localization", "robot_localization", "robot_pose_ekf", "robot_pose_ekf", "robot_pose_ekf", "robot_pose_ekf", "odom.pose.pose.position.x", "odom.twist.twist.linear.z", "robot_pose_ekf", "robot_localization", "robot_localization"], "answer_code": ["r_l", "r_l", "r_l", "two_d_mode"], "url": "https://answers.ros.org/question/277543/localizing-on-a-vertical-plane/"},
{"title": "How does robot_localization finds tf between odom and base_link?", "time": "2018-01-20 08:50:30 -0600", "post_content": [" ", " ", " ", " ", "Hello", "Is there any documnetation that describes how the relation between odom frame and based_link frame is calculated?", "It seems that some sort of self-calibration is happening there", "Thank you. I was totally misunderstood about robot_localization and robot_pose_ekf. I thought that they can calculate the transformations between sensor frames automatically. This is called self-calibration. ", "I was trying to fuse IMU with Camera (visual odometry). I was putting my IMU frame as base_link and the camera frame as odom. I thought that robot_localization calculates the transofrmation between two. Now I understand that setting IMU frame as base_link is wrong. The base_link is actually some point at the middle of robot's chasis. I have to publish a tf between imu_link and camera frame (odom). ", "Now I can see that the power of robot_localization is multi_rate fusion of multiple sensors but with known inter-sensor transformations. This is different from MSF:", "which tries to estimate inter-sensor transformations by knowing initial estimates of those transformations and updating them in the state vector.", "Ah, I understand your confusion now. Yes, the EKF doesn't really know anything about the relationships between sensors, as you've stated. It just wants to know, for a given measurement, how to transform it into a frame that it \"knows\" about, and you provide those transforms."], "answer": [" ", " ", "I'm not sure what you mean by calibration in this case, but I can tell you what the ", "->", " transform is.", "The ", " library exists to define coordinate frames and the relationships between them. REP-105 defines some of the principal coordinate frames in ROS, but we'll focus on ", " and ", ".", "The ", " frame is a coordinate frame that is firmly affixed to the robot. In other words, the +X axis extends straight out of the front of the robot, the +Y axis extends directly to the robot's left, and the +Z axis extends straight up in the air. No matter where the robot goes, the ", " frame is attached to the robot's origin, and so those axes remain in the same location w.r.t. the robot. Just imagine that someone 3D printed a small coordinate frame and glued it to your robot.", "Now, as your robot moves around in the world, we have to know what its pose is in that world. That \"world\" is the ", " coordinate frame. When the robot starts out, it's typically at position ", " and with rotation/yaw/heading of ", ". That means your pose in the ", " frame is ", " with a yaw of ", ". If you drive forward 1 meter, than your pose in the ", " frame is ", " with a yaw of ", ". If you then turn left pi/2 radians, your pose in the ", " frame is ", " with a yaw of ", ". If you drive forward another meter, your robot's ", " frame pose is ", " with a yaw of ", ".", "The key point is this: your robot's pose in the world is equivalent to the ", "->", " transform (really, you could say it's ", "->", ", but that's not really important for this discussion). In other words, if I want to transform a point from the ", " coordinate frame to the ", " (world) coordinate frame, you need that transform, and that transform is just your robot's ", " frame pose.", "All the state estimation nodes are doing in ", " is computing the robot's pose in the world frame (e.g., ", " or ", ") and doing two things:", "The way it computes those things is via sensor fusion. If you have wheel encoders that say you are driving at 1.0 m/s forward, then the EKF integrates those velocities over time, so after 10 seconds, it says you went 10 meters straight forward. If you also have an IMU that says you are turning with some velocity, it integrates those at the same time, so if your wheel ..."], "answer_details": ["Sending out a ", " message with the robot's pose, velocity, and respective covariances", "Sending out a transform from the ", " parameter (e.g., ", ") to the ", " parameter (e.g., ", "). Note that this data is the same as the data in (1), but just sent out as a ", " message.", " ", " ", " ", " "], "answer_code": ["tf", "(0, 0)", "0", "(0, 0)", "0", "(1, 0)", "0", "(1, 0)", "pi/2", "(1, 1)", "pi/2", "r_l", "nav_msgs/Odometry", "world_frame", "base_link_frame", "tf"], "url": "https://answers.ros.org/question/280369/how-does-robot_localization-finds-tf-between-odom-and-base_link/"},
{"title": "moveIT multiple move_group nodes", "time": "2018-02-05 11:28:08 -0600", "post_content": [" ", " ", " ", " ", "Hi", "Is it possible to run multiple move_group nodes under different name spaces and then specifiy in code which one to use?\nI want to control a robot that is running ros including a move_group and a high level interface for that (like go to pos x or move joint to y). Since it runs on a rasperzy pi its computational power is quite poor so i want to do the scene planing including object avoidance on another machine on the network that then should give the execution command to the robot. Therefore I launch a second move_group with planing scene context etc, in a different namespace (", ")", "I figured out how to send the trajectiry to the robot, but i have problems at generating the trajectory since in my code that runs on the \"offline\" machine i use the move group interface like this : ", "I could not find a way to specify which action servers should be taken for these instances (it takes the one from the ", ", but i want to take those of the ", "\nDoes anyone of you have experience with running multiple move_group nodes, and how to adress this correctly. The documentation does not say a word about how to do that.", "Best", "Is this a duplicate of ", "? If so, please decide which one you'd like to keep open and close the other.", "Yes, sorry, somehow the browser screwd up during login and it did not show me that the question was posted, so i reposted, sorry for that. This is the actual question!"], "answer": [], "question_code": ["/planningNS", "moveit::planning_interface::MoveGroupInterface::Options loadOptions(\"arm\");\n\nmoveit::planning_interface::MoveGroupInterface move_group(loadOptions);\nmoveit::planning_interface::PlanningSceneInterface planning_scene_interface;\n", "/move_group node", "/planningNS/move_group"], "url": "https://answers.ros.org/question/281830/moveit-multiple-move_group-nodes/"},
{"title": "How to setup Intelisense on VS Code for ROS C++", "time": "2018-02-09 19:43:02 -0600", "post_content": [" ", " ", " ", " ", " is even underlined in green. I have this added to the c_cpp_properties.json file. I also installed the ROS extension (which I'm not sure does anything). ", "When I hover over top of the include, it gives me an error:", "Everything else ROS works... This in Ubuntu 16.04 and ROS Kinetic.", "I meet the same problem. I find \"x86intrin.h\" in /usr/lib/gcc/x86_64-linux-gnu/5/include. Add this path to c_cpp_properties.json and #include \"ros/ros.h\" is OK."], "answer": [" ", " ", "It's probably doable to set this up manually, but I would recommend to take a look at ", ". It has a link to an extension that should take care of all of this for you."], "question_code": ["#include \"ros/ros.h\"", "{\n    \"configurations\": [\n        {\n            \"browse\": {\n                \"databaseFilename\": \"\",\n                \"limitSymbolsToIncludedHeaders\": true,\n                \"path\": [\n                    \"/home/ben/workspaces/ROS/cir-kit/devel/include\",\n                    \"/opt/ros/kinetic/include\",\n                    \"/home/ben/workspaces/ROS/ackermann/src/twist_to_ackermann-1/include\",\n                    \"/usr/include\"\n                ]\n            },\n            \"includePath\": [\n                \"/home/ben/workspaces/ROS/cir-kit/devel/include\",\n                \"/opt/ros/kinetic/include\",\n                \"/home/ben/workspaces/ROS/ackermann/src/twist_to_ackermann-1/include\",\n                \"/usr/include\",\n                \"/usr/include/c++/5\",\n                \"/usr/include/x86_64-linux-gnu/c++/5\",\n                \"/usr/include/linux\",\n                \"/opt/ros/kinetic/include/ros\",\n                \"/usr/include/x86_64-linux-gnu\"\n            ],\n            \"name\": \"Linux\",\n            \"intelliSenseMode\": \"clang-x64\"\n        }\n    ],\n    \"version\": 3\n}\n", "#include errors detected. Please update your includePath. IntelliSense features for this translation unit (/home/ben/workspaces/ROS/ackermann/src/twist_to_ackermann-1/src/twist_to_ackermann.cpp) will be provided by the Tag Parser.\ncannot open source file \"x86intrin.h\" (dependency of \"ros/ros.h\")\n"], "url": "https://answers.ros.org/question/282295/how-to-setup-intelisense-on-vs-code-for-ros-c/"},
{"title": "How to make IMU orientation reflect in Rviz", "time": "2018-02-22 21:38:40 -0600", "post_content": [" ", " ", " ", " ", "I've built a small R2D2 like robot composed of three parts, a cylindrical torso, a panning cylindrical neck, and tilting spherical head, all equally modeled in a URDF file. There's also an IMU positioned at the front-collar of the torso cylinder, and a camera positioned at the front of the head sphere.", "With a tf/robot_state_publisher node running, I can visualize my robot in Rviz like:", "My IMU is publishing standard Imu messages to the ", " topic, which Rviz is displaying and rendering as a separate axis. It's difficult to see from the cluttered display, but it's the unlabeled axis at the bottom that's rotated askew from the rest of the model. When I rotate my robot, and thus rotating the IMU, the IMU's axes correctly pans/tilts/yaws in Rviz. However, the rest of the robot remains stationary. For example, when I lay my robot on its side, here's how Rviz renders it:", "Notice the IMU's green y-axis is now pointing straight up, but the rest of the robot remains unchanged. If my robot starts moving via its wheels, and thus updates odometry information on the standard ", " topic, it begins moving in Rviz, but any pan/tilt/yaw is not reflected. How do I fix this?", "If I'm publishing IMU data, and my IMU is represented by a linkage connected via a fixed joint to my base_link, then why isn't Rviz showing the IMU movement as a separate axis instead of apply it to the base_link?", "Is the problem happening because I've not correctly linked up my IMU data to a linkage? Is it a mistake to try and apply IMU data to an IMU linkage, or should I instead merge my IMU's orientation data directly into my odometry messages and forget trying to explicitly model the IMU in URDF altogether?", "I wanted to model the IMU in URDF to help TF take into account it's asymmetric placement (forward of the centerline).", "My URDF looks like:"], "answer": [" ", " ", " ", " ", " It sounds like what you are missing is robot_localization  ", "  (or  ", "  or hector_localization or something else similar) that will take imu data and odometry and then generate a transform that will improve upon a pure odometry based map to base_link. ", "It is important to have your imu frame in your urdf, the Imu message frame has to match up with the name of the imu frame in the urdf, it will see the imu to base link transform and do the right thing with it.", "(I haven't it used it before so corrections to the above are welcome)"], "question_code": ["/imu/data_raw", "/odom", "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"myrobot\">\n    <xacro:include filename=\"$(find ros_myrobot_description)/urdf/materials.urdf.xacro\" />\n    <xacro:property name=\"torso_radius\" value=\"0.075\" /><!-- 150 mm diameter = 75 mm radius -->\n    <xacro:property name=\"torso_height\" value=\"0.16\" /><!-- 160 mm height -->\n    <xacro:property name=\"neck_height\" value=\"0.015\" />\n    <xacro:property name=\"camera_radius\" value=\"0.008\" />\n    <xacro:property name=\"camera_thickness\" value=\"0.005\" />\n    <xacro:property name=\"M_PI\" value=\"3.141592653589793\" />\n\n    <link name=\"base_link\">\n        <visual>\n            <geometry>\n                <cylinder length=\"${torso_height}\" radius=\"${torso_radius}\" />\n            </geometry>\n            <origin xyz=\"0 0 ${torso_height/2}\" rpy=\"0 0 0\" />\n            <material name=\"red\" />\n        </visual>\n    </link>\n\n    <link name=\"imu\">\n        <visual>\n            <geometry>\n                <box size=\".03 .015 .01\" />\n            </geometry>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <material name=\"orange\" />\n        </visual>\n    </link>\n\n    <link name=\"neck\">\n        <visual>\n            <geometry>\n                <cylinder length=\"${neck_height}\" radius=\"${torso_radius}\" />\n            </geometry>\n            <origin xyz=\"0 0 ${neck_height/2}\" rpy=\"0 0 0\" />\n            <material name=\"green\" />\n        </visual>\n    </link>\n\n    <link name=\"head\">\n        <visual ..."], "url": "https://answers.ros.org/question/283476/how-to-make-imu-orientation-reflect-in-rviz/"},
{"title": "individual elements of stereo_msgs DisparityImage", "time": "2018-02-27 10:13:17 -0600", "post_content": [" ", " ", " ", " ", "Hi, ", "I am trying to print out the individual elements of a ", " message for some debugging. I am not sure how the data is stored in the main array? And regardless of how they are stored, is the data the disparity value for each pixel (in pixel coordinates)? ", "I don't want to have to convert to ", " to do simple debugging tasks. But when I did in any case by using ", ", the elements show some values up to the power of 12 which is not reasonable. My focal length is in the order of 100s of pixels (again I am not sure what focal length in pixels is). The baseline in the same message seem to be correct. Moreover, the data is visualized reasonably, if I divide each value by the max. disparity and then scale it to 0-255. So what are the values of the individual elements? ", "Thank you."], "answer": [], "question_code": ["stereo_msgs DisparityImage", "cv Mat", "cv_bridge"], "url": "https://answers.ros.org/question/283880/individual-elements-of-stereo_msgs-disparityimage/"},
{"title": "Sensor Fusion: Frame Offset for robot_location  (ZED and IMU) [closed]", "time": "2018-03-02 09:04:12 -0600", "post_content": [" ", " ", " ", " ", "UPDATE: ", "Previous Question:I am little new to the EKF world, I was trying to follow various solutions posted by people in ROS world to receive the sensor fusion ideal for my robot. I am trying to understand how to implement the sensor fusion using ZED and IMU: MPU6050. ", "My ZED is mounted 38\" (+Z direction) high from the base, and 18\" (-X direction) backwards from the center of the base at 25 degree angles down. While my IMU is located respect to ZED at 9\" from ZED (+X direction) and 9\" (-X direction) backwards from the center of the base.", "I have looked at :", " ", " ", " ", " ", " ", "I am unable to produce the results I want. What I am trying to do:\nUsing ZED camera VO, fusing with with IMU MPU6050 odometry using EKF for a powered wheelchair such that I can generate elevation mapping of the room. For now I am bypassing the odom generated by the wheelchair encoders. ", "Update: This is the code I constructed after trying to following all the other solutions, however I still can't generate the fusion. I was able to run code, but I still have issues with my tf. I am not sure how to fix it, can you probably tell me what requires to be done for tf. I did try to follow the link you suggested for tf wiki, however I am still confused.", "Updated Code:", "I see several problems with your set up:\n1. ROS employs ", " and ", " conventions. So you need to fix them first, e.g units are SI, coordinate frames are right-hand. Follow ", "...", "..to make sure your tf tree is correct.", "2) configuration of robot_localization: for odom_0, fusing absolute x,y,z position from VO IMO is not a good dea. VO without correction from IMU typically would lose track of scene due to blurring, sudden movements, etc,. ZED uses rolling shutter which...", "..in my experience is not good at find features for tracking.", "for imu_0: I'm not sure why you have 2 setups for IMU for which the difference whether or not fusing accelerations. Is there any reason to do this?", "for imu_0 you also fuse velocities, which may not be a good idea because the imu simply integrates accelerations to generate those values. You end up fusing duplicate measurement.\nFinally, you set both differential and relative option to true, is there any reason to do that?", "I would suggest you follow ", " and ", " since they provide detail setup for r_l.", "Do not try to fuse everything at once. Try to fuse one sensor at a time and make sure r_l output what you expect. And the most important thing is that you have to tune the noise and state estimator error covariances to make sure they fit your sensors. It is tedious but unavoidable.", "Thanks for your help. I am noticing the the things you have pointed, and you are right I should use tf to help me identify the correct tf frame. As for your second point, how should I fuse the odom0? I thought since the camera has VO it might be the best to use that, as for  IMU two setups, is..", "because the wheelchair is going to be on slope, and in that case the acceleration might be non-linear due to gravitational force. However, I am not sure what I thought is ideal set up. As for fusing velocities, do you suggest fusing acceleration?"], "answer": [" ", " ", "Wow, there's a lot going on here. If I were you, I would start small, and work my way up. I'd get those wheel encoders involved again, and start with an extremely simple EKF setup that just fuses the wheel encoder data with the IMU data. Make sure that is behaving and doing what you expect, then add a new sensor. Make sure you know the coordinate frames of all your sensor data and have your transforms set up accordingly.", " Hey,Tom.Can you help me see where the problem is, thanks; \n "], "question_code": ["<launch>\n\n  <arg name=\"rgb_frame_id\"                          default=\"camera_link\"                   />\n  <arg name=\"rgb_topic\"                             default=\"/camera/rgb/image_rect_color\"  />\n  <arg name=\"depth_topic\"                           default=\"/camera/depth/depth_registered\"/>\n  <arg name=\"rgb_camera_info_topic\"                 default=\"/camera/rgb/camera_info\"       />\n  <arg name=\"imu_topic\"                             default=\"/imu/data\"                     />\n  <arg name=\"imu_ignore_acc\"                        default=\"true\"                          />\n  <arg name=\"imu_remove_gravitational_acceleration\" default=\"false\"                         />\n\n <!-- Localization-only mode -->\n  <arg name=\"localization\"      default=\"false\"/>\n  <arg     if=\"$(arg localization)\" name=\"rtabmap_args\"  default=\"\"/>\n  <arg unless=\"$(arg localization)\" name=\"rtabmap_args\"  default=\"--delete_db_on_start\"/>  \n\n  <group ns=\"rtabmap\">\n    <!-- Visual Odometry -->\n    <node pkg=\"rtabmap_ros\" type=\"rgbd_odometry\" name=\"visual_odometry\" output=\"screen\" args=\"$(arg rtabmap_args)\">\n      <remap from=\"rgb/image\"       to=\"$(arg rgb_topic)\"/>\n      <remap from=\"depth/image\"     to=\"$(arg depth_topic)\"/>\n      <remap from=\"rgb/camera_info\" to=\"$(arg rgb_camera_info_topic)\"/>\n      <remap from=\"odom\"            to=\"/vo\"/>\n\n      <param name=\"frame_id\"               type=\"string\" value=\"$(arg rgb_frame_id)\"/>\n      <param name=\"publish_tf\"             type=\"bool\"   value=\"false\"/>\n      <param name=\"publish_null_when_lost\" type=\"bool\"   value=\"true\"/>\n      <param name=\"guess_from_tf\"          type=\"bool\"   value=\"true\"/>\n\n      <param name=\"Odom/FillInfoData\"      type=\"string\" value=\"true\"/>\n      <param name=\"Odom/ResetCountdown\"    type=\"string\" value=\"1\"/>\n      <param name=\"Vis/FeatureType\"        type=\"string\" value=\"6\"/> \n      <param name=\"OdomF2M/MaxSize\"        type=\"string\" value=\"1000\"/>\n    </node>\n\n    <!-- SLAM -->\n    <node name=\"rtabmap\" pkg=\"rtabmap_ros\" type=\"rtabmap\" output=\"screen\" args=\"$(arg rtabmap_args)\">\n      <param name=\"frame_id\"        type=\"string\" value=\"$(arg rgb_frame_id)\"/>\n\n      <remap from=\"rgb/image\"       to=\"$(arg rgb_topic)\"/>\n      <remap from=\"depth/image\"     to=\"$(arg depth_topic)\"/>\n      <remap from=\"rgb/camera_info\" to=\"$(arg rgb_camera_info_topic)\"/>\n      <remap from=\"odom\"            to=\"/odometry/filtered\"/>\n\n      <param name=\"Kp/DetectorStrategy\"    type=\"string\" value=\"6\"/> <!-- use ..."], "url": "https://answers.ros.org/question/284234/sensor-fusion-frame-offset-for-robot_location-zed-and-imu/"},
{"title": "tango ros streamer has java runtime exception", "time": "2018-01-06 17:44:04 -0600", "post_content": [" ", " ", " ", " ", " I installed tango_ros_streamer from the play store, following the instructions here:\n ", "The app loaded and connected to the roscore on my main computer and the ROS indicator turned green.  However, the tango indicator is yellow and I am not seeing any data other than in the /android/IMU topic, which is publishing normally.  The log indicates a Java runtime exception after \"ROS CONNECTION latch released!\"", "Here are the topics that exist:", "/android/imu", "/rosout ", "/rosout_agg", "/tango/status", "Here are the nodes that exist:", "/android", "/parameter_node", "/rosout", "/tango_service_client_node", "Here is the tango log file (192.168.1.62 is the correct address for the tango):"], "answer": [" ", " ", " ", " ", " The original Tango development kit will not work with the latest version of Tango ROS streamer.  However, it does work with an earlier version, specifically this one:\n ", "Also, be sure to point the camera of the device to a \"feature rich environment\" (e.g. pointing at your surroundings, not lying on a table), since the app needs a first valid pose.", "Also, in this version of the app, to receive data on topics, you should enable each type of data you want from the navigation drawer (press the menu icon on the right corner).", " See this link for the explanation:\n ", "To add to the above, it looks like the Tango dev kit (yellowstone tablet) supports streamer releases up to and including 1.3.0.", " So I'd recommend tablet users go with 1.3.0, as it has a few nice features over the earlier versions. \n "], "question_code": ["> ed: Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</parameter_node,\n> http://192.168.1.62:33283/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/Registrar( 4394): Response<Success,\n> Success, []> I/Registrar( 4394):\n> Response<Success, Success, []>\n> I/DefaultPublisher( 4394): Publisher\n> registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</tango_service_client_node,\n> http://192.168.1.62:38113/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/DefaultPublisher( 4394): Publisher\n> registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</android,\n> http://192.168.1.62:56484/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/Registrar( 4394): Registering\n> subscriber:\n> Subscriber<Topic<TopicIdentifier</tango/status>,\n> TopicDescription<std_msgs/Int8,\n> 27ffa0c9c4b8fb8492252bcad9e5c57b>>>\n> I/Registrar( 4394): Registering\n> publisher:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</android,\n> http://192.168.1.62:56484/>,\n> TopicIdentifier</android/imu>>,\n> Topic<TopicIdentifier</android/imu>,\n> TopicDescription<sensor_msgs/Imu,\n> 6a62c6daae103f4ff57a132d6f95cec2>>>>\n> I/Registrar( 4394): Response<Success,\n> Success, []> I/Registrar( 4394):\n> Response<Success, Success, []>\n> I/DefaultPublisher( 4394): Publisher\n> registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</android,\n> http://192.168.1.62:56484/>,\n> TopicIdentifier</android/imu>>,\n> Topic<TopicIdentifier</android/imu>,\n> TopicDescription<sensor_msgs/Imu,\n> 6a62c6daae103f4ff57a132d6f95cec2>>>>\n> I/RunningActivity( 5105):\n> initAndStartRosJavaNode\n> I/RunningActivity( 5105): Waiting for\n> ROS CONNECTION latch release...\n> I/RunningActivity( 5105): ROS\n> CONNECTION latch released!\n> E/RunningActivity( 5105): Uncaught\n> exception of type class\n> java.lang.RuntimeException\n> I/Registrar( 5105):\n> MasterXmlRpcEndpoint URI:\n> http://192.168.1.62:11311/\n> I/Registrar( 5105):\n> MasterXmlRpcEndpoint URI:\n> http://192.168.1.62:11311/\n> I/Registrar( 5105):\n> MasterXmlRpcEndpoint URI:\n> http://192.168.1.62:11311/\n> I/Registrar( 5105): Registering\n> publisher:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</android,\n> http://192.168.1.62:57114/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/Registrar( 5105): Registering\n> publisher:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</tango_service_client_node,\n> http://192.168.1.62:55304/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/Registrar( 5105): Registering\n> publisher:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</parameter_node,\n> http://192.168.1.62:51005/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/Registrar( 5105): Response<Success,\n> Success, []> I/Registrar( 5105):\n> Response<Success, Success, []>\n> I/Registrar( 5105): Response<Success,\n> Success, []> I/DefaultPublisher(\n> 5105): Publisher registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</parameter_node,\n> http://192.168.1.62:51005/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/DefaultPublisher( 5105): Publisher\n> registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</android,\n> http://192.168.1.62:57114/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout>,\n> TopicDescription<rosgraph_msgs/Log,\n> acffd30cd6b6de30f120938c17c593fb>>>>\n> I/DefaultPublisher( 5105): Publisher\n> registered:\n> Publisher<PublisherDefinition<PublisherIdentifier<NodeIdentifier</tango_service_client_node,\n> http://192.168.1.62:55304/>,\n> TopicIdentifier</rosout>>,\n> Topic<TopicIdentifier</rosout ..."], "url": "https://answers.ros.org/question/279027/tango-ros-streamer-has-java-runtime-exception/"},
{"title": "arv_camera_new() doesn't open the camera when called within a nodelet", "time": "2018-02-19 22:12:19 -0600", "post_content": [" ", " ", " ", " ", "Platform Details:", "Ubuntu 14.04 LTS\nLinux Kernel 4.4.0-112-generic", "ROS Version: Indigo", "ROS Package: ", " from here  - ", " ", "Aravis LIbrary -  ", " from here - ", " . I have also tried version 0.5.11 with same results.", "Cameras used (Same behavior with all cameras and all are GigE cameras):", "Each camera is tested separately and is  powered through PoE (Power over Ethernet) and the Ethernet packet size has been set to 9000 MTU. ", "Command to run after compiling the package: ", "The node then detects the camera and can print the device ID and also can access Vendor name and other camera specific parameters with Aravis API's. However, the call to ", " in ", " returns NULL or doesn't return at all (hangs). ", "After trying different debugging methods, I narrowed down the issue to Nodelet manager in ", ".\nIn the modified ", " below, the ", " function can detect the camera and return a camera object. ", "As you can see, I have commented out the Nodelet Manager and directly calling the Aravis APIs to access the camera. I also wrote a separate non-ROS C++ program to use Aravis API's and can access the camera without any issues. ", "Further more, the test scripts provided within aravis library (Ex: ", ") work fine and can connect and stream data from the cameras.", " ", "I have reproduced the issue with two machines with same platform and software versions mentioned above. Note that the code is directly taken from the ", " github repo mentioned above and modifications are only done to ", " as shown above while debugging.", "What might be going wrong? Anyone experienced similar issues with this package? Any help in debugging would be greatly appreciated. ", "After digging deeper into Aravis library(0.4.1) and glib (v2.0), I found out that the function ", " behaves differently for a node and nodelet implementation. Below is the function call to ", " in ", "For the filename string - ", ", the ", " after ...", "I was going to suggest making an issue on the main repo but it looks like that one isn't getting maintained, and there are many many forks with different variations- some of those may or may not have your issue.  Maybe you can get in contact with one of the fork authors that has made recent changes.", "I have reached out to the fork author. Waiting for their response."], "answer": [" ", " ", "Here is a more up to date camera_aravis pkg :", "It works fine for us in nodelet.", "Thanks. Even with the updated camera_aravis pkg, the issues exists since the problem is with glib as I have mentioned in the EDIT part of the question details.", "Do you know why the \"regex_split.c\" test script was created in your package repo? Did you face a similar issue?", "Yes I did this test because I also had this problem.\nBut we use it now without problem on Ubuntu 16.04.", "So, did you have a workaround for this issue in Ubuntu 14.04? What was your solution?", "I did not find any solution for 14.04...", " ", " ", "Have you tried using the tools provided with the underlying ", " library? It has been awhile but from what I remember the library provides some basic camera viewer that I initially used to test to make sure the underlying library was working. Looking at their ", " it seems like you could use the arv-viewer binary to test.", "Also I apologize for the state of camera_aravis, in my fork I tried to combine the features all of the useful camera_aravis forks but these has not been well tested. All of the cameras used for testing were from The Image Source.", "Thanks for the response. Yes, as mentioned in the question, the sample programs provided in the aravis library work fine. I am planning to use the nodelet architecture and that's what is causing the issues. Do I need to take care of anything in the nodelet?", "I see it now. Sorry for the terrible knee jerk response on this. Lemme look back at how I was using this. I will try to find something that might be useful to ya.", "I saw you tested with aravis 0.4 and 0.5.11. When testing 0.5.11 did you ensure 0.4 was completely removed? My hazy memory says that I added some changes that were specific to newer versions of aravis and I had to ensure aravis 0.4 was removed from my system before camera_aravis would link in 0.5.11", "I first tested with 0.5.11 and it didn't work. So, I switched to 0.4 and I have been using that now for most of my debugging. I did run uninstall script for 0.5.11 to remove it before using 0.4. Is there a way to ensure only one version is present in my system?", "find /usr -name '", "' will show all of the libraries and other things installed by aravis", "Got this :\n/usr/local/share/doc/aravis", "Actually that folder is empty. Nothing related to aravis is in /usr", "Hmm I guess it could be installed somewhere else. Maybe check what the install paths are set to when the configure script is run?"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "IDS GV-527xFA-C", "Allied Vision Mako G-319C", "Allied Vision GT1930C"], "question_code": ["camera_aravis", "aravis-0.4.1", "rosrun camera_aravis camera_node", "arv_camera_new()", "arv_camera_new()", "#include <ros/ros.h>\n#include <nodelet/loader.h>\n#include <camera_aravis/camera_nodelet.h>\n\nint main(int argc, char** argv)\n{\n  ros::init(argc, argv, \"camera_node\");\n  //nodelet::Loader manager(true);\n  //nodelet::M_string remappings;\n  //nodelet::V_string my_argv;\n  arv_update_device_list();\n  printf(\"Devices: %d\\n\", arv_get_n_devices());\n  ArvCamera *pCamera = arv_camera_new(NULL);\n  if(pCamera){\n    printf(\"Device Opened: %s\\n\",arv_camera_get_vendor_name(pCamera));\n  }\n  else{\n     printf(\"Device couldn't be opened\\n\");\n  }\n  //manager.load(ros::this_node::getName(), \"camera_aravis/CameraNodelet\", remappings, my_argv);\n  ros::spin();\n}\n", "arv_camera_new()", "arv_open_device()", "arv_get_n_devices()", "arv_get_device_id()", "camera_aravis", "g_regex_split", "g_regex_split", "arvgvdevice.c", "tokens = g_regex_split (arv_gv_device_get_url_regex (), filename, 0);\n", "local:GV-527xFA-C_1_3_120.zip;70000000;3fd0", "tokens"], "url": "https://answers.ros.org/question/283187/arv_camera_new-doesnt-open-the-camera-when-called-within-a-nodelet/"},
{"title": "Initialisation of Dynamixel motors in Multi-turn mode(kinetic)", "time": "2018-02-23 00:41:18 -0600", "post_content": [" ", " ", "Hello All,", "We are using Dynamixel XM-430-R350 motors for our robot. I am able to run the motor in multi-turn mode by setting value of register 11 to 4.\nI need the range of motor to be from 0 to 11430 (digital values in terms  of 4095 per 360deg).", "Currently I am defining these minimum and maximum values in the yaml file used for the motors.\nBut when the motors are powered up, they read  the encoder and set the current position from 0 to 4095. So I have to manually move the motors to the lowest rotation (i.e. in range of 0 to 4095).", "Is there any way, by which the motor initialisation is possible, without the need of this manual movement.", "I am not very sure, how much I was able to convey my problem. Please ask for more clarification."], "answer": [], "url": "https://answers.ros.org/question/283482/initialisation-of-dynamixel-motors-in-multi-turn-modekinetic/"},
{"title": "How to invert coordinate frames of a Transform", "time": "2018-02-27 04:01:41 -0600", "post_content": [" ", " ", " ", " ", "I have a fixed transform coming from the ", " pkg, that gives me the markers pose into the camera_link. I want to get the inverse of it and save the exact data but reversed. The problem is that being the marker frame reversed, the inverse gives me wrong result. ", "How to have x and y markers coordinates to be consistent with the camera_link ones? What is the easiest way to achieve this?\nI want to have x pointing forward (red) and y on the left (green). ", "Thanks for anyone helping me out!", "Cheers,\nSimone.", "Since you already have a transform from your markers to camera_link, have you tried to apply a rotation with respect to camera_link in your marker's frame?", "Hi ", " thanks for the comment! Not yet, I do not know the way to code it, I am new to ROS and tf, How would you to that? Can you append here a simple code snippet or code steps I have to take? \nI appreciate your help!"], "answer": [" ", " ", "This is a ", "\nYou want to do something like", "Follow the tutorial to get the marker's positions."], "answer_code": ["try{\n      listener.lookupTransform(\"camera_link\", \"ar_marker\",ros::Time(0), transform);\n    }\n"], "url": "https://answers.ros.org/question/283860/how-to-invert-coordinate-frames-of-a-transform/"},
{"title": "Indoor navigation using odom & beacons - how to fuse it together?", "time": "2018-02-05 20:06:35 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "We're working on a research project where we have an indoor drone and a system that uses UHB (40Hz) to track its position. ", "We're running EKF2 on the drone itself to retrieve odometry data. We get a nav_msg/Odometry from the drone to a base station running ROS master. Due to our hardware design, we can't retrieve raw sensor data but only the final /odom is published.", "We were able to command the drone to navigate using only /odom (move forward by x m at y m/s). But it drifts away before reaching its goal. We're not using move_base but just a simple combination of TF (odom->base_link) & cmd_vel to navigate.( Basically from my understanding, this is what the drone believes it is and should be).", "Now the indoor tracking system is accurate up to 2cm (but still jumps a little, we are working on it), it publishes a custom message to topic /beacon/pos with x, y, z and a timestamp in milliseconds. ", "We wrote a TF publisher which publishes this xyz as nav_msgs::Odometry to /map and has a child_frame_id as odom. ", "We can see both the odom and the indoor system. But note the odom is different from the one reported by our indoor system. (even at start).", "We also have systems like MarvelMind and Pozyx to test our system's accuracy.", "Questions:", "Is this sufficient? ", "I'm not sure if I am making it very clear, but I tried. Hope someone can help us out.", "Thank you. Please let me know if there is something else I should share.", "We were able to get the map->odom->base_link and it looks like this (green is odom from drone and red is from our system ) We can ignore the direction since we only have x,y,z values from our system, but it feels its mirrored or not aligned. How do we fix this? I am assuming that the green ones should align with red ones (not the direction but x y & z)", "And obviously the EKF jumps - \n", "What I did not understood is: Why do you bother with the map frame if you have a static transform from map to odom? Wouldn't it be better to configure everything to use only odom frame?", "I am not sure if that is a static transform or not. Right now I just get the x,y,z position from the UHB sensor and create a transform from map->odom. I am not sure of this part.", " Hi ", " did you manage to solve your issues? I want to do something similar, see my query  ", " . If you have any suggestions, please let us know! "], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", "How do we create a tf link between map & odom, is this a static transform that we can create? (Simple as moving the start pose of odom to match with map?) For now, we just published that data via a publisher like this - ", "How do we fuse this data together so that we can give a goal in the rviz ?  robot_localization looks promising, just launch one EKF with input /odom (Drone) & /odom from the indoor system? Will this be sufficient to navigate on a path (like a square or circle)?"], "question_code": ["broadcaster.sendTransform(tf::StampedTransform(tf::Transform(tf::Quaternion(0,0,0,1), tf::Vector3(x,y,z)),\nros::Time::now(), \"map\", \"odom\"));"], "url": "https://answers.ros.org/question/281854/indoor-navigation-using-odom-beacons-how-to-fuse-it-together/"},
{"title": "I am trying to mark locations on a map in an automated manner!", "time": "2018-02-12 04:15:29 -0600", "post_content": [" ", " ", " ", " ", "The above is a link to a very powerful Image classifier using Deep learning . I have managed to perform ", " . ", "I need to know how i can automate the process of ", " . ", " ", "The idea is to use something like this in, say , supermarkets . A robot running Gmapping and a good robust image classifier like darknet can keep updating the location of different items .", "So essentially i am seeking for a python script which can mark name tags on the current position/ anypositon on the map* **"], "answer": [], "url": "https://answers.ros.org/question/282456/i-am-trying-to-mark-locations-on-a-map-in-an-automated-manner/"},
{"title": "turtle_tf_message_filter msgcallback function is never called", "time": "2018-04-21 22:11:03 -0600", "post_content": [" ", " ", "Hi, I'm a green hand.And learning tf, but I meet a trouble. Please help me!", "I am I'm following from this page  ", " ,but meet the msgcallback never called. Here is my code:", "First,launch a launch file when I finished in ", " ,", ",  ", "Then ,run the file , it call me that:", "And I am sure msgCallback function is never called! But I don't know how to solve this trouble.\nPlease help me !"], "answer": [], "question_code": ["#include \"ros/ros.h\"\n#include \"tf/transform_listener.h\"\n#include \"tf/message_filter.h\"\n#include \"message_filters/subscriber.h\"\n\nclass PoseDrawer\n{\n    public: \n        PoseDrawer() : tf_() , target_frame_(\"turtle1\")\n        {   \n             ROS_INFO(\"PoseDrawer construct starting!\");\n\n             point_sub_.subscribe(nh_, \"turtle_point_stamped\", 10);\n\n             tf_filter_ = new tf::MessageFilter<geometry_msgs::PointStamped>(point_sub_, tf_, target_frame_, 10);\n\n             try\n             {\n                   ROS_INFO(\"try starting!\");\n\n                   //The wrong, msgCallback is never called!!!! And others ware OK!\n                   tf_filter_->registerCallback( boost::bind(&PoseDrawer::msgCallback, this, _1));\n\n                   ROS_INFO(\"try ending!\");\n             }\n             catch(tf::TransformException &ex)\n             {\n                   ROS_ERROR(\"%s\",ex.what());\n             }\n\n            ROS_INFO(\"PoseDrawer construct end\");\n\n        };\n\n        virtual ~PoseDrawer()\n        {\n              ROS_INFO(\"PoseDrawer destory starting!\");\n        };\n\n    private:\n        message_filters::Subscriber<geometry_msgs::PointStamped> point_sub_;\n\n        tf::TransformListener tf_;\n\n        tf::MessageFilter<geometry_msgs::PointStamped> * tf_filter_;\n\n        ros::NodeHandle nh_;\n\n        std::string target_frame_;\n\n        //Callback to register with tf::MessageFilter to\n        // be called when transforms are available\n        void msgCallback(const boost::shared_ptr<const geometry_msgs::PointStamped>& point_ptr)\n        {\n              ROS_INFO(\"msgCallback is listening!\");\n\n              geometry_msgs::PointStamped point_out;\n\n              try\n              {   \n                    tf_.transformPoint(target_frame_, *point_ptr, point_out);\n\n                    printf(\"point of turtle 3 in frame of turtle 1 Position(x:%f y:%f z:%f)\\n\",\n                               point_out.point.x,\n                               point_out.point.y,\n                               point_out.point.z);\n               }\n               catch (tf::TransformException &ex)\n               {\n                   printf(\"Failure %s\\n\", ex.what());\n               }\n          };\n\n          int main(int argc, char ** argv)\n          {\n            ros::init(argc, argv, \"pose_drawer\"); //Init ROS\n            PoseDrawer pd; //Construct class\n            ros::spin(); // Run until interupted \n          };\n};\n", " $ roslaunch learning_tf start_demo.launch\n", "$ rosrun learning_tf turtle_tf_message_filte\n\n[ INFO] [1522254522.667447695]: PoseDrawer construct starting!\n[ INFO] [1522254522.669954462]: try starting!\n[ INFO] [1522254522.669980229]: try ending!\n[ INFO] [1522254522.669992621]: PoseDrawer construct end\n"], "url": "https://answers.ros.org/question/289302/turtle_tf_message_filter-msgcallback-function-is-never-called/"},
{"title": "Problem with Logitech C270 webcam and Usb_cam", "time": "2018-04-11 10:54:34 -0600", "post_content": [" ", " ", "Hi everyone, I'm new in ROS and I'm triying to use \"usb_cam\" for recieve images from a Logitech C270 webcam, but I have a problem with the visualizer \"image_view\". ", "I can connect with the webcam using \"rosrun usb_cam usb_cam_node\" but when I want to see the visualizer using \"rosrun image_view image_view image:=/usb_cam/image_raw\", I only see a green image from the webcam. Also, I tried to visualizing the webcam images with rqt_image_view and I can see the same.", "With others programs or apps in Ubuntu the image webcam it's succesfully, but in ROS I can't use the webcam without see a green image.", "I've read in this forum and I've tried a lot of options for resolve my problem and I couldn't do it. For this motive, I write this post and I wonder... What's my problem?"], "answer": [" ", " ", "The default parameters for usb_cam are unlikely to work, you'll need to discover them and then have a rosrun like:", "You can use guvcview to determine working resolutions and formats, or a command line like", " I've written this up here (and a lot of other related ros webcam tips):  ", "Hi Lucasw, thanks for your response. I'm going to try this and find the parameters of my webcam to repeat the whole process.", "Hi Malatesta, did you solve the problem? tell me please becouse I'll buy this camera and use it with ros.", "I couldn't solve the problem and I used another camera, but it's possible my camera didn't work correctly. You can try it with your new logitech."], "answer_code": ["rosrun usb_cam usb_cam_node _image_width:=1280 _image_height:=720 _framerate:=15 _video_device:=/dev/video0 _pixel_format:=mjpeg\n", "v4l2-ctl --device=0 --list-formats-ext\n"], "url": "https://answers.ros.org/question/288327/problem-with-logitech-c270-webcam-and-usb_cam/"},
{"title": "nmea_navsat_driver /fix topic hangs indefinitely", "time": "2018-04-30 13:02:12 -0600", "post_content": [" ", " ", "Hey everyone!", "I'm running into an issue that I've previously resolved and have seen posted in a few different places, so apologies if it's a duplicate. This weekend, I started getting this problem again where I run the nmea navsat driver ROS package to read in GPS strings via USB, and can see the serial messages via \"$ cat /dev/<device>\", but when I try to echo the \"/fix\" topic it just hangs.", " Have the robot read GPS data via USB from a GPS unit.", "With the robot powered on, the ROS core is started with the following env vars using the robot's IP address:", "GPS device is plugged into the robot's USB port, I see the device shows up in /dev:", " ", "Listed devices: ", "I'm able to display the serial data coming into the robot:", "Example printout:", "Next, I try running the nmea navsat driver ROS package and match the GPS settings for port and baud rate. It seems like these settings being wrong is a classic way for this not to work, but given step 3 above, it seems like that isn't the issue here.", "NOTE: I get this warning when I run the package, but I think it's only temporary at startup. I've had this same warning a \nweek ago when everything was working. The warning:"], "answer": [" ", " ", "For my particular situation, this doesn't seem like a ROS issue and ended up being an issue on the Emlid Reach RS GPS unit side.", "As I mentioned in the OP, the problem described in this post didn't happen until after updating the GPS unit's control/monitoring software, ReachView. Given that I was able to verify the serial data (i.e., GPS strings) coming into the robot, I really didn't expect the GPS software update to be a factor in the issue at all. The only reason I was suspicious of the upgrade causing any issues was that it was the only thing changed between it working and not working.", "When I downgraded my ReachView app version from 2.11 back to 2.10, and ran the same setup described above, I get the /fix and /vel topics echoing again.", "I'd like to attempt to recreate this exact same behavior before fully committing to the idea that the ReachView upgrade caused an odd bug with ROS, the NMEA driver, or something along the chain.", "The command ", " may help you troubleshoot. I used it when I had a problem with a ROS node and serial port configuration.  I had a serial-to-USB adapter, so I used ", " to see the working and non-working configurations and find the difference."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "OS - Ubuntu 14.04", "More OS info - Linux CPR-J100-0057 3.16.0-77-generic #99~14.04.1-Ubuntu SMP Tue Jun 28 19:17:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux", "ROS - Indigo 1.11.20", "GPS Package - nmea_navsat_driver 0.4.2", "GPS unit - Emlid Reach RS, ReachView app 2.11", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["export ROS_MASTER_URI=http://192.168.131.11:11311", "export ROS_IP=192.168.131.11", "ls /dev/ttyA*", "/dev/ttyACM0  /dev/ttyACM1", "cat /dev/ttyACM1", "$GPGSA,A,3,07,08,09,11,23,48,51,,,,,,3.1,1.7,2.6*3D\n$GLGSA,A,3,67,68,77,78,88,,,,,,,,15.2,4.7,14.4*2A\n$GPGSV,2,1,07,07,44,322,48,08,79,128,45,09,55,248,45,11,27,170,37,1*65\n$GPGSV,2,2,07,23,39,190,42,48,26,246,37,51,45,220,43,,,,,1*58\n$GLGSV,2,1,05,67,22,239,41,68,22,296,44,77,49,019,38,78,27,317,38,1*7B\n$GLGSV,2,2,05,88,36,166,37,,,,,,,,,,,,,1*4D\n$GNGST,173725.20,1.220,,,,4.514,4.529,10.741*5C\n$GNVTG,,T,,M,0.09,N,0.17,K,A*32\n$GNRMC,173725.40,A,3128.5281612,N,08331.7091448,W,0.04,,300418,,,A*4A\n$GNGGA,173725.40,3128.5281612,N,08331.7091448,W,1,12,0.0,108.391,M,-28.500,M,0.0,*61", "rosrun nmea_navsat_driver nmea_serial_driver _port:=/dev/ttyACM1 _baud:=4800", "[WARN] [WallTime: 1525110076.762267] Received a sentence with an invalid checksum. Sentence was: \n'$GPGSV,2,1,08,07,45,323,43,08,80,117,46 ..."], "answer_code": ["stty -F /dev/ttyACM1", "stty -F /dev/ttyUSB0"], "url": "https://answers.ros.org/question/290071/nmea_navsat_driver-fix-topic-hangs-indefinitely/"},
{"title": "Does an finished odometry node exists?", "time": "2018-06-13 04:29:51 -0600", "post_content": [" ", " ", "Hello everybody.", "I'm new to ROS. I finished some of the beginner tutorials and than started to work on my robot platform from the university. Its a pretty easy setup with two powered wheels and one passive wheel for stabilization.", "I was surprised that some of the nodes existed already. Like the joy_node for publishing all the data from the wireless joystick i have. And also the SICK-Node to read out the data from the laser scanner.", "After writing my own node for the elmo controllers i thought about starting with the odometry. But well, to be honest, each time i look at the code of someone knowing what he/she is doing, my own code looks like building something with Lego bricks instead of using the 3D printer. That's also the problem with the code spinets i found so far. I always couldn't understand parts of them.", "So, the question is, is there a finished node for this pretty standard kinematic? Just like the node for the joystick, which publishes the data on a topic. Here it would be publishing the estimated position just by feeding in the geometric dimensions (wheel diameter, wheel distance, counts per turn) and the encoder values. I'm pretty sure i'm not the first one needing something like this. But so far i couldn't find something finished as a standalone node.", "Thanks for the time of everybody reading till here :D", "Best regards,\nLukas", "Using: Linux 64, ROS Indigo", "You might want to take a look at ", " or "], "answer": [" ", " ", "There are many.  But you want one that works and is simple.   Just last night I was browsing code in the ROS Serial package.  There is example code there and one is an odometer node.   This example generates its own fake X, Y and Theta values but you can get your own from wheel encoders.", "The trick:  Given the rotation done in the last time period (Call it delta-Theta) the rest is easy.  But how to calc. rotation?  The key is that rotation is the difference in wheel speed.  Subtract min(left, right) from both left and right and one speed will be zero.   I assume you can calc rotation in the special case where one wheel is locked.   So convert all cases to the spacial case you can solve by assuming one wheel is moving at the difference in speed and the other is locked.  then assume the center of your robot tracks the average wheel speed in X and Y using the average heading over the time period.   ", "Every Delta-T you find deltaTheta, dX and dY and add those to T,X, Y and report those in the message.   ", "I think the trick really is to talk the entire process out loud BEFORE you write even a line of code", " Are you referring to this ", "?", " ", " ", "Hello,", "i ended up doing it by myself. And since maybe someone is interested in (or what to make suggestions). Anything i could/should change ?"], "answer_code": ["#include \"ros/ros.h\"                                    \n#include \"std_msgs/Int64MultiArray.h\"   \n#include \"geometry_msgs/Vector3.h\"      \n#include \"math.h\"               \n\n// -------------------------------------------------------------\n\n// prototype + global Var\nvoid encoderCallback(std_msgs::Int64MultiArray array);\n#define TIME        0\n#define E_L         1\n#define E_R         2\n\nbool new_data;\n\ngeometry_msgs::Vector3 encoder;     // time, left, right\n#define ENC_T               encoder.x\n#define ENC_L               encoder.y\n#define ENC_R               encoder.z\n\ngeometry_msgs::Vector3 encoder_new;// time, left, right\n#define ENC_T_NEW       encoder_new.x\n#define ENC_L_NEW       encoder_new.y\n#define ENC_R_NEW       encoder_new.z\n\ngeometry_msgs::Vector3 position;        // x, y, theta\n#define POS_X               position.x\n#define POS_Y               position.y\n#define POS_THETA           position.z\n\n#define R   75      // [mm] wheel radius\n#define D   530     // [mm] distance between wheels\n#define C   148000  // [] counts per turn, ecperimental value \n\n// -------------------------------------------------------------\n\nint main(int argc, char **argv)\n{\n\n    // init node\n    ros::init(argc, argv, \"OdometryNode\");      \n    ros::NodeHandle n;                  \n    ros::Rate loop_rate(10);    \n\n    // init topics\n    ros::Publisher  Position  = n.advertise<geometry_msgs::Vector3>(\"Odometry_Pos\", 1);\n    ros::Subscriber Encoders  = n.subscribe<std_msgs::Int64MultiArray>(\"MotorDrive_encoder\", 1, encoderCallback);\n\n    // init global vars\n    POS_X       = 0;\n    POS_Y       = 0;\n    POS_THETA   = 0;\n    new_data    = false;\n\n    // wait for first encoder values arrive\n    while (new_data == false && ros::ok()){\n        ros::spinOnce();\n        loop_rate.sleep();}\n    ENC_T = ENC_T_NEW;\n    ENC_L = ENC_L_NEW;\n    ENC_R = ENC_R_NEW;\n    new_data = false;\n\n    // values independent from encoders\n    float M_PI_2 = 2.0*M_PI;\n    float d_2 = D/2;\n    float r_PI_c = R*2*M_PI/C;          // orientation in: rad (=2*PI) or deg (=360)\n    float r_PI_d_c = R*2*M_PI/(D*C);    // constant for the angle\n\n    while (ros::ok())\n    {\n        // just when needed\n        if(new_data == true)\n        {\n            // get driven distance in counts\n            long e_l = ENC_L_NEW - ENC_L;\n            long e_r = ENC_R_NEW - ENC_R;\n            ROS_INFO(\"e_l %li, e_r %li\", e_l, e_r);\n\n            // save new data as current ones\n            ENC_T = ENC_T_NEW;\n            ENC_L = ENC_L_NEW;\n            ENC_R = ENC_R_NEW;\n\n            // rotation + maybe transation\n            float dx, dy, alpha;\n            if((e_l - e_r) > 10 || (e_l - e_r) < -10)\n            {\n                // radius + angle of the movement   \n                float e_temp = (e_l + e_r) / (e_l - e_r);\n                float r = d_2 * e_temp;\n                alpha = (e_l - e_r) * r_PI_d_c;\n\n                // move in robot frame\n                dx = r * sin(alpha);\n                dy = r * (1-cos(alpha));\n\n            }\n            else    // just translation\n            {\n                dx = e_l * r_PI_c;\n                dy = 0.00;\n                alpha = 0.00;\n            }\n\n            // prepare transformation\n            float temp_cos = cos(POS_THETA);\n            float temp_sin = sin(POS_THETA);\n\n            // transform to global frame (rotation matrix)\n            POS_X       += dx*temp_cos - dy*temp_sin;\n            POS_Y       += dx*temp_sin + dy*temp_cos;\n            POS_THETA    += alpha;\n            POS_THETA   %= M_PI_2;\n\n            // publish new estimate of position\n            Position.publish(position);\n            new_data = false;\n        }\n\n        // tell ROS to repeat\n        ros::spinOnce();\n        loop_rate.sleep();\n    }\n\n    return 0;\n}\n\n// -------------------------------------------------------------\n\nvoid encoderCallback(std_msgs::Int64MultiArray data_new)\n{\n    // just save the data, rest is in main loop\n    ENC_T_NEW = data_new.data[TIME];\n    ENC_L_NEW = data_new.data[E_L];\n    ENC_R_NEW = data_new.data[E_R];\n\n    // flag to process new data\n    new_data = true;\n    return;\n}\n"], "url": "https://answers.ros.org/question/294000/does-an-finished-odometry-node-exists/"},
{"title": "Ending nodes/topics from python script", "time": "2018-06-27 11:21:39 -0600", "post_content": [" ", " ", "Hello! To preface, I working with a turtlebot3 running ROS on Ubuntu, and working with the package frontier-exploration. I am designing a Python script that will send the turtlebot to its point of origin when the battery is low, and stop when it reaches that point. It operates by specifying a very small perimeter around the bot's point of origin. Now the problem: I want the bot to completely stop moving when it reaches its goal, but I cannot seem to publish a Twist to cmd_vel that will stop the bot. Is there a way that I can shutdown frontier-exploration from my Python script? Thank you!", "Can you please update your question with what you've attempted so far (i.e., your code)?", "You should post a new question about your inability to stop the robot using commands. Shutting down the node is a heavy-handed way to stop frontier-exploration and there should be a better way. (If there isn't, your question might trigger someone to produce one.)"], "answer": [" ", " ", "You can manage nodes from a Python program using the ", ". Keep in mind that this API is listed as \"very unstable\", although it has been in that state since forever and to be honest I don't think it will change that much from now on.", "The ", " might also interest you. It replaces ", ", and as well as several other useful features, it provides a ROS service that you can use to start and stop nodes."], "answer_code": ["rosmon", "roslaunch"], "url": "https://answers.ros.org/question/295503/ending-nodestopics-from-python-script/"},
{"title": "Has anyone written a trajectory plotting tool?", "time": "2013-03-21 22:51:07 -0600", "post_content": [" ", " ", " ", " ", " asked on ", ":", "I'd like a tool that plots a series of joint trajectories. In my work, I continuously re-plan motions, meaning I need each trajectory to spline smoothly at a particular time with the previously\ncommanded trajectory. Having a graphical tool to verify that this is happening correctly would be very useful.", "Has anyone already written such a tool?\nOtherwise, I guess I'll start getting familiar with rqt and matplotlib...", " answered on ", ":", "I have a tool based around a library here: ", "It also uses a JointWatcher to record the values that are actually performed. \n", " answered on ", ":", "the rqt GUI has a 2D plotter rqt_plot that can plot numerical data from messages.\nIt is very similar to rxplot, but there is no restriction (as far as I know) on how many variables can be plotted. It's just dependent on you CPU power...\nrqt_plot has three different drawing backends selectable from it's configuration menu:\n- qwt plot (fast, but not using message timestamps and needs python-qwt bindings)\n- pyqtgraph (fast and needs pyqtgraph to be installed which is not in the repos)\n- matplotlib (slow but always available)", " answered on ", ":", "Even without any further information how you want to plot the trajectories I would recommend looking into QGraphicsScene as the backend.\nMay be this is a better fit than using matplotlib.", "Hi ,\nDavid Lu could you explain how to use your program???\n\nthanks"], "answer": [" ", " ", " ", " ", "recently my college wrote the rqt plugin for visualize joint trajectory , try ", " ", " ", " ", " ", "I've used ", " to visualize joint trajectory messages as they are being sent to the controllers. The downside is that there is no proper way to reconstruct the spline. For example, you can visualize the position of the waypoints and see the plotting tool reconstruct a smooth curve, but you can't enforce the actual waypoint velocity and acceleration specified in the message.", "I don't know if there exists an ", " plugin for reconstructing splines from ", " messages, but if not, it would be a very valuable addition.", "A question from my side would be: are you explicitly computing the blending patches for trajectory replacement, or are you letting the controller (eg. ", ") do that for you?.", "rqt_plot does not know anything about trajectories, it just plots numeric data over time (like rxplot, but with a choice of multiple plotting backends). Feel free to add a plugin for trajectory plotting, which can reuse the plotting backends.", " ", " ", "We had a student in our lab who implement a real-time plotting tool that worked pretty well under ROS (but also supports YARP) displaying numerous trajectories at high frequencies. Rxplot used to not deliver in those situations. You can have a look at it here:", "https://code.google.com/p/streamvis/", "I am still using the tool under fuerte. Don't know if it works under groovy out of the box. The user community is pretty small but the tool is still supported. In case you encounter installation problems do not hesitate to send an email. ;)", "I have installed the streamvis but not able to add  ROS topic to view data. How can i add ROS topic in streamvis."], "answer_code": ["apt-get install ros-kinetic-rqt-joint-trajectory-plot"], "url": "https://answers.ros.org/question/58907/has-anyone-written-a-trajectory-plotting-tool/"},
{"title": "Error installing ROS on RPI 3B+", "time": "2018-06-11 05:15:48 -0600", "post_content": [" ", " ", " ", " ", "I am trying to folow ", " guide to install ROS on a Raspberry Pi 3 B+ with Debian-Stretch and at 4.1 point I need to folow ", " where I managed to reach the 3.2 Rebuild your workspace point. This is the point where I get this error:", "Before this error I got another 2-3 errors that I managed to resolve but this is beyond my power.", "Running ", "  I manage toinstall 57 package from 121 needed."], "answer": [" ", " ", "The obvious error is that you are missing a file named \"class_loaderConfig.cmake\" or \"class_loader-config.cmake\". According to ", " you can find the missing file in the package libclass-loader-dev.", "Note that you will probably need to install many more of these dependencies.", " ", " ", "\"Could not find a package configuration file\"", "Maybe check it."], "question_code": ["==> Processing catkin package: 'pluginlib'\n==> Building with env: '/home/pi/ros_catkin_ws/devel_isolated/rosconsole/env.sh'\n==> cmake /home/pi/ros_catkin_ws/src/pluginlib -DCATKIN_DEVEL_PREFIX=/home/pi/ros_catkin_ws/devel_isolated/pluginlib -DCMAKE_INSTALL_PREFIX=/opt/ros/kinetic -G Unix Makefiles in '/home/pi/ros_catkin_ws/build_isolated/pluginlib'\n-- Using CATKIN_DEVEL_PREFIX: /home/pi/ros_catkin_ws/devel_isolated/pluginlib\n-- Using CMAKE_PREFIX_PATH: /home/pi/ros_catkin_ws/devel_isolated/rosconsole;/opt/ros/kinetic\n-- This workspace overlays: /home/pi/ros_catkin_ws/devel_isolated/rosconsole;/opt/ros/kinetic\n-- Using PYTHON_EXECUTABLE: /usr/bin/python\n-- Using Debian Python package layout\n-- Using empy: /usr/bin/empy\n-- Using CATKIN_ENABLE_TESTING: ON\n-- Call enable_testing()\n-- Using CATKIN_TEST_RESULTS_DIR: /home/pi/ros_catkin_ws/build_isolated/pluginlib/test_results\n-- Found gmock sources under '/usr/src/googletest': gmock will be built\n-- Found gtest sources under '/usr/src/googletest': gtests will be built\n-- Using Python nosetests: /usr/bin/nosetests-2.7\n-- catkin 0.7.11\nCMake Warning at /opt/ros/kinetic/share/catkin/cmake/catkinConfig.cmake:76 (find_package):\n  Could not find a package configuration file provided by \"class_loader\" with\n  any of the following names:\n\n    class_loaderConfig.cmake\n    class_loader-config.cmake\n\n  Add the installation prefix of \"class_loader\" to CMAKE_PREFIX_PATH or set\n  \"class_loader_DIR\" to a directory containing one of the above files.  If\n  \"class_loader\" provides a separate development package or SDK, be sure it\n  has been installed.\nCall Stack (most recent call first):\n  CMakeLists.txt:4 (find_package)\n\n\n-- Could not find the required component 'class_loader'. The following CMake error indicates that you either need to install the package with the same name or change your environment so that it can be found.\nCMake Error at /opt/ros/kinetic/share/catkin/cmake/catkinConfig.cmake:83 (find_package):\n  Could not find a package configuration file provided by \"class_loader\" with\n  any of the following names:\n\n    class_loaderConfig.cmake\n    class_loader-config.cmake\n\n  Add the installation prefix of \"class_loader\" to CMAKE_PREFIX_PATH or set\n  \"class_loader_DIR\" to a directory containing one of the above files.  If\n  \"class_loader\" provides a separate development package or SDK, be sure it\n  has been installed.\nCall Stack (most recent call first):\n  CMakeLists.txt:4 (find_package)\n\n\n-- Configuring incomplete, errors occurred!\nSee also \"/home/pi/ros_catkin_ws/build_isolated/pluginlib/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/pi/ros_catkin_ws/build_isolated/pluginlib/CMakeFiles/CMakeError.log\".\n<== Failed to process package 'pluginlib': \n  Command '['/home/pi/ros_catkin_ws/devel_isolated/rosconsole/env.sh', 'cmake', '/home/pi/ros_catkin_ws/src/pluginlib', '-DCATKIN_DEVEL_PREFIX=/home/pi/ros_catkin_ws/devel_isolated/pluginlib', '-DCMAKE_INSTALL_PREFIX=/opt/ros/kinetic', '-G', 'Unix Makefiles']' returned non-zero exit status 1\n\nReproduce this error by running:\n==> cd /home/pi/ros_catkin_ws/build_isolated/pluginlib && /home/pi/ros_catkin_ws/devel_isolated/rosconsole/env.sh cmake /home/pi/ros_catkin_ws/src/pluginlib -DCATKIN_DEVEL_PREFIX=/home/pi/ros_catkin_ws/devel_isolated/pluginlib -DCMAKE_INSTALL_PREFIX=/opt/ros/kinetic -G 'Unix Makefiles'\n\nCommand failed, exiting.\n", "sudo ./src/catkin/bin/catkin_make_isolated --install-space /opt/ros/kinetic"], "url": "https://answers.ros.org/question/293775/error-installing-ros-on-rpi-3b/"},
{"title": "What is the motor specifications of Clearpath Husky robot?", "time": "2018-05-02 17:41:37 -0600", "post_content": [" ", " ", "Can someone provide insight on the Clearpath Husky robot motors? especially the torque, speed & power.  ", "Probably not what you wanted to hear, but what did Clearpath tell you when you asked them?", "I didn't get any response from them", "Are you an actual customer of them? If so, that would seem strange to me.", "I am not an existing customer, but i am planing to buy one for my projects.", "Have you communicated this to Clearpath? I would assume they'd be willing to provide adequate information to potential customers."], "answer": [], "url": "https://answers.ros.org/question/290282/what-is-the-motor-specifications-of-clearpath-husky-robot/"},
{"title": "robot_localization  non uniform inaccurate odometry", "time": "2018-04-04 02:13:01 -0600", "post_content": [" ", " ", " ", " ", "Im running a simulation in Gazebo combining the odomery data (coming from the differential drive plugin) and the imu data (from GazeboRosImuSensor plugin). Im quite new to ROS and im totally confused with the ENU and NED frames ..", "This is what I did 1)placed the IMU in its neutral position", "2)setup a static transform broadcaster which broadcasts my base_link to odom frame here i made the rotation.", "I have rolled it by 180 and yaw by 90 . this is the code snippet in my launch file", " I tested the IMU and it shows correct accelerations as shown (  ", " . )  ", "This is my paramter config file for the robot_localization package ", "The problem is that the filtered odometry data is moving at 90 degees to original raw odometry data ", "What do you mean by stop for sometime? Can you post some more details as to what happens when it stops?", "I plot the odometry data in rviz, The raw odometry data used to move with the robot in the circle but the filtered one stops for few seconds .", "I solved this issue . turned out i forgot to comment out example imu configs in tthe paramete file\nBut still i have the problem of wrong imu orientation", "when i move the bot forward the filtered data is moving at 90 degrees ", " The red line is raw odometry data. the green is the filtered one."], "answer": [" ", " ", "I feel like this is a duplicate question and I answered something that was pretty much identical recently, but the EKF is combining measurements from two data sources: your IMU and your odometry. You are fusing the absolute yaw value from your IMU, and velocity data from your odometry. So your EKF output poses will have the same \"shape\" as your raw odometry, but will be at the heading provided by your IMU. If your EKF output was identical to your odometry input, you wouldn't need a filter."], "question_code": ["<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"imu_tf_publisher\" args=\"0 0 0 1.57 0 3.14 base_link imu_link 10\"/>\n", "odom0_config: [false,  false,  false,\n               false, false, false,\n               true, true, false,\n               false, false, true,\n               false, false, false]\nimu0_config: [false, false, false,\n              false, false, true,\n              false, false, false,\n              false, false, true,\n              true, true, false]\n\nodom0_queue_size: 2\nimu0_queue_size: 5\n\nodom0_nodelay: false\nimu0_nodelay: false\n\nodom0_differential: false\nimu0_differential: false\n\nodom0_relative: false\nimu0_relative: false\n"], "url": "https://answers.ros.org/question/287438/robot_localization-non-uniform-inaccurate-odometry/"},
{"title": "Playing big files with rosbag", "time": "2013-03-11 00:11:11 -0600", "post_content": [" ", " ", "Hi everybody,", "I am running Ubuntu 12.10, with ROS Groovy. My computer has a i7 processor with 6GB of RAM.", "I am recording in rosbag data from two kinects (and two skeletons). Of course, I din't recorded the \"PointClouds\", but only the raw images.", "However, my files go up to 50 GB for a 12minutes videos...", "My problem is that for such big files, \"rosbag play\" takes forever to start to play the data. I was wondering on the way it works. Indeed, I thought it played a stream of the data, thus, whatever the size of the file, it should not take that much longer to start playing.\nHowever, as I saw in my experiment, it is not the case. Thus I maybe thought that rosbag play is putting a lot in memory, or doing some \"precomputation\" ?", "If someone who knows how rosbag works could enlighten me of his knowledge, it would be great.", "I know this question is very technical, but I would really like to find a way to open my files after recording them, without having to buy a new \"very powerful and expensive\" computer...", "My first thought would be to record each topic in one bag, and then replay them all in the same time... But seems a bit stupid to do that, as I think rosplay would need to \"realign\" in time all the messages.", "Thanks in advance,", "Steph", "I don't know at all about the internals but does it make a difference if you do a ", " on that bag?", "The bag file has no problems... it's just big :-)\nWhat I am doing now is rosbag filter everything, putting every topic in a single bag file... It's long, and a bit stupid in my opinion...."], "answer": [" ", " ", " ", " ", "you can filter out your data, I had some similar problems, here is an example script:", " ", " ", "There is an even older, but more active, related question:", "That refers to a github ticket, which is still open, though:", " seems to perform better."], "question_code": ["rosbag fix"], "answer_code": ["#!/bin/bash\n#filter out the bag files for the raw data\nsource ~/.bashrc\n\ncd /to/your/bag directory/\n\nrosbag filter original.bag filtered.bag 'topic==\"/lms1xx_front/scan\" or topic==\"/lms1xx_front_tilt/sc an\" or topic==\"/lms1xx_back/scan\" or topic==\"/fmData/imu_rx\" or topic==\"/fmInformation/imu\" or topic==\"/kinect2/depth/camera_info\" or topic==\"/kinect2/depth/image\" '\n", "rqt_graph"], "url": "https://answers.ros.org/question/57727/playing-big-files-with-rosbag/"},
{"title": "ros_canopen and Maxon EPOS2. Could not switch controller.", "time": "2018-05-28 03:28:23 -0600", "post_content": [" ", " ", " ", " ", "We are trying to setup ros_canopen with a Maxon's Epos2 controller. Our enviroment is Ubuntu 16.04 with ROS Kinetic, and \nros_canopen version: 0.7.9.", "We followed documentation described in ", ", and mostly these two ", " and ", " .Now we have a basic communication between ros_canopen and Epos2 controller.", "We had some issues when we setup the configuration. Maxon provides a tool for Windows called ", ", which allows to manage the controllers. ", "First, we configured the controller with EPOS Studio to handle canopen protocol, and we exported the configuration to a EDS file for ros_canopen. We lost some time until we figured out that exported EDS file didn't contain the configuration that was using the controller and we overwrited the parameters within ", " section.", "Then, we tried to initialize the controller, but we had an issue with the Interpolated Position Mode. ros_canopen defines such modes input in 60c1sub1 register, but Epos2 controller doesn't. Despited it isn't the best solution, we changed that mode to accept 0x607A (Target Position) in ", ", but we do not use that mode.", "At this point we have another issue, when we power on the controller, by default it is in disabled state (control word 0xxx xx0x) and if we ", " we get \"Transition timeout; Could not enable motor; Transition timeout\". If we Enable the controller using EPOS Studio and call init service, then ros_canopen goes one step forward and makes Homing with it's default homing method (in this case actual position: 35). After that, the controller is set in disabled state.", "Finally we launch our controller with the following configuration:", "We set  PDO as:", "When we load the joint_position_controller we get following error:", "The same happens with joint_velocity_controller.", "Basically we have two questions:", "1) How can we init the driver without manually enabling from EPOS Studio?", "2) How can we proceed to success in loading the controllers?", "Thanks!", "Edit: Mathias thanks for your reply. As you said, I've set ", "in  dcf_overlay, but nothing changes. I can monitor controller's registers with EPOS Studio in 'realtime' though USB. The value of ", "only changes only if ", " is called after enabling the controller. The value ", "is automatically set to 6 (homing method). I assume this is how it should work.", "I've talked with Maxon's support and ...", "Hi ", " !", "How did you configured the EDS file to overwrite the dcf_overlay section. And how did you know that you have the wrong configuration?", "I have the same error but with 60c1sub0. Could you explain how did you change the mode in motor.h?", "Many thanks, \nJorge", "You can manually edit EDS file, or create an overlay as it is explained in ", "I've edited ", "replacing the mode\ntypedef ModeForwardHelper<motorbase::interpolated_position, int32_t,=\"\" 0x607a,=\"\" 0,=\"\" 0=\"\"> InterpolatedPositionMode", "Hi ", "\nI try to replace the typedef as you explain, but it fails when compiling the code:", "the second error:", "Sorry ", ", I just copy/paste the command, and the format of the next was changes\n", "Hi ", ", I try that change before I commented the other errors. I can compile with that modification, but the error when I try to initialize the motors still.", ": please don't post updates as answers. Only use answers if you're answering your own question. For everything else, please edit your original post. Use the ", " button/link for that.", ", how do you know that the motor has activated the PDO?\nI'm monitoring the CAN with wireshark, but nothing happens... the motor receives the command but I don't know if it is correctly or not"], "answer": [" ", " ", " ", " ", "How can we init the driver without manually enabling from EPOS Studio?", "Please try to set a value for ", " (e.g. 1) in dcf_overlay.\nNot all devices switch to \"Operation enabled\" without a mode.", "2) How can we proceed to success in loading the controllers?", "Does Maxon prohibit mode switching to certain motor states?\nYou could try to set the ", " param to 4 (\"Switched on\").", "Update: You might need to set ", " (\"Ready_To_Switch_On\") to shutdown before switching."], "question_code": ["EPOS Studio", "dcf_overlay", "rosservice call /driver/init", "joint_position_controller:\n  type: position_controllers/JointPositionController\n  joint: motor_1_joint\n  required_drive_mode: 1\njoint_velocity_controller:\n  type: velocity_controllers/JointVelocityController\n  joint: motor_1_joint\n  required_drive_mode: 3\n", "can0  181   [2]  00 01        # Controlword\ncan0  281   [2]  40 07        # Statusword\ncan0  381   [1]  06           # Modes of Operation Display\ncan0  481   [4]  FD FF FF FF  # Position Actual Value\n", "Could not switch to mode 1, reason: Mode switch timed out.\n[ERROR] [1527494153.154665304]: motor_1_jointcould not enter mode 1\n[ERROR] [1527494153.155116906]: Could not switch one joint for joint_position_controller, will stop all related joints     and the controller.\n[ERROR] [1527494153.928573312]: Could not switch one joint for joint_position_controller, will stop all related joints and the controller.\n", "\"0x6060\":\"1\"", "0x6060", "rosservicell call .../init", "0x6060", "motor.h", "error: an assignment cannot appear in a constant-expression\n typedef ModeForwardHelper<MotorBase::Interpolated_Position, int32_t,=\"\" 0x607A,=\"\" 0,=\"\" 0=\"\"> InterpolatedPositionMode;\n", "/home/jorge/WS/epos_ws/src/ros_canopen/canopen_402/include/canopen_402/motor.h:209:94: error: wrong number of template arguments (3, should be 5)\n         typedef ModeForwardHelper<MotorBase::Interpolated_Position, int32_t,=\"\" 0x607A,=\"\" 0,=\"\" 0=\"\"> InterpolatedPositionMode;\n", "\ntypedef ModeForwardHelper<MotorBase::Interpolated_Position, int32_t, 0x607A, 0, 0> InterpolatedPositionMode;\n", "edit"], "answer_code": ["0x6060", "switching_state", "switching_state"], "url": "https://answers.ros.org/question/292443/ros_canopen-and-maxon-epos2-could-not-switch-controller/"},
{"title": "Upgrade from TurtleBot3 Burger to Waffle", "time": "2018-07-25 02:48:47 -0600", "post_content": [" ", " ", "Hello all,", "I finally got my Turtlebot3 Burger and I must say it is nice to see robot with LIDAR for its price.", "I'm mostly using it to scan environment with SLAM and navigate it using predefined waypoints. Navigating robot trough simple environments (just walls) works very good but as soon I test it in more realistic environments it starts to give me headache.", "For example, I have stand fan with large round base. As LIDAR is on top of robot it only detects thin tube sticking out of round base. When I give command to robot to pass around stand fan it always hits round base, wheels start to slip and robot looses orientation in space. Looks like there is big difference in data coming from odometry and Lidar and robot is having difficulties to define position in space. And I have to manually define 2D pose of robot in order to help robot find its position. \nTo improve SLAM performances I was thinking to upgrade my Burger by adding Intel RealSense R200 camera. Would it be possible to add this camera and fuse data with LIDAR to improve detection of obstacles?\nAlso, would it require more powerful miniPC? I was thinking to upgrade to Intel Joule 570x but I can see it is discontinued. \nWhat about Intel Up board? Like this one: ", "I'm new into ROS and suggestions from more experienced members will be very helpful. I hope you can give me some tips to improve my robot."], "answer": [], "url": "https://answers.ros.org/question/298531/upgrade-from-turtlebot3-burger-to-waffle/"},
{"title": "HectorSLAM catkin_make crashes Pi - Can I copy workspaces? [closed]", "time": "2018-08-07 02:21:47 -0600", "post_content": [" ", " ", "Hey guys I'm relatively new to ROS so my understanding may be a bit average but here's my situation. ", "I'm trying to build an autonomous car using RPLiDAR and Hector slam with a Raspberry Pi as the onboard processor. I'm running ROS Kinetic and Ubuntu Mate). When I try to build a catkin workspace with the hector SLAM and RPLiDAR packages the Pi crashes at 5%. I've tried the -j1 option with catkin_make and the build process gets to about 90% and crashes again. I assume this is because the Pi doesn't have the processing power to handle this task.", "I have two solutions in mind. I've built the same workspace on my laptop and it works but when I copy it to my Pi and try to run it i get an error saying the file doesn't exist. Is it possible to simply copy a prebuilt catkin workspace to another machine and use it?", "Another solution I'm thinking of is to run the workspace on my laptop and retrieve the USB data from the Pi over SSH to build the maps. Is this possible?", "Hopefully this isn't too much rambling its my first post thanks in advance!", "Please make sure you have enabled swap on your RPi. Without it, compilation of any serious program will always crash.", "re: copying: no, not without doing something called cross-compiling.", "Hey gvdhoorn thanks for your response! I tried using cmake again on the workspace that got up to 90% after rebooting and it worked fine. I didn't see your response before I tried this but will keep it in mind in future!", "Hi TintinQuarantion I have the same problem here, can you share the solution how to fix this problem? thank you"], "answer": [], "url": "https://answers.ros.org/question/299850/hectorslam-catkin_make-crashes-pi-can-i-copy-workspaces/"},
{"title": "DWA planner failed to produce path [closed]", "time": "2018-07-25 23:44:21 -0600", "post_content": [" ", " ", " ", " ", "Hi all:", "When I run mapless move_base, it always complained \"DWA planner failed to produce path\". Is there log somewhere I can check why it failed to produch path? ", "The output is like:", "However, I can see a green curve from the robot to the goal in rviz. ", "Here are the parameters:", "costmap_common.yaml:", "costmap_local.yaml:", "Planner.yaml:", "Move_base.launch:", "There should be log in `~/.ros/log/", "Can you post your param files you're using (local / global costmap settings?). Also the green curve from the robot to the goal in RViz might be the global path.", "ROS log is at ~/.ros/log, but it did not see anything related DWA planner. Where is the local planner log?", "There are session logs. you have to search inthere"], "answer": [" ", " ", "You need to post your parameters. It is really easy to specify parameters that makes it impossible to generate a path or restrict it so much in alot of cases you can't do it. Tuning DWA can be very painful.", "I'd recommend starting with turtlebot default parameters, it's a good starting point for most differential drive robots and then tune from there once you have a starting point. ", "I have edit my original post and added parameters", "Did you try what I suggested starting with the turtlebot params?", "No, I did not since I felt husky was more like my car-like robot than the turtlebot. Did you see a particular parameter that was quite different from turtlebot?"], "question_code": ["[ INFO] [1532580173.243852203]: Got new plan\n[ WARN] [1532580173.245276133]: DWA planner failed to produce path.\n[ INFO] [1532580173.443843662]: Got new plan\n[ WARN] [1532580173.445304680]: DWA planner failed to produce path.\n[ WARN] [1532580173.643893361]: Clearing costmap to unstuck robot (3.000000m).\n[ INFO] [1532580174.043834647]: Got new plan\n[ WARN] [1532580174.045632208]: DWA planner failed to produce path.\n[ WARN] [1532580174.243951804]: Rotate recovery behavior started.\n[ERROR] [1532580174.244415898]: Rotate recovery can't rotate in place because there is a potential collision. Cost: -1.00\n[ INFO] [1532580174.643841480]: Got new plan\n[ WARN] [1532580174.645396002]: DWA planner failed to produce path.\n[ WARN] [1532580174.843851086]: Clearing costmap to unstuck robot (1.840000m).\n[ INFO] [1532580175.243841053]: Got new plan\n[ WARN] [1532580175.245196120]: DWA planner failed to produce path.\n[ WARN] [1532580175.443900326]: Rotate recovery behavior started.\n[ERROR] [1532580175.444206757]: Rotate recovery can't rotate in place because there is a potential collision. Cost: -1.00\n[ INFO] [1532580175.843830584]: Got new plan\n[ WARN] [1532580175.845862800]: DWA planner failed to produce path.\n[ERROR] [1532580176.043937473]: Aborting because a valid control could not be found. Even after executing all recovery behaviors\n", "footprint: [[-0.5, -0.33], [-0.5, 0.33], [0.5, 0.33], [0.5, -0.33]]\nfootprint_padding: 0.01\n\nrobot_base_frame: base_link\nupdate_frequency: 4.0\npublish_frequency: 3.0\ntransform_tolerance: 0.3\n\nresolution: 0.1\n\nobstacle_range: 5.5\nraytrace_range: 6.0\n\n#layer definitions\nstatic:\n    map_topic: /map\n    subscribe_to_updates: true\n\nobstacles_laser:\n    observation_sources: laser\n    laser: {data_type: LaserScan, clearing: true, marking: true, topic: scan, inf_is_valid: true}\n\ninflation:\n    inflation_radius: 0.1\n", "global_frame: odom\nrolling_window: true\n\nplugins:\n  - {name: obstacles_laser,           type: \"costmap_2d::ObstacleLayer\"}\n  - {name: inflation,                 type: \"costmap_2d::InflationLayer\"}\n", "controller_frequency: 5.0\nrecovery_behaviour_enabled: true\n\nNavfnROS:\n  allow_unknown: true\n  default_tolerance: 0.1\n\nTrajectoryPlannerROS:\n  acc_lim_x: 2.5\n  acc_lim_theta:  3.2\n\n  max_vel_x: 1.0\n  min_vel_x: 0.0\n\n  max_vel_theta: 1.0\n  min_vel_theta: -1.0\n  min_in_place_vel_theta: 0.2\n\n  holonomic_robot: false\n  escape_vel: -0.1\n\n  yaw_goal_tolerance: 0.1\n  xy_goal_tolerance: 0.2\n  latch_xy_goal_tolerance: false\n\n  sim_time: 2.0\n  sim_granularity: 0.02\n  angular_sim_granularity: 0.02\n  vx_samples: 6\n  vtheta_samples: 20\n  controller_frequency: 20.0\n\n  meter_scoring: true\n  occdist_scale:  0.1\n  pdist_scale: 0.75\n  gdist_scale: 1.0\n\n  heading_lookahead: 0.325\n  heading_scoring: false\n  heading_scoring_timestep: 0.8\n  dwa: true \n  simple_attractor: false\n  publish_cost_grid_pc: true\n\n  oscillation_reset_dist: 0.25\n  escape_reset_dist: 0.1\n  escape_reset_theta: 0.1\n\nDWAPlannerROS:\n  acc_lim_x: 2.5\n  acc_lim_y: 0\n  acc_lim_th: 3.2\n\n  max_vel_x: 0.5\n  min_vel_x: 0.0\n  max_vel_y: 0\n  min_vel_y: 0\n\n  max_trans_vel: 0.5\n  min_trans_vel: 0.1\n  max_rot_vel: 1.0\n  min_rot_vel: 0.2\n\n  yaw_goal_tolerance: 0.1\n  xy_goal_tolerance: 0.2\n  latch_xy_goal_tolerance: false\n", "<launch>\n  <arg name=\"no_static_map\" default=\"true\"/>\n\n  <arg name=\"base_global_planner\" default=\"navfn/NavfnROS\"/>\n  <arg name=\"base_local_planner\" default=\"dwa_local_planner/DWAPlannerROS\"/>\n  <arg name=\"local_costmap_size\" default=\"3.0\"/>\n\n  <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <param name=\"base_global_planner\" value=\"$(arg base_global_planner)\"/>\n    <param name=\"base_local_planner\" value ..."], "url": "https://answers.ros.org/question/298666/dwa-planner-failed-to-produce-path/"},
{"title": "How to get Jackal clearpath running outdoors?", "time": "2018-07-19 03:54:02 -0600", "post_content": [" ", " ", "I am using a clearpath jackal robot which I am running the nav stack on to do slam_gmapping and amcl localization. Currently I ssh into the jackal over wifi to run the nav stack on the robot, but I am now wanting to do autonomous navigation in an outdoors environment. Does anyone have any idea how I would be able to do autonomous navigation outside where I will not have access to a wifi network? Any help would be much appreciated, I am very new to ROS. Thank you!", "Has anyone tried using conman to host an access point? The tutorial says it is experimental and I may have to flash jackal back to factory state so I am a bit worried about it.", "Could you not take a 12v battery and a small wifi router with you so you have a wifi network with you wherever you go?", "I am using it for a while now, it's pretty handy that you can just turn on jackal and ssh into jackal's network and play around.", "Ahh thats a good idea. Thank you very much! Although now for some reason gmapping and amcl do not seem to be working properly indoors either so I need to sort that out first ahah", "In a pinch you can use your laptop as a wireless hotspot that the Jackal connects to. Also keep in mind that if you use Jackal to create the hotspot, you will not be able to connect to the internet through wifi, not easily at least. So exposing the ethernet ports so they are easy to acces is good.", "Can't you just insert an USB-WLAN-Stick and run hostapd on the jackal?", "Thank you for the suggestions guys! What I've ended up doing is using a wifi router as a repeater to amplify the wifi signal from inside and that has worked."], "answer": [], "url": "https://answers.ros.org/question/297905/how-to-get-jackal-clearpath-running-outdoors/"},
{"title": "px4 mavros wrong local position with vision fusion", "time": "2018-08-01 08:31:20 -0600", "post_content": [" ", " ", "HI. I don't know where to crash my head, hope you can help me (really sorry to bother you):\nI can't understand why the two poses (slam - yellow || mavros local position - turquoise) and relative odoms ( green || red respectively) are so different?\nFrom the px4 side I triple-checked the bit mask on EKF2_AID_MASK and yes, I'm using EKF2"], "answer": [], "url": "https://answers.ros.org/question/299301/px4-mavros-wrong-local-position-with-vision-fusion/"},
{"title": "How to synchronize two different ros programs?", "time": "2018-08-06 23:30:15 -0600", "post_content": [" ", " ", "I'm running two programs. One is image_pipeline to generate point cloud data from stereo images, and another is detection algorithm on 2D images. The main theme of these programs are to identify the object in 2D image and find the corresponding points located in point cloud data i.e., finding the object in three dimensional.", "But here, the image pipeline algorithm is running very fast and detection algorithm is too slow. When both the algorithms are running, the image and point cloud data's are not getting synchronised. Can anyone please help me how to get rid of this kind of problems in ROS.", "power of probability"], "answer": [], "url": "https://answers.ros.org/question/299836/how-to-synchronize-two-different-ros-programs/"},
{"title": "Epos_hardware write output error when running example.launch", "time": "2016-10-05 11:11:44 -0600", "post_content": [" ", " ", "Hello,", "I am working with EPOS2 70/10 and a maxon EC motor whose shaft is already attached to load. I am using epos_hardware indigo-devel branch package in UBuntu 14.04 x86 4.4.0-38-generic and ROS indigo. I am trying to use the epos_hardware package in order to control velocity on motor. After dealing with the usb rights problems (further info solved in other questions), now I am able to detect the motor using list_devices node. I just adapted the example's launch, urdf and yaml files in order to match my motor settings. ", "The problem is than when running the launch file (which loads motor settings and enables velocity and position controller) loads the controllers (joint_state and velocity_controller), starts them (look at info message below), the green led is running continously (which means that is properly working) and then the following message appears:", "When trying to set a velocity (rostopic pub /velocity_controller/command std_msgs/Float64 '##'; #:Number between my motor velocity range) the /diagnosis topic shows me this:", "So, velocity and position tells me that the shaft is not moving but torque is supposedly not zero (no access to see shaft as is already installed on the machine) and current is even sometimes exceeding my nominal values. From this I take that the problem may be that:", "From what I have read, the command library functions return 0 in case that they are unsuccessful so this is the main reason why I am focusing the problem of not moving the load on this error instead of the overload case. Yet, I do not know any way to check ..."], "answer": [" ", " ", "At this point I kept working with the package. I wanted to manage multiple EPOS in a CAN net by managing them through accessing to one of them via USB. The following message was kept being printed periodically as I mentioned in my question:", "I could launch the controller but I lost control of the EPOS after sometime by not know reason. I did not know if the problem was due to the limited handling package capability of EPOS or if I was not defining rigth my URDF or if there was some kind of bug in the implementation of the controllers of the hardware abstraction of ros_control (not so much reliable documentation explaining in detail its behavior).", "The level of documentation of this package is quite poor from my point of view. A lot of explanations on the ROS Code API is missing and it gets really messy to look debug sometimes. It is a great idea but it gives the sensation that the work was left have done (as realease version was published as 0.0.3). ", "The USB connection works but when trying to make it work in the configuration I was testing the USB falls making it impossible to communicate with the EPOS unless the code is relaunch. Then you have to pray for the system to be able to clear errors on the first try.", "I debugged my code, my URDF and I was not understanding the reasons behind this. Also tried in Windows LabView 2015 and the system did not lost control, the problem of the implementation of this package is that depends on dll libraries which are not supported in Linux based OS so I could not (yet) working with the linux library implementation of epos control.", "It seems that the USB communication is not suitable or robust enough so I would recommend to change to RS232 (google the differences on how the communication is done in each case). At least now it looks that I am not experiencing the USB connection fall but I will update with my results in future.", "Hope I can save you some time that ", " ", " ", "  Thank you for your answer! I am also now trying to switch from USB to RS232 but I am not able to use epos_hardware package to initialize the motor. I have in the past successfully used USB connection with epos_hardware. I have also used the EPOS_studio package to control motor through RS232 before trying with ROS. What are all the changes needed in the yaml file to shift from USB to RS232? ", "Thanks in advance!", "So I replaced MAXON RS232 as the protocol stackname with Maxon_RS232 and it started working! Thanks for the help!", "Yeap, back on the days this was a great headache for me. I had a 1 master-3 slaves configuration, since changing the protocol I had no problem.\nI am glad this has help you to save time!"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "The load is too much for the motor\ntorque to even move.", "There might be a problem with the\nepos_library function when accessing\nto the parameters or a problem with\ncommunication.\n", "Kernel compatibility problem", "Combination of both or other random\nissue (because former team members\nwere able to run the motor free\nshaft with this package)."], "question_code": ["[INFO] [WallTime: 1475677663.913912] Started controllers: joint_state_controller1, velocity_controller1\n\nWrite processed successfully, but number of bytes written is 0: Operation now in progress\nWrite processed successfully, but number of bytes written is 0: Operation now in progress\nWrite processed successfully, but number of bytes written is 0: Operation now in progress\n", "---\nheader: \n  seq: 501\n  stamp: \n    secs: 1475678169\n    nsecs: 978535919\n  frame_id: ''\nstatus: \n  - \n    level: 0\n    name: epos_hardware: my_joint_actuator1: Motor\n    message: Enabled\n    hardware_id: 662080006194\n    values: \n      - \n        key: Actuator Name\n        value: test_joint_actuator1\n          - \n            key: Enabled\n            value: True\n          - \n            key: Fault\n            value: False\n          - \n        key: Voltage Enabled\n        value: True\n      - \n        key: Quickstop\n        value: True\n      - \n        key: Warning\n        value: False\n  - \n    level: 1\n    name: epos_hardware: my_joint_actuator1: Motor Output\n    message: Nominal Current Exceeded (Current: 10.506000 A)\n    hardware_id: 662080006194\n    values: \n      - \n        key: Commanded Velocity\n        value: 250 rpm\n      - \n        key: Operation Mode\n        value: Profile Velocity Mode\n      - \n        key: Nominal Current\n        value: 9.8399999999999999 A\n      - \n        key: Max Current\n        value: 10.5 A\n      - \n        key: Position\n        value: 0 rotations\n      - \n        key: Velocity\n        value: 0 rpm\n      - \n        key: Torque\n        value: 10.506 Nm\n      - \n        key: Current\n        value: 10.506 A\n      - \n        key: Target Reached\n        value: True\n      - \n        key: Current Limit Active\n        value: False\n"], "answer_code": ["Write processed successfully, but number of bytes written is 0: Operation now in progress\nWrite processed successfully, but number of bytes written is 0: Operation now in progress\nWrite processed successfully, but number of bytes written is 0: Operation now in progress\n"], "url": "https://answers.ros.org/question/245047/epos_hardware-write-output-error-when-running-examplelaunch/"},
{"title": "is it necessary to have URDF model for diff_drive_controller", "time": "2018-08-16 15:20:34 -0600", "post_content": [" ", " ", " ", " ", "Hello, ", "I am building a differential drive robot and trying to exploit the power of ros_control by using the diff_drive_controller into my project", "I have found that I have to fulfill some hardware requirements to the controller in order to be able to use it  and that's what I have done but due to my limited knowledge in URDF and TF I can't understand the part of the left_wheel_joint and right_wheel_joint in the configuration of the controller", "shall I first make a URDF model of my robot to be able to use the diff_drive_controller or I can just make a TF to use or it's not possible?\nwhat shall I write in the following parameters?", "I will appreciate your help ", "Please change the title of your question.", "The title makes it seem like you want to know whether urdf is required for TF ", ", but your question text appears to be asking whether a urdf is needed for use with ", ".", "Those are two different things.", "Yes I missed that sorry"], "answer": [" ", " ", "Yes, it's necessary to have a URDF. The parameters ", " and ", " need to be set to the joint names from the URDF.", "In your URDF, you also need a transmission for that joint. If you want to look at an example, in my ", " stack I use the ", " (but only in Gazebo, not on the real robot). I have converted the xacro to a URDF (using ", ") and deleted a couple of things that are not relevant to your question. The result is here:", "You can see that there is one joint for each wheel and a transmission for both. There are also four caster wheels; you don't need them of course if your robot doesn't have them. Note that you need to run ", " if you want to try the URDF.", "Thank you for your help and for sure I will use your URDF code as a reference to write my own \nThank you for your time", "I have another question, is it necessary to have URDF model even if I will not simulate the robot I just want to interface it directly to hardware?", "Yes. I had already removed everything from the URDF that is only necessary for simulation. But your URDF could even be much simpler than my example; for example, the caster wheels could be deleted. Also you don't even need the visual and collision tags for your purpose.", "Thanks a ton"], "question_code": ["left_wheel\nright_wheel\n", "diff_drive_controller"], "answer_code": ["left_wheel", "right_wheel", "diff_drive_controller", "xacro --inorder mir.urdf.xacro > mir.urdf", "sudo apt install ros-kinetic-mir-description"], "url": "https://answers.ros.org/question/300763/is-it-necessary-to-have-urdf-model-for-diff_drive_controller/"},
{"title": "Roscore force unregister of node that died", "time": "2018-09-18 13:23:58 -0600", "post_content": [" ", " ", "Hi!", "I believe I'm running into a situation in a multi machine setup. ", "Setup", "What I believe I'm observing sometimes is that if I forcibly kill the second machine (for example, disconnect the power), since the ros::shutdown() is not called the roscore still thinks that the node is there and lists it.", "Is there any way I can force roscore to kill the topic that is connected to the node that doesn't exist anymore? ", "Since I'm using anonymous node names (and I have to do it in my case) is that if I forcibly reset my slave computer I will end up with a rosnode list showing two topics, one of which is not connected to anything.", "Any tips how to handle it neatly would be highly appreciated!", "rosmaster doesn't have any timeout for nodes if they crash, so it will keep remembering them until a new node starts with the same name. I'm pretty sure this isn't a new issue, but I can't find a bug report for it.", "Related Q&As: ", " and ", ".", " should do the trick.", "Thanks for the helpful comments! I figured a better approach than force unregistering the node. Instead in my case it's sufficient to use ", " and keep track of alive nodes", "Wondering how this compares to ", " (at least in C++)."], "answer": [" ", " ", "After thinking about it for some time and doing some experiments I decided unregistering a node is a bad idea in my case (it seems you can create quite a lot of trouble if a node is not available because for example the network is down for a second).", "What I found to be a great alternative without a brute force approach is ", " package that allows me to keep track of nodes that are see as alive. Then I can use this information to keep track of 'pingable' nodes and use them accordingly in my setup."], "question_details": [" ", " ", " ", " ", " ", " ", "Machine running rosmaster ", "Second machine that's properly setup to recognize the machine running roscore"], "question_code": ["rosnode cleanup"], "url": "https://answers.ros.org/question/303706/roscore-force-unregister-of-node-that-died/"},
{"title": "How can I publish camera footage from Insta 360 camera?", "time": "2018-08-18 05:02:26 -0600", "post_content": [" ", " ", "Hi,I have used usb_cam node for publishing a single camera footage earlier. But when I connected the Insta 360 degree camera and tired to publish it I see nothing a screen with dots half and green half at bottom.So can any one please help me how to publish the live footage of Insta 360 Camera in Ros."], "answer": [" ", " ", "This is the launch setup, we've been using with the Insta360:", "Thankyou@Stefan Kohlbrecher. \nYes I tried what you suggested,but when I try to launch it I have got this as an output.\n\" Unable to open camera calibration file [/home/bittu/.ros/camera_info/camera.yaml]\"\nSo can explain me how to get this file.", "You normally obtain this by standard camera calibration. As such calibration is meaningless for the Insta360's dual fisheye output, you should be able to ignore this warning. Or is this a error that causes the driver to not come up correctly?", "Thank you for the reply.Yes,I was able to ignore that and was successful in publishing the footage.", "This launch file worked with which camera version? Insta360 nano, pro, pro2? Did work for 3D streaming?"], "answer_code": ["<launch>\n  <arg name=\"camera_name\" default=\"camera360\" />\n  <arg name=\"frame_id\" default=\"$(arg camera_name)_center_link\"/>\n  <arg name=\"calibration_name\" default=\"insta360\"/>\n\n  <node pkg=\"nodelet\" type=\"nodelet\"\n    name=\"$(arg camera_name)_nodelet_manager\" args=\"manager\"\n    output=\"screen\"\n    respawn=\"true\"/>\n\n  <node pkg=\"nodelet\" type=\"nodelet\"\n    name=\"$(arg camera_name)_nodelet_loader\" \n    args=\"load libuvc_camera/driver $(arg camera_name)_nodelet_manager\" \n    output=\"screen\"\n    respawn=\"true\">\n\n    <!-- Parameters used to find the camera -->\n    <param name=\"vendor\" value=\"0x2e1a\"/>\n    <param name=\"product\" value=\"0x1000\"/>\n    <param name=\"serial\" value=\"0\"/>\n    <!-- If the above parameters aren't unique, choose the first match: -->\n    <param name=\"index\" value=\"0\"/>\n\n    <!-- Image size and type -->\n    <param name=\"width\" value=\"3008\"/>\n    <param name=\"height\" value=\"1504\"/>\n    <!-- choose whichever uncompressed format the camera supports: -->\n    <param name=\"video_mode\" value=\"mjpeg\"/> <!-- or yuyv/nv12/mjpeg -->\n    <param name=\"frame_rate\" value=\"30\"/>\n    <param name=\"pub_every_n_th_image\" value=\"3\"/>\n\n    <param name=\"timestamp_method\" value=\"start\"/> <!-- start of frame -->\n    <param name=\"camera_info_url\" value=\"\"/>\n\n    <param name=\"brightness\" value=\"100\"/>\n\n    <remap from =\"/image_raw\" to=\"/$(arg camera_name)/image_raw\"/> -->\n    <remap from =\"/camera_info\" to=\"/$(arg camera_name)/camera_info\"/>\n    <remap from =\"/set_camera_info\" to=\"/$(arg camera_name)/set_camera_info\"/>    \n  </node>   \n\n</launch>\n"], "url": "https://answers.ros.org/question/300896/how-can-i-publish-camera-footage-from-insta-360-camera/"},
{"title": "Multiple ros nodes like shells?", "time": "2018-09-15 21:53:17 -0600", "post_content": [" ", " ", "Hi,", "I am building an autonomous rc car. Right now I am building navigation stack and it is almost finished, after that I will proceed with teb local planner. If I manage to do it I want to add; my opencv code which detects lane and follows it, another opencv for sign detection and finally I don't know how will I do it but a code that I will give coordinates to the car and it will go there in the map that I detected, edited before.", "Let's say I manage to do all of them (opencv's are done, I hope teb will be too very soon); how can I run them all in order? Lane follow and sign should be my core code for driving, then I should have teb local planner and finally coordinate stuff. Would it work if I just run all these nodes together or should I create a new package? If both works, what would be the most easy one for computational power?", "I'm not sure to understand correctly your issue here : you want to know how to combine all your nodes to have a complete navigation ? \nFrom my understanding I think you could use the ", " nodelet that takes in input your velocity commands. Setting a greater priority to the sign detection...", "...than the lane follow and use ", " to send your differents goals once calculated from the coordinates of the car. So yes you can run all the nodes together."], "answer": [], "question_code": ["cmd_vel_mux", "actionlib"], "url": "https://answers.ros.org/question/303443/multiple-ros-nodes-like-shells/"},
{"title": "Rviz receives LaserScan messages but doesn't display them", "time": "2018-05-14 11:28:53 -0600", "post_content": [" ", " ", "Hi there", " I made a dummy package that is publishing fake laser scan messages (following this code:  ", "  ). ", "I use this command to launch my package:", "Then, I try to display it with:", "Finally,  I use this command in order to avoid fixed-frame related errors,: ", "I set global_frame as \"fake_laser_frame\" and I add the topic /fakeScan (type LaserScan).\nEverything seems to work great, only green checks and I can see the number of receives messages increase.\nBut I don't see my laser scan displayed ! I tried with a kinect and package \"laserscan_kinect\", it works.", "What am I missing ?", "More info:\nWhat I'm trying to do : for the moment, juste learning really. My final is goal to use a Lidar Lite v3 to generate Laser Scan messages.", " My code:  ", "Screenshot: (I tried, but not enough reputation).", "For debugging, maybe you can try and see output of ", " to check what values are actually being published.", "Hi Erenaud, I prepared a video explaining what could be your problem, but you figured it out before I published. Anyway, here it is just in case it helps you understand some basic ROS concepts.\n", "Wow, you did a great job here ! Thank you for your help ! Don't worry, I'm pretty sure it will help others :)", "After a short time count> 100, but \"range_max = 100\". And rviz not show it data.\nYou must restart the node, after the configured rviz and can seet data shot time.\nOr set big limat for range_max or set limit for ranges [i] and intensities [i] less then range_max."], "answer": [" ", " ", "Okay, I don't know if it's because I restarted my computer or my eyes were too tired yesterday, but my code is in deed working ! The contrast between background and Laser Scan point is very poor but everything is well displayed."], "question_code": ["roslaunch beginner_tutorials fakeLaserScan.launch\n", "rosrun rviz rviz\n", "rosrun tf static_transform_publisher 0 0 0 0 0 0 1 map fake_laser_frame 10\n", "rostopic echo /fakeScan"], "url": "https://answers.ros.org/question/291095/rviz-receives-laserscan-messages-but-doesnt-display-them/"},
{"title": "Tracking QR codes within Gazebo result in off-centre transforms", "time": "2018-09-21 09:24:07 -0600", "post_content": [" ", " ", " ", " ", "Hi all,\nI'm trying to run the package Fiducials on a quadcopter within a gazebo simulation. There are two nodes that are ran during this, the aruco_detect node that recognises the QR codes within the camera feed and the fiducial_slam node that calculates the relative position of the robot. \nThe aruco_detect node works fine, with the fiducials being recognised within the virtual camera stream.There is a map file ~/.ros/slam/map.txt I have edited to produce the markers you can see within the RViz simulation that represent the position of the markers relative to the world frame. These markers are used for the pose estimation. The markers are red if they are in view and green is they are declared in the map but not within camera view.  However, the fiducial_slam produces slightly offset transforms 'fid100' and fid101 for the the fiducial markers named 100 and 101. I cannot work out the cause of this offset. \nI think it may be something to do with fid0 frame but I'm not entirely sure what this frame signifiies as it does not say within the documentation. I include the launch file that I launch the two nodes from. No changes from the original source code have been made. I am running ROS kinetic on ubuntu 16.04 and gazebo 7", " Here are screenshots to make this a lot clearer in a shareable link: \nDrone on the ground looking at QR codes in gazebo :\n ", " Drone on ground looking at QR codes in Rviz ( robot goes through floor)\n ", "There is also a static transform publisher from my cam_optical_frame to my base_link as described by:", "EDIT", "/Camera_info topic shows ...", "The error is the offset between the fid104 & fid105 tf frames and the red squares?  Are you confident the squares are actually where they need to be to line up with gazebo?  It looks like from the Image view that the tf frames are spot on the center of the observed tags.", "Does your camera have distortion coefficients?  I wonder if some parts of your system are using them and others are not, creating the error.", "Hi, yes the squares are in the correct position. I wondered that so I ran the cameracalibration.py for monocular cameras found at the opencv package and my camera still does contain distortion parameters and the camera info topic published by the camera is in the edit. Could I set these as zero?", "No, only the ", " is the distortion, and it already is all zero.  If that was a calibration for a real camera (not a gazebo simulated one) there should be something there.  Also is there a possibility that gazebo is using a different camera info than the one other parts of the system are using?", "Look for two different camera_info topics.  Is there a set of launch files online that you are you using you can link to to?", " looks wrong, but may not be causing any problems either."], "answer": [], "question_code": ["       <arg name=\"camera\" default=\"/cam/camera\"/>\n      <arg name=\"image\" default=\"image\"/>\n      <arg name=\"transport\" default=\"compressed\"/>\n      <arg name=\"fiducial_len\" default=\"0.44237\"/> \n      <arg name=\"dictionary\" default=\"7\"/>\n      <arg name=\"do_pose_estimation\" default=\"true\"/>\n     <node pkg=\"aruco_detect\" name=\"aruco_detect\"\n    type=\"aruco_detect\"  respawn=\"false\">\n    <param name=\"image_transport\" value=\"$(arg transport)\"/>\n    <param name=\"publish_images\" value=\"true\" />\n    <param name=\"fiducial_len\" value=\"$(arg fiducial_len)\"/>\n    <param name=\"dictionary\" value=\"$(arg dictionary)\"/>\n    <param name=\"do_pose_estimation\" value=\"$(arg do_pose_estimation)\"/>\n    <remap from=\"/camera/compressed\"\n        to=\"$(arg camera)/$(arg image)/$(arg transport)\"/>\n    <remap from=\"/camera_info\" to=\"$(arg camera)/camera_info\"/></node>\n\n    <arg name=\"map_frame\" default=\"world\"/> \n  <arg name=\"base_frame\" default=\"base_link\"/>\n  <arg name=\"publish_tf\" default=\"false\"/>\n  <arg name=\"future_date_transforms\" default=\"0.0\"/>\n  <arg name=\"publish_6dof_pose\" default=\"true\"/>\n\n  <node type=\"fiducial_slam\" pkg=\"fiducial_slam\" \n    name=\"fiducial_slam\" output=\"screen\">\n\n    <param name=\"map_file\" value=\"$(env HOME)/.ros/slam/map.txt\" />\n    <param name=\"map_frame\" value=\"$(arg map_frame)\" />\n    <param name=\"odom_frame\" value=\"\" /> \n    <param name=\"base_frame\" value=\"$(arg base_frame)\" />\n    <param name=\"future_date_transforms\" value=\"$(arg future_date_transforms)\" />\n    <param name=\"publish_6dof_pose\" value=\"$(arg publish_6dof_pose)\" />\n    <param name=\"do_pose_estimation\" value=\"$(arg do_pose_estimation)\"/>\n\n    <param name=\"fiducial_len\" value=\"$(arg fiducial_len)\"/>\n    <remap from=\"/camera_info\" to=\"$(arg camera)/camera_info\"/>\n    <remap from=\"camera_frame\" to=\"cam_optical_frame\"/>\n\n  </node>\n", "<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_footprint_to_optical_cam_frame\"  args=\"-0.4 0.0 0 1.57 0 -1.57 /base_link /cam_optical_frame 100\"/>\n", "D:", "<remap from=\"camera_frame\" to=\"cam_optical_frame\"/>"], "url": "https://answers.ros.org/question/303972/tracking-qr-codes-within-gazebo-result-in-off-centre-transforms/"},
{"title": "ROS1 on Udoo x86", "time": "2018-09-28 04:59:12 -0600", "post_content": [" ", " ", " ", " ", "Hello Folks,", "We are building a Robotic Arm, and for that I am searching for some SBC.\nI checked few boards, and reduced my search to 2 boards:", "a. Udoo x86", "b. Odroid XU4", "I found many people using ROS on Odroid boards, but I think we may need more computing power.\nSo I thought of Udoo x86 Ultra, which uses Intel Celeron, and I think it should be powerful enough of a typical low-end laptop.", "Would love to  listen any experiences and opinions about this board for ROS.\nAnd if you came across some more powerful board, please  share.", "Saurabh"], "answer": [], "url": "https://answers.ros.org/question/304457/ros1-on-udoo-x86/"},
{"title": "launch \"global_planner\" without move_base", "time": "2018-10-05 13:22:55 -0600", "post_content": [" ", " ", "Hey everybody,", "I'm still working on my platform which is already running in the meantime. But I had to do some workarounds. To be more precise i didn't manged to set up the move_base to work with my bot. Either it is the setup/params or the odometry (which i programmed myself). But the Pathfinding works so far. So i programmed a path executor. A node, that just follows the path (the green line from the (global?) planner - NAVfn_path). No local planner, nothing extra. Just try to follow this line as close as possible. The bot is really slow - no dynamics to take into account. Moving objects are avoided because the path gets recalculated once every second or so.", "Now to the \"problem\": i want to get rid of all the unneeded code running inside of move_base. So i tried to set up a costmap (costmap_2d) with the parameters i used with the move_base (since the costmap there just looks nice as it is right now :D). The standalone costmap looks as expected in rviz. Next step would be to start the \"global_planner\". from this ", " it is doing everything i need. I can even change the type of calculation there. And i hope i can just reinit the planner once it dies because the goal lies within an obstacle (the move_base tends to do this. The carrot_planner would do what i need, but ignores obstacles at all - even walls). But ... how do i call the planner? I just tried ", "the [tab] just auto completed the command. So i would expect the node exists. I added it to the launch file i had prepared and it worked. for the goal i used", "Well, the planner complained the bot is outside of it's costmap. So i displayed it in rviz. And yes, it's outside, or more resize at 0.0/0.0 . And the costmap is blank. The costmap from the costmap_2d-node looks still nice and the bot is at the correct position (amcl works fine).", " ", "the documentation doesn't name anything like this. No topic the node subscribes to or a parameter with the name. all in all the documentation seems to be limited to the minimum since the planner doesn't seem to be intended to be used outside of move_base at all ?", "rosparam list:", "Launchfile (shorted/abstract):", "params.yaml (but since ..."], "answer": [" ", " ", "And i would like to answer my own question. it came to me as i wrote down that the two nodes have almost exactly the same costmap/params. The global_planner just comes with it's own costmap. All i had to do is to load the exact same yaml-file together with the global_planner. And as expected: the node complains and doesn't calculate a path, but it stays alive and you can just enter a valid goal and the planner will than give you a plan.", "Maybe this can help someone trying the same as me :D"], "question_code": ["$ rosrun glob[tab]al_planner [tab]planner\n", "$ rostopic pub /planner/goal geometry_msgs/PoseStamped '{header: {frame: map}, pose: {position: {x: 1.0, y: 2.0, z: 0.0}}}'\n", "...\n/costmap_2d_node/costmap/(many different parameters here)\n...\n/planner/costmap/(almost the same parameters as above)\n....\n", "<launch>\n    <include> hardware driver, laserscanner, encoder, odometry </include>\n    <node> map server </node>\n    <node> amcl </node>\n\n    <node pkg=\"costmap_2d\" type=\"costmap_2d_node\" name=\"costmap_2d_node\" clear_params=\"true\">\n        <rosparam file=\"$(find navigation)nav_config/params.yaml\" />\n    </node>\n\n    <node pkg=\"global_planner\" type=\"planner\" name=\"planner\" />\n\n    <robotdescription> my own xacro </robotdescription>\n    <robot_state_publisher ... />\n    <joint_state_publisher ... />\n    <node> rviz </node>\n</launch>\n"], "url": "https://answers.ros.org/question/304965/launch-global_planner-without-move_base/"},
{"title": "Setpoint VS RC Override : which is better solution for quadcopter positioning with opencv?", "time": "2018-10-06 03:26:33 -0600", "post_content": [" ", " ", "Hey all. I want to ask some question before I'm dead exhausting...", " I've seen several pages like  ", "  or  ", "What I want to do is drone to fly following some colored circles with OpenCV, in approximately 1m high.", "First I tried setpoint topics, but after some research, I've noticed that there're some topics called 'RC Override'.", "So, what I'm going to say, is...which is better solution for drone positioning with computer vision? setpoint topic? OR RC Override??", "Once again, what I'm going to do is", "PX4 QUAD VISION POSITIONING with companion com and opencv, and following some circles(red green blue colored) in line(like this : O O O O O O, but with different colors each.).", "Any of your reply will help(or LITERALLY SAVE) me. Have a good time!", "ROS : Kinetic with mavros, mavros-extras PX4 : latest, stable build OpenCV : 3.3.1(default with ros)", "Just some expectation management: I'm not sure the intersection between ROS Answers readers and those heavily into MAVROS is very big, so it may take some time before you get a response.", "Navigating the drone with setpoint topics is recommended. I was in a similar dilemma last year and learnt it the hard way. My experimentation ", ". Feel free to clone / fork for your use."], "answer": [" ", " ", "Navigating the drone with setpoint topics is recommended. I was in a similar dilemma last year and learnt it the hard way.\nMy experimentation ", ".\nFeel free to clone / fork for your use.", "Thank you. I'll try it. But here's my question : why setpoint is recommended?\nI think it'll be faster & easy-accessible with fake RC signal.\nAlso, what if setpoint is unstable? ROS topics say some weird local z value when first started,\nlike -1.18 something or 6.12 something..it differs all the time"], "url": "https://answers.ros.org/question/304993/setpoint-vs-rc-override-which-is-better-solution-for-quadcopter-positioning-with-opencv/"},
{"title": "Rosserial_Python Node Unresponsive", "time": "2018-10-19 14:52:13 -0600", "post_content": [" ", " ", " ", " ", "In my robot I have two Teensy 3.5 microcontrollers set up as nodes using rosserial_python. Each controls a number of actuators and reports on their positions. One of these nodes will occasionally and randomly completely stop responding with no indication as to why. If you publish a command to its topics no errors will appear but the node will do nothing with the command. If you echo one of its publishers it will just sit there blankly until you kill it, even for topics that publish several times a second. The logs show nothing weird going on, but to get it to do anything you have to kill the process with CTRL+C, then after a while it will say \"escalating to sigterm\" and the node will finally exit. If you relaunch the node it will resume working as usual.", "I'm using ROS Kinetic on Ubuntu 16.04. These issues have occurred across multiple computers and multiple microcontrollers so it's not a hardware issue. I never have this problem with the other node despite it publishing much larger messages such as odom.", "I cannot cause the issue to happen. I've killed and relaunched the node, cycled power, left it running a long time but nothing seems to impact how often it happens It is very intermittent seems to happen every couple days but when the robot is supposed to functional without interaction on startup it's very frustrating.", "If anyone has any idea as to why this may occur I'd be grateful."], "answer": [], "url": "https://answers.ros.org/question/306288/rosserial_python-node-unresponsive/"},
{"title": "Robot does not move in rviz", "time": "2018-10-21 05:19:09 -0600", "post_content": [" ", " ", " ", " ", "I am using 2dnav in rviz and robot works fine until suddenly it stops moving when setting another goal. I am attaching a screenshot from my rviz and you can see that both local and global planners work and release messages to their path topics (green and purple lines) but robot does not move. Any idea why?", "Here is my Trajectory yaml file:", "Is the local planner giving you any warnings? Have you tried running it with the default local / global planner parameters?"], "answer": [], "question_code": ["TrajectoryPlannerRos:\n  max_vel_x: 0.2\n  min_vel_x: 0.1\n  max_vel_theta: 0.35\n  min_vel_theta: -0.35\n  min_in_place_vel_theta: 0.25\n\n  acc_lim_theta: 0.25\n  acc_lim_x: 2.5\n  acc_lim_y: 2.5\n\n  holonomic_robot: false\n\n  meter_scoring: true\n\n  xy_goal_tolerance: 0.15\n  yaw_goal_tolerance: 0.25\n"], "url": "https://answers.ros.org/question/306334/robot-does-not-move-in-rviz/"},
{"title": "How to reset Kuri using the ROS interface", "time": "2018-10-29 21:40:55 -0600", "post_content": [" ", " ", "My Kuri is having trouble doing the hardware-reset (holding down power and up-buttons to reset the software).  Her buttons might not be connecting continuously, so she never makes the sad-reset sound.\nIs there a way I can force her to reset via the ROS interface?"], "answer": [" ", " ", "If you're having difficulty resetting Kuri using the hardware buttons, you can attempt to reset Kuri by triggering the reset in software. First, ssh into your Kuri and then run the following commands:", "You should hear Kuri play a sad sound and then you will be disconnected from Kuri as she reboots. After you hear the boot chime, you can connect and onboard Kuri using the Kuri app. You can also ssh into Kuri at this time."], "answer_code": ["source /opt/gizmo/setup.sh\nrostopic pub -1 /triggers gizmo_msgs/Command '{name: \"reset robot\", params: []}'\n"], "url": "https://answers.ros.org/question/307106/how-to-reset-kuri-using-the-ros-interface/"},
{"title": "RosAria not receiving messages", "time": "2018-10-29 09:47:27 -0600", "post_content": [" ", " ", "#Resources\nMacbook Air with Ubuntu virtual machine via VMWare", "\nPioneer3 AT robot", "\nROS", "\nRosAria", "#Context\nI have at least most of the setup for Ros+RosAria already working:", "The RosAria node running on the robot confirms it\u2019s able to connect to the master, and if I run the command ", " on the laptop I can see ", " as one of the available topics.", "In the terminal window where I try to publish to ", " I\u2019m told that the message is sending but the robot shows no signs of having received it, neither console message nor movement of the wheels.", "However, if I quit the RosAria node on the robot and restart it while leaving the ", " command running on my laptop, the robot registers the message once upon startup, moves accordingly for a bit, and then does nothing.", "I\u2019ve also tried to ", " other topics like battery state, motors status and pose, but I don\u2019t receive any messages from the robot.", "I already tried changing the parameters ", " and ", " according to ", ".", "#Question\n_Why is the Pioneer robot with RosAria running not receiving the ", " commands from my client if it\u2019s apparently able to communicate with the master node successfully?_", "#Disclaimer\nThe project I was working on where this question came up changed, and as a result I\u2019m not currently using this equipment anymore. So I can\u2019t really test suggested solutions and it\u2019s no longer an emergency, though I still wanted to post the question since I was having issues asking it before.", "Can you please post the value of ", " and ", " for ", " computers involved? And the IPs of all those machines?"], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "The master ROS node running on a laptop at ", " via ", "The RosAria node running on the robot\u2019s built-in computer at ", " via ", ", the port set accordingly to access the robot's motors and sensors via ", ", ", ", and ", ".", "Another terminal window on the laptop running "], "question_code": ["192.168.1.112", "roscore", "192.168.1.108", "rosrun rosaria RosAria", "rosparam set...", "ROS_MASTER_URI=192.168.1.112:11311", "ROS_IP=192.168.1.112", "rostopic pub /RosAria/cmd_vel geometry_msgs/Twist \u201clinear:... angular:...\u201d", "rostopic list", "/RosAria/cmd_vel", "/RosAria/cmd_vel", "rostopic pub", "rostopic echo", "/RosAria/TicksMM", "/RosAria/RevCount", "cmd_vel", "ROS_MASTER_URI", "ROS_IP"], "url": "https://answers.ros.org/question/307031/rosaria-not-receiving-messages/"},
{"title": "Getting started in the world of drone programming", "time": "2017-03-16 10:11:54 -0600", "post_content": [" ", " ", "Hey guys, I'am new in this site but i think i will pretty active in the next few months!\nMy idea is program a drone so that it follows objects alone. I have some programming experience  but i don't yet have a drone. \nI know that the Parrot drones can be programmed with ROS and i get the idea that people who get into this projects follow that path, i'am i correct?\nI found out recently that it's also possible to program DJI drones, how does that work?", "I need the drone to have around 30 minutes of battery(ideally more, but if not possible that will do). The camera needs to be decent as well and i would also like the drone to be robust. (not fun burning $$). ", "So based on the above what drone would you recommend me, and how should i tackle this? (i'am thinking i need to study computer vision and learn to use OpenCV, what more?)"], "answer": [" ", " ", "If you are new to drone programming, I would recommend you to start out with a simulator (unless you have many $$ to burn).", " There is some options out there using the Gazebo simulator, check this video:  ", "  it contains instructions on how to get it running in Ubuntu 14.04 and ROS Indigo: ", "(I reproduce the instructions here just for reference):", "I will give that a go for the moment.", "Excuse me, is it works with the latest ROS version(melodic)?.", "I haven't tried it myself, why don't you give it a try?", " ", " ", " ", " ", "Hello ", ",", " You can use the SJTU_DRONE simulation, it's available here in this link:  ", "Basically, this simulation provides to you the following features:", "forward looking camera :  /drone/front_camera/image_raw", "downward looking camera: /drone/down_camera/image_raw", "sonar data:  /drone/sonar", "laser range data: /drone/laser", "If you want a tutorial, there's this video to follow and see it working:", "It presents the basics of a gazebo parrot ardrone simulation. You'll be able to program and send high level commands to the robot in a few minutes.", " It's using RDS, but if you want to have it in your own machine, you can clone here:  ", "Clone, install the dependencies and compile it.", "It's using ROS indigo and gazebo 7.", "Some features of the simulation:", "/drone/takeoff (std_msgs/Empty)", "/drone/land (std_msgs/Empty)", "/drone/posctrl (std_msgs/Bool)", "/drone/velctrl (std_msgs/Bool)", "/cmd_vel (geometry_msgs/Twist)", "I hope it can help you!", "As it is, this answer isn't very useful. Link only answers don't actually ", " the question, they only tell the asker to go somewhere else. This answer would be greatly improved by putting the steps/code from the video in the answer.", "Thanks for the feedback, I've improved the answer to give the same instructions there're in the video.", " ", " ", " ", " ", "Hello All! ", "I am also working with Drone Parrot 2.0 and my objective is to control drone using GUI and path planning functionality to cover autonomous flight using ROS and Python.", "I have pushed the current status of the work to GIT. You may access if required as well please update if you can able do changes to it.", " Here is the link of the repository:  ", "it includes some unwanted files and code you can only concentrate on main.py and basic_controller.py files.", "As of now, the current status is that ", "You can control the drone with functionalities Take-Off, Land, Turn and Linear movements through GUI. ", "You can also see trajectory based autonomous flight (Straight Line, Square, Rectangle, Triangle, Circle) to achieve information is fed through files.", "Basic_Controller.py allows you to control drone using the keyboard. ", "Dynamically NavData is also getting updated on GUI.", " Screenshot -(  ", " ) ", "Special Thanks to TUM and GAI_Tech! ", "WIth Regards,\nPavan", "Can you please update your answer with the relevant steps vs providing a link to the github repo and decription?"], "answer_code": [" -----------Install Ros indigo, Gazebo, Tum Simulator---------------\n\n\" Before install ROS and GAZEBO Ubuntu, you should install UBUNTU 14.0 in your machine \" \n\n1. Install Git\n- sudo apt-get install git\n\n3. Instal ROS-GAZEBO\nyou can follow the instruction how to install of ROS \n- http://wiki.ros.org/indigo/Installati...\n\n\n4. Install Tum Simulator\n- You can follow the instruction in https://github.com/dougvk/tum_simulator or..\n\n1. Create a workspace for the simulator\n\n```\nmkdir -p ~/tum_simulator_ws/src\ncd ~/tum_simulator_ws/src\ncatkin_init_workspace\n```\n2. Download dependencies\n\n```\ngit clone https://github.com/AutonomyLab/ardron... # The AR.Drone ROS driver\ngit clone https://github.com/occomco/tum_simula...\ncd ..\nrosdep install --from-paths src --ignore-src --rosdistro indigo -y\n```\n3. Build the simulator\n\n```\ncatkin_make\n```\n4. Source the environment\n\n```\nsource devel/setup.bash\n```\nHow to run a simulation:\n\n1. Run a simulation by executing a launch file in cvg_sim_gazebo package:\n\n```\nroslaunch cvg_sim_gazebo ardrone_testworld.launch\n```\n"], "url": "https://answers.ros.org/question/257182/getting-started-in-the-world-of-drone-programming/"},
{"title": "rosserial kinetic [Error 5] Input/output error", "time": "2018-12-23 10:43:08 -0600", "post_content": [" ", " ", " ", " ", "I sent a message from a moveit demo.launch (angles of the joints from sensor_msgs::JointState) which I created from moveit setup assistant to a .cpp file, and publish a converted value to my .ino file (arduino file).", "The connection and subscriber/publisher worked fine when connected only an arduino board (I used arduino mega 2560 here, and chose the correct serial port and baud rate), but when I connected my servo motors to the arduino board, the connection was failed and showed me:", "The error seemed cause by the connection of the servo. But I have run the ServoControl tutorial in rosserial.", "Is there anyone can help me for this?", "Is the requested voltage too big to be given?", "This is my first time on ROS Answers, sorry for the bad typesetting.", "Thanks, Wolf", "Edited 1: error information", "Can you please update your question with a copy and paste of the entire error?", "jayess: error updated.", "BTW, how can you make your typesetting so clear?", "If you're trying to run servos and the arduino mega off just the usb port of your computer then there probably isn't enough current. A computer's usb port can only provide 500mA, if the servo tries to draw more it will brown-out the arduino.", " Paste your formatted text into the text editor, highlight it, click on the ", " button or press ", ".", "PeterBlackerThe3rd:", "That is one of my guess, too.", "But I did run the ServoControl example with the same motor before, it worked fine...", "So I am not sure if that would be the answer.", "Btw, I connected another battery to the motor to supply the working current/voltage.", "jayess:\nIt works, lol! Thank"], "answer": [], "question_code": ["Run loop error: [Error 5] Input/output error\n", "[WARN] [1545576797.165756]: Last read step: data checksum\n[WARN] [1545576797.168735]: Run loop error: [Error 5] Input/output error\nTraceback (most recent call last):\n  File \"/home/wolf/Mechanical_Practice/catkin_ws/src/rosserial/rosserial_python/nodes/serial_node.py\", line 89, in <module>\n    client.run()\n  File \"/home/wolf/Mechanical_Practice/catkin_ws/src/rosserial/rosserial_python/SerialClient.py\", line 554, in run\n    self.port.flushInput()\n  File \"/usr/lib/python2.7/dist-packages/serial/serialutil.py\", line 532, in flushInput\n    self.reset_input_buffer()\n  File \"/usr/lib/python2.7/dist-packages/serial/serialposix.py\", line 566, in reset_input_buffer\n    termios.tcflush(self.fd, termios.TCIFLUSH)\ntermios.error: (5, 'Input/output error')\n", "101010", "Ctrl-k"], "url": "https://answers.ros.org/question/311408/rosserial-kinetic-error-5-inputoutput-error/"},
{"title": "hokuyo UST-10lx LiDAR data not consistent", "time": "2018-12-20 23:41:48 -0600", "post_content": [" ", " ", " ", " ", "Hi: I am using hokuyo UST-10LX LiDAR on my robot moving in an indoor environment. I found the data is not consistent. At one moment, the data is like the left part of the image below:", "The robot is the green bar in the image, moving at a slow speed, about 10mph, toward the bottom of the image. Yellow dots are LiDAR points. One side of the wall is pointed by the red array and is at the correct location in this image. The wall is kind of at the back of the robot. The LiDAR has 270 degree view.", "At the next moment, however, the data is like the right part of the image above. The wall location is too close to the robot and is totally wrong.", "Anyone knows why?", "At first glance this looks like it may be a hardware issue. Is the lidar being subjected to any vibration when the robots moving? Also does the lidar remain distored permanently when this happens or is it a transitory problem?", "what do you mean by \"distored\"", "Sorry, typo. I meant distorted.", "Hello, I have exactly the same problem with this exactly same model mounted on a clearpath's ridgeback platform. And actually I am getting this ghost data at some positions when the robot is stationary (no external vibrations). Any idea if Hokuyo scanners are problematic?"], "answer": [], "url": "https://answers.ros.org/question/311272/hokuyo-ust-10lx-lidar-data-not-consistent/"},
{"title": "Robot Localization (Odom + IMU) - Persistent Error in Yaw", "time": "2018-11-21 19:51:52 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone,", "I have been trying to fuse my robot's odometry and IMU using EKF. However, I noticed that the filtered yaw was always lesser than the actual yaw of the robot. I tried rotating my robot on the spot and comparing the yaw published by both the IMU and the filtered odometry.   ", "In the image, the green arrows are from the EKF filtered odometry while the gold arrows are produced by directly taking the imu's yaw. When the robot has made one full rotation, the imu's arrows form a complete circle as expected but the filtered odometry's yaw stops short of a full rotation. This error slowly builds up and increases the more I rotate the robot. ", "This is my EKF params configuration file:"], "answer": [" ", " ", " ", " ", "Thank you for providing sample input messages. It might be helpful if you could capture two messages from when the robot is actually being rotated.", "A few possibilities spring to mind.", "This error slowly builds up and increases the more I rotate the robot.", "This is to be expected, regardless of what you change. Your IMU's yaw is coming from a magnetometer (I assume), so it won't be subject to drift. In the EKF, you are only fusing yaw ", ", and the errors in that velocity are going to integrate over time.", "EDIT in response to updated question:", "I am still rather confused about the base_footprint->imu transform though. May I ask why this transform is necessary and just the /imu topic itself is insufficient? Also, if there is to be more than one imu, then wouldn't there be multiple imu frames needed? I do not recall seeing any options to set the imu frame.", "I'd recommend that you have a look at the ", ", but just as a short example, consider a system where you have two identical IMUs mounted on your robot. One is mounted upside-down, and the other is mounted right-side-up. If we just fused their data in the EKF, then when we rotate our robot clockwise, one sensor would read +Z angular velocity, and the other would read -Z angular velocity. That obviously doesn't make sense, but how can the EKF know that one of the sensors is upside-down?", "The answer is that you need to use transforms to tell the EKF how that sensor is mounted. So you define a transform from ", " to ", ", and it might be just the identity transform (which means the IMU is mounted at your robot's body origin). In that IMU's mesages, you would want to make sure that the ", " was set to ", ". Then the EKF knows to look for a transform (via ", ") from that ", " to the ", "s that it knows about. For the other IMU, you would define a transform from ", " to ", ". That transform would have a roll angle of ", " radians. Then you would configure the IMU node to change the ", " in the message to ", ". When the EKF receives that message, it can again use ", " to look up how to transform that data into a frame that it can use.", "Thank you Tom, I have updated my question with some results and thoughts. To summarize: removing ", " solved the yaw error problem, which then exposed the problem of IMU data not being fused. This is then solved by introducing a static transform from ", " -> ", ".", " Hello Tom ", ", can you have a look at this question, please. It is very similar to this one (problem is the same)\n ", "  \nThank you in advance "], "answer_details": ["Given the much smaller covariance for the IMU (which makes sense), I wonder if your wheel encoder odometry is producing the wrong sign when turning. One thing you didn't show in your image was the rotation for raw odometry.", "What is providing the ", "->", " transform? Without it, your IMU data is not going to get fused. ", "You should turn off the ", " parameter while you sort this out.", " ", " ", " ", " "], "question_code": ["frequency: 20\nsensor_timeout: 0.05\ntwo_d_mode: true\ntransform_time_offset: 0.0\ntransform_timeout: 0.0\n\nprint_diagnostics: true\ndebug: false\ndebug_out_file: /path/to/debug/file.txt\npublish_tf: true\npublish_acceleration: false\n\n\nmap_frame: map              # Defaults to \"map\" if unspecified\nodom_frame: odom            # Defaults to \"odom\" if unspecified\nbase_link_frame: base_footprint  # Defaults to \"base_link\" if unspecified\nworld_frame: odom           # Defaults to the value of odom_frame if unspecified\n\n\nodom0: /odom\nodom0_config: [false,  false,  false,\n               false, false, false,\n               true, true, false,\n               false, false, true,\n               false, false, false]\nodom0_queue_size: 2\nodom0_nodelay: false\nodom0_differential: false\nodom0_relative: false\n\n\nimu0: /imu\nimu0_config: [false, false, false,\n              false,  false,  false,\n              false, false, false,\n              false,  false,  true,\n              false,  false,  false]\nimu0_nodelay: false\nimu0_differential: false\nimu0_relative: true\nimu0_queue_size: 5\nimu0_remove_gravitational_acceleration: false\n\n\nuse_control: true\nstamped_control: false\ncontrol_timeout: 0.1\ncontrol_config: [true, false, false, false, false, true]\n\nacceleration_limits: [2.6, 0.0, 0.0, 0.0, 0.0, 6.5]\ndeceleration_limits: [2.6, 0.0, 0.0, 0.0, 0.0, 6.5]\nacceleration_gains: [0.8, 0.0, 0.0, 0.0, 0.0, 0.9]\ndeceleration_gains: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n\n\nprocess_noise_covariance:\n [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0.05, 0,     0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0.350, 0,     0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0,     0.010, 0,    0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,\n  0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0 ..."], "answer_code": ["base_footprint", "imu", "use_control", "frame_id", "tf2", "frame_id", "frame_id", "pi", "frame_id", "imu_upsidedown", "tf2", "use_control", "base_footprint", "imu"], "url": "https://answers.ros.org/question/309077/robot-localization-odom-imu-persistent-error-in-yaw/"},
{"title": "Poitcloud is published at 2Hz when using 2 3D cameras gazebo_ros_depth_camera", "time": "2019-01-14 11:43:38 -0600", "post_content": [" ", " ", "Hello,", " I have a Gazebo simulation using ROS and in my mobile robot I am using two 3D cameras described by a gazebo_ros_depth_camera plugin ( ", " .). I have setup the plugin as follows: ", "Although I have the cameras setup at 200 Hz update frame rate, the publishing pointcloud rate (e.g. rostopic hz /camera3D_2/points) is between 4 to 7 Hz. Also, when I launch rviz, the publishing rate drops to 2 Hz. Do you think this is relative to some short of Gazebo physics update frequency setup, or maybe is relevant to my computer hardware resources? Any Ideas?", "Thank you!", "The depth camera plugin is computationally expensive (in my situations, the depth camera instances takes more CPU than the physics and rendering engines combined) so its no shock that its inconsistent or slow in \"real-time\", though it may be correct in the scaled time gazebo is working in...", "Try on a more powerful computer or server, write your own plugin to give you only the things you need, etc", "Ok! Thank you for your answer. It makes sense. :)"], "answer": [], "question_code": ["<plugin name=\"camera3D_controller\" filename=\"libgazebo_ros_depth_camera.so\">\n            <robotNamespace>/</robotNamespace>\n                <alwaysOn>true</alwaysOn>\n            <!-- Keep this zero, update_rate will control the frame rate -->\n            <updateRate>200.0</updateRate>\n            <cameraName>camera3D_1</cameraName>\n             <imageTopicName>image_raw</imageTopicName>\n            <cameraInfoTopicName>camera_info</cameraInfoTopicName>\n            <depthImageTopicName>depth/image_raw</depthImageTopicName>\n            <!-- neither camera info is getting published, frame_id is empty\n                in points and both image headers -->\n             <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n            <pointCloudTopicName>points</pointCloudTopicName>\n            <frameName>camera3D_1_link</frameName>\n            <!-- TODO(lucasw) is this used by depth camera at all? -->\n             <hackBaseline>0.07</hackBaseline>\n            <pointCloudCutoff>0.001</pointCloudCutoff>\n            <distortionK1>0.0</distortionK1>\n            <distortionK2>0.0</distortionK2>\n            <distortionK3>0.0</distortionK3>\n            <distortionT1>0.0</distortionT1>\n            <distortionT2>0.0</distortionT2>\n  </plugin>\n"], "url": "https://answers.ros.org/question/312715/poitcloud-is-published-at-2hz-when-using-2-3d-cameras-gazebo_ros_depth_camera/"},
{"title": "ros_serial loses sync with Arduino only when motors are running", "time": "2019-01-13 00:41:55 -0600", "post_content": [" ", " ", " ", " ", "We have an Intel NUC communicating with an Arduino Mega using ros_serial. There are several motors attached to our bot.   The motors are controlled using a ", " which takes a PWM value from the Arduino. When we are sending messages to Arduino while keeping the motors off, everything is working perfectly. We can send as many messages as we want, as frequently as we want and with any values of PWM. But when we are trying to send a message when all the motors are powered, ros_serial loses sync after a few messages. We are unable to debug this issue since the only apparent change for us is the current drawn by the motors. Can anyone help us with this? We are using ROS Kinetic with Ubuntu 16.04 ", " Edit: Link to circuit flowchart:  ", "The motors and the driver don't just draw current from the power supply, they create electrical noise that can cause issues as you're experiencing. Without more info about your setup though it would be hard to guess what is causing issue.", "Schematic available?", "Is your Arduino powered by the same power line as the motors? I experienced a lot of blackouts with my Arduino when motors are operational due to power fluctations created due to the back EMF.. Also, electrical / inductive interference is a big possibility if your motors are big enough.", ", that's our guess too but we don't know how to fix it. I added a link to the circuit flowchart in the question.", ", the Arduino is powered by the USB port connected to the onboard computer (NUC) but they all share the same ground. Could that be a reason? There are other USB devices connected to the NUC like cameras and IMU and they are working fine.", "I would look at the potential ground link between the motors, motor controllers and the Arduino. If you have a scope handy I'd hook it up and see how noisy that part of your ground is. Potential solution could be to opto-isolate the PWM signals between your Arduino and motor controllers.", "This remove the need for a direct ground link between the controllers and the Arduino", ", what Pete said, that's the only relation between your motors and Arduino. Apart from that, it could be inductive or capacitive noise depending on how big the motors are."], "answer": [], "url": "https://answers.ros.org/question/312601/ros_serial-loses-sync-with-arduino-only-when-motors-are-running/"},
{"title": "Intel realsense D435 Pointcloud2 without RGB", "time": "2018-11-20 10:36:21 -0600", "post_content": [" ", " ", " ", " ", "Dear all, ", "I was wondering if there is a way to have access to the ", " instead of the ", " in the ", " ROS Package. ", "The idea is to use as less as possible the CPU computational power.", "I have looked at ", " but it was not working properly.", "Thanks,"], "answer": [" ", " ", "I got the solution in the last version of the  ", ".", "The following configuration should be defined in order to achieve the PointCloud withou RGB:", "Unfortunately the current version does not support the ", " therefore the result is not the same as I wished cause when you choose the ", " the PointCloud will have more than twice of Points which leads to have even a higher CPU consumption with respect to the PointCloud with RGB.", " ", " ", "we meet similar issue in ros2_object_analytics,  subscribe pointcloud xyzrgb and split to rgb and xyz. to reduce transport bandwidth.", "The code for reference.\n", ", ", "Thanks a lot for your answer. I will try it and if it solves my problem I will mark it as an answer. ", "But I really like to deactivate the XYZRGB PointCloud2 from the driver source.", " ", " ", "My fork will do this for you ", ". ", "Also, if you're just looking for something to work, I release a snap package as well. Source is ", ", but available on the snap store. Install instructions for the snap store are in the readme as well.", "The ros realsense drivers haven't been stable since the r200's so if you're trying to do other things, I would consider forking and working on it.", "Thanks a lot for your answer. I really like to try your fork but unfortunately I did not achieve to compile it. I use Linuxu 16.04 with Kernel 4.15.0-39-generic; ROS kinetic and the librealsense2 installed on my Linux."], "answer_code": ["<!-- Intel Realsense D435 driver -->\n<include file=\"$(find realsense2_camera)/launch/rs_camera.launch\">\n<arg name=\"pointcloud_texture_stream\" default=\"RS2_STREAM_ANY\"/>  \n<arg name=\"enable_pointcloud\"         default=\"true\"/>\n<arg name=\"enable_color\"        default=\"false\"/>\n<arg name=\"enable_depth\"        default=\"true\"/>\n<arg name=\"enable_infra2\"        default=\"false\"/>\n<arg name=\"enable_infra1\"        default=\"false\"/>\n<arg name=\"align_depth\"         default=\"false\"/>\n<arg name=\"depth_width\"         default=\"640\"/>\n<arg name=\"depth_height\"        default=\"480\"/>\n</include>\n"], "url": "https://answers.ros.org/question/308978/intel-realsense-d435-pointcloud2-without-rgb/"},
{"title": "Is that a good choice for the architecture?", "time": "2018-11-19 16:01:51 -0600", "post_content": [" ", " ", "Hello,", "I want to ask whether this idea will be good:\n1. All of my navigation and other high level algorithms will be written in ROS 2 and will be stored in cloud (AWS,Heroku).\n2. I have a sensor (more specifically camera), which will be mounted on a robot and will send the image data to my cloud via ROS2 bridge. \n3. After receiving data, further computations will be done in the cloud.", "This really depends on if you have the bandwidth to transfer enough sensor data to the could, if the latency is not going to be a problem and if you really need the processing power of the amazon cloud. Also are there any safety concerns around an unexpected loss of connection?"], "answer": [" ", " ", "This is not a good idea.", "Navigation, path-planning and path-following controllers generally requires loop rates between 10 and 100Hz, and a corresponding processing (and network) latency of less than 10 to 100mS.", "The average WiFi and 3G/4G/LTE cellular networks can barely meet these latency guarantees, and even when they do, there isn't a lot of latency budget left over to do computation. If the AWS or Heroku servers are more than a few hundred kilometers away, the round-trip time to the servers and back can start to add up, too!", "You might be able to get acceptable performance if you work directly with a wireless carrier to get dedicated bandwidth and latency guarantees, or if you control your local wifi network and communicate with servers that are in the same building, but those options are out of reach for most users.", "It is possible to offload some of your very heavy processing to cloud services, but keep in mind that the latency will be high. This could be good for things like recognizing objects, updating a very large map shared between many robots, or a variety of other things that aren't terribly sensitive to latency.", "What will be your suggestion? Basically, we have a low level stuff happening in the cloud (actually moving the wheels and etc) and latency there will not be a problem. So should I get the hardware to implement navigation locally? What will be a good choice here?", ":", "Basically, we have a low level stuff happening in the cloud (actually moving the wheels and etc) and latency there will not be a problem", "can you clarify this sentence? Are you saying you already have this working?", " What I mean is that the actual wheel encoders are already implemented in the cloud. So I was thinking to implement navigation internally in a robot and use some socket communication to obtain the wheel encoder values (for odometry), do navigation and send the required motions back to cloud", "What sort of connections are you using? Decoding encoder signals is typically something done with cycle times of milliseconds or less. How are you delegating that to a cloud instance? Or are you describing a system in which a base driver publishes ", "s (for example) and a remote node ..", ".. subscribes to that and publishes the conversion to engineering units (and proper ROS msgs)?"], "answer_code": ["int"], "url": "https://answers.ros.org/question/308921/is-that-a-good-choice-for-the-architecture/"},
{"title": "Issues with odometry in robot_localization", "time": "2018-11-30 10:07:25 -0600", "post_content": [" ", " ", " ", " ", "I'm using the robot_localization package with ROS Kinetic and I'm trying to fuse the IMU and GPS data from the Matrice M100 similar to the work done ", ". ", "My issue is the odometry/gps topic looks wacky and isn't in line with what I expect the output to be. If you take a look at the picture below, red is the real drone odometry, purple is the ", "  and green is ", ". I can't upload files yet but you can see it at this ", ". Occasionally, I experience jumps but nothing too big at all. I recorded a bag file and plotted the coordinates of the drone and raw GPS and ", " seem to be close enough as seen ", ": however, the 3d position messages on the odometry topic is inaccurate as shown ", ":  I'm not sure if I'm doing something wrong in the configuration . ", "I've also attached a copy of the bag file ", "  Any help is appreciated.", "Here's a sample of my dual_ekf_navsat file: "], "answer": [" ", " ", " ", " ", "Please post sample messages from every sensor input.", "Can you explain what is wacky in the images you posted? It looks to me that the state estimate is closely tracking your odometry, and that the GPS odom values are just bound to the ground plane. You have ", " set to ", ", so that makes sense. And remember that there are no headings associated with the GPS poses, so they won't look the same.", "Anyway, another issue with your configuration is that you are fusing absolute X, Y, and Z position from your odometry, as well as absolute X, Y, and Z position from your GPS data. If your odometry is subject to drift (and I assume it is), then you will reach a point where those positions don't match. When that happens, the filter will start leaping back and forth between the two measurement sources. An alternative symptom - and I think you're seeing it - is that if your odometry covariance is much smaller than your GPS position covariance, the filter is going to effectively follow the odometry values. But again, I can't really say that with any certainty until you post sample messages.", "Also, it looks like you left the ", " ", " comment in there from the sample config file, but changed the ", " value to 0. Does your IMU read 0 facing east?", "Finally, if your odometry source is internally integrating velocity data to get position, and then you fuse ", " the position and velocities in the EKF, you're biasing the EKF. If I were you, I'd set the first three values (X, Y, and Z) to false in your EKF configuration for ", ".", ":", "OK, a few things:", ", Edited the original question."], "answer_details": ["Your ", " data is given in the ", " frame. I assume a transform exists from ", " to ", "?", "You ", " topic is in the ", " frame. I assume you have a transform from ", " to ", "?", "Your ", " pose data is given in the ", " frame, yet you are trying to fuse it absolutely (i.e., the pose data, and not the velocity data) into your ", " frame EKF. That's going to require the ", "->", " transform, which the ", " frame EKF is itself producing. Since you have an IMU providing your orientation, I would recommend that you fuse ", " velocity data from ", " in both your EKF instances. That, or (a) change the ", " to ", " in your ", " topic, (b) fuse only the pose data from ", ", and (c) fuse only velocity/acceleration data from the IMU (a, b, and c are for the ", " frame EKF; for the map frame, I'd flip them and use velocity from the odometry and orientation from the IMU).", "The ", " component of a quaternion isn't the same as the yaw component of orientation, so you'll need to convert that quaternion to Euler angles to get the offset. My guess is that you just want ", " ", " ", " ", " "], "question_code": ["odometry/filtered_map topic", "odometry/gps", "gps/filtered", "ekf_se_odom:\n  frequency: 30\n  sensor_timeout: 0.1\n  two_d_mode: false\n  transform_time_offset: 0.0\n  transform_timeout: 0.0\n  print_diagnostics: true\n  debug: false\n\n  map_frame: map\n  odom_frame: odom\n  base_link_frame: m100/base_link\n  world_frame: odom\n\n  odom0: dji_odom\n  odom0_config: [true, true, true,\n                 true, true, true,\n                 true,  true,  true,\n                 true, true, true,\n                 true, true, true]\n  odom0_queue_size: 10\n  odom0_nodelay: true\n  odom0_differential: false\n  odom0_relative: false\n\n  imu0: imu/data\n  imu0_config: [false, false, false,\n                true,  true,  true,\n                false, false, false,\n                true,  true,  true,\n                true,  true,  true]\n  imu0_nodelay: false\n  imu0_differential: false\n  imu0_relative: false\n  imu0_queue_size: 10\n  imu0_remove_gravitational_acceleration: true\n\n  use_control: false\n\n  process_noise_covariance: [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0.06, 0,     0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0.025, 0,     0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0.025, 0,    0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0.02, 0,    0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0.01, 0,    0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0.01, 0,\n                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0 ..."], "answer_code": ["zero_altitude", "navsat_transform_node", "yaw_offset", "yaw_offset", "dji_gps", "gps_link", "imu_data", "dji_odom", "dji_odom", "frame_id", "dji_odom", "dji_odom", "w", "pi ..."], "url": "https://answers.ros.org/question/309717/issues-with-odometry-in-robot_localization/"},
{"title": "Turtlebot3 burger is not charging [closed]", "time": "2019-01-17 08:19:22 -0600", "post_content": [" ", " ", "Hi,", "We are two students working on a project involving several turtlebot3 burger. We never used turtlebots before, and we are experiencing dufficulties to charge them. We do as requested in the online manual : unplug the battery, take it out of the robot, plug the white part into the blue charger and then connect the charger to electricity. We tried it on every single battery and with every blue charger we have, it doesn't work. The red light on the blue charger is off.\nAre we doing something wrong ? Is there a problem with our robots ?"], "answer": [], "url": "https://answers.ros.org/question/313024/turtlebot3-burger-is-not-charging/"},
{"title": "How to modify move_base minimum velocities?", "time": "2019-01-16 04:26:04 -0600", "post_content": [" ", " ", " ", " ", "I am using navigation stack in order to navigate my robot. I know that after giving a goal, move_pase constructs global and local plans. Using local plan, move_base publisher to /cmd_vel topic in order to move my robot.", "I have written yaml file to set the parameters for the local plan like this:\necovery_behavior_enabled: true\nclearing_rotation_allowed: true", "As you can see, I specifically said that the minimum x vel is 0.8 because from my motor speed calculations:", "When the x.linear velocity is small my wheels do not spin as it has not enough power. However, after setting a goal and listening to /cmd_vel topic I get this message:", "Why is linar X is below 0.8? having those values, from my callback formula it will only deliver 20% of duty cycle and it is not enough.", "My move_base launch file:", "Where are you using this callback? Maybe you are overwriting the cmd_vel somewhere. The local planner will limit the cmd vel to 0.8 in its output but that can be overwritten, especialy if topics are renamed. Can you also post the contents of your move_base.launch?", " I can confirm that no other topic publishes to cmd_vel (by rostopic info ...). I have also added the move_base launch\nI am using the callback inside other package where I control my motor", "Please make sure that no one access cmd_vel while move_base is using it. \nKeyboard teleop is always flooding 0 velocities too.", "For the left and right power, you might need to have the effort multiplier to tune cmd_vel to equal actual speed - by physically or visually measuring on actual robot", "or use the wheel encoder output topic (/odom) to measure how exactly the wheel speed is and try to multiply the power to match the output velocities>", " Yes I did tune the velocity and I got around 0.5 m/s (0.47 -> 0.52) on 100% duty cycle. I just tried again on my PC and it worked, will try again on Embedded", "A comment: move base outputs velocity commands. It sounds like you're using it as a torque command. Not the same thing. Your robot is responsible for outputting whatever duty cycle is required to meet the velocity requirements. You may get it to work, but unlikely to work well. Do you have encoders?", " Then how would you recommend me using the velocity commands and duty cycle? Yes I do have encoders, optical ones. Can you answer below? As I have wasted a lot of time trying to figure out how to combine those.", " Also, I managed to get the velocity restrictions, just rewrote everything."], "answer": [], "question_code": ["TrajectoryPLannerROS:\n  acc_lim_x: 1\n  acc_lim_y: 0 # as we have a differential robot\n  acc_lim_theta: 1.6\n  max_vel_x: 1.6 # configure, was 0.2\n  min_vel_x: 0.8 # was 0.1\n  max_vel_y: 0.0  # zero for a differential drive robot\n  min_vel_y: 0.0\n  max_vel_theta: 1.0 # 0.35\n  min_vel_theta: 0.0 #-0.35\n  min_in_place_vel_theta: 0.4 #0.25\n  min_rot_vel: 0.1\n  escape_vel: 0.0\n\n  holonomic_robot: false\n\n  meter_scoring: true\n\n  xy_goal_tolerance: 0.3 # 0.15, 0.3->about 20 cm\n  yaw_goal_tolerance: 0.20 # 0.25, about 11 degrees\n  latch_xy_goal_tolerance: true  #false\n\n  sim_time: 4 # <2 values not enough time for path planning and can't go through narrow\n  sim_granularity: 0.05 # How frquent should the points on trajectory be examined\n  angular_sim_granularity: 0.025\n  vx_samples: 5\n  vy_samples: 0 # 0 for differential robots\n  vtheta_samples: 5\n  controller_frequency: 15\n\n  pdist_scale: 0.8\n  gdist_scale: 0.4\n  occdist_scale: 0.01\n  oscillation_reset_dist: 0.2\n  heading_scoring: false\n  heading_lookahead: 0.1 # how far to look ahead in meters\n  heading_scoring_timestep: 0.8\n  dwa: true\n  publish_cost_grid_pc: false\n  global_frame_id: odom\n  simple_attractor: false\n\n  reset_distance: 4\n\n  prune_plan: true\n", "def callback(self, msg):\n    transVelocity = msg.linear.x\n    rotVelocity = msg.angular.z\n    velDiff = (self.wheelSep * rotVelocity) / 2.0\n    leftPower = (transVelocity + velDiff) / self.wheelRadius\n    rightPower = (transVelocity - velDiff) / self.wheelRadius\n    self.pwm_left.ChangeDutyCycle(leftPower)\n    self.pwm_right.ChangeDutyCycle(rightPower)\n", "linear: \n  x: 0.55\n  y: 0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: -0.368421052632\n", " <!-- Launch move_base -->\n   <node pkg=\"move_base\" type=\"move_base\" name=\"move_base\" output=\"screen\"> \n     <!--param name=\"base_global_planner\" value=\"simple_planner/SimplePlanner\"/-->\n   <rosparam file=\"$(find map)/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n   <rosparam file=\"$(find map)/config/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n   <rosparam file=\"$(find map)/config/local_costmap_params.yaml\" command=\"load\"/>\n   <rosparam file=\"$(find map)/config/global_costmap_params.yaml\" command=\"load\"/>\n   <rosparam file=\"$(find map)/config/trajectory_planner.yaml\" command=\"load\"/>\n   <rosparam file=\"$(find map)/config/move_base_params.yaml\" command=\"load\"/>\n   <rosparam file=\"$(find map)/config/global_planner_params.yaml\" command=\"load\"/>\n   </node>\n"], "url": "https://answers.ros.org/question/312892/how-to-modify-move_base-minimum-velocities/"},
{"title": "Robot reverses at high speed before following path given by ros nav stack", "time": "2019-02-28 16:07:18 -0600", "post_content": [" ", " ", "Hi,", "I am using ros nav stack in conjunction with google cartographer (Mapping and localization) to navigate the robot through a known map. Right now, the robot follows the path generated with acceptable accuracy. But,often, once the path has been generated, the robot reverses at the highest speed set in the params file (escape_velocity parameter), and then starts to move forward correctly on the genrated path.", "I have attached images of all my param file:\n1. ", "\n2. ", " This is a link to a video of how it looks on rviz.  ", "The thinner line in green is the plan generated by ros nav stack. The thicker line seen later in the video is the actual robot movement. You can see that the robot first reverses and then starts moving forward.", "I am new to this forum so please let me know if I need to give anymore data for anyone to answer this", "Has anyone else has this probelm? Will apprecite any tips on fixing this!\nThanks in advance!"], "answer": [], "url": "https://answers.ros.org/question/317088/robot-reverses-at-high-speed-before-following-path-given-by-ros-nav-stack/"},
{"title": "motors sensetivity", "time": "2019-02-20 09:58:56 -0600", "post_content": [" ", " ", "Hi,", "I have a mobile platform with two powerful dc motors than it needed. Therefore, its minimum  velocity is about 0.4 m/s and angular velocity is 1 rad/s (minimum) that is higher that the normal platforms in the market. \nThe problem is that in the navigation, the \"cmd_vel\" commands that sent are start in very small values like 0.05 m/s and same values in the angular velocities commands.\nIt is possible to limit the minimum linear velocity in the yaml file but the minimum angular velocity it is not possible.\nBecause of that, my platform can't move constantly and behave in strange ways like it is lose is trajectory and can't return to it again.\nSomeone has an idea what I can do to solve that issue.", "Thanks", "I would strongly recommend looking at your motor controllers so that you can have a better control of the speed of your wheels. This is not a problem caused by having powerful motors, but a problem caused by your controllers not being good enough.", "Hi,\nThe problem is not the controller for sure. These motors came with a worm gear. \nIt is like to put a track engine in motor cycle ;)", "Can you describe how you are controlling the motors and what type feedback you're using? Peter is correct that you need to look at the control scheme. Perhaps the controllers are good but just not being used in the best way. There is no question it is a control issue when you can't control the speed", "I am using roboteq controller with serial connection which get the data by arduino. It get an HEX value and I have about 15 value that I can used because it is like I send int values. If I set low value it won't move and high value it will move fastly. So I can work in this range only.", "The minimum power which the motor starting produce 0.3 m/s velocity. I can't, with this controller at least to provide lower velocity. I have a platform with 4 wheels that 2 wheels are connected togheter so it is an ddition problem with the navigation", "OK I see the issue. You do not have encoders for the controller to use to control the motor speed and you're dealing with stiction in the motor that requires a high voltage command to get moving. Once moving it's going too fast...continued...", "If you can't add encoders (do so if you can) you may have some success with adding a short high voltage pulse at the start of motion to get the motor turning and then revert to the command level coming from ROS. Even a short pulse (50mS) may be able to improve function. But encoders would be better.", "Yes. Like you said. The motors required high voltage to get moving and the can provide minimum velocity of 0.3 m/s with a small resolution for changes between the minimum to the maximum velocity. In your opinion I can get from that condition a good navigation?"], "answer": [], "url": "https://answers.ros.org/question/316218/motors-sensetivity/"},
{"title": "How to use ACADO in ROS", "time": "2018-03-01 02:48:41 -0600", "post_content": [" ", " ", "Hi everyone, ", "i am new to the ROS community but I already like ROS a lot, its so powerful and convenient to use. ", " For a recent project of mine, i am looking for a possibility to run ACADO codes as a node, which should then take over the task of state estimation of my robot. I already got my hands on ACADO using its MATLAB interface and know a little about it. When browsing on the Internet, i noticed that ROS provides support for ACADO!!!!! I was so happy to see this, but the docu page  ", "  doesn't show much about instructions on how to use ACADO within ROS. Can someone who has already worked with ACADO in ROS, or anyone who knows something about the usage please help me getting started with this great package. Any help would be appreciated! ", "Thank you all in advance. ", "I believe the reason that you're not finding much documentation is because ", " is only a ", " release of the upstream (ie: non-ROS-specific) regular library. It's not a full integration with ROS (as much as that makes ..", ".. sense).", "Clearpath appears to be using it for one of their products or services, so perhaps they can provide some more information, but afaict, the ", " pkg that you linked to is just that: the library itself.", "Thanks for your info. I was hoping to use it out of the box. I did come to notice Clearpath using it. But i doubt they would provide support for non customers. Maybe there is someone who used acado in ros anyway and can provide me some insight. But ill try to figure out how to link acado libs in ros"], "answer": [" ", " ", "I've recently added acado to the buildfarm for kinetic. You should be able to install ", "."], "question_code": ["ros-indigo-acado", "acado"], "answer_code": ["ros-kinetic-acado"], "url": "https://answers.ros.org/question/284039/how-to-use-acado-in-ros/"},
{"title": "Transforming in combined tree", "time": "2019-03-08 19:54:17 -0600", "post_content": [" ", " ", " ", " ", "I have a task of using a static ur5 manipulator to detect and point at a mobile turtlebot. I am a beginner and I'm told transforms is the solution I want. (Please comment if you disagree).", "I have a ur5 and a turtlebot3 spawned into a an empty gazebo world.", "I have a static transform publisher, shown below, in my launch file which I use to combine the tf trees of both robots.", "So the merged tf tree is as follows:\n", "The green bordered section on the right of the image is the turtlebot3 tf tree. Everything else is the ur5 tf tree. \nAs my static transform code shows, I attached the turtlebot tree to the ur5 tree at the world node.", "Further down the ur5 tree I highlighted in red the ", " node which is the node of origin for my attempted transform. The destination node is the ", " node, the second and last node on the turtlebot3 side of the tree.", "However, my ", " attempts at return ", " (false). So as you would expect lookup_transforms errors:", "And, just to test, I tried a ", " between nodes entirely on the ur5 side of the tree, i:e base_link to world, but still the result was false.", "I'm not sure what I'm missing and/or doing wrong. I would appreciate some help."], "answer": [" ", " ", "You're 99% of the way there, there is just one detail of how transform listeners work you're missing. ", "You cannot create a transform listener then query it for transforms straight away, you need to give it a second or more otherwise it will fail to find a transform as in your case. To fix this you can add a one second sleep after you create the listener object then this should start working. ", "The reason is that the tf listeners needs to build up a buffer of tf messages before it is aware of all the transforms and is able to correctly answer tf queries. ", "Hope this helps. ", "Thank you buddy ;-)"], "question_code": ["<node pkg='tf2_ros' type='static_transform_publisher' name='ur5_turtlebot3_transform' args='0 0 0 0 0 0 1 world odom'/>\n", "def __init__(self):\n\n    # declarations\n    self.robot = moveit_commander.RobotCommander()\n    self.scene = moveit_commander.PlanningSceneInterface()\n    self.manipulator = moveit_commander.MoveGroupCommander('ur5_bot')\n\n    self.tf_buffer = tf2_ros.Buffer()\n    self.tf_listener = tf2_ros.TransformListener(self.tf_buffer)\n\n    try:\n        # transform_stamped = tf_buffer.lookup_transform('wrist_3_link', 'base_footprint', rospy.Time())\n        transform_stamped = self.tf_buffer.can_transform('wrist_3_link', 'base_footprint', rospy.Time())\n        rospy.loginfo(transform_stamped)\n    except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:\n        rospy.logerr(e)\n"], "url": "https://answers.ros.org/question/318078/transforming-in-combined-tree/"},
{"title": "UR10 tool flange 24V power control", "time": "2019-04-02 00:13:42 -0600", "post_content": [" ", " ", "Hi, i have been working on controlling a relay with the power output of the UR10 flange connector. \ni recently got the ur_modern_driver/io_states publisher working and noticed that it does not publish the voltage that is on the output of said pin. The voltage on that pin can only be 0,12 or 24. I know that because with the teach panel i can set that output to those values. So now i do not know if i can use the ur_msgs service of SetIO.srv to set this output.", "Do i need to program this myself or is this possible? I have been looking for a long time and cannot find documentation that says if this is possible. "], "answer": [" ", " ", " ", " ", "It's not well documented, but the ", " service should allow you to configure the tool voltage when you specify the ", " for ", ", and then setting ", " to either ", ", ", " or ", ".", "Some pseudo-code (neither Python nor C++):", "This would set the tool voltage to 12V after invoking the service.", "Edit:", "I have been looking for a long time and cannot find documentation that says if this is possible. ", "I've submitted ", " to clarify the use of ", " a bit.", "Once that gets merged, the code changes slightly (constants added):", "For reference: ", " is where the driver processes the incoming service request, and ", " is where the tool voltage request is processed.", "yes that worked, thank you very much"], "answer_code": ["FUN_SET_TOOL_VOLTAGE", "fun", "state", "0", "12", "24", "req = SetIO()\nreq.fun = SetIO.FUN_SET_TOOL_VOLTAGE\nreq.state = 12\n", "SetIO", "req = SetIO()\nreq.fun = SetIO.FUN_SET_TOOL_VOLTAGE\nreq.state = SetIO.STATE_TOOL_VOLTAGE_12V\n"], "url": "https://answers.ros.org/question/320110/ur10-tool-flange-24v-power-control/"},
{"title": "unable to get Point Grey USB camera work in ubuntu", "time": "2012-11-13 07:59:06 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have problem in using POINT GREY FIREFLY MV FMVU-13S2C digital camera to work on Ubuntu 11.04.", "This is the version of ubuntu.", "As far as I know, POINT GREY FIREFLY MV FMVU-13S2C uses IEEE1394 interface to transfer data with computer. So I downloaded the Coriander \u2013 the linux GUI for IEEE1394/Firewire cameras.", "However, when I plugged in the camera and launched the coriander, this error appears:", "Warning: could not find a Digital Camera on the bus.\nPlease check that:\n-the cables are properly set\n-the devices on the bus are properly powered\n-your camera is compliant with the Digital Camera specs (see ", ").", "Then I run the command \u201clsusb\u201d:", "Apparently, the first output is the camera.", "Then I guess maybe I need to configure udev to work with camera. So I run \u201csudo gedit /etc/udev/rules.d/10-pointgrey.rules\u201d and type in the following text.", "Then I restart the udev by \u201csudo restart udev\u201d and restart coriander again. But the same error still appears. ", "Then I opened the official website of the POINT GREY and downloaded the FlyCapture2 software. Surprisingly, This software is able to recognize the camera. Why coriander cannot recognize camera? camera1394 ros node also cannot recognize camera.", "Can any help me to solve this problem? Thanks in advance!", "The Point Grey Chamelion (USB) has been reported to work with camera1394. Some USB cameras do support the IIDC digital camera interface. Perhaps this one does too. But I don't know how the driver can discover the device.", "is cheese USB camera supported? Cheese also cannot find the USB camera"], "answer": [" ", " ", "Ok, I got it work by the following steps. I think the key is to add correct udev rules and give the user right to access the udev group which FlyCapture installation will do for you.", "Install common ROS camera driver stack which includes camera1394 package which we need for Point Grey Firefly MV USB camera. \nsudo apt-get install ros-groovy-camera-drivers", "Go to ", "/ and download libdc1394\nExtract to desktop and enter in the extracted folder.\nFollow the readme file to install libdc1394", "If I may further improve on the accepted answer:", "3.1/ You can safely ignore dependencies issues with the 2 xxx_gui packages since we are only interested in drivers here (!=user interface).", "I needed an additional step:\n4/ execute flycap2-conf from the SDK package for the right udev rules", " ", " ", "Try this udev rule: ", "It'll add /dev/firefly. Regarding camera1394, we've used both the Chameleon and the Firefly with an unmodified camera1394 install.", "Hi, Ryan. I tried this udev rule and /dev/firefly had been added. But this added device doesn't work with usb_cam package which works well with my integrated camera. This is the error message: VIDIOC_QUERYCAP error 25, Inappropriate ioctl for device", "This udev rule worked but when I ran the camera1394_node I got a segmentation fault. Any ideas?", " ", " ", "We have an identical Firefly camera in our lab. Point Grey provides a library to interface with the camera as it does not work like a normal webcam, at least not as far as I can tell. A guy in our lab wrote a wrapper around that library which basically grabs the image data via the library and publishes it via ROS. I can try and see if I get the code in a format that others can use it. I'm not sure if we can directly distribute the library provided by Point Grey but at least the wrapper should be no problem.", "There is, however, a caveat and that is that the library provided by Point Grey only works with antique versions of Ubuntu, 10.04 or something along those lines. They linked against some very dated libraries which means that newer versions of the used libraries have different symbol names.", "If no one else knows of a better way to get that camera up and running I'll see what I can do on my end to make the code available.", "Hi Lionel, I changed to Ubuntu 10.04. I found there is a pgr_camera_driver package. Is it a wrapper? I have problem make that package after I downloaded it. There is error \"error: pgr_camera/PGRCameraConfig.h: No such file or directory\".", " ", " ", "You said that the camera uses IEEE1394, yet you posted the result of lsusb. Isn't your camera using USB? I saw in the ", " that both interfaces are available. If that is the case, then neither coriander nor camera1394 should work.", "I just checked, your specific camera model (FMVU-13S2C) is USB", "http://www.ptgrey.com/support/kb/index.asp?a=4&q=17&ST=   says \"Point Grey USB 2.0 cameras are designed to implement the IIDC 1394-based Digital Camera Specification. Beginning with version 2.1.0, you can use the libdc1394 package to program applications against Point Grey USB cameras.\"", " ", " ", " ", " ", "In my case, the problem was with my user's groups. Although ", " and ", " showed ", " among the others (the group that was used in the udev rules), running just ", " or ", " showed that I didn't have that group.", "If this matches your case, I know of 2 possible causes:", " ", " ", "Follow ", " answer. I provide here for reference my udev rules written by the SDK config executable \"flycap2-conf\". It writes 40-pgr.rules", "Notice the difference with other given udev rules ", " becomes ", ".", "Furthermore I'm using Ubuntu 14.04.4 with ros-indigo-pointgrey-camera-driver.\nCamera is"], "answer_details": ["Go to ", " and download FlyCapture v2.3 for Firefly MV 13S2C\nExtract to desktop and enter in the extracted folder.\nFollow the readme file to install FlyCapture", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "You haven't logged out / logged in since the group was added.", "You were hit by a bug in lightdm: ", ", ", ".", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["adrian@ubuntu:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 11.04\nRelease:    11.04\nCodename:   natty\n", "adrian@ubuntu:~$ lsusb\nBus 003 Device 002: ID 1e10:2002 Point Grey Research, Inc. \nBus 003 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 002 Device 004: ID 17ef:1003 Lenovo Integrated Smart Card Reader\nBus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 007: ID 04f2:b217 Chicony Electronics Co., Ltd \nBus 001 Device 006: ID 0a5c:217f Broadcom Corp. Bluetooth Controller\nBus 001 Device 005: ID 147e:2016 Upek Biometric Touchchip/Touchstrip Fingerprint Sensor\nBus 001 Device 004: ID 046d:c52b Logitech, Inc. Unifying Receiver\nBus 001 Device 003: ID 0765:5001 X-Rite, Inc. \nBus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n", "BUS==\"usb\", SYSFS{idVendor}==\"1e10\", SYSFS{idProduct}==\"2002\", GROUP=\"plugdev\"\n"], "answer_code": ["SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2000\", GROUP=\"plugdev\", SYMLINK+=\"firefly\", MODE:=\"0666\"\n", "groups username", "id username", "flirimaging", "groups", "id", "ATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2000\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2001\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2002\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2003\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2004\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"2005\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3000\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3001\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3004\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3005\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3006\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3007\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3008\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"300A\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"300B\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3100\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3101\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3102\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3103\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3104\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3105\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3106\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3107\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3108\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3109\", MODE=\"0664\", GROUP=\"pgrimaging\"\nATTRS{idVendor}==\"1e10\", ATTRS{idProduct}==\"3300\", MODE=\"0664\", GROUP=\"pgrimaging\"\nKERNEL==\"raw1394\", MODE=\"0664\", GROUP=\"pgrimaging\"\nKERNEL==\"video1394*\", MODE=\"0664\", GROUP=\"pgrimaging\"\nSUBSYSTEM==\"firewire\", GROUP=\"pgrimaging\"\nSUBSYSTEM==\"usb\", GROUP=\"pgrimaging\"\n", "[0]Serial: 14435635, Model: Grasshopper3 GS3-U3-23S6C, Vendor: Point Grey Research, Sensor: Sony IMX174 (1920x1200 CMOS), Resolution: 1920x1200, Color: true, Firmware Version: 2.8.3.0\n"], "url": "https://answers.ros.org/question/48244/unable-to-get-point-grey-usb-camera-work-in-ubuntu/"},
{"title": "Have two nodes publish to same topic", "time": "2019-03-18 12:01:25 -0600", "post_content": [" ", " ", "So I have two nodes publishing to the topic ", " and I want one to have priority over the other. Right now it just switches between the two. Is there a way to have them both publish then assign priority to one?", "More info on my particular problem. I have a exploration node sending to the topic, then have another node which sends only when battery is too low. I don't want to change anything in the exploration alghoritm, just have the battery node publish to the topic ", " and overwrite what the exploration node publishes."], "answer": [" ", " ", "It sounds like multiplexing may help. From the ", " is a ROS node that subscribes to a set of incoming topics and republishes incoming data from one of them to another topic, i.e., it's a multiplexer that switches an output among 1 of N inputs. Services are offered to switch among input topics, and to add and delete input topics. At startup, the first input topic on the command line is selected.", "To use it with ", ":", "and to use it with ", " see ", ".", "I solved the problem. What I did was to change the topic names of the two nodes then simply have a new control node subscribe to both of those topics. This control node would then decide which of the two topics to publish. From what I can read this method is very similar to what jayess described above, and that method seems easier than creating a whole new node.", "You can also consider using nodelets for this as it provides features as zero copy of data and reduced networking (", "). There is already nodelet for similar use case and that is "], "answer_code": ["mux", "rosrun", "rosrun topic_tools <outopic> <intopic1> [intopic2...] [standard ROS arguments]\n", "roslaunch"], "url": "https://answers.ros.org/question/318738/have-two-nodes-publish-to-same-topic/"},
{"title": "3D Obstacles \"forgotten\" if there is a gap between 3d camera FoV and the robot base", "time": "2016-04-04 17:44:36 -0600", "post_content": [" ", " ", "What's an algorithm or technique that doesn't forget about obstacles in the following situation? ", "I've observed obstacles mistakenly cleared using the VoxelLayer costmap_2d plugin on ROS Jade. This happens when there is a blindspot on the floor between what the 3d camera sees and the robot due to the camera's position. ", "Here's a side view of a ToF camera with a 90 degree field of view. The red dots are points in the point cloud sensed by the camera. The doted lines are rays from the points to the camera.", "Here's a view of how VoxelLayer marks the voxels.  Voxels can be free (green), occupied (yellow), or unknown (white). Voxels are cleared (set to free) when a ray passes through them. Voxels are marked (set to occupied) when a point is inside them only if the point is higher than the parameter min_obstacle_height.", "Now imagine the depth camera has moved left about two cells. The rays for the points on the ground pass through the previously marked voxels and clear them. The robot has effectively forgotten about the box in front of it.", "The robot then attempts to crush the box. Ideas?"], "answer": [" ", " ", "An approach we've used in the past for this is  to limit the clearing rays to be not the full width of the sensor scan, but only a subset to avoid clearing partial voxels. It's more conservative, but will require better coverage with your sensors.", "That's why you can configure different input topics for marking and clearing separately. You can filter the raw scan to remove the edges and label it for clearing only. And use the raw scan for marking."], "url": "https://answers.ros.org/question/230995/3d-obstacles-forgotten-if-there-is-a-gap-between-3d-camera-fov-and-the-robot-base/"},
{"title": "Kobuki \"malformed subpayload\"", "time": "2013-01-14 06:04:54 -0600", "post_content": [" ", " ", " ", " ", "I'm working with Turtlebot2 (with the Kobuki base) and running ROS Groovy.  I'm intermittently seeing error messages like the ones below.  Once these error messages start to appear, the robot becomes unresponsive.  Relaunching the Turtlebot by running:", "roslaunch turtlebot_bringup minimal.launch", "doesn't help.  Nor does power-cycling the robot. The only thing that seems to help is rebooting the notebook and re-launching.  This works for a few minutes until the error messages reappear.   I haven't been able to reliably reproduce the conditions that cause the error.  I'm running version 1.1.3 of the Kobuki firmware.  Any advice would be much appreciated. ", "EDIT:\nAfter the error messages above, the ftdi device associated with the Kobuki no longer shows up after an lsusb.", "Here is some additional output from dmesg in case it means anything to anyone:", "Hi nsprague! During the development of Kobuki, we once had an USB issue, which caused the netbook's USB controller to hang up. It also forced us to reboot the netbook to get it working again. Unfortunately, we were not able to reproduce this reliably. ", "The few times we did, we noticed that Kobuki's battery level was low. Could you test your netbook with Kobuki on fully loaded batteries again? My colleague is using an Acer Aspire One, but its model number is D255. He hasn't reported any issues with it, but I'll ask him again.", "This definitely wasn't an issue with the battery.  It seems to be specific to the netbook model.  It turns out that this Acer has all three external USB ports on the same USB bus.  I've been told that there are issues with putting the Kinect and the Kobuki on the same bus. ", "Ok, thanks for the feedback nsprague.", "I'm having the same exact issues with a Turtlebot2-kobuki and the netbook Acer Aspire One. Any workaround found?", "Do you have the same situation fherroro? i.e. kinect and kobuki usb's on the same bus.", "I never found a workaround.  I purchased new netbooks: Asus X201e's.  These don't have the kobuki issue described above, but they have their own issues.  Neither wired nor wireless Ethernet are well-supported under Ubuntu. ", "I just ordered brand new turtlebots and am using them on an Intel NUC model NUC5i7RYH with the Ubuntu provided on the USB stick.  I get the exact error messages.  Has any one figured out any workarounds in the last 4 years?"], "answer": [" ", " ", " We found out that the same ASUS netbook model is distributed with varying hardware (different wired and wireless network cards). Hence, driver support is difficult. Two turtlebot distributors are working hard on finding a solution. Join the turtlebot mailing list to stay up to date!", " ", " ", " ", " ", "I had the same problem with my Acer Aspire V5 netbook. It looked like the USB Controller was causing the problem, since all three USBs are running on one controller. \nAfter updating my Linux Kernel to the newest version (3.8 something), I had neither the ", " nor the ", " Error anymore!", " ", " ", "I found the same problem with PC NUC, although updating to the newest version. "], "answer_code": ["Kobuki : Timed out while waiting for serial data stream", "Kobuki : malformed sub-payload detected"], "url": "https://answers.ros.org/question/52203/kobuki-malformed-subpayload/"},
{"title": "Looking for: Ethernet based IO device", "time": "2019-04-29 04:25:22 -0600", "post_content": [" ", " ", "Hi there", "I'm working on a project where I need to communicate with a lot of different hardware by sending TTL signals and reading voltages and ampers from various sensors and planning to use ROS.", "Someone told me that there exists a IO device that connects to ROS through ethernet but I haven't been able to find it through Google.", "Anyone know if such a device exists?", "Kind regards\nFrimann Kjerulf", "If the ethernet requirement is a \"must\": ethercat, powerlink, modbus, opc-ua and some other fieldbuses use ethernet and many of those have either fieldbus \"couplers\" or slave devices that can function as IO couplers. They don't natively \"speak\" ROS but a bridge node/driver is never far away.", "Thanks I\u00b4ll look into that."], "answer": [" ", " ", "You could use a small board like a Raspberry Pi for this. Connect your sensors to the Pi, run ROS on the Pi and write a ROS node in Python or C++ to read the sensors and send ROS messages over Ethernet / Wifi."], "url": "https://answers.ros.org/question/322106/looking-for-ethernet-based-io-device/"},
{"title": "How to stop Turblebot under any emergency", "time": "2019-05-11 04:00:51 -0600", "post_content": [" ", " ", " ", " ", "Dear all,\nI am using a robot turblebot3-burger to do test. I write some test code to publish the control message (line-speed/angular-speed) to /cmd_vel. \nFollow the TB3 instruction before move the burger I need launch remotely (through ssh) on TB3 the below command:", "if I run rosnode list remotely Ican see some node are start up, so at the moment I should be ready to publish the  control message to /cmd_vel.\nMy question are: ", "Q1: as soon as I stop publish the control messge -- either due to some communication problem, code bug, or I simply stop the node on PC by Ctrl+C, the robot will not be stop -- the only way I can do right now is to switch -off the power robot.\nThis is anoying and I wonder if there should be any safety node that should run in turblebot3-burger --either it can stop the Robot when recieving sme control message from other node , or a time-out should happen when there is no control message published to it?", "Q2: Occationally I noticed after I reboot the turblebot3-burger , and run up the service as above command, it will immediately start moving, don't know the exactly reason--I guess if it is because some remaining \"command\" still exist in some \"buffer\" that was not cleared? ", "\nAnyway, is there any safe way that after reboot the turblebot3-burger and reset it to a absolutely \"known\" state?", "Thanks a lot"], "answer": [" ", " ", " ", " ", "This becomes not only an anoying but a reliable issue I think? ", "When doing test when I want to interupt the test (before the programing running finished) simply using Ctrl+C can terminate the program but TB3 are keeping moving (at the last control twist message sent to cmd_vel I guess).", "Search around there is no native solution, some ", " gave some python code and I don't fully understand and no time to study..", "It is so disappointed that such a basic feature can not be done within some simple way, like a watch-dog feature which is so common for a iot device..", "This is probably not what you came here to hear/read, but seeing as this is very specific to the TB3 stack: have you asked Robotis? They're the ones developing/maintaining the TB3 software, so would be the first to ask these sort of questions about specific functionality of their software.", " I can not find the right support window. They said just raise question here and with the turtlebot3 tag to set.", "Anyway one thought is that when some issue happen  as soon as /CMD_vel is still avalible, can I directly write some messge to that topic to stop the robot ?"], "question_code": ["roslaunch turtlebot3_bringup turtlebot3_robot.launch\n"], "answer_code": ["[post](https://answers.ros.org/question/292512/stop-my-robot-if-cmd_vel-doesnt-receive-a-message-within-a-certain-time-period/)"], "url": "https://answers.ros.org/question/322940/how-to-stop-turblebot-under-any-emergency/"},
{"title": "Equip Arduino based lawn mower with ROS", "time": "2019-05-13 05:57:03 -0600", "post_content": [" ", " ", "Hi there,", "I build a lawn mower based on Arduino few years ago (Ardumower). To bring it even further, I plan to re-invent it using ROS.", "My main target is to build a robust path planning and to get rid of random crossing my lawn for hours. Currently, everything is handeld by Arduino code and I wonder how to start best. Please don't hate me for asking how to begin. I read through lots of tutorials but I'm still lost how to set up best.", "Robot is a 2 wheel robot (like Turtlebot i assume) equipped with:\n- 3x sonar\n- bumper sensor\n- perimeter wire (like a fence)\n- IMU\n- Odometry\n- monitoring power consumption of motors (to detect high lawn and overload)\n- Raspberry PI 3 (or Zero) with PI camera (currently for FPV view and manual remote control)\n- Bluetooth", "My Idea is to keep Arduino in place but to use it as a basic controller for the robot. It should note ROS about any problem (Bumper hit, Sonar trigger, overload, perimeter crossed etc). ROS should than decide, what to be done next (reverse x cm and turn to left in a given angle etc.) Arduino will always stop robot in any event and wait for instructions coming from ROS. For communication between ROs and Arduino I plan to use ros_serial.", "Is this the right way to go? Should I attach sensors like IMU, Odometry etc directly to RPI or is it ok to keep them to Arduino and send only data to ROS?\nWhich version should I start? It has to run on RPI 3, maybe on zero, so I plan to use Kinetic. Is this still valid?", "Sorry for this stupid questions, you provide so many information here, too much for me to get the right track to start."], "answer": [], "url": "https://answers.ros.org/question/323055/equip-arduino-based-lawn-mower-with-ros/"},
{"title": "Hokuyo UST-10LX power options", "time": "2019-04-23 18:03:58 -0600", "post_content": [" ", " ", "Hi, I am currently using Hokuyo UST-10LX on my RC car and have separated battery to power the lidar and the car. To reduce the weight and complexity, I would like to use one battery to power both. Can I use this battery to power lidar?"], "answer": [], "url": "https://answers.ros.org/question/321734/hokuyo-ust-10lx-power-options/"},
{"title": "moveit! for python3", "time": "2019-05-03 03:37:03 -0600", "post_content": [" ", " ", "The question is quite simple:", "How can we use moveit! with python3 in ROS kinetic or melodic?", "Do we have to compile moveit! for python3? Is it supported for python3?", "The solutions like launching moveit! with python2 and then launch on the other side python3 is not possible for the environment I'm dealing with.", "Any insights? Thankyou", "Probably the only sustainable way to do this is to use the ", ", and have python3 \"on the side\" of the bridge."], "answer": [], "question_code": ["rosbridge"], "url": "https://answers.ros.org/question/322404/moveit-for-python3/"},
{"title": "Can one node be a client for several action/service servers?", "time": "2019-05-17 02:22:07 -0600", "post_content": [" ", " ", " ", " ", "I have one client which runs a state machine that coordinates between planner node, perception node, localization node and motion control node. I am using ROS action to communicate between state machine and planner node. I want to trigger motion control node when planner node sends a goal status as SUCCEED. Is it possible to use same client state machine node as client for other servers?\nIf possible is it the same also with ROS2 (crystal)?", "does the answer to ", " help?", "Question answered? Please tick the checkmark to the left of the answer to mark the question as answered. It should turn green."], "answer": [" ", " ", " ", " ", "The Q&A ", " links should already give you an idea, but to make it explicit:", "Is it possible to use same client state machine node as client for other servers?", "yes.", "If possible is it the same also with ROS2 (crystal)?", "yes.", "In both cases: action/service clients are \"just variables\" in your program. You can create a virtually unlimited (within reason) number of clients that way.", "Multiple servers in a single node is also supported btw."], "url": "https://answers.ros.org/question/323414/can-one-node-be-a-client-for-several-actionservice-servers/"},
{"title": "Create ROS-I interface for a non-supported robot", "time": "2019-04-29 04:36:17 -0600", "post_content": [" ", " ", " ", " ", "Hello, ", " I have an Estun industrial robotic arm (from China)  ", "  which has no ROS-Industrial driver.  ", "Is there any way to make it compatible with ROS-I? I'm not expecting a complete solution, but hopefully someone can point to some guide / example that would be useful for me to do this interfacing. ", "EDIT:", "Currently the robot can only be programmed via a teach pendant, using a C-like language. \nIt can also be interfaced via MODBUS, so I can write a program which accepts something from the MODBUS (positions / coordinates etc) and move the robot arm accordingly. ", "There might be other communication methods, but the vendor is hiding them from us (for business reason I suppose). If needed, I could ask them to make these available.", "Can you say anything about what sort of interfaces are available for those robots (ie: ethercat, powerlink, plain UDP, TCP, RS232, can(open), etc)? Any vendor-supported real-time external motion interfaces available? Also: how are those robots programmed? A custom language developed by the OEM? Does that support socket communication? Can motions be completely controlled by/initiated from a program?", " please see edit.", "You don't need to @-mention someone that has already commented on your question. They'll get notified automatically.", "Hello, do you have any update on this? I am working with a similar robot and I am curious about your progress.", "Sorry no update, we decided that this is too much work and just go without ROS at the moment."], "answer": [" ", " ", " ", " ", "Taking this a bit wider (so not necessarily a \"ROS-I interface\"):", "Currently the robot can only be programmed via a teach pendant, using a C-like language. It can also be interfaced via MODBUS, so I can write a program which accepts something from the MODBUS (positions / coordinates etc) and move the robot arm accordingly.", "that would be something that ", " work. Given proper buffering and blending of motions this could be a viable first approach.", "If you'd like to reuse something like ", " you could take a look at any of the server-side (ie: controller) programs in ", " or ", ". They're no longer being developed (as better interfaces are now available), but it should give you an idea.", "There might be other communication methods, but the vendor is hiding them from us (for business reason I suppose). If needed, I could ask them to make these available.", "Depending on your actual use-case / application it might be worth it to ask them."], "answer_code": ["industrial_robot_client", "abb_driver", "fanuc_driver"], "url": "https://answers.ros.org/question/322110/create-ros-i-interface-for-a-non-supported-robot/"},
{"title": "Question about laserscan and hector_slam mapping", "time": "2019-05-23 15:19:15 -0600", "post_content": [" ", " ", " ", " ", "Hello, I am using hector slam to build the map with original balgfile odometry data. ", "The picture is the result with delayed laserscan data overlayed on map. I have set ", ", ", ", ", " ,", " in the launch file, by which I think all the scan data has been included to build the map. ", "My question is: what does the color of laserscan particles in Rviz mean? As for area of \u2460 \u2464, the red particles form a boundary, while \u2461 \u2462 \u2463 have no red particle(maybe some orange particles), and there is no boundary formed at all. If the color represents the particles of the same height, then why area \u2463 with high density of light green particles forms no boundary?", "what picture are you refering to?", "The picture I uploaded...", "hmm, no picture for me.", "That's weird"], "answer": [" ", " ", "In RViz->Displays->Laser Scan->Color Transformer, you can set the laser scan colours to reflect the intensity of the laser scan. The colour depends on the intensity of the laser scan message received; first, the intensity at each point is normalised, and then a colour is computed based on that intensity. You can read more on the ", ".", "Thank you!"], "question_code": ["laser_z_min_value=-200", "laser_z_max_value=200", "laser_min_dist = 0", "laser_max_dist=200"], "url": "https://answers.ros.org/question/323896/question-about-laserscan-and-hector_slam-mapping/"},
{"title": "IMU covariance matrix setting for robot_localization", "time": "2019-03-13 06:31:07 -0600", "post_content": [" ", " ", " ", " ", "Hi guys :)", " I'm trying to fuse an odometry source with the data coming from an IMU. To do this, I'm using robot_localization. I'm having difficulties in understanding how to set the covariance matrix for the IMU. At the moment, I have a covariance matrix filled with zeros. As you can read from here ( ", " ) this is an error:  ", "-> Missing covariances. If you have configured a given sensor to fuse a given variable into the state estimation node, then the variance for that value (i.e., the covariance matrix value at position (i,i), where i is the index of that variable) ", ". If a 0 variance value is encountered for a variable that is being fused, the state estimation nodes will add a small epsilon value (1e\u22126) to that value. A better solution is for users to set covariances appropriately.", "Ok, so let's set the covariances appropriately. \nI think I only have two options: calculate it or get the values from the datasheet.", " So, let's look at the datasheet. I'm using the X-NUCLEO-IKS01A1 board with LSM6DS0 IMU. The datasheet is here  ", "  . As you can see there's a table on page 9 that talks about noise, like 'Gyroscope RMS noise in normal/low-power mode', etc. But how can I relate these values to variances? I didn't find anything on the web apart from this answer here  ", "  where they say: \"If you haven't got a background in random processes and signal analysis then you're going to have a rough time relating this back to real-world numbers, particularly if you're doing any kind of sensor fusion. Even the \"big boys\" in the sensor fusion game can't easily map sensor noise to system behavior without lots of simulation and head-scratching.\" ", "The other option is to calculate it. Again, I didn't find any standard approach to do it. I came up with the idea to just collect data for a while placing the IMU in a very firm way, then calculating the variance (so assuming the covariance matrix diagonal). Does it make sense an approach like this?", "Moreover, do you know if there's a standard approach to set the covariance matrix? What's the complete spectrum of the alternatives? ", "Thanks", " EDIT 26/05/19:\nI have temporarily postponed solving the problem with a sound approach in a favor to a trial and error one as proposed here:  ", "I have also found some interesting material that I still didn't have time to look at, but I would like to share with the readers hoping to be helpful:", " Allan variance method (Allan DW (1966) Statistics of atomic frequency standards.\nProceedings of the IEEE 54(2): 221\u2013230):  ", "My experience is that, if the sensor vendor doesn't provide the information, computing it can be black magic. Your approach seems reasonable to me. You could rotate the IMU through N full rotations in a given axis, manually integrate the velocities, and see what the final error is, and then back out the values from that.", "I understand that at the end of \"n\" rotations, we'll end up with some error.  Are you saying there's a way to back out a variance for the rotational rate from this? If so, can you explain a little more how to go about doing this?", "Off the top of my head, rotating ", " times means you have an expected rotation of ", " radians (ground truth, expected value).  Your integrated IMU velocity data measures ", " radians. So your error per measurement in ", ", where ", " is the number measurements. So maybe try ", "?", "I was proposing to calculate the data variance just placing the IMU in a static position. With your approach, ", ", aren't you introducing an error that depends on the numerical integration, thus, depending on the frequency of measurement?", " you could also normalize by the number of seconds elapsed for your N rotations, which gives you the error/sec, which is really what you wanted anyway.", "If the gyro is not actively rotating, I'm not sure that you can characterize the error in it. But I'm not an expert in such things.", "If you ", " end up solving this, please update this question and accept the answer, or just close it.", "No one of the reasons to close a question applies to my case."], "answer": [], "question_code": ["N", "2pi * N", "M", "(M - 2pi * N) / C", "C", "[(M - 2pi * N) / C]^2"], "url": "https://answers.ros.org/question/318445/imu-covariance-matrix-setting-for-robot_localization/"},
{"title": "Can't visualize Pepper's PointCloud using PCL viewer", "time": "2018-02-16 05:24:56 -0600", "post_content": [" ", " ", " ", " ", "I have written a piece of code which subscribes to my Pepper Robot's depth_registered/points topic, converts the Sensor::msg object to pcl::PointCloud using pcl::fromROSMsg(), when I try to visualize the pcl point cloud via PCL viewer I get a green red black screen(refer ", ") \nI'm able to see the pointcloud on RViZ.\nI've tried converting the Sensor::msg PointCloud2 object to both pcl::PointCloud<pcl::pointxyzrgb> and pcl::PointCloud<pcl::pointxyz>, both gives the same output.", "Here's the snippet of code,", "Also the depth coordinates from Pepper that I'm printing are very small in the order of 10^35. Is this expected?\nROS Kinetic\nUbuntu 16.04\nAny help?", "Some suggestions: save a pcd file, try it with pcl_viewer on the command line and share the file here.  Print out the xyz of the first 10 points or so.  Add a screenshot of what the proper cloud looks like in rviz.  Supply your cloud viewer small artificial pointcloud instead of real data.", "Thanks for the suggestions, I tried saving to a pcd file, I'm able to see the point cloud on pcl_viewer, but it's inverted. Dunno why it didn't work programmatically. Also, I can't upload the screenshot, less karma. Thanks anyway.", "If you can put your code in the question text it may be possible to see what is wrong.", "Hi, I've added the code. Also I subscribed to depth/image_rect, which I think is the rectified depth image, and now all the depth values are normal.", " The viewer ought to be created outside the callback while the callback only updates the viewer with new points- but CloudViewer doesn't support multithreading  ", "  - PCLVisualizer is supposed to.  Though when I last tried it it crashed a lot ", "It may not be your issue but it isn't a good design to have a ros message callback that comes to a halt for an interactive viewer rather than returning as quickly as possible.", "So, how to show point cloud with pcl?  I create the viewer outside the callback but the it does not refresh. Do you mean the message uses multithreading method and the view will not work?", "Crosslinking to "], "answer": [" ", " ", "What you're seeing is just the three axes zoomed in. Use the mouse wheel to scroll out and you'll see your scene. I felt pretty silly when I finally figured that out."], "question_code": ["void callback(const sensor_msgs::PointCloud2ConstPtr &cloud) {\n\npcl::PointCloud<pcl::PointXYZ>::Ptr cloudGround(new pcl::PointCloud<pcl::PointXYZ>);\npcl::fromROSMsg(*cloud, *cloudGround);\npcl::visualization::CloudViewer viewer (\"Simple Cloud Viewer\");\nviewer.showCloud (cloud);\nwhile (!viewer.wasStopped ())\n{\n}\nconst std::string file_name = \"pointcloud.pcd\";\npcl::io::savePCDFileASCII (file_name, *cloudGround);\n\n}\n"], "url": "https://answers.ros.org/question/282923/cant-visualize-peppers-pointcloud-using-pcl-viewer/"},
{"title": "ROS power management - quality of service", "time": "2018-03-22 10:12:48 -0600", "post_content": [" ", " ", "Hi,", "I was digging around in my /dev/ folder today and noticed a file called network_latency and network_throughput. This peaked my interested as I am making a robot that communicates over cell modem. After researching a little bit I found that these files are used for pm-qos (power-management - quality of service). A program can write to this files to set QOS settings for that program. I was wondering if anyone out there has used these for a robot? They seem like they could be helpful but I don't see any mention of anyone using them with ROS."], "answer": [" ", " ", "I'm not aware of any ROS packages directly interacting with these. They are potentially useful for robots communicating over those network links. I suspect that if people are using that functionality they are likely doing it outside of their ROS context, as it's more of a system configuration than a data processing node."], "url": "https://answers.ros.org/question/286253/ros-power-management-quality-of-service/"},
{"title": "Unable to run Gazebo with ROS kinetic", "time": "2019-05-29 20:17:23 -0600", "post_content": [" ", " ", "Hello,", "I have been following the tutorials in the ", " book and I have hit a snag. I have gotten to the part where we first attempt to simulate a turtlebot in Gazebo. When I try to run the terminal command from the book;", " I end up with ", " After much online searching, to no avail, I decided to completely do a fresh install of ubuntu and ROS. However, I seem to still be getting the same error. ", "For my specs I am running ubuntu 16.04 LTS on an HP-mini-110 with a 1.6GHz dual core intel atom CPU and an intel 945GME graphics processor. I know that its not super powerful but I hope thats not the issue.", "See below for the full output from the terminal leading up to the error. It mentions something about a log file but it doesn't seem to exist, at least not where it says.", "Can you please update your question with a page number?", "sure thing! it was on page 125 of Programming Robots with ROS", "For my specs I am running ubuntu 16.04 LTS on an HP-mini-110 with a 1.6GHz dual core intel atom CPU and an intel 945GME graphics processor. I know that its not super powerful but I hope thats not the issue.", "this may well be the problem, actually.", "An i945 is barely a video card and the support for accelerated OpenGL is really minor. Gazebo is a heavy graphical application, relying on proper 3D acceleration to render just about everything (and sometimes even for GPU accelerating the simulation).", "One thing to try could be to see whether Gazebo can start on its own (so without ROS involved). If it does, that still doesn't mean that everything should be fine: a full simulation running inside Gazebo will tax your system and could exercise parts of the simulator that an empty, stand-alone run doesn't.", "If Gazebo stand-alone also doesn ...", "I'm assuming that you're using the gazebo_ros launch file. You can pass the verbose=true to the launch file and will start gazebo with more console outputs. That might give you a clue as to why it's seg faulting."], "answer": [], "question_code": ["roslaunch turtlebot_gazebo turtlebot_world.launch", "[gazebo -1] process has died [pid 9621...", "    art@art-HP-Mini-110-1100:~/.ros/log/60fd69d0-7f0c-11e9-813a-0ceee68c4bed$ roslaunch turtlebot_gazebo turtlebot_world.launch\n... logging to /home/art/.ros/log/60fd69d0-7f0c-11e9-813a-0ceee68c4bed/roslaunch-art-HP-Mini-110-1100-15373.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\n\nDone checking log file disk usage. Usage is <1GB.\n\nxacro: Traditional processing is deprecated. Switch to --inorder processing!\nTo check for compatibility of your document, use option --check-order.\nFor more infos, see http://wiki.ros.org/xacro#Processing_Order\nxacro.py is deprecated; please use xacro instead\nstarted roslaunch server http://art-HP-Mini-110-1100:38215/\n\nSUMMARY\n========\n\nPARAMETERS\n * /bumper2pointcloud/pointcloud_radius: 0.24\n * /cmd_vel_mux/yaml_cfg_file: /opt/ros/kinetic/...\n * /depthimage_to_laserscan/output_frame_id: /camera_depth_frame\n * /depthimage_to_laserscan/range_min: 0.45\n * /depthimage_to_laserscan/scan_height: 10\n * /robot_description: <?xml version=\"1....\n * /robot_state_publisher/publish_frequency: 30.0\n * /rosdistro: kinetic\n * /rosversion: 1.12.14\n * /use_sim_time: True\n\nNODES\n  /\n    bumper2pointcloud (nodelet/nodelet)\n    cmd_vel_mux (nodelet/nodelet)\n    depthimage_to_laserscan (nodelet/nodelet)\n    gazebo (gazebo_ros/gzserver)\n    gazebo_gui (gazebo_ros/gzclient)\n    laserscan_nodelet_manager (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    spawn_turtlebot_model (gazebo_ros/spawn_model)\n\nROS_MASTER_URI=http://localhost:11311\n\nprocess[gazebo-1]: started with pid [15400]\nprocess[gazebo_gui-2]: started with pid [15402]\nprocess[spawn_turtlebot_model-3]: started with pid [15410]\nprocess[mobile_base_nodelet_manager-4]: started with pid [15411]\nprocess[cmd_vel_mux-5]: started with pid [15412]\nprocess[bumper2pointcloud-6]: started with pid [15413]\nprocess[robot_state_publisher-7]: started with pid [15414]\nprocess[laserscan_nodelet_manager-8]: started with pid [15415]\nprocess[depthimage_to_laserscan-9]: started with pid [15416]\n[ INFO] [1559175098.446825532]: Finished loading Gazebo ROS API Plugin.\n[ INFO] [1559175098.455395173]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting...\n[ INFO] [1559175099.021084819]: Finished loading Gazebo ROS API Plugin.\n[ INFO] [1559175099.026340125]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting...\nSegmentation fault (core dumped)\n[gazebo-1] process has died [pid 15400, exit code 139, cmd /opt/ros/kinetic/lib/gazebo_ros/gzserver -e ode /opt/ros/kinetic/share/turtlebot_gazebo/worlds/playground.world __name:=gazebo __log:=/home/art/.ros/log/60fd69d0-7f0c-11e9-813a-0ceee68c4bed/gazebo-1.log].\nlog file: /home/art/.ros/log/60fd69d0-7f0c-11e9-813a-0ceee68c4bed/gazebo-1*.log\nSegmentation fault (core dumped)\n[gazebo_gui-2] process has died [pid 15402, exit code 139, cmd /opt/ros/kinetic/lib/gazebo_ros/gzclient __name:=gazebo_gui __log:=/home/art/.ros/log/60fd69d0-7f0c-11e9-813a-0ceee68c4bed/gazebo_gui-2.log].\nlog file: /home/art/.ros ..."], "url": "https://answers.ros.org/question/324285/unable-to-run-gazebo-with-ros-kinetic/"},
{"title": "package suggetions for tracking simple object/marker?", "time": "2019-06-07 04:47:58 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I want to determine the 3D movement (instantaneous pose) of a simple object, e.g., a green tennis ball in a 2x2x2 meter space using a realsense D435 camera. I read the similar questions in the forum and found for example find_object_3d, tod, vr4, but based on the answers i could not conclude which would provide the best results.", "The realsense is placed ~1.5meters from the workspace (that should be recorded), and i highly suspect that it will provide quite unreliable results, especially if the tennis ball is 3 meters far from the camera..", "Could you recommend what setup/solution/package should be used for the aforementioned problem?", "So the pose of a simple green colored marker is required to be determined. Maybe OpenCV-based method suits for this task?", "Thank you for the help in advance. "], "answer": [" ", " ", "maybe this package can give you initial start... it is not mine, ", "Thank you, it works as i expected."], "url": "https://answers.ros.org/question/324955/package-suggetions-for-tracking-simple-objectmarker/"},
{"title": "/map not connecting to tf links for loading multiple robots in rviz", "time": "2019-06-20 05:08:34 -0600", "post_content": [" ", " ", "I am trying to load two robots in Gazebo and visualize them in Rviz to eventually do a leader-follower application. I believe I have the robots initialized in Gazebo correctly and mostly in Rviz, but am having trouble getting the /map topic to link up to either of the robots. I can teleop with the robots separately in Gazebo using their respective namespaces. I am simulating using a turtlebot3 waffle_pi. I'm thinking the problem has something to do with remapping from \"map\" to \"/map\" or in how my amcl node is called under the tf_prefixes for my two robots.", "From running ", " I can see that both of my two robots are initialized with their respective connections to /tf and /gazebo. (would upload output image but not enough user points). ", "From running ", " I can see that both robots have root parents of \"bot1_tf/odom\" and \"bot2_tf/odom\" where \"botX_tf\" is the tf_prefix. The children connections seem to match output from running a single robot in Gazebo/Rviz. The only thing different seems to be that \"/map\" is the parent for \"/odom\" for the single robot, while there is no \"/map\" parent for the two robots. ", "Upon loading Rviz I see a green check next to URDF making me thing Rviz is reading in the model correctly, though all other tf topics have errors. When I manually enter either tf_prefix in the \"TF Prefix\" field in Rviz under \"RobotModel\" I see errors saying \"No transform from [tf_prefix/base_footprint] to [map]\" and so on for the other tf topics (base_footprint, base_link, base_scan, camera_link, camera_rgb_frame, camera_rgb_optical_frame, caster_back_left_link, caster_back_right_link, imu_link, wheel_left_link, wheel_right_link). ", "Running ", " results in the following:", "Running ", " results in the following:"], "answer": [" ", " ", "I think it is because amcl is still looking relative to the namespace for the map (because it is using the service \"static_map\" per default). Try adding either ", " or ", "to the amcl.launch file, depending on which method for getting the map you want to use. The /map topic is only used when the \"use_map_topic\" parameter is true. You can see more here: ", "Tried both of those, neither seemed to successfully link move_base to /map. Using either of those options, and only one at a time, resulted in the following ", ". I also got that same result when adding both together. What seems to be a step in the right direction is including both but using \"map\" and \"/map\" instead of \"static_map\" and \"/static_map\". So, adding both of the following lines", "Gives the following ", ". ", "In all cases the nodes were all still connected from tf_prefix/odom and down, but without a top connection to /map, see the ", "Do you think there's more I'd need to do in the move_base launch file?"], "question_code": ["rosrun rqt_graph rqt_graph", "rosrun tf view_frames", "rosnode list", "/bot1/amcl\n/bot1/move_base\n/bot1/robot_state_publisher\n/bot2/amcl\n/bot2/move_base\n/bot2/robot_state_publisher\n/gazebo\n/gazebo_gui\n/map_server\n/rosout\n/rviz\n", "rostopic list", "/bot1/amcl_pose\n/bot1/camera/parameter_descriptions\n/bot1/camera/parameter_updates\n/bot1/camera/rgb/camera_info\n/bot1/camera/rgb/image_raw\n/bot1/camera/rgb/image_raw/compressed\n/bot1/camera/rgb/image_raw/compressed/parameter_descriptions\n/bot1/camera/rgb/image_raw/compressed/parameter_updates\n/bot1/camera/rgb/image_raw/compressedDepth\n/bot1/camera/rgb/image_raw/compressedDepth/parameter_descriptions\n/bot1/camera/rgb/image_raw/compressedDepth/parameter_updates\n/bot1/camera/rgb/image_raw/theora\n/bot1/camera/rgb/image_raw/theora/parameter_descriptions\n/bot1/camera/rgb/image_raw/theora/parameter_updates\n/bot1/cmd_vel\n/bot1/imu\n/bot1/initialpose\n/bot1/joint_states\n/bot1/move_base/current_goal\n/bot1/move_base/goal\n/bot1/move_base_simple/goal\n/bot1/odom\n/bot1/particlecloud\n/bot1/scan\n/bot2/amcl_pose\n/bot2/camera/parameter_descriptions\n/bot2/camera/parameter_updates\n/bot2/camera/rgb/camera_info\n/bot2/camera/rgb/image_raw\n/bot2/camera/rgb/image_raw/compressed\n/bot2/camera/rgb/image_raw/compressed/parameter_descriptions\n/bot2/camera/rgb/image_raw/compressed/parameter_updates\n/bot2/camera/rgb/image_raw/compressedDepth\n/bot2/camera/rgb/image_raw/compressedDepth/parameter_descriptions /bot2/camera/rgb/image_raw/compressedDepth/parameter_updates\n/bot2/camera/rgb/image_raw/theora\n/bot2/camera/rgb/image_raw/theora/parameter_descriptions\n/bot2/camera/rgb/image_raw/theora/parameter_updates\n/bot2/cmd_vel\n/bot2/imu\n/bot2/initialpose\n/bot2/joint_states\n/bot2/move_base/current_goal\n/bot2/move_base/goal\n/bot2/move_base_simple/goal\n/bot2/odom\n/bot2/particlecloud\n/bot2/scan\n/clock\n/gazebo/link_states\n/gazebo/model_states\n/gazebo/parameter_descriptions\n/gazebo/parameter_updates\n/gazebo/set_link_state\n/gazebo/set_model_state\n/initialpose\n/map ..."], "answer_code": ["<param name=\"use_map_topic\" value=\"true\"/>", "<remap from=\"static_map\" to=\"/static_map\" />", "<param name=\"use_map_topic\" value=\"true\" />\n<remap from=\"map\" to=\"/map\" />\n"], "url": "https://answers.ros.org/question/326360/map-not-connecting-to-tf-links-for-loading-multiple-robots-in-rviz/"},
{"title": "Arduino (rosserial) node hangs during I2C reading + PWM", "time": "2016-12-19 14:10:23 -0600", "post_content": [" ", " ", "I've been trying to troubleshoot a strange problem for a few days now. I've got a ROS Kinetic node running on a Raspberry Pi 3 (Ubuntu Mate). I'm utilizing the rosserial_arduino package to handle communication between the Arduino and my master node. Current setup is basically an Arduino Mega 2560 with a SEED Studio Motor Shield powering 2 wheels, plus an ITG3205/ADXL345/HMC5883L 9-DOF chip. I currently have two publishers set up - one outputting twistStamped messages and one chatter topic outputting loop iterations and debug information.  I've got one subscriber set up receiving an int16 message from a homespun teleop script - basically listening for keys 2,4,5,6,8.", "Everything works perfectly if there is no power to the motor shield (I'm using an external 6v pack). I am able to echo out twist messages which are directly linked to the xyz axis on my gyro and accel. My chatter topic shows me my current loop iteration and when I send key commands I see  them echoing out as well. Similarly, if the motor power is off, I still get LED signals on the shield showing the current motor states.", "If I turn on power supply to the motors and send a signal other than \"full stop\" which is key 5, everything hangs up. it might take a few loops but it will hang within 1-4 seconds of the motor signal being sent and received. Resetting the board alleviates the issue and all my topics begin to send/receive again.", "I have traced this problem down to the ITG3205 portion that performs the I2C communication (itgRead function) - specifically after the first call to Wire.endTransmission().", "Any ideas? I've tried swapping out the arduino, the shield, and the 9-dof chip with no success. As long as i've got wheels are spinning, everything hangs on gyro read. I've included relevant code snippets below."], "answer": [" ", " ", "From my testing, it appears rosserial_arduino can subscribe to both Twist and TwistStamped messages correctly; but, the callback will not work if the message type is TwistStamped."], "question_code": ["void SeeedMotorV2::allStop()\n{\n    digitalWrite(leftmotorspeed,0);\n    digitalWrite(rightmotorspeed,0);\n      digitalWrite(leftmotorForward,LOW);\n     digitalWrite(rightmotorBackward,LOW);\n      digitalWrite(leftmotorBackward,LOW);\n      digitalWrite(rightmotorForward,LOW);\n\n}\n\nvoid SeeedMotorV2::forward(int speed)\n{\n  allStop();\n  analogWrite(leftmotorspeed,speed); //pin 10\n  analogWrite(rightmotorspeed,speed);//pin 9\n  digitalWrite(leftmotorBackward,LOW);//pin 13\n  digitalWrite(rightmotorBackward,LOW);//pin 11\n  digitalWrite(leftmotorForward,HIGH);//pin 12\n  digitalWrite(rightmotorForward,HIGH);//pin 8\n}\n", "unsigned char ITG3205::itgRead(char address, char registerAddress)\n{\n  //This variable will hold the contents read from the i2c device.\n  unsigned char data=0;\n\n  //Send the register address to be read.\n  Wire.beginTransmission(address);\n  //Send the Register Address\n  Wire.write(registerAddress);\n\n  //End the communication sequence.\n  Wire.endTransmission();\n\n  //Ask the I2C device for data\n  Wire.beginTransmission(address);\n  Wire.requestFrom(address, 1);\n\n  //Wait for a response from the I2C device\n  if(Wire.available()){\n    //Save the data sent from the I2C device\n    data = Wire.read();\n  }\n\n  //End the communication sequence.\n  Wire.endTransmission();\n\n  //Return the data read during the operation\n  return data;\n}\n\n//itgAddress = 0x68\n//GYRO_XOUT_H = 0x1D\n//GYRO_YOUT_L = 0x1DE\nint ITG3205::readX(void)\n{\n  int data=0;\n  data = itgRead ..."], "url": "https://answers.ros.org/question/250366/arduino-rosserial-node-hangs-during-i2c-reading-pwm/"},
{"title": "Disabling UR5e teach pendant and turning on the arm with ROS", "time": "2019-06-18 11:48:58 -0600", "post_content": [" ", " ", "Hello,", "So I have a system to power the UR5e, but now I want to disable the teach pendant and instead of doing the initialization sequence (the 5 circles) from the teach pendant (turning off the brakes, etc..) and would like to do with ROS. I have tried some stuff with the service /ur_driver/robot_enable, but I am not sure if this is what is suposed to do. ", "Besides this I also tried using the /ur_driver/URScript topic with \"power on\" and I get the following error:", "What should I do to be able to avoid using the teach pendant every time I turn on/off the UR5e?"], "answer": [" ", " ", "this is expected to be implemented in the new ros driver that ur are working on that was presented about at the ros industrial conference last year", "Yes, so the answer is: ", " does not support this. It checks the state of the robot controller and will not allow remote control until the robot has been properly initialised and/or the controller is in an operational state.", "Getting it in that state / TP-less operation was not part of the design requirements and as such is not supported.", "presentation here ", " control modes details are at  5:09", "So is there any way to use the UR5 arm without having the teach pendant always connected? Is it possible to initialize the arm without the teach pendant or ROS?", "I don't know one currently, there is an email address at 18:00 on the video to sign up for beta testing the new driver when it is ready", "There are multiple actually, one of which would be using the dashboard server, but:"], "answer_details": [" ", " ", " ", " ", "I'm not entirely sure they work with e-series (as they have additional safety features)", "I'm not entirely sure it'd be a good idea to automate these steps, unless we're talking about a deployment in a (semi) controlled environment / by an integrator"], "question_code": ["[ERROR] [/ur_hardware_interface]: Robot is not ready, check robot_mode\n"], "answer_code": ["ur_modern_driver"], "url": "https://answers.ros.org/question/326145/disabling-ur5e-teach-pendant-and-turning-on-the-arm-with-ros/"},
{"title": "Strange movement with hector_navigation", "time": "2019-06-15 09:39:58 -0600", "post_content": [" ", " ", "Hello there!", "Me and my team are trying to get a mobile robot running for autonomous exploration. Since the only sensor we have available is a ", " sensor (RPLIDAR A2), we went with ", "(", "), because it requires no additional sensors.", "Mapping works great, all obstacles are being mapped accordingly, the robot pose is also okay. However, when we try to do navigation, we get some strange results. We are using the ", "package (", ") - more specifically, the ", " and the ", ". The path gets computed fine, and velocity commands get published (which we then process through our own Motor Controller node). ", "However, the robot keeps moving ", " all the time - it eventually reaches the goal, but it is constantly moving back and forth. Our costmap (edited the one that comes with the package) is as follows:", "We modified the parameters of the costmap that came with hector_navigation to fit our mobile base (modified: footprint, inscribed_radius, and curcumscribed_radius). We looked at the code in hector_navigation and really have no idea what the cause for this behavior is.", "The rest of the hardware used is:", "Sabertooth Dual 12A 6V-24V R/C Regenerative Motor Driver", "Did anyone else have this problem? How do we get the robot to simply move along the path, instead of going back and forth before eventually reaching the goal? Could it be that it is due to limited hardware?", "Additional information that could be useful:", "We can provide any additional info if needed. ", "\"It is only linear movement that causes the back-and-forth motion.\"", "\"Linear\" \"back-and-forth\" means it moves forwards, then backs up, then forward again?\nIs it possible that the math in your control node has an error that sets speed higher than requested by Hector?", "Yes, it moves forwards, then it backs up, then it moves forwards again, and so on. The math in the control node is correct as we checked it multiple times and tested it with manual driving. However, the motor driver we are using (Sabertooth Dual) is not very precise, so it could be that the end result that is sent to the motors is different - maybe this is something that is wrong with the hardware (i.e. the motor driver not being good enough) so it can't be fixed?", "If you're still working on this issue, monitor the cmd_vel topic and find out if move_base is issuing commands and robot is following them, or if robot is doing it on it's own.", "\nWe monitored the cmd_vel topic, the commands issued correspond to the movement of the robot. But we do think that the robot is not moving precise enough, e.g. the command being 0.35 m/s linear, and the robot moving 0.45 or 0.5 m/s. This is most probably a hardware issue with the motor driver, right?", "Could be hw. Could be tuning. I have no way to know but if it is hw it would interesting that both channels have same issue. Maybe driver lacks power."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Lynxmotion Aluminum A4WD1 Rover Kit", " ", " ", "We are using ROS Kinetic on an NVidia Jetson TX2 board", "Turning works perfectly - as soon as a new path is computed, the robot turns into the desired direction. It is only linear movement that causes the back-and-forth motion."], "question_code": ["    global_costmap:\n\n  map_type: costmap\n  track_unknown_space: true\n  unknown_cost_value: 255\n  obstacle_range: 2.5\n  raytrace_range: 3.0\n  footprint: [[0.18, 0.15], \n              [-0.18, 0.15],\n              [-0.18, -0.15],\n              [0.18, -0.15]]\n  inflation_radius: 0.32\n  #transform_tolerance: 0.5 \n  inscribed_radius: 0.15 \n  circumscribed_radius: 0.24 \n\n  global_frame: /map\n  robot_base_frame: /base_link\n  update_frequency: 0.5 \n  publish_frequency: 0.1 \n  static_map: true\n  rolling_window: false\n\n  #Investigate what this actually does\n  cost_scaling_factor: 10.0\n"], "url": "https://answers.ros.org/question/325880/strange-movement-with-hector_navigation/"},
{"title": "Not getting laser scan runing libgazebo_ros_gpu_laser plugin", "time": "2019-06-23 01:11:11 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "I have been following the tutorials in the book ", " and I am on the part on page 306 where we create a simulation of the robots laser scanner in gazebo. My goal is to use what I learn from the book to be able to make my own autonomous robot for school. The computer I will be using in my final robot doesn't seem to be powerful enough to run gazebo and rviz. Thus I am trying to split all the code up so that the robot computer runs all the navigation and everything that would be on the robot itself while the simulation is running on my beefier machine. i am running ROS Kinetic and Ubuntu on both", "With that said the problem I am running into is that when I run the simulation I can't see any data from the laser scanner in rviz. To investigate, I ran ", " and this is what I got back"], "answer": [" ", " ", "If I understand you well you are trying to see the output of your 2D laser scan. Is your world empty? if so there is not object to reflect back the laser points therefore your laser /scan message will include nothing but infinity distance measured by the laser (there is no object to return back any laser points). Also as a good rule of thumb, when you visualize it in rviz make sure you are on the right tf tree. (i.e. base_link)", "thank you for your reply! the tutorial says to start in an empty world and place items in front of the robot to see the laser lines in RViz. i also tried placing the robot in the willowGarage example world (commented in my code). in both instances the output from rostopic was as seen above. what do you meen by make sure youre on the right tf tree?", "Display->Global Option->Fixed Frame: make sure in this section in rviz, you are on your base_link frame of your robot. What's your rosrun rqt_tf_tree rqt_tf_tree? so you can see all inf being printed at your terminal when your rostopic echo the laser topic?", "My rviz configuration is as you said. I ran tf_ tree (", ") the hokuyo_link is coming off of the base_link but I notice there are 0 transforms. What does this mean? And yes no matter what I do to the robot in gazebo I show all inf in rostopic."], "question_code": ["rostopic echo /scan", "---\nheader: \n  seq: 7178\n  stamp: \n    secs: 180\n    nsecs: 212000000\n  frame_id: \"hokuyo_link\"\nangle_min: -1.57079994678\nangle_max: 1.57079994678\nangle_increment: 0.00436940183863\ntime_increment: 0.0\nscan_time: 0.0\nrange_min: 0.10000000149\nrange_max: 30.0\nranges: [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf ..."], "url": "https://answers.ros.org/question/326674/not-getting-laser-scan-runing-libgazebo_ros_gpu_laser-plugin/"},
{"title": "How does the RVIZ marker work and how can I turn it off?", "time": "2019-07-03 02:50:57 -0600", "post_content": [" ", " ", "Hi,", "I'm having an Intel Aero Ready to Flight Drone with Ubuntu 16.04 installed. It uses Cartographer for SLAM and RVIZ for visualization. When using the tool, I see on RVIZ, there are some markers is drawn during the flight. The marker is a green-red-blue-axis (look like an x-y-z graph) with coordinate on top. I notice it usually drop the marker at the following situation: ", "Can I ask what the coordinate mean? Why, in my case, only the second number increased? And what if I do not want to observe those markers, how can I turn it off?", "Thank you.", "I encountered the same problem and here is the screenshot (", "). Clearly, you can see there are coordinate along the trajectory.\n What I understood now is that RVIZ will subscribe the topic called submap_list that is published by Cartographer to present the map. However, there is a list of submaps being generated as the robot moves and each submap has a version number as well as its pose. The \"Displays\" panel has no options for one to turn off that number only unless one turn off all the submaps. \nIt's neat to know how many submaps have been generated with current setup but it looks better to me if without the numbers. Any hint will be appreciated."], "answer": [" ", " ", "RViz has a \"Displays\" panel that has a list all the things you are seeing.\nYou can tick/untick the display types to see/unsee them in the main window.\nTake a look at ", " for more info.", "Regarding your other question, it would be better to ask them in a different question.", "Thank you for your answer. We will look at the panel and the guide to see if we can turn it off.", "By referring to other question, do you mean these 2 questions? \"What the coordinate mean? Why, in my case, only the second number increased?\"", " yes, I was referring to those 2 questions.\nIf my answer solves your problem, please mark it as \"correct\""], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "When started (initial position), so the marker always be (0,0)", "When there is a change on the path (observe an obstacle and change the path). It will increase the second number to (0,1), (0,2)", "When completed the objective/goal (moved to the designated position). A marker is also dropped (0,3)"], "url": "https://answers.ros.org/question/327556/how-does-the-rviz-marker-work-and-how-can-i-turn-it-off/"},
{"title": "How to launch compressed image node", "time": "2019-07-11 16:25:50 -0600", "post_content": [" ", " ", "Hey,\nI'm struggling with compressed image transport node on ros kinetic -  the problem is that i have to pass image over tcp connection but when I do that without commpression i have quickly filled up my connection. ", "The idea is to compress image on slave and decompress that on master - image processing node needs decompressed image. I can't read proper launch file which can make it. I cannot find how to launch this compression, I'm looking for something like :", "Also I don't know which option is better weather compress to png or jpg - connection is pretty fast but board is asus thinker board  and I'm limited by it's power ", "As far as I understood the compressed_image_transport package will automatically publish a cenpressed version if the original image is published via image transport: \"The compressed_image_transport package provides plugins for the \"compressed\" transport, which sends images over the wire in either JPEG- or PNG-compressed form. Notice that compressed_image_transport is not a dependency of your package; image_transport will automatically discover all transport plugins built in your ROS system.\""], "answer": [], "question_code": [" <node pkg=\"image_transport\" type=\"republish\" name=\"rgb_compress\" args=\"raw in:=/camera/rgb/image_raw compressed out:=/rgb_republish\">\n"], "url": "https://answers.ros.org/question/328313/how-to-launch-compressed-image-node/"},
{"title": "Battery damaged after full discharge", "time": "2019-08-19 01:56:05 -0600", "post_content": [" ", " ", "One of my pupils let the battery plugged on the Turtlebot Burger 3 for one year, and now it does not charge anymore: the charger lights don't activate, neither the green light nor the red one. Is there a way to reactivate the battery ?", "It might be more expedient to contact Robotis about this. ROS Answers is not a forum dedicated to TB3 (hardware) support, and while Robotis' employees take a look here every now and then, I believe it will be quite some time before you'll receive a response."], "answer": [], "url": "https://answers.ros.org/question/330995/battery-damaged-after-full-discharge/"},
{"title": "Azure Kinect ROS driver \"Failed to open K4A device at index 0\"", "time": "2019-07-30 12:14:21 -0600", "post_content": [" ", " ", " ", " ", "I've gotten the kinect azure driver to build. I'm a bit new to ROS, so let me know if this is the wrong way to start the driver. But I went to the folder with the launch files and used the command roslaunch driver.launch", "~/catkin_ws/src/Azure_Kinect_ROS_Driver/launch$ roslaunch driver.launch", "these errors occur", "then it gives this error", "followed by this", "I do have the Kinect Azure plugged in, as well as the extra power cable for it", "When I use lsusb several Microsoft devices that are not listed when the Kinect is not plugged in show up, so I think it's detected", "I can see that it is failing to open the device and that it doesn't have permission. I know sudo doesn't work on roslaunch. There must be some other way to allow it permission to use this device.", "Update", "I tried going into /etc/udev/rules.d and added a file named Microsoft.rules and putting these lines in it", "With 045e being the vendor ID for microsoft, which is what shoes up when I use lsusb on the Azure kinect", "So I restarted the terminal for those rules to take effect but now when I got to the launch folder and type ", "\nit can't find the kinect azure driver at all."], "answer": [], "question_code": ["libusb: error [_get_usbfs_fd] libusb couldn't open USB device /dev/bus/usb/011/022: Permission denied\nlibusb: error [_get_usbfs_fd] libusb requires write access to USB device nodes.\n", "[ERROR] [1564506111.686955539]: Failed to open K4A device at index 0\n[ERROR] [1564506111.687012324]: Failed to open a K4A device. Cannot continue.\n", "[node-4] process has died [pid 13824, exit code -6, cmd /home/username/catkin_ws/devel/lib/azure_kinect_ros_driver/node __name:=node __log:=/home/username/.ros/log/b8ede8f0-b2eb-11e9-b319-4ccc6ace1192/node-4.log].\nlog file: /home/username/.ros/log/b8ede8f0-b2eb-11e9-b319-4ccc6ace1192/node-4*.log\n", "Bus 011 Device 022: ID 045e:097c Microsoft Corp.\nBus 011 Device 023: ID 045e:097d Microsoft Corp. \nBus 011 Device 021: ID 045e:097a Microsoft Corp. \n[...]\nBus 010 Device 016: ID 045e:097e Microsoft Corp. \nBus 010 Device 015: ID 045e:097b Microsoft Corp.\n", "SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"045e\", MODE=\"0666\"\nSUBSYSTEM==\"usb_device\", ATTRS{idVendor}==\"045e\", MODE=\"0666\"\n", "roslaunch driver.launch"], "url": "https://answers.ros.org/question/329675/azure-kinect-ros-driver-failed-to-open-k4a-device-at-index-0/"},
{"title": "strange movement with dwa", "time": "2019-09-12 11:40:01 -0600", "post_content": [" ", " ", "Dear all, ", "I am trying to perform navigation with my robot. I have created a map using rtabmap_ros with stereo+2Dlaser configuration. I have tried to tune the navigation parameter, but still there is this strange behaviour. (", "). I think it might be localization problem. I am not sure. Can some guide me?", "Here are my costmap parameters:", "global costmap:", "local_costmap:", "common_costmap:", "local_planner:\nDWAPlannerROS:", "Can you A) zoom in B) visualize the cost cloud and local path C) clarify which part of the navigation surprised you", "Apologies, ", " is the zoomed version showing local path (green) and global path (blue).\nWhat I find strange is that the robot is not following the local path.  And the local path does not get updated like the global path.", "Also, I do not understand why the move_base takes some time at the beginning to send velocity command.", "How many velocity commands is is sending per second", "I need to check it. I will report it tomorrow.  It is already night here.", "Does poor localization affect navigation?", "rate of velocity commands seems to be decreasing.\nrostopic hz /navigation/cmd_vel  gives me", "Hello David,\nCould you give me a hint, what might be wrong?", "What topic is the green line?", "Green line represents the global plan from DWA planner. The topic is:"], "answer": [" ", " ", "Please visualize the local plan at /move_base/DWAPlannerROS/local_plan"], "question_code": ["#global_costmap:\nglobal_frame: map\nrobot_base_frame: base_link\nupdate_frequency: 1\npublish_frequency: 1\nstatic_map: true\nrolling_window: false\nalways_send_full_costmap: false\ntransform_tolerance: 0.5\ninflation_radius: 0.55\ncost_scaling_factor: 10.0\n\nplugins:\n   - {name: static_layer, type: \"rtabmap_ros::StaticLayer\"}\n   - {name: obstacle_layer, type: \"costmap_2d::ObstacleLayer\"}\n   - {name: inflation_layer,  type: \"costmap_2d::InflationLayer\"}\n", "local_costmap:\n  global_frame: wheel_odom\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 4.0\n  height: 4.0\n  resolution: 0.03\n  inflation_radius: 0.55\n  cost_scaling_factor: 5.0\n  origin_x: -2.0\n  origin_y: -2.0\n", "footprint: [[0.2,0.28],[-0.45,0.28],[-0.45,-0.28],[0.2,-0.28]]\nobstacle_range: 2.5\nraytrace_range: 3.0\nfootprint_padding: 0.02\nrobot_radius: 0.36\n\ntransform_tolerance: 2\nobservation_sources: laser_scan_pls point_cloud_sensor\nlaser_scan_pls: {sensor_frame: sick_pls_link,  data_type: LaserScan,  topic: /sick_pls/scan,  marking: true,  clearing: true}\n\npoint_cloud_sensor: {sensor_frame:  bumblebee_left,    data_type: PointCloud2,    topic: /planner_cloud,     expected_update_rate: 0.5,     marking: true,     clearing: true,   min_obstacle_height: -99999.0,    max_obstacle_height: 1.5}\nmap_type: costmap\n", "###Robot Configuration Parameters\n\n  acc_lim_x : 1.5\n\n    #The x acceleration limit of the robot in meters/sec^2 \n\n  acc_lim_y : 2\n\n    #The y acceleration limit of the robot in meters/sec^2 \n\n  acc_lim_theta : 3\n    #The rotational acceleration limit of the robot in radians/sec^2 \n\n  max_vel_trans : 0.12\n\n    #The absolute value of the maximum translational velocity for the robot in m/s \n\n  min_vel_trans : 0.05\n\n    #The absolute value of the minimum translational velocity for the robot in m/s \n\n  max_vel_x : 0.12\n\n    #The maximum x velocity for the robot in m/s. \n\n  min_vel_x : 0\n\n    #The minimum x velocity for the robot in m/s, negative for backwards motion. \n\n  max_vel_y : 0\n\n    #The maximum y velocity for the robot in m/s \n\n  min_vel_y : 0\n\n    #The minimum y velocity for the robot in m/s \n\n  max_vel_theta : 0.35\n\n    #The absolute value of the maximum rotational velocity for the robot in rad/s \n\n  min_vel_theta : -0.2\n\n    #The absolute value of the minimum rotational velocity for the robot in rad/s \n\n  meter_scoring: true\n\n     #\n  min_in_place_vel_theta: 0.0\n\n###Goal Tolerance Parameters\n\n\n  yaw_goal_tolerance : 0.1\n\n    #The tolerance in radians for the controller in yaw/rotation when achieving its goal \n\n  xy_goal_tolerance : 0.1\n\n    #The tolerance in meters for the controller in the x & y distance when achieving a goal \n\n  #latch_xy_goal_tolerance : false\n\n    #If goal tolerance is latched, if the robot ever reaches the goal xy location it will simply rotate in place, even if it ends up outside the goal tolerance while it is doing so. \n\n\n\n###Forward Simulation Parameters\n\n\n  sim_time : 1.3\n\n    #The amount of time to forward-simulate trajectories in seconds \n\n  sim_granularity : 0.025\n\n    #The step size, in meters, to take between points on a given trajectory \n  angular_sim_granularity: 0.017\n\n  vx_samples : 3\n\n    #The number of samples to use when exploring the ...", "subscribed to [/navigation/cmd_vel]\nno new messages\nno new messages\nno new messages\nno new messages\nno new messages\nno new messages\nno new messages\nno new messages\naverage rate: 7.104\n    min: 0.087s max: 0.203s std dev: 0.05103s window: 6\nno new messages\nno new messages\naverage rate: 1.499\n    min: 0.087s max: 3.298s std dev: 1.17773s window: 7\nno new messages\naverage rate: 1.378\n    min: 0.087s max: 3.298s std dev: 1.10086s window: 9\naverage rate: 1.447\n    min: 0.087s max: 3.298s std dev: 0.98927s window: 11\naverage rate: 1.537\n    min: 0.087s max: 3.298s std dev: 0.91835s window: 13\nno new messages\nno new messages\nno new messages\naverage rate: 1.166\n    min: 0.087s max: 3.797s ...", "/move_base/DWAPlannerROS/global_plan\n"], "url": "https://answers.ros.org/question/332903/strange-movement-with-dwa/"},
{"title": "rosbag record over network", "time": "2019-09-09 07:03:51 -0600", "post_content": [" ", " ", " ", " ", "Hi", "Is it recommended or common to record bags over a network? That is to store the bag file on a machine within the local network. Suppose the robot recording the bag was connected to the network over WiFi,  would there be sufficient bandwidth for bag recording though? Suppose the resolution is 640x480 at 30fps.", "It really depends on your network and the rate at which you're trying to record data into your rosbag. The compressed rosbag structure will take up less space than the original topics, so it should be more bandwidth efficient to run ", " on your robot and the save the file to a network drive, than it would for rosbag to subscribe to the same topics over the network.", "If you can tell us a bit more about your setup and what types of topics you need to record and we can let you know if it's realistic or not.", " Thanks. I was thinking to record the minimal needed for a XYZRGB point cloud, thus these topics:\n", "My reading is that ", " is 2 bytes per pixel and ", " is 1 byte per pixel. At 640 x 480 @30fps, that's 26MB/s. Not sure how much ", " needs. For 802.11n, perhaps that's a bit too much?", " will be 3 bytes per pixel, one each for red, green and blue. You ", " want to use the compressed version of these topics if you're sending the data over a wi-fi network. The last time I checked there isn't any compression support for 16 bit mono (i.e. depth) images in ROS. ", "rosbag can use zip compression as it creates a bag file though. You can test this by recording a short test bag on your robot and then calculating it's data rate. This will give you a good idea if it's possible to stream over your wi-fi.", "I'm giving it a try now, but I'm not sure if I'm doing it right. Am I meant to run ", " on the network machine, set ", " to ", " and run the camera driver and ", " on the robot? Strangely I'm not getting any recording though.", "Regarding your issue of not getting any recording you might take a look at the ROS ", ".", "From your brief problem description I am guessing that there might be no full two way connectivity.\nYou can easily check that by trying to subscribe to one topic published on your remote master from the command line. If this does not work either you know it is a network issue.\nIn order to record bag files or more generally subscribe to topics published on a remote machine, the hostnames of all participating machines have to be resolvable.", "While setting ", " is sufficient to view all available topics, it might be also necessary to ", " on the local PC you are using for recording the bagfile.", "A different option would be setting ", " explicitly on you local machine and add the host name to ", " file on the ...", "I've got it working now. The network connection is definitely not good enough..."], "answer": [], "question_code": ["rosbag record", "\ncamera/depth_registered/image_raw\ncamera/depth_registered/camera_info\ncamera/rgb/image_raw\ncamera/rgb/camera_info\n", "camera/depth_registered/image_raw", "camera/rgb/image_raw", "camera_info", "camaera/rgb/image_raw", "roscore", "ROS_MASTER_URI", "http://ip:11311", "rosbag record", "ROS_MASTER_URI", "export ROS_IP=<IP Address>", "ROS_HOSTNAME", "/etc/hosts"], "url": "https://answers.ros.org/question/332577/rosbag-record-over-network/"},
{"title": "Control System loop rate using CAN Communication", "time": "2019-09-02 01:47:24 -0600", "post_content": [" ", " ", " ", " ", "Hi, ", "We have 9 joints in our robot and following ", " -  joint_velocity_controller and CAN as a communication protocol between controller manager and Real Robot. ", "It is observed for our robot, the controller \"update\"  function is called at a loop rate of only 20-25 Hz(max) and most of the readings suggests 1kHz loop rate for robots like baxter etc. Not sure if this is the band width limitation of can bus because of 8 joints or some thing with our code. ", "What is the usual control loop rate that robots have? And how do you achieve Do u use CAN bus too ?", "You can see the canbus usage with an pcan dongel really easy.  Is also shows collisions and so on. Dou you have an rt Linux? Rt can driver. Have you enough processing power.", "Hi ", " , Currently, we are not using any rt Linux  at one stage we tried using Apollo rt-kernel but the improvement was not much / satisfactory. I think we have enough processing power, we are using intel NUC with i5 processor and 16GB RAM.  I am more curious about communication protocol used in other robots and loop rated achieved via CAN communication in other robots.", "We often drive in a 10ms loop rate on a Rt PLC for an differential drive robot.", "For 9 joints one can bus could be critical.", "What is the boadrade you are using. ", "Sopport your can motor driver 1ms control.", "As far I know FRANKA usas also 1ms control, but internally it is ethercat.", "Look for the schunk lwa p4 or uarm xarm they both use canbus internally.", "How meny can telegrams you need to get you status.", "If you collect all 9 drive status to get the imput for the next command camds for all drives then you do not achieve you goal. You have to control every drive induvidual."], "answer": [" ", " ", "The faster you update, the less is you control error. Less time less change. The faster you can react to errors.", "Actually, we are trying to synchronize positions of two joints but the control loop is so slow (20 Hz) it is giving us a very hard time."], "url": "https://answers.ros.org/question/332131/control-system-loop-rate-using-can-communication/"},
{"title": "How to read data from a serial port and publish it on a topic in C++", "time": "2019-10-01 08:30:50 -0600", "post_content": [" ", " ", "Hello,", "I am working on a ", " (BMS), and I need to collect data from it (battery voltage, temperature, etc...).\nThe connection is made through a USB cable, thanks to a ", " because the data is in a \"serial form\".", "I managed to connect the BMS to my virtual machine and I can see it in the list when I type ", " in the terminal of Ubuntu :", "Now my problem is that I need to read the data from this BMS and I don't know how to do :", "I use ROS only for a week now so I am very unexperimented, if you have an answer to my question please detail it as much as possible !", "Thank you", "Have you already checked whether there is some code that can interface with your BMS? A C/C++ or Python library perhaps? Is it completely custom, or could the mfg perhaps have some code?", "It might even be that there already is a ROS node available for it.", "Have a look at ", " these are examples to interface your FTDI IC. Regarding ROS you can either constantly publish your data or set up a service which responds with the read data. It really depends on what exactly you want to make your code do.", " : I searched on the internet but the BMS is sold by 123Electric and this company provides a software to use the BMS, so I don't think there is some open code anywhere... But maybe I don't searched on the right websites, do you know where I should do my researches ?", " These example are made for Windows I think so I can't use them on Ubuntu.\nYes I have seen the difference between publisher and service, for my work I can use both, the other programs will adapt their behavior to mine.", "the BMS is sold by 123Electric and this company provides a software to use the BMS, so I don't think there is some open code anywhere", "But you did say:", "Because I have the communication protocol, explaining that I should send a \"$\" and then receive data", "So the only thing that exists is a protocol spec?", "I don't think there is some open code anywhere", "it doesn't need to be open-source. A simple C/C++ or Python library would already help quite a bit.", " : I searched on internet but found nothing related to this device...", "Ok. Well. You could always ask the manufacturer. If you've already done that, it might be that writing something yourself is the way forward.", "Maybe you better use data transfer serial port and access via ", "? \nAs far as I know, it can work without problems on  Ubuntu, and there is functionality for creating a virtual com port."], "answer": [" ", " ", "For first test you can use the screen Programm to send the character $ you may need an neu line press enter. You need the boudrate", "For example so:  ", "You may need to add you user to plugdev and dailout group. ", "Then you can use to Programm it. I would recomand to use python because it is mutch simpler for the serial port handling. ", "If you have to use cpp the you may use the \nBoost serial_port as boost is paart of ros\n", "Thanks for your answer !", "I am going to study Boost serial_port, to see if I can use it"], "answer_details": [" ", " ", " ", " ", "What is the screen Programm ? Do I need to download it ?", "What is plugdev and dailout group ?", "I think I can use Python if needed"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Should I create a publisher to publish this data on a topic ?", "Should I download an extension made to deal with serial connection ?", "How could I, at least, \"ping\" my BMS to see if it is well connected to my Virtual Machine ? Because I have the communication protocol, explaining that I should send a \"$\" and then receive data but I don't know how to communicate with the BMS, where should I write this \"$\" ?"], "question_code": ["serial-over-ethernet.com"], "answer_code": ["screen /dev/ttyUSB0 9600\n"], "url": "https://answers.ros.org/question/334343/how-to-read-data-from-a-serial-port-and-publish-it-on-a-topic-in-c/"},
{"title": "Ubuntu got stuck (black/brown screen) before login? [closed]", "time": "2019-09-25 13:47:03 -0600", "post_content": [" ", " ", " ", " ", "Screenshot: ", "For some reason, Google drive is not working, try this: ", "I can't paste the image here because I don't have above 5 points, sorry.", "The reason I've closed this question is that it's off-topic for this site.", "Here on ROS Answers we already have over 48000 questions, and only so many users answering them. As such, to make sure we don't drown in questions, we must try to stay on topic as much as possible.", "Your post is about a feature of Ubuntu. While that is an OS that is often used with ROS, that does not make your question on-topic. There is no relation ROS (or at least: it's highly unlikely that ROS has anything to do with this).", "I would suggest you seek help on other fora that are dedicated to this kind of questions, such as Ask Ubuntu.", "Please also note that you've posted the same question 24 hours ago: ", ". That was also closed, for the same reason, but with a slightly less explanatory response.", "Please don't post duplicates. Regardless of the topic.", "I can't paste the image here because I don't have above 5 points.", "If you would go back to take a look at some of your earlier questions that have received answers, and then marked some of those questions as ", ", you would receive karma points which would allow you to post screenshots.", "You can accept answers by clicking the checkmark to the left of the answer. It should turn green.", "Thanks!  I just reposted the question on Ubuntu instead."], "answer": [], "url": "https://answers.ros.org/question/333942/ubuntu-got-stuck-blackbrown-screen-before-login/"},
{"title": "rplidar disconnects kinect 360", "time": "2019-08-28 05:29:05 -0600", "post_content": [" ", " ", "when I start the lidar, the Kinect is disconnected. and it turns on only after rebooting the pc.\nKinect has external power\nros version: kinetic;\nUbuntu: 16.04", "Doesn't sound like a ROS Problem, so it is offtopic. But try an external power source for rplidar.", "Separately powered usb hub helped"], "answer": [], "url": "https://answers.ros.org/question/331840/rplidar-disconnects-kinect-360/"},
{"title": "How to fix Ubuntu 18.04's black screen on Virtual machine? [closed]", "time": "2019-09-24 13:21:33 -0600", "post_content": [" ", " ", " ", " ", "It was working yesterday.  But it shows a black screen when I started Ubuntu with VM.  I restarted my computer and VM a couple of times, but it's still showing a black screen.  Why?  Can someone help me?  T_T", "I think it is not a ros problem but more a problem of you Ubuntu or vm problem. So please only ros stuff here. ", "Hey I just got it working by choosing \"Start Normal\" under the big green arrow ... Not sure why tho"], "answer": [], "url": "https://answers.ros.org/question/333826/how-to-fix-ubuntu-1804s-black-screen-on-virtual-machine/"},
{"title": "Adapting into twist geometry messages", "time": "2019-10-22 02:12:26 -0600", "post_content": [" ", " ", "Hey guys!", "Sorry for the newbie question. I am working on an ROS robot and ive been using Int32 to transmit motor power and servo angles. However, the other day I came across the twist message type, and it looks like what i need, specially considering we will want to eventually use the move_base. ", "Im afraid i do not fully understand how twist works, and i would greatly greatly appreciate some help adapting my program.", "I use and arduino with a polou motor controller and a node for each motor at this time. I will the leave my current program over here just for reference:", "Thank you in advance!", "We're all here to help, so don't take this as me writing \"please go away and find it out yourself\". Thing is, this exact topic has been discussed quite a large number of times here on ROS Answers.", "So it would be really appreciated if you could do a quick search (Google: ", " or similar keywords), read those Q&As and then come back here to tell us which ones you've read and if they're unclear, explain what is still unclear about them.", "You'll perhaps also want to take a look at ", ", in particular the ", " (that is, if you're using a differential drive setup of course)."], "answer": [], "question_code": ["twist to wheel velocities site:answers.ros.org", "ros_control"], "url": "https://answers.ros.org/question/335882/adapting-into-twist-geometry-messages/"},
{"title": "How to diagnose when a URDF problem stops the gazebo_ros_control from receiving", "time": "2019-10-14 18:14:08 -0600", "post_content": [" ", " ", " ", " ", "I have a URDF ackermann vehicle that works just fine, until I put the LiDAR on top of it. The LiDAR works. I can visualize the point cloud in RVIZ, but Gazebo stops responding to the messages I publish to the joint controllers. I echo the joint topic and see the messages going in, but the wheels don't move. ", "UPDATE: ", "\nThat is the problem. [WARN] [1571345774.639031, 0.000000]: Controller Spawner couldn't find the expected controller_manager ROS interface.", "How can I diagnose this odd issue? Here is my xacro file. The trouble maker is the last bit, where I define the sensor link for the Velodyne laser. This same bit works in other robots... Any ideas? Thanks. ", "I don't know for sure or what is happening, but the ", " files ", " load Gazebo plugins in their ", "s. It could be that what you have in your model is conflicting somehow with what the VLP xacro is doing.", "You may have already guessed that, but to test, you could disable the Gazebo-plugin in the VLP ", " and see whether that changes anything.", "I do not know if this matters but the definition of the transmission since ROS Indigo is:", "Seems like to me, that you are not able to use the controllers cause they are not loaded in RobotHW.", "That is the problem. [WARN] [1571345774.639031, 0.000000]: Controller Spawner couldn't find the expected controller_manager ROS interface.", "This warning came up after i improved some other things about the urdf that were causing warnings. I tried using your example here: ", "<gazebo>\n    <plugin filename=\"libgazebo_ros_control.so\" name=\"ros_control\">\n      <robotnamespace>/ack</robotnamespace>\n      <robotparam>/ack/gem</robotparam>\n      <controlperiod>0.001</controlperiod>\n      <robotsimtype>gazebo_ros_control/DefaultRobotHWSim</robotsimtype>\n      <legacymodens>true</legacymodens>\n    </plugin> ", "\n  </gazebo>", "My namespace is /ack and my robot name is gem. I'm running out of ideas... ", "For one experiment I generated the urdf from my xacro, It's the temp2.txt file here: ", "The xacro is gem_1kg_LiDAR.xacro", "I added the output from the launch in my original post. Any advice is appreciated. Thanks.", "Please update your question with this sort of information and make sure to format it properly. Right now it's very hard to read, and comments are too limited to contain this sort of update.", "Edit your original question. Use the ", " button/link for that.", "Hi,", "The repo you posted is not accessible. It would be good if you give read access.", "On the other hand, I think the problem does not rely on the VLP-16 instantiation, and the plugin seems to launch correctly. That warning can be caused by several things, like lack of computing power that force the controller manager to not have enough time to load, conflicts when loading plugins (like the VLP-16)...", "I am using the files you provided and I think you may have some collisions problems between links. My Gazebo is unable to start both the model and the controllers.", "Also ensure you have installed ", " and ", ", ", ".", "Furthermore, you are running ", " and ", " in a different namespace, this is not advisable unless you do the proper remaps.", "Hi Weasfas, thanks. I will update and try using one namespace for both robot_state_publisher and joint_state_publisher. Also, I set my repo to Public. (I thought I already had...) ", "The computer resources I think are fine. I have another robot using joint publishers along with this same LiDAR setup. Maybe it is a conflict. I will have to do more experiments.", "Hi ", "After doing some tests with your package I found a several problems that, once fixed, let me display and control the base on Gazebo.", "First of all, you need to take care of the tag ", " in the gazebo reference, because is causing self colliding problems in your model.", "Next, you will need to change the VLP-16 description to adjust to your environment: you are loading a ", "5 Lidar, that is huge depending on the machine and resources allocate by Gazebo, Try to lower that number and launch the simulation. With those fixes you will be able to launch it and Gazebo will have enough time to load the controllers."], "answer": [], "question_code": ["    <?xml version=\"1.0\"?>\n    <robot name=\"gem\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n\n    <!-- Vehicle Dimensions -->\n    <xacro:property name=\"wheel_base_length\" value=\"1.753\"/>\n    <xacro:property name=\"wheel_base_width\" value=\"1.245\"/>\n    <xacro:property name=\"wheel_diameter\" value=\"0.584\"/><!--.292 -->\n    <xacro:property name=\"wheel_thickness\" value=\"0.178\"/>\n\n    <!-- Macros -->\n    <xacro:macro name=\"wheel\" params=\"name radius width material caster_offset mass\">\n      <link name=\"${name}\">         \n            <collision> \n              <origin xyz=\"0 0 0\" rpy=\"0 1.5708 1.5708\" />\n              <geometry> \n               <cylinder length=\"${width}\" radius=\"${radius}\"/> \n              </geometry> \n            </collision>\n            <visual> \n              <origin xyz=\"0 0 0\" rpy=\"0 1.5708 1.5708\" /> \n              <geometry> \n                <cylinder length=\"${width}\" radius=\"${radius}\"/> \n              </geometry> \n              <material name=\"black\"/> \n            </visual>\n            <inertial> \n              <origin xyz=\"0 0 0\" rpy=\"0 1.5708 1.5708\" /> \n              <mass value=\"0.2\"/> \n              <!--<cylinder_inertia m=\"0.2\" r=\"0.3\" h=\"0.1\"/> -->\n          <cylinder_inirtia m=\"${mass}\" r=\"${radius}\" h=\"${width}\" />\n          <!--<inertia ixx=\"0.23\" ixy=\"0\" ixz=\"0\" iyy=\"0.23\" iyz=\"0\" izz=\"0.4\"/>-->\n          </inertial> \n        </link> \n      <gazebo reference=\"${name}\"> \n        <mu1 value=\"2.0\"/> \n        <mu2 value=\"2.0\"/> \n        <kp  value=\"10000000.0\" /> \n        <kd  value=\"1.0\" /> \n        <fdir1 value=\"0 1 0\"/> \n        <material>${material}</material> \n      </gazebo>\n    </xacro:macro>\n\n    <xacro:macro name=\"wheel_steer\" params=\"lr lr_reflect\">\n      <xacro:wheel name=\"${lr}\" radius=\"${wheel_diameter/2}\" width=\"${wheel_thickness}\" material=\"Gazebo/Blue\" caster_offset=\"0\" mass=\"6\"/>\n      <xacro:wheel name=\"${lr}_assembly\" radius=\"0.1\" width=\"0.001\" material=\"Gazebo/White\" caster_offset=\"-.5\" mass=\"0.5\" />\n\n      <joint name=\"${lr}_hinge\" type=\"revolute\"> \n        <parent link=\"chassis\"/> \n        <child link=\"${lr}_assembly\"/> \n        <origin xyz=\"${wheel_base_length/2} ${lr_reflect*(wheel_base_width/2)} ${wheel_diameter/2}\" rpy=\"0 0 0\" />  \n        <axis xyz=\"0 0 1\" rpy=\"0 0 0\" /> \n        <limit effort=\"100\" velocity=\"1\" lower=\"-1\" upper=\"1\"/> \n        <dynamics damping=\"0.0\" friction=\"0.0\"/> \n      </joint> \n      <joint name=\"${lr}_rotate\" type=\"continuous\"> \n        <parent link=\"${lr}_assembly\"/> \n        <child link=\"${lr}\"/> \n        <origin xyz=\"0.0 ${lr_reflect*(wheel_thickness/2)} 0.0\" rpy=\"0 0 0\" />  \n        <axis xyz=\"0 1 0\" rpy=\"0 0 0\" /> \n        <limit effort=\"100\" velocity=\"50\"/> \n        <dynamics damping ...", "velodyne_description", ".xacro", ".xacro", "  <gazebo>\n    <plugin filename=\"libgazebo_ros_control.so\" name=\"ros_control\">\n      <robotNamespace>robot_ns</robotNamespace>\n      <robotParam>robot_description</robotParam>\n      <controlPeriod>0.001</controlPeriod>\n      <robotSimType>gazebo_ros_control/DefaultRobotHWSim</robotSimType>\n      <legacyModeNS>true</legacyModeNS>\n    </plugin>    \n  </gazebo>\n\n<transmission name=\"trans_name\">\n  <type>transmission_interface/SimpleTransmission</type>\n  <joint name=\"joint_name\">\n    <hardwareInterface>hardware_interface/EffortJointInterface</hardwareInterface>\n  </joint>\n  <actuator name=\"joint_motor\">\n    <mechanicalReduction>1</mechanicalReduction>\n  </actuator>\n </transmission>\n", "edit", "gazebo_ros_pkgs", "ros_control", "ros_controllers", "robot_state_publisher", "joint_state_publisher", "<selfCollide>", "samples:=187"], "url": "https://answers.ros.org/question/335329/how-to-diagnose-when-a-urdf-problem-stops-the-gazebo_ros_control-from-receiving/"},
{"title": "Oscillation using TEB local planner with car-like setup", "time": "2019-08-16 04:25:52 -0600", "post_content": [" ", " ", "I've come across other topics mentioning similar problems, oscillation when using the TEB local planner for a car-like robot. But nothing has helped me so far, this topic yet remains unanswered, which I think experiences the same problem: ", "Our robot uses a GPS, IMU and encoders for the wheels. All is fused together with robot_navigation package.", "When I simulating the same robot in Gazebo everything works fine, and I do NOT see this oscillation when navigating.\nHowever, when I use the real robot, and give it a goal, it immediately turns left, and then after a while turns right, making up this snake like path. ", "I've tried multiple configurations in teb_local_planner_params. This is the current config:", "This is the global costmap params:", "And this is the local_costmap_params:", "I am facing the same problem while using the TEB, after weeks of confusion my team find that it might be happening because of less computational power of my laptop CPU. When we trying to implement the same on i7 desktop CPU its map updating smoothly but because of some issues with networking we still can't able to run it completely on i7 but we are little bit sure its computation which update with delay which left the robot with some constant velocity for some time.\nHence the robot start oscillating right and left.", "Yes, we faced that issue as well, when you're running the planner it gives you warnings indicating the loop time is compromised.\nBut the main problem was actually that the GPS location on the robot and in the .xacro model didn't match. On our real robot we put in on the back of the robot, but in the simulation it was located in the center. So what we did was change the GPS and placed it in the center of the robot :) Then it begun to work! "], "answer": [" ", " ", "The main problem was actually that the GPS location on the robot and in the .xacro model didn't match. On our real robot we put in on the back of the robot, but in the simulation, it was located in the center. So what we did was change the GPS and placed it in the center of the robot :) Then it begun to work!"], "question_code": ["    base_local_planner: teb_local_planner/TebLocalPlannerROS\n\nTebLocalPlannerROS:\n\n#### Miscellaneous Parameters ####\n  odom_topic: \"odometry/imu_filtered\"\n  map_frame:  \"/map\" \n\n#### Robot Configuration Parameters ####\n  acc_lim_x:           0.15\n  acc_lim_theta:       0.3\n  max_vel_x:           0.15\n  max_vel_x_backwards: 0.15 \n  max_vel_theta: 0.2 \n\n# ********************** Carlike robot parameters ********************\n  min_turning_radius: 1       # Min turning radius of the carlike robot (compute value using a model or adjust with rqt_reconfigure manually)\n  wheelbase: 0.4525                  # Wheelbase of our robot\n  cmd_angle_instead_rotvel: true \n# ********************************************************************\n\n  max_vel_y: 0.0\n  acc_lim_y: 0.0 \n\n  footprint_model/type:         \"point\"     \n\n  #### Goal Tolerance Parameters ####\n  #   2.5  degrees: 0.0436332313\n  #   5.0  degrees: 0.0872664626\n  #   10.0 degrees: 0.174532925\n  xy_goal_tolerance:  0.2\n  yaw_goal_tolerance: 0.2 \n  free_goal_vel:      false \n\n  #### Trajectory Configuration Parameters ####\n  dt_ref:                            0.3 \n  dt_hysteresis:                     0.1\n  min_samples:                       3  \n  global_plan_overwrite_orientation: true\n  global_plan_viapoint_sep:         -0.1\n  max_global_plan_lookahead_dist:    10\n  force_reinit_new_goal_dist:        1.0   \n  feasibility_check_no_poses:        4 \n  publish_feedback:                  false\n  shrink_horizon_backup:             true\n  allow_init_with_backwards_motion:  false\n  exact_arc_length:                  false\n  shrink_horizon_min_duration:       10.0\n  teb_autosize:                      true\n\n\n  #### Obstacle Parameters ####\n  min_obstacle_dist:                   0.5\n  include_costmap_obstacles:           false\n  costmap_obstacles_behind_robot_dist: 0.1\n  obstacle_poses_affected:             1\n  inflation_dist:                      0.0 \n\n  include_dynamic_obstacles:                   false\n  legacy_obstacle_association:                 false\n  obstacle_association_cutoff_factor:          5.0 \n\n#  The following parameters are relevant only if costmap_converter plugins are desired (see tutorial):\n#  \"costmap_converter::CostmapToPolygonsDBSMCCH\"\n#  \"costmap_converter::CostmapToLinesDBSRANSAC\"\n#  \"costmap_converter::CostmapToLinesDBSMCCH\"\n#  \"costmap_converter::CostmapToPolygonsDBSConcaveHull\"\n  costmap_converter_plugin:      \"\" \n  costmap_converter_spin_thread: true \n  costmap_converter_rate:        5.0  \n\n  #### Optimization Parameters ####\n  optimization_activate: true\n  optimization_verbose: false\n  no_inner_iterations:              5  \n  no_outer_iterations:              4  \n  penalty_epsilon:                  0.1 \n  weight_max_vel_x:                 2.0 \n  weight_max_vel_theta:             1.0 \n  weight_acc_lim_x:                 1.0 \n  weight_acc_lim_theta:             1.0 \n  weight_kinematics_nh:             1000.0 \n  weight_kinematics_forward_drive:  1000.0\n  weight_kinematics_turning_radius: 10.0 \n  weight_optimaltime:               100.0\n  weight_obstacle:                  1.0\n  weight_viapoint:                  1.0 \n  weight_inflation:                 0.1 \n  weight_adapt_factor:              2.0 \n\n  #### Parallel Planning in distinctive Topologies ####\n  enable_homotopy_class_planning:  true\n  enable_multithreading:           true \n  max_number_classes:              4    \n  selection_cost_hysteresis:       1.0 \n  selection_obst_cost_scale:       100.0 \n  selection_viapoint_cost_scale:   1.0  \n  selection_alternative_time_cost: false \n  roadmap_graph_no_samples:        15\n  roadmap_graph_area_width:        6.0 \n  h_signature_prescaler:           1.0 \n  h_signature_threshold:           0.1 \n  obstacle_heading_threshold:      1.0\n  visualize_hc_graph:              false \n  viapoints_all_candidates:        false\n  switching_blocking_period:       0.0\n", "global_costmap:\n  global_frame: /map\n  robot_base_frame: /base_footprint\n  update_frequency: 1.0\n  static_map: true\n\n  static_map: true\n  rolling_window: true\n  width: 25.0\n  height: 25.0\n  resolution: 0.010\n  transform_tolerance: 10.0\n  cost_scaling_factor: 0.0\n  unknown_cost_value: 253\n  inflation_radius: 0.0\n", "local_costmap:\n  global_frame: /world\n  robot_base_frame: /base_footprint\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: true\n\n  static_map: false\n  rolling_window: true\n  width: 5.0\n  height: 5.0\n  resolution: 0.01\n  transform_tolerance: 10.0\n  cost_scaling_factor: 0.0\n  unknown_cost_value: 253\n"], "url": "https://answers.ros.org/question/330868/oscillation-using-teb-local-planner-with-car-like-setup/"},
{"title": "How to consecutively set multiple goals in OpenPlanner?", "time": "2019-10-31 07:34:15 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm trying to follow the latest tutorial of OpenPlanner, ", ".\nIn this tutorial, it is seen (00:02:04 ~ 00:02:27) that multiple goal locations are consecutively set by using 2D Nav Goal in rviz. \nHowever, I could set only one single goal location, with no such red and green circles indicating goal locations. What possibly makes this difference?", "FYI, the version of OpenPlanner is different. It is backed by the fact that the version in the tutorial has much more options specifically in the compute tab than the version I'm using. The version I'm using has come with the latest Autoware (v1.12), so it should be newer than the version in the tutorial (v1.5).", "I'd like to describe my problem with images but I'm lacking of karma. You'd rather watch ", ". \nThanks!", "You'd rather watch ", ". ", "let's not link people to more off-site videos.", "I'd like to describe my problem with images but I'm lacking of karma. ", "please do post screenshots, but ", " to show UI. For terminal text and code, copy paste into your question.", "I've given you sufficient karma to attach images."], "answer": [" ", " ", "Hello,", "The steps are all correct, except choosing the rviz config file, op_planner_sim.rviz doesn't visualize the goal points. \nif you use open_planner.rviz it contain visualization items you need. ", "Regards, ", "Thanks to your help, Hatem, the goal points are now visualized by /op_destinations_rviz [visualization_msgs/MarkerArray] in open_planner.rviz."], "url": "https://answers.ros.org/question/336524/how-to-consecutively-set-multiple-goals-in-openplanner/"},
{"title": "Intel NUC PC crashes while using Rviz", "time": "2019-11-11 09:24:06 -0600", "post_content": [" ", " ", "Hello ROS-Community,", "To reproduce the error I launch following Commands:", "These are my Specs: \nNUC Intel Core i5-7260U, 8GB RAM, 240GB SSD, 1TB HDD", "I use the PC with Turtlebot 2 (ros kinetic). ", "Camera: Orbbec Astra", "These are my settings in Rviz:", "I also tried launching minimal.launch and astra.launch on the PC and using Rviz on another laptop. It works but i have a very low framerate. I also tried it while charging the turtlebot. That did not work either.", "Has anyone had a similar error?\nHelp is much appreciated", "Yours faithfully", "Can you describe what actually happens when you do this? Any error messages, or other visible effects?", "When i start rviz and set the parameters, i can see for some seconds video from the camera, after that the pc crashes.", "Where could i look for error messages?", "What do you mean by \"the pc crashes\"? Does it reboot, does it shut off, do you get kernel error messages?", "the pc shuts off", "Hmm. It could be many things, but it sounds like a power supply problem to me. I'd make sure all the drivers on the NUC are up to date, but if that doesn't help, my guess is you are stressing the NUC beyond what you normally do, and it is drawing too much power (Maybe it's the camera drawing too much) and then it shuts down because the power supply can't provide enough. Do you have the correct power supply for the NUC?", "I agree with ", ": HW shutting down when heavy workloads are applied would seem to point to either thermal issues or PSU problems. I'd check both of these."], "answer": [], "question_code": ["roslaunch kobuki_node minimal.launch \nroslaunch astra_launch astra.launch\nroslaunch rviz rviz\n"], "url": "https://answers.ros.org/question/337331/intel-nuc-pc-crashes-while-using-rviz/"},
{"title": "Need help to get coordinates from rviz running rplidar", "time": "2019-11-07 13:02:55 -0600", "post_content": [" ", " ", "Hello everyone,\nI am running ROS Melodic on raspberry pi 4 (Debian). I have installed rplidar as well as Hector_Slam and both are running fine. I can view track my sensor as well as view the map in rviz now. The point is that I want to extract the coordinates of the points that are shown in rviz and use that to create my own map. I have tried using rosbag record to record the bag file. I run the roscore and open rviz and then play the bag file using rosbag play but the rviz does not show anything. I do not know if my bag file contains the data I need. I do not know how pointcloud, laserscan or pointcloud2 works in rviz. Could you please help me with this? I am stuck in this step for far too long.\nThank you for your time. Much appreciate!", "What command did you use to record data to rosbag?", "I used $ rosbag record -a to record everything. I tried using $ rosbag record /scan as well. Both didnt work.", "Can you play your rosbag alone in one terminal and in a new terminal, enter ", " and see which topics have shown up?", "This is what I see in the terminal while a bag file is playing:\npi@raspberrypi:~ $ rostopic list\n/clock\n/initialpose\n/map\n/map_metadata\n/move_base_simple/goal\n/poseupdate\n/rosout\n/rosout_agg\n/scan\n/slam_cloud\n/slam_out_pose\n/tf\n/tf_static\npi@raspberrypi:~ $", "Is any data showing up when you do ", " other than 'NAN' or '0'?\nAlso, how is your RPi being powered up?", "A lot of data shows up while i use that code. Also, I am using 5v adapter to power my raspberry pi.", "Can you share your ROS bag?", "Deepak, sorry for the late reply. Here is the bag file.\n", "Please help me extract coordinates of the points for each revolution."], "answer": [], "question_code": ["$ rostopic list", "$ rostopic echo /scan"], "url": "https://answers.ros.org/question/337098/need-help-to-get-coordinates-from-rviz-running-rplidar/"},
{"title": "NUI camera and NUI audio kinect xbox360 not detected on ubuntu", "time": "2019-11-21 11:19:13 -0600", "post_content": [" ", " ", "i've installed freenect library, but when i lusb that's only show NUI motor..", "I also add power supply dc 12 v 2 A to my kinect", "this is my lsub shown"], "answer": [" ", " ", "You may post your dmesg wen you connect the kinect. As a far I know the kinekt likes to have its own usb 2.0 root hub. Is any led on the kinect light up? ", "i've  used usb 2.0 root hub, and its's still not working, yes the led in kinnect is light up", "And your dmesg output just after plug it in"], "url": "https://answers.ros.org/question/338244/nui-camera-and-nui-audio-kinect-xbox360-not-detected-on-ubuntu/"},
{"title": "Kinect Xbox 360 RaspberryPI", "time": "2019-11-13 10:57:23 -0600", "post_content": [" ", " ", " ", " ", "Hello to everyone ! I need some help with the Kinect using RPI.", "I used a RPI 3 model B and i connected the kinect, with external power supply. All works great, but when i tested the USB transfer speed, i noticed that the speed was ~20MB/s, when in my PC, the speed is 27MB/s. Anyone knows how to improve that speed ? ", "I also tested using RPI 4 model B, but the USB transfer speed was worse, I got 15MB/s only.", "Thank you very much for your help."], "answer": [], "url": "https://answers.ros.org/question/337589/kinect-xbox-360-raspberrypi/"},
{"title": "ROS setup with cmake", "time": "2019-11-12 05:06:17 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "Is it possible to compile ROS from source code using a single cmake command without using rosdep and so on tools ?", "Regards,\nS.Ancelot", "Edit: We have add a look at ROS, and the reasons to willing understand and being able to compile it is a consequence of our feedback regarding ROS.", "Unfortunately, at time of writing, we think this project is nice only for research projects , but not for use in industrial  company context. And thus not ready for ROS-INDUSTRIAL for next reasons. And this is mainly a software locking issue.", "As machine builder and robot integrators, we should be able to provide robotician guys a platform easy to setup , use and ideally  enhance.", "The first step is setup. ROS should be able to be setted up easily on any platform. either binary packages are available for the operating system you are using or we can build it and integrate it. \nWe are building our own CNC software and we must being able to know what goes inside.", "This means, we will decide on which operating system to run, kernel settings , which drivers and so on ..... At the moment software is locked to some distributions and system dependencies. That's really a locking issue.", "In our software (and robotics) research development team (and any software development team), guys are used to use common used tool for developing software (CMake, autoconf, gcc compilers ....etc etc ....). Here you are providing a kind of black box (rosdep and so on tools), we can not master what is doing on the backside??? We  must be able to monitor / check any package licenses used too.", "It looks like on the back side, CMake is used, that's good to know. Cmake is provided with all battery included, this is why we don't understand the need to provide a frontend layer that complicates it, any developer don't know wtf it does,  and will  finally loose everybody.  Regarding cmake, it is easy  to download / use any particular libs that won't be available in the system and provide a complete build / install system, thus fullfills all the requirements for ROS ?. CMake can too be parameterized thanks to options. \nHaving a look at ", " link you provided, they seem being in \"real life \" reading only the introduction:  ", "for portability reasons and for\n  customizing the build via CMake\n  options", "Regarding usage:\nWe have not be able to use it in real contexts, since we had to provide a \"custom ROS computer\" in our applications and next to our machines (Our applications are integrated : no need to have a computer for each function, one is enough). \nBut from offline programming viewpoint:\nTo use ROS, command line usage is needed. This is really of another epoch, and can not be deployed in industry . Some tools need being developped to avoid it.\nThen, we should be able to customize interfaces and functions to the final application .\neg simulation software (eg gazebo, to provide the ..."], "answer": [" ", " ", " ", " ", "re: your edit: thanks for the edit, this provides a lot more context and to be honest your original question was really unanswerable without that context.", "This is a prime example of an xy-problem I believe.", "As to your edit:", "Unfortunately, at time of writing, we think this project is nice only for research projects , but not for use in industrial company context. And thus not ready for ROS-INDUSTRIAL for next reasons.", "If you feel this technology is not for you, that is ok. No one is going to force you to use it.", "As to use in industrial contexts: it could very well be true that for your use-cases using ROS does not bring value. However I would not make a blanket statement as you do in your edit. The events listed ", " alone indicate that there is certainly a viable way to use ROS 1 in industrial contexts. I would refer you to the RIC-EU 2018 conference recordings (", ", ", ", ", " and ", ") to get an impression of the many companies and research organisations using it in production environments (and yes: also in RnD).", "And this is mainly a software locking issue. As machine builder and robot integrators, we should be able to provide robotician guys a platform easy to setup , use and ideally enhance.", "The first step is setup. ROS should be able to be setted up easily on any platform. either binary packages are available for the operating system you are using or we can build it and integrate it. We are building our own CNC software and we must being able to know what goes inside.", "This means, we will decide on which operating system to run, kernel settings , which drivers and so on ..... At the moment software is locked to some distributions and system dependencies. That's really a locking issue.", "This is what I meant with providing more context: your question \"can I install ROS using CMake\" seems to really have been: \"", "\".", "The answer to ", " question would be: yes, there are.", "Take a look at ", " (and ", ") for instance which provides an \"OpenEmbedded Layer for ROS\". The various ", " presentations about Docker-based build and deployment pipelines may also be interesting (2018 had some nice ones). There have also been presentations about orchestration and fleet management using tools such as Kubernetes and others.", "Builds and deployment for embedded systems are being discussed in the ", " category on ROS Discourse and in the ", " category.", "While I'm not going to pretend this will solve your specific problems, it shows that others (including a rather large number of companies) have recognised that alternative deployment strategies to ", " need to exist and those have been created.", "I would perhaps suggest to take a look at those, as right now I believe you have a somewhat skewed or one-sided impression of ROS ..."], "answer_code": ["catkin_make"], "url": "https://answers.ros.org/question/337420/ros-setup-with-cmake/"},
{"title": "Move base navigation malfunctions", "time": "2019-12-12 10:27:55 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I am developing a diff wheel autonomous robot using the move_base package. I use wheel encoders, a lidar sensor and AMCL to estimate the current pose. When I set a simple goal, 1 meter to the right of the initial pose which I manually set, the global plan of NavfnRos seems fine. However the local plan deviates from this plan, which causes the robot not to get to its goal. See the screenshot below.\n", "\nYou see the upper green line is the path as retrieved from topic /move_base/TrajectoryPlannerROS/local_plan. ", "\nThe other green line is the path as retrieved from topic /move_base/NavfnROS/plan.\nThe rectangle is the robot's footprint, and the attached arrow the current pose. ", "\nLastly, the lower arrow is the goal pose. ", "My question is: why does the local plan diverge from the global plan? It would make a lot more sense if the local plan would turn right instead of left. ", "\nAny help would be appreciated!", "\nHere is an RVIZ capture video where you see the behaviour of the local path: ", "Thanks in advance,\nSimon"], "answer": [" ", " ", "First off, your image isn't included. But I think your question boils down to this sentence ", "why does the local plan diverge from the global plan?", "for which I do not need an image to answer. You could implement a local planner (controller) to follow a path directly. These are generally referred to as pure pursuit controllers and I'm sure you can find a couple online. Even DWB (the later version of DWA) can be tuned that way, given time. ", "There are several reasons why you may want a controller to deviate from a path, I'll mention a few intuitive ones.", "The global planner in ROS is A* or Dijkstra that do straight graph search. They do not take into account the kinematics of the vehicle so paths do not meet the kinematic constraints of the robot itself. Deviating lets the robot actually follow it in some cases.", "There may be situations where you would like to not replan often unless absolutely required. In that case, you would like your robot to be able to deviate from the route that it planned (lets say over 100 meters, around corners, where the robot can't possibly know the current state of the environment) so it can continue on its way. ", "Sometimes, stuff just happens. The local costmap generally updates faster and could be higher resolution than the planning space. In the case that something unexpected happens, its useful for the robot to be able to smoothly react to it (if possible) rather than stop dead in its tracks while it tries to replan. ", "Thanks for your answer! I changed my image link, hope it works now. Just to point something out: the robot does not only deviate from the global plan, but also it does not reach the goal while it is a very simple one: just +1 meters in x direction. So that is actually my main issue with the move_base package.", "This appears to be issues with localization or positioning, not move base."], "url": "https://answers.ros.org/question/339811/move-base-navigation-malfunctions/"},
{"title": "Path planned through unmapped region for goal present inside map.", "time": "2019-11-26 06:20:23 -0600", "post_content": [" ", " ", " ", " ", "could someone explain why sometines the global path planned by the robot for navigation ventures into the unmapped region? \nImages of the path planned to the goal are attached below. \nThe planned path is in green.\n", "Through normal understanding, the global planner plans the shortest path in the mapped region and wouldn't traverse the unmapped region even if the goal point appears much closer through this.", "Is there a way to correct this apart from editing the map? ", "Global planner : NavfnROS", "Local planner  : TrajectoryPlannerROS", "OS: Ubuntu 14.04.        ROS: Indigo. ", "Thank you.", "Edit: "], "answer": [" ", " ", " ", " ", "Make sure you have set in ", ":", "I have observed that sometimes the first plan (after setting a goal) can still be wrong, but then it gets updated by the next plan (which is correct). If the planner frequency is high enough, then the robot does not have a chance to do anything undesirable.", "Also, from:", "So, make sure you have also set in ", ":", " Yeah you're right about the parameter. In-fact i had taken care of that earlier by changing the default value that's ", " to ", ".  But the problem still persists and that is waht intrigued me.  The presence of ", " is to prevent the planner in planning into unknown regions. ", "So, if planing into unknown regions still persists, what else could influsence it? \nAlso waht value of planner frequency do you think is generally good? I've tried with ", " and few values moderately greater than that. ", "Edited the question with another image of the wrong planning happening.", "Check this...", "The above may solve your issue.", "Robots around here use: ", " (in move_base_params.yaml), which seems to be a reasonable balance between keeping the plan updated with respect to incoming data vs. CPU loading.", "Exactly. This is what I was trying out the other day after checking about the global planner ", " on the Wiki page. ", "The above discussion is the one I was referring. ", "The robot ", " is as follows ", "So, as the ", " parameter wasn't present earlier, i thought adding it under the ", " would suffice. But it didn't change anything.", "  any suggestion on this? I appreciate the help provided.", "Please attach the entire modified 'costmap_common_params.yaml'."], "answer_code": ["navfn_global_planner_params.yaml", "allow_unknown: false          #Specifies whether or not to allow navfn to create plans that   traverse unknown space, default true\n", "track_unknown_space", "costmap_common_params.yaml", "  track_unknown_space:  true    #true needed for disabling global path planning through unknown space\n", "True", "False", "allow_unknown: false", "0.0", "planner_frequency: 1.0", "costmap_common_params.yaml", "obstacle range: 2.5\nraytrace_range: 3.0\n\nfootprint: [[0.35, -0.3], [0.35, 0.3], [-0.35, 0.3], [-0.35, 0.3]]\ninflation_raduis: 0.05\n\nobstacle_layer:\n    observation_sources: hokuyo_laser\n    hokuyo_laser: {sensor_frame: hokuyo_laser_link, data_type: LaserScan, topic: /hokuyo_base/scan, marking: true, clearing: true}\n", "track_unknown_space", "obstacle layer"], "url": "https://answers.ros.org/question/338615/path-planned-through-unmapped-region-for-goal-present-inside-map/"},
{"title": "local costmap window slowly drifts over static global map", "time": "2019-01-31 21:12:06 -0600", "post_content": [" ", " ", "I recently purchased turtlebot3 burger. I am running kinetic. I created a map using gmapping and explorer_lite. Next I started navigation. In rviz I set a goal 1 meter behind and in a rotated 90 degrees to the right. The robot starts to move backwards which it does not do well. It starts to buck up and down. The robot then stopped but the local costmap slowly consistently drifted backwards. This is the 2nd time I saw this happen. The first time it was a smaller map. in both instances the robot was stopped. Eventually I get a errors about DWA planner failed to produce a path and scans being out of bounds. The scans shown in the cost map still look look correct for the location of the robot. It just does not match the static map. Any ideas?", "Without more information (maybe a screen vid? or even better a rosbag with all topics) it is hard to debug... What I've seen sometimes is that localization is drifting, even though the robot is standing still. This would explain all problems you describe. However, this is just a guess...", "I retried with a bag recording but of course is did not happen.  It might be tied when the battery getting low.I will update with a bag file when I can reproduce the issues.", "Please follow up with a response, once you found a solution. Also, if you didn't or if it didn't occur. This might help future readers.", "Thanks.", "Sorry I do not have a way to record. This is definitely only happens when the turtle3 burger  battery gets low. I here a single beep. The robot stops moving but the local window keeps moving in the same direction at the same speed until the robot position on the local windows gets to the edge of the global map or over a know obstacle on the global map window. Then the local window stops moving and starts throwing all kinds of messages about not able to find a plan. ", "My theory is that some how the global planner continues to use that last speed and direction even though the robot is not moving.  Something I need to check is if odom still being published by the raspberrypi after the beep. ", "This is difficult to diagnose as it only happens when the battery get low (every time for the 4 ...", "Well, if the battery gets low and the motors shut down, it could very well be that the odometry is still being published. If communication is down, some drivers still publish the last value.", "My guess is that this is the problem"], "answer": [" ", " ", "As mgruhler said to opencr stops sending cmd_vels to the motors but the raspberry pi is still up and publishing the odom. The opencr is the one responsible for sending the odom through the pi. I have seen the wheel encoders to jitter back and forth and the imu also jitters a little. The opencr uses the imu for the orientation in odom and the wheel encoder jitter would produce some linear motion in the odom. ", "The opencr does make a sound but it is only a single beep and it is easy to miss. So now that I know what to look for I just simply stop and recharge.", "mgruhler Sorry for stealing the points for answering but I didn't know how to upgrade you comment to an answer ", " all good. Happy you figured out what the issue is."], "url": "https://answers.ros.org/question/314418/local-costmap-window-slowly-drifts-over-static-global-map/"},
{"title": "Finding/making power cable to get power out of Kobuki", "time": "2019-11-23 09:22:45 -0600", "post_content": [" ", " ", "I have a variant of TurtleBot2 and I'm attempting to get power out of the Kobuki base, so that I can power a small external LCD screen like ", " Based on my very limited electronics knowledge, I believe the 12V/1.5A output from Kobuki should have sufficient power for such a small monitor (consumes 6W on 12V, therefore draws only 0.5A).", "My problem is I don't know where I can buy the power cable to connect between Kobuki and the monitor. There doesn't seem to be one available, and according to another post, I might need to make my own cable: \n", "However I'm an electronics newbie and don't know how to do it.", "Based on the answer on the post above, I understand that I need to order a \"housing connector\" (DigiKey part number WM1783-ND ", ") and need the \"crimp pins\" to put in  and some kind of \"crimp tool\" to stick them together, but I don't know how to do that, and don't know how to connect it with a standard DC male plug so that it become a power cable I can use.", "Any help is appreciated thank you!"], "answer": [], "url": "https://answers.ros.org/question/338404/findingmaking-power-cable-to-get-power-out-of-kobuki/"},
{"title": "Open Mobile Manipulator Project [closed]", "time": "2019-12-22 03:56:07 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "Not a question here ^ but i have almost finished my graduation project and i would like to share!!", "It is a cheap Mobile Manipulator educational robot, like a diy approach to a turtlebot", "It costs aproximately 600-700 it consists of\njetson nano (100), arduino due(30), 2 L298N motor drivers(10), 4 geared dc motors with encoders (120) for speed control and odometry, 1 ydlidar X4 (100), 1 kinect v1(20 used preferably something like realsense camera 200), 1 diy robotic arm 6DOF of amazon(200), 1 (I2C servo driver), 3 300w dc-dc buvk converters(25),1 lipo 5000 mah battery(50), robot base could be just 2 leveled wood base with rods and dc motor brackets.", "I hope this would be helpfull to someone if you are interesting in replicating this project email me at ", ".", "youtube video ", "\ngithub ", "Thanks!!", "This looks like a great project. I would however recommend you post this to the ", " category of ", " instead, as this does not appear to be an actual question.", "Note: I've closed this not because I don't feel you've created something nice and it's wrong to want to share it with the community, but because ROS Answers is about questions (yes, that is slightly strange, put this way), and this is not a question.", "I would really recommend you post this on ROS Discourse.", "I've also removed your email address to avoid spam.", "yes i think it will be more visible there... i have posted it\nthanks!!"], "answer": [], "question_code": ["<email removed>"], "url": "https://answers.ros.org/question/340524/open-mobile-manipulator-project/"},
{"title": "Indoor robot navigation with lasers", "time": "2019-12-21 03:18:28 -0600", "post_content": [" ", " ", " ", " ", "I have a mobile robot that moves inside an empty room with four walls and it should follow a specific path described as a series of x and y values. I was thinking to mount on the mobile robot four distance laser sensors for natural objects without reflectors (I can't use range lasers because the room is 80x50 meters and the range lasers usually cover up to 20 meters).", "Please, see the attached picture and let's say we have a robot (green square) with the four distance lasers mounted along its four cardinal points and it is moving since the room (black square) and it has to follow the blue square path with starting point is x3, y3.", "With the laser sensors it can measure its distance from the walls, in each moment, but how can I make it move accordingly with the goal coordinates?", "Is there any ROS node which I can use as starting point to develop my ROS node?\nDo you have suggestions on the algorithm to use or on other sensors/methods?", "Does the ", " topic work fine with laser distance sensors?", "Hello guys,", "We have a precise (\u00b12cm) indoor navigation system (Indoor \u201cGPS\u201d) that is designed to provide real-time location coordinates for autonomous robots, vehicles, AR and VR system.", "Demo how it works: ", ".", "Basics: You have stationary beacons (not Bluetooth or WiFi) every 10-50 meters. And you trace a mobile beacon on your robot with \u00b12cm precision. You get the coordinates either directly from the beacon on your robot via USB or UART or SPI or I2C; or you can get data from the central controller - depending on the needs of your system.\nDetailed description of the protocol is available.", "More information is on ", "\nI will be happy to provide you with more information, if needed.", "P.S.", "You can use the following coupon as \"Welcome Discount\" 3% OFF srg_a55k_3", "The most important is that the system is readily available and you ...", "Honestly speaking they look like all the other UWB products that you can find on the market and they provide only +/-50 cm accuracy in best scenarios. I could be interested but I doubt that they can reach a so accurate precision.", "You can contact me by email (check my profile) in order to better talk about your system", "Hi Marcus, Thank you for interest of our system. I do not see your email in the profile. May I ask you to send me mail to sergey(at)marvelmind.com? I will answer for all your questions. \nThank you in advance.", ": it would be interesting (and beneficial to the community) if you could post a comment here with whatever you learn about ", "'s product.", " yes, I will share everything for sure!\n", ": I sent you an email.", "By the way, i've found another company that provides a very high accuracy (+/-2cm) with radio beacons, but their system is very expensive (>10.000 USD).", " if you what I can share all info what you need + additional discount coupon for community.", "We have a lot of demo of our products, video and docs. ", "Just for example", "-\u00a0https://www.youtube.com/watch?v=YAU-WXz26YY\n-\u00a0https://www.youtube.com/watch?v=OXetXiDyAZI\n-\u00a0https://www.youtube.com/watch?v=MccIB2pUFaM", "I'm not necessarily interested in your product(s). I just wanted to make sure your comments are not just marketing for your company, but would result in a contribution to the question ", " posted.", "If you would have a proper ROS driver (adhering to the relevant REPs, etc), you could perhaps post in the ", " category on ROS Discourse. Plain advertisements are not wanted there though."], "answer": [" ", " ", "I think the intent of your question is more general than the specific questions you asked, so I'll try to answer:", "There are several different packages available within ROS to do motion planning with the Navigation Stack sort of being the \"hello world\" application for robots equiped with Lidar and wheel encoder.", "You could make your 4 laser distance sensors output data that could be built into a laser/scan message. But that message with only 4 active data points would be of limited use in a typical robot environment.", "The issue of range for laser scanners is not new.  Your proposed idea of 4 distance sensors pointing at the walls will work fine provided you can assure that the robot will ", "If 1) and 2) are met you would need to generate an odom message based on the readings of the lasers and move_base could be used to do path planning. Of course there are some caveats regarding noise and accuracy of your lasers. But I'll skip that for now. ", "In the case for 4 stationary lasers on a platform that cannot be assured to remain oriented, I can imagine an algo that determines accurate long term orientation and location provided the following are met:", " The algo I'm imagining is more deterministic than AMCL (meaning more likely to fail in noisy conditions)", "A more general setup that can use well-tested ROS apps would be to mount one or more of your 80m sensors on a rotating platform and generate a multi-angle laser/scan message. With a rotating laser(s) you could see the walls, detect orientation of the robot to the walls and track obstacles.", "Many people have put laser ranger finders on rotating platforms and posted youtube videos of the process. I have built a laser scanner based on a cheap range finder. It has better range than the laser scanners I bought but it's noisier.", "Thanks for your support. I forgot to mention that I have a very accurate IMU with heading information on board, the robot moves on mecanum wheels, I know the starting point and the room dimensions are known. Unfortunately the robot should also move in different directions not necessary parallel to the walls. I could mount the range finder on a rotating platform but I don't think it will rotate as fast as a scanner laser do. Moreover, I need to be very accurate because the robot has to reproduce and write (with a paint sprayer) specific shapes on the floor. I can't figure out a good solution to implement this.", "Wow. You have a tough challenge. It's a lot bigger than your laser scanner. You will need to develop an algo that not only accurately can move and locate the robot, but that also knows where the wheels are and remembers what it just painted so it can avoid rolling over fresh paint. Of course if you need to paint a closed loop then you may be stuck with one wheel inside the loop. ", "But back to your laser question: I'd test out the heading indoors if it is based on magnetics. I've not had luck using compass on an indoor robot. Wires, pipes, rebar in concrete can all impact compass performance.In my garage when the robot crosses over an iron sewer pipe buried 1 meter down, the compass deflects by ~15 degrees.\nIMU will drift over time. You really need a stationary reference like the ...", "Fortunately, I can let the wheels to roll over fresh paint! The imu is a mti-300 AHRS from Xsens which is very accurate but is is based on magnetic, I guess. I can use markers or reflectors but the problem is the long distance (the room is huge). Speed is not mandatory, I prefer accuracy over speed. Unfortunately, long range is necessary since the room is wide, unless we can use some kind of solution based on short range.. but I don't know if such solution exists.", "Just an observation:", "You could make your 4 laser distance sensors output data that could be built into a laser/scan message. ", "don't do this. Distance sensors are very different from laser scanners, or at least in the type of output they produce.", "For one thing, laserscan messages essentially encode sensor data in polar coordinates. 4 distance sensors do not. For distance sensors, use ", ". For multiple sensors (and if desired), use ", ", which supports encoding measurements coming from 'arbitrarily' mounted sensors that just happen to result in points in space.", "Using ", " for something not a laser scan violates semantics.", ": the image you show labels the distances as \"distance x1 from wall\" implying that x1 is also from a certain wall, but from your later comments it would appear that is not the case. The scanners could be detecting reflections from any of the four walls, correct?", "Yes each sensor measures distance from each wall, every moment. They detect reflections from wall and I can even use additional markers/detectors if needed. May be using additional markers can lower the maximum long distance required by lasers?", "If I use a scanner laser (like the LMS111 or the UTM-30LX), I place different markers all over the room and I create an accurate 2D map, the robot can move by using ", " and ", "? because I think it's hard to use a distance laser sensor with AMCL", "Yes. A laser scanner and wheel encoders is what is used is the simplest implementation of AMCL and move_base AKA The Navigation Stack.\nJust out of curiosity, why are you considering adding markers?"], "answer_details": ["be and stay oriented parallel to the walls ", "you have some way to either detect obstacles or can be assured there will not be any obstacles.", "you know the starting location and orientation of the robot", "you have accurate measurements of the room", "the lasers are suitably quiet, stable, and repeatable", "you know which way the wheels are rotating (assuming you have wheels)", " ", " ", " ", " "], "question_code": ["laser/scan", "https://www.youtube.com/channel/UC4O_kJBQrKC-NCgidS_4N7g/videos", "marvelmind.com"], "answer_code": ["but how can I make it move accordingly with the goal coordinates?\n\nIs there any ROS node which I can use as starting point to develop my ROS node? Do you have suggestions on the algorithm to use or on other sensors/methods?\n", "Does the laser/scan topic work fine with laser distance sensors?\n", "sensor_msgs/Range", "sensor_msgs/PointCloud2", "LaserScan", "AMCL", "move_base"], "url": "https://answers.ros.org/question/340495/indoor-robot-navigation-with-lasers/"},
{"title": "What are the rules for launch-prefix?", "time": "2020-01-09 04:07:09 -0600", "post_content": [" ", " ", " ", " ", "I want to see what I can do with the seemingly powerful ", " for ", ".  However, I can't quite decipher when it will accept it and when it throws "], "answer": [" ", " ", " ", " ", "The particular error you show just tells us that you have something in that attribute which is not valid XML.", "As ", " files are XML, you need to ensure that whatever you have in the ", " attribute is valid XML as well. This means: no special characters, no ", ", no ", ", etc.", "You'll have to escape/encode those characters before putting them in that attribute. I 'd suggest looking up some sites about XML and entity encoding.", "What are the rules for launch-prefix?", "Ignoring the \"valid XML\" aspect, here are some guidelines:", "there may be more, perhaps other board members can contribute."], "answer_details": ["it needs to be something that can actually be executed", "it needs to be something which does not interfere with ", "'s process monitoring (or: if it would interfere you'll just run into problems when trying to shutdown your launch session)", "it needs to be able to process the \"rest of the command line\" (ie: path and name of node binary, any additional args to the node binary)", " ", " ", " ", " "], "question_code": ["launch-prefix", "roslaunch", "RLException: Invalid roslaunch XML syntax: not well-formed (invalid token):\n"], "answer_code": ["RLException: Invalid roslaunch XML syntax: not well-formed (invalid token):\n", ".launch", "launch-prefix", "\"", "&", "roslaunch"], "url": "https://answers.ros.org/question/341305/what-are-the-rules-for-launch-prefix/"},
{"title": "turtlebot3 is not functionnig on his bettery i don't undrstend the problem", "time": "2020-01-15 12:51:32 -0600", "post_content": [" ", " ", "i build the turtlebot3 burger all the parts are connected by the manual .the respbary3 is with rasberian. i did all the setups as they written in the e-manual. when i conacet the respbary3 to a power source through the mini usb and all the components work but when i turn the turtle on by the battery the respbary3 does not turn on their is a red light on all the time and the green light flashing. in the opencr the only lights that turns on is the power and the state is flashing. i don't understand whet is the problem.    ", "I would strongly suggest you contact ROBOTIS directly, instead of posting here on ROS Answers."], "answer": [], "url": "https://answers.ros.org/question/341812/turtlebot3-is-not-functionnig-on-his-bettery-i-dont-undrstend-the-problem/"},
{"title": "Ros-CanOpen Initialization Fails", "time": "2020-01-15 12:06:27 -0600", "post_content": [" ", " ", "Hello,\nI have a SV2D10-C-CE CanOpen Controller from Applied Motion: ", "I am attempting to use it with ros-canopen. I have mapped the following PDOs, and i ensured that this is reflected correctly in the .eds file:", "RPDO1: transmission: 255 mapped:\n         0x60400010\n         0x60600008", "RPDO2: transmission: 255 mapped:\n         0x607A0020\n         0x60840020", "RPDO3: transmission: 255 mapped:\n         0x60FF0020", "TPDO1: transmission: 01 mapped:\n        0x60410010\n        0x60610008", "TPDO2: transmission: 01 mapped:\n        0x60640020", "TPDO3: transmission: 01 mapped:\n        0x606C0020", "When I called the /drivier/init service, the following error is displayed:", "I have figured out why this is occuring, to explain, please see the following output of the candump:", "As you can see, the first tpdo transmit, which is mapped to the status word, is returning the the value 0x1400, which corresponds to the \"NOT READY TO SWITCH ON\" State. When the device is in this state, the ros-canopen driver waits until the controller automatically transitions to \"SWITCH ON DISABLED\", from which the ros-canopen driver begins to write the control words necessary to transition up to \"OPERATION ENABLED\". At first i thought there is a problem with my controller firmware/hardware, that was preventing it from transitioning to \"SWITCH ON DISABLED\" on power up. However, after inspecting this with SDO requests for the status word, I realized that the controller was properly transitioning to \"SWITCH ON DISABLED\" on power up. I then realized that the ros-canopen driver was doing something that the controller did not like, and which was preventing it from re-transitioning to the \"SWITCH ON DISABLED\" state. After inspecting the above candump ..."], "answer": [], "question_code": ["[ INFO] [1579109777.314460976]: Using fixed control period: 0.010000000\n[ INFO] [1579109830.961996313]: Initializing XXX\n[ INFO] [1579109830.962750802]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1579109830.963268901]: Current state: 2 device error: system:0 internal_error: 0 (OK)\n[ERROR] [1579109836.659053103]: Transition timeout\n[ERROR] [1579109841.659389982]: Transition timeout\n[ INFO] [1579109841.663667053]: Current state: 2 device error: system:125 internal_error: 0 (OK)\n[ INFO] [1579109841.663777128]: Current state: 0 device error: system:125 internal_error: 0 (OK)\n[ INFO] [1579109841.663908567]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n[ERROR] [1579109841.663994661]: CAN not ready\n[ INFO] [1579109841.664163760]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n", "  can0  701   [1]  7F\n  can0  000   [2]  82 01\n  can0  701   [1]  00\n  can0  601   [8]  2B 17 10 00 E8 03 00 00\n  can0  581   [8]  60 17 10 00 00 00 00 00\n  can0  000   [2]  01 01\n  can0  701   [1]  05\n  can0  080   [0]                     #This block repeats (Sync+PDO transmit) until timeout occurs\n  can0  181   [3]  00 14 00\n  can0  281   [4]  00 00 00 00\n  can0  381   [4]  00 00 00 00        #End of block\n  can0  080   [0] \n  can0  181   [3]  00 14 00\n  can0  281   [4]  00 00 00 00\n  can0  381   [4]  00 00 00 00\n  can0  701   [1]  05\n  can0  601   [8]  2B 17 10 00 00 00 00 00\n  can0  581   [8]  60 17 10 00 00 00 00 00\n  can0  000   [2]  02 01\n"], "url": "https://answers.ros.org/question/341810/ros-canopen-initialization-fails/"},
{"title": "ROS Create2 ca_driver dashboard?", "time": "2020-01-20 13:41:46 -0600", "post_content": [" ", " ", "I have a Create2, using the ca_driver.", "It publishes several topics on the battery.", "when I run:", "rostopic echo /battery/charge", "It returns a number in std_msgs/Float32", "I would like to have a dashboard, like the turtlebot dashboard to show the battery states.", "Can I use the create_dashboard package for the Create2?\n", "Can I use rqt_robot_dashboard to build my own?", "Or would that be re-inventing the turtlebot dashboard?"], "answer": [], "url": "https://answers.ros.org/question/342128/ros-create2-ca_driver-dashboard/"},
{"title": "robot_localization - angular drift when fusing IMU", "time": "2020-01-13 07:23:15 -0600", "post_content": [" ", " ", " ", " ", "I've run into an interesting issue. It seems that when I'm fusing angular velocity it ends up adding a significant angular drift to odometry output.", "My robot platform is a differential_drive robot with a caster wheel. On my robot I have two IMUs: one in the front (Realsense Tracking camera) and another one in the back (Phidgets Spatial IMU).", "Here is my starting robot_localization  config:", "Here is the output for the /rs_t265/imu topic (mounted in front):\n", "and here is the output when using Phidgets (mounted in the back of the robot):\n", "The green odometry in the picture is the wheel odometry that is very close to the actual path traveled by the robot. The odometry marked in red is the output of robot_localization package.", "Almost all sources I've seen on the internet suggest fusing x and y velocity from the odometry topic and angular velocities of the IMU (magnetometer doesn't work well enough for me due to magnetic interference). The only way I can get the fused odometry appear much closer to the wheel odometry is by fusing the yaw of odom0 message, however it seems to make the filter not take the imu into account very much:", "Am I'm missing something in my setup? I think I read up all the information on robot_localization that's available on the internet and didn't come to any obvious conclusions. I'd appreciate any feedback you might have!", "EDIT: Some additional information: For each of the IMU setups the sensor frame seems to be correctly specified in the TF tree and is correctly contained in the appropriate IMU message fields.", "EDIT2: Output when using madgwick filter to produce an IMU message with orientation field and fusing the resultant yaw looks as follows:\n", "Unfortunately in 3 of the 4 corners I turn at there are huge metallic object affecting the magnetometer.", "EDIT3: Trying to fuse yaw rate from wheel odom.", "Before yaw rate fusion:\n", "After fusing yaw rate:\n", "When I said that my platform used encoders I lied a bit - actually I only have data from the motor's hall sensors. This makes the data quite noisy, below you can see a graph with wheel odom velocity on x and y and yaw rates (from wheel odom (green) and imu (purple). Would this noise contribute to my issue?", "EDIT4: Sample sensor input:", "Wheel odometry(/rr_robot/mobile_base_controller/odom):", "Do any of the IMUs give you out the orientation? I'd use that over others. You may want to try putting one of the IMUs through a complementary filter and then the output orientation into R_L.", "One of the IMUs gives out orientation, however, the environment I run the robot in contains huge metallic object near 3 of the corners I turn at. I'll edit the question to add the visualisation of this one. It's actually using Madgwick and should fuse magnetometer with other sensors.", "Are you _sure_ all the sensors are being fused? I presume you are using a bag file to get repeatable results; how are you modifying the covariance values in your bag file? Did you define static transforms for the IMU data frame_ids? And are you plotting the right output topic? The fact that _nothing_ changes the output leads me to believe one of the inputs is being ignored. What happens if you turn off the IMU input?", "So the filter is clearly trusting your IMU far more than your odom data. At this point, I would turn on debug mode (set the debug output file to an absolute path), let the robot go through a single turn, then quickly kill it. Zip that and post it here.", "Edited the question and posted a google drive link to the zipped debug file.", "BTW, you have ", ", but are fusing roll and pitch data. Those are going to get ignored."], "answer": [" ", " ", " ", " ", "So you're fusing only velocity data. Every velocity measurement contains error. When you integrate the velocity measurements into a pose estimate, you also integrate the errors. Drift will ", " be an issue when you have no absolute pose data included.", "But note that your wheel encoder odometry yaw is ", " accumulating error, before you even send it to the EKF. The robot's odometry will just be counting encoder ticks, which is also subject to error.", "In any case, the root of the issue here is that your IMU is likely noisier/more biased than your wheel odometry. Try fusing yaw velocity from your wheel odometry ", " the IMU, and see what happens.", ": yeah, your covariance values for angular velocity are preventing the filter from caring about the measurements from wheel odometry. Your wheel encoders have a ", " angular velocity variance of 0.03 (standard deviation of 0.173 rad/sec per measurement), whereas your ", " input is reporting an angular velocity variance of 1.2184696791468346e-07 (standard deviation of 0.0003491 rad/sec per measurement). That means you are telling the filter that your IMU is many orders of magnitude more trustworthy than your wheel odometry. You might as well not even be fusing wheel odometry yaw velocity in that case.", "I'd also check your process noise covariance for yaw velocity, and make sure it's not too small, lest the filter ignore measurements because it trusts its own internal model too much (and the angular velocity model is literally just projecting the current velocity to the next timestep, so it shouldn't).", ": OK, so odom fusion works by itself. My money is on sensor frequency now. What is your odometry message frequency vs. the IMU frequency? Even with equivalent covariance values, if you have 10 IMU messages for every odometry message, your odometry data will be effectively lost.", "OK, you have two problems.", "First, your IMU is ", " off. As an example, at one of your time steps, your wheel odometry is showing an angular velocity of -0.4103, but in that same moment, the IMU is reading -0.61551. If you _just_ include IMU rotational data (turn off the angular velocity for wheel odom), I'm guessing the output would be a bit uglier.", "Your process noise covariance is too high, given the noise in your sensors. What's happening is that you get an odom velocity with a tiny covariance, on the order of e-11. The filter does a predict, which adds the process noise to that quantity in its internal covariance estimate. It ends up with a covariance of something e-5. So when we do the correct step, the wheel odometry data is trusted completely, and the filter effectively jumps to the velocity in the wheel odom data. Then you get an IMU message. Again, the filter predicts across that time delta, and its internal covariance estimate moves to something ...", "Many thanks for your suggestion Tom. I've updated the question with some further screenshots. Fusing heading rate from the wheel odom didn't change anything and I'm wondering if it is due to the noise in my data (my platform currently is using hall sensors in the motor instead of encoders).", "My guess is that this a covariance issues. Please post sample input messages from every sensor input. It sounds like your IMU is under-estimating its error, or that the wheel odometry is over-estimating its error.", "Many thanks for your help! I've just edited the question and pasted the sample input of every topic I've tested my setup with. At one point I was playing with the covariance values of the wheel odometry but without too much success unfortunately.", "Many thanks for your continued support! Weirdly none of the changes I make seem to have any effect, I've updated the original questions with some more screenshots in case you would have time to take a look at it once more.", "The wheel odom frequency is around 50Hz, while the IMU is 62.5Hz (checked with rostopic hz while the bag was playing).", "Many thanks for your pointers Tom, that really helps! Could the \"IMU being way off\" the result of high noise of my wheel odom angular twist? In my edit 3 I've posted a graph of heading rate from both wheel odom and IMU and although you can clearly see the noise in one of them it seems to follow the 'trend' quite well.", "What I find odd is that I see similar behaviour (but the effects are flipped) on two different sensors (Realsense T265 and Phidgets IMU). I'll try to apply all of your recommendations and see where that gets me. ", "Do you think that getting proper wheel encoders could also help in this case?", "Looking closer at the data it looks like my wheel odom angular twist has a resolution of 0.1, which doesn't sound nowhere near usable. Will try to sanitize the data before I get on with anything else.", "The problem is that your raw wheel encoder data, as you reported, looks right: you said the robot actually traversed a rectangle (and came back to the same starting point), and when you just use wheel odometry, that's what you see. So while I agree that something is afoot, I don't know that I believe it's your encoders. Maybe try driving in a rectangle, and write a small script that integrates all the angular velocities from one IMU. When the robot returns to the start position, make it rotate to the initial heading. The summed IMU data should agree."], "question_details": [" ", " ", " ", " ", " ", " ", "I've updated the question to show the output with imu config commented out\n", "I use only joint state information from the bag file and run differential_drive controller with this data, by changing the config I can modify the covariance", "I've checked the time in all messages and it's consistent", "All static transforms are there and the tree seems OK (checked with rqt_tf_tree and also with roswtf)", "I checked rosnode info with ekf node and rqt_graph and can confirm that both imu and odom topics are fed into the ekf node. I also confirmed that I'm subscribed to the correct topic (odometry/filtered)"], "question_code": ["odom_frame: odom\nbase_link_frame: base_link\nworld_frame: odom\npublish_tf: true\nfrequency: 25\n\ntwo_d_mode: true\n\nodom0: /rr_robot/mobile_base_controller/odom\nodom0_config: [false, false, false,\n               false, false, false,\n               true, true, false,\n               false, false, false,\n               false, false, false]\nodom0_differential: false\n\n# imu0: /rs_t265/imu\nimu0: /imu/data_raw\nimu0_config: [false, false, false,\n              false, false, false,\n              false, false, false,\n              true, true, true,\n              false, false, false]\n\nimu0_differential: false\nimu0_relative: true\n", "header:\n  seq: 2443\n  stamp:\n    secs: 1579080868\n    nsecs: 941759306\n  frame_id: \"odom\"\nchild_frame_id: \"base_link\"\npose:\n  pose:\n    position:\n      x: 2.38348355885\n      y: 0.0650385644131\n      z: 0.0\n    orientation:\n      x: 0.0\n      y: 0.0\n      z: -0.502680731125\n      w: 0.864472140994\n  covariance: [0 ...", "two_d_mode"], "answer_code": ["Z", "imu/data_raw"], "url": "https://answers.ros.org/question/341611/robot_localization-angular-drift-when-fusing-imu/"},
{"title": "Global Costmap Detects Obstacle When None is there", "time": "2020-01-16 20:47:01 -0600", "post_content": [" ", " ", " ", " ", "I'm running Cartographer on ROS Kinetic on Ubuntu 16.04 using an rplidar as my sensor source.\nWhen SLAM is started, everything look normal and clean as shown in the image below.\n", "However as the robot moves throughout the environment, I can see small spots, some of which I have highlighted with a red circle , of the Global Costmap that has not been cleared out. This is quite a big issue as it will affect navigation. The path planner will try to avoid these small remnants of the global costmap and also if a navigation goal is set on top of one of these remnants, it will fail.\n", " ", "Here's my global costmap config file: ", "Any ideas on how to resolve such an issue?", "Is that from a map or a costmap? That color scheme implies its a from the costmap, not your cartographer map, unless you're using the costmap coloring for your map-map.", " That's right, it's coming from the global costmap. However it should not be there because there are no obstacles in that area.", " what sensor do you use? Given how frequently ", " is used in such a use case I doubt that it is a bug.\nPlease share your costmap configuration and, if possible, a bagfile with the sensor data where your issue appears.", " I'm using an rplidar as my data source. I've updated the issue by including the global costmap config file. A bagfile  is currently unavailable. I'm hoping you can find something in the config file. Thanks.", "Please don't add a link to a Google Drive or something. This file will go away at some point and one cannot refer to it anymore. Also, it is not searchable.\nPlease edit your question again and paste the relevant content (i.e. get rid of any unnecessary comments) and format the pasted file with the preformatted text button (the on with 1010101 on it)... Thanks", " Thanks for the suggestion. I edited the question. Appreciate the help.", "Alright, more questions (having a bagfile where the issue is shown would really help :-) )", " (Sorry a bagfile is currently unavailable.)", "The \"ghost readings \" appear anywhere in between as shown in the second image highlighted with a few red circles. I do not know why the global costmap puts that there.", "The scan messages are highlighted as green dots around the borders of the map or obstacles. From rviz I see no laser scan data at those spots.", "The lidar basically measures the distance to any obstacles it encounters. We ignore any readings within a 23 cm radius of the robot and we accept the rest as far as 8 m. Anything over 8 m will also be ignored."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Where do the \"ghost readings\" appear? At max range of the LiDAR? Or anywhere in between?", "Have you checked if there are actually readings in the scan message at those spots?", "Do those readings also occur in the local costmap or exclusively in the global one?", "What is the behavior of the LiDAR \"shooting in the air\" (I don't have an RPLiDAR at hand to test), 0, max_range, inf?"], "question_code": ["global_costmap:\n  ## Plugins ##\n  plugins:\n    - {name: \"static_layer\",    type: \"costmap_2d::StaticLayer\"}\n    - {name: \"obstacle_layer\",  type: \"costmap_2d::ObstacleLayer\"}\n    - {name: \"inflation_layer\", type: \"costmap_2d::InflationLayer\"}\n  ## Coordinate frame and tf parameters ##\n  global_frame:         \"map\"\n  robot_base_frame:     \"base_footprint\"\n  transform_tolerance:  0.5\n  ## Rate parameters ##\n  update_frequency:     1.0\n  publish_frequency:    0.5\n  ## Map management parameters ##\n  static_map:           true\n  rolling_window:       false\n  origin_x:             0.0\n  origin_y:             0.0\n  width:                10.0\n  height:               10.0\n  ## Map type parameters ##\n  map_type:             \"costmap\"\n  ### ObstacleCostmapPlugin parameters ###\n  track_unknown_space:  true\n  footprint_clearing_enabled: true\n### Obstacle Layer ###\n# ref. http://wiki.ros.org/costmap_2d/hydro/obstacles\n  obstacle_layer:\n    ## Sensor management parameters ##\n    observation_sources:  \"lidar\"\n    lidar:\n     topic:              \"/scan_filtered\" #scan\n      sensor_frame:       \"base_scan\" #lidar_frame\n      observation_persistence: 0.0\n      expected_update_rate: 0.3   # 0.15 == 6.66 Hz for RPLiDAR\n      data_type:          \"LaserScan\"\n      clearing:           true\n      marking:            true\n      max_obstacle_height: 0.4    # RPLiDAR height is 0.35\n      min_obstacle_height: 0.2\n      obstacle_range:     5.5     # RPLiDAR effective range is 5.5\n      raytrace_range:     8.0     # RPLiDAR max range is 8.0\n      inf_is_valid:       true\n    ## Global Filtering Parameters ##\n    max_obstacle_height:  1.0     # Orbbec-Astra height is 1.3\n    obstacle_range:       5.5     # maximum range to insert obstacles into the costmap using sensor data\n    raytrace_range:       8.0     # maximum range to raytrace out obstacles from the map using sensor data\n    ## Misc ##\n    enabled:              true\n    combination_method:   1       # 1 - maximun\n### Static Layer ###\n# ref. http://wiki.ros.org/costmap_2d/hydro/staticmap\n  static_layer:\n    unknown_cost_value:         255\n    lethal_cost_threshold:      100\n    map_topic:                  \"map\"\n    first_map_only:             false\n    subscribe_to_updates:       false\n    track_unknown_space:        true\n    use_maximum:                false\n    trinary_costmap:            false\n    ## Misc ##\n    enabled:                    true\n", "costmap_2d"], "url": "https://answers.ros.org/question/341906/global-costmap-detects-obstacle-when-none-is-there/"},
{"title": "robot_localization:The /odometry/filtered frequency is not stable", "time": "2020-01-10 04:21:48 -0600", "post_content": [" ", " ", "I'm going to fusing the data of visual odom and imu.\nQuestion 1\nBy calculating the difference between the timestamps of two adjacent frames, it was found that the fused data frequency was unstable, but the result of rostopic hz /odometry/filtered was stable.Why is that? How do you get a stable output frequency?", "Question 2\nI plotted the /odometry/filtered data and the visual odom data in Matlab.As shown below.\n", "\nThe x-coordinate is the timestamp, and the y-coordinate is the quaternion x.\nThe blue dot is visual odom data, the green dot is /odometry/filtered data.\nThe /odometry/filtered data is more unevenly distributed than the visual odom data.\nMy config is shown below.\n      ", " ", " ", "What does \u201cunstable\u201d mean? Can you provide numbers or examples. Also what\u2019s the last commit hash you\u2019re using?", "\nSorry for the late reply. The following values are the frequencies obtained by calculating the difference between the timestamps of two adjacent frames. I set the frequency to 50, but they don't look like \"stable\" 50hz from the timestamp. And, some adjacent timestamps are the same, so the frequency obtained is inf.\nThe last commit hash is 2d2976a737bbea00f638d9519783aaa9645a2c64. \nAuthor: Tom Moore ", "\nDate:   Mon Aug 26 10:06:08 2019 +0100", "49.7020227754802\n66.3571699784837\n99.4028676383458\n49.7757523972278\n33.1675655158234\n39.7930229689857\n49.7267715508554\n66.3571699784837\nInf\n28.4251673940741\n33.1675655158234\n49.7261820078722\n49.7515449854694\n49.7757523972278\nInf\n28.4172713537538\n39.8088856407969\n49.7261820078722\n66.3571699784837\n39.7930229689857\n199.207029209214"], "answer": [" ", " ", " ", " ", "This can be indicative of having old measurement data. If you get two identical time stamps in a row, it's because the filter published its state, and then received an old measurement, backed up in its history, inserted the measurement, and then fused the subsequent measurements, and re-published the last state. Try eliminating one input to see if that fixes the timing. If it does, then make sure your measurements are correctly stamped. You can also turn off the history buffer.", " in response to comments.", "First, it's probably a lot easier to post links to code, rather than copying and pasting it here. It also lets me know what code branch you're using. The code in question isn't from ", ", so I'll assume you're using ", ".", "The longer answer requires us to walk through one cycle for the filter. This refers to the code branch you are using, rather than the latest stuff, which works on a timer (but would, I think, have the same behaviour).", "Hopefully you can see the problem here. In step 2, if there are three measurements in the first cycle and eight measurements in the second cycle, then there will be an uneven amount of time between thw two publications. We can't know how many measurements we'll have, so we can't know how long it will take to fuse them all.", "There are (at least) two alternatives, though I'm open to hearing more:", "Thanks for you reply. After that, I changed the imu0_queue_size parameter  to make it larger. The problem of two identical time stamps has been solved. The reason I think is that my IMU frequency is too high. Now I find a piece of code in ros_filter.cpp that forces the computation time of each frame to be a fixed 1/ frequency.The relevant code is shown below.\nros::Duration loop_elapsed_time = ros::Time::now() - loop_end_time;\nif (loop_elapsed_time > loop_cycle_time)\n{\n    ROS_WARN_STREAM_DELAYED_THROTTLE(1.0, \"Failed to meet update rate! Took \" << std::setprecision(20) <<\n    loop_elapsed_time.toSec() << \" seconds. Try decreasing the rate, limiting sensor output frequency, or \"\n    \"limiting the number of sensors.\");\n}\nelse\n{\n   ros::Duration sleep_time = loop_cycle_time - loop_elapsed_time;\n   sleep_time.sleep();\n}\nloop_end_time = ros::Time::now();", "I think that's why the frequency of calculating timestamps is not stable, but the \"rostopic hz\" is stable.\nPlease point out if I'm wrong."], "answer_details": ["At time ", ", we call spinOnce(), which fires all the callbacks for the sensor messages we received. We throw them all into a measurement queue.", "We walk through that queue, fusing each measurement in temporal order.", "After fusing the measurements, we are now at time ", ", because fusing the measurements obviously takes time. ", "The elapsed time is ", ". If the configured frequency of the filter is ", ", our cycle time is ", ". But we ate up some of our cycle time fusing measurements, so we only need to sleep for ", " seconds. I should note that the ", " object does all this logic for you, but I wasn't aware of that when I wrote these, I think. But I digress...", "Go back to step 1.", "Do all the fusing, then wait until the end of the cycle, and publish the last computed state. This is a bad idea, because you'd be publishing old information from time ", " at some later time, ", ".", "Sleep until the end of the cycle, and then predict the state using the filter's prediction stage to get the state at time ", ", and publish that instead. This is also not great, for two reasons. First, the filter will have to go back in its history in the ", " cycle, because while you were sleeping (before the predict/publish), more measurements arrived in the queue. So now you have to rewind the ...", " ", " ", " ", " "], "question_code": ["  <param name=\"map_frame\" value=\"map\"/>\n  <param name=\"odom_frame\" value=\"odom\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom\"/>\n\n  <param name=\"transform_time_offset\" value=\"0.0\"/>\n  <param name=\"transform_timeout\" value=\"0.0\"/>\n\n  <param name=\"odom0\" value=\"/loop_fusion/odometry_rect\"/>\n  <param name=\"imu0\" value=\"/mynteye/imu/data_raw\"/>\n\n  <rosparam param=\"odom0_config\">[true, true, true,\n                                  true, true, true,\n                                  false,  false,  false,\n                                  false, false, false,\n                                  false, false, false]</rosparam>\n\n  <rosparam param=\"imu0_config\">[false, false, false,\n                                 false,  false,  false,\n                                 false, false, false,\n                                 true,  true,  false,\n                                 false,  false,  false]</rosparam>\n\n  <param name=\"odom0_differential\" value=\"false\"/>\n  <param name=\"imu0_differential\" value=\"false\"/>\n\n  <param name=\"odom0_relative\" value=\"false\"/>\n  <param name=\"imu0_relative\" value=\"false\"/>\n\n  <param name=\"imu0_remove_gravitational_acceleration\" value=\"true\"/>\n\n  <param name=\"print_diagnostics\" value=\"true\"/>\n\n  <param name=\"odom0_queue_size\" value=\"5\"/>\n  <param name=\"imu0_queue_size\" value=\"20\"/>\n\n  <param name=\"debug\"           value=\"false\"/>\n  <param name=\"debug_out_file\"  value=\"debug_ekf_localization.txt\"/>\n\n  <rosparam param=\"process_noise_covariance\">[0.03, 0,    0,   0,    0,    0,    0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0.03, 0,   0,    0,    0,    0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0.04, 0,    0,    0,    0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0.03, 0,    0,    0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0.03, 0,    0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0.06, 0,     0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0.025, 0,     0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0.025, 0,    0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0,     0.05, 0,     0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0,     0,    0.002, 0,     0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0,     0,    0,     0.002, 0,     0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0,     0,    0,     0,     0.004, 0,    0,    0,\n                                              0,    0,    0,   0,    0,    0,    0,     0,     0,    0,     0,     0,     0.01, 0,    0,\n                                              0,    0,    0,   0,    0,    0 ...", "Update issue templates\n"], "answer_code": ["melodic-level", "kinetic-devel", "t0", "t1", "e = t1 - t0", "N", "1/N", "1/N - e", "ros::Rate", "t1", "t2", "t2"], "url": "https://answers.ros.org/question/341425/robot_localizationthe-odometryfiltered-frequency-is-not-stable/"},
{"title": "turtlebot3_teleop turtlebot3_teleop_key.launch doesn't move", "time": "2020-01-28 12:39:55 -0600", "post_content": [" ", " ", "Hi, \nI am following the turtlebot3 tutorial. ", "In remote-PC, run", "In turtlebot terminal, run", "then, back to remote-PC, run", "can see the rviz GUI and the scan points.", "then run,", "can see the topics like odom, scan, battery, cm_vel_rc100, imu .......\nthen run,", "no error, see the wasd instructions. typing w, can see the acc refreshing with new speed.", "but the robot doesn't move.\nIn topic \"cmd_vel\" is updated with new speed,\nbut in topic \"cmd_vel_rc100 \" the speed is still 0.", "thank you"], "answer": [" ", " ", "if you type ", " in the terminal, do you see anything subscribed to it? same for ", " is anything publishing to it?", "Thank you, Joe.\nI run the demo again, it worked.\nCheck the topic ", " and ", ".\nthe ", " node is publisher, and the ", " is subscriber under the topic ", ".\nin the topic ", " only one publisher --- ", ", no subscriber.", "the topic of ", " can be found with ", " command.", "Thank you again, I will check with these commands next time."], "answer_code": ["rostopic info cmd_vel", "rostopic info cmd_vel_rc100"], "url": "https://answers.ros.org/question/342747/turtlebot3_teleop-turtlebot3_teleop_keylaunch-doesnt-move/"},
{"title": "Running ROS + Gazebo on Raspberry Pi", "time": "2020-01-29 10:42:55 -0600", "post_content": [" ", " ", "I know that in principle it should work. Run linux on the Pi, connect a screen, keyboard and mouse, install ROS and Gazebo and it should work.", "But I have not tried it myself. Specifically running ros and Gazebo at the same time to make the Raspi a viable albeit low power development platform for ROS.", "Have you tried it, and if so could you share what you used?", "What was your experience?", "I'm pretty sure you can run it because they do have binaries within Ubuntu repositories for arm. I'm not sure how the performance will be though. Maybe a raspberry pi 4 would be a good option instead of a RPI3."], "answer": [], "question_details": [" ", " ", "model Raspi", "OS installed", "ROS version"], "url": "https://answers.ros.org/question/342846/running-ros-gazebo-on-raspberry-pi/"},
{"title": "How to convert Twist msg to PWM signals?", "time": "2019-02-15 00:10:58 -0600", "post_content": [" ", " ", "I am designing a actuator module for my RC Car.", "My joystick output a geometry_msgs/twist <> from which I extract the linear velocity and angular velocity. \nBut I have to send 2 PWM signals (Power and Turn) to the dc motor and servo motor and I am unable to establish the relationship.", "Power has a direct relationship with linear velocity. So that relation ship is easy.\nBut for the angular velocity, how do I decide whether to change power or turn ??", "what kind of car are you using ?\nDiff_drive like a tank \nor like a normal car (ackerman_steering) ?", " Dig here:  ", "I am using ackerman_steering"], "answer": [" ", " ", "This will convert the Twist to Ackermann messages:\n", "This will convert angular velocity to steering angle. ", "\nOnce you have the steering angle you can send the servo PWM value that corresponds to that angle.  Or put a control loop around your steering angle sensor."], "answer_code": ["def convert_trans_rot_vel_to_steering_angle(v, omega, wheelbase):\n  if omega == 0 or v == 0:\n    return 0\n\n  radius = v / omega\n  return math.atan(wheelbase / radius)\n\nv = data.linear.x\nsteering_angle = convert_trans_rot_vel_to_steering_angle(v, data.angular.z, wheelbase)\n"], "url": "https://answers.ros.org/question/315702/how-to-convert-twist-msg-to-pwm-signals/"},
{"title": "Color Indication for Robot in Stage ROS", "time": "2020-02-11 10:11:01 -0600", "post_content": [" ", " ", "Hi everyone,\nI have simulated a roomba in stage simulator. I want to give some kind of color indication while running the robot - green when it is moving, red when it has stopped. One example of doing this can be changing the color of robot itself, other may be putting LEDs on it. Is there a way to implement this programmatically in Stage? ", "Even though it's possible to use Stage with ROS, I doubt there are many users of it that frequent this forum.", "Not to discourage you. Just some expectation management."], "answer": [], "url": "https://answers.ros.org/question/343840/color-indication-for-robot-in-stage-ros/"},
{"title": "Converting Kinect RGB image to OpenCV gives wrong colours", "time": "2011-03-05 02:18:10 -0600", "post_content": [" ", " ", "Hey everyone", "I have been playing a bit with the kinect camera using the openni driver. When running openni_node.launch and viewing the rgb image using rviz, then everything looks perfect, but when I create a node that has to the purpose to read the rgb image, convert it to a cv image and displaying it using imshow, then the colours are all wrong. Red is blue, blue is red, green is something yellow and yellow is light blue. I have been trying to use RGB8 and BGR8 encodings, but nether of them shows any difference.\nA sample of my code is shown below, inspired from the example in the ", ":", "Does anyone know what might cause this problem? I am using the newest version of Diamondback. ", "Regards", "Sebastian Aslund"], "answer": [" ", " ", "Hi Sebastian,", "Here's a similar piece bit of Python code that works for me with my Kinect.  Like you, I am using the latest version of Diamondback and the latest Ni stack.  The steps I am using are:", "$ roslaunch openni_camera openni_node.launch", "$ rosrun pi_head_tracking_tutorial ros_to_cv.py", "where of course the second line will be different for you.  Here is the ros_to_cv.py code.  If you run it as a node, you'll also have to change the manifiest name in the roslib.load_manifiest('pi_head_tracking_tutorial') line.", " ", " ", "Hi Sebastian,", "I had the same issue and found out that using toCvShare instead of toCvCopy resulted in an image with good colors. Maybe you can use that until this issue is fixed.", " ", " ", "I managed to make it work and it returns the image nicely, posted a mail on the mailing list pointing out that there is apparently an error in the cv_bridge for diamondback, as using the tutorial for cturtle also returned a nice image. ", " ", " ", "I cant see any difference between your code and my own, besides the language used, so I have tried running your code but I get the following error: ", "rosCv was a quick node name for your code, I am not used to python in any way, so I do not know if I missed something. \nI added ", " to my manifest file. I added ros_to_cv.py to my src folder, copied your code, changed the second line to ", " and in my cmake list wrote ", "If someone else have an idea to what might be wrong, then do not hesitate to write :)", "Regards", "Sebastian"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "[rosbuild] Building package\nros_kinect_template   ", "[rosbuild]\nCached build flags older than\nmanifests; calling rospack to get\nflags   ", "[rosbuild] Including\n/opt/ros/diamondback/stacks/ros_comm/clients/rospy/cmake/rospy.cmake   ", "[rosbuild] Including\n/opt/ros/diamondback/stacks/ros_comm/clients/roslisp/cmake/roslisp.cmake", "[rosbuild] Including\n/opt/ros/diamondback/stacks/ros_comm/clients/cpp/roscpp/cmake/roscpp.cmake", "-- Configuring done   CMake Error: CMake can not determine linker\nlanguage for target:rosCv   ", "CMake\nError: Cannot determine link language\nfor target \"rosCv\".   ", " ", " ", " ", " "], "question_code": ["void processRGB::processImage( const sensor_msgs::Image::ConstPtr& img ) {\n    CvImagePtr cv_ptr;\n\n    try {\n\n        cv_ptr = toCvCopy(img, enc::BGR8); //enc::RGB8 also used\n\n    } catch (cv_bridge::Exception& e) {\n\n        ROS_ERROR(\"cv_bridge exception: %s\", e.what());\n        return;\n    }\n\n    imshow(\"Kinect RGB image\", cv_ptr->image);\n    waitKey(3);\n}\n"], "answer_code": ["#!/usr/bin/env python\n\nimport roslib\nroslib.load_manifest('pi_head_tracking_tutorial')\nimport sys\nimport rospy\nimport cv\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass vision_node:\n    def __init__(self):\n        rospy.init_node('vision_node')\n\n        self.cv_window_name = \"OpenCV Image\"\n\n        cv.NamedWindow(self.cv_window_name, 1)\n        self.bridge = CvBridge()\n        self.image_sub = rospy.Subscriber(\"/camera/rgb/image_color\", Image,\\\n        self.callback)\n\n    def callback(self, data):\n        try:\n          cv_image = self.bridge.imgmsg_to_cv(data, \"bgr8\")\n        except CvBridgeError, e:\n          print e\n\n        cv.ShowImage(self.cv_window_name, cv_image)\n        cv.WaitKey(3)\n\ndef main(args):\n      vn = vision_node()\n      try:\n        rospy.spin()\n      except KeyboardInterrupt:\n        print \"Shutting down vison node.\"\n        cv.DestroyAllWindows()\n\nif __name__ == '__main__':\n    main(sys.argv)\n"], "url": "https://answers.ros.org/question/9290/converting-kinect-rgb-image-to-opencv-gives-wrong-colours/"},
{"title": "Velocity Cartesian Controller strange behavior", "time": "2020-02-17 04:27:25 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am using a cartesian velocity controller for a robotic manipulator (UR3). The main functionality of the controller is that it accepts linear and angular velocities for the end effector. Then using IK it computes the joint velocities and updates the joint positions based on the joint velocities and the current joint positions. In the next example, the goal is for the end effector to track the green trajectory as shown in the image below. The orientation of the end effector remains constant while the linear velocities are computed based on waypoints along the trajectory. The problem is that the robot exhibits a strange behavior as shown in this ", ". By running several tests I concluded that the manipulator has the tendency to lean towards its right when given velocities in a specific axis (y axis in this case), as shown in the end of the gif. Below the main functionality of the controller is shown, which is based on the Kinematics and Dynamics Library (KDL). ", "Does anybody know the potential reason behind this strange behavior? Is it due to incorrect gravity modelling or could there be some other issue? The system is build in ROS Melodic, Ubuntu 18.04 and Gazebo 9.0. Thanks in advance and please let me know if there is any other information I could provide.", "Please attach your image and gif directly to the question.", "Also: have you resolved ", "?", "Finally:", "this looks like it is integrating velocity to get a position and then commanding that. That would seem to contradict:", "The main functionality of the controller is that it accepts linear and angular velocities for the end effector and it produces joint velocities using IK which are passed to ros controllers.", "I attached the image but couldn't attach the gif. As for your second question, no unfortunately I have not resolved it yet. ", "Indeed the description of the functionality of the controller was incorrect, so I modified it as well. Thanks for your comments."], "answer": [], "question_code": ["for(std::size_t i=0; i < this->joint_handles_.size(); i++){\n    this->joint_msr.q(i) = this->joint_handles_[i].getPosition();\n    this->joint_msr.qdot(i) = this->joint_handles_[i].getVelocity();\n}\n\nik_vel_solver->CartToJnt(this->joint_msr, x_vel_des, q_vel_cmd);\n\nfor(std::size_t i=0; i < this->joint_handles_.size(); i++) {\n    this->joint_handles_[i].setCommand(this->joint_msr.q(i) + q_vel_cmd_(i)*time.toSec());\n}\n\nfk_vel_solver_->JntToCart(this->joint_msr, x_vel);\nfk_pos_solver_->JntToCart(this->joint_msr.q, x);\n", "this->joint_handles_[i].setCommand(this->joint_msr.q(i) + q_vel_cmd_(i)*time.toSec());\n"], "url": "https://answers.ros.org/question/344324/velocity-cartesian-controller-strange-behavior/"},
{"title": "DWA planner not considerate robot footprint", "time": "2020-02-10 06:49:25 -0600", "post_content": [" ", " ", "Hello everybody,", "we want to use cartographer together with move base to do some frontier based semantic exploration. We use Ubuntu 18.04 and ros melodic. Cartographer is set up nicely and runs without any issues. As sensor input for move base we use two SICKS300 and odometry information calculated from the robot encoders. We have set up all the  necessary tfs. With our setup, we are able to get plausible global and local costmaps. We choose the navfn as global and dwa as the local planner. As long as our robot does not have to avoid any obstacles, everything runs smoothly.", "But if there are obstacles the dwa planner does not seem to take into account the footprint of the robot (see rviz screenshot). ", " ", "The blue line is the global path (navfn), the green one is the global dwa path and the red one is the dwa local path. We are confused and do not understand why the robot's footprint touches the inflation layer in such a way. ", "Here are our configuration files: "], "answer": [], "question_code": ["obstacle_range: 5.0\nraytrace_range: 5.0\nmap_topic: \"/ctv_10/map\"\nfirst_map_only: false\nfootprint: [[-1.03,0.46], [0.38,0.46], [0.38,-0.46], [-1.03,-0.46]]\n\nplugins: \n  - {name: static_map,       type: \"costmap_2d::StaticLayer\"} \n  - {name: inflation,        type: \"costmap_2d::InflationLayer\"}\n  - {name: obstacles,        type: \"costmap_2d::VoxelLayer\"}\n\nstatic_map:\n  lethal_cost_threshold: 55\n  subscribe_to_updates: true\n\ninflation:\n  inflation_radius: 0.55\n  cost_scaling_factor: 10.0\n\nobstacles:\n  observation_sources: scanFront scanBack \n  scanFront: {  data_type: LaserScan, \n                sensor_frame: ctv_10/laser_front_link, \n                clearing: true, \n                marking: true, \n                topic: /ctv_10/front_scan, \n                expected_update_rate: 10}\n\n  scanBack: {   data_type: LaserScan, \n                sensor_frame: ctv_10/laser_back_link, \n                clearing: true, \n                marking: true, \n                topic: /ctv_10/back_scan, \n                expected_update_rate: 10}\n", "global_frame: \"ctv_10/map\"\nrobot_base_frame: \"ctv_10/base_link\"\ntransform_tolerance: 20.0\nupdate_frequency: 5.0 \npublish_frequency: 5.0\nrolling_window: false \nalways_send_full_costmap: false\nstatic_map: true\nmap_type: costmap\ncost_scaling_factor: 10.0\n", "global_frame: \"ctv_10/map\"\nrobot_base_frame: \"ctv_10/base_link\"\ntransform_tolerance: 10.0\nupdate_frequency: 5.0\npublish_frequency: 2.0\nrolling_window: true\nalways_send_full_costmap: true\nstatic_map: false\nmap_type: costmap\nwidth: 5.0\nheight: 5.0\nresolution: 0.05\n", "# robot configuration parameters\nmax_vel_trans: 0.5\nmin_vel_trans: 0.1\nmax_vel_x: 0.5 \nmin_vel_x: 0.1   \nmax_vel_y: 0.1 \nmin_vel_y: -0.1 \nmax_vel_theta: 1.0\nmin_vel_theta: 0.4\nacc_lim_x: 0.8\nacc_lim_y: 0.8\nacc_lim_theta: 1.0\nacc_lim_trans: 0.1 \n\n# goal tolerance parameters\nxy_goal_tolerance: 0.1\nyaw_goal_tolerance: 0.07\nlatch_xy_goal_tolerance: false\n\n# forward simulation parameters\nsim_time: 4.0                        \nsim_granularity: 0.025          \nvx_samples: 3                        \nvy_samples: 3                         \nvth_samples: 20                       \n#controller_frequency: 15.0\n\n# trajectory scoring parameters\npath_distance_bias: 32.0\ngoal_distance_bias: 24.0              \noccdist_scale: 0.01                  \nforward_point_distance: 0.325\nstop_time_buffer: 0.2\nscaling_speed: 0.25\nmax_scaling_factor: 0.2\npublish_cost_grid: false\n\n# oscillation prevention parameters\noscillation_reset_dist: 0.05\n\n# global plan parameters\nprune_plan: false\n"], "url": "https://answers.ros.org/question/343713/dwa-planner-not-considerate-robot-footprint/"},
{"title": "How to plot the position of ethzasl_ptam in rqt_plot ?", "time": "2019-09-23 13:49:45 -0600", "post_content": [" ", " ", " ", " ", "I have a ROS node (ethz-ptam) which publishes a topic ", " which is of type ", " message. Running:", "gives the following:", "I'd like to use rqt_plot to plot a field (say Point position ", "). But I tried everything to \"call\" it in rqt_plot, but it doesn't work, the rqt_plot window shows like it should but the field is not plotted. From ", " I know that it is being published, so everything is working except rqt_plot. Also if I write inside the text-input-field in rqt_plot, like they say in the docs (", "): ", " or ", " nothing shows. According to the tutorials, in rqt_plot you should be able to hit the ", "Button next to the text-input-field, but this one is never active in my case, so it can't be pressed.", "I am pretty sure that I am making some kind of mistake in referencing the message field  (like ", "). But since I tried all variants I could think of,  I'd really apprechiate some advice on how to do this and call/reference this field so that the position fields are displayed in rqt_plot.", "Thanks!"], "answer": [" ", " ", "What you have seen in the docs is wrong, can you link your doc ? In the official documentation of the rqt_plot package there is this ", " specifying the proper syntax.", "The link says :", "The input value should be the full path to the value", "So in ", " you need to type ", "...", "If you want to plot the x coordinate from your topic you need to type :", "NB : There are 3 ", " because the output of ", " seems to indicate that your topic ", " publish ", " messages instead of ", " as you said. That means the first ", " is from the topic name, the second one is the field of type ", " and the last one for the ", " field.", "I would link the post I found regarding the syntax, but somehow now I can't find it anymore. Really stupid since I am sure about what I saw there and posted in my question. But someone must have cleared the browser history of this lab computer. Sorry.", "No problem, even in the official documentation it can be misunderstanding as it's not the same syntax when using ", " in command line or with ", ".", " ", " ", "Hi,", "I think you might want to try ", ",  that is similar to rqt_plot but more powerful."], "question_code": ["/vslam/pose", "geometry_msgs/PoseWithCovariance", "rostopic type /vslam/pose | rosmsg show\n", "std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\ngeometry_msgs/PoseWithCovariance pose\n  geometry_msgs/Pose pose\n    geometry_msgs/Point position\n      float64 x\n      float64 y\n      float64 z\n    geometry_msgs/Quaternion orientation\n      float64 x\n      float64 y\n      float64 z\n      float64 w\n  float64[36] covariance\n", "x", "rostopic echo /vslam/pose", "topic-name/field:field:field", "/vslam/pose/x", "/vslam/pose/x:y:z", "+", "x"], "answer_code": ["rqt_plot", "/topic_name/field/field", "/vslam/pose/pose/pose/position/x\n", "pose", "rosmsg show", "/vslam/pose", "geometry_msgs/PoseWithCovarianceStamped", "geometry_msgs/PoseWithCovariance", "pose", "geometry_msgs/PoseWithCovariance", "geometry_msgs/Pose", "rqt_plot", "GUI"], "url": "https://answers.ros.org/question/333733/how-to-plot-the-position-of-ethzasl_ptam-in-rqt_plot/"},
{"title": "which camera is better to use with ROS: kinect or astra ?", "time": "2017-09-05 17:07:32 -0600", "post_content": [" ", " ", " ", " ", "Hello every one ", "I am working on a project that needs a camera with a depth sensor and I am wondering which one of these tow cameras will be better for me to use the xbox 360 kinetic or the astra pro ", "and which one is more supported in ROS or there is no difference ?"], "answer": [" ", " ", " ", " ", "They are both fully supported with drivers readily available. For the Orbbec Astra drivers, see ", ". For the Kinect 360 see ", ". The better camera is simply the one with the better performance characteristics that suit your application. This includes the depth image size, depth range, field of view etc. I'm not sure what an Astra costs, but 360 Kinects can be picked up very cheap second hand.", " ", " ", " ", " ", "Orbbec Astra has 3 types and their ranges are better than Kinect. Kinect needs extra power but Orbbec Astra does not. Kinects are not aimed for Robotics. On the other hand, you may find cheap second hand Kinect sensors."], "url": "https://answers.ros.org/question/270274/which-camera-is-better-to-use-with-ros-kinect-or-astra/"},
{"title": "sending velocity commands to the robot throught cmd_vel topic", "time": "2019-08-25 06:45:28 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI want to send velocity comands to move th bebop drone up and then forward. Here is my code:", "I can see that the velocities are being published through the rqt_graph here:\n", "but the drone is not moving at all! it is not even rotating the helices.\nAny suggestion!!!\nThanks", "The code you show does not appear to initialise the ", " at all. Is that a copy-pasta or actual code?", " I forgot that line that I added now. this is my code"], "answer": [" ", " ", " ", " ", "Hello,", "have you tried to post directly a speed in order to see if the robot accepts the command?", "Please watch out the drone, I do not know which speeds are proper for you.", "See this git. Sunk has made a wonderful job. I have used his solution, with some modifications at the robot I've work now.", "Good luck!", " , thanks for your answer, actually it works that way, but how can I manage with the node that publish the commanded velocity, is there anything wrong with the sleep rate or the spining!!!\nPlease help", " thanks again. Actually I followed the same structure of the code in the link\nbut the problem is solved by just send an empty message to /bebop/takeoff topic before running the node.\nI have another question!!! where to get the battery state so I can send an error message when the battery is low!!!\nPlease help"], "question_code": ["#include<ros/ros.h>\n#include<nav_msgs/Odometry.h>\n#include<geometry_msgs/Twist.h>\n#include<stdlib.h>\n#include<math.h>\n\nfloat tag_yaw,tag_x,tag_y,tag_z;\nros::Publisher pub;\nros::Subscriber sub;\ngeometry_msgs::Twist msg;\nfloat err;\n\n\n void getpos(const nav_msgs::Odometry& posmsg){\n\n if (posmsg.pose.pose.position.z<= 1){\n\n ROS_INFO_STREAM( \"Showing position : \"\n << \"x1 =\" << posmsg.pose.pose.position.x\n << \"y1 =\" << posmsg.pose.pose.position.y\n  << \"z1 =\" << posmsg.pose.pose.position.z);\n\n   msg.linear.x=0.0;\n   msg.linear.y=0.0;\n   msg.linear.z=  0.1;   // exp ( 0.1);\n   msg.angular.x= 0;\n   msg.angular.y= 0;\n   msg.angular.z=0.0;\n    }\n    else {\n\n    ROS_INFO_STREAM( \"Showing position : \"\n    << \"x =\" << posmsg.pose.pose.position.x\n    << \"y =\" << posmsg.pose.pose.position.y\n    << \"z =\" << posmsg.pose.pose.position.z);\n\n     msg.linear.x=0.8*(tag_x-posmsg.pose.pose.position.x);\n     msg.linear.y=0.0;\n     msg.linear.z=  0.0;   // exp ( 0.1);\n     msg.angular.x= 0;\n     msg.angular.y= 0;\n     msg.angular.z=0.0;\n     }\n\n      pub.publish(msg);\n     }\n\n\n   int main(int argc, char **argv){\n   ros::init(argc, argv,\"velBebop\");\n   ros::NodeHandle nh;\n\n  tag_x= .5;\n  tag_y= 0.0;\n   tag_z= 0.0;\n   sub = nh.subscribe(\"/bebop/odom\",100, &getpos) ;\n    pub = nh.advertise<geometry_msgs::Twist>(\"/bebop/cmd_vel\" ,100);\n\n    ros::spin();\n    return 0;\n     }\n", "Publisher"], "answer_code": ["rostopic pub /cmd_vel geometry_msgs/Twist -r 10 -- '[0.2, 0.0, 0.0]' '[0.0, 0.0, 0.0]'\n"], "url": "https://answers.ros.org/question/331522/sending-velocity-commands-to-the-robot-throught-cmd_vel-topic/"},
{"title": "Error with installing additional package", "time": "2019-03-30 16:46:27 -0600", "post_content": [" ", " ", "Hi all )))", "Sorry, if my question have been repeated but I haven't found the solution(\nI was trying to install gmapping package but the error was.", "I wrote this", "and I got this", "Why did the package not be installed?", "Thank you very much))))"], "answer": [" ", " ", " ", " ", " has not been released into Melodic, so that's why you can't install it using ", ".", "See ", ", ", " and ", ". Notice how the wiki page has no version button for ", ". The ", " for Melodic also doesn't show any ", " packages (meaning it hasn't been released).", "As far as I understand there are also no plans to release it for Melodic. I can't find a reference, but I believe the idea is that other packages (such as Google's Cartographer) are much more powerful and feature complete, or at least much better maintained.", "If you really want/need ", " on Melodic, you could attempt a from-source build.", "You may refer to ", " for a workflow that resulted in a successful build at the time the answer was written."], "question_code": ["sudo apt-get install ros-melodic-gmapping\n", "Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nE: Unable to locate package ros-melodic-gmappin\n"], "answer_code": ["gmapping", "apt-get", "melodic", "gmapping", "gmapping"], "url": "https://answers.ros.org/question/319971/error-with-installing-additional-package/"},
{"title": "Command line argument passing in rosrun", "time": "2018-07-30 06:20:28 -0600", "post_content": [" ", " ", "Hi All,", "I am trying to pass the command line arguments while running ", "So that a particular block of code could be executed based on the parameter.", "Below is the approach.", "Also when I try to print the value of param it shows nothing.", "Can anybody help me to fix this please.", "Thank you,\nKK"], "answer": [" ", " ", " ", " ", "I was able to get the expected result by updating the code as given below.", "command to run ", " ", " ", "Just to complement, I think your original code is a bit confusing. You are expecting to have a parameter with the type \"string\", instead of expecting a specific value for a given parameter.", ". For example, if you run with ", " or ", ", you have ", " return because it expects to fill a string variable, not a boolean or an integer. But if you run ", ", you have ", " return, because the type matches.", "Just to complement, you can remove the parameter ", " or you can use it and remove ", ", it gets cleaner. I tried in my local environment and it worked like that:", " Thanks for your response. Could you please let me know in this case how you would be mentioning the parameter while executing the rosrun ?", "For example:", " Yes, that's exactly how I am running it.You can check in this video I recorded  ", " Thanks man. Gone through the video, it was really helpful. \nIn your case instead of using a separate variable name, you have used the same \"param\", correct ?", "Exactly! As far as I understood, you don't need two parameters, but a single one is enough to choose a behavior in your algorithm (blue, green, yellow, etc.)\nFurthermore, you can change the type of param you want to use. If you have only 2 options, you can use boolean. For more options, integer..", " I had more than 5 cases, so I used if else to compare strings. I have posted the approach I used as answer in this,", " ", " ", "it only works when you write this ros::NodeHandle nh(\"~\"); ", " ", " ", "According to rosrun ", " you can do following:", "Concrete in your case to run \"blue\" code should be:", " Thanks for the swift response. I will try and update you.", " .. Unfortunately the given method did not work.", "In your code snippet above you're printing out the value of Param before it is being set, so it will always be empty. If you move the ROS_INFO statement inside the first if then it should show the parameter value.", " . Thanks for the response. I was able to fix the issue and got the expected results. Will be posting the answers.", " ", " ", "Add _ (underscore) before the parameter name to make it identify and remap as a variable inside the node. Adding nothing will help the ROS to identify it as a topic and remap the same."], "question_code": ["  rosrun <package> <node> <parameter>\n", "     int main(int argc, char *argv[])\n     {  \n            std::string param;\n            ros::init(argc, argv, \"node_name\");\n            ros::NodeHandle nh(\"~\");\n            ROS_INFO(\"Got parameter : %s\", param.c_str());\n\n            if(nh.getParam(\"blue\", param))\n            {\n                 blue();    //Run program blue\n            }\n            else if(nh.getParam(\"green\", param))\n            {\n                green();\n            }\n            else\n            {\n                 cout << \"Don't run anything !! \" << endl;\n            }\n            return 0;\n       }\n"], "answer_code": [" int main(int argc, char *argv[])\n {  \n       ros::init(argc, argv, \"node_name\");\n       std::string param;\n       ros::NodeHandle nh(\"~\");\n       std::string check;\n       nh.getParam(\"param\", check);\n       cout << check << endl;\n       ROS_INFO(\"Got parameter : %s\", check.c_str());\n\n        if(check.compare(\"blue\") == 0)\n        {\n          cout << check << endl;\n          blue();\n        }\n         else if(check.compare(\"greeen\") == 0)\n        {\n          cout << check << endl;\n          green();\n        }\n        else\n        {\n          cout << \"Control has come out !!! \" << endl;\n        }  \n        return 0;\n  }\n", " rosrun package node_name _param:=blue\n rosrun package node_name _param:=green\n", "nh.getParam()", "_blue:=true", "_blue:=1", "false", "_blue:=some_text", "true", "param", "check", "#include<ros/ros.h>\n#include<iostream>\n\nusing namespace std;\n\n int main(int argc, char *argv[])\n {  \n    std::string param;\n    ros::init(argc, argv, \"node_name\");\n    ros::NodeHandle nh(\"~\");\n    nh.getParam(\"param\", param);\n    ROS_INFO(\"Got parameter : %s\", param.c_str());\n\n    if(param.compare(\"blue\") == 0)\n    {\n        cout << \"blue \" << endl;\n    }\n    else if(param.compare(\"green\") == 0)\n    {\n        cout << \"green \" << endl;\n    }\n    else\n    {\n        cout << \"Don't run anything !! \" << endl;\n    }\n    return 0;\n}\n", " rosrun my_package my_nodename _param:=blue  ??\n", "rosrun my_package my_node _my_param:=value\n", "rosrun my_package my_node _blue:=true\n", "rosrun package node _variable:=value", "rosrun package node topic:=/topic_name"], "url": "https://answers.ros.org/question/299014/command-line-argument-passing-in-rosrun/"},
{"title": "What is the next ROS distribution?", "time": "2019-04-19 14:53:42 -0600", "post_content": [" ", " ", "Currently, the most recent distribution is Melodic. In keeping with the release schedule, a new version should come out in about a month. Has any information about the next distribution been released? I can't find so much as a name, much less potential changes or a release date."], "answer": [" ", " ", "Discussion and updates about upcoming ROS releases happens on the ROS discourse server.", " The next ROS release will be Noetic Ninjemys:  ", "  (name announced as part of the Melodic release announcement). ", " It looks like there has been some discussion around planning the Noetic release, but it does not look very active:  ", "  . I think most of OSRF's energy is focused on ROS2 at the moment. ", " A key part in this answers is linked to from the Noetic discussion:  ", " . Specifically, the decision to only make ROS releases on Ubuntu LTS releases was made, so Noetic will be on the next Ubuntu LTS, 20.04. "], "url": "https://answers.ros.org/question/321483/what-is-the-next-ros-distribution/"},
{"title": "Print subset of message", "time": "2018-10-23 03:14:59 -0600", "post_content": [" ", " ", "I would like to programmatically print a subset of ", ", specifically only the flags set to true. I have found only ", " regarding the display of messages, and the answer suggests to use ", " which, in this case is a method which returns a long string made like this:", "Is storing this string and parsing it the only way I can get the names of the errors to use in a for loop that checks which flags are true? Is there no way to get the name of the single error from the message itself?", "You could take a look at ", ".", "Afaik there is no OotB support for this, but that library tries to fill that gap.", "If you know the message (thus all the fields) maybe you could compare each field in the callback and only print if true like that :", "What ", " suggests is certainly an approach for this specific message.", "I took the question by ", " to be a bit more general in which case hard-coding wouldn't be feasible.", "If you are still intereste to know the answer, I can write an example for you (I am the author of ", ").", "I am, I was looking at this question again exactly yesterday to do another similar thing to a message and couldn't remember the name of your package, but in the end delayed the task in favour of another."], "answer": [" ", " ", "This is one of the problems that ", " can solve for you.", "I created an example here just for you ;)", "As you can see, you can deserialize __any__ ROS message into a list of Key/Value pairs, one for each field of the message.", "In your particular case, to keep the example simple, we just used the fact that we know that all the fields are bool.\nhere you can see an incomplete snippet of the solution:", " ", " ", "Thanks ", " for the suggestion! Thanks ", ", but as said I'm looking for something more general. I thought there would be some built-in way that I wasn't seeing/finding. I looked into ros_type_introspection and it is more powerful than what I need now... I ended up parsing, for now. Cheers!", "I ended up parsing, for now", "that's basically what ", " does.", "But it introduces a dependency, where I only really need a tidbit of the functions the introspection grants.  The package would seem much more suited to the task if I didn't already know I want to parse a franka_msgs::Errors.", "If your approach works for you +1.", "I just wanted to clarify what ", " does.", "re: dependency: it's released on every major ROS release, so a single ", " or ", " away.", "I had not realized this. I'll add a todo and think about it in the future. Thanks again for the great help!"], "question_code": ["ros::message_traits::Definition<franka_msgs::Errors>::value()", "return \"bool joint_position_limits_violation\\n\\\nbool cartesian_position_limits_violation\\n\\\n...\n\";\n", "if (msg->joint_position_limits_violation)\n{\n    ROS_INFO(\"joint_position_limits_violation\");\n}\n//Same for each fields of the message\n"], "answer_code": ["void DeserializeAndPrint(RosIntrospection::Parser* parser,\n                         const std::string& topic_name,\n                         std::vector<uint8_t> & buffer) {\n\n    using namespace RosIntrospection;\n    FlatMessage flat_container;\n    parser->deserializeIntoFlatContainer( topic_name,\n                                          absl::Span<uint8_t>(buffer),\n                                          &flat_container, 100 );\n\n    RenamedValues renamed_numerical_values;\n    parser->applyNameTransform( topic_name,\n                               flat_container,\n                               &renamed_numerical_values );\n\n    for(auto& it: flat_container.value)\n    {\n        bool value = it.second.convert<int8_t>();\n\n        if( value )\n        {\n            std::cout << it.first.toStdString() << \" : \"\n                      << it.second.convert<double>() <<  std::endl;\n        }\n    }    \n}\n", "ros_type_introspection", "ros_type_introspection", "apt-get install ..", "rosdep"], "url": "https://answers.ros.org/question/306488/print-subset-of-message/"},
{"title": "Arduino Ethernet Driver | Custom hardware publishing on topic via ethernet", "time": "2017-05-04 04:22:37 -0600", "post_content": [" ", " ", " ", " ", "I have a ", " which I am using with an ", ".  My ultimate goal (TL:DR) is to publish these sensor values on a ROS topic via ", ".", "I am using a Ethernet shield and have completed the ", " which walked through creating a server which 'publishes' the sensor data via HTTP. On my PC browser (on the same network) I can navigate to the server/ethernet shield IP address and view the sensor values. \nI am familiar with fundamental ROS concepts but just a beginner in Ethernet communication, TCP/IP, etc", "So what I'm trying to do is write a driver which 'reads' these sensor values, formats them into a custom rosmsg and publishes this rosmsg on a topic. ", "I would appreciate some tips if anyone is familiar with interfacing between ", " and ", ". How would one go about the above things I mentioned? I guess it's to set up a socket listening to the sensor but have no idea how to start on this. I realise that a many sensor drivers do a similar thing as I'm trying to do. I know some SICK sensors communicate via Ethernet so effectively I'm trying to reproduce the same funtionality with my custom hardware. I did look at some of these drivers however couldn't understand them very well. If anyone has any links to tutorials / guide of something similar that would be great.", "Also if anyone has any wisdom to impart or tips to offer, that would be great! ", "Cheers"], "answer": [" ", " ", " ", " ", "Will answer this myself.", "I used the Arduino example \"WebServer\" from the library Ethernet2 as my ", ". I then ", " to 'listen' to this data. I used ", " from the website Binary Tides as a base for my client socket and ", " to understand how the code works. ", "\nThe next part was to use a string stream to convert the data being received into int arrays (actually vectors) of my desired size. I then created a custom ROS msg by following ", " then formatted my data as desired and published it using a basic ros publisher node. ", "If anyone is trying to ", " from an ", " (e.g. custom sensor) on a ROS ", " via ", " (TCP/IP) feel free to ask for help", "hey I am trying something same, so I need to subscribe some ros messages and control servo motors. the communication has to be over ethernet. Any suggestions?", "the arduino has to subscribe the ros messages", "If you follow the links in my answer, you should be able to at least create a ROS node which can communicate with Arduino via Ethernet. Then you need make it subscribe to your servo topic as well. The Arduino Example \"WebServer\" is already reading data, have a look at that.", " is my code. You will have to edit most of it to be used in your application", "Hey Thank you, i ll take a look at your code and will make sense out of it. Thanks for the help . i really appreciate it . Will contact you if face problems", "Hi, I'm trying to do the opposite way com, I mean, I'm trying to send data to arduino trough the Ethernet shield for remote operation of some actuators.... Could you please give me some ideas for do this? Thank you", "Hi Noel, what I've discussed will let you communicate both directions with your Arduino. You will need to write a subscriber for the topic and the code I linked above can be used to send that data to your Arduino. Then you can use that to control your actuators.\nHope that helps, Max", " ", " ", " ", " ", "Max Carr did a fantastic job, thanks man! Having said that, I had a hard time with (1) getting the Arduino code figured out, and (2) a C++ program to communicate with the Arduino. My contribution (if you can call it that) to this topic is to provide the following:", "You still have to convert the C++ code to a ROS package, but that is where Max Carr's answer is not at all ambiguous, and there are a ton of resources to figure that out on the ROS website. I will touch on that at the end. However the bulk of thsi answer deals with Arduino to C++ communication via sockets.", "The hardware is a PC and an Arduino Mega with an ethernet shield. I have a switch in between.", "Quick note on the Arduino side of things - I see Arduino as a cheap, and effective way to collect the status of sensors (both analog and digital), and control simple devices (lights, relays, etc...). Since you are running ROS, you have ample processing power in your PC, so I don't see you sending the Arduino tons of data to crunch. right? So, the code below receives a 3 character request, where the first two characters are capital letters, followed by a ", " as the end of transmission terminator. The Arduino code receives this message, and depending on the combination of letters (ex: AA~, or AB~, or ZZ~, etc....) the Arduino will take the appropriate action (send a pre-canned command to some device, turn on actuators, or in my case respond with the status of various sensors). In my case I am sending 8 variables back to the C++ program (6 int and 2 float). I am packaging each data type with letters. so the response looks like this: ", "With the 3 ~ characters serving as the end of transmission."], "answer_details": [" ", " ", " ", " ", "Arduino code that uses sockets (not HTML or HTTP) to communicate to a C++ program. Arduino program can receive different types of requests from C++, and dish out the requested data.", "C++ easily sends messages to Arduino, and collects the response.", " ", " ", " ", " "], "answer_code": ["~", "A123B\nC456D\nE98F\nH-23.4532I\n....\n~~~\n", "#include <SPI.h>\n#include <Ethernet.h>\n#include <SimpleTimer.h> //see: https://playground.arduino.cc/Code/SimpleTimer\n#include \"arduino2.h\" //see: https://www.codeproject.com/Articles/732646/Fast-digital-I-O-for-Arduino\n\n// The Mac address is BADA5512456 (BAD A55 [ASS] followed by 123456).\nbyte mac[] = {0xBA, 0xDA, 0x55, 0x12, 0x34, 0x56};\n\n//The IP address is obviously static, and there is no conflicts with any \n//other device on the robot. \nIPAddress ip(192, 168, 0, 21);\n\n// Initialize the Ethernet server library and assign  \n// and port you want to use (port 80 is default for HTTP):\nEthernetServer server(23500);\n\n//variables that allow me to figure out what the client is requesting\nint iterator = 0;\n//made bigher than 3 elements to accomodate bad requests that don't follow my AA~ protocol;\n//where the client sends a letter, followed by a letter followed by a ~ character to \n//let the server know the ..."], "url": "https://answers.ros.org/question/261052/arduino-ethernet-driver-custom-hardware-publishing-on-topic-via-ethernet/"},
{"title": "Can't ROS version compatibility issue for packages be resolved forever?", "time": "2018-10-09 06:53:52 -0600", "post_content": [" ", " ", " ", " ", "I have been running ", " on ", ". As it is the latest version of ROS and ROS does not allow the packages built in previous versions, I am really stuck in my project development. I am referring some very nice books and their ROS packages to understand ROS as a beginner, but the books and the packages are created sometime back and hence they have ", " based data. As a result, I am limited to theoretical reference only. \nI am referring the books that are listed as ", ".", "For example, currently I am referring the Chapter 9 of the book ", " that has Chefbot working as autonomous robot. The robot is developed based on ROS Kinetic and hence are its ROS packages. When I try to source the workspace using those packages, I end up with this error:", "I also tried ", " to ignore the errors and initiate ", "(I am not using ", " as it does not work for other packages) but it throws an error of not being able to process the package. ", "So I kindly request the community members to work in a direction which will remove this ROS version compatibility issue. I do not expect the solution to be robust as of now, but some patch development should be initiated that is what I suggest. I am keen and ready to be the part of this process provided some guidelines are given regarding the versioning of the packages.", "You should still be able to build the package even if it's not written specifically for melodic. Just building the package from source should be fine. Being said it doesn't mean you won't have any issues. But most of the time you should be alright.", "Just to reiterate ", "'s point:", "As it is the latest version of ROS and ROS does not allow the packages built in previous versions", "This is not actually true. It's perfectly possible to build packages from source, in a Catkin workspace.", "If you can post about an actual problem with a ..", ".. specific package, we can see whether we can help you solve that. Right now this question is too ambiguous and not something we can do something about.", "Also:", "the books and the packages [..] have ROS Indigo/Kinetic based data", "This is a problem with those books, not necessarily with ROS. We ..", ".. cannot retard or delay the development of ROS because of compatibility concerns with books that may be available unfortunately.", "kindly see the edited question.", "As a result, I am limited to theoretical reference only. I am referring the books that are listed as Standard Books for ROS.", "there are no \"standard books for ROS\".", "The wiki page you link to is just a page listing some of the books available to the general public. They are not endorsed or ..", ".., reviewed or recommended in any way.", "The wiki is editable by ", " with an account. After registering, you could also add a book to that list.", "As to the pkgs that ", " can't find: those all appear to be pkgs related to the Turtlebot 2, which indeed haven't been released yet. See ", " for another recent question about that.", "What you can try is to place those pkgs in your Catkin workspace and to build them from source."], "answer": [" ", " ", " ", " ", " is completely correct. The APIs and features of ROS have been fairly stable for a long time, likely your older references are completely valid, you'll just need to modify how you install any packages that they are working with. Often a package released on an older ROS version may also be released on a newer ROS version. If that is the case, you simply need to add modify calls to ", " to refer to ", " instead of Indigo or Kinetic. E.g. ", ". To check if a package has been released for Melodic you can try using ", " to search the available packages on your machine:", "You can also visit the ROS wiki page and check out the status at the top of the page. Clicking the different ROS distributions in blue rectangles right at the top will change page to be for different ROS releases. Right below, there are some green informational bubbles letting you know if the package has been released for that version of ROS. Note that if that particular wiki page doesn't have a Melodic section, that's a good sign the package hasn't been released yet.", "If a package hasn't been released yet, it's often just a matter of finding the source code for the package, adding it to your workspace, ensuring you have the correct dependencies (tools like ", " and ", " can help greatly with this) and then running ", ". If you're lucky, this will \"just work\", and it likely will much of the time. In my experience, the most likely packages to have issues are packages related to simulation. This is because Gazebo and its APIs have evolved rapidly and each version of ROS is built around different Gazebo versions. Sometimes API changes in Gazebo are precisely why a particular simulation package hasn't been released in a newer version of ROS.", "Finally, you are asking \"the community members\" to release all packages for all versions of ROS (a tall order for reasons that have little to do with ROS itself). As a ROS user, you are part of that community. It takes all of our efforts to keep packages up-to-date and bug free. The best way to help ensure a healthy ecosystem is to contribute yourself. File bug reports, submit pull requests, volunteer to maintain orphaned packages, release your own packages, etc. These are all ways you can help the community. This may feel intimidating as a beginner, but stick with it and you'll be ready to help out in no time.", " : I have elaborated my question with an edit.", "Thank you for motivating. I am in."], "question_details": [" ", " ", " ", " ", " ", " ", "Shaunak Agastya Vyas (TINU)"], "question_code": ["shaunak@robokits:~/catkin_ws$ rosdep install --from-paths src --ignore-src --rosdistro=melodic -y\nERROR: the following packages/stacks could not have their rosdep keys resolved\nto system dependencies:\nchefbot_bringup: Cannot locate rosdep definition for [depthimage_to_laserscan]\nchefbot_gazebo: Cannot locate rosdep definition for [create_node]\nchefbot_description: Cannot locate rosdep definition for [create_description]\n", "rosdep install --from-paths src --ignore-src --rosdistro=melodic -y -r", "catkin_make_isolated --force-cmake", "catkin_make", ".. cannot retard or delay the development of ROS because of compatibility concerns with books that may be available unfortunately.\n", "rosdep"], "answer_code": ["apt-get", "melodic", "sudo apt-get install ros-melodic-std-msgs", "apt-cache search", "jarvis@togwotee:~\u27eb apt-cache search std-msgs\ncl-std-msgs - LISP interface for Standard Robot OS Messages\nlibstd-msgs-dev - C/C++ headers for Standard Robot OS Messages\npython-std-msgs - Python interface for Standard Robot OS Messages\nros-std-msgs - Message definitions for Standard Robot OS Messages\nros-melodic-std-msgs - Standard ROS Messages including common message types representing primitive data types and other basic message constructs, such as multiarrays.\n", "rosdep", "rosinstall", "catkin_make", "This may feel intimidating as a beginner, but stick with it and you'll be ready to help out in no time.\n"], "url": "https://answers.ros.org/question/305226/cant-ros-version-compatibility-issue-for-packages-be-resolved-forever/"},
{"title": "costmap_2d plugin", "time": "2018-10-15 21:04:24 -0600", "post_content": [" ", " ", " ", " ", "Hello. I am a ROS beginner. I am doing SLAM of turtlebot 3 (waffle) in the ubuntu 16.04, ros (kinetic) personal computer environment. I modified local_costmap_params.yaml to use the costmap _ 2 d plugin (Inflation Layer). After the change, it became as follows.", "local_costmap: global_frame: /odom robot_base_frame: /base_footprint", "update_frequency: 10.0 publish_frequency: 10.0 transform_tolerance: 0.5", "static_map: false \nrolling_window: true width: 3 height: 3 resolution: 0.05 \nplugins: - {name: inflation_layer, type: \"costmap_2d::InflationLayer\"}", "However, after starting gazebo normally,", "\uff04 export TURTLEBOT3_MODEL=waffle \uff04roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=$HOME/map.yaml", "When you do this,", "Warning: Invalid argument \"/map\" passed to canTransform argument target_frame in tf2 frame_ids cannot start with a '/' like: at line 134 in /tmp/binarydeb/ros-kinetic-tf2-0.5.18/src/buffer_core.cpp", "The error will come out in succession. Could I ask the reason? I feel that I am doing some fundamental mistake. Thank you.!", "Do you have a costmap_common_params.yaml file? If yes please update your answers with it.", "thank you.I have a costmap_common_params_waffle.yaml .this is OK?"], "answer": [" ", " ", " ", " ", "Try using the following ", ":", "The problem most probably is a TF2 thing. It does not accept leading slashes on TF2 frame names so make sure that in your common, local and global params files you dont use a \"/\" in the frame names", "I did that. However, I get an error message. Also, even if you restore the same error message will appear. This error message will appear continuously.", "Warning: Invalid argument \"/map\" passed to canTransform argument target_frame in tf2 frame_ids cannot start with a '/' like: \n         at line 134", "according to your question you had something like:", "Did you try setting the frames without ", " so it looks like:", "The same error occurs in either way. Also, reinstalling the package of turtlebot 3 will cause the same error, so it may be due to a different location somewhere.Image added.\nThe costmap is in error in RViz.", "maybe it is coming from the ", " or  ", ". Check if there the frame is put with a ", "Looks like it is a TF2 thing. It does not accept leading slashes on TF2 frame names so make sure that in your common, local and global params files you dont use a \"/\" in the frame names", "Thank you. The \"/\" in global was the cause.However, this image is displayed even though you do not use the plugin. Is plugin(static,obstacle,inflation) being used as a standard?", "I am glad it fixed your initial problem. You can mark the answer as correct now by clicking the green tick. As for the plugins, the static, obstacle or inflation ", " are used as standard plugins. You can also write and implement your own custom plugin", "Thank you! I have made a new question about the new plugin. Please answer if you are good!"], "answer_code": ["local_costmap_params.yaml", "local_costmap:\n  global_frame: odom\n  robot_base_frame: base_footprint\n  map_type: costmap\n\n  static_map: false\n  rolling_window: true\n  width: 3.0\n  height: 3.0\n  resolution: 0.05\n  always_send_full_costmap: true\n\n  update_frequency: 10.0\n  publish_frequency: 10.0\n  transform_tolerance: 0.5\n\n  plugins:\n  - {name: inflater_layer, type: \"costmap_2d::InflationLayer\"}\n\n  inflater_layer:\n    inflation_radius: 0.2 # adjust the inflation for your system setup\n    cost_scaling_factor: 1.0\n", "local_costmap: \n  global_frame: /odom \n  robot_base_frame: /base_footprint\n...\n", "\"/\"", "local_costmap: \n  global_frame: odom \n  robot_base_frame: base_footprint\n", "global_costmap_params.yaml", "costmap_common_params.yaml", "\"/\""], "url": "https://answers.ros.org/question/305880/costmap_2d-plugin/"},
{"title": "Object detection with radar", "time": "2018-09-18 04:52:40 -0600", "post_content": [" ", " ", " ", " ", "Edit1: I did now some research and so I realized that I have to describe the problem in more detail. I have a radar sensor which provides a pointCloud to me with the measured points. Objects that are more than 100m away are normaly represented by only one point in the point cloud. Now I am not sure if it is really a object or a measurement error. So the solution is to watch over for example 5 samples and if all 5 sample includes the point, with a small change because of the movement, than there is a object. This should also work if within the 5 samples is one point missing because of an measurement error.", "I found during my research the kalman filter but i am not sure if it is the right one and how to use is. Thanks.", "br Harald", "Hi ", "I am using a radar sensor to detect moving objects. The speed of the moving objects (e.g. cars and persons) is up to 140kmh. I get a pointCloud from my radar sensor and if an object is moving with for example 50kmh I see the track of the object. Now I need a algorithm that is able to detect an moving object also when the object is only represented by one point. So my question is if there is any recommendation for a specific filter or any  tip that could help me to do that! Thanks.", "BR Harald", "Hey Harald, ", "What kind of radar are you using? What's the hardware and did you get an open source to make it play in ROS? I'm interested in knowing that! "], "answer": [" ", " ", " ", " ", "hi ", "\npls check  ", " library it's a power full lib for point cloud(like opencv for image)\nand also there is some works in automated driving like ", "\nthat i think this similar for your research"], "url": "https://answers.ros.org/question/303637/object-detection-with-radar/"},
{"title": "Hector Exploration Slow?", "time": "2016-12-09 16:16:52 -0600", "post_content": [" ", " ", " ", " ", "Hello, I'm experimenting with the Hector Navigation Stack and I'm quite happy with the results I'm getting. However, right now I find that Hector Exploration is too slow for my application, in the Youtube videos from the authors it seems a lot faster, but when I'm running it takes more than 30 seconds every time I request the exploration service. Does anybody have ideas how to 1) make the planned trajectory farther from the start point, and 2) make goal finding and path planning faster? Many thanks in advance.", "The following is a summary of the log from Hector Exploration:", "Please make sure to use the ", " button (the one with ", " on it) for log (code, console, launch files, etc) copy/pastes: without it, the text won't show (due to filtering). Just select the copied text and press the button (or use ", ").", "Thanks.", "Thank you. I have updated the format. ", "I'm surprised by the number of frontiers it found. That seems like a lot of test, which might be why it is so slow."], "answer": [" ", " ", "hector_exploration generally has been used by us on maps smaller than 50m x 50m (1024 x 1024 cells @0.05m resolution). There is no explicit handling performed for very large maps (such as tiling or other approaches), so for arbitrary large maps, things might become arbitrary slow. That being said, the output still looks a bit weird. It says a 2048 x 2048 size map is used. That's 4194304 grid cells. A little further down, the planner says it found 4176345 frontier cells (cells that contain a transition from free to unknown space). That's 99.534% of all map cells being detected as frontier cells, which is not plausible. Can you edit your post with an image of your map?", " A new implementation of exploration planning using the grid_map library is available here:  ", " . I'd recommend giving that one a shot. It should also be more efficient on large maps, as it only touches cells that are reachable by the robot. ", " ", " ", " ", " ", "My Android application for Hector Quadrotor's control gets pretty slow when the computer is only on the battery, when it's plugged in the normal power outlet. Maybe it's a power mode that slows down the pc CPU where the master ROS is running. Maybe this simple information can help someone."], "question_code": ["[ INFO] [125.754800471]: HectorCM: is up and running.\n[ INFO] [125.799118221]: Loading from pre-hydro parameter style\n[ INFO] [125.884867888]: Using plugin \"static_layer\"\n[ INFO] [125.987612471]: Requesting the map...\n[ INFO] [126.302194221]: Resizing costmap to 2048 X 2048 at 0.050000 m/pix\n[ INFO] [126.396465555]: Received a 2048 X 2048 map at 0.050000 m/pix\n[ INFO] [126.414453555]: Using plugin \"obstacle_layer\"\n[ INFO] [126.431127221]:     Subscribed to Topics: \n[ INFO] [126.466592721]: Using plugin \"inflation_layer\"\n[ INFO] [126.597621721]: [hector_exploration_planner] Initializing HectorExplorationPlanner\n[ INFO] [126.936871221]: [hector_exploration_planner] Parameter set. security_const: 0.500000, min_obstacle_dist: 1000, plan_in_unknown: 1, use_inflated_obstacle: 1, p_goal_angle_penalty_:50 , min_frontier_size: 5, p_dist_for_goal_reached_: 0.250000, same_frontier: 0.250000\n[ INFO] [181.322802958]: Exploration Service called\n[ INFO] [181.553172208]: [hector_exploration_planner] Size of trajectory vector for close exploration 223\n[ INFO] [181.553525541]: [hector_exploration_planner] pushed 10 goals (trajectory) for close to robot frontier search\n[ INFO] [203.212077703]: [hector_exploration_planner] exploration: found 4184151 frontiers!\n[ INFO] [213.636407867]: [hector_exploration_planner] exploration: plan to a frontier has been found! plansize: 25\n[ INFO] [240.141801611]: Exploration Service called\n[ INFO] [240.368448777]: [hector_exploration_planner] Size of trajectory vector for close exploration 458\n[ INFO] [240.368647444]: [hector_exploration_planner] pushed 15 goals (trajectory) for close to robot frontier search\n[ INFO] [263.229025855]: [hector_exploration_planner] exploration: found 4183563 frontiers!\n[ INFO] [274.304115019]: [hector_exploration_planner] exploration: plan to a frontier has been found! plansize: 51\n[ INFO] [326.958585507]: Exploration Service called\n[ INFO] [327.206913507]: [hector_exploration_planner] Size of trajectory vector for close exploration 806\n[ INFO] [327.207185257]: [hector_exploration_planner] pushed 33 goals (trajectory) for close to robot frontier search\n[ INFO] [349.500774335]: [hector_exploration_planner] exploration: found 4181744 frontiers!\n[ INFO] [359.918440916]: [hector_exploration_planner] exploration: plan to a frontier has been found! plansize: 18\n[ INFO] [379.455494828]: Exploration Service called\n[ INFO] [379.681278661]: [hector_exploration_planner] Size of trajectory vector for close exploration 1016\n[ INFO] [379.681586161]: [hector_exploration_planner] pushed 35 goals (trajectory) for close to robot frontier search\n[ INFO] [402.406080989]: [hector_exploration_planner] exploration: found 4177748 frontiers!\n[ INFO] [413.360849903]: [hector_exploration_planner] exploration: plan to a frontier has been found! plansize: 18\n[ INFO] [432.999668898]: Exploration Service called\n[ INFO] [433.214620565]: [hector_exploration_planner] Size of trajectory vector for close exploration 1230\n[ INFO] [433.214923148]: [hector_exploration_planner] pushed 42 goals (trajectory) for close to robot frontier search\n[ INFO] [454.773803143]: [hector_exploration_planner] exploration: found 4176345 frontiers!\n[ INFO] [465.059485641]: [hector_exploration_planner] exploration: plan to a frontier has been found! plansize: 17\n[ INFO] [484.091777386]: Exploration Service called\n[ INFO] [484.319620636]: [hector_exploration_planner] Size of trajectory vector for close exploration 1434\n[ INFO] [484.319980219 ...", "101010", "ctrl+k"], "url": "https://answers.ros.org/question/249777/hector-exploration-slow/"},
{"title": "turtlebot3 low battery beeping", "time": "2018-07-03 17:43:18 -0600", "post_content": [" ", " ", "Hello! To preface, I am running ros on a turtlebot3 using Ubuntu 16.02. I am wondering where I can change the turtlebot's source code so that it does ", " beep when the battery is low. Thanks!"], "answer": [" ", " ", "Hi ", ",", "The buzzer you mentioned is equipped with OpenCR, the main controller of TurtleBot3. OpenCR checks the voltage of the battery and when the voltage is below 11V, the buzzer sounds. If you do not want to use it, you can modify the turtlebot3_diagnosis.cpp part of OpenCR. The related code is the link below.", "Please refer to the link below for the information needed for this modification. :)", " Setting the Arduino IDE for OpenCR\n ", " How to modify ROS library code (OpenCR)\n ", "Fantastic, thank you!!"], "url": "https://answers.ros.org/question/296145/turtlebot3-low-battery-beeping/"},
{"title": "Texturize robot consisting of STL-files", "time": "2018-07-14 09:14:23 -0600", "post_content": [" ", " ", "I created robots links with CATIA exported them as STL an constructed my own robot with URDF. Now I'm able to move my little mobile robot around the world in Gazebo. Yeah. \nThen I mounted an Universal Robot Arm on top of my rover and there where textures on the arm. I looked in the XACRO-Description and found out, they use DAE files with textures to colorize the robots links.\nIs it possible (and if with which software) to import STL-files attach textures on it and export it as DAE as easy as possible?\nHow would you texturize my existing robot with STL files?", "Thanks for your help\nMax"], "answer": [" ", " ", "There are many programs that can do this. Personally I'd use blender. It's free and very powerful will loads of you tube tutorials. ", "Ok thanks. I did it. Worked fine for me."], "url": "https://answers.ros.org/question/297365/texturize-robot-consisting-of-stl-files/"},
{"title": "What does intensity mean intensity exactly ?", "time": "2018-07-04 10:38:10 -0600", "post_content": [" ", " ", "Hello,", "I am getting a from a rosbag a point cloud which as been register with a velodyne. I'm visualizing it with a PointCloud2 display type. ", "I have some artifacts on the result so I'm trying to understand exactly what is appending, but i don't understand what Intensity mean in the \"Channel Name\" field or in the \"Color Transformer\" field.", "Does someone can help me understand what the intensity represent ? Is it the number of point at the same place ? ", " I looked in the documentation i founded but whiteout any success...\n ", " ", " "], "answer": [" ", " ", "Regarding LiDAR data intensity refers to the energy of the reflected laser pulse. This is a secondary measurement on most modern LiDAR sensors alongside the primary time measurement used to determine depth.", "The exact units of these measurements vary from one manufacturer to another but essentially represent the infrared reflectance of the surface at that point, often normalised for depth.", "In RVIZ when you select 'intensity' you're telling RVIZ to color the point cloud displayed with the intensity values of each point. Using this coloring it is usually possible to make out the patterns of different materials or paints on flat surfaces for example.", "Thanks a lot for your answer! That's exactly what I was asking for!"], "url": "https://answers.ros.org/question/296259/what-does-intensity-mean-intensity-exactly/"},
{"title": "Outdoor trailer URDF model", "time": "2018-07-30 16:45:36 -0600", "post_content": [" ", " ", "Hello everyone.", "So I'm facing a very basic scenario I'm having trouble setting up. It consists of a powered vehicle (4 wheels, rear transmission) drawing an unpowered trailer (2 wheels, no transmission). The hitch connecting the two parts is a joint that is not fixed, so the trailer won't be aligned with the vehicle when the vehicle is turning, just like an articulated bus.", "What I need from this model is the trailer's pose relative to the vehicle, because my gps antenna is located somewhere in the vehicle and my camera is located somewhere in the trailer, so in order to project what my camera is seeing I need this transform to update in real-time.", "So far, I have wrote the URDF file following the tutorial and controlled the wheels in my vehicle using rqt_robot_steering. Gazebo shows how the robot moves and publishes transforms for the hitch revolute joint. So far so good.", "When I try to merge this to my real project, where the vehicle's location (base_link) is set by my high-freq, high-precision gps antenna and the ros package ", ", I don't get the vehicle moving in Gazebo nor articulating in RViz. Probably because I need some sort of transmission to move the vehicle in Gazebo. The thing is, I don't even know if I actually need Gazebo for this. All I need is an approximate position for the trailer.", "Any ideas on how I can deal with this?"], "answer": [" ", " ", " ", " ", "You do not need Gazebo when you are working with the real hardware. Ideally, your packages should be set up so that they are agnostic to the vehicle running as a simulation in Gazebo or as real hardware.", "Gazebo is not a visualisation tool, so when running the real hardware you should not expect Gazebo to respond.", "Gazebo is providing joint position information and publishing transforms based on its simulation of the vehicle's bodies and joints. For your hardware, you will need to provide this information yourself. For example, you could put an encoder on the joint so that you can publish the position of the pivot between the powered vehicle and the trailer. You should publish this information to the ", " topic. The URDF tutorials have a ", " on how to do this. (For testing your URDF in ", ", you can use the ", ", which will mimic the joint position moving.)", "Once you have the joint state being published, you need to publish the updated transforms to ", ". The ", " can do this for you automatically. It reads in your URDF and looks for new joint states, then publishes the updated transforms automatically.", "When you get transforms flowing, everything will magically start working. ", " will display your articulated vehicle in its correct state, and you will be able to get the pose of the camera relative to the powered vehicle or to any other frame easily from ", ".", "Thank you very much"], "question_code": ["geonav_transform"], "answer_code": ["/joint_states", "rviz", "joint_state_publisher", "/tf", "robot_state_publisher", "rviz", "tf"], "url": "https://answers.ros.org/question/299084/outdoor-trailer-urdf-model/"},
{"title": "Launching PowerCube on multiple CAN busses", "time": "2018-07-17 15:24:35 -0600", "post_content": [" ", " ", "Hello I am working with a Schunk LWA3 (DOF 7) in ROS with the Schunk PowerCube package. ", "I have success launching the PowerCube and initializing the modules. The issue is that the arm has the modules broken out onto two difference CAN busses which are connected to different ports on a PCI card. The .yaml file that is taken in by the ROS node is: ", "Again, this works fine and modules with Ids 1,2,3,4 are initialized just fine. However when I try to include a new ", " at ", "  (to find modules 5,6,7) it will take whatever device is mentioned second and load the corresponding modules. ", "Ive tried:", "This method still causes the first device listed to be used and only those modules found. I also attempted to edit my launch file to look for two different .yaml's but only the second is considered in this case. ", "Does anyone have experience with this or any suggestions?", "Thank you !", "Kyle "], "answer": [" ", " ", "I was able to fix the issue with a CAN y-cable and connecting them to the same bus on one PCAN device. ", "Im not sure how I will communicate with the PG70 gripper which powercube in this method recognizes as a rotary joint and requests a new configuration using the windows powercube. Most of the gripper support seems to be serial communication "], "question_code": ["powercube_chain parameters\ncan_module: PCAN\ncan_device: /dev/pcan0 \ncan_baudrate: 1000\nmodul_ids: [1,2,3,4] \njoint_names: [arm_1_joint, arm_2_joint, arm_3_joint, arm_4_joint]\nmax_accelerations: [0.4,0.4,0.4,0.4]\nmin_publish_duration: 0.01 \nhorizon: 0.1 \nfrequency: 100\n\nptp_vel: 0.4 # rad/sec\nptp_acc: 0.2 # rad/sec^2\nmax_error: 0.15 # rad\n", "can_device", "/dev/pcan1", "devices: [\"/dev/pcan0\", \"/dev/pcan1\"]\n"], "url": "https://answers.ros.org/question/297723/launching-powercube-on-multiple-can-busses/"},
{"title": "Pointers towards using robot_localization for VO + Wheel odometry", "time": "2018-06-20 07:40:44 -0600", "post_content": [" ", " ", " ", " ", "Hey , i am trying to setup a EKF fusion for combining odometry coming from two topics ", "The main source of Odometry is coming from wheel encoders , the visual odometry is there much like a correction mechanism like while using IMUs. Having said that , my VO node publishes Twist values , is it better to fuse the wheel odometry using Twist values using robot_localization ?", "\nThe TF of wheel encoder odometry is as follows ", "TF of the visual Odometry is as follows", "[EDIT #1 adding sensor data samples ]", "Please post sample messages for every sensor input.", "Hey , I have edited the question and added the sample data . I think I have resolved the issue of rviz crashing , it was because the TF not set properly .", "Now ,The covariance on the fussed output keeps building up into a larger and larger value. How do I systematically tackle this ?"], "answer": [" ", " ", " ", " ", "You have a few issues.", "You are fusing velocities from your visual odometry source, but your visual odometry source has no ", ". If you look at the ", ", you'll see that the ", " is the TF frame of the twist (velocity) data, but you are leaving this empty. That means the EKF is going to ignore it completely.", "You have ", " set to ", ", which means you want a full 3D state estimate, but you are only fusing 2D variables. Unmeasured variables in the EKF will result in absolute explosions of those variables' covariance values, which will have irritating effects on other variables. Turn on ", " or start measuring the other 3D variables.", "Your state estimate covariance is growing without bound for your X and Y position, because you are only fusing velocity data for those. The filter will integrate those velocities, but it will also integrate their errors. If you had, for example, a laser scanner, you could localize against the map to provide X/Y position that would cap the growth in covariance. You are fusing two data sources that accumulate error over time, though, so endless growth in your covariance is actually appropriate. You should really fuse yaw velocity from your wheel encoders, and not absolute yaw.", "Note, though, that if you fuse absolute X, Y, and yaw from your wheel odometry, AND your wheel odometry covariance is NOT calculated correctly (e.g., it's static, as opposed to growing without bound), that will cap the covariance as well, but it will be incorrect.", "In general, yes, I recommend fusing velocity data for odometry sources, including visual odometry. If you had some means of localizing the robot globally (e.g., GPS or scan-to-map localization or overhead camera localization), you would fuse those absolute pose values.", " in response to comments:", "I'm not sure we're on the same page with what an IMU does or how it constrains drift (or doesn't). An IMU typically produces the following quantities:", "If you fuse roll, pitch, and yaw into the EKF, that data is coming from accelerometers and magnetometers, which are measuring the IMU's orientation with respect to a fixed frame. So if you fuse those values, you will constrain drift ", ", and their covariances will reach a steady-state and will stop growing.", "If you fuse velocity or linear acceleration data from an IMU, but do NOT fuse the roll, pitch, or yaw data, your roll, pitch, and yaw covariance will grow without bound.", "In this EKF, we are concerned with the following quantities:", "I am taking this approach to build my visual odometry node .", "But the drift is just too much . What if i use the Visual Node as simply an error correction node much like a IMU enabled EKF localization node . ", "Are there other  similar work?", "My greatest constraint is Computational power .", "I have to run everything on a RPI3B+ .", "I have already tried Viso2 ,  ORB-SLAM2 ,RPG-SVO .(Too Slow!)", "recently came across ", " , yet to test it .", "Even if you did that, the error you're correcting is error in ", " measurement, not pose measurement. Odometry accumulates error over time, and you covariance will reflect that. The only way to constrain drift in the EKF output is to have an absolute pose measurement.", "While using an IMU , we are not getting an absolute pose , right?", "So can a visual odometry node be used in a similar way ?", "okay , now i understand better .\nOne last question , is there a way to estimate Angular Velocity and Linear Acceleration  ?", " returns , R \u2013 Recovered relative t \u2013 Recoverd relative transl", "\"Another problem occurs when the camera performs just pure rotation: even if there are enough features, the linear system to calculate the F matrix degenerates. \"", "What if i use wheel odometry as an absolute scale ?", "Your wheel odometry is generated by integrating encoder ticks. It has the same issue. If it's written correctly, the pose variables will have covariances that grow without bound.", "Is it a problem for your covariances to grow? Or are you trying to reject bad measurements from your VO?"], "answer_details": [" ", " ", "Absolute roll, pitch, and yaw", "Angular velocity", "Linear acceleration", "X, Y, Z position - can be measured directly, or integrated from linear velocity data.", "Roll, pitch, yaw - can be measured directly, or integrated from angular velocity data", "X, Y, Z velocity - can be measured directly, or integrated from linear acceleration data", "Roll, pitch, and yaw velocity - can only be measured directly", "X, Y, Z acceleration - can only ...", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Visual Odometry (X , Y and yaw)", "Wheel Encoders ( X , Y and yaw)"], "question_code": ["Assertion `!pos.isNaN() && \"Invalid vector supplied as parameter\"' failed.\n", " frame_id: \"odom\"\n child_frame_id: \"base_link\"\n", "frame_id: \"raspicam\"\nchild_frame_id: ''\n", "<node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization_custom\" clear_params=\"true\"> <!-- renamed to ekf_localization_custom to prevent name conflicts -->\n\n  <param name=\"frequency\" value=\"30\"/> <!-- this value should be fine -->\n\n  <param name=\"sensor_timeout\" value=\"0.1\"/> <!-- this value should be fine -->\n\n  <param name=\"two_d_mode\" value=\"false\"/> <!-- could make this true to stick to XY plane.. -->\n\n  <!-- based these values off of the ROScon presentation -->\n  <param name=\"map_frame\" value=\"map\"/>\n  <param name=\"odom_frame\" value=\"odom\"/>\n  <param name=\"base_link_frame\" value=\"base_link\"/>\n  <param name=\"world_frame\" value=\"odom\"/>\n\n  <param name=\"odom0\" value=\"/optical_flow\"/>\n  <param name=\"odom1\" value=\"/ubiquity_velocity_controller/odom\"/>\n\n  <!-- settings for using Twist -->\n  <rosparam param=\"odom0_config\">[false, false,  false,\n                                  false, false, false,\n                                  true, true, false,\n                                  false, false, true,\n                                  false, false, false]</rosparam>\n\n\n <rosparam param=\"odom1_config\">[false, false, false,  \n                              false, false, true, \n                              true, true, false,   \n                              false, false, true,\n                              false, false, false]</rosparam>\n\n  <param name=\"odom0_differential\" value=\"false\"/>\n  <param name=\"odom1_differential\" value=\"false\"/>\n  <param name=\"publish_null_when_lost\" value=\"false\"/>\n  <param name=\"odom0_relative\" value=\"false\"/>\n  <param name=\"odom1_relative\" value=\"false\"/>\n\n  <param name=\"odom0_queue_size\" value=\"10\"/>\n<remap from=\"/odometry/filtered\" to=\"/odometry/visual\" />\n</node>\n", " header: \n  seq: 3786\n  stamp: \n    secs: 1529497452\n    nsecs: 845127379\n  frame_id: \"odom\"\nchild_frame_id: \"base_link\"\npose: \n  pose: \n    position: \n      x: 0.50\n      y: 0.20\n      z: 0.0\n    orientation: \n      x: 0.0\n      y: 0.0\n      z: 0.01\n      w: 1.0\n  covariance: [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]\ntwist: \n  twist: \n    linear: \n      x: 0.8334\n      y: 0.0\n      z: 0.0\n    angular: \n      x: 0.0\n      y: 0.0\n      z: 0.1233\n  covariance: [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0 ..."], "answer_code": ["child_frame_id", "child_frame_id", "two_d_mode", "two_d_mode"], "url": "https://answers.ros.org/question/294668/pointers-towards-using-robot_localization-for-vo-wheel-odometry/"},
{"title": "ROS Subscriber in Android", "time": "2018-05-24 03:33:15 -0600", "post_content": [" ", " ", " ", " ", "I would like to make a Subscriber on my Android-Application which can subscribe the batterydata from the robotino.\nThese Data should be displayed on the apply. In this project i have two nodes which the predecessor make it. In one of then they subscribe the camera an, which I use the source code to hope they working for my extra function, but they show nothing.  Here my Sourcecode for the subscription", "In my MainActivity I call the \"getBatterystatus\" to receive the data from the battery:", "I publish some data but my app is crashing all the time. My Question is: How make a simple Subscriber for my Android-Application?", "Edit: Thank you for your answer. I hope that is the errorlog what are your need. The app crash doesnt make some error. I think the problem is the part from the messageListener. ", " can you copy the error that is making the application crash here? How are you executing the node that has that code?\nThe code of the node looks alright (see  ", " ). "], "answer": [" ", " ", " ", " ", "Hi! Can you update the question with the error that's making the app crash? Try debugging your app with ADB (see ", " for reference, you can see exactly what's going on with your app with it).", " The code for the node looks OK. How are you executing it? Have you taken a look at  ", "  Perhaps that can give you a hint. ", "I'd say the error is somewhere else. Use ADB and take a closer look to the error messages.", "EDIT: note that you can copy the code in your question, not as an answer, please take into account that this is not a forum.", "I just saw this line. What is this doing?", "You should make ", " just a plain double, and do", "i change the code and it isnt crashing but I dont received any data from the Subscriber.", " Try adding some logs to the callback. Use  ", "  as a reference; log the value you get in the callback. Use  ", " to check that the node has actually subscribed", "okay thanks i received the data but i cant pass over the data from the node to the RosActivity.", "ok, we are getting there then. What if you log the value you get outside the node instead of setting it directly? What is ", " returning?", "The NodeTeleopPublisher.getBatterystatus is zero. I checked the value \"batterystatus\" in the methode which is zero too. Do you know what is the problem?", "Is your whole code public? perhaps if I can take a quick look at all of it I can give you more help.\nAre you printing the value that you are getting inside the callback? Is it something different from 0?", "No this whole code was made from a student on my university. I must make a additional function for the application. In the ", " method i get the data, but in the ", " method i get zero."], "question_code": ["@Override\npublic void onStart(ConnectedNode connectedNode) {\n // Battery Subscriber\n    battery_subscriber = connectedNode.newSubscriber(GraphName.of(topic_basename + \"battery_status\"), std_msgs.Float64._TYPE);\n    battery_subscriber.addMessageListener(new MessageListener<std_msgs.Float64>() {\n        @Override\n        public void onNewMessage(Float64 message) {\n            batterystatus = callable.call(message);\n        }\n    });                                                                                                                                                                            public Float64 getBatterystatus() { return batterystatus;\n}\n", "    pTextBatteryStatus = (TextView) findViewById(R.id.robot_battery);\n    pTextBatteryStatus.setText(String.valueOf(NodeTeleopPublisher.getBatterystatus()));\n", "05-24 15:03:03.659 28077-28077/? E/HAL: load: couldn't find symbol HMI\n05-24 15:03:03.660 28077-28077/? E/memtrack: Couldn't load memtrack module (Invalid argument)\n05-24 15:03:03.660 28077-28077/? E/android.os.Debug: failed to load memtrack module: -22\n05-24 15:03:03.708 28077-28077/? E/HAL: load: id=fm != hmi->id=fm\n05-24 15:03:03.708 28077-28077/? E/fm_if.c: find the id:fm and begins to open the devices\n05-24 15:03:04.954 28092-28092/? E/HAL: load: couldn't find symbol HMI\n05-24 15:03:04.955 28092-28092/? E/memtrack: Couldn't load memtrack module (Invalid argument)\n05-24 15:03:04.955 28092-28092/? E/android.os.Debug: failed to load memtrack module: -22\n05-24 15:03:05.003 28092-28092/? E/HAL: load: id=fm != hmi->id=fm\n05-24 15:03:05.003 28092-28092/? E/fm_if.c: find the id:fm and begins to open the devices\n05-24 15:03:05.785 27146-27174/? E/ExternalAccountType: Unsupported attribute readOnly\n05-24 15:03:05.785 27146-27174/? E/CSP_ExceptionCapture: Unsupported attribute readOnly\n05-24 15:03:05.901 5459-5497/? E/LogCollectService: Level = 256\n05-24 15:03:11.144 28104-28104/? E/HAL: load: couldn't find symbol HMI\n05-24 15:03:11.144 28106-28106/? E/HAL: load: couldn't find symbol HMI\n05-24 15:03:11.144 28106-28106/? E/memtrack: Couldn't load memtrack module (Invalid argument)\n05-24 15:03:11.144 28104-28104/? E/memtrack: Couldn't load memtrack module (Invalid argument)\n05-24 15:03:11.144 28106-28106/? E/android.os.Debug: failed to load memtrack module: -22\n05-24 15:03:11.144 28104-28104/? E/android.os.Debug: failed to load memtrack module: -22\n05-24 15:03:11.202 28106-28106/? E/HAL: load: id=fm != hmi->id=fm\n05-24 15:03:11.202 28104-28104/? E/HAL: load: id=fm != hmi->id=fm\n05-24 15 ..."], "answer_code": ["\nbatterystatus = callable.call(message);\n", "batteryStatus", "\nbatteryStatus = message.getData();\n", "rostopic info", "NodeTeleopPublisher.getBatteryStatus", "onNewMessage", "getBetterystatus"], "url": "https://answers.ros.org/question/292118/ros-subscriber-in-android/"},
{"title": "Twist with covariance stamped Compass heading", "time": "2018-04-22 14:03:29 -0600", "post_content": [" ", " ", " ", " ", "My team and I are fairly new to ROS and we are trying to make an autonomous robot and we are using a compass to tell our heading.", "The problem was we were using a borrowed IMU and acted like a compass using gyros and we got our robot to work decent with that IMU but created a lot of drift. ", "So we are using now a Ada Fruit LSM303DLHC compass and accelerator which has very little drift the only thing I'm struggling with is trying to get ros my arduino code to spit out a geometry message twist with covariance stamped, because that is what we had our old IMU spitting out and it worked well.", "I have the arduino code that reads the heading and I have already calibrated the compass offsets. I just need to get my code to output that type of message. if anyone can help that would be greatly appreciated.", "The code is publishing integers to the Serial monitor a the moment.", "Here is the heading code:", "May I suggest you edit your question text and add some full stops, commas and perhaps divide it up into some parapgraphs? This is just about unreadable.", "How is you system set up? Are you trying to implement your arduino code in ROS? Are sending a message from an arduino board to a computer running ROS?", "I hope my edits to my question help both of you understand my question a bit more thank you for the feedback."], "answer": [" ", " ", "You may have a bit more work to do. The problem is that Twist messages are relative, they only include linear and angular velocities. The Compass you are using is provides absolute heading information.", "The correct way to fuse the new absolute heading information with your system will not be the same as the way you were fusing the original relative angular information.", "I recommend looking at the ", " which includes some great documentation on this matter. If you decide to use that package you'll have a very powerful ", " to estimate the position and motion of your robot."], "question_code": ["#include <Wire.h>\n#include <LSM303.h>\n\nLSM303 compass;\n\nvoid setup() {\n  Serial.begin(9600);\n  Wire.begin();\n  compass.init();\n  compass.enableDefault();\n\n  /*\n  Calibration values; the default values of +/-32767 for each axis\n  lead to an assumed magnetometer bias of 0. Use the Calibrate example\n  program to determine appropriate values for your particular unit.\n  */\n  compass.m_min = (LSM303::vector<int16_t>){ -4096,  -4096,  -4096};\n  compass.m_max = (LSM303::vector<int16_t>){ +1497,  +1969,  +1482};\n}\n\nvoid loop() {\n  compass.read();\n\n  /*\n  When given no arguments, the heading() function returns the angular\n  difference in the horizontal plane between a default vector and\n  north, in degrees.\n\n  The default vector is chosen by the library to point along the\n  surface of the PCB, in the direction of the top of the text on the\n  silkscreen. This is the +X axis on the Pololu LSM303D carrier and\n  the -Y axis on the Pololu LSM303DLHC, LSM303DLM, and LSM303DLH\n  carriers.\n\n  To use a different vector as a reference, use the version of heading()\n  that takes a vector argument; for example, use\n\n    compass.heading((LSM303::vector<int>){0, 0, 1});\n\n  to use the +Z axis as a reference.\n  */\n  float heading = compass.heading();\n\n  Serial.println(heading);\n  delay(100);\n}\n"], "url": "https://answers.ros.org/question/289348/twist-with-covariance-stamped-compass-heading/"},
{"title": "Got confused with frame_ids", "time": "2018-01-13 15:42:13 -0600", "post_content": [" ", " ", " ", " ", "I'm fusing GPS, Visual Odometry and IMU. After reading the documentations with some try and error tests, finally I can get a reasonable output.", "I put the frame_id of my Visual Odometry output as odom. I put the frame_id of my IMU as base_link. The GPS data is transformed to odom frame using navsat_transform_node and all three sensors are fused in ekf_node. Because Visual Odometry and GPS both produce x,y,z in state vector, I put differential mode to true for visual odometry as it is less accurate than GPS. I want you to explain me if this configuration is wrong. (I will send my launch file if you need)", "Anyway", "I'm confused about coordinate frames. The first time I read about the package, I though that it can handle multiple sensors in their own coordinate frames. I thought that It includes some sort of self-calibration. But Now, what I get is that the package only can produce /tf message between odom and base_link frames. ", "Is this true? I mean if I want to fuse a sensor in another frame, say odom_b, then do I have to provide /tf from odom_b to odom?", "Next question: I know that visual odometry becomes less and less accurate over time. When I publish its poses, I just put the covariance of current measure and don't account for previous covariances. Does the filter accumulates those covariances it self?", "<launch>", "</node> ", "</launch>", "Hi Tom ...", "Please post your full launch file, and a sample message for every input (e.g., your NavSatFix message, your IMU message, and your visual odometry message).", "Great! Would you mind accepting the answer below? Just click the checkmark."], "answer": [" ", " ", " ", " ", "I put the frame_id of my Visual\n  Odometry output as odom. I put the\n  frame_id of my IMU as base_link. The\n  GPS data is transformed to odom frame\n  using navsat_transform_node and all\n  three sensors are fused in ekf_node.\n  Because Visual Odometry and GPS both\n  produce x,y,z in state vector, I put\n  differential mode to true for visual\n  odometry as it is less accurate than\n  GPS. I want you to explain me if this\n  configuration is wrong.", "It's always ", " helpful to post launch files AND sample input messages (preferably collected when your robot is moving) for questions about r_l. Without them, I can't really comment with any certainty about what your optimal setup is. This leads to two questions:", "Does your visual odometry produce velocity data, i.e., is the ", " section of the message filled out? If it does, then set ", ", ", ", and ", " to ", " in the config file for visual odometry, and set ", ", ", ", and ", " (the velocities for ", ") to ", ".", "If your visual odometry only produces pose data, then yes, set the visual odometry ", " parameter to ", ".", "I'm confused about coordinate frames.\n  The first time I read about the\n  package, I though that it can handle\n  multiple sensors in their own\n  coordinate frames. I thought that It\n  includes some sort of\n  self-calibration. But Now, what I get\n  is that the package only can produce\n  /tf message between odom and base_link\n  frames.", "I would thoroughly read ", " to start. The package ", " handle sensors in their own coordinate frames, certainly, but the package's job is to provide a transform from your ", " to your ", " in your configuration file. In your case, it's producing ", ". That is the ", " transform that ", " ", ".", "However, it still works with data in any coordinate frame, ", " from that coordinate frame to your ", " or ", ". Let's say, for example, that your IMU is mounted upside-down on your robot and is placed on the back end of the robot. This means that your IMU's coordinate frame is ", " ", ", since it's not at your robot's origin. That means ", " must provide, via URDF + ", " or via a ", " ", ", a transform from ", "->", ", and then set the ", " in your IMU messages to ", ". When r_l receives the IMU message, it will see that its ", " is IMU, and will try to transform it to ", ".", "So: if you are fusing ", " data, you need to make sure there's a transform from your sensor message's ", " to your ", ". If you are fusing ", " data, then you need to make sure that there's a transform from your sensor's ", " (for nav_msgs/Odometry messages) or ", "you must provide, via URDF/robot_state_publisher", "shouldn't that be: \"via URDF + robot_state_publisher\"? URDF on its own doesn't do much, and neither does ", ". They're going to need each other :)", "Sorry, yes, I was using the ", " to treat them as a group. Will clarify, TY.", "Can't upvote this more than once, but +100 for analysing these posts with such detail every time ", ".", "Seems sensor fusion is not easy, and ROS new comers and experienced users need quite some help.", "Thanks, ", ". I should really take these questions as an opportunity to improve the wiki. I also wish ROS Answers were easier to search, because I find I often answer similar sorts of questions. I know you can use Google, but I wish that the native search were as powerful.", "re: update wiki: yes, a faq might be an idea? Would save you from some typing perhaps. Although I've experienced that you can't always just point to a faq item unfortunately.", "re: search: ", "."], "answer_details": [" ", "How is your IMU mounted? If it's upside-down or sideways or rotated on its side, then having its frame as ", " is not going to work well.", " ", " ", " ", " "], "question_code": ["<node pkg=\"robot_localization\" type=\"navsat_transform_node\" name=\"navsat_transform_node\" respawn=\"true\">\n\n  <param name=\"magnetic_declination_radians\" value=\"0\"/>\n\n  <param name=\"yaw_offset\" value=\"0\"/>\n  <param name=\"zero_altitude\" value=\"true\"/>\n\n  <remap from=\"/imu/data\" to=\"/imu_with_cov\" />\n  <remap from=\"/gps/fix\" to=\"/fix\" />\n  <remap from=\"/odometry/filtered\" to=\"/odometry/filtered\" />\n\n</node>\n\n<node pkg=\"robot_localization\" type=\"ukf_localization_node\" name=\"ukf_se\" clear_params=\"true\">\n\n<!-- ======== STANDARD PARAMETERS ======== -->\n\n<param name=\"frequency\" value=\"10\"/>\n\n<param name=\"sensor_timeout\" value=\"0.1\"/>\n\n<param name=\"two_d_mode\" value=\"true\"/>\n\n<!-- Defaults to \"odom\" if unspecified -->\n<param name=\"odom_frame\" value=\"odom\"/>\n<!-- Defaults to \"base_link\" if unspecified -->\n<param name=\"base_link_frame\" value=\"imu_link\"/>\n<!-- Defaults to the value of \"odom_frame\" if unspecified -->\n<param name=\"world_frame\" value=\"odom\"/>\n\n<param name=\"transform_time_offset\" value=\"0.0\"/>\n\n<param name=\"odom0\" value=\"/vo\"/>\n<param name=\"odom1\" value=\"/odometry/gps\"/>\n<param name=\"imu0\" value=\"/imu_with_cov\"/>\n\n<rosparam param=\"odom0_config\">[true, true, true,\n                                false, false, false,\n                                false, false, false,\n                                false, false, false,\n                                false, false, false]</rosparam>\n\n<rosparam param=\"odom1_config\">[true, true, true,\n                                false, false, false,\n                                false, false, false,\n                                false, false, false,\n                                false, false, false]</rosparam>\n\n<rosparam param=\"imu0_config\">[false, false, false,\n                               true, true, true,\n                               false, false, false,\n                               false, false, false,\n                               false, false, false]</rosparam>\n\n<param name=\"odom0_differential\" value=\"true\"/>\n<param name=\"odom1_differential\" value=\"false\"/>\n<param name=\"imu0_differential\" value=\"false\"/>\n\n<param name=\"odom0_relative\" value=\"true\"/>\n<param name=\"odom1_relative\" value=\"true\"/>\n<param name=\"imu0_relative\" value=\"true\"/>\n\n<param name=\"print_diagnostics\" value=\"true\"/>\n\n<!-- ======== ADVANCED PARAMETERS ======== -->\n\n<param name=\"odom0_queue_size\" value=\"5\"/>\n<param name=\"odom1_queue_size\" value=\"50\"/>\n<param name=\"imu0_queue_size\" value=\"50\"/>\n\n<param name=\"debug\"           value=\"false\"/>\n\n<param name=\"debug_out_file\"  value=\"debug_ekf_localization.txt\"/>\n", "<node pkg=\"my_mono_vo\" type=\"monocularVO\" name=\"monocularVO\" respawn=\"true\">\n\n</node>\n"], "answer_code": ["Twist", "X", "Y", "Z", "\u1e8a", "\u1e8e", "\u017b", "XYZ", "differential", "base_link", "world_frame", "base_link_frame", "odom->base_link", "r_l", "world_frame", "base_link_frame", "robot_state_publisher", "tf", "static_transform_publisher", "frame_id", "frame_id", "base_link", "frame_id", "world_frame", "child_frame_id", "frame_id ...", "robot_state_publisher", "/"], "url": "https://answers.ros.org/question/279751/got-confused-with-frame_ids/"},
{"title": "robot_localization and navsat_transform_node results", "time": "2017-12-23 13:20:40 -0600", "post_content": [" ", " ", " ", " ", "Hi to all, Hi Tom,", "I'm using ", " and the ", " to obtain the 3D position of my mobile robot. \nI'm using XSens IMU Mti-300 with magnetometer, a SICK laser, Husky A200 and C94-MP8 U-blox GPS with RTK correction.", "The node runs fine and I receive no errors, but when I try to run the bag in ", ", I see that there are lot of jumps and that even the ", " (blue arrows) is not accurate even if the robot is moving on a straight line.", "In this ", " [16MB], the robot is moving on a straight line and as you can see the path is not accurate.", "(The steering maneuver at the end is correct)", "I think that the jumps on ", " (red arrows) and ", " (green arrows) are caused by the GPS (it is in FIXED mode, but may be sometimes it loses the signal with the base station), but I can't explain why the ", " is not accurate.", "Can you give me some suggestions, please?", "it is possible to set a priority on ", " to obtain more accurate results with the GPS signal is not good?", "This is my launch file:", ": I tried to play the ", " without the ", " topic and the path is more accurate even if there are still some strange jumps mostly during steering!", "Please post sample input messages from every sensor.", "Thank you, Tom! I edited my main question by adding the input from the IMU, the GPS and the odometry topic. The samples come from this ", " file [16MB].", "In what frame are you visualizing data in ", "? Remember that the \"local\" odom data is going to get transformed to the map frame by ", " before it gets displayed. This accounts for the jumpiness, but it doesn't account for the fact that they are not perfectly overlaid on each other.", "I'm using the \"map frame\" as \"Fixed Frame\" in RVIZ.", "I uploaded a ", " about the first seconds of the test in RVIZ. It's very strange that the points are not aligned since the robot is only moving straight forward.", "Can you add one sample output message for each? Also, try visualizing with the ", " as odom. You should at least see the odom data no jump around. You're not manually publishing the ", "->", " transform somewhere, are you?", "Oh! You are visualizing the wrong thing for the \"local\" data. You are visualizing the Husky raw odometry. Visualize the ", " data."], "answer": [" ", " ", " ", " ", "You are visualizing the wrong topic for the \"local\" data. Here's what I see when I visualize ", " (green), ", " (red), and ", " (blue):", " ", " == ", ":", " ", " == ", ":", "Both of those look completely correct to me. Your GPS is a bit noisy, but that's normal.", "From the wiki:", "Visualization of raw GPS tracks:", "I figured out why it's so much more jittery. You need to get rid of the ", " value for ", " in your ", " static transform:", "That's a bug; I'll file a ticket. Thanks for the report!", "Even with that fix, though, the direction of travel doesn't quite match the GPS. The robot has a tiny bit of lateral motion, which suggests something is not right with your IMU heading or the declination parameter.", "So you're saying the path is more accurate/in line with the GPS readings when you use the negative declination value (-0.0684)? If so, yes, that makes sense, and it's what I suggested in the comments. Your deviation from \"true north\" is 0.0684 radians ", ":", "Clockwise rotation in an ENU frame is negative, hence the -0.0684 declination value. I realize ", " uses different standards than geographic services, but it's just so that the nodes handle rotations consistently.", "Thank you for your answer! I can't understand why the data are correct. I drive the robot along a straight line and so I expect to see a straight line like it happens when you set the ", " as ", ". When I set the map as the fixed frame, I can't understand why the points drift on the left", "Both local and global coordinates are coincident, but they drift and jump randomly. Why do you consider these data as correct? Shouldn't I see a straight line like it happens in the ", " frame?", " No, they are correct. The blue arrows are your GPS readings, and the filter is just updating itself accordingly. If you plot your raw GPS data using a tool like  ", "  or the mapviz package, you will see that the raw GPS data jumps around in the exact manner you see. ", "Having said that, the ", " output is a lot noisier than I'd expect. I'll look into it.", "Yes, the GPS is not very accurate, but it does not have all the jumps as showed in RVIZ by the navsat_transform_node. I can't explain why since I was expecting to see only few jumps when the GPS module loses the fixed position.", " How was you able to draw the GPS path so fast? When I use  ", " , I always need to manually copy only few GPS data in a TXT file and then upload it on the website. Is there any faster way to do that? ", ", then edit the column headers for latitude and longitude to get rid of the ", " bit, then upload. Really, we should use something like ", " instead.", "But I concede that the output of ", " is not very good. I'll look into it."], "question_code": ["RVIZ", "/odometry/filtered/local", "/odometry/gps", "/odometry/filtered/global", "/odometry/filtered/local", "/odometry/filtered/local", "<!-- Test launch file for two EKF instances and one navsat_transform instance -->\n\n<launch>\n\n <rosparam command=\"load\" file=\"$(find husky_control)/config/control.yaml\" />\n\n  <node name=\"base_controller_spawner\" pkg=\"controller_manager\" type=\"spawner\" args=\"husky_joint_publisher husky_velocity_controller --shutdown-timeout 3\"/>\n\n\n\n <node pkg=\"rosbag\" type=\"play\" name=\"rosbag_play\" output=\"screen\" args=\"--clock /home/rocco/Desktop/gps/vine.bag -k -d 3\"/>      \n\n\n  <node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_imu\" args=\"0 0 0 0 0 0 base_link base_imu\" />\n  <node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_gps\" args=\"0 0 0.4 0 0 0 base_link gps\" /> \n  <node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_laser\" args=\"0 0 0.4 0 0 0 base_link laser\" /> \n  <node pkg=\"tf2_ros\" type=\"static_transform_publisher\"  name=\"base_to_realsense\" args=\"0 0 0 -1.5707963 0 -1.5707963 base_link realsense_frame\" />\n\n    <!-- Local (odom) instance -->\n    <node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization_local\" clear_params=\"true\">\n      <param name=\"frequency\" value=\"10\"/>\n      <param name=\"sensor_timeout\" value=\"0.1\"/>\n      <param name=\"two_d_mode\" value=\"false\"/>\n\n      <param name=\"map_frame\" value=\"map\"/>\n      <param name=\"odom_frame\" value=\"odom\"/>\n      <param name=\"base_link_frame\" value=\"base_link\"/>\n      <param name=\"world_frame\" value=\"odom\"/>\n\n      <param name=\"transform_time_offset\" value=\"0.0\"/>\n\n      <param name=\"odom0\" value=\"/husky_velocity_controller/odom\"/>\n      <param name=\"imu0\" value=\"/imu/data\"/>\n\n      <rosparam param=\"odom0_config\">[false, false, false,\n                                      false, false, false,\n                                      true, true, true,\n                                      false, false, true,\n                                      false, false, false]</rosparam>\n\n      <rosparam param=\"imu0_config\">[false, false, false,\n                                     true,  true,  true,\n                                     false, false, false,\n                                     true,  true,  true,\n                                     true,  true,  true]</rosparam>\n\n      <param name=\"odom0_differential\" value=\"false\"/>\n      <param name=\"imu0_differential\" value=\"false\"/>\n\n      <param name=\"odom0_relative\" value=\"false\"/>\n      <param name=\"imu0_relative\" value=\"false\"/>\n\n      <param name=\"imu0_remove_gravitational_acceleration\" value=\"true\"/>\n      <param name=\"print_diagnostics\" value=\"true\"/>\n\n      <param name=\"odom0_queue_size\" value=\"10\"/>\n      <param name=\"imu0_queue_size\" value=\"10\"/>\n\n      <param name=\"debug\"           value=\"false\"/>\n      <param name=\"debug_out_file\"  value=\"debug_ekf_localization.txt\"/>\n\n      <remap from=\"/odometry/filtered\" to=\"/odometry/filtered/local\"/>\n    </node>\n\n    <!-- Global (map) instance -->\n    <node pkg=\"robot_localization\" type=\"ekf_localization_node ...", "/ublox_gps/fix", "rviz", "rviz", "fixed_frame", "/odometry/filtered/local"], "answer_code": ["/odometry/filtered/local", "/odometry/filtered/global", "/odometry/gps", "rviz", "fixed_frame", "rviz", "fixed_frame", "0.4", "Z", "bl_gps", " <node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_gps\" args=\"0 0 0.0 0 0 0 base_link gps\" />\n", "r_l", "odom", "fixed frame", "odom", "navsat_transform_node", "rostopic echo gps_topic -p > out.txt", "field.", "mapviz", "navsat_transform_node"], "url": "https://answers.ros.org/question/278118/robot_localization-and-navsat_transform_node-results/"},
{"title": "How can I reduce drift in my robot's trajectory?", "time": "2016-11-02 23:07:14 -0600", "post_content": [" ", " ", " ", " ", "My robot used to travel in straight lines between point A and point B. Now it doesn\u2019t. I want to figure out why and how I can make it do that again by forcing the local path planner to follow the global path as closely as possible. I've done my best to summarize the problem here, but if you don't want to read an essay skip to the solution to this ROS question where I've re-stated the problem more clearly, and the work-around.", "If a robot travels directly between two points and there are no obstacles in the way, then its trajectory between those points should be a perfectly straight line (in theory). My robot used to behave like this, as shown below:", "And now it doesn't, it consistently follows a bowed/curved trajectory every time. The radius of the curve changes but otherwise the trajectory is identical. The final stretch back to the starting point also has a funny kink it. Example:", "Now before I go any further, I do understand that this problem is largely aesthetic and doesn't matter. And I know that I could work around this by adding more goal points and breaking the longer lines down into a series of shorter ones. However the fact remains that I can't currently explain this behaviour and I'd like to solve the problem if only for my own education. ", "So I would like to correct this behaviour and answer a few questions", "Here is all the relevant system information:", " package used is ", ". There are two instances of ekf_localization_node (local and global). IMU and encoder data is fused in the (local) /odom frame. IMU, encoder and GPS data is fused in the (global) /map frame - this is the frame /move_base uses for navigation. I can provide the specific launch files if requested.", "The output from the global EKF is always accurate and doesn't drift. The local output accumulates error over time and tends to drift quite badly by the end of a run.", " I use the ", " package (version 2.2.2) to set up my navigation stack. I use the ", " with the following parameter changes to ", " and ", ": ", "To the best of my knowledge, these are the only parameters that have changed between the two graphs shown above.", "The full navigation parameter files are ", "Currently, in all cases the global path is perfectly straight ...", "Even though I cannot help you here, I'd like to give a big ", " for this nice problem description. IMO, the average question lately seems to be: \"My robot is not working!!! What can I do?!?!?!\"", "So it is nice to read such a good problem description for a change! Thank you!", "Its hard to say what your problem is, it could be so many different things. But the behavior your seeing can be caused by a constant force that the PID controller has to deal with, (like one of the motors being bad). You could put the robot up on blocks and test that all the wheels are still driven.", "That's why its taking so long to debug. I've checked that all the wheels work, and they seem fine. But I think I've discovered what could be creating that constant force you mentioned - have a look at Update 1.", "Most of the position estimators that I've worked with use the linear and angular velocity from odometry to propagate the previous position estimate to the new position estimate (and so the absolute heading from odometry is completely ignored)", "So that's probably why the absolute odometry heading doesn't appear to be adversely affecting your EKF.", "From the data, it looks like your robot drives slightly to the right when commanded to go straight. Maybe you need to re-tune the PID controllers now that the motors have worn in a bit?", "You say is used to work and now it doesn't? Do you know what has changed between then and now?", "Thanks very much for your help! I've addressed your comments in \"Update 3\""], "answer": [" ", " ", " ", " ", "In the parameters for the ", " node, set: ", "Setting ", " in the parameters for ", " also helps as it makes the robot turn corners tighter.", "I haven't found the real cause of the problem, I haven't figured out what changed between when I started and now. What I have done is found a work-around. I'll try to re-state the key issue and the solution as best as I can because the original question above is too big and horrible to read now. ", "Currently, the ", " node has a parameter called ", ", this dictates the rate at which the global planner will re-plot the global path. The ROS default is 0.0, but the default in the Clearpath files that I'm using is 20 Hz (", "). With reference to this heavily exaggerated illustration, this is essentially the problem:", "In travelling from A to B, the robot is obviously experiencing some drift (for whatever reason), so its just not travelling straight. But when it gets off the original path (bold green), rather than getting back on it, the global path is simply re-plotted (skinny green) starting from where the robot ", " is. Eventually the robot will get close enough to the goal that the local planner and controller will overrule the source of the drift. Hence the robot follows the dashed, light blue path from A to B instead of following a straight line between both points. ", "The simple solution is to set ", ", that way \"the global planner will only run when a new goal is received or the local planner reports that its path is blocked\" This means that the global path never changes between goals, so the local planner rarely deviates from it. This means that instead of this:", " ", "You get this:", "Not perfect, but a hell of a lot better and good enough for now. In addition, the local path planner can be a little too generous with its turning arcs, so I suggest setting ", " in the parameters for ", " (assuming that is the local planner you are using of course). This forces the planner to score trajectories based on angles rather than distance, and overall gives you tighter turns. ", "For reference, on a Clearpath Jackal, the ", " parameter is found at the following filepath: ", "If I happen to stumble across the real cause of the drift, or if I figure out anything else that improves the trajectory, I'll update this again. Thanks again to everyone who helped.", "I have similar sensors as your robot, I have a question. when you do the IMU and odom fusion, did your IMU has a yaw feedback ie. north or south? ", "Not sure what you mean sorry. My IMU reports a yaw from its magnetometer. Yaw reads 0 when facing ", " and increases ", ". No this doesn't adhere to REP103 but navsat_transform deals with it using the yaw_offset parameter. Everything else comes out in the wash."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " Ubuntu 14.04 (laptop), ROS Indigo Igloo (robot)", " Clearpath Robotics Jackal UGV (real, not simulated)", " IMU, wheel encoders, high accuracy RTK GPS ", " ", " ", "xy_goal_tolerance increased to 0.35 m", "heading_scoring = true (false by default)", "Costmap size increased to 120x120 m", "All instances of any \"global_frame\" parameter have been changed to /map.", "All map (local and global) \"update_frequency\" parameters reduced from 20 Hz to 10 Hz", "Reduced \"controller_frequency\" and \"planner_frequency\" parameters from 20 HZ to 10 Hz", " ", " ", " "], "answer_code": ["planner_frequency = 0.0", "heading_scoring = true", "planner_frequency", "planner_frequency = 0.0", "heading_scoring = true", "planner_frequency", "/opt/ros/indigo/share/jackal_nagivation/params/move_base_params.yaml"], "url": "https://answers.ros.org/question/247116/how-can-i-reduce-drift-in-my-robots-trajectory/"},
{"title": "How to add ROS to path in VS Code?", "time": "2017-03-09 01:56:18 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "Is anyone using VS Code to program in ROS? I kind of like this IDE ( haven't used it earlier ) and want to use it as it comes with a lot of features which I like. ", "My question is when I ", " it puts a green squiggle below it, and also for service file generated headers it can't really locate them. Any suggestions on how to add on to the path to avoid this.", "To be noted, there is no problem in building it as it is already in the path of Linux terminal...", "There isn't a section for VS Code on the ", ", but the instructions for one of the other IDEs might provide some hints."], "answer": [" ", " ", "The ", " now includes an entry for VSCode, which points to a ROS extension. Among other things, this extension will add the appropriate include paths. I tried this and it works nicely.", " ", " ", "In addition to clyde answer:", "1- Open VSCode and (Ctrl+Shift+X) and search for ROS ", "2- Install ( ROS VSCode Extension )", "3- Restart VSCode ", "4- Make sure to open your workspace ex: file ->openfolder ->catkin_ws", "5- The extension will pop up a msg to configure your ROS distro if it cants configure it automatically ", "6- Also you can use the terminal from VSCode (Ctrl+~) as usual ubuntu terminal ", "7- Build your catkin workspace at least once from this terminal", "8- You can ", " in your files and you shouldn't see green squiggle below \n    if everything is configured properly.", "9- At the left bottom of VSCode you will find [x ROS Master] that's mean ROS core is not running \ntry to run $ roscore from the terminal in VSCode and notice [\u2714 ROS Master]", "I am trying with simple talker.cpp program. I could not find a way to RUN it. I have followed all your steps.\nWhen I debug  I get this following error: \nUnable to start debugging. Program path '/home/catkin_ws/src/beginner_tutorials/scripts/talker.cpp' is missing or invalid.", "GDB failed with message: \"/home/catkin_ws/src/beginner_tutorials/scripts/talker.cpp\": not in executable format: File format not recognized", "This may occur if the process's executable was changed after the process was started, such as when installing an update. Try re-launching the application or restarting the machine.", "Can you give me some suggestions please..."], "question_code": ["include <ros/ros.h>"], "answer_code": ["include <ros/ros.h>"], "url": "https://answers.ros.org/question/256565/how-to-add-ros-to-path-in-vs-code/"},
{"title": "Anybody tried Intel\u00ae Euclid\u2122 Development Kit", "time": "2017-08-15 00:48:15 -0600", "post_content": [" ", " ", "I am a software engineer working mainly on robotic systems. I am wondering if anybody has tried the Intel 3D depth camera for robotic platform? How about its performance? From its description by Intel, it seems quite powerful and compact, I am considering to try this."], "answer": [" ", " ", " ", " ", "Hi,", "I received my Euclid this week and I'm trying to understand the Intel Euclid aproach now. Seems to be easy to extent some basic behaviors for students.", "I've used an Intel NUC i5 (16 GB Memory, 256 GB SSD disk) with R200 before and I will give Euclid a first try now.", "I did not expect too much, but simple SLAM with my Roomba should work. This Eucid box is really comfortable and easy to handle. I expect for small projects, it's enough power and better than using a Raspi and a lot of additional own build modules for IMU, CAM and other IO modules.", "Cheers", "Chrimo", "Please keep us updated. I have looked at it but it's a lot of money for a hobbyist. Is it stereo camera or projector and camera like kinect? It's not obvious on the website but they say it can be used outdoors.", "I bet, this cooperation will give ros euclid users more informations for missing hw documentation and api.", "Today, I'm tried the Euclid outdoors and I'm not very happy with the depth image...\nSmall trees or plants are not detected or shown.\nInside it is OK.\nI need to check it without direct sun later.", "After 2 month of working with Euclid, I must say:\nSoftware and SDK are OK and have nice UI, but the hardware is unusable for real robots.\nPoor accu and charging capabilities, instable overheated board design and very poor ros support for new integrated sensors.", "Did it come with any 'promises' of 'ROS support' for the sensors?", "no, still waiting for feedback... coming soon :-)"], "url": "https://answers.ros.org/question/268825/anybody-tried-intelr-euclidtm-development-kit/"},
{"title": "Test kinect:No devices connected.... waiting for devices to be connected", "time": "2016-05-27 23:18:48 -0600", "post_content": [" ", " ", " ", " ", "My environment is Ubuntu12.04, ros indigo. \"http://learn.turtlebot.com\" was followed by me, but when i run", "the logs of terminal show ", "It hits me with long time, help me!"], "answer": [" ", " ", " ", " ", " is deprecated, you should use freenect_stack instead from ", "And make sure the Kinect is connected, and if you are using Kinect V2, it should be connected to Intel USB3.0 port.", "Once you install it, you need to start publishing the kinect camera topics by running the freenect_launch file:", "Then open another shell to visualize the rgb image or any other topic:", "Yes and if that doesn't work then make sure the Kinect is getting 12v of power. Pretty obvious but it happens to the best of us hehe", "I follow your approach, but the problem still appear.Could you give me a way which include all step for connect kinect2?", "The kinect seems working!\nI just edited my answer, check it out", "Is this with Kinect v2?", "Should works for both.\nfreenect_launch seems already working!", "Should? Have you tested with both?", ", Yes, freenect_launch already working. But no devices connected.... waiting for devices to be connected. I'm sure that i connect Kinect2 with my computer, typing lsusb, 3 Microsoft Corp had arised.", " ", " ", " ", " ", ",The same problem still appears in my terminal. There is messages in detial.\nWhen i type lsusb into the terminal, the messages shows following:", "then i tried:", "Looks like it might be there although it is strange that they do not say camera or audio after Microsoft Corp. I have not used the Kinect 2. It could be another device (like a mouse). ", "Are you sure freenect is not publishing any topics? Type $ rostopic list (after running freenect)", "After typing rostopic list do you see any topics under /camera? There should be a whole bunch of them (ie /camera/depth_registered/points)", "When I run \"roslaunch freenect_launch freenect.launch depth_registration:=true\", the error, [ INFO] [1464504903.449033967]: No devices connected.... waiting for devices to be connected, appears again.After typing rostopic list , i see truly many topics under /camera.", "While i typing \"rosrun image_view image_view image:=/camera/pth/image\", nothing is there  showing. Many topic of /camera has been tried,nothing appears.", "This may be an issue with Kinect v2. I have not used it before so I can't help beyond this. I would start by making sure that freenect actually works with Kinect v2. Check if the lib supports it.", "If you have a Kinect v1 try it with that and see if it works.", "I don't have kinect v1. -_-    Using libfreenect2 and iai_kinect2, I can get some data from kinect2. But some program does not perfectly run.", "What do you mean by not running perfect? If you have not calibrated your Kinect you may get some warnings but it should still work. Are you able to see point cloud in rviz?", " ", " ", " ", " ", "What do you see when you type lsusb into the terminal? You should see 3 Microsoft Corp devices in the list (Kinect v1):", "Microsoft Corp. Xbox NUI Camera", "Microsoft Corp. Xbox NUI Motor", "Microsoft Corp. Xbox NUI Audio", "If it is there then try:\n$ roslaunch freenect_launch freenect.launch depth_registration:=true", "Not sure with Kinect v2. But have a look here: ", " I only have Microsoft Corp. Xbox NUI Motor listed for Kinect XBOX 360. I searched all over web for solution, unsuccessfully. Could you maybe provide some insight? When I plug it in other PCs I have all 3 devices listed. Could it be that something  is conflicting with it on Ubuntu 16.04? ", "\nTnx", "It may be a problem of the power, when I do not have connected to the power I only see \"Motor\"."], "question_code": ["roslaunch openni_launch openni.launch\n", "[ INFO] [1463638647.225005146]: No devices connected.... waiting for devices to be connected\n"], "answer_code": ["openni_kinect", "sudo apt-get install ros-indigo-freenect-stack\n", "roslaunch freenect_launch freenect.launch\n", "rosrun image_view image_view image:=/camera/rgb/image_color\n", "$ lsusb\nBus 002 Device 002: ID 8087:8000 Intel Corp. \nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 002: ID 8087:8008 Intel Corp. \nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 003: ID 045e:02c4 Microsoft Corp. \nBus 004 Device 002: ID 045e:02d9 Microsoft Corp. \nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 003 Device 004: ID 046d:c52f Logitech, Inc. Unifying Receiver\nBus 003 Device 007: ID 04d9:1503 Holtek Semiconductor, Inc. Shortboard Lefty\nBus 003 Device 002: ID 045e:02d9 Microsoft Corp. \nBus 003 Device 008: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n", "$  roslaunch freenect_launch freenect.launch depth_registration:=true\n ... logging to /home/turtlebot/.ros/log/06249706-2549-11e6-a74b-fcaa14dfa0b1/roslaunch-turtlebot-M4HM87P-00-17646.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://localhost:54035/\n\nSUMMARY\n========\n\nPARAMETERS\n* /camera/camera_nodelet_manager/num_worker_threads: 4\n* /camera/depth_rectify_depth/interpolation: 0\n* /camera/depth_registered_rectify_depth/interpolation: 0\n* /camera/disparity_depth/max_range: 4.0\n* /camera/disparity_depth/min_range: 0.5\n* /camera/disparity_registered_hw/max_range: 4.0\n* /camera/disparity_registered_hw/min_range: 0.5\n* /camera/disparity_registered_sw/max_range: 4.0\n* /camera/disparity_registered_sw/min_range: 0.5\n* /camera/driver/data_skip: 0\n* /camera/driver/debug: False\n* /camera/driver/depth_camera_info_url: \n* /camera/driver/depth_frame_id: camera_depth_opti...\n* /camera/driver/depth_registration: True\n* /camera/driver/device_id: #1\n* /camera/driver/diagnostics_max_frequency: 30.0\n* /camera/driver/diagnostics_min_frequency: 30.0\n* /camera/driver/diagnostics_tolerance: 0.05\n* /camera/driver/diagnostics_window_time: 5.0\n* /camera/driver/enable_depth_diagnostics: False\n* /camera/driver/enable_ir_diagnostics: False\n* /camera/driver/enable_rgb_diagnostics: False\n* /camera/driver/rgb_camera_info_url: \n* /camera/driver/rgb_frame_id: camera_rgb_optica...\n* /rosdistro: indigo\n* /rosversion: 1.11.10\n\nNODES\n  /camera/\n    camera_nodelet_manager (nodelet/nodelet)\n    debayer (nodelet/nodelet)\n    depth_metric (nodelet/nodelet)\n    depth_metric_rect (nodelet/nodelet)\n    depth_points (nodelet/nodelet)\n    depth_rectify_depth (nodelet/nodelet)\n    depth_registered_rectify_depth (nodelet/nodelet)\n    disparity_depth (nodelet/nodelet)\n    disparity_registered_hw (nodelet/nodelet)\n    disparity_registered_sw (nodelet/nodelet)\n    driver (nodelet/nodelet)\n    points_xyzrgb_hw_registered (nodelet/nodelet)\n    points_xyzrgb_sw_registered (nodelet/nodelet)\n    rectify_color (nodelet/nodelet)\n    rectify_ir (nodelet/nodelet)\n    rectify_mono (nodelet/nodelet)\n    register_depth_rgb (nodelet/nodelet)\n  /\n    camera_base_link (tf/static_transform_publisher)\n    camera_base_link1 (tf/static_transform_publisher)\n    camera_base_link2 (tf/static_transform_publisher)\n    camera_base_link3 (tf/static_transform_publisher)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[camera/camera_nodelet_manager-1]: started with pid [17664]\n[ INFO] [1464491179.502564627]: Initializing nodelet with 4 worker threads.\nprocess[camera/driver-2]: started with pid [17685]\nprocess[camera/debayer-3]: started with pid [17701]\nprocess[camera/rectify_mono-4]: started with pid [17718]\nprocess[camera/rectify_color-5]: started with pid [17732]\nprocess[camera/rectify_ir-6]: started with pid [17754]\nprocess[camera/depth_rectify_depth-7]: started with pid [17780]\nprocess[camera/depth_metric_rect-8]: started with pid [17800]\n[ INFO] [1464491179.859241746]: No devices connected.... waiting for devices to be connected\nprocess[camera/depth_metric-9]: started with pid [17834]\nprocess[camera/depth_points-10]: started with pid [17922]\nprocess[camera/register_depth_rgb-11]: started with pid [17951]\nprocess[camera/points_xyzrgb_sw_registered-12]: started with pid [17980]\nprocess ..."], "url": "https://answers.ros.org/question/235440/test-kinectno-devices-connected-waiting-for-devices-to-be-connected/"},
{"title": "Navigation Stack : Speed of Robot", "time": "2017-01-23 14:40:27 -0600", "post_content": [" ", " ", "So finally got things working, again people on here are awesome and I am actually now reading a book on ROS to fill in the gaps.", " I used code from  ", " Parameters on the Robot seem to be in\n ", " ", "My question is, does either of these files control the turning speed of the robot as it is struggling to turn on carpet?"], "answer": [" ", " ", "Yes, the file ", " will control the turning speed (as well as the longitudinal speed). Specifically, the parameters: ", ", ", ", and ", ". These are standard parameters for the base_local_planner package. You will find their definitions ", ".", "The second file you reference contains parameters for the costmap, which you can read more about ", ".", "However, the maximum velocity setting may not be you problem. When turning on carpet you could be power limited, or also be experiencing excessive slip, which will not only lengthen the turning time, but also add time just for the localization to realize the robot has not turned sufficiently far.", "Thanks for the help, thats awesome!", "When you say power limited are you saying the motors are not powerful enough?", "It is possible. But without knowing anything about your robot I can't say for sure, I just though I would mention it since you emphasized the behaviour on carpet. A robot with small wheels, weak motors, and on plush carpet, could have difficulty moving.", "Thank you, could I ask if max_vel_theta = 1 does that mean 100% power?", "No, that means 1 radians/sec. As per the definition of max_vel_theta on the base_local_planner link I provided: \"The maximum rotational velocity allowed for the base in radians/sec\".", "Oh sorry I am pretty new to this, Now I just need to work out the actual value :)", "No worries. Just always try to look at the documentation for whatever you are doing. Most information you need is usually there."], "answer_code": ["base_local_planner_params.yaml", "max_vel_theta", "min_vel_theta", "min_in_place_vel_theta"], "url": "https://answers.ros.org/question/252799/navigation-stack-speed-of-robot/"},
{"title": "I need help writing a controller node for a drone", "time": "2017-01-12 04:12:47 -0600", "post_content": [" ", " ", " ", " ", " I tried searching alot, and couldnt find any good tutorials for learning how to write a controller node for a drone, (though this was informative  ", " ) ", "I want to write a controller node for autonomous drone movement, as defined in a cpp file,", "ie: set yaw=value for a small time, then pitch=value (to move forward), then throttle=value to move up and so forth."], "answer": [" ", " ", " ", " ", "I would advise you to look into the Ardunio Pilot open source project, what you're trying to achieve is incredibly difficult. People have put tens of thousands of hours into developing UAV autopilots and there are some very good ones out there.", "Get one of these systems working for you, and become familiar with how they operate and then decide if it's really something you want to try and build from scratch.", "I'm not trying to put you off at all, I've worked on a few really fascinating projects with autonomous drones. It's a lot of fun, but you don't have to re-invent such a complex wheel yourself.", "Ah okay, now I see what you're trying to achieve. You want to build a ros node that can link to a UAV using the MAVLINK protocol to then control what that UAV does. The link you posted should be a good place to start.", "But what exactly are you trying to make the UAV do? The MAVLINK protocol is quick high level, you don't have any control over the UAV's pitch, that is entirely under control of the autopilot. You can however tell the UAV to fly to a particular waypoint using a MAV_CMD_NAV_FOLLOW command, this position can then be updated to make drone move around.", "Think about it like this, using the MAVLINK you can be the navigator of the drone, and tell it where to be and where to go. But the on-board outpilot will always be the driver, and will control the pitch, yaw and throttle of the drone. These use complex control algorithms and estimates of the current position from multiple sensors in order to safely fly the drone. Most autopilots will also ignore any mavlink commands if the battery is a dangerously low and return to their landing site and land safely for example. If you can tell me a bit more about your project I'll see if I can get you started.", " sorry, i just want to do something similar to  ", "  using a controller, using existing controller is also is fine, it just needs to be autonomous, problem is i cant find tutorials specific to a quad anywhere ", "I also want to write my own controller for quadrotor so it would be great if you suggest some tutorials and and some material to start with, basically i want to know that how roll, pitch, yaw etc. are set and  what values should be given so that it can maintain its balance and what algorithm we can use to get these values and from where we can find these type algorithm to set the motor values."], "url": "https://answers.ros.org/question/251805/i-need-help-writing-a-controller-node-for-a-drone/"},
{"title": "baxter sim_kinematics dies (both left and right) with error -11", "time": "2016-10-27 11:52:23 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am running baxter simulation via gazebo using their simulator. But I get following errors while doing a roslaunch.", "error 1:", "A solution to the above problem was found in this ", " ", "error 2:", "Strangeness of the error:", "We have two PC's having same hardware, same OS (Ubuntu 14.04) and ROS(indigo) and same baxter files (same git versions). In one PC is works perfectly fine, while the other it does not. Another problem is, it doesn't write any log file as shown in the error description. It only writes logs of those files that was successfully loaded.", "Could any one please suggest anything or give a guidance on how to debug it?", "Are you sure both PCs are running the same software (versions)? Inexplicable ", "s (that's what ", " errors are) can be caused by ABI incompatibilities. Perhaps one of the PCs is running newer/older versions of relevant packages.", "the files in my PC was copied from the files from the working PC.", "That doesn't guarantee it'll work.It could actually be the cause.", "And \"the files\": which files are you referring to here: some parts of ROS proper, baxter sim pkgs, both, or something else?", "i mean all source files in the catkin_ws/src folder that relates to baxter simulator. I re-run cakin_make in mine, even I tried to directly downloading from the git again and recompiling it. Both didn't prove any good.", "Could be that there is actually just really something wrong with the code itself. I recommend reporting that to Rethink on the appropriate Baxter issue trackers.", "Do please report back if you get this resolved."], "answer": [" ", " ", "it had something to do with the urdf parser files. i reintalled all of those related ros packeages, and it started working. !!!", "Please just accept your own answer, as you really answered the question.", "We don't really close questions here on ROS Answers. The 'green checkmark' is much more of a sign that a question is answered."], "question_code": ["[baxter_emulator-8] process has died [pid 8399, exit code -11, cmd /home/mjm/catkin_workspaces/baxter_ws/devel/lib/baxter_sim_hardware/baxter_emulator /home/mjm/catkin_workspaces/baxter_ws/src/src/baxter_simulator/baxter_sim_hardware/images/researchsdk.png __name:=baxter_emulator __log:=/home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_emulator-8.log].\nlog file: /home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_emulator-8*.log\n", " [baxter_sim_kinematics_left-6] process has died [pid 8381, exit code -11, cmd /home/mjm/catkin_workspaces/baxter_ws/devel/lib/baxter_sim_kinematics/kinematics left __name:=baxter_sim_kinematics_left __log:=/home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_sim_kinematics_left-6.log].\n    log file: /home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_sim_kinematics_left-6*.log\n    [baxter_sim_kinematics_right-7] process has died [pid 8391, exit code -11, cmd /home/mjm/catkin_workspaces/baxter_ws/devel/lib/baxter_sim_kinematics/kinematics right __name:=baxter_sim_kinematics_right __log:=/home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_sim_kinematics_right-7.log].\n    log file: /home/mjm/.ros/log/edd5c38c-9c62-11e6-9b5e-14109fd8a0ab/baxter_sim_kinematics_right-7*.log\n", "SEGFAULT", "-11"], "url": "https://answers.ros.org/question/246605/baxter-sim_kinematics-dies-both-left-and-right-with-error-11/"},
{"title": "How can I power a kinect using the iRobot Create 2 base? (Turtlebot)", "time": "2016-06-06 16:03:17 -0600", "post_content": [" ", " ", "Hey everyone!", "I'm creating a turtlebot from scratch. I have an iRobot Create 2, a kinect sensor (and all its cables), and an acer netbook.", "How do I provide 12V power to the kinect using the create 2? I saw a lot of tutorials online and they say to use the serial port on the create 2 base. However, if I do this, then how do I connect the create 2 to my netbook?", "Currently I am using the serial to USB cable to connect the irobot to my netbook. Is there a way to splice this cable and use it for powering the kinect?", "Or will I need an external 12V battery?", "Thanks! I'm relatively new to electronics, so any help is appreciated!"], "answer": [" ", " ", "iRobot has published instructions about getting power from the Create 2's internal battery ", ". The Kinect can be powered by the main brush motor driver, you just have to regulate it down to 12V. They recommend using an inductor but I did not end up needing one. You just have to send power to the main brush motor when you start up your turtlebot, which is opcode 144 of the Create 2 open interface.", " ", " ", "I have recently successfully spliced the USB cable shipped with the Create. You may have come across ", ". I believe there are only 4 pins used to communicate with the robot and the remaining 3 unused pins include an unregulated power source and ground. Unfortunately, upon inspecting the cable these 3 pins were not actually connected to the insulated wires inside the cable. I ended up just salvaging the USB end of the cable and soldering my own 7-pin connector at the other end. I connected a voltage regulator to a pair of the unused power and ground wires and was able to power a RaspberryPi no problem :)", "I only required 5V, but if I recall correctly the unregulated power line puts out about 14V. In any case you'll probably need a regulator.", "This sounds like a good idea. Why did you solder your own 7 pin connector if the cable shipped with the create already has one?", "Also, do you recommend a voltage regulator with a switch? I don't want my kinect eating away my robot's battery.", "I found the existing 7-pin difficult to salvage because of it's enclosure and I had another one lying around anyways. I also used a switch, which is really handy if you want cut the power without actually disconnecting the cable."], "url": "https://answers.ros.org/question/236239/how-can-i-power-a-kinect-using-the-irobot-create-2-base-turtlebot/"},
{"title": "Ar_pose doesn't recognize tag in tum_simulator. Please help.", "time": "2014-11-29 15:24:42 -0600", "post_content": [" ", " ", " ", " ", "Hello, ", "I would like to use ar_pose with image from ardrone camera. I don't have real quadrotor, I am using tum_simulator with Gazebo.", "My problem is that ar_pose doesn't recognize tags in simulation, so I have few questions:", "screens: ", "Gazebo and ar_tag which I add ", "I have add two lines to launch file which is responsible for running Gazebo + tum_simulator + ardrone_autonomy ", " so when I launch this files I have above topics remapped: \n ", "next I launch ar_pose: ", "RESULTS:\nscreen from rviz (simulation) ", "(screen1) ", "Problem is that there is warming NO IMAGE RECEIVED and it is true. Preview from camera using 'CAMERA' gives black window, preview should be done with 'IMAGE'. \nAR_POSE uses this 'camera style' preview, no 'image style'. \nHow to solve this problem? \n (screen2) ", "\n (screen3) ", "EDIT:", "it started working! \nI change fixed frame to ar_marker and it seems to be working! :) ", "but it gives me message in terminal:", "(screen4) ", "Cze\u015b\u0107, u\u017cywasz ar_sys czy ar_pose ?", "Cze\u015b\u0107 :) \nwola\u0142bym pisa\u0107 po ang poniewa\u017c jest wi\u0119ksze prawdop. \u017ce kto\u015b jeszcze mo\u017ce mi pom\u00f3c. ", " I am using ar_pose from this localisation:  ", " \ndo you have any idea? ", "nie ma sprawy. And i tried to use ar_pose but it was a little difficult to setup and it seems as if maintenance hasnt been done in a while . Not sure what changes where made on the fork you are using though. Have you looked at ", "? Just started to set it up myself", "I haven't heard about this package before. You are using real camera or camera in gazebo simulator? Everything works fine?", " Some time ago I found this repository:  ", "I have calibrated ar_pose to my cam in laptop so I would rather don't change repository."], "answer": [" ", " ", "I believe you've hit on the problem in #2: there needs to be a white border around the marker in order for it to be detected.", "Agreed. This would cause a issue", "I have added white borders to this tag, changed way of remapping and changed fixed frame. \nIt works, I am a little bit surprised. Could you look at the error which I pasted at the bottom of the post?", " ", " ", "Hi green96, I'm trying to do the same as you. Can you tell me how you include a marker in gazebo? Thanks!", " ", " ", "I'm also working with Arucos I'm using the ar_sys package.", "In simulation when you use gazebo you don't need to calibrate the camera, because all the camera matrix goes in the camera_info messages. But if you work with a real camera you will have to calibrate the camera.", "You will have to define the size of the arucos, if you don't tell the size the program will detect the arucos but it wil give the wrong results. Like the real distance is 5meters it will tell you some other value."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Should I do camera calibration? (I don't have real cam, I am using cam which is in tum_simulator)", "What parameters should be set in ar_pose like: marker marker_width? (I am using tags in simulation)", "Why ar_pose doesn't recognize this tag? "], "question_code": ["<remap from=\"/ardrone/camera_info\" to=\"/camera/camera_info\"/>\n<remap from=\"/ardrone/image_raw\" to=\"/camera/image_raw\"/>\n", "<launch>\n\n  <node pkg=\"rviz\" type=\"rviz\" name=\"rviz\" \n    args=\"-d $(find ar_pose)/launch/live_single.rviz\"/>\n  <node pkg=\"tf\" type=\"static_transform_publisher\" name=\"world_to_cam\" \n    args=\"0 0 0.5 -1.57 0 -1.57 world camera 10\" />\n\n\n  <node name=\"ar_pose\" pkg=\"ar_pose\" type=\"ar_single\" respawn=\"false\"\n    output=\"screen\">\n    <param name=\"marker_pattern\" type=\"string\"\n      value=\"$(find ar_pose)/data/4x4/4x4_19.patt\"/>\n    <param name=\"marker_width\" type=\"double\" value=\"200\"/>\n    <param name=\"marker_center_x\" type=\"double\" value=\"0.0\"/>\n    <param name=\"marker_center_y\" type=\"double\" value=\"0.0\"/>\n    <param name=\"threshold\" type=\"int\" value=\"100\"/>\n    <param name=\"use_history\" type=\"bool\" value=\"true\"/>\n\n  </node>\n</launch>\n", "[ERROR] [1417704617.003409185, 3461.051000000]: Ignoring transform for child_frame_id \"ar_marker\" from authority \"unknown_publisher\" because of a nan value in the transform (0.609564 -0.180967 -3.568543) (-nan -nan -nan -nan)\n[ERROR] [1417704622.185442670, 3463.722000000]: Ignoring transform for child_frame_id \"ar_marker\" from authority \"unknown_publisher\" because of a nan value in the transform (0.984986 -0.604149 -2.678788) (nan nan nan nan)\n[ERROR] [1417704623.374384366, 3464.377000000]: Ignoring transform for child_frame_id \"ar_marker\" from authority \"unknown_publisher\" because of a nan value in the transform (0.590453 -0.226787 -2.891703) (nan nan nan nan)\n"], "url": "https://answers.ros.org/question/198576/ar_pose-doesnt-recognize-tag-in-tum_simulator-please-help/"},
{"title": "what can be done with a RPI ?", "time": "2016-06-04 08:29:51 -0600", "post_content": [" ", " ", "Hello,", "First time hobbyist developer, wondering what can be done with ROS if installed on a RPI board such as the RPI3 or Zero?", "Which robot functions have people successfully implemented, such as face recognition, mapping, navigation via different sensors; and how many functions might run concurrently and at what quality and accuracy?  ", "Thanks,\nPerigalacticon"], "answer": [" ", " ", " ", " ", "Your question is too generic, but if you are asking about computational power I can tell you from my experience that it is possible to execute in a Rpi2 or Rpi3 relatively weighty processes like slam algorithms (hector slam or gmapping) if they are properly parametric.", "A rpi3 and even a rpiZero  are enough to do robot navigation using the ros navigation stack if it is properly parametrized too.", "Thanks I was just wondering what can be done if I use ROS with an RPI, and what people have been able to do successfully and if anyone knows of similar projects with code so the wheel doesn't have to be reinvented. I am definitely interested in SLAM, and vision based \"human detection\" capabilitie"], "url": "https://answers.ros.org/question/236073/what-can-be-done-with-a-rpi/"},
{"title": "Simulating a custom robot going up a ramp in Gazebo [closed]", "time": "2011-12-14 18:50:54 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I tried to simulate a custom 4-wheel robot model (URDF) going up a ramp in gazebo but it cannot do so. It can navigate on the ground plane using a custom base controller but when it tries to go up the ramp it gets stuck.", "What am I missing in the URDF?\nSome pointers to implement this is appreciated.", "Thank you", "CS", "Here is my URDF", "I still dont have a good grasp of the tags anchor, mechanicalReduction", "\n<robot xmlns:sensor=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor\" xmlns:controller=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\" xmlns:interface=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\" xmlns:xacro=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\" name=\"my_robot\">"], "answer": [" ", " ", "The way I solved this problem was to drastically increase the ", " value in the ", " tag for the wheel joints. I wrote my controller for those wheels using a PID controller (located in the ", " package). I use this PID controller to control the speed of the wheels. Since the controller subscribes to cmd_vel, the PID will continue to increase the motor power to maintain the desired velocity. ", "You could also try adding the following inside of your transmission descriptions:", "You'll have to figure out what value works best for you. "], "question_code": ["<include filename=\"$(find rover_base_controller)/urdf/materials.urdf.xacro\" />\n\n<!-- This adds the laser macro, including the scan for gazebo and link name -->\n<include filename=\"$(find pr2_description)/urdf/sensors/hokuyo_lx30_laser.urdf.xacro\" />\n\n<xacro:property name=\"scale\" value=\"1.0\"/>\n<xacro:property name=\"base_length\" value=\"0.6\"/>\n<xacro:property name=\"base_width\" value=\"0.3\"/>\n<xacro:property name=\"base_height\" value=\"0.1\"/>\n<xacro:property name=\"wheel_base1_length\" value=\"0.02\"/>\n<xacro:property name=\"wheel_base1_width\" value=\"0.02\"/>\n<xacro:property name=\"wheel_base1_height\" value=\"0.07\"/>\n<xacro:property name=\"wheel_base2_length\" value=\"0.02\"/>\n<xacro:property name=\"wheel_base2_width\" value=\"0.05\"/>\n<xacro:property name=\"wheel_base2_height\" value=\"0.01\"/>\n<xacro:property name=\"wheel_base3_length\" value=\"0.02\"/>\n<xacro:property name=\"wheel_base3_width\" value=\"0.02\"/>\n<xacro:property name=\"wheel_base3_height\" value=\"0.06\"/>\n<xacro:property name=\"wheel_thickness\" value=\"0.06\"/>\n<xacro:property name=\"wheel_radius\" value=\"0.053\"/>\n<xacro:property name=\"mast_height\" value=\"0.2\"/>\n<xacro:property name=\"mast_length\" value=\"0.01\"/>\n<xacro:property name=\"mast_width\" value=\"0.01\"/>\n<xacro:property name=\"mast_vertical_bar_height\" value=\"0.01\"/>\n<xacro:property name=\"mast_vertical_bar_length\" value=\"0.01\"/>\n<xacro:property name=\"mast_vertical_bar_width\" value=\"0.2\"/>\n<xacro:property name=\"M_PI\" value=\"3.14159265\"/>\n\n\n<xacro:macro name=\"default_inertial\" params=\"mass\">\n    <inertial>\n        <mass value=\"${mass}\" />\n        <inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\"\n                 iyy=\"0.01\" iyz=\"0.0\"\n                 izz=\"0.01\" />\n        </inertial>\n</xacro:macro>\n\n<!-- base_footprint is a fictitious link(frame) that is on the ground right below base_link origin,\n         navigation stack depends on this frame -->\n    <link name=\"base_footprint\">\n        <inertial>\n            <mass value=\"0.0001\" />\n            <origin xyz=\"0 0 0\" />\n            <inertia ixx=\"0.0001\" ixy=\"0.0\" ixz=\"0.0\"\n                     iyy=\"0.0001\" iyz=\"0.0\"\n                     izz=\"0.0001\" />\n        </inertial>\n\n        <visual>\n            <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n            <geometry>\n                <box size=\"0.001 0.001 0.001\" />\n            </geometry>\n                <material name=\"Green\"/>\n        </visual>\n\n        <collision>\n            <origin xyz=\"0 0 ${wheel_base1_height + wheel_radius}\" rpy=\"0 0 0\" />\n            <geometry>\n              <box size=\"0.001 0.001 0.001\" />\n            </geometry>\n        </collision>\n    </link>\n\n<link name=\"base_link\">\n    <visual>\n        <geometry>\n            <box size=\"${base_length*scale} ${base_width*scale} ${base_height*scale}\"/>\n        </geometry>\n        <material name=\"Blue\"/>\n    </visual>\n    <collision>\n        <geometry>\n            <box size=\"${base_length} ${base_width} ${base_height}\"/>\n        </geometry>\n    </collision>\n    <xacro:default_inertial mass=\"50\"/>\n</link>\n\n<joint name=\"base_footprint_joint\" type=\"fixed\">\n    <!-- NB: While you would think this would make it go up, it is oddly reversed.\n         This moves the joint to 0,0,0 where we want it ..."], "answer_code": ["effort", "<limit>", "<motorTorqueConstant>200.0</motorTorqueConstant>\n"], "url": "https://answers.ros.org/question/12352/simulating-a-custom-robot-going-up-a-ramp-in-gazebo/"},
{"title": "Which single board computer should I chose?", "time": "2016-03-26 06:43:19 -0600", "post_content": [" ", " ", " ", " ", "Hi ros community", "I' m working with ros to make a robot which can go itself with image proccessing. I am using lots of hardware in this project and I used pcduino V3 until this time. But I want to change pcduino because I' m having some trouble with it.", "I looked lots of single board compuers on the internet and finally find 4 boards to chosing. But I cannot decide which board I should choice for best performance. Can anyone help me for it?", "I can give 100$ at most, so I find:", "Which board is the powerfull board from other boards or are there more powerpull board from these?(less $100)", "Thanks a lot..", "Take a look at the BeagleBone Black too. It costs $55 and you can install Ubuntu 14.04.x for ARMhf and ROS Indigo for ARMhf on it. It also has a ton of I/O pins.", "Thanks for your advice but beagle bone black' s performance is less than all off the boards in the list. So I shouldn' t prefer it.", "Do you run from a battery? If so you might want to ditch the Cortex A7 ones.", "I have to use battery because I am working with a mobile robot. But it causes power consumption problem if I choice like odroid xu4.", " I'm curious, why is that? Does the Cortex A7 consume a lot of power? Is that also true of the A8? Thanks!", "Its not that the A7 in particular consumes a lot of power, its actually pretty good. It is just that various benchmarks (including my own) seem to indicate a better performance/watt ratio for the A53 then the older A7/A8/A9."], "answer": [" ", " ", "If you want more powerful performance, you will usually need to provide more power, this is one of the trade-offs you need to consider. ", " we used x-u4 with turtlebots and were very pleased with their performance. We used  ", "  to step down the power from the kobuki base. ", "Thanks for your advice. I have one more question.", "I want to use 7 A/h Li-ion battery in my project. Is it enough for this board?", "Depends how long you hope it will run (and what voltage the battery is). The x-u4 only peak very briefly at more than 3A as they start up, most of the time they are in the 1-2A range, but that will also depend on what you are getting them to calculate. We used the eMMC memory option with them.", "Thanks again.", "If I can work more than 30 minutes without charging is enough for me. Some other additional hardware (like wireless) will work with it. And 7 A/h battery will feed only them. If not enough can you advice a battery?", " ", " ", "If you like to have an easy ros usage you should use a board with Ubuntu 14.04 support. With your spec I think the odroid xu4 is the best. But clock speed is not the only think. The Ram size is important to.", "The other thinks are Linux drivers for open cl and open gl es, if you need some kind of visualization of ros.", "The PI 3 should be nice to but there is no Ubuntu 14.04 support. You can use debian Jessie, but the environment setup is non that easy but possible.", "Thanks for your answer.", "The ram size in these boards are same and 2 GB so I don' t need to write it. I want to chioce odroid xu4 but it has a very big power problem. How can I give it 4A without adapter?", " ", " ", "I have looked at many boards and have benchmarked a few of them.\nBBB will not have the performance that you need.", "Qualcomm's dragonboard 410C is a good board.  75$ USD. ", " I installed Indigo on it using CHROOT\n( ", " ) ", "RPI3 Should do the job, GPU not as good as the DB but you probably will not use the GPU unless you write your own image processing algorithms using the GPU directly.", "Pine64 is another one to try.", "Basically I think an A53 process would be a good choice.  if you had a larger budget then the NVIDIA Jetson X1 would be ideal :)"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Banana Pi M3          ==>  1.8Ghz * 8 Cores / PowerVR SGX544MP1 / wireless / 8 Gb Emmc Flash /2 usb ports", "Orange Pi Plus 2      ==> 1.6Ghz * 4 Cores / Mali 400 Mp2            / wirekes / 16 Gb Emmc Flash / 4 usb ports", "Odroid XU4             ==> 2.0Ghz * 4 Cores + 1.4 Ghz * 4 Cores / Mali T 628 MP6 / no wireless / no flash / 3 usb ports", "Odroid C2               ==>  2.0Ghz * 4 Cores / Mali 450                   / no wireless / no flash           / 4 usb ports"], "url": "https://answers.ros.org/question/230198/which-single-board-computer-should-i-chose/"},
{"title": "Navigation path is red in rviz", "time": "2016-03-18 11:37:41 -0600", "post_content": [" ", " ", "I am using a turtlebot 2 and am working on autonomous navigation.  Whenever I am sending navigation goals in rviz, the path shows up as flashing red instead of green even though the robot is able to execute the navigation.  For navigation of a known path, this is not an issue but when I try to do autonomous gmapping of an unknown environment, the DWA path planning fails often and I think the issue might be related to the red path.  My only thought is that it is related to the robot_footprint because I always get the error \"You must specify three points for the robot_footprint.\"  Anyone know what might be causing this?"], "answer": [" ", " ", " ", " ", "Your problem has nothing to do with the path color.\nYou can actually choose this in ", ".\nSee ", ".", "You should obviously configure your robot footprint correctly.\nHowever, there is not enough information to help you with the other errors. At least describe the behaviour in more detail (how is DWA failing, what do you expect), what commands do you launch, ...", "Well, the path is sometimes green but will periodically flash to red, which suggests that this is not a settings problem.  In terms of the robot footprint, I have the robot_radius set to 0.20 in the costmap_common_params.yaml but I get the error \"you must set at least three points for footprint..\"", "I tried setting the footprint to a 0.4x0.4 square and got the error \"Footprint spec is empty, maybe missing call to setFootprint?\"", "There are probably two paths overlapping. There is nothing in rviz that will flash different colors as an indication of an error state."], "answer_code": ["rviz"], "url": "https://answers.ros.org/question/229499/navigation-path-is-red-in-rviz/"},
{"title": "In ros gmapping, how to use Intel Research Lab dataset?", "time": "2016-04-27 07:38:55 -0600", "post_content": [" ", " ", " ", " ", "I don't know how to use the Intel Research Lab dataset (.clf).  Should I read the file and publish the lase_scan and tf topic?  If so,how should I do in detail?  thanks."], "answer": [" ", " ", "I had some time on my hands, so I wrote a script for you: ", " .", "The script converts the .clf file into a rosbag file. The alternative would have been to publish laser_scan and tf, as you suggest, but then we would also have to handle publishing the ", " topic (since the data has timestamps from the past). By converting to a bag file, ", " handles this for us.", "Thanks. I have converted the .clf file to .bag file by running the  clf_to_rosbag.py .\nBut when I run the bag file ,open the rviz, no map  received.I input the command: \"rosrun gmapping slam_gmapping scan:=base_scan\" and  it print nothing. I think if it works well ,it should print something . thanks", "It should be ", ", not ", ", since I wrote the laser data on the topic ", ".", "Still no map .And in rviz, Map shows no  \" No transform from [] to [map]\",how to deal with it ? Thanks", "OK,thanks very much.I have got it. I need to publish a tf  base_link -> base_laser ,and I modify the .py you provided , the tf in it is odom->base_link . Now ,it works.  thanks!", "Glad to help! BTW, you closed the question; I've reopened it, since we usually don't do that here. Please \"accept\" my answer instead.", "i don't know how tp accept your answer ,please tell me", "Just click the round checkmark icon next to my answer and ", " until it turns green. Don't click again.", "Hello,can you give me the clf_to_rosbag.py you midified? Thank you!"], "answer_code": ["/clock", "rosbag --clock", "scan:=laser", "scan:=base_scan", "laser"], "url": "https://answers.ros.org/question/233042/in-ros-gmapping-how-to-use-intel-research-lab-dataset/"},
{"title": "Connect two HDL32e together", "time": "2016-03-16 08:07:19 -0600", "post_content": [" ", " ", " ", " ", "Hey,", "I have two HDL 32e velodyne laser scanners and I connected using a switch to my computer.", "I ran the velodyne package and I'm getting the pointcloud.", "The problem is both the pointclouds of the different HDL's are getting combined.", "Is there a possibility to seperate the pointclouds as I want to use tf and build a map?"], "answer": [" ", " ", " ", " ", "It looks like there is a parameter called device_ip:", "It isn't used by any of the launch files, so you would have to add it to nodelet_manager.launch (and pass it down from a launch file that includes that one), and then you would launch it separately for each ip, and likely put each launch into a separate namespace.  If you get it working you should try to get the changes merged into the repository.", "That parameter was added fairly recently. A pull request adding it to the appropriate launch files would be welcome.", "hi ", ", thanks, I managed to get it working.", " I don't know how to do a \"pull request\". I can share the changes with you.", " You probably just git cloned  ", "  and made changes to it?  You could paste the changes into an answer here (they won't fit in comments), but I highly recommend making a github account if you don't have one, and then you fork the velodyne repository... ", "... then git clone it and copy your changes in, and then after pushing them in there will be a green pull request button on your github velodyne page.", "Yes, it's really not hard, and it saves a lot of time for a busy maintainer (like me). :-)"], "answer_code": ["  private_nh.param(\"device_ip\", devip, std::string(\"\"));\n  if(!devip.empty())\n    ROS_INFO_STREAM(\"Set device ip to \" << devip << \", only accepting packets from this address.\" );\n  input_->setDeviceIP(devip);\n"], "url": "https://answers.ros.org/question/229231/connect-two-hdl32e-together/"},
{"title": "ekf_localization: odometry/filtered topic:Frequency too low?", "time": "2016-03-08 10:18:00 -0600", "post_content": [" ", " ", " ", " ", " I am using robot_localization to fuse gps and imu referencing this Tom Moore answer:   ", "My setup is the same with a full-size SUV vehicle, Beaglebone Black board, Phidgets imu w/mag, ublox gps, with no odometry sensors. I am running Ubuntu 14.04.4 LTS, deb 4.1.15-ti-rt-r40 armv7l, ros indigo, and robot_localization with the ekf filter.", "I've been able to produce decent results, but the mobile trial awaits resolving issues, so everything is tested in the static position.  I've calibrated the magnetometer for hard and soft iron on the vehicle and incorporate the factors in the driver.  I use an overall launch file that separately calls the devices and robot_localization.  REP's 103 and 105 were adhered to with the imu (as far as I can tell), since the raw mag points (X) east at zero and the other checks (Y and Z) work out.  Declination is accounted for in the navsat_transform launch file.", "The issue arises with the diagnostic output where this occurs:", "I've checked the frequencies of all the incoming topics and they all seem ok except for the /tf topic which also shows around 1 hz.  The transform frames correctly seem to tie odom to base_link with child frames phid_imu and gps.", "A secondary factor may be in play is the 0 yaw covariance factor produced by the Phidgets imu driver.  I cannot seem to find where this can be adjusted to help convergence.  The gps is running at 4hz(fortunately) so  it will determine all gps inputs (ie. /odometry/gps).", "I am asking how to resolve the low ekf_localization diagnostic issue and will likely have to ask another question about the yaw covariance in the Phidgets driver.", " I've placed the diagnostic output, device output, launch files, node graph, and transform tree on Dropbox   ", "Any ideas towards this end?\nPlease forgive the text formatting.", "Regards and Thank you,", "b2256", "EDIT 03/10/16\nPer Tom Moore's comment below, I did perform the ekf/imu test and found that if the data rate value in the imu was slowed to 40ms, the frequencies of /odometry/filtered and /tf  were helped.  I then implemented the full launch with the same result although it takes a few minutes to stabilize. Will these values at 25 hz be sufficient for operation?  Additional tweaks involved reducing the ekf data rate to 28 hz hoping to stabilize (with limited success at ~25.6 hz, std dev 0.0155s, and max of 0.353s).  Note that these results were via ssh (from a Windows laptop) into the ...", "Why is ", " publishing at 1 Hz? The filter is going to look up transforms from the ", " of your sensor inputs to the ", " of your ", " or ", " frames. Also, can you please post sample messages for each input?", "Tom,  I do not have any idea why tf is at 1 Hz. Nor where I might find the params?  The sensor inputs are included in the 030716_diagnostics.odt file from the Dropbox link.  The node diagram as well as the transform tree are there as well.  Any advice would be greatly appreciated.", "Can you try stripping out everything from the launch file and starting with one sensor alone?", "Yes. Should I start with just the imu.launch?  That is where the call for the madgwick (pre)filter (imu_filter_madgwick/ImuFilterNodelet) is setup.  I expect that it will be not until early this evening before I can get back to it (at work.)  Thank you in advance for an ongoing dialog.", "No problem. Which sensor you use first is irrelevant; what matters is that you run with just one sensor and see if the behavior improves. I'm just trying to troubleshoot sensors.", "I've now tried imu only and gps only; both work as expected.  When the ekf and navsat are added topics /gps/filtered and /tf go to around 1 hz.  I've also tried on a desktop as opposed to the BBB, and frequencies are excellent. An intermittent imu connection problem also fixed. More on Dropbox link", "No, sorry, what I meant was run the EKF with, for example, just the IMU as input. Don't run ", ". Strange that it runs on the desktop. Are you building from source?", "No, not from source but it did work.  See edit above for continuation."], "answer": [" ", " ", " With Tom's (Moore) help, I've discovered that the answer lies in two elements:  1) for BeagleBone Black, there is a cpu frequency governor.    Make sure the maximum cpu is utilized unless you're using battery power.  It turns out that the default may be as low as 300 mHz... not enough.  Raise it to your comfort level.  Here is the source I used:   ", "   2) Arriving at optimal imu input frequency vs ekf_localization frequency (also navsat) is by trial and error.  Proceed until you can reduce the cpu load and increase the frequency of the output node (in my case, /odometry/filtered) to a stable compromise. "], "question_code": ["level: 1 \n  name: ekf_localization: odometry/filtered topic status \n  message: Frequency too low. \n  hardware_id: none \n  values: \n    - \n      key: Events in window \n      value: 17 \n    - \n      key: Events since startup   \n      value: 172 \n    - \n    key: Duration of window (s) \n    value: 16.443676 \n    - \n      key: Actual frequency (Hz)  \n      value: 1.033832 \n    - \n      key: Minimum acceptable frequency (Hz) \n      value: 55.800000 \n    -\n      key: Minimum acceptable frequency (Hz) \n      value: 72.600000\n", "tf", "frame_ids", "frame_id", "navsat_transform_node"], "url": "https://answers.ros.org/question/228533/ekf_localization-odometryfiltered-topicfrequency-too-low/"},
{"title": "Reasons for slow response to actions, services, messages?", "time": "2013-09-07 20:52:10 -0600", "post_content": [" ", " ", " ", " ", "Hi guys!", "I'm trying to debug a tool, which responds to service calls and action goals with a lag of 10 to 20 seconds depending on the load of the machine (50% - 90%). I determined this time lag using the debug output, which shows when the action server receives the action goal.", "What could be the reasons for this significant time lag?", "In my use case - pick & place using MoveIt - the node running the action servers and service providers is causing most of the load. So my first guess is, that something is blocking the respective callbacks.", "Is this just a matter of not enough horse power? Which parts of the code could cause this blocking/slowing down?", "Interesting is that actions, services, messages of nodes running in parallel are processed fine, i.e. there is only a small time difference between sending and receiving action goals, responding to services, receiving messages.", "Thanks for your help!", "Additional info using the tools recommended in ", " 's answer:", "As I mentioned above, the load is usually around 50 ~ 70% (desktop) / 50% ~ 90% (robot) (using top). iftop is an interesting tool! Shows me that the local traffic (lo interface) goes up to 2Gb/s, when starting to process point clouds.", "Which process is generating the load? Is it the roscore or one / multiple of the started nodes?", "The rosmaster load is neglectable. The main load comes from MoveIt's move_group node (more details added in the question). And it's only that node's topics, services and actions, which are processed extremely slow. The other nodes run fine, what is probably because the CPU is not fully used."], "answer": [" ", " ", " ", " ", "More than an answer, this may serve as a first diagnostics step.", "How loaded is your system when this happens?. I recommend a first sweep with the following tools:", " to check ", " and ", ". ", " (may require sudo) to query the ", ", eg. ", " for loopback only.", "It might also be good to check the ", ", eg. ", ". ESTABLISHED, CONNECTED correspond to sockets currently in use, while TIME_WAIT, CLOSE_WAIT are pending to close. Pay special attention to the latter, as large counts here can indicate lots of short-lived sockets, which usually occur in ROS environments when you frequently query the master (non-persistent service calls or parameter reads). Many socket opening/closing operations will increase your system CPU load (shown in top under Cpu(s) .... sy).", " From the updated question details.", "Could you post for completeness the CPU load and network traffic values with pointcloud perception disabled?.", "It seems that the pointcloud messages are taking up a lot of bandwidth, and (de)serializing + processing them (coordinate system change, self-filtering, object detection, etc.) is in turn consuming significant CPU resourecs (maxing out a core, leaving no room to the scheduler to process all incoming messages).", "What kind of pointcloud input are you feeding ", "?. If it's the raw input from a Kinect-like RGBD sensor, that might indeed prove prohibitive. Preprocessing the pointcloud might help. These are some indicative numbers I took some time ago:", "Finally, if you need point clouds at discrete time instances (as opposed to a continuous stream), gate pointcloud traffic through an on-demand snapshotter.", "Thanks for listing these helpful tools. I added the result of them to my question. To me CPU load looks OK as well as the sockets. The network traffic looks high, but then I have no idea what is \"high\" and \"low\" regarding the traffic on the lo interface. Do you have any experience with it?", "Extra info added. Your suggestions about preprocessing the point cloud is a good idea. I wonder however, if there are other ways to improve this situation. There is still 10% of the computation power unused. Also, there are multiple cores available. Can't that be used?", " ", " ", " ", " ", " Just found this interesting Q&A:  ", "  Could multiple callback queues be useful in the current case?  "], "answer_details": ["Original cloud contains 200k-300k valid points.", "Crop to a bounding volume of interest (~1 order of magnitude less points)", "Downsample with octomap (additional ~2 orders of magnitude reduction)", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "top\n", "with point cloud processing: ~40% idle, load average: ~2.5 (quad-core CPU: i5-2500 CPU @ 3.30GHz), move_group process at ~130%", "w/o pcp: ~90% idle, load average: ~0.5", "iftop\n", "with pc: ~2Gb/s", "w/o pcp: ~5Mb/s", "sockets\n", "When idling few (~5) sockets show up every few seconds (5 - 10). For each motion plan action a few more sockets pop up."], "answer_code": ["top", "iftop", "sudo iftop -i lo", "netstat | grep ESTABLISHED | wc -l", "move_group"], "url": "https://answers.ros.org/question/77161/reasons-for-slow-response-to-actions-services-messages/"},
{"title": "Can a SMACH state machine be paused and resumed?", "time": "2013-04-21 14:20:21 -0600", "post_content": [" ", " ", "Hello,", "I've been working through the ", " and ", " tutorials.  Teer appears to make it easy to pause and resume a collection of tasks.  I was wondering if there is a way to do something similar in SMACH.", "My use case is a robot that is running around the house doing things by executing a SMACH state machine.  However, it is also monitoring its battery levels using a SMACH MonitorState.  When the battery levels fall too low, the robot should navigate to its docking station, recharge, then resume the execution of the original state machine where it left off.", "I can imagine using a global \"pause/resume\" flag in the main state machine across all its contained states but this doesn't feel very SMACH-like.  Does any one know how to implement this kind of pause/resume using SMACH?", "Thanks!", "\npatrick"], "answer": [" ", " ", "So, it is possible to set the initial state (and userdata) of a SMACH state machine with ", ". So if your battery monitor preempted the state machine, and you stored the active states of the containers you cared about, you could \"resume\" from a given state of execution later on. You'd have to do the bookkeeping on your own, but you should be able to get the behavior that you want. ", "What SMACH really needs is an extension for event-based transitions, and then you could distribute the responsibility of pausing / resuming in each active container.", "Many thanks ", ".  This will definitely get me on the right track.", " ", " ", "I must say that this is half an answer, half a new question...", "I have just learnt about the ActionServerWrapper and I think is the place to go. But you need an extending class to manage  request belong currently supported start and cancel. I'm thinking in overloading the goal received callback and inspect the goal for a field like \"sm_command\" or so containing commands like \"start\", \"pause\", \"resume\", \"stop\", \"set_userdata\", etc. Of course, \"start\" would be just the current behavior of starting the state machine.", "Do you think this can be a good idea?", "Of course, we still need to implement some pause/resume mechanism, as ", " explained. Here I'm just guessing what could be a good external interface! "], "url": "https://answers.ros.org/question/61244/can-a-smach-state-machine-be-paused-and-resumed/"},
{"title": "Trying to run ardrone_driver and stuck on \"Checking AR.Drone version\"", "time": "2014-06-06 15:51:11 -0600", "post_content": [" ", " ", "After I installed ROS on my Ubuntu 12.04.4 machine I followed the instructions of this tutorial to install ardrone_autonomy package and  tum_ardrone package:", "-wiki.ros.org/tum_ardrone-", "When I successfully installed all the required packages, I followed the steps to Run the packages with the command rosrun.", "Before executing the \"rosrun ardrone_autonomy ardrone_driver\" command I connected to the ARdrone 2.0 via wifi. The ip address of the drone is 192.168.1.1", "When I execute the command \"rosrun ardrone_autonomy ardrone_driver\" I get stuck on \"Checking AR.Drone version\"", "Any suggestion how to solve this?", "FYI:\nI tested the ardrone with my ipad and the freeflight app and it works perfectly. After testing it I pressed the reset button on the drone (leds flashed and the drone reset) because i read that it might still be paired with the ipad."], "answer": [" ", " ", "You are probably seeing this error because your computer is connected to a LAN network which is configured to use ", " IP range. This is the same IP range used by the wireless network created by the AR-Drone. The easy fix would be to disconnect your LAN cable (disable that network) before running the driver. The more advanced fix involves modifying the routing table of your Linux machine.  As ", " suggested, you can confirm that the traffic is being routed correctly by using ", " before running the driver. ", " ", " ", "I am having the same issue.  I can telnet to the drone at 192.168.1.1, but when I try running:\nrosrun ardrone_autonomy ardrone_driver ", "I get:\nGetting AR.Drone version...\nover and over on the screen..", " ", " ", "What's the IP of your computer? ", "You may need to hard-code the drone's ip using", " ", " ", "I was having the same problem... u must have paired with ur iphone once.. so, u need to pair it up with ur pc first.. press the reset button in battery holder place(use a neddle type thing to press it)... then telnet 192.168.1.2( for drone 2.0)  or telnet 192.168.1.1( for drone 1.0)", " ", " ", "For me, I first open a command window and type  ", " before running the driver. "], "answer_code": ["telnet 192.168.1.1", "rosrun ardrone_autonomy ardrone_driver -ip 192.168.1.1\n"], "url": "https://answers.ros.org/question/172978/trying-to-run-ardrone_driver-and-stuck-on-checking-ardrone-version/"},
{"title": "How do i reset the odometry topic on ardrone_autonomy", "time": "2015-06-28 12:21:58 -0600", "post_content": [" ", " ", "Hi,\nIm trying to read the /ardrone/odometry topic, but it is showing me very large values of x and y position Just When it starts. Im am wondering if there is a way to reset the odometry topic so it shows me 0 for the positions on the beginning. Also, the time that it shows is very large, so it has been counting long ago.", "Thanks", "I faced the same problem. But I had an workaround. if you subtract the values of x, y and t obtained from the first call back and the subsequent values, you get the values for that session of operation. But I am eager to know if there is an actual way to reset the topic.", "Well, thats a solution, thanks!\nJust to know, how do you differentiate the first callback from the rest?"], "answer": [" ", " ", " ", " ", "This is what I did. ", "Update: If you update AR Drone firmware, you can eliminate this problem. There are a few ways to update the firmware. The easiest is thorough the official AR.FreeFlight App by Parrot. The newer versions of this app has an option to update the firmware. Just make sure your phone battery and AR Drone battery is full and click update. First the file will get transferred and after a few drone-turn on-off episodes the green lights will turn on indicating a successful firmware update. Then odometry works properly.", " Does this solution work?"], "answer_code": ["class xyz{ \n  Private:\n    int count;\n    double init_x, init_y, init_t;\n public: \n    xyz( ros::NodeHandle &nh){\n       count = 0;\n       define the callback here\n    }\n   void callback(...){\n       if (count == 0){\n          store the values of x, y and t in init_x, init_y and init_t\n          count ++;\n       }else{\n          subtract initial values with the current values\n      }\n }}\n"], "url": "https://answers.ros.org/question/212371/how-do-i-reset-the-odometry-topic-on-ardrone_autonomy/"},
{"title": "rviz Bus error on Nvidia Jetson tx1 board", "time": "2016-01-12 23:23:20 -0600", "post_content": [" ", " ", "Hi,", "I am trying to run rviz on Nvidia Jetson Tx1 board, but the following error appears:", "Could anyone help me?", "Hi, have you solved this problem? Could you teach me? I meet the same problem!", "Hi,  have you solved this problem? Could you teach me? I meet the same problem!"], "answer": [" ", " ", "You need to run:\nunset GTK_IM_MODULE", " From the ROS Arm instruction page:\n ", "Using RVIZ\nIt is not recommended to run rviz on most ARM-based CPUs. They're generally too slow, and the version of OpenGL that is provided by the software (mesa) libraries it not new enough to start rviz.", "'IF' you have a powerful board with a GPU and vendor-supplied OpenGL libraries, it might be possible to run rviz. The IFC6410 and the NVidia Jetson TK1 are two such boards where rviz will run, although neither is fast enough for graphics-heavy tasks such as displaying pointclouds.", "Note that rviz will segfault if you have the GTK_IM_MODULE environment variable set, so it's best to unset it in your ~/.bashrc:", "unset GTK_IM_MODULE", "Hi, i followed your advice but unsuccessed!"], "question_code": ["Bus error\n"], "url": "https://answers.ros.org/question/223995/rviz-bus-error-on-nvidia-jetson-tx1-board/"},
{"title": "Is ROS compatible with Python and can it run on a Raspberry Pi", "time": "2015-06-17 06:09:26 -0600", "post_content": [" ", " ", "Looking to add sensors to a robot using Raspberry Pi and the programming language Python. I was wondering if ROS would be a suitable operating system."], "answer": [" ", " ", "ROS despite its name is not really an operating system, it is more like a middle-ware that facilitates controlling your robot and it works on top of Ubuntu or any other Linux distribution. I believe there is already builds of ROS that you can install directly on your Raspberry Pi, as if you were to install any other software on your board, you can find a tutorial ", ". And ROS is implemented in Python as well, if you do the ROS tutorials you will see that the rospy library offers access to all ROS functionalities from Python. ", " ", " ", " ", " ", "ROS has a nice Python interface (rospy).     You can use ROS on Raspberry Pi / Rasbian.  I'd recommend the RPI-2 - the RPI-1 will be very limited due to memory and low CPU power (you might want to consider the ODROID-C1 RPI clone  as well since it is same price and faster).  Not all ROS packages are available for the ARM processors.  For more info, see here:", ".", " ", " ", "ROS is a so called meta-operating system, this means that it's not an OS itself, it need an OS like Ubuntu to run.\nBeside that, the two main languages supported by ROS are C++ and python.", " ", " ", "If you use a raspberry pi 2, you can use Ubuntu Snappy, which is a version of Ubuntu for Arm architecture. \nYou can get it here: ", "Make sure you do the partitioning under the usage section, as it will give you a full sized SD card. To install ROS, I followed the instructions here, and it worked out well:  ", "This worked just fine for me, I was able to use ROS without a problem, I even compiled OpenCV in about 2 or so hours!"], "url": "https://answers.ros.org/question/211528/is-ros-compatible-with-python-and-can-it-run-on-a-raspberry-pi/"},
{"title": "Quadrotor GUI", "time": "2015-07-23 23:10:30 -0600", "post_content": [" ", " ", "Hello all, ", "Is there a good open source GUI for piloting a quadrotor based on ROS ? ", "Thank you.  "], "answer": [" ", " ", " ", " ", "I do not know about any special GUI. If I would need it, I'd create a perspective for it in rqt.\nWell the big question is, what do you need to show there?", "Camera? -> rqt_image_view", "Battery? -> multiple rqt_plugins available (unfortunately no standard one)", "You could also add RViz to it to Visualize a 3D Model of the copter.", " I wondered a bit myself and found that one:\n ", "Looks pretty useful I think, but I do not see if you can get the code.", " Maybe you can get it here, but I do not know if it is really the same one:\n ", " ", "Hi, thank you for ground_station this is close to what I was looking for. As for your quesiton, all quadrotors are similar and data you want to see in UI is also similar. Image_View, battery, roll, pitch, etc. I want to make it as generic as possible.", " ", " ", "Quadcopter ROS GUI in google gave me :", "Model, control and simulate a quadcopter\n", "I am not sure this is the kind of control you want."], "url": "https://answers.ros.org/question/214464/quadrotor-gui/"},
{"title": "Cannot see the scan data of rplidar on raspberry pi", "time": "2015-05-10 20:38:28 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone, I am running ROS indigo on Raspberry Pi, I successfully installed ", " for Pioneer robot and ", " for Asus Xtion Live Pro, they both works well.", "I successfully installed ", ", the nodes are properly launched, but when I run ", "I can't see any output. I tried it on my PC(Ubuntu 14.04 LTS, x86_86), everything goes well. I also tried to run the demo of rplidar ", ", the output data is right. It is so strange, could you give me some advice to solve the problem?", "Hi, I encountered the same problem, and wondering whether you've found a solution yet?  I connect rplidar to the Pi via usb.  RPLidar health status : 0 (which means ok), but rostopic echo /scan shows nothing as you.  Thanks", "Yes, I've got the solution. Here is what I encountered, once I connect rplidar to my laptop, a new device /dev/ttyUSB0 will appear, however, when I tried to read data from it, /dev/ttyUSB0 disappears! This may cause the problem. You can check.", "This may be an issue of Raspberry Pi. I use a USB hub, and solve the problem, so weired......", "I see.  thanks for your response.  I tried to power the raspberry pi with a 2.1A usb battery pack, and it seems to work (more testing needed for its reliability), but not with 1A power supply.  I think the issue might come down to whether we provide enough power or not.", "Hi Krist, my solution with 2.1A usb battery pack is not very reliable.  It's good sometimes, but not always.  So, when you use the USB hub, do you connect them as battery > RPi > USB hub > RPLidar? Or do you have a 2nd battery pack to power the USB hub?  Thanks", "No, I don't have a 2nd battery.", "hmmm, so a usb hub without additional power somehow works... interesting... let me give it a shot.  thanks!"], "answer": [" ", " ", "As it seems to be a current problem to me, I looked into what's the current limit on my Raspberry Pi 2 Model B, and found that the default limit is 600mA.  However, this can be increased to 1.2A.  All you need to do is to add a line ", " in ", " (see ", " for more detail).  Then, I encounter no more usb disconnection when start reading and publishing the scan from RPLidar.  RPLidar works with Pi like a charm!", "I also hooked up a usb current meter between my 2.1A battery pack and the Pi.  The reading is 0.4x Amp when just connecting the RPLidar to the Pi.  However, when start reading the scan data, the reading increases to 0.5x Amp, which maybe too close to the 0.6A default current limit and causes the usb disconnection.   Maybe you don't need the usb hub anymore. :)", "You did a great job, I'm sure I will try it, thanks!", "It works. correct answer.", "I am using a Raspberry Pi 3 and that max_usb_current didn\u00b4t work!! But when I change to a better power supply, worked fine!! It\u00b4s really a matter of energy!! Thanx a lot!! Saved my day!!"], "question_code": ["ROSARIA", "openni2_camera", "rplidar_ros", "$ rostopic echo /scan\n"], "answer_code": ["max_usb_current=1", "/boot/config.txt"], "url": "https://answers.ros.org/question/208932/cannot-see-the-scan-data-of-rplidar-on-raspberry-pi/"},
{"title": "[openni_tracker] InitFromXml failed: Can't create any node of the requested type!", "time": "2011-04-14 19:21:32 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to use the openni_tracker node in the openni_kinect stack but when I try and run it I get the following error message:", "InitFromXml failed: Can't create any node of the requested type!", "I'm using ros-diamondback and I'm trying to use openni_tracker with the kinect. This is really frustrating because I already had it working. Any help on why I'm getting this error would be appreciated.", "Also, because openni_tracker doesn't display a view of the camera's image I was wondering if there was an easily accessible node that could display the skeleton tracking image tracker is based on.", "Thanks", "Anthony"], "answer": [" ", " ", " ", " ", "You need to register the license key for the dev modules.", "\"sudo niLicense -l 0KOIk2JeIBYClPWVnMoRKn5cdY4=\"", "Steven ", " ", " ", "I am using ROS Hydro on Ubuntu 12.04 with a Microsoft Kinect.  From top to bottom, this is what worked for me:", "Go to ", " from isura (Thanks) and download ", ".\nExtract the zip and tar into a temporary folder (i.e., ~/tmp).", "Ensure your Microsoft Kinect is properly connected, then run:", "In a separate terminal window run:", "In another terminal window run:", "In Rviz, change Global Options > Fixed Frame to openni_depth_optical_frame. ", "\"Add\" the visualization (bottom left) PointCloud2. ", "Change PointCloud2 >Topic to /openni/depth_registered/points (but whatever you pick works). ", "\"Add\" the visualization (bottom left) TF.", "Stand in front of the Microsoft Kinect and make the \"Psi Pose\".  Axes should appear on all your joints.", "I hope this helps you.  There are a lot of different versions of NITE out there and this is the one that happened to work for me.", "it still have respond:", "HELP!!", "I also meet the problem:\n", "\nBut when I remove and reinstall the NITE and Sensor and change the usb port by many times, it worked. I just run two commands: ", " ", " ", "I find a solution in this link:", "You need to use Sensor-Bin-Linux-x64-v5.1.2.1 rather than  NITE-Bin-Dev-Linux-x64-v1.5.2.23", " ", " ", "Hi Antony,", "I am facing the same problem., did you find any solution ?", "Thanks \nD", "Hi, I met the same problem. Do you have the solution?", " ", " ", " ", " ", "I'm not sure which platform you're using but on my ", ", I experienced the same thing and found it occurs only when the power seems not be provided (light is off on USB cable that's connected to ", "). I solved this issue by first running:", "Adding license wasn't necessary for me. HTH. ", " ", " ", "I'm a new user of ROS openni . Now I am using ubuntu 12.04 and my ROS dist is groovy.\nwhen I run the command rosrun openni_tracker openni_tracker,the only thing I got is a error message:\nInitFromXml failed: Can't create any node of the requested type!", "I followed the solution above provided  but no use", "I googled a lot ,but not a correct solution for me.Any help will be appreciated ,It really drives me crazy.", "Thanks", "Please not use answer section to ask a question even if it's related. Open a new question and refer to this thread.", "sorry won't happen again", " ", " ", "This is how i solved this issue, \nFirstly you need to make sure you NITE 1.5.2.23 installed on your system", "Then run ", "roslaunch openni_launcher\n  openni.launch", "this is necessary for the tracker to work but the point is not include in the documentation", "now, in a new terminal run ", "rosrun openni_tracker openni_tracker", "you shouldnt get any error now for running it, check its working by standing in front for kinect and running rviz ", "I was having problem with openni_tracker as it returned Couldn't connect to any devices..  Hence I installed freenect_launch as they both more or less in the same topics and nearly equivalent... But as you said after running freenect_launch also I am facing the same error. Please help"], "answer_code": ["sudo apt-get install ros-<ros_distro>-openni-camera\nsudo apt-get install ros-<ros_distro>-openni-launch\nsudo apt-get install ros-<ros_distro>-openni-tracker\n", "NITE-Bin-Linux-x64-v1.5.2.23.tar.zip", "cd ~/tmp/NITE-Bin-Dev-Linux-x64-v1.5.2.23/\nsudo ./install.sh\n", "roslaunch openni_launch openni_launch camera:=openni\n", "rosrun openni_tracker openni_tracker\n", "rosrun rviz rviz\n", "[ERROR] [1454926421.236391161]: NITE is likely missing: Please install NITE >= 1.5.2.21. Check the readme for download information. Error Info: User generator failed: Can't create any node of the requested type!\n", "\n NITE is likely missing: Please install NITE >= 1.5.2.21.\n", "roscore  \nrosrun openni_tracker openni_tracker\n", "turtlebot", "iRobot Create", "rosrun turtlebot_node kinect_breaker_enabler.py &\n"], "url": "https://answers.ros.org/question/9737/openni_tracker-initfromxml-failed-cant-create-any-node-of-the-requested-type/"},
{"title": "Information about Turtlebot2", "time": "2012-10-25 05:06:43 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I'll be teaching a robotics course this spring using ROS and Turtlebots as the platform.  I currently have several of the original Turtlebots and intend to order several more in the near future.  I noticed today that the Turtlebot2 is now available for purchase.  I'm trying to determine which version I should buy, and I'm having trouble finding very much information about the new release.  I'm hoping that someone in the know can help me answer a few questions, or point me in the right direction.", "-Is there currently support for the new Turtlebot in Fuerte? ", "-To what degree will code for the new Turtlebot be backward compatible with the old Turtlebot.  Will the existing Turtlebot node still work with the new base? Or will there be a new stack? ", "-It looks like the new Turtlebot is designed to provide power to a laptop.  What laptop(s) will be supported? I've been looking at the Acer Aspire One AO756-4854.", "-Is the low-level interface to the Kobuki base compatible with the interface to the Create?  Is there a published specification for the Kobuki interface? "], "answer": [" ", " ", "I will try to answer your questions in order:", "\nCurrently, yes, and we will build Fuerte images for the new TurtleBot.", "\nWe are attempting to maintain the code as close to the current structure as possible. There will definitely be another package for communication with the drive base. Code which is written for an old TurtleBot should work with a new one (unless you're using messages like turtlebot_node/sensor_state). Code which is written for a new one should also work with an old one (unless you're using Kobuki-specific messages or require the better sensors you will get with the new drive base)", "\nLaptops are a constant source of frustration for us, as manufacturers are now EOLing their laptops on a 6 month rotation and they never work out of the box with Linux. We are currently shipping the Asus 1025C with our TurtleBots.", "\nNo, it is not compatible on a driver level. There is no formal published spec yet.", "Can you say what voltage the laptop power connection is and how much power it provides?", "The power connection is 19 V and will provide 2 A to the netbook when the robot is docked. I believe the current plan is to provide no power on this connector to the netbook when the robot isn't docked (to avoid draining the battery), but I'll have to confirm that", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "To add some extra information to Ryan's post above.", "We are currently hacking on the code in fuerte - you can track the development branches on ", ". I am planning to upgrade the development for groovy relatively quickly - mostly due to catkin and the fact we need to do quite alot of our new development for turtlebot around catkin as well and these won't be compatible with fuerte. ", "So groovy would seem to be a better long term fit for turtlebot2 and I'll certainly make sure groovy installs are manually possible whilst we transition.", "The new code on github is currently using environment variables to define various hardware parts which would be different (currently TURTLEBOT_BASE, TURTLEBOT_STACKS, TURTLEBOT_3D_SENSOR). These get used to launch the appropriate drivers and nodes. This may change, but the launching concept should remain somewhat similar. ", "My perspective is that the new software being developed on github is essentially what defines 'Turtlebot2'. That software is reconfigurable to run with whatever turtlebot-compatible hardware you have.", "We (Yujin Robot) are in the same situation. Currently we are looking at shipping the ASUS1225B until further notice (not yet fixed).", "We're still developing the kobuki driver. It is important to note that there is an REP for the turtlebot - ", ". This is not yet completely polished, though it attacks the major points. The kobuki and create drivers both do the basics - they provide consistent /odom and /cmd_vel topics and the official turtlebot applications are ok. Some of the sensors (e.g. bump sensors) could be made compatible if there is demand for it. If you'd like to get your voice heard, probably the best place to do so is the ", ".", "This week we are also finalising some contact points for Kobuki - mailing list, email, blog, twitter and rebuilding the official web site. I will advertise information for these once done (expect early next week). These should help fill in some holes for people.", " ", " ", "I am also considering a turtlebot2. Can anyone please answer?\n1. What about simulation environment for turtlebot2? Is it Gazebo similar to previous turtlebot?\n2. I saw a robotic arm attached with turtlebot2 sometimes? If I buy a new turtlebot2, will the manipulator arm be also attached with it?\nThank you", "Thank you for your answer. Would you please also tell me whether I can program turtlebot2 (with arm) using hydo? Can I buy turtlebot arm separately from manufacture? Is it available?", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Have you looked at this: ", "/ ? I'm assuming there will be fuerte support.", "There also was a specs page, which seems 404 now. IIRC it was a 19V connector.", "The specs page was moved - now at ", "/", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Here's what I have figured out about the charging port (from the ", "):"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Yes, it's Gazebo.", "Most manufacturers don't sell TurtleBots with arms due to the lack of quality low-cost arms.", "In the future, please post this as a separate question so others can find it easier.", " ", " ", " ", " ", "The laptop charger port is active only when the laptop is plugged in. If you are planning on manually charging the robot, you could charge whatever laptop you place on the robot at the same time.", "If you are interested in long term autonomous operation through the docking station, the Kobuki itself uses a 19v/3.16A (60 Watts) charger and 19v/2.1A (40 Watts) are available for laptop recharging. Any laptop that uses a 40 Watt AC Adaptor will probably be fine.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/46782/information-about-turtlebot2/"},
{"title": "Simple 2D Navigation Goal is making robot to drive circles endlessly", "time": "2015-01-08 17:40:20 -0600", "post_content": [" ", " ", " ", " ", "Hi", "I have created URDF for simple three-wheeled robot with differential drive with laser scanner on top of it.", "All packages created for ROS Hydro can be found  ", " ", "In package robone_description there is folder urdf with robot description. In folder launch there is file display.launch which starts RViz with robot model and robot state publisher node.", "The package robone_2dnav contains two launch files", "Configuration for those nodes can be found in the folder config for robone_2dnav package.", "Also I have created package robone_gazebo to start simulation of the robot.\n", "Previously I have some issues with SLAM gmapping but now map of the environment is good.", "To reproduce issues I am doing following steps", "Robot will start making clockwise circles in the room indefinitely or until it collide with the wall.", "Go directly to the point which is 1m in from of robot. There is no obstacles to avoid on its path.", "The question is what I am doing wrong\n  in this simple setup of navigation\n  stack?", "If bag file is needed please provide names of the topics which need to be provided because for all topics such file is too big to send.", "Link to cmd_vel.log\n  ", "If you echo the velocity commands coming out of the navigation stack, do they seem reasonable?", "David,", "Actually I have interesting data in cmd_vel\nAt the beginning of the movement I have cmd_vel with angular.z > 0 the rest of the linear and angular components are zero.\nThis is only for few commands.", "After this linear.x became 0.5 and angular.z start decreasing towards zero. Other linear and angular components are zero.", "So it seems that robot is turning at the beginning of movement however Navigation Goal is in from of it.", "I have looked in RViz and it seems that my base_link is oriented in the following way\n  green axis is pointing from center of robot to it's front\n  red axis from center to right side.\n  blue axis from center to the top.", "P.S. David could you please share with us some tips and tricks how do you debug move_base navigation code?", "Thanks in advance", "Is navigation stack assuming that red axis should point from robots center to front?\nThis is how I can explain this turn at the beginning of the movement to the Navigation Goal."], "answer": [" ", " ", " The frame conventions are here:  ", "  X Y Z corresponds with Red Green Blue, so you should have the red axis pointing forward. ", " ", " ", "David,", "Thank you for your comments.", "I have changed orientation of the base_link in the way that axis X (red one) is pointing forward and axis Y (green one) is pointing on the left hand side and my robot is reaching goal in front of it almost perfectly.", "Great! Please hit accept on my answer above (and in the future don't answer your own question just to say that it worked).", "ok, I will :) \nThank you one more time"], "question_details": [" ", " ", " ", " ", " ", " ", "gmapping.launch - starts SLAM gmapping", "move_base.launch - starts move base node", "roslaunch robone_gazebo robone.launch", "roslaunch robone_description display.launch", "roslaunch robone_2dnav gmapping.launch", "roslaunch robone_2dnav move_base.launch", "In Gazebo office wall will block all view. Please right click on the working area of the Gazebo and in context menu select View->Wireframe so you\u2019ll be able to see robot in the building\u2019s room. ", "In RViz select \u201c2D Nav Goal\u201d in the toolbar. Click on point approximately 1 meter in front of robot. "], "url": "https://answers.ros.org/question/200634/simple-2d-navigation-goal-is-making-robot-to-drive-circles-endlessly/"},
{"title": "robot hits the obstacles and turns around occasionally using navigation stack", "time": "2015-01-13 12:44:04 -0600", "post_content": [" ", " ", " ", " ", "Hi everyone,", "I am using the navigation stack on my own wheeled robot. at first i used a converter to convert 220 v to 14.7 v to run my robot. using this power my robot navigated nicely in the environment. Then i equipped my robot with a 25 v LiPo battery and now when navigating, it hits the obstacles much and when i command it to go to a far distance goal, turn around much. my navigation config is as follow. what should i do? I appreciate any help.", "base_local_planners_params.yaml:", "costmap_common_params.yaml:", "global_costmap_params.yaml:", "local_costmap_params.yaml:", "I'm not sure I understand what the problem is. When the power is low, the configuration works but when the power is higher it doesn't? Does that just mean the robot is driving faster?", "Thanks David for edition.\nNo, this is not the problem. I computed the new acceleration and velocity of the robot when using battery and set them in the config. files. the robot doesn't move faster. The planner computes the global path well bu i see that the local planner gives wrong commands.", "The wrong commands of the local planner causes the robot to go far from the global path or turn around and hits the obstacles."], "answer": [" ", " ", "Are you perhaps specifying your rotation speed incorrectly? It should be in radians/s", "the approach i computed the rotation velocity is that at first i computed the maximum linear velocity of the robot and then doubled it and divide it by the distance between the wheels. 13.736 means that the robot can rotate twice (4 pi) in one second and i think it seems logical.", "That's just faster than I expected.", " ", " ", "hello,\nIt is clear that your Lipo is more powerful than your AC adapter. This is normal case :)\nSo the robot is moving in higher linear and angular velocities. and of course these velocities are more close to the values you set previously. You have two solutions:"], "answer_details": [" ", " ", " ", " ", "re-tune (actually reduce) the linear and angular velocities in your settings. or", "increase the safety distances in your setting files.", " ", " ", " ", " "], "question_code": ["max_vel_x: 2.5\nmin_vel_x: 0.5\nmax_vel_theta: 13.736\nmin_vel_theta: -13.736\nmin_in_place_rotational_vel: 2\nescape_vel: -0.5\nacc_lim_theta: 3.12\nacc_lim_x: 0.568\nacc_lim_y: 0.568\nxy_goal_tolerance: 0.5\nyaw_goal_tolerance: 3\nholonomic_robot: false\n", "obstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[ -0.146,  -0.372],\n            [ 0.354, -0.372],\n            [ 0.554, 0],\n            [0.354, 0.372],\n            [-0.146,  0.372]]\nrobot_radius: ir_of_robot\ninflation_radius: 0.6\nrobot_radius: 0.5\n\nobservation_sources: laser\n\nlaser: {sensor_frame: laser, data_type: LaserScan, topic: /scan, marking: true, clearing: true}\n", "global_costmap:\n  global_frame: /map\n  robot_base_frame: /base_footprint\n  update_frequency: 5.0\n  static_map: true\n  resolution: 0.05\n  width: 0\n  height: 10\n", "local_costmap:\n  global_frame: /odom\n  robot_base_frame: /base_footprint\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 6.0\n  height: 6.0\n  resolution: 0.05\n"], "url": "https://answers.ros.org/question/200897/robot-hits-the-obstacles-and-turns-around-occasionally-using-navigation-stack/"},
{"title": "topic_tools is not working", "time": "2014-12-10 06:30:01 -0600", "post_content": [" ", " ", "Hi all,\nI want to use topic_tools to have a RPY representation of Quaternions.\nI installed the package but it semms that topic_tools does not contain the transform package.\nI have Hydro + Ubuntu 12.04", "Could you plz state what is not working (error message, command you attempted to run, or something)...", "The error is just there's no executable. So if I run ", " it says could not find executable. The list of executables include : ", ", ", ", ", " and ", "Ok, just checked, the issue is valid for my system, too;) running hydro on Ubuntu 12.04; no \"transform\" executable in the anywhere ros installation folder /opt/ros/hydro while throttle, mux, relay etc. are there in /opt/ros/hydro/lib/topic_tools folder", "It appears the transform tool was added in commit ", " on the ", " branch. You could try building the ", " branch of ", " in your workspace to try and add this functionality to Hydro.", " Ok, is it then correct that transform is still shown in the wiki, even if I switch to hydro doc ( "], "answer": [" ", " ", "As ", " stated in the comment transform tool was first added to the indigo-devel branch, therefore is not available under ROS hydro and previous versions....", "Then, whoever has the power to change the page, please remove the wrong switches"], "question_code": ["rosrun topic_tools transform <some parameters>", "mux_<something>", "relay", "throttle", "drop", "indigo-devel", "indigo-devel", "ros_comm"], "url": "https://answers.ros.org/question/199223/topic_tools-is-not-working/"},
{"title": "looking for a robot to buy", "time": "2015-02-02 00:29:44 -0600", "post_content": [" ", " ", " ", " ", "My laboratory is going to buy a robot for research within the prize range of 7200 us dollars. Would you please suggest one with following features as much as possible?", "Thank you", "Any suggestion pls?", "I'm not aware of any robots with those features in that price range; sorry.", "Especially the arm requirement will be tricky. Decent robot arms start around 25.000 \u20ac, I believe."], "answer": [" ", " ", "In that price range getting a powerful robot is difficult, but you can have a look at the ", ". Also check ", ". It comes in a price range of around 4500 USD. You will still have to get an arm for your robot or build one from scratch like the ", ". ", "Thank you for your suggestion", " ", " ", "Hi Ahyan,", " We can help you with this requirement. Could you please mail me @  ", "Thanks and Regards,\nRavi", "ROS is about openness and sharing information. Don't be so secretive, give us some links to your robots! :-)", " We have not updated our website with the new robots which we have launched. Please take a look at one of the robots we made last year in this link:  ", " . ", "Cool, thanks!"], "question_details": [" ", " ", "Ros supported ", "Simulation feature", "Kinect sensor", "Having an arm", "Good mobility"], "url": "https://answers.ros.org/question/202211/looking-for-a-robot-to-buy/"},
{"title": "I need to write a subscribe to the cmd_vel for turtlebot", "time": "2014-11-06 19:23:24 -0600", "post_content": [" ", " ", "I need to write a subscribe to the cmd_vel topic, where am publishing to turtlebot the gazebo. So I want to return the speed turtlebot through the subscriber, not chei code examples for this case, I'm using an example of publisher where the directional control turtlebot in the gazebo. would be grateful for any help! !", "I know I have 2 variables that the publisher would velocidada the angular and linear speed, but I need a topic that returns me the speed if not possible at least the subscriber's cmd_vel help me.", "Thank you", "Voc\u00ea diz publicar e subscrever do n\u00f3 odom ? funciona mesmo com o turtlebot simulation ?", "Yes it works even with turtlebot.\n/odom gives you the current speed and current position of your robot. But I advise you...you didn't tell too much about your project", "So that's exactly what I need, first I want to publish turtlebot speed, ie move it around the map, but I'm doing it for cmd_vel topic and noticed that just do to set values for position update,", "Which is the precise speed real robot just need a publisher and a simple subscribe so I can publish and move the turtle and have it return the actual speed!"], "answer": [" ", " ", " ", " ", "using ", " you get the information you need.\nIt publishes actual position and velocity informations of your robot.", " takes just the desired velocity and moves your robot accordingly.\nUsually one publishes on ", " to move the robot and subscribes on ", " to get the wanted informations.", "I can strongly suggest to buy the following ", " because it has a very good any clear example on what you are going to do.", "Oh how wonderful is exactly what I need, my difficulty now would be like this I do know well the publisher subscriber but do not yet have examples of subscriber!", "Muito o bom o livro pena que tava querendo esse subscriber logo : /////, e sou do brasil n\u00e3o achei o livro nas livrarias daqui. Oh god vou procurar no forum ver se acho um exemplo de subscriber simples para o /odom. Muito obrigado", "You can buy the book online as PDF file. It is not expensive\nMuito obrigado para mim, but click on the small green check symbol to post the answer as given"], "answer_code": ["/odom", "/cmd_vel", "/cmd_vel", "/odom"], "url": "https://answers.ros.org/question/196933/i-need-to-write-a-subscribe-to-the-cmd_vel-for-turtlebot/"},
{"title": "Bottom camera problem", "time": "2014-11-07 06:36:36 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI use gazebo 2.2 with ROS package tum_simulator. This is a package with simulator of quadrotor AR Parrot Drone. ", "I had thought that bottom camera is not working and only front camera is OK, but I noticed that image from bottom camera don't display ground but sky. (strange) ", " Please check this screen:  ", "This is a world from Gazebo and two windows from rviz. Left image is a view from bottom camera :( and right image is a view from front camera. Black shape near window of the house is quadrotors' model.", "Have you ever heard abut this problem? Any ideas? ", "I notice that when I run this node I got some errors, I hope that this is a reason of problems with camera. ", "Please help. "], "answer": [" ", " ", " ", " ", "It was an issue with the orientation of the bottom camera on a urdf file (@green96 found the issue). You can look ", " for more information.", " ", " ", " ", " ", "Thank you for your response :) \nCould you tell me, do this errors on start have any influence? I find some info that upgrading sdformat will help and resolve it.\nI don't understand why ", "doesn't work. There is second option to install if from source but according to ", " \nwhen I do what is in Prerequisites (on this site) I will get rid of sdformat which I have now and other ROS packages like:"], "question_code": ["Error [SDF.cc:788] Missing element description for [offset]\nError [SDF.cc:788] Missing element description for [drift]\nError [SDF.cc:788] Missing element description for [driftFrequency]\n[ INFO] [1415319050.065958118, 659.736000000]: Camera Plugin (ns = /)  <tf_prefix_>, set to \"\"\n[ INFO] [1415319050.244706409, 659.736000000]: Camera Plugin (ns = /)  <tf_prefix_>, set to \"\"\nError [SDF.cc:788] Missing element description for [accelOffset]\nError [SDF.cc:788] Missing element description for [accelDriftFrequency]\nError [SDF.cc:788] Missing element description for [rateOffset]\nError [SDF.cc:788] Missing element description for [rateDriftFrequency]\nError [SDF.cc:788] Missing element description for [headingOffset]\nError [SDF.cc:788] Missing element description for [headingDriftFrequency]\nError [SDF.cc:788] Missing element description for [driftFrequency]\nError [SDF.cc:788] Missing element description for [driftFrequency]\nError [SDF.cc:788] Missing element description for [offset]\nError [SDF.cc:788] Missing element description for [driftFrequency]\nError [SDF.cc:788] Missing element description for [velocityOffset]\nError [SDF.cc:788] Missing element description for [velocityDriftFrequency]\n[ INFO] [1415319050.590175130, 659.736000000]: Using imu information on topic ardrone/imu as source of orientation and angular velocity.\n"], "answer_code": ["sudo apt-get install libsdformat2-dev\n", "  gazebo2 libsdformat-dev libsdformat1 ros-indigo-desktop-full\n  ros-indigo-gazebo-plugins ros-indigo-gazebo-ros ros-indigo-gazebo-ros-pkgs\n  ros-indigo-simulators\n"], "url": "https://answers.ros.org/question/196959/bottom-camera-problem/"},
{"title": "How to write a motor controller driver?", "time": "2014-09-04 16:28:27 -0600", "post_content": [" ", " ", "Hi All,", "I need to interface with Orion Robotics' RoboClaw motor controllers and so require some kind of driver.\nMore info on the device ", ".", "I will have several of them connected via USB and need to talk to them individually.", "I would like to write the generic driver and create a new instance for each physical device.", "I was thinking that I will need a node to run to maintain the connection to the device...?\nBut should requests be received using services or messages?", "Thanks for any help,", "Chris."], "answer": [" ", " ", "I recently wrote a python ROS node for Roboclaw that runs on Indigo. I'm cleaning it up for release but it didn't take too long to get a basic driver working with pyserial. My biggest issue has been read errors that sometimes need to trigger a reset of the port connection. You'll want to subscribe to a /cmd_vel topic with geometry_msgs/Twist message type to handle your linear and angular velocity commands, if that's suitable with your use case. I read encoder values and publish them on /odom for odometry.", "Service calls would make sense if you want to change change PID parameters, voltage cutouts, etc.", "Hello,\nI was just about doing the same thing. Do you plan to publish your work ? \nUsing Twist messages sounds fine for a differential robot powered by the RoboClaw, but what about using it as a 2 independent motors controller ? Twist messages don't make that much sense here. What would you use ?", " I am maintaining the roboclaw driver now, let me know if you have any problems with it.\n ", " ", " ", "Setting the voltages and (possibly even) getting feedback from the Integrated dual quadrature decoders are tasks that make sense using topics (publishing and subscribing). If there are configuration changes (control options?) this seems to be a task that is reasonable for service calls.", " ", " ", "It sounds like you're on the right track. Typically for anything which will be continuous streaming over a topic is recommended. Including controlling motors. "], "url": "https://answers.ros.org/question/191958/how-to-write-a-motor-controller-driver/"},
{"title": "rqt_graph colors and arrows", "time": "2014-07-08 16:26:51 -0600", "post_content": [" ", " ", " ", " ", "What is the color code for rqt_graph? \nAnd do the bi-directional arrows mean a service between two nodes or just that the node subscribes to AND publishes to the other node or both? ", "Is there some sort legend for colors, shapes, arrows etc. ? ", "Thank you"], "answer": [" ", " ", " ", " ", "As far as i know, every publish/subscribe has its own arrow, even if nodes are forming a pub/sub loop.", "for color code :", "Anything you hover on becomes red. With stuff before that blue and after green. If there's bi-directional arrows, it shows up blue-green. So that can't be right ^. \n\nI have seen orange / yellow too. The only thing I know for certain is topics are in boxes and nodes in ellipses."], "answer_details": ["Red box : Topic ", "blue circle : node publishing under the topic ", "Green circle : node subscribing to that topic", " ", " ", " ", " "], "url": "https://answers.ros.org/question/185598/rqt_graph-colors-and-arrows/"},
{"title": "What are publishers and subscribers?", "time": "2014-07-07 07:05:18 -0600", "post_content": [" ", " ", "Hi , can anyone explain the basic concepts of publishing and subscribing to me in ROS? I am still a beginner hence , I have limited knowledge in it. Thanks in advance"], "answer": [" ", " ", " ", " ", "There is a simple way to look at it. (People who don't know the basic Operating Systems concepts - like me - tend to find it utterly difficult to understand ROS. Tutorials, I feel, should not assume computer science knowledge).", "(1). Whatsapp = ROS.", "(2). Your contacts list = ROS env variables in the .bashrc file...", "Example: You are registered on whatsapp with your phone number. If you have a friend's contact, and if he happens to be on whatsapp you can see him.", "Analogy: The computer itself is 'registered' by using ROS_HOSTNAME. This is usually set to the ip of the computer whose bashrc you are looking at. If you have the ROS_MASTER_URI set, and if the computer with that ip is running a ROS master you can see it.", "(3). Whatsapp groups = ROS TOPIC\nThis analogy is crude. But, I hope it helps.", "If someone adds you to a group, you start receiving messages from them (whenever a participant posts a message). Each contact in whatsapp is analogous to a node in ROS. So, if a node registers a subscription to a particular topic, it receives data from the topic (whenever a message is posted on a topic). Note here that no-one needs to add that node to the topic. It can register a subscription, if it has the data-type of the data that is being posted on the topic (this is not true in whatsapp. The creator of a group adds participants to it.). Similarly a node can also publish on that topic (simply send a data structure to it).", "What happens when you send a message to a ROS topic?\nWe usually do,", "refer to the tutorials (", ")) for what this means.", "Then, ROS will serialize the data into bytes and send it to all subscribers (including a 'rostopic echo' - it is after-all a subscriber). But, on the receiving end, it will be assembled back together (de-serialzed). This means on the receiving end we need to specify the data-type properly. This is done in the definition of the callback function (c++).", "What is this ConstPtr? It simply expands to:", "Why is it const? We don't want to modify it even accidentally. Assume that ", " is of type ", " (only that it is not a normal pointer: it is a smart pointer) and to de-reference you can simply do ", " or ", "Another point that starters take time to understand is that one node can subscribe and publish to the same topic (just like whatsapp groups). And one node can subscribe to one topic and publish to another at the same time (One group is muted and the other is not ...", "Gosh , thank you so much for the detailed explanation ! I now have a better idea of how a topic works and although i have not covered everything , i'll definitely get a head start from here. Thank you once again and have a good day !", "You can accept the answer if it has answered your question by clicking the green tick mark ;-)\nHave a fine day ahead!", " ", " ", "Please read the ", " and the ", " for those explanations. Feel free to ask  questions about those documents if something specific is unclear.", " ", " ", " ", " ", "Hi\nI m new here too.", "So...if I understood (I did all the first Tutorials on the ROS Webpage) one should take ROS as a OS, which masters different nodes. So please let me understand better, the idea is: I create one node for one sensor, one node for the controller PID and one node for my motor DC. lets say I start a topic and sensor's node is going to publish on it. This is going later to be subscribed by the controller which makes some calculation and opens a new topic for the motor. The motor reads the right values (for istance: speed) subscribing to the second topic, which has the values coming form the controller.", "I m right?\nOr I am completely wrong about the way of working of ROS?", "I know...it takes time but it is really hard to understand, which is the correlation between ROS (form the tutorials) and a real robot/application", "regards ", "Your understanding is correct. But, ROS is much more than what you have stated. For example, the example you have stated is so simple. All three things can be performed on a single Node in different threads. Keep going. All the best!", "Ahahah, ok but one should start with simple things, or not?\nOf course can ROS more than what I wrote above, but as a concept should be right.\nAnyway...I find your statement very interesting. Could you pleas tell me, how (conceptually) do manage all those thing from one node. I mean... let say node Main collect data from the sensor, calculate the PID and rcontrol the motor, why should then publish those information on three different topic? Why topic at all?!?!?", "Suppose I wrote a node for all the stuff mentioned above..... Suppose you want to only invoke my methods when a particular condition is satisfied. Lets say, you are using an intelligent network of sensors and calculating when the user is sad. At that instant you want to simply trigger a robot to ...", "...trigger a robot to go near him. Of course, the sensing, controlling and other code has been written by me and wrapped into a ros package. Then, you have access only to the binary.. you don't have anything more. To trigger the \"robot goto x y theta\" you will need to make use of topics.", "Another example: I want to process point clouds. But, if I transport the messages and process it on another machine, there is a huge transport lag. But, instead I can write a nodelet (which is an object that will be dynamically loaded into it's own thread) and attach it to the camera_nodelet_manager", "Theoritically, the stuff still happens on the same node - in different threads. And using nodelets and subscribing to the topic /camera/depth/points means that there is a zero copy pointer pass (much like a function with pointer arguments), though I am still using topics."], "answer_code": ["ros::Publisher my_pub = nh.advertise<my_pkg::MyDataType> (\"/topic_name\", 1) // register a publisher.\n\nmypkg::MyDataType garbage_data; // Create the variable. Later you will set the variables.\n\nmy_pub.publish(garbage_data); // Send the data to the topic.\n", "void callback(const my_pkg::MyDataTypeConstPtr _msg)\n", "boost::shared_ptr <const my_pkg::MyDataType>", "_msg->", "*_msg"], "url": "https://answers.ros.org/question/185205/what-are-publishers-and-subscribers/"},
{"title": "sicktoolbox baud rate", "time": "2014-09-06 13:26:33 -0600", "post_content": [" ", " ", "Hello, ", "I am using UDOO board and a sick lms 200. I get an error when i run the sicktoolbox_wrapper which says \"failed to detect baud rate\". If not that the program waits indefintely at startup. ", "  Attempting to initialize the Sick LMS...\n  Attempting to open device @ /dev/ttyUSB0 ", "the program waits here forever or it says \"failed to detect the baud rate\". I've tried the same serial to usb converter cable on my x86 machine and it works perfectly. I sometimes get the following error ", "\" ", " Attempting to initialize the Sick LMS...\n    Attempting to open device @ /dev/ttyUSB0\n        Device opened!\n    Attempting to start buffer monitor...\n        Buffer monitor started!\n    Attempting to set requested baud rate...\nA Timeout Occurred! 2 tries remaining\nA Timeout Occurred! 1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n    Failed to set requested baud rate...\n    Attempting to detect LMS baud rate...\n        Checking 19200bps...\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: Unable to set device attributes!\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: Unable to set device attributes!\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: Unable to set device attributes!\n/opt/ros/hydro/lib/sicktoolbox_wrapper/sicklms: symbol lookup error: /opt/ros/hydro/lib/sicktoolbox_wrapper/sicklms: undefined symbol: _ZN3ros7console5printEPNS0_10FilterBaseEPvNS0_6levels5LevelEPKciS7_S7_z", "You have ", " open questions and an open bug about your problems already. Please update those before asking a new question.", "sorryy.. forgot to update them."], "answer": [" ", " ", "yup got it working. I ripped open the usb-serial converter that i had been using and turns out the chip inside was of some chinese make and wasnt compatible with the ftdi_sio module. After a while i remembered that i had an old serial to ttl logic converter which also happens to be a serial-usb converter. It uses a silabs chip compatible with cp210x. Although i was quite skeptical since it uses a max232 IC  to convert the serial signal to ttl levels. But anyhow i connected it and it worked right away. ", "On a side note though, will the max 232 IC cause me to miss some scans?? ", "Why does it say partial scan?..", "Most of the max232 series of ICs have a rated baud rate. As long as you stay below that, you shouldn't lose any data.", "I'm not sure what that message means. It may be normal. Do you think it's causing problems?", "No not really.. hopefully wont be a problem. Just wanted your say on this.", " ", " ", "I'm not too sure about the baud rate detection, but it looks like the driver is crashing due to an ABI mismatch:", "I would start with an ", " and ", " to install the latest version of the driver, and see if it still has this problem.", "i did the upgrade.. the ABI mismatch doesnt occur now but the baudrate issue is still present. I have checked the cable manually (using multimeter) as well it seems to be receiving the signals properly.Let me show you the output of dmesg | grep tty it might give u a better overview of the situation.", "[  421.762333] ftdi_sio ttyUSB0: error from flowcontrol urb\n[ 2290.997438] ftdi_sio ttyUSB0: ftdi_set_termios FAILED to set databits/stopbits/parity\n[ 2291.005463] ftdi_sio ttyUSB0: ftdi_set_termios urb failed to set baudrate\n[ 2291.012692] ftdi_sio ttyUSB0: urb failed to clear flow control", " This still looks like a driver/hardware problem. The only relevant post I've been able to find suggests using a powered USB hub:  ", "okay. I'll try doing that. Thanks alot for your help ahendrix. :) ."], "answer_code": ["Requesting partial scan data stream..\n", "Requesting partial scan data stream.. why am i getting this??..\n", "/opt/ros/hydro/lib/sicktoolbox_wrapper/sicklms: symbol lookup error: /opt/ros/hydro/lib/sicktoolbox_wrapper/sicklms: undefined symbol: _ZN3ros7console5printEPNS0_10FilterBaseEPvNS0_6levels5LevelEPKciS7_S7_z\n", "apt-get update", "apt-get upgrade"], "url": "https://answers.ros.org/question/192103/sicktoolbox-baud-rate/"},
{"title": "Problem with ardrone_driver", "time": "2014-06-12 05:09:29 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to roslaunch ardrone_driver. I have installed the ardrone_autonomy and tum_ardrone packages. when i try to roslaunch ardrone_driver i get this message:", "[ardrone_driver-1] process has died\n  [pid 19905, exit code -11, cmd\n  /opt/ros/hydro/ardrone_autonomy/bin/ardrone_driver\n  __name:=ardrone_driver __log:=/home/michael/.ros/log/1b4dd950-f22c-11e3-a388-d0df9a5bc38d/ardrone_driver-1.log].\n  log file:\n  /home/michael/.ros/log/1b4dd950-f22c-11e3-a388-d0df9a5bc38d/ardrone_driver-1*.log\n  all processes on machine have died,\n  roslaunch will exit shutting down\n  processing monitor... ... shutting\n  down processing monitor complete done", "And if i leave it long enough it says:", "Timeout when reading navdatas -\n  resending a navdata request on port\n  5554", "If someone knows any solution to this problem i would appreciate the help.", "I'm using ROS Hydro on Ubuntu 12.04. The ardrone is an AR Drone 2.0 firmware 2.3.3"], "answer": [" ", " ", " ", " ", "Which .launch file from ardone_autonomy did you try to launch?", "Just to go over the basic, you start the driver with ", ". Make sure your computer is connected to the drone's wi-fi.", "WiFi is on and connected to ardrone.i tried rosrun ardrone_autonomy ardrone_driver and i get this:\n\n", "\n\nI even tried to connect it to my iphone and update it to 2.4.8 but i get stuck as you can see at the picture above.", "open up the launch file you are trying to launch in a text editor. You'll see, that it will launch a couple of other nodes. To further investigate which node excactly is dying, you have to open up a couple of terminals and launch each node one after another. Post your findings in your question", "The other option might be to just open up the log file described in your output ;) Usually there is more detailed information there too", "The picture looks OK to me. Can you get data from the drone say, with rostopic echo ardrone/navdata? If you connect to the drone from your phone, disconnect your computer.\n\nIf you want an example for flying AR.Drone with ROS, there is this tutorial: ", "i made this video to show you where is the problem. if anyone knows any solution to this i will appreciate the help. ", "After you get those lines of [INFO], open *another* terminal, run: \"rostopic list\". You should see /ardrone/navdata among the output. If you do, the driver works. If you run \"rostopic pub -1 /ardrone/reset std_msgs/Empty\" you should see the drone LEDs change to red (green if you do it again)."], "answer_code": ["rosrun ardrone_autonomy ardrone_driver"], "url": "https://answers.ros.org/question/173485/problem-with-ardrone_driver/"},
{"title": "Development boards to use with ROS", "time": "2014-04-28 15:25:19 -0600", "post_content": [" ", " ", "There are already some questions of this type around the ROSanswers, however they are either pretty outdated or lack information. ", "Since I'm going to develop an UAV I'm searching for a development board that's small and light, as some GPIO ports with PWM (since I will need to control rotors on my  aerial vehicle), some USB ports (I would need at least 3, but if the board has less than that I can probably work around that with an usb hub), good enough performance to handle GPS, IMU and camera data and that runs Ubuntu (12.04?) and ROS.", "From what I saw till now boards like beaglebone black and raspberry pi seem to work with ROS, however I couldn't actually find projects that use this boards in order to prove how well they behave. ", "Can you help me?"], "answer": [" ", " ", "I know a number of projects have had good luck with the BeagleBone Black and the Odroid U3. The beaglebone will have more I/O, and the Odroid U3 will have significantly more processing power. I've seen a few unpublished projects now that are running an Asus Xtion device off a Odroid U3 using ROS, and doing some pretty cool work.", "I also know at least one group that is successfully running ROS on the Radxa Rock board, and it seems like a pretty solid board.", "The new Nvidia Jetson TK1 board looks interesting, but they haven't started shipping yet, so there haven't been any projects using them. It it runs Ubuntu, you should be able to install ROS on it.", "I would stay away from the Raspberry Pi. They tend to be underpowered and overhyped, and it's more difficult to install ROS on them.", "The beaglebone black seems like a great board and it was the one I prefered to begin with. Is the Ubuntu and ROS installation smooth? I have never worked with that board (in fact I have never worked with any non-pc board besides arduino). Do you have any links to ROS projects on beaglebone? Thanks", "Oh! Besides that, using an usb hub (with external power) won't be a problem with such a board, right? Cause the board only has 1 usb port.", "I got my beablebone black up and running with Ubuntu and ROS in a few hours; the installation was quite smooth. I've seen a couple of projects using them in person, but I don't think any are published online.", " ", " ", " ", " ", "I have a similar project to use on-board computer vision for obstacle avoidance. I did a little research on what development platforms where suitable for that and I finally bought a Cubieboard 2. It is powerful and cheap, but if you need connectivity you may preffer the cubieboard 3 (named truck).", "What's the main difference bettwen cubieboard 3 and 2 that make you say that? \nAnd how's the ubuntu and ROS installation on a cubieboard?", "I haven't yet managed to install Ubuntu nor ROS. I chose it because it was 1Ghz dual core and under 100$. Nonetheless, I'll use an ardupilot for low-level computation like stabilization and driving the motors, and I'm not sure how this could be accomplished using the cubieboard.", " ", " ", "Note that for quadrotor/UAV use, commonly a microcontroller system is used to perform low level control/stabilization (sending out PWM/motor commands under hard real-time constraints), while a board like the ODROID might additionally be used for additional higher level computation (image processing etc.). It is possible in principle to do it all on the computer board, but that will likely require using a hard real-time Linux variant and significant effort both in terms of hard- and software development. If you search for \"odroid autopilot\" you\u00b4ll find mostly examples of the above mentioned separation of microcontroller-based dedicated autopilot and the ODROID used additionally.", "It's a blimp system that will fly slowly and in favorable weather conditions so stabilization won't be a problem. ODROID is a great platform however since I don't really want to use another board for the motor control (weight problems). Good suggestion anyway!", " ", " ", "I'm contemplating building a new controller board for this purpose, to integrate with my java based autonomous robot controller. Nothing I've found available off the shelf has low enough latency response times, power usage, and high enough system resources to handle hundreds of simultaneous ros topics/services and equal numbers of high speed low latency hardware connections.", " ", " ", "ahendrix, you wrote: \"I also know at least one group that is successfully running ROS on the Radxa Rock board, and it seems like a pretty solid board.\"", "I recently have purchased a Rock and would like to know more about this group. If they have a website or something, can you please either post or PM me.", "Thanks!", "I think they're using the ROS ARM debs on top of the provided Ubuntu or Linaro image for the board. They don't have a website or public docs that I'm aware of; sorry.", " ", " ", " ", " ", "I had a similar project which used an on-board embedded system.", "The board we choose is Odroid XU (similar to the Odroid U3 ", " mentioned but much more powerful)\nWe install the Ubuntu 13.04 to support the ", ", the kernel is downloaded form the forum of Odroid.", "It works perfectly, we use its serial port to communicate with another board which has GPIO/PWM pins also with an IMU. There are also some usb ports on XU that allow us use usb-port device. ", "One more suggestion is the GALILEO from Intel.", "The GALILEO has its own ", ", so I think that should also be a good choice."], "url": "https://answers.ros.org/question/158421/development-boards-to-use-with-ros/"},
{"title": "Using Moveit! to Control the End of a Hydraulic Arm in Cartesian Coordinates", "time": "2014-08-21 06:14:11 -0600", "post_content": [" ", " ", " ", " ", "I have a small hydraulic arm (similar to an excavator arm) that consists of a rotating base, a boom link and a stick link. The boom and stick links move in a vertical plane whose orientation is defined by the rotating base. I would like to control the distal end (end furtherest from the base) of the stick link in two ways: Cylindrical coordinates and Cartesian Coordinates.", "What are the main steps required to achieve this? So far I have a primitive model of my robot arm and have run through the MoveIt setup guide, but I am having difficulty with working out how to implement the joint combinations and kinematic chains. So far the behaviour of MoveIt! seems to be focused in joints only. I could set up an end effector in MoveIt! but the arm does not have an end effector as such.", "Here is a screenshot of the situation.", "What I have done is that I have defined the tip of the 'stick' (where the ball is) as a universal joint consisting of two continuous joints, with a dummy link in the middle.", "I can move the arm fine by dragging the red and blue arrows. However, if I try to move the arm by moving the green arrows it will not move. Such movement should be accommodated by having the base rotate, something that is not happening. Indeed, if I drag the green arrows it 'breaks' the position so that I have to reset it before I can choose any other valid position. Note that if I select a 'random valid' joint end state it does pick states with the base rotated.", "I am finding the whole thing perplexing. Can anyone shed light on how I could drag the position of the end of the arm using all three arrows without it breaking anything?", "Thanks, Bart", "Forgive my ignorance (as in: I might not fully understand this), but wouldn't defining a kinematic chain work here? Create one including all links, so the \"rotating base, boom link and stick link\". As long as proper joints have been defined between those links, the planners should be able to work it. Don't try to add joints to groups, then links to the chain simultaneously. Define only one. The end effector isn't needed yet.", "It does work, in the sense that a solution can be found, but I cannot control the joint angles and coordinates in the way I want to, and I don't see where the MoveIt! setup assistant has any flexibility to let me adjust it to what I want.", "You're going to have to be a bit more specific than \"what you want\". Whay did you try, what didn't work and what do you want to achieve? Please update your question with that information (instead of in the comments). Also, see if the ", " on moveit cfgs can fill in any of the missing pieces."], "answer": [" ", " ", "I haven't used Moveit! much, and this might not be what is causing your specific problem, but KDL, which is what Moveit! uses for inverse kinematics by default, is really only meant for arms with at least 6 degrees of freedom. ", "For arms with less than 6 dof, I would recommend either using an ", " or solving the inverse kinematics by hand. Your arm sounds like an articulated manipulator, so you could google for \"inverse kinematics articulated manipulator\" and you will likely find a solution that is ready to use (but solving it by hand isn't too hard). Personally, if it were me, I'd just use IKFast because it is straight forward and requires less thinking lol. ", "As for solving using Cylinderical and Cartesian Coordinates, just solve using Cartesian Coordinates, then have a wrapper functions that converts cylinderical into cartesian before being input to the inverse kinematics solvers...and then convert the output of the IK solver from cartesian back to cylinderical before returning. ", "I am trying to follow the ", ", however they skip steps particularly when describing the planning groups and I can't find the worked example they use on the page. I end up with it failing to find a variable to solve.", "P.S. I am using the ", " solver. The physical arm is capable of doing this kind of 3D translation.", "particularly when describing the planning groups", "You shouldn't have to define any planning groups. IKFast plugins are generated using the URDF (converted to collada), and a specification of a chain (you use the ", " for that). The plugin you generate is then (only) usable for a planning group (chain) that contains the same links and you'll have to figure out the proper type of IK for your manipulator.", " See ( ", " ) for my experience with IKFast and MoveIt. ", "You are on the right track! Your problem is indeed caused by a missing 3D IK solver. When you drag the green arrow to the right, you are requesting a 6D pose that is unreachable, since you're not simulateously rotating it. The 3D IK finds the missing rotation for any given 3D translation."], "answer_code": ["translation3d", "openrave-robot.py <myrobot_name>.dae --info links"], "url": "https://answers.ros.org/question/190808/using-moveit-to-control-the-end-of-a-hydraulic-arm-in-cartesian-coordinates/"},
{"title": "Can Kobuki_soft be used for control real turtlebots?", "time": "2014-04-13 20:31:10 -0600", "post_content": [" ", " ", " ", " ", " has two parts: Kobuk_softapps and kobuki_softnode.  Is this pakage only used for simulation of Kokuki in gazebo? or can it be used for controlling real Kobuki robots?"], "answer": [" ", " ", "Yes. They are designed to work as simulation since kobuki itself does not have any 3d sensor and computation power. ", "If you want to try out the real robot, you might want to checkout rapps in  turtlebot_apps instead. such as turtlebot_navigation and map_nav in turtlebot_core_apps. They are not identical but provide similar functionality."], "url": "https://answers.ros.org/question/152606/can-kobuki_soft-be-used-for-control-real-turtlebots/"},
{"title": "Using sicktoolbox on hydro", "time": "2013-11-08 06:10:47 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to follow the sick laser tutorial, but I get this error \"Couldn't find executable named sicklms\" when i type rosrun sicktoolbox sicklms\nI am using ros hydro on ubuntu 12.04 \nI am following the tutorial for the sicktoolbox_wrapper.", "*I ran rosrun sicktoolbox_wrapper sicklms , that's were the output comes from", "Thank you, I am getting a timeout when trying to connecting to the Laser. This is the screen.", "I'm using a Plugable USB to RS-232 connector to connect the laser, I'll RS directly. ", "Please edit the main question instead of posting more information in the answer."], "answer": [" ", " ", "Thank you for the help, it turn out my cable was bad.", " ", " ", " ", " ", "Also just noticed you need to run this command", "Did you run:", "Also check if there's anything under:", "If you're on a pioneer, please use the connect_delay parameter:\n", "Does your laser have a solid green light, indicating it's ready for use?", "Does it work in Windows or any other software?  It's possible your cable or adapter is incorrect (RS 232, RS 422, broken wire)."], "question_code": ["*** Attempting to initialize the Sick LMS...\n    Attempting to open device @ /dev/ttyUSB0\nSickLMS2xx::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\n        Device opened!\n    Attempting to start buffer monitor...\n        Buffer monitor started!\n    Attempting to set requested baud rate...\nA Timeout Occurred! 2 tries remaining\nA Timeout Occurred! 1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n    Failed to set requested baud rate...\n    Attempting to detect LMS baud rate...\n        Checking 19200bps...\nSickLMS2xx::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\nA Timeout Occurred! 2 tries remaining\nA Timeout Occurred! 1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n        Checking 38400bps...\nSickLMS2xx::_setTerminalBaud: ioctl() failed while trying to set serial port info!\n    NOTE: This is normal when connected via USB!\nA Timeout Occurred! 2 tries remaining\nA Timeout Occurred! 1 tries remaining\nA Timeout Occurred - SickLIDAR::_sendMessageAndGetReply: Attempted max number of tries w/o success!\n        Checking 500Kbps...\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: ioctl() failed!\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: ioctl() failed!\nERROR: I/O exception - SickLMS2xx::_setTerminalBaud: ioctl() failed!\n[ERROR] [1383940385.005614654]: Initialize failed! are you using the correct device path?\nterminate called after throwing an instance of 'SickToolbox::SickThreadException'\nAborted (core dumped)\n"], "answer_code": ["rosrun sicktoolbox_wrapper sicklms\n", "sudo apt-get install ros-hydro-sicktoolbox-wrapper\n", "ls /opt/ros/hydro/lib/sicktoolbox_wrapper/\n", "rosrun sicktoolbox_wrapper sicklms _connect_delay:=30.0\n"], "url": "https://answers.ros.org/question/99096/using-sicktoolbox-on-hydro/"},
{"title": "Kinect on Kuka LWR", "time": "2014-01-28 14:13:40 -0600", "post_content": [" ", " ", " ", " ", "Has anybody succeeded in mounting a Kinect (or any piece of equipment using USB 2.0) as the end-effector of the Kuka LWR, and gotten the USB signal out through the cables that the arm provides?", "I am running into problems (USB handshake failing) doing that, and was wondering if somebody has successfully done it and, if so, if they could please describe their setup. I am trying to avoid routing a cable through the outside of the arm.", "(I know, it's not strictly a ROS question, but was hoping that the ever-expanding and enterprising ROS community might have some thoughts... :)"], "answer": [" ", " ", "I've seen some similar issues when trying to extend the USB cable on the Kinect; in particular when mounting a Kinect on a PR2.", "In particular, the USB cable on the Kinect is very close to the maximum length allowed by the spec, and this can cause signal integrity and delay problems if cable extenders are used.", "The common solution to this that I've seen is to cut and shorten the Kinect cable. This is also a good chance to replace the pesky non-standard kinect connector with a standard USB connector and a 12V power connector for your system.", "If you're just interested in testing the cable integrity, I've often found it useful to plug in a common USB device such as a thumb drive or a cell phone, and verify that it shows up and works on the USB host."], "url": "https://answers.ros.org/question/123736/kinect-on-kuka-lwr/"},
{"title": "control loop lose its desired rate", "time": "2014-04-12 21:37:51 -0600", "post_content": [" ", " ", " ", " ", "###Hi,every earnest fork##\nIn the past of couples weeks, I have been tuning robot's navigation. But it still have some troubles when going to nav goals even with a static map.", "One of those question is control loop always lose its desire rate. When I use my own notebook on the robot, I set control frequency = 8Hz, and it runs good, but when I use another computer, the warning always jump onto screen. I know the reason is either the latter computer sucked or some parameter was not properly set.", "Here it is the key configuration about the latter computer", "Intel Atom D2550 1.86GHz", "2G RAM", "and base_local_planner parameter"], "answer": [" ", " ", "Those messages are related to your computer not being able to complete all the calculations in the frequency you asked. So if you have those warnings in one machine but not in the other, that means the culprit is the lack of power on your computer. \nYou can relax some of the parameters if you want to keep the 8hz frequency, specially the last block in your config file. For instance, try to reduce the ", ", ", " and ", " and increase ", " and ", ".", "Thanks for your suggestion ."], "question_code": ["controller_frequency: 8\nrecovery_behavior_enabled: true\nclearing_rotation_allowed: true\n\nTrajectoryPlannerROS:\n   max_vel_x: 0.55\n   min_vel_x: 0.25\n   max_vel_theta: 1.2\n   min_vel_theta: -1.2\n   min_in_place_vel_theta: 0.9\n   escape_vel: -0.4\n   acc_lim_x: 2.0\n   acc_lim_y: 2.0\n   acc_lim_theta: 2.5\n\n   holonomic_robot: false\n   yaw_goal_tolerance: 0.3 # about 18 degrees\n   xy_goal_tolerance: 0.2  # 10 cm\n   latch_xy_goal_tolerance: true\n   pdist_scale: 0.8\n   gdist_scale: 0.4\n   meter_scoring: true\n\n   heading_lookahead: 0.5\n   heading_scoring: true\n   heading_scoring_timestep: 0.8\n   occdist_scale: 0.3\n   oscillation_reset_dist: 0.1\n   publish_cost_grid_pc: true\n   prune_plan: true\n\n   sim_time: 2.0\n   sim_granularity: 0.04\n   angular_sim_granularity: 0.025\n   vx_samples: 18\n   vtheta_samples: 30\n   dwa: true\n   simple_attractor: false\n"], "url": "https://answers.ros.org/question/152376/control-loop-lose-its-desired-rate/"},
{"title": "which is  the best board with ros", "time": "2013-10-24 05:28:13 -0600", "post_content": [" ", " ", " ", " ", "hi dear\nI want to use ros on a board like beagleboard,odroid,cubie board\nbut I do not have enough information about ros and the boards\ncan some one help me which board is the best and why?", "We're using an Intel NUC board with a Core i3. Works like a charm.", "Hi Hendrik\ncould you tell me about details of this board\nwhich version of linux you use on it?\nand how hard is working with it ?", "It's a pretty neat small board with an mSATA and two memory slots and a Core i3 processor. Here's /proc/cpuinfo of the board: ", " Working with it is like on a normal PC. Ubuntu 13.04 64bit runs pretty well on it."], "answer": [" ", " ", "Depends on what you want to do with it. There is are instructions for the ", " and the ", ". But it depends on your application if those boards have enough computing power."], "url": "https://answers.ros.org/question/93936/which-is-the-best-board-with-ros/"},
{"title": "Is there a way to wait for certain topics to be published in a launchfile?", "time": "2011-12-13 03:44:33 -0600", "post_content": [" ", " ", "I'd like to run several nodes within one launchfile. The system only works properly provided necessary topics are published before some nodes are launched.", "Is there a way to ensure a node is running before other node is started? Or, even better, to ensure a ", " or ", " are published before a node gets triggered within a launchfile?", "I guess, even if not with ROS, this could be possible with bash scripts. Has anyone out there already provided such functionality?", "Cheers."], "answer": [" ", " ", " ", " ", "AFAIK, the short answer is: No.\nYou cannot even control the order nodes are started.", "The better way should be that the nodes that are held back provide such functionality themselves, e.g. using a tf waitForTransform call.", " ", " ", "You can also block waiting for services to be available; the nodes that Must Come Up First No Matter What could provide some dummy service, and the nodes that Must Come Up Second No Matter What could block waiting for that service to exist; that way, they don't start processing until the first ones are up.", " ", " ", " I have figured out a way to do it. I build a script that nicely enables you to delay parts of your launch file. See my answer here, which includes clear instructions and the script:\n ", " ", " ", " I experienced a similar issue. I would find that if I run all the nodes at once on boot, most of the time, one of the stereo cameras would not work.\nTo fix this, I've created a number of scripts to stagger the launch based on what topics are running and made a public repository for anyone to use:\n ", "Take a look at the README.md for more information.", " ", " ", "Maybe this powerful yet simple tool -- ", " -- can help you to manage launch files. "], "question_code": ["tf", "topic"], "url": "https://answers.ros.org/question/12327/is-there-a-way-to-wait-for-certain-topics-to-be-published-in-a-launchfile/"},
{"title": "Problem with running p3dx robot using ROS", "time": "2013-10-23 06:42:49 -0600", "post_content": [" ", " ", " ", " ", "I am trying to connect a p3dx robot to my laptop using RosAria. The connection is being successful but the cmd/vel command is not being accepted as the TickMM is set to a different value. Can anyone suggest a method to change the TicksMM value permanently. I have tried using dynamic_reconfigure but this does not solve the problem. Thanks in advance", "what value of TicksMM allows your robot to move?", "It had worked for a TicksMM value of 128 the last time that we ran the robot", "What is the output of \n\n$ rostopic list", "no output...the cursor blinks and nothing happens..."], "answer": [" ", " ", " ", " ", "Have you tired grabbing the latest version of RosAria from github today? A fix has recently been applied for this kind of issue:\n", "In a terminal, run the ROS master server:", "In another terminal, run the robot driver (RosAria):", "In another terminal, verify topics are being published:", "If you don't see any topics listed, something is wrong with your ROS Environment settings. ", "When rosaria launches, it will set TicksMM et al from the ROS parameter server iif they are set. If the parameters are not set, it will read the values from the robot's EEPROM and set them. There are a few ways to tune them if needed:", "If you are still are having problems, post the output of rosrun rosaria RosAria and I'll see what I can do for you. :) ", " ", " ", "ashesh@ashesh-Inspiron-5521:~$ rostopic list\n/RosAria/battery_recharge_state\n/RosAria/battery_state_of_charge\n/RosAria/battery_voltage\n/RosAria/bumper_state\n/RosAria/cmd_vel\n/RosAria/motors_state\n/RosAria/parameter_descriptions\n/RosAria/parameter_updates\n/RosAria/pose\n/RosAria/sonar\n/clicked_point\n/initialpose\n/move_base_simple/goal\n/rosout\n/rosout_agg\n/tf\nashesh@ashesh-Inspiron-5521:~$ rostopic echo /RosAria/pose", "this is what I get when i choose rostopic list.\nBut when i give the second command of obtaining the pose, nothing happens", "I restarted my system and the commands started working...I could control the robot...however after 2 hours, I restarted my system again and this time its not working at all (same problem)...it pauses/hangs when we try to read any topic or pass any commands...what do you think is happening?? I am not changing anything", "what kind of usb-serial interface are you using?", "I am using a USB 2.0 interface...", "What chipset or model is it? Some have intermittent problems like that.  I've found best results with FTDI based devices and Prolific as well (not all, some are knockoffs that don't work as well.)", "am not able to figure out which chipset it is....it gives me \"Intel Corporation 7 Series/C210 Series Chipset Family USB\" in the details", " ", " ", " ", " ", "I am working along with the person who posted the original question. ", "I am able to connect to the robot but we can't get the pose as well as we can't set any velocity to make it move.\nActually, we could able to move the robot properly 2 days back with same commands in the same PC. May be we are missing something today?", "Also, I downloaded the latest version of ROSARIA today and tried. But, no success. ", "Below is the output from the terminal:", "\"_port:=/dev/ttyUSB0\" shouldn't be required as it's the default, but also shouldn't make a difference.", "And when we try to run either the pose or the velocity commands using rosrun it just gets paused and nothing happens....", "What commands are you referring to exactly?", "there were two commands that we were using:\n\"rostopic echo /RosAria/pose\"\nand \n\"rostopic pub -1 /RosAria/cmd_vel geometry_msgs/Twist '{linear:  {x: 0.05, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}' \""], "answer_details": ["dynamic_reconfigure", "launch file", "@launch via _TicksMM:=XXX", "There is a tool that lets you change the value in the robot's EEPROM from adept (I dont remember what it's called but you can find it on their wiki)", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["$ roscore\n", "$ rosrun rosaria RosAria\n", "$ rostopic list\n", "$ rostopic pub -1 /RosAria/cmd_vel geometry_msgs/Twist '{linear:  {x: 0.05, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}'\n", "raj@ubuntu:~$ rosrun rosaria RosAria _port:=/dev/ttyUSB0\n[rospack] Error: stack/package rosaria not found\n[ INFO] [1382556066.136615207]: RosAria: using port: [/dev/ttyUSB0]\nCould not connect to simulator, connecting to robot through serial port /dev/ttyUSB0.\nSyncing 0\nSyncing 1\nSyncing 2\nConnected to robot.\nName: Purdue_2052\nType: Pioneer\nSubtype: p3dx\nArConfig: Config version: 2.0\nLoaded robot parameters from p3dx.p\nArRobotConnector: Connecting to MTX batteries (if neccesary)...\nArRobotConnector: Connecting to MTX sonar (if neccesary)...\n[ INFO] [1382556066.783694745]: Setting TicksMM from ROS Parameter: 128\n[ INFO] [1382556066.787164745]: Setting DriftFactor from ROS Parameter: 0\n[ INFO] [1382556066.789165374]: Setting RevCount from ROS Parameter: 16570\n[ INFO] [1382556066.812910459]: RosAria: publishing new recharge state -1.\n[ INFO] [1382556066.813135248]: RosAria: publishing new motors state 0.\n[ INFO] [1382556066.896789292]: RosAria: publishing new motors state 1.\n"], "url": "https://answers.ros.org/question/93636/problem-with-running-p3dx-robot-using-ros/"},
{"title": "Are there any potential issues when working with older versions of ROS on the following robotic setup?", "time": "2013-09-29 08:55:11 -0600", "post_content": [" ", " ", "So I am working on a project, and the robot that I have access to is similar to a turtlebot based off an iRobot. It has a Kinect as well, ofcourse. The issue is that this particular robot was developed on a system with the following specs -", "Since this system is a \"research robot\", I don't wish to update it in case I mess it all up and end up losing the work that was done on it.", "What are the potential problems that I might face in this case -", "I understand this isn't a typical ROS related issue, but it would be helpful if anyone could point out any issues and/or solutions for this."], "answer": [" ", " ", "The older openni drivers had problems -- however, they also had stable points. Being on an older Ubuntu release with an older ROS install, you might hit one of those stable points. I think you'll just have to test it and see.", "That Atom is not all that powerful, depending on what you are doing, you may want to turn down the Kinect frame rate or image size (see the openni_camera driver docs) to help out your code. Regardless, the question above is far too broad -- you can certainly find ", " motion planning algorithm for which you cannot run it on that Atom, and you can certainly find several motion planning algorithms for which the Atom is overkill -- it really all depends on what you are doing and at what framerate/image size.", "Thank you! I will look into it further then.", " - I was provided with a replacement laptop. The replacement seems to be worse than the above configuration. \n\nIntel Pentium M, 1.70 GHz, 1 GB RAM\n\nI guess this won't do either?"], "question_details": ["2x Intel Atom CPU N270, 1.60 GHz, 2GB", "Ubuntu 11.10", "ROS 1.6.8", "Do I need to update it in order to be\nable to work with Kinect properly\n(considering I won't get much support\nfor the old ROS)?", "Will implementing any motion planning\nalgorithm (utilizing real-time info\nfrom the kinect) on this system lead\nto performance issues?"], "url": "https://answers.ros.org/question/84831/are-there-any-potential-issues-when-working-with-older-versions-of-ros-on-the-following-robotic-setup/"},
{"title": "How can I use roslisp on Hydro?", "time": "2013-09-25 10:18:53 -0600", "post_content": [" ", " ", "We would like to use roslisp with Hydro. I have not used it before. ", "It looks like some of the pieces are currently missing. The wiki documentation is not up to date since Fuerte: ", "There is a ros-hydro-roslisp Ubuntu package for Precise. But, I don't see any packages like these:", "Are those packages still needed?", "The tutorial page still points to the ", " SVN repository. Is that correct? I did find a ", " repo on github that appears to support catkin. Is that the current source? Where are the tutorials now?", "I am happy to help with fixing things up, but need some direction to get started.", "@georg-bartels can you help?"], "answer": [" ", " ", "I think the last release of roslisp was for Groovy. My guess is that nobody really had time to take care of releasing it for Hydro. Anyway, you should be able to use roslisp from source, given lisp messages are still generated.", "The only system dependency roslisp requires is ", ", so make sure that it is installed:", "Then clone the ", " into your Catkin workspace and build it. To use it, you need to start sbcl with a custom init file:", "I definitely suggest to use Slime, a really powerful Emacs mode for Lisp coding (I think nowadays there is also a vim port available). If you have rosemacs installed, you shouldn't need to configure anything. Just start roslisp with ", ". rosemacs was hosted on kforge, so I think the official repository is gone by now, but you could check out ", ".", "If you don't have rosemacs, make sure to set the lisp binary to load ", " as above, e.g. by putting the following line in your emacs config:", "Unfortunately I don't know how the state of the tutorials is. They haven't been touched in a while and I don't have time for too much maintenance at the moment. But I guess the code in the tutorial is still valid.", "At least the roslisp package is released for Hydro as far as I can see: ", "It is, ros-hydro-roslisp installs on Precise. I was asking about the roslisp_common, roslisp_support and roslisp_tutorials packages. The tutorials refer to them. Are they still needed? Where are the sources?", "Thanks for the slime recommendation, rosemacs is great. For the record, it is installable as a rosemacs-py Debian package.", "roslisp_common and roslisp_support are dry packages in Groovy. Nobody has catkinized them and therefore they are not available in Hydro. roslisp_tutorials seems to be not even in Groovy.", "`roslisp_support` is not required anymore. It contained sbcl and the roslisp tests. In recent Ubuntu versions, SBCL is new enough to be used. `roslisp_common` containts cl_tf and and an action client and server. To use it, you need to install the dry stack from source at the moment.", "OK, where is the current source? Still on code.ros.org?", "Yes. You can check it out from ", " you need to migrate these repos somewhere else asap since the Willow Garage servers can go away at any time (as announced on ros-users before)."], "question_code": ["ros-hydro-roslisp-common\nros-hydro-roslisp-support\nros-hydro-roslisp-tutorials\n"], "answer_code": ["sbcl", "sudo apt-get install sbcl\n", "sbcl --load <path to roslisp>/scripts/roslisp-sbcl-init\n", "M-x slime-ros", "roslisp-sbcl-init", "(setq inferior-lisp-program \"sbcl --load <path to roslisp>/scripts/roslisp-sbcl-init\")\n"], "url": "https://answers.ros.org/question/83296/how-can-i-use-roslisp-on-hydro/"},
{"title": "Roomba 500 series installation problem", "time": "2012-02-16 01:11:03 -0600", "post_content": [" ", " ", "Dear all,", "I am trying to install roomba_500_series in a fresh installation of Ubuntu 11.10 oneiric, which comprises the turtlebot.eu laptop.", "The problem I am facing is that the Source: svn ", " no longer works.", "Can someone point me out to the correct source?", "Regards,", "CG"], "answer": [" ", " ", " ", " ", "In general, it always helps to copy-paste complete error messages. 'no longer works' is not really informative. See ", " for guidelines.", "Here at TUM, we also have a couple of EU turtlebots and decided not to use the roomba_500_series node because it has some problems that are really critical for us (e.g. not having a watchdog to stop the robot when no command is received for some time, different topic layout than the original turtlebot, ...). Instead, we made a few patches to turtlebot_node to make it work with the roombas. The patches are already upstream and should be in the newest release of the turtlebot stack. You can bring the robot up by using this launch file:", "You can also change the minimal.launch file in the package turtlebot_bringup to set the above node parameters. But don't forget to remove robot_pose_ekf in that case since the EU turtlebots don't have a gyro Then Willow Garage's original turtlebot apps should all work.", " Links to our patches:", "Dear Lorenz, I also belong in TUM. Which is your Institute?", "Computer science, our group is the Intelligent Autonomous Systems Group (", ").", "these patches are released in turtlebot 0.9", " ", " ", "AFAIK, the roomba_robot package works just fine ! I used the Roomba 500 series drivers on a Roomba 560 robot, details can be found here -- ", " ", " ", "Try the latest electric version -  this is supposed to work better.  Especially since the diamondback stacks didn't account for differences between the icreate and 500 series hardware differences.", " ", " ", "Never mind, Lorenz,  I just found the robot types file.  Danke.", "I am not getting the electric stack to work.  I am getting both a baud rate error and a complaint about the laptop battery being in Ma  instead of Mw -  is this some EU standard;-) ?", " ", " ", "I spent a couple of hours yesterday trying to get the electric stacks to work. I got the\nlatest releases for electric,  turtlebot-robot, turtlebot-viz.   While I got the diamond back stacks to work (teleop, Dashboard)  with the only change being the baud rate in the\ndriver section,  I couldn't get any part of the electric stacks to work.  I did change the baud rate in the electric stacks.", "In using the minimal.launch or the launch file suggested by Lorenz,  none of the functionality is there.  There are SCI errors,  the laptop battery can't be read, the diagnostics fail. On either launch, the dash board that comes up is entirely greyed out. I would guess that the baud rate is not being accounted for, but that doesn't explain why it can't read the laptop battery.", "So I am falling back to diamondback to continue my Turtlebot EU development.", "We noticed that odometry on the roombas is really bad, just changing the baud rate might cause problems there. After the next release of the turtlebot stack, it should be fully compatible to the EU flavor. minimal.launch should definitely work. SCI errors should be gone after patch #121 (link above)", " ", " ", "Lorenz, I like your suggestion - This is a good idea -  I am hoping robot_type roomba figures out the proper baud rate for the sci interface, and there is a robot type create or turtle,", "If using the turtlebot distribution on a 500 series,  you need to change the SCI baud rate.  In the turtlebot_driver.py file  change self.sci from 57600  to 115200.  I had to do this for my home built interface cable which used a Freeduino Bib.", "Also -  what is TUM?", "We tested our patches over a few weeks and they seem to be fine. Of course, we added an option for setting the baud rate. We also had to patch the sensor-data reader and handling of odometry. I'll add links to the tickets in my answer. TUM = Technische Universit\u00e4t M\u00fcnchen"], "answer_details": [" (applied)", " (applied)", " (applied)", " (applied)", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["<launch>\n  <node pkg=\"turtlebot_node\" type=\"turtlebot_node.py\" name=\"turtlebot_node\" output=\"screen\">\n    <rosparam>\n      robot_type: roomba\n      publish_tf: true\n      has_gyro: false\n    </rosparam>\n  </node>\n</launch>\n"], "url": "https://answers.ros.org/question/27755/roomba-500-series-installation-problem/"},
{"title": "Kinect not getting power from Irobot create base. is it openni problem??", "time": "2013-07-22 22:23:12 -0600", "post_content": [" ", " ", " ", " ", "now i use turtlebot(create). but kinect don't get power from Irobot create base.\nusb version is 2.0, and ubuntu 12.04 , ROS fuerte.\nKinect is a red status light.\nis this problem openni problem?? if It is, please let me know what to do", " \n\"[ INFO] [1375071903.030941613]: No devices connected.... waiting for devices to be connected\"", " ", "Node: /openni_driver\nTime: 1375071912.050908274\nSeverity: Info\nLocation: /tmp/buildd/ros-fuerte-openni-camera-1.8.6/debian/ros-fuerte-openni-camera/opt/ros/fuerte/stacks/openni_camera/src/nodelets/driver.cpp:DriverNodelet::setupDevice:218\nPublished Topics: /rosout", "No devices connected.... waiting for devices to be connected", "what is the problem?? if you know that, please let me know.", "thank you ^^", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "answer": [" ", " ", " ", " ", "I am facing the same problem that kinect not getting power from Irobot create base....\n", " ", "I read to a tutorial by Prof. Jason O'Kane he says that : \n", "I tried it but this was not working. The create base is connected to ttyACM0 port as the serial to USB connector definition.\nThen I checked the power supply of pin 10-11-12 of DB25 connector and found that the above command working well and I robot create giving power supply of 14V DC. So it is problem of 12V DC regulator circuit board. Which I will replace soon.", "If you get to resolve the problem then please do let me too by posting the solution. Thanking you.", "sure, I operated 'rosrun create_node kinect_breaker_enabler.py'. but I don't solve it.\\\nif I know that, I will notice you", "I sended mail about my question.", "to whom you sent the e-mail?", "turtlebot company -> iheartenginerring\n\nI attached contents below", "I found this solution. My power cable(from create base to kinect) is wrong.\n+12v cable and ground cable connected incorrectly.\nCould you look at this part again????", "is it working now?\nPlease e-mail me the detail about the connector n circuit you are using and steps to run. mailID- ", "I sended e-mail to you ^^", " ", " ", " ", " ", "the cable connecting from create base to kinect is wrong.", "I replace new cable and then kinect is operating ^^", "it is the same script file (kinect_breaker_enabler.py) that i told you earlier. \nthis file is getting run through command  ", "\n\nrosrun create_node kinect_breaker_enabler.py ", "\n\ncan you tell them that you had run this command but it does not worked", "I also use **rosrun create_node kinect_breaker_enabler.py** but it does not worked..\nDo you have a multi-meter to test the voltage? and test the voltage??", "yes i have let me test it but frankly speaking if the proper voltage is coming on port then the LED on the connector of kinect will glow well and will indicate that power is coming properly so no need to test it with multi-meter", "could you ask them to get new answer??????????", "no, I didn't receive reply. if I receive it, I'll tell you"], "answer_code": ["**rosrun create_node kinect_breaker_enabler.py**\n"], "url": "https://answers.ros.org/question/68197/kinect-not-getting-power-from-irobot-create-base-is-it-openni-problem/"},
{"title": "Hokuyo utm-30lx restart command", "time": "2013-04-09 22:43:03 -0600", "post_content": [" ", " ", " ", " ", "Hello :)", "I have a Hokuyo utm-30lx mounted on the patrolbot from mobilerobots, it was working quite nice with a 12V external power supply, then I decided to connect it to the 12V PWR right power I/O bay  and suddenly I got no power from the bay and I had to turn the 12V power on again from the LCD panel. But, suddenly my computer could not read the device /dev/ttyAMC0", "I have tried with to connect it under  windows and it is the same problem.", "Just wondering whether there is a way to find out the error and make the device work again", "I am using ROSARIA in ROS. ", "Is there any command to the laser so I can restart the device?", "Cheers :)"], "answer": [" ", " ", "Ehm, that doesn't sound like a software problem. I don't know your robot, but suddenly no power shouldn't happen (I guess). The only thing I could image if there is a power cycle is that the device switched. Check with ", " if that happened. There is then usually a /dev/ttyACM1 or similar."], "answer_code": ["dmesg"], "url": "https://answers.ros.org/question/60368/hokuyo-utm-30lx-restart-command/"},
{"title": "TurtleBot unable to contact my own server", "time": "2011-08-16 06:28:16 -0600", "post_content": [" ", " ", " ", " ", "A question from Marc Howard:", "I am having a problem getting the turtlebot minimal.launch program to run.  It gives me the following error:", "A common cause is that the machine cannot ping itself.  Please check\nfor errors by running:", "For more tips, please see", "I have checked both .bashrc files and they have the following lines in them:", "Laptop .bashrc file:", "Workstation .bashrc file:", "I have tried replacing those with the actual IP addresses and that did not work either, and documentation I found on the net advised that the way I have it above was correct.  I also don't understand on a fundamental level where it assigns \"IP_OF_LAPTOP\" or\"IP_OF_WORKSTATION\"  to the actual number.  ", "I went through the NetworkSetup tutorial and the EnvirmonmentVariables turtorial with no luck.  My computers also do not have names on the network (aka they cannot be accessed with a URL other than their ip address.  Do you have any suggestions?  Thank you very much!"], "answer": [" ", " ", "In a tutorial when we write something in all caps like IP_OF_LAPTOP, we mean that you should replace IP_OF_LAPTOP with your actual IP. You can find out the IP of a computer in Ubuntu by typing ifconfig in a terminal. ", "you should see something like: ", "In this example the IP_OF_LAPTOP is 192.168.1.4", " ", " ", " ", " ", "I am having a similar problem but I have put in the correct addresses (I believe):", "Unable to contact my own server at [http://http:10.5.6.187:59339/].\nWorkstation IP = 10.5.6.93\nTB IP = 10.5.6.187", "I have dashboard up in Full mode with 0,1,2 all green.", "They can ping themselves no problem as well.", "Well I figured it out. Realized I had accidentally added http: to the turtlebot hostname. ", "I have a similar problem. I redid the echo export commands for both the Host and Master, but it doesn't change the values... I feel like when I use roslaunch turtlebot_dashboard, it's getting these variables from a different file rather than .bashrc...", "What do you mean by \"it doesn't change the value\"? If you do first export, then echo you show see, if the environment variable gets set or not. Further than that, make sure that you do roslaunch in the terminal you set the environment variables in.", " ", " ", "I had a similar problem where my workstation would not connect to master. My silly issue was because I forgot the ", " before the IP address in ", "."], "question_code": ["Unable to contact my own server at [http://IP_OF_LAPTOP:42375/].\nThis usually means that the network is not configured properly.\n", "ping ip_of_laptop\n", "export ROS_MASTER_URI=http://IP_OF_LAPTOP:11311\nexport ROS_HOSTNAME=IP_OF_LAPTOP\n", "export ROS_MASTER_URI=http://IP_OF_LAPTOP:11311\nexport ROS_HOSTNAME=IP_OF_WORKSTATION\n"], "answer_code": ["lo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:88474362 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:88474362 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:9302480192 (9.3 GB)  TX bytes:9302480192 (9.3 GB)\n\nwlan0     Link encap:Ethernet  HWaddr 48:5d:60:7d:22:e6  \n          inet addr:192.168.1.4  Bcast:192.168.1.255  Mask:255.255.255.0\n          inet6 addr: fe80::4a5d:60ff:fe7d:22e6/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:1093031 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:1422901 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:430045105 (430.0 MB)  TX bytes:1327311556 (1.3 GB)\n", "WORKSTATION:~$ echo $ROS_MASTER_URI\nhttp://10.5.6.187:11311\nWORKSTATION:~$ echo $ROS_HOSTNAME\n10.5.6.93\n\nturtlebot@turtlebot:~$ echo $ROS_MASTER_URI\nhttp://10.5.6.187:11311\nturtlebot@turtlebot:~$ echo $ROS_HOSTNAME\nhttp:10.5.6.187\n", "http://", "ROS_MASTER_URI"], "url": "https://answers.ros.org/question/10920/turtlebot-unable-to-contact-my-own-server/"},
{"title": "Problem using the provided groovy RPi Image", "time": "2013-05-28 03:16:10 -0600", "post_content": [" ", " ", " ", " ", "Hi Everyone!", "So I was going for a quick test using the image provided here: ros.org/wiki/groovy/Installation/Raspbian/Source (sorry, I'm new here and not allowed to post Hyperlinks). After I flashed it to an SDcard and booted I found that ROS does not seem to be present at all (echo $ROS_DISTRO returns nothing, the different ROS_ variables are not present). I also tried setting up my WiFi, with no success (Dongle visible in lsusb, but not in lsmod, nor in ifconfig). When I tried to use the wpa_gui from the X environment, the Pi did not respond to keyboard or mouse input anymore. All the devices mentioned before are connected using a powered USB-hub. As a matter of fact, the Pi itself is powered from that hub.", "So now I am wondering: either I am doing strange mistakes at many places or the image provided does not work (the way I think it should - Maybe I misunderstood the introductory text and only the source should be present. But what sense would be in that?). Has anyone tested the functionality of the mentioned image and could give me a hint as to what it is?", "Cheers"], "answer": [" ", " ", "The Raspi will not work will all WiFi dongles, check ", " for which have been tested. You may of blown the polyfuse if you drew too much current over 150 mA without external power.  Does it work without the WiFi?  I use that image that kalectro kindly provided and have no problems with WiFi or ROS. Source the catkin folders also since I believe most the packages are compiled from source.", "The WiFi Dongle works with the Pi, I used it with the other image I mentioned above (that one did not have tf, among others, which I need to use). But I will have a look at the kalectro image, thanks for the hint!", "Here is the link to his original post on the Raspi forums ", " ", " ", " ", " ", "Thanks so much to balto and davinci! ", "The download balto indicated (http://www.raspberrypi.org/phpBB3/viewtopic.php?f=37&t=6552&p=354127) is a little cumbersome (the fourth attempt did finally work w/o a CRC error), but the image itself works like a charm. Of course, the setup.bash needs to be sourced to use groovy (I used ", " and ", ", the latter may be sufficient, followed by the usual ", "), but WiFi works instantly (with a Realtek-Chipset on the WiFi dongle). It does, however, barely fit on a 16GB SD card. As stated on the RPi forum, one might want to remove the  two swapfiles used during compilation from \"/\" (", ") to gain around 4GB of space.", "EDIT: After some hazzle  with the Image I swapped over to a stock image and installed ROS using a repository. The steps taken are described in a tutorial: ", " (humm... in the preview the link worked, but for the final version, it is parsed completely different. Funny!) - it does not lead to a full ROS install, but should work for most basic stuff. You will most likely only want the first steps.", "Your method is better, I just did a fresh install.  Thank you!", " ", " ", "Did you do:", "The other problem, the RPi has stability issues with power supplies as you probably know. Perhaps try to power the RPi from a seperate power supply. Did the wifi work on other distributions? ", "OK, so this did not work the way I expected it to. sourcing does actually help, thanks for that.\nBut for the other Point: the exact same setup worked with another image, yes. However, I will try a different power supply.", "So, I tried some different methods to supply power (Mobile Phone Charger, USB 3.0 Port) and ended up with the same result - WiFi dongle is not usable (the driver is installed), startx starts x, but my input devices won't work.", "I also got a new error message popping up when starting the GUI, some Openbox Syntax Error in an XML file. I guess, this is uncorrelated to the other problem."], "answer_code": ["source /opt/ros/groovy/ros_catkin_ws/install_isolated/setup.bash\n", "echo \"source ~/catkin_ws/devel/setup.bash >> ~/.bashrc", "source ~/.bashrc", "sudo rm /swapfile*", "http://www.ros.org/wiki/asctec_mav_framework/Tutorials/Raspberry%20Pi%20Setup", "echo \"source /opt/ros/groovy/setup.bash\" >> ~/.bashrc\nsource ~/.bashrc\n"], "url": "https://answers.ros.org/question/63705/problem-using-the-provided-groovy-rpi-image/"},
{"title": "Confused about getting XV11 laser data into rviz", "time": "2013-05-05 16:37:25 -0600", "post_content": [" ", " ", " ", " ", "Okay, so I finally know enough about ROS to totally tie myself up in knots here. \nI've got my Neato laser up and running and pumping but data to the /scan topic but not seeing anything in rviz. ", "I'm running Groovy on a Beaglebone w/ Precise LTS (12.04). ", "I'm running rviz remotely on a separate laptop, also running Groovy on Precise LTS. ", "I have a Neato LIDAR hooked up to it, running the cwru-ros-pkg XV11 laser driver.\nrostopic shows the laser successfully publishing to a topic called \"/scan\":", "I'm getting data from rostopic echo on both the Beaglebone and the laptop I'm running rviz on:", "But nothing is showing up in rviz. ", "I'm using the cwru-ros-pkg xv11_laser_driver which publishes to the /scan topic, not sensor_msgs/LaserScan.  I'm sure I'm missing a big chunk of something here but am not exactly sure what. ", "Any help from those who've been able to get data from their XV11 laser into rviz or other parts of the ROS stack would be appreciated.", "Thanks!\n'dillo"], "answer": [" ", " ", "The problem showing the data in RViz was most likely caused by having \"fixed frame\" in rviz set to the wrong thing.  You can see the frame_id in the laser message headers is \"neato_laser\".  RViz either needs to have that as the fixed frame, or it needs TF messages (on topic /tf) being published which show the relationship between the \"neato_laser\" coordinate frame and the fixed frame.  Typically these are specified in a URDF describing the robot's various links and sensors (each with its own coordinate frame name) and published by robot_state_publisher.  Then there is \"amcl\" which does localization which can give the relationship between the robot's base_link and the map.  Since \"map\" is a very common fixed frame to use, that is the default in rviz.", " ", " ", "This refers to the CWRU package for the XV-11 LDS.", "I got rviz to work using fixed frame \"neato_laser\"  and topic \"scan\"  I am using a variable voltage regulator to control the speed and the data is intermittent.\nAbout a 1/3 of the time I get valid data, and the rest of the time I get noise. Not sure if this is due to a speed control problem, interface or a faulty sensor.  It was working much better with Fergie's neato_node but the XV-11 main board stopped talking on the USB.", "I built it in Groovy using just 'make' in a ros_workspace. Not sure how Dillo built his, but his data looks much cleaner. I am going to try a becnh vailable power supply to see if that helps.", "Procedure:", "git clone the cwru package -> make semi-stable,  make xv_11_laser_driver.\n  source the setup.bash - check to see that you can find the driver.", "roscore", "rosrun xv_11_laser_driver  neato_laser_publisher -port:=/dev/ttyUSB0", "rosrun rviz rviz", "DrBot - I had the same problem, and fixed this by adding the _firmware_version:=2 parameter, as explained here: ", " ", " ", "The topic name is ", ", I expect its message type is ", ". You can verify that with this command:", "To display it with ", ", follow ", "."], "question_code": ["ubuntu@arm:~$ rostopic list\n/rosout\n/rosout_agg\n/scan\nubuntu@arm:~$\n", "armadilo@talon:~$ rostopic echo /scan\nheader: \n  seq: 15555\n  stamp: \n    secs: 1367714272\n    nsecs: 996305089\n  frame_id: neato_laser\nangle_min: 0.0\nangle_max: 6.28318548203\nangle_increment: 0.0174532923847\ntime_increment: 0.000163100005011\nscan_time: 0.0\nrange_min: 0.0599999986589\nrange_max: 5.0\nranges: [0.2980000078678131, 0.3059999942779541, 0.3140000104904175, 0.3240000009536743, 0.3330000042915344, 0.34200000762939453, 0.35100001096725464, 0.3610000014305115\n"], "answer_code": ["/scan", "sensor_msgs/LaserScan", "$ rostopic type /scan\n"], "url": "https://answers.ros.org/question/62152/confused-about-getting-xv11-laser-data-into-rviz/"},
{"title": "How to set up OpenCV(which is part of ROS) in Qt creator in Ubuntu12.04", "time": "2013-04-14 04:14:43 -0600", "post_content": [" ", " ", " ", " ", "I have ", " in my computer. I want to use an IDE for my projects, I already have installed Qt creator from Ubuntu Software center.", "Is it possible to use ROS opencv in an IDE and work. If yes,can somebody help me with ", "Thank you"], "answer": [" ", " ", "Hi! ", "Here is a little trick that I used (if anyone knows a \"cleaner\" way to do this, please let us know):", "1- Create a symbolic link to ", " inside the directory ", ".", "2- Create the appropriate .pro file for your Qt project", "If the file ", " already exists you can copy ROS's file with another name, for instance: ", " and then use ", "Thank you for the answer....but when I do cd /usr/local/lib/pkgconfig , I get ", " .... am I supposed to create a new directory there ??", "Yes, if it does not exist you should create it. Remember to do it as root (sudo)", "I did exactly as you said... earlier when I tried to run the program it would say ", "... Now it s fine but doesnt display the output image... a terminal opens and stays as such... nothing happens...... ", "Do i need to take care of some other settings in Qt creator such as build/Release configuration, etc this is the progam (http://i.imgur.com/tD8nWzY.png) I tried and .pro file (http://i.imgur.com/sprITVF.png)", "I tested your code, when I launch it from within Qt environment (using the green \"Play\" button) a terminal appears and nothing else happens, as you mentioned.But, if I execute the program from a console (find the folder where the project is built and execute ./Hello_World), then it works!", "Thank u.....yes it works fine if I do as u said.....", " The motive of going for an IDE is quicker access through the GUI...am I right? Any idea where I might hav missed some setting or something ?? did you get any similar problem??", "Finally I got it working... I changed some settings in Qt creator.... In ", "... Now the code works as I click the green \"Play\" button !"], "answer_code": ["cd /usr/local/lib/pkgconfig\nln -s /opt/ros/fuerte/lib/pkgconfig/opencv.pc opencv.pc\n", "TARGET= MyQtProject\nTEMPLATE = APP\n\nSOURCES += main.cpp\n\nCONFIG += link_pkgconfig\nPKGCONFIG += opencv\n"], "url": "https://answers.ros.org/question/60630/how-to-set-up-opencvwhich-is-part-of-ros-in-qt-creator-in-ubuntu1204/"},
{"title": "Fuerte and FCL", "time": "2013-05-07 08:15:59 -0600", "post_content": [" ", " ", "I am running on Fuerte (Groovy is not an option) and I would like to use FCL (Flexible Collision Library). I've installed the ros-fuerte-fcl package with apt-get, but cannot find any information on how to actually use it. Can someone point me in the right direction?", "I've also tried the standalone FCL package, and it builds fine, and it runs the included test fine, but there doesn't seem to be any examples or documentation to it, either."], "answer": [" ", " ", "When I was getting started using FCL, I found these references helpful:", "From the ROS side, you'll need to make sure ", " is listed as a dependency in your package's ", ".  Also, the ", " should include a ", " line to link the library to your executable.  As shown in the FCL test programs, you'll need to explicitly link individual header files for the specific FCL modules you want to use: ", ", etc.", "To start, I'd try creating a small test program.  You could construct a few primitives (spheres or cubes) of known geometry, at known distances, and call the FCL methods to check for collisions or distances.  This is essentially what is done in the FCL unit-test programs.  This test program will allow you to verify that you have your ROS environment set correctly (dependencies, includes, linking, etc.) and at least a basic working implementation of FCL.  From there, you can expand the FCL portion to more directly implement what you're trying to do for your application.", "Mostly, I had to stumble my way through getting started.  But once I had some working code, the library was fairly easy to use and very powerful!  We're doing distance checks between large meshes at a very acceptable rate.  Other than the initial setup headaches, I've been very pleased.", "Great information, thanks!"], "answer_details": ["the original ", " documents many of the core concepts", "the ", " on the ROS wiki.  Especially under the ", ", as many FCL capabilities are exposed as bare functions in the root ", " namespace.", "the various ", " unit-test programs, which can be reviewed ", " ", " ", " ", " "], "answer_code": ["fcl", "test_*.cpp", "fcl", "manifest.xml", "CMakeLists.txt", "target_link_libraries(my_exe fcl)", "#include \"fcl/collision_node.h\""], "url": "https://answers.ros.org/question/62332/fuerte-and-fcl/"},
{"title": "motor encoder", "time": "2013-02-26 12:19:55 -0600", "post_content": [" ", " ", " ", " ", "can i use stepper motor instead of  dc motor with encoder to measure speed and the direction of the robot?", "to built eddie robot what type of encoder is used and what informtatiom it gives?", "can i make the encoder on my own? how to do that give some idea?"], "answer": [" ", " ", "To answer your first question, Yes you can use a stepper motor instead of a DC motor. They are fundamentally different motors and controls. With a DC motor your applied voltage will give you the motor speed and current will give you motor torque. However if you want to control position you will need a position rotary encoder of some type and utilise this to generate your motor signal via a closed loop.", "With a Stepper motor however the drive you apply will specify the position it will occupy. The drive you apply will be a repeating pattern to make the motor increment its position by a step (hence stepper motor). By increasing the rate at which you change the drive commands you can make the motor step faster increasing its speed. A normal small type stepper motor usually has a max speed of about 200rpm after which the rate at which you change the commands becomes too fast for the motor to generate the required forces to turn the motor.", "With a stepper motor you will not need an encoder if you can assume in your control code that the motor has increased a step as planned, (this is open loop control). If you need assurance that the motor is working as planned you can add an encoder if you like.", "For both motors, digital control will require a driver package to deliver the power required to operate the motors. ", " has a good range. ", "Not sure of eddie robot, but you should check out Austria Microsystem's AS5040 magnetic encoder. They are quite cheap an offer a range of encoding options from absolute to incremental encoding to 10 bits. ", " ", " ", " ", "Another cheap DIY solution is to use a mouse, a laser or a ball mouse mounted under the robot. They are reasonably easy to read out with an arduino. ", "I love that idea!", " ", " ", "I have heard of people using stepper motors, but don't know how it's done.", "The robot I built is a lot like EddieBot.  (", ")  I used these encoders from RobotShop:", " with these wheels & motors: ", "  I used an Arduino to read out the encoders.", "I've tried cheaper encoders (in this project: ", "/), but the accuracy was not very good.  It was a good way to get started and learn though."], "url": "https://answers.ros.org/question/56380/motor-encoder/"},
{"title": "Inverse Kinematics algorithm implementation", "time": "2013-03-16 10:55:13 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Dear Friends,", "I am newbie to ROS and Ubuntu and a Masters Student. I am basically a windows user and have now switched to Ubuntu to learn ROS. But ROS + UBUNTU = (GREAT FUN); ", "As my masters project I have designed an Inverse Kinematics algorithm for joints. My supervisor advised me to implement the same using ROS showing the algorithm in action using Simulation. I completed the tutorials and was able to understand most of the ROS fundamental and now my questions are", "Q1 - Am I on the right direction? Do I need ROS to show the algorithm in action using Simulation? I have already done this using Flash and Webgl. ", "Q2 - Are there any tutorials for ROS to get started with regards to Kinematics and Joints ?", "Regards,", "Srinivasan!"], "answer": [" ", " ", "ROS is a great way for you to test/demonstrate your inverse kinematics algorithms!", "The ", " package provides a plugin interface for integrating different kinematics algorithms into the standard ROS ecosystem.  See the ", " for an introduction on how to set up your own kinematics plugin.", "Once you have implemented the standard kinematics-plugin interface for your algorithms, then you can easily swap between the default ROS kinematics methods and your own algorithms, for comparison and benchmarking.", "You can test your kinematics in a few different ways:", "1) ", " - create a ", " package for your robot arm, then use the ", " to interactively drag the end-effector and observe the joint solutions computed by your IK algorithms.", "2) ", " - write a program to call the ", " for a set of random, critical, or evenly-spaced poses to evaluate the performance (timing, accuracy, etc.) of your kinematics algorithms.  One easy method is to iterate through all valid joint-angle combinations, and calculate a round-trip kinematic solution (forward-kinematics followed by inverse-kinematics).  Comparing the IK results against the original joint-angles can help evaluate your IK algorithms.", "Good luck!  Hope these pointers are helpful.", "Thank you very very much for having taken time to have answered my queries despite the stupidity of mine for having not included in the query about the technical details like the ubuntu and the ros verison.", "The details about the versions of the softwares I am using", "Ubuntu 13.04\nROS - Groovy", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Since you are talking about kinematics and not dynamics, I recommend you don't bother with a Gazebo simulation at all. It's a really nice tool, but I don't see the need for you.", "You can make things move and visualise with RViz only - just feed your motion commands directly into the current state (i.e. joint position/velocity command = joint position/velocity state).", "Have a look at the ", " package for more details. We use this tool a lot in order to visualise our robot models.", "Further than that I strongly recommend to take a look at the ", ". It contains various IK solvers and is also used in multiple kinematics packages/tools in ROS (e.g. ", "). Also worth taking a look at is the IKFast module from ", ". The major difference between both is that the solvers in KDL are iterative, while the IKfast plugin uses an analytic solver - each naturally with its own strengths and weaknesses.", "Good luck with your project!", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Although the arm_navigation stack is a comprehensive package, it may go beyond the capabilities you are trying to test, and you may get lost learning the arm_navigation stack and gazebo (the simulator) and ros all at the same time.", "If you are trying to develop your own algorithms you may want to try developing these first without the arm_navigation stack, but making use of ROS's distributed programming and message formats which are quite powerful.", "If this sounds sensible, I'd suggest getting a model of your arm done in ", " (noting ROS fuerte uses gazebo 1.0) using a urdf file. Create your own ", " to simulate your arm's control system. And then write a couple of nodes for your IKcontrol and for your DesiredPoseGoal. If you use basic messaging provided by ROS such as the ", " and perhaps define a couple of your own basic message groups for defining your Cartesian_Pose_Goals in a simple way then you will have the basics for testing a number of your own algorithms.", "This way you can focus on your IK development for your specific configuration in a simple and uncluttered way. The arm_navigation_stacks are generalistic and take a bit to understand and the documentation is by no way a yellow brick road, and you can get easily sidetracked and not everything may be relevant for your case.", "Though that being said, the arm_navigation_stack is very professional and worth a look to understand a lot of the concepts around what makes IK difficult. ", "Thank you very very much for having taken time to have answered my queries despite the stupidity of mine for having not included in the query about the technical details like the ubuntu and the ros verison.\nThe details about the versions of the softwares I am using\nUbuntu 13.04 ROS - Groovy"], "url": "https://answers.ros.org/question/58343/inverse-kinematics-algorithm-implementation/"},
{"title": "Suggestions on using irobot create with hokuyo urg 04lx / turtlebot?", "time": "2012-09-27 02:03:24 -0600", "post_content": [" ", " ", " ", " ", "I was wondering if its possible to use hokuyo URG 04LX lidar sensor with the turtlebot or irobot create.The kinect has a limited range of 50 degrees scan. I want to use the hokuyo for larger scans (180-240 degrees).  How do we use the same turtlebot power and sensor board to connect the hokuyo laser? I have seen that people have successfully used arduino with roomba and hokuyo, but has anyone being able to use create / turtlebot successfully and build 2d maps. "], "answer": [" ", " ", "Sure, that should work fine. The only thing, you need to do is use 5V instead of the 12V for Kinect.", " ", " ", "We use a Yanec external battery (6 Ah) with 5V and 12V (it has also a third outlet) for simultaneously powering the Hokuyo and the Kinect. We have soldered the proper connectors to each device to prevent inadvertently connecting the Hokuyo to something else then 5V (a mistake will cost you many $$$)."], "url": "https://answers.ros.org/question/44660/suggestions-on-using-irobot-create-with-hokuyo-urg-04lx-turtlebot/"},
{"title": "problem using Turtlebot Interactive Markers", "time": "2012-05-02 04:34:22 -0600", "post_content": [" ", " ", "Hi everyone,", "I'm trying to use interactive markers to move the turtlebot using Ros Electric version.\nThe connexion with the turtlebot seems ok. The turtlebot service is running on the laptop. The dashboard is all green!!", "I'm following the \"TurtleBot Teleoperation Using Interactive Markers\" tutorial. When I click on \"interact\" button on rviz, the interactive markers appear. But when I click on them to move the robot, only the camera moves. It's like I didn't change mode.", "Has someone an idea of what is going wrong?", "Thanks a lot for your help."], "answer": [" ", " ", "I suggest using \"rostopic echo /cmd_vel\" on the TurtleBot netbook to verify that when you're interacting, the messages are being received by the netbook. If they're not, you may need to make sure ROS_IP or ROS_HOSTNAME are set properly on the machine running rviz. (check ", " for more information)", " ", " ", "Hey Barriere,", "Were you able to figure this out. I am getting the same issue.", "I tried \"rostopic echo /cmd_vel\", it give me correct values corresponding to motion. But the robot doesn't move. The circle is blue not green like you mentioned.", "Thanks", " ", " ", "Hi Ryan,", "Thank you for your answer. But I don't think my problem is a communication problem between my desktop PC and the laptop. ", "I have made some basic test like \"rostopic list\" that succeed. I can teleop the turtlebot using my desktop pc keyboard. I can launch slam process and see result on rviz using my desktop PC. But I can't use the interactive marker.", "As I said in my first post it's like nothing appends when I click on interact button.\nThe markers appear but when I click on the 3D view it seems that we are still using \"move camera\" mode.", "The only difference I can see with the tuto is that the circle marker is not green but blue. (Maybe color has a sense)", "Thank you a lot for your help!", "Hello,", "I'm running the pr2 demo that is available for indigo and I'm getting the same issue.  When I am in 'Interact' mode, I can put my cursor over the joint I want to control and the Interact Marker will appear, but if I try to click on one of the controls, all that happens is the camera moves."], "url": "https://answers.ros.org/question/33112/problem-using-turtlebot-interactive-markers/"},
{"title": "Kinect for Xbox or Kinect for PC for SLAM?", "time": "2013-01-28 14:57:42 -0600", "post_content": [" ", " ", " ", " ", "Hello, i want to buy Kinect for my SLAM project, i'm confused between PC one and Xbox one. \nWhich one is better in terms of range of the sensor? Does the PC one need power supply?\nNeed your advice. \nThanks. ", "I'm only installing ROS modules. I'm not trying using OpenNI for SLAM. Although it is one of the solution as given in this website http://www.hessmer.org/blog/2011/04/10/2d-slam-with-ros-and-kinect/  Please help with the compatibility of Asus Xtion Pro for SLAM. Thanks!", "I'm not sure if I understand your issue... OpenNI is simply a way for you to get sensor data, both from the Kinect and from the Asus Xtion Pro. Once you have that, then you can think about how to use it for SLAM."], "answer": [" ", " ", "As far as I am aware they are both pretty much identical if you're using OpenNI, except for this thing called near mode where Kinect for PC gives you data in the 40-300cm range instead of the 80-400cm range (as mentioned ", "). Both need a power supply. If you want a similar sensor that does not need a power supply, you could try the ", " which is OpenNI-compatible.", "If you are going for the alternative, you should look for the Xtion Pro LIVE (", "/) since the normal Xtion Pro does not have an rgb camera."], "url": "https://answers.ros.org/question/53575/kinect-for-xbox-or-kinect-for-pc-for-slam/"},
{"title": "USB Graphics Card for Ubuntu 12.04LTS", "time": "2013-01-28 03:54:48 -0600", "post_content": [" ", " ", "Does anyone know about a USB Graphics card that has drivers that function in Ubuntu 12.04 LTS? The graphics card on my PC does not support multi-display functionality in Ubuntu OR RViz feeds from the kinect sensor...", "Any suggestions on a good USB graphics card with Linux drivers?", "Thanks!\nAndy", "What devices is your PC made of?"], "answer": [" ", " ", "Andy, a USB graphics adapter is a waste of money. Why don't you tell everyone what your ROS project is, then they can help you better? You don't mention if it must be a laptop, or battery operated?", "The easiest path is to get yourself a spare desktop PC (can be second hand) with a nice NVidia graphics card. Setup a fresh Ubuntu install and you'll be set."], "url": "https://answers.ros.org/question/53529/usb-graphics-card-for-ubuntu-1204lts/"},
{"title": "Control architecture design - help!", "time": "2013-01-16 08:27:51 -0600", "post_content": [" ", " ", "Dear all,", "I am having a control architecture design decision to take and I was thinking some of you guys have probably already faced the same kind of problems. So here follows the description.", "We are currently designing a mobile robot + mounted arm with multiple controlled degrees of freedom and sensors. In the initial configuration there are 10 positioning motors to control and we are considering modifications were this number would scale up. Sensors include an IMU (gyros + accelerometers + magneto) + kinect (for SLAM) + motor encoders.", "I am considering the following architecture:", "I know my framework needs to be scalable to account for more motors, more sensors, more PCs (eg. for external mocap). I was wondering whether there were ", " (I know no brainer solutions do not exist but maybe there are typical ones that are often used). I have looked at papers related to robots architecture (eg. HRP2\u2026), most often they describe the high level control architecture but I have yet to find information on how to have the low level communicate with the high level and in a scalable way. Did I miss something?", "My main problem is to decide how to have the different RTx communicate with PC1. I am not sure which technology of BUS/netword to use to connect the fast RT machines ensuring the motor control with PC1. I have considered TCP/IP, CAN and UART. There may be other solutions, I do not know about:", "Is TCP/IP really a no go because of its non-deterministic characteristics? It is so easy to use\u2026", "At the moment no solution really seems obvious to me. And as I can find no serious robot exemple using a specific reliable and scalable solution, I do not feel confident to make a choice. Anyone has a clear view on this ...", "Another bus to consider is (EtherCat)[", "]."], "answer": [" ", " ", "Welcome arennuit!", "Nice first question! However, since it is not really ROS-related, you might get more and maybe better answers at general Robotics forums/Q&As, such as ", ". Anyway, here are my thoughts:", "You already did the important step of splitting your system up into RT and non-RT areas. Now, the question remains how to connect them. To answer this, you need to figure out whether or not you need a hard real-time connection between both.", "For example, if you plan to do joint trajectory interpolations and send each position/velocity command at hight speed to the motors, you might want to have a deterministic/low jitter connection to your motor control boards.", "I'm not sure, what requirements you have: You stated, you want to determine (deduct) desired motor positions and speeds on the PC, but you also plan to run those control loop(s) at only 30hz. So, in case you don't do something like a fine-grained joint trajectory computation on the PC, I wouldn't expect you to need a real-time connection between your PCs and RTs. Hence, TCP/IP could be well suited for your system (as you stated, cheap and simple).", "If however at some point you have the need for it, you can look into CAN[1] and Ethercat controllers for your PC. Be aware, that those controllers are often quite expensive. CAN has the advantage, that especially the devices/slaves (not the PC controller) are much cheaper than Ethercat, which on the other hand is more powerful (e.g. much higher bandwidth). Furthermore, you will need a RT system for your PC, such as Xenomai, in order to be able to run the fast control loops, which need to communicate with your RTs, in the real-time space. Setting up such a system and writing correct programs for it is also more difficult and time-consuming, than writing programs for normal user space.", "[1] I don't think CAN is so exotic in the Robotic world. Companies such as ", " and ", " are working with it. Also, we recently starting experimenting with it. ", "I hope, I could help you a bit!\nKeep us updated how your robot will look like in the end! :-)", "Hello bit-pirate, your answer was very clear and very helpful. It definitely opens doors. Thanks a lot!", "I'm happy, I could help. If you find your question answered, please mark it accordingly. Thx!"], "question_code": ["1. RTx: Positioning motors control\n    - Achieved by a RT system (either Raspeberry Pis running an RTOS such as Xenomai or bare metal microcontrollers)\n        -> let us call these machines RTx, with x=1,2,3\u2026 depending on the number of microcontrollers\n    - Each of them controlling 2 or 3 motors with related encoders (not sure about the technology of the motors yet)\n    - Fast loop: 200Hz\n\n2. PC1: SLAM computation + IMU computation + fusion (SLAM + IMU + mocap) + high level logic (decide the robot\u2019s task and deduct the motors desired position and speed)\n    - Compute on a powerful machine (vanilla linux, no RT)\n        -> Let us call this machine PC1\n    - Onboard robot\n    - Using ROS \n    - Slow loop: 30Hz\n", "1. TCP/IP: not deterministic but easy to put in place. Is non determinism a real issue (as it will only be used at at slow speed 30Hz anyways)?\n2. CAN: slow, very reliable, targeted to cars ( have seen there are some exemples using CAN with robots but it looked exotic)\n3. UART: if I had only had one RT machine for motor control I would have considered UART but I guess this port does not scale well with many RTx\n"], "url": "https://answers.ros.org/question/52501/control-architecture-design-help/"},
{"title": "Turtlebot Navigation Error in Fuerte and 12.04", "time": "2012-09-12 06:58:58 -0600", "post_content": [" ", " ", " ", " ", "I am trying to run the navigation demo in Fuerte and 12.04 Precise, after completely successfully running the SLAM GMAP demo, and I get the following rolling list of errors which goes on forever.  Does anyone know what might be wrong?"], "answer": [" ", " ", " ", " ", "It turns out that this was caused by leftover processes running.  Two solutions to this problem:", "Reboot turtlebot and workstation, and cycle power to the create.", "or", "Kill all services related to the turtlebot. (especially nodelet and XnSensorServer)", "Then it works fine!"], "question_code": ["rscan/CloudThrottle openni_manager cloud_in:=/camera/depth/points cloud_out:=cloud_throttled __name:=pointcloud_throttle __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/pointcloud_throttle-4.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/pointcloud_throttle-4*.log\nrespawning...\n[pointcloud_throttle-4] restarting process\nprocess[kinect_laser_narrow-6]: started with pid [8744]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[pointcloud_throttle-4]: started with pid [8746]\n[openni_launch-3] process has died [pid 8605, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load openni_camera/OpenNINodelet openni_manager __name:=openni_launch __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/openni_launch-3.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/openni_launch-3*.log\nrespawning...\n[openni_launch-3] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[openni_launch-3]: started with pid [8772]\n[kinect_laser-5] process has died [pid 8632, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load pointcloud_to_laserscan/CloudToScan openni_manager cloud:=/cloud_throttled __name:=kinect_laser __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser-5.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser-5*.log\nrespawning...\n[kinect_laser_narrow-6] process has died [pid 8635, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load pointcloud_to_laserscan/CloudToScan openni_manager cloud:=/cloud_throttled scan:=/narrow_scan __name:=kinect_laser_narrow __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser_narrow-6.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser_narrow-6*.log\nrespawning...\n[kinect_laser-5] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_laser-5]: started with pid [8824]\n[kinect_laser_narrow-6] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_laser_narrow-6]: started with pid [8829]\n[ INFO] [1347468668.184002927]: Sim period is set to 0.20\n[openni_launch-3] process has died [pid 8693, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load openni_camera/OpenNINodelet openni_manager __name:=openni_launch __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/openni_launch-3.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/openni_launch-3*.log\nrespawning...\n[openni_launch-3] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[openni_launch-3]: started with pid [8884]\n[pointcloud_throttle-4] process has died [pid 8643, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load pointcloud_to_laserscan/CloudThrottle openni_manager cloud_in:=/camera/depth/points cloud_out:=cloud_throttled __name:=pointcloud_throttle __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/pointcloud_throttle-4.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/pointcloud_throttle-4*.log\nrespawning...\n[kinect_laser-5] process has died [pid 8741, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load pointcloud_to_laserscan/CloudToScan openni_manager cloud:=/cloud_throttled __name:=kinect_laser __log:=/home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser-5.log].\nlog file: /home/turtlebot/.ros/log/661c8e88-fcf9-11e1-8620-485d607f80f5/kinect_laser-5*.log\nrespawning...\n[kinect_laser_narrow-6] process ..."], "url": "https://answers.ros.org/question/43723/turtlebot-navigation-error-in-fuerte-and-1204/"},
{"title": "Is VPN necessary for networking turtlebot?", "time": "2012-10-13 04:55:28 -0600", "post_content": [" ", " ", "Hi ,", "I was testing my system for installing the turtlebot setup. I was wondering if VPN is anecessary step in the networking setup? Can the two pc's communicate only via wifi ? I did not understand the concept of VPN completely.. (Lack of detailed VPN etup instructions on the turtlebot tutorial page I guess)", "Next, I want to know how does one map a large area say corridors and large multiple rooms at the same time. The wifi router cannot have such large range to continue networking between the two pc's. ", "How to map large areas in such situation with turtlebot pc and workstation pc? "], "answer": [" ", " ", "VPN is only a secure way to communicate. You can run ROS over normal Wifi without problems.", "The distance problem is solved by only processing data on the robot. If that isn't powerful enough, record a logfile and process that on the workstation later.", "thanks a lot, can you please tell me how to record the log file?", "Use the rosbag command. See: "], "url": "https://answers.ros.org/question/45783/is-vpn-necessary-for-networking-turtlebot/"},
{"title": "Wireless Network", "time": "2012-07-10 06:37:55 -0600", "post_content": [" ", " ", "It looks like you need a Wireless Network in order to operate the turlebot, is there anyway to get around this/use the turtlebot with only one computer?  "], "answer": [" ", " ", "You can reconfigure the TurtleBot to start without a wireless network with the following:", "This will have the side-effect of being unable to access the ROS network on the TurtleBot from other computers.", " ", " ", "I do not think there's a need to reconfigure the TurtleBot at all. The turtlebot laptop is connected to the robot with 2 cables (Kinect and Create). As long as the cables are long enough, you can run everything on the laptop. But get ready to spend some time running after the robot (especially when you start the autonomous navigation)", "If the TurtleBot laptop is unable to connect to a wireless network, ROS will not automatically start in the background upon robot power-on. Check out /etc/init/turtlebot.conf on a TurtleBot machine to see what I mean.", "turtlebot.conf just starts the service automatically on bootup. It is equivalent to running \"sudo service turtlebot start\" at a console as per the turtlebot bringup tutorial. I have tried following the tutorial without any network (or any workstation for that matter) and it works fine."], "answer_code": ["roscd turtlebot_bringup/upstart\nsudo ./install.bash lo\n"], "url": "https://answers.ros.org/question/38453/wireless-network/"},
{"title": "PNI Digital Compass Driver", "time": "2011-07-09 10:43:11 -0600", "post_content": [" ", " ", " ", " ", "I've recently acquired a ", " and would like to interface it with ROS. It communicates over serial using a binary protocol described in their datasheet. The protocol appears to be proprietary, but I am not familiar enough with the protocols used by other manufacturers to know for sure.", "Basic searches in the ROS package database haven't turned up any existing packages. Before I take the time to write one, has anyone else written a ROS node for communicating with this compass?", " It's not completely polished, but here is the ", " node that I wrote. It is written in Python and publishes a ", " message with the ", " field populated."], "answer": [" ", " ", "I'm not aware of any PNI drivers.  We would have had one for you, but I wrote that one for our custom FPGA solution.  ;)  No getting that one out.", "It shouldn't be too difficult to write using ", " or ", ".  If you remember to make it generic for all their compasses, and reconfigurable (maybe even with ", "), you would make it very valuable to the community.", "PS: my suggestion on an output message would be a ", ".  Our compass also output pitch and roll, although not filtered, so this message type could output all of those values at once.", " ", " ", "With the new and latest version I have timeout problems with my PNI Prime. I also has a 0.5s timeout for almost all commands, but it seems to be actually more. This is the exception raised: ", "Plesase, note that I've only changed the name of the driver to pni_dev, and create my own wrapper on top of it. Everything is the same. I increase the timeout to 1s in a >24h experiment, but at some point it wasn't enough.", "Do you have any idea that could explain this behaviour?", "Thanks in advance,\nEnrique", "Hi, I'm not seeing a correspondence between the line numbers you've posted here and the ones in fieldforce_tcm/src/fieldforce_tcm.py (@ commit b3c270b ).Is pni_prime_imu.py a different file currently in the fieldforce_tcm package? Are there any edits to it? Is there any more to this exception?", "Well, this pni_prime_imu.py file is mine, just  a wrapper over the driver fieldforce_tcm.py. Let's say it's an equivalent to compass.py. However, the problem is simply the fact that the timeout expires before something arrives. I set it to 1s, instead of 0.5s. This happens in recv.", "I also have to say, that in this particular case of a >24h experiment, I run out of batteries, so that was the reason because the sensor didn't reply any longer. Anyway, please note the fact that I increased the timeout from 0.5s to 1s. That's what I don't understand. Do you have similar problems?", "I can't do anything about the battery life :) . I have however written some code in the ros node that attempts to restart the compass if a timeout in data occurs (and increased the timeout). It'll be pushed to the main repo as soon as we test it (for now, it is in github.com/jmesmon/fieldforce_tcm).", "Sure ;) Thanks. Actually, those two things can be done: increase timeout, catch exception and retry. Anyway, it seems strange the documentation says only 0.5s, when it happens to be far more.", "We've updated the driver with the code which attempts to reset it.", " ", " ", "I also have this exact same compass. Has anyone made progress towards a solid driver for this device?", "Thank you,\nTim", "I updated my original post with a link to the (mostly) finished node.", " ", " ", " ", " ", "Hi,", "I can say it also works for the PNI Prime model.", "For the source I downloaded (a few days ago), you might detect some problems with the (let's say) \"k\" and no \"k\" var names if you ask the device for more \"components\", e.g. accelerometer and magnetometer readings.\nIt's straightforward to correct though, and maybe it's already fix in the repo (but didn't check, don't know). Let me know if you want to know the details.", "instead of", "Only the 'K' has to be remove, for the last three items.", "I've detected some problem using your driver with my PNI Prime. When I stop the driver the sensor is left in a state such that when I re-start the driver again it fails. Then, I re-start again and it works. See below the call stack output (summarized):", "Have you found this problem too? I've also tried to put a ", " at the end. It only happens that the error changes. And, at the very beginning, I've also put ", " and ", ", to ensure config mode ---I guess. But still the same.", "Thanks for this great contribution, mkoval!\nEnriquepowerDown", "One of my colleagues and I made some changes to the \"components\" code earlier this week, so I am not surprised if there is a bug. Can you describe what the problem is in more detail?", "I just edited the answer with the details, as well as a problem when I re-start the driver, for the PNI Prime.", ": Both of those problems should be fixed in the latest code in my repo. This update adds a script for limited tilt calibration (IRRC) and, most importantly, discards unexpected packets. Let me know if that fixes the issue.", ": I cannot try it now, but I'll let you know if it works. BTW: I also had to add this to the rosdep.yaml file:\npyserial:\n  ubuntu: python-serial", ": Also, pip install decorator is required in rosdep.yaml, i.e. it's installed the same way as crcmod.", ": To make it work with my PNI Prime I had to increase the timeout (pni_dev.py:421) to 1s, instead of 0.5s (the manual says 0.5s, but it seems to be not enough); I'm using an USB-serial adapter, btw. Also, when closing (Ctrl+C), the node it's not correctly \"kill\": it scales to SIGKILL!", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello, I have the exact same compass and was wondering if you ever got those drivers done? Are you will to share them?", "Thank you.", "I updated my original post with a link to the (mostly) finished node."], "question_code": ["nav_msgs/Imu", "orientation"], "answer_code": [" File \"pni_prime_imu.py\", line 331, in <module>\n    main()\n  File \"pni_prime_imu.py\", line 327, in main\n    raise e\npni_dev.TimeoutException: Did not recv frame_id (5,) within time limit.\n", "    Datum     = namedtuple('Datum', [\n        'Heading', 'Temperature', 'Distortion', 'CalStatus',\n        'PAligned', 'RAligned', 'IZAligned',\n        'PAngle', 'RAngle', 'XAligned', 'YAligned', 'ZAligned'\n    ])\n", "    Datum     = namedtuple('Datum', [\n        'Heading', 'Temperature', 'Distortion', 'CalStatus',\n        'PAligned', 'RAligned', 'IZAligned',\n        'PAngle', 'RAngle', 'KXAligned', 'KYAligned', 'KZAligned'\n    ])\n", "    compass.setConfig(Configuration.kMountingRef, Orientation.kOrientationSTD0)\n  File \"pni_dev.py\", line 311, in setConfig\n    self._recvSpecificMessage(FrameID.kSetConfigDone)\n  File \"pni_dev.py\", line 260, in _recvSpecificMessage\n    raise IOError('Response has unexpected frame id: {0}.'.format(frame_id))\nIOError: Response has unexpected frame id: 5.\n"], "url": "https://answers.ros.org/question/10546/pni-digital-compass-driver/"},
{"title": "Pr2 simulation speed", "time": "2012-03-05 04:26:46 -0600", "post_content": [" ", " ", " ", " ", "Hello all.\nDoes anyone know if there is an easy way to speed up the performance of a simulation of a PR2 robot in gazebo?\nHas anyone tried to execute some nodes, like the navigation nodes on different machines? Any other experience?\nWould be great if someone could share his/her experience!", "Best,\n  Reini"], "answer": [" ", " ", " ", " ", "There are two ways: More power for gazebo or less precision.", "The first will be exactly what you said. Run everything on separate machines or cores.\nFor me, there was a dramatic change switching from a dual core to a hex core machine.", "If you cannot do that adjust the stepTime parameter in the physics section of the world.\nObviously once you get it too rough it will become very unstable, but as long as you don't need it as physically correct as possible, but just look believable that is viable.", "Hello. Thank you for your quick reply. Could you please be more specific where to set the mentioned parameter?", "It is in the world file.", "Ok found it. Thanks a lot"], "url": "https://answers.ros.org/question/29124/pr2-simulation-speed/"},
{"title": "Which Robot for ROS?", "time": "2012-01-30 23:40:40 -0600", "post_content": [" ", " ", "Hi,", "which of the supported robots would you recommend for using with ROS (educational purposes)? Turtlebot seems to lead the way but are there any real alternatives? Anything that comes close to Turtlebot (and is in the same/similar price range)? "], "answer": [" ", " ", "I would say that the Turtlebot is the most well-supported platform on these forums, just out of experience. That is not to say that other robots aren't well-supported, but there are lots of tutorials and help for Turtlebot users. I'm not sure of any other robots really in that price range. ", " ", " ", "I've just built my own basically-a-turtlebot with a netbook, an iRobot Create, and a kinect. It's not as 100% pretty, but it works well. ", "Basically ", ", but the latest one uses a turtlebot power board instead of the handmade one since it included a gyro that I needed.", " ", " ", "The ", " platform offers similar features to the Turtlebot and has an integrated \"powerboard\" that allows for a single charger.  I believe the Turtlebot requires separate chargers for the computer and the iRobot Create, whereas the Bilibot has one plug to power/charge the create and the main computer supply.  It also has Intel i3 processor that's a bit beefier than the dual-atom you get in the Turtlebot's netbook.  Plus it ships with an arm (albeit very limited).", "Unfortunately, other than basic setup instructions the documentation/tutorials for the Bilibot platform are severely lacking.  As a contributing member of the project, I'm working with my local university to help find and fix some holes in that area over the upcoming semester (they purchased a number of them as general-purpose ROS machines).", " ", " ", "Hi Jeff,", "I hope you are fine. Totally agree with you, I have been struggling with documentation. I have a 2012 bilibot. Do you know how to operate the biliarm (robotic arm)? What package will do this? Does it come pre installed or we need to download it separately and build it with rosmake?", "Regards,\nManuel"], "url": "https://answers.ros.org/question/12822/which-robot-for-ros/"},
{"title": "turtlebot Roomba/ Create power issues.", "time": "2012-02-14 07:03:23 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Does anyone know what the internal power checking  routines on a Roomba do  and how they work?  I am thinking they don't look just at the voltage, but may have some way to sense current. ( I am working with a Roomba 500 series base).", "I am guessing if it is over 15v?  it thinks it is on charge,  and below some other voltage 13?  it needs to be charged.  I am struggling with multiple dead roomba battery packs.  If I can feed a fixed voltage through the external unregulated power connector,\nperhaps I can better manage performance."], "answer": [" ", " ", "In general the charging logic for the roomba/create is not the best. Typically the roomba/create batteries charge to something like 15-17V and below about 13V it shuts off. On the create the charging logic will not try to charge a battery after it have been through one charging cycle and will let a battery completely drain until it is unplugged and re-plugged in. ", "Is the logic the same on the Roomba and what is the logic?  I've tried using an accessory battery -  but the dashboard acts as if it isn't there or can't be detected.  The Roomba knows when it is on or off the charger, but switching it to full mode seems hit or miss.  "], "url": "https://answers.ros.org/question/27623/turtlebot-roomba-create-power-issues/"},
{"title": "Kinect - no devices connected on openni_node.launch", "time": "2011-11-27 11:39:59 -0600", "post_content": [" ", " ", " ", " ", "Hi all, I'm starting with the turtlebot but can't get the Kinect to be recognized. I am runnig diamondback.", "I redid ", " just to make sure it was there, but didn't solve the problem.", "Output of ", ":", "Output of roslaunch openni_camera openni_node.launch:", "Thanks in advance for any help!"], "answer": [" ", " ", " ", " ", "Hi,\nlsusb should give 3 devices of Microsoft as below.", "I mean this is not w r t turtlebot, but in general when connecting Kinect using external power supply. When its disconnected from external supply it gives the one device as in your case.", "Thanks,\nKarthik", " ", " ", "I have the current version from the openni_kinect and running ROS-electric with Lubuntu 11.10.", "lsusb gives me 3 devices of the Microsoft Kinect so power supply is on (green light is blinking):", "Output of roslaunch openni_camera openni_node.launch:"], "answer_details": [" ", " ", " ", " ", "I haven't a clue what could be the error?", "Or have I still missing to set some settings or parameters?", " ", " ", " ", " "], "question_code": ["sudo apt-get install openni_kinect", "lsusb", "Bus 005 Device 002: ID 13d3:3315 IMC Networks \nBus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 001 Device 008: ID 045e:02b0 Microsoft Corp. \nBus 001 Device 007: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 001 Device 002: ID 13d3:5702 IMC Networks \nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n", "Checking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://turtlebot-laptop:42441/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /openni_node1/use_indices\n * /openni_node1/depth_registration\n * /openni_node1/image_time_offset\n * /openni_node1/depth_frame_id\n * /openni_node1/depth_mode\n * /openni_node1/debayering\n * /rosversion\n * /openni_node1/projector_depth_baseline\n * /openni_node1/rgb_frame_id\n * /openni_node1/depth_rgb_translation\n * /openni_node1/depth_time_offset\n * /openni_node1/image_mode\n * /openni_node1/shift_offset\n * /openni_node1/device_id\n * /openni_node1/depth_rgb_rotation\n\nNODES\n  /\n    openni_node1 (openni_camera/openni_node)\n    kinect_base_link (tf/static_transform_publisher)\n    kinect_base_link1 (tf/static_transform_publisher)\n    kinect_base_link2 (tf/static_transform_publisher)\n    kinect_base_link3 (tf/static_transform_publisher)\n\nauto-starting new master\nprocess[master]: started with pid [22450]\nROS_MASTER_URI=http://192.168.1.11:11311\n\nsetting /run_id to a30405ca-f056-11e0-83fc-5c260a051546\nprocess[rosout-1]: started with pid [22463]\nstarted core service [/rosout]\nprocess[openni_node1-2]: started with pid [22473]\nprocess[kinect_base_link-3]: started with pid [22476]\nprocess[kinect_base_link1-4]: started with pid [22477]\nprocess[kinect_base_link2-5]: started with pid [22478]\nprocess[kinect_base_link3-6]: started with pid [22479]\n[ INFO] [1317931571.886741090]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931572.887058521]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931573.887393253]: [/openni_node1] No devices connected.... waiting for devices to be connected\n"], "answer_code": ["Bus 002 Device 009: ID 045e:02ae Microsoft Corp. \nBus 002 Device 008: ID 045e:02ad Microsoft Corp. \nBus 002 Device 007: ID 045e:02b0 Microsoft Corp.\n", "Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 003 Device 002: ID 045e:0040 Microsoft Corp. Wheel Mouse Optical\nBus 001 Device 015: ID 045e:02c2 Microsoft Corp. \nBus 001 Device 017: ID 045e:02be Microsoft Corp. \nBus 001 Device 018: ID 045e:02bf Microsoft Corp.\n", "user@userPC:~$ roslaunch openni_camera openni_node.launch\n... logging to /home/user/.ros/log/47a4639a-7dc4-11e1-bfb8-000e35832721/roslaunch-userPC-20363.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://userPC:33231/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /openni_node1/use_indices\n * /openni_node1/depth_registration\n * /openni_node1/image_time_offset\n * /openni_node1/depth_frame_id\n * /openni_node1/depth_mode\n * /openni_node1/debayering\n * /rosversion\n * /openni_node1/projector_depth_baseline\n * /openni_node1/rgb_frame_id\n * /openni_node1/depth_rgb_translation\n * /openni_node1/depth_time_offset\n * /openni_node1/image_mode\n * /openni_node1/shift_offset\n * /openni_node1/device_id\n * /openni_node1/depth_rgb_rotation\n\nNODES\n  /\n    openni_node1 (openni_camera/openni_node)\n    kinect_base_link (tf/static_transform_publisher)\n    kinect_base_link1 (tf/static_transform_publisher)\n    kinect_base_link2 (tf/static_transform_publisher)\n    kinect_base_link3 (tf/static_transform_publisher)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[openni_node1-1]: started with pid [20381]\nprocess[kinect_base_link-2]: started with pid [20382]\nprocess[kinect_base_link1-3]: started with pid [20383]\nprocess[kinect_base_link2-4]: started with pid [20384]\nprocess[kinect_base_link3-5]: started with pid [20385]\n\n[openni_node1-1] process has died [pid 20381, exit code -4].\nlog files: /home/user/.ros/log/47a4639a-7dc4-11e1-bfb8-000e35832721/openni_node1-1*.log\n"], "url": "https://answers.ros.org/question/12107/kinect-no-devices-connected-on-openni_nodelaunch/"},
{"title": "Rebuilding with Intel Compiler", "time": "2011-05-12 11:34:45 -0600", "post_content": [" ", " ", "Hi all,", " I saw on the page  ", "  how edit a package's makefile to use the Intel C/C++ compiler. Is it possible to build the entirety of diamondback using that compiler? ", "Thanks!\n~Robin"], "answer": [" ", " ", "I haven't tried this, but you can probably configure it globally as a toolchain just as you would a cross-compiling toolchain. This involves setting the variables as described on the rosbuild page in $ROS_ROOT/rostoolchain.cmake and compile flags in $ROS_ROOT/rosconfig.cmake.", " can probably help you set this up. In particular, you might want to use eros' ", " and ", " helper tools or just check out the format of some of the pre-existing cmake modules in the ", " and ", " packages.", "What you will run into trouble with though is rosdeps and some 3rd party packages. You won't have an easy way to install intel compiled rosdeps into the intel root. Also, 3rd party packages unfortunately are mostly hacked by Makefile and are consequently ignorant about ros' cmake configuration, so they'll need custom hacking of their own Makefiles. ", "Still, you should be able to get at least the core packages compiling if you can compile/install eros' ", " (installs apr, apr-util, log4cxx and boost into your toolchain root). You can then build around that. Not for the light hearted, but might be a useful approach for the high performance computing types.", "Attempting to do this just illustrates how powerful the rosdeps concept is and how limited you can find yourself without it.", " I wanted to rebuild a package using an intel compiler as well could you please tell me what you did ? Thank you .. i'm new to ubuntu, makefiles and cmake so .. i'm using ros groovy... \nis it possible to only build one particular package using icc ? or one particular file and somehow use it?"], "url": "https://answers.ros.org/question/9950/rebuilding-with-intel-compiler/"},
{"title": "Error when compiling RGBDSLAM for Kinect", "time": "2011-03-01 23:12:25 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I'm having a bit of trouble getting the RGBDSLAM ros competition entry to compile on my 32 bit ubuntu 10.10 machine. I am compiling it on a fresh (today) version of diamondback but get the following lines when trying to compile using step 4 (after following the previous steps) of the guide...", "phil@phil-Latitude-E6400:~$ rosmake --rosdep-install rgbdslam\n[ rosmake ] Packages requested are: ['rgbdslam'] ", "\n[ rosmake ] Logging to directory/home/phil/.ros/rosmake/rosmake_output-20110302-125811\n[ rosmake ] Expanded args ['rgbdslam'] to:\n[] ", "\n[ rosmake ] WARNING: The following args could not be parsed as stacks or packages: ['rgbdslam']\n[ rosmake ] ERROR: No arguments could be parsed into valid package or stack names.", "I can get data from the kinect and view in rviz, after following the relevant wiki guides. Have I missed something simple?", "Thanks in advance!"], "answer": [" ", " ", "Did you download the sources for the rgbdslam? It seems rosmake is unable to find the package. You should add the path to the sources for rgbdslam to your ROS_PACKAGE_PATH. I tried using rgbdslam on diamondback with kinect and you also need to make some adjustments to the code to make it compile. But even then it seems that the image message from openni_camera has changed and rgbdslam can't do anything with it.", " ", " ", "Do you still have the problem or did you solve it in the meantime?", " ", " ", " ", " ", "Ah\nI finally got my install working!", "Turns out I had the same problem as this person:\n", "I am green with Linux so it took me a while as I was missing some of the basics. I will do my best to explain what I did so others have a chance at fixing it.", "Someone above mentioned .bashrc, but I didn't even know what it is. Well, its in your user folder (/home/taylor in my case) but it is hidden. So open up Nautilus (the file browser), navigate to your user folder, and press Ctrl-h to toggle hidden files if you don't see .bashrc. bashrc runs when you open up a new terminal (I think) and it sets up the environment variables for the terminal session. Anything put in there gets run.", "Well, the ROS installation wiki says to add \"source /opt/ros/diamondback/setup.bash\" to the end of your .bashrc file. So, /opt/ros/diamondback/setup.bash is a script that gets run when your terminal opens. It runs a file in that same folder called setup.sh, which sets the ROS environment variables.", "You can't just set environment variables in your .bashrc file because it will just overwrite some of the stuff that the setup.sh file does. So, navigate to /opt/ros/diamondback/ and open setup.sh for editing. Unfortunately, its set as read only for the user. I'm sure this isn't the right way, but I fixed that by opening a Nautilus browser window as root, by typing ", "in a terminal.", "With a root Nautilus window, navigate back to /opt/ros/diamondback/ and edit setup.sh to add the new folders that contain the packages you want to use.", "Specifically, there should be a line:", "add your user folder to that line like so (where you replace my name with your user name!)", "I also saw another folder mentioned and decided to add it just to be safe, so my whole line read like this:", "Now close all your terminal windows and run", "It should find all your packages and build properly! It did for me and I finally have it running. :) If there are any problems, run ", "To update your packages and try again."], "answer_code": ["sudo Nautilus\n", "export ROS_PACKAGE_PATH=/opt/ros/diamondback/stacks\n", "export ROS_PACKAGE_PATH=/opt/ros/diamondback/stacks:/home/taylor\n", "export ROS_PACKAGE_PATH=/opt/ros/diamondback/stacks:/home/taylor:/home/user/ros/ros-pkg\n", "rosmake --rosdep-install rgbdslam\n", "sudo apt-get update\n"], "url": "https://answers.ros.org/question/9255/error-when-compiling-rgbdslam-for-kinect/"},
{"title": "How to set an estimated position without using rviz", "time": "2011-10-03 06:01:10 -0600", "post_content": [" ", " ", "In my project, I want to use multiple turtlebots to navigate in a previously known map to patrol an area. \nMy netbooks uses 2GB of RAM and an Atom N455 processor - it is not so good as D525, but i think it could make some things on ROS...\nI noticed that rviz and another graphic processes requires a lot of processing and video resources, so I want to avoid using rviz and those processes, because I don't need graphical resources.\nBut how can I set a estimated position of the robots without using rviz?\nAnd do you believe that my current netbook (Atom N455 and 2GB RAM) can support an autonomous navigation?", "Thanks in advance,", "Lucas"], "answer": [" ", " ", " ", " ", "You could try using one of the lightweight rviz alternatives for machines with little graphics power. Those can be found ", ". ", "Additionally, you could create your own ", " message and publish it to amcl that way (if you know the robot's pose). The ", " message is of the type ", ".", ": For this message type, the ", " field is filled in by ROS automatically, and the stamp is zero, so you can ignore both of those fields. The ", " is /base_link. Finally, the covariance is a 6x6 matrix that is filled in with set values. I've provided an example message below that was taken straight from Rviz.", " ", " ", " ", " ", "Normally, people don't run rviz on the computers on their robots. Instead, they use another laptop/desktop computer and set the environment variable ROS_MASTER_URI to the robot before running rviz. Example:", "In the above example, the robot has the domain turtlebot1. You can also just use an ip address there or set up your hosts file as explained ", ".", "If you really need an alternative to rviz, see DimitriProsser's answer."], "answer_code": ["initialpose", "initialpose", "seq", "frame_id", "header: \n  seq: 14\n  stamp: \n    secs: 0\n    nsecs: 0\n  frame_id: /base_link\npose: \n  pose: \n    position: \n      x: -5.48213577271\n      y: -3.74665260315\n      z: 0.0\n    orientation: \n      x: 0.0\n      y: 0.0\n      z: 0.457581479883\n      w: 0.889167694683\n  covariance: [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06853891945200942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n", "export ROS_MASTER_URI=http://turtlebot1:11311\nrosrun rviz rviz\n"], "url": "https://answers.ros.org/question/11396/how-to-set-an-estimated-position-without-using-rviz/"},
{"title": "hokuyo_node error code 15->15", "time": "2011-09-08 00:32:59 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "our hokuyo laser has suddenly stopped working.\nOn rosrun-ing hokuyo_node the following error message is displayed:", "The mentioned wiki page doesn't cover our error code and mentions that these codes are undocumented. None of the offered solutions fix the error, which has been preventing us from using the hokuyo for a week now.", "Does anyone know what this error code means or how to resolve this error?", "Thanks in advance"], "answer": [" ", " ", " ", " ", "The protocol definition also lists error code 15 as \"Others\".", "So, the hokuyo worked fine and now suddenly doesn't anymore after you power cycled the Hokuyo and rebooted the system it's running on? You can also try another computer to be safe.", "Unfortunately, in that case I think there is a problem with the hokuyo itself. You can try checking that the power is OK and data connection is worked. Also see if the output of ", " shows any errors when you plug it in.", "As a minimal test on the software site you can try connection with a terminal program to the device (e.g. gtkterm) and send the V command (basically V<enter>). The hokuyo should reply with version information.", " ", " ", "Hi p schwab,", "You should contact to your provider or directly to Hokuyo since this seems to be a hardware problem.", "If this happens when you are scanning, the Communication Protocol Specification for SCIP2.0's Table 3 of the page 15/32 says that errors from 10 to 17 are \"Others\" (temporary document from 4/12/2009)", "Anyway, have you tried Hokuyo drivers? You can find them here: ", " There is a sample program that can give you some more information.", "We had problems with one UTM30LX (the red light blinked instead of keep on) they had to update the firmware, but now there's a tool which you can do it by yourself.", "I hope this helps you a bit."], "question_code": ["[ERROR] [1315484154.288808300]: Laser returned abnormal status message, aborting: Trouble!! Error Code=[15->15] You may be able to find further information at ", "/"], "answer_code": ["dmesg"], "url": "https://answers.ros.org/question/11150/hokuyo_node-error-code-15-15/"},
{"title": "How to connect Kinect to a 12v battery", "time": "2011-04-05 05:59:14 -0600", "post_content": [" ", " ", " ", " ", "Hello there...", "i just started my trials with kinect... i have to mount my kinect on my robot which has a 12v battery. i am a completely noob in the field of electronics...so can any one tell me how to proceed.", " i have taken a peek at the tutorials over here.....\n ", " \nbut could not follow any thing.\ncan any one tell me is there any ready made solution just to connect kinect to a 12v battery? ", "any simplest solution without any soldering business is really appreciated ...:P", "hope i am not wishing too much :P"], "answer": [" ", " ", "Just cut the cable between the power adaptor and the Kinect connector (not the USB connector). Split the cable in the two wires and strip a small end of them.", "Go to the local electronics store and ask for cage clamps (like these: ", ", just make sure you have the ones for flexible cable types, not the ones for solid cable.\nConnect the - from the battery to the white cable from the kinect and the + from the battery (or robot) to the brown one and you are ready without doing any soldering.", "I used crimp connectors for this (the same type as used internally in the PR2) but this requires you to buy an expensive crimping tool.", "Personally I used a DC step-up booster than will convert 6v-42v to a steady 12v, even when the battery is under load.", " ", " ", "You're going to need to do ", " soldering but it's fairly straightforward. However it sounds like you should try and get someone to give you a hand as you definitely don't want to wreck either your Kinect, the battery, or yourself.", "I can offer up as proof of concept though the Kinect-on-quadrotor work that I did (see this ", "). In my work, the Kinect is connected directly to a (nominally) 12V 8000 mAh LiPo battery, in parallel with the quadrotor's power leads. It works great, no voltage regulation was needed. YMMV though, especially using a different type of battery.", "I recall seeing some pinout information, probably on the OpenKinect wiki, take a look there and you might want to try their mailing list as well--your question is not specific to ROS after all.", "Good luck!", " ", " ", "Any progress with your kinect hacking?"], "url": "https://answers.ros.org/question/9647/how-to-connect-kinect-to-a-12v-battery/"},
{"title": "How much horsepower is needed to run the kinect stack", "time": "2011-07-03 01:35:21 -0600", "post_content": [" ", " ", " ", " ", "I have the kinect installed but RVIZ is really slow, a frame every few seconds I'm assuming it is my graphics card that is causing it.I have a Dell Inspiron 9400 and it has an ATI Mobility Radeon X1400. Any idea if this is good enough? Is there any way to spec a computer/video card combo or a way to test a computer? My pc is a dual core 1.8Mhz with 2 gigs of ram. That is close to the specs for the netbook that comes with the turtlebot. Could it be something else in my system causing the slowness. The CPU's are running at 98% when the kinect is running.", "Thanks"], "answer": [" ", " ", " ", " ", "There are two (semi-independent) issues here: rviz and the kinect proper.", "Fire up the openni drivers (without rviz), do ", "; this will tell you how quickly your machine can actually turn kinect data into point clouds. If that number is smaller than you need (and the CPU is spiked), the answer is that your computer isn't fast enough.", "How quickly rviz can visualize those clouds is a different question, and is more about your graphics card. There is substantial CPU overhead in the serialize -> transmit -> deserialize step to get the data into rviz (note that a kinect point cloud, at frame rate, is about 300 MB/sec), which could also be a problem if just getting the data is already maxing your CPU.", "If the problem is just GPU (", " gives you satisfactory speeds, and you have leftover CPU), you could apply a ", " to intelligently downsample your pointcloud; that might help rviz keep up.", "Roughly circa the release of eturtle, two things will happen. First the new openni drivers (currently partway available as ", ") will have a \"record player\" mode, allowing you to store the raw depth and RGB images, and produce point clouds later, via bag playback (meaning you can slow everything down without losing data). You can already roll your own version of this, to some degree; I use something similar on my netbook-based robot. Second, the drivers will become ", ", meaning you can do away with the serialize-transmit-deserialize overhead in your nodes (although not with rviz).", " ", " ", " ", " ", "So we ran Octomap + Kinect + lots of other tools on an i5.  I think that was 4 threads and ran at ~3GHz.  That setup used about 0.123374032 horsepower.", "The biggest change for me was just making the pointcloud smaller.  At full resolution, you get over 300,000 points per scan.  At the smallest resolution, you get 19,200 which was more than enough for me.  Be sure to check out the ", " answer.", "As for visualization, with 300,000 points at 30Hz, I know for sure that 2 GTX460 in SLI can do it. :P  However, my laptop with Nvidia Quadro could visualize, but it really did make it hot and sometimes lagged out.  The most important thing would be to make sure your graphics drivers are up to date and the latest version you can find.", "Finally, watch out about the VoxelGrid filter.  If you set the resolution too small, it will slow you down instead of helping you out.  The algorithm in there is O(n^3) and will have to do operations for each cube.  So if you pick something rather small like 0.01m, you'll basically be checking every mililitre (cm^3) in your Kinect volume.  I think that 0.2m was good and 0.1m was about the lowest I would push that.", "Also, running everything in the nodelets will help.  I know they can be confusing, so take a look at my launch files.  ", " and  ", ".", " ", " ", " ", " ", "Processing the Kinect can be done on processors as small as Atoms or Arm processors. (Only the most recent generations.  )", "The Asus EeePc 1215N has enough cpu to do kinect processing and run the navigation stack.  And I have even seen demos of the Kinect plugged into a ", " and doing navigation.  ", "Visualizing the point clouds is another topic entirely.  This basically requires a discrete graphics card with proper drivers to support 3D acceleration.  Though it doesn't require top end ones unless you want to render many frames at once.  "], "answer_code": ["rostopic hz /camera/rgb/points", "rostopic hz"], "url": "https://answers.ros.org/question/10469/how-much-horsepower-is-needed-to-run-the-kinect-stack/"},
{"title": "ROS Thesis", "time": "2011-06-05 21:04:52 -0600", "post_content": [" ", " ", " ", " ", "I am in my final semester of my masters program and I have decided to base my thesis on ROS. I would like to do something related to the benefits of using ROS or to enhance ROS itself in someway. Apart from those ideas I would appreciate any other ideas as topics for my thesis as long as they are related to ROS. I have a few other ideas as well, but I am also curious as to what the community might come up with.", "I also thought about doing a demonstration of a live robot that makes use of ROS in an interesting and novel way. I have a few potential robot kits that I can do the demo with, but I am not sure if they can be integrated with ROS. And the ones that can be integrated, are out of my budget. Is there a cheap robotic kit that I can use with ROS for my thesis? ", "Thanks"], "answer": [" ", " ", "You should check Tutlebot: ", "Otherwise, almost any robot should be potentially controllable using ROS. You only need to develop a package that implements a ROS wrapper on top of whichever interface your robot uses.", " ", " ", "A useful thesis about ROS could be about embedded systems using it, like you said. You can use the ", " which can be found for $150,00 at ", ". There is a big community interested on it and a lot of material. \nThere is also a ", " for the Beagle Board so you can use it and project a little but very powerful robot to, for example, education programs, RoboCup and such like that. Even a basic PLC, why not.", " ", " ", "If you have an android phone, you could do home/office automation using ROS, like ", ", or control a simple robot remotely.", " ", " ", "You can have a look at Morgan Quigley open arms project, if you have a FabLab in the neighbourhood you can start doing very interesting stuff very low cost. "], "url": "https://answers.ros.org/question/10149/ros-thesis/"},
{"title": "Some questions about the Turtlebot & the Create irobot", "time": "2011-10-24 09:43:58 -0600", "post_content": [" ", " ", "Question, can I use a different iRobot then the create to make a turtlebot,I've found a few of the irobot vacuums that are reasonable in cost. or is it just better to buy a Create? ", "The 2nd question would be, if another iRobot will work, is it better to \"play\" with it without adding the Kinect sensor at 1st, to get a feel how the robot is different, or does that matter any at all. I have most of the parts that are used for the Turtlebot now, I am missing a iRobot (which is probably the most important thing :-) ) But have the kinect, and an aspire one I can use for this project. ", "Money's tight, so doing this as cheap as possible is going to be the key for me. And this will be the 1st time I've used ROS, but from what I've read/seen this is what I want to use for my robotic projects.", "Thanks for the help."], "answer": [" ", " ", "Below I have tried to outline the trade offs between using an irobot create and a roomba:", ": The kinect requires about 0.5-0.8A when running (when the fan turns on there is a current spike). The irobot create has the ability to source/provide 1.5A of current through the DB25 connector in the bay of the robot. The roomba only has the ability to source/provide 0.5A through the MiniDIN connector. Currently documentation is provided on turtlebot.com for building or purchasing a board to power the kinect from an irobot create; you will need to figure out something for the roomba (TurtleBot.eu has been working on a solution for this as well you might be able to get a power board from them). ", ": ", " The wheel encoders on the irobot create and early version roombas are terrible, users have reported that later version roombas have better encoders and do better with localization. However if you want to use an irobot create you will need a gyro and this is designed into the TurtleBot power board. The encoders might be good enough on the roomba but this is completely untested.", ": The TurtleBot stack is meant to run on an irobot create and is tested against that platform. However users have been working with roombas as well, so you could get support from this forum. Just be aware that it may not be a plug and play solution."], "url": "https://answers.ros.org/question/10373/some-questions-about-the-turtlebot-the-create-irobot/"},
{"title": "VSLAM based navigation with Kinect, where do I begin?", "time": "2011-02-16 01:21:48 -0600", "post_content": [" ", " ", " ", " ", "I'm interested in developing a robot (UAV) indoor navigation system basing on a MS Kinect (or Kinect-like, like the ", ") sensor. I'm specifically interested in corridor following. ", "I've recently bought a ", " platform and installed Ubuntu and ROS on it. I'd like to start off without any flying platform and implement the navigation layer first. So here is my plan (and questions):", "Any suggestions welcome, tom."], "answer": [" ", " ", "I was also unable to get a Primesense SDK from them directly, however you might want to continue trying as the Primesense device is quite a bit smaller, lighter, and less power hungry than a Kinect (which certainly has to be a good thing for a UAV).", " ", " ", "You could start with the Kinect and have a look what Patrick has done in the 3D contest. ", " However as your onboard resources are limited (unless you're able to compute on the pointclouds from the internal DSP, I'm very interested in this, but I have no time) a combination of a regular camera and ultrasonic height measurement will do just fine I guess.\nWe'll probably have some thesis students working on this also.", "Look at this question + ansers: ", "Look at this question: ", " and my answer: ", " ", " ", "ASUS are supposed to be releasing the ", ", an Primesense SDK based device. It may be easier to obtain than the Primesense SDK. ", " for a Q2 2011 release.  Note that some photos show it without the color sensor (ie. depth only).", "Have you tried playing with the ", " in the recent OpenNI competition?", "Given bandwidth limitations, you may want to decimate the point cloud. ", " may be a good starting point.", " ", " ", "-> I haven't tried Primesense sensor but I'd say it is better to go with Kinect because there's more potential for it and lot of people are working in this direction (so more available sources). Also primesense programs libraries work with Kinect. I've used Mesa SR3K and SR4K but Kinect outperforms easily in speed, resolution and robustness."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "I have to buy a sensor for VSLAM (ie. Kinect or so): do you have any suggestions on which one should I choose? I unfortunately wasn't able to connect PrimeSense recently, is it even worth it, or should I stick with the original Kinect?", "Which stacks should I start playing with? I'm particularly interested in indoor navigation and attitude estimation (I could fuse with ", " based attitude data afterwards).", "I would then try to implement a distributed system (Desktop PC + BB-xM) via ROS, with both peers doing a part of calculations. How would you design such a system, what potential risks do you see right away?"], "url": "https://answers.ros.org/question/9087/vslam-based-navigation-with-kinect-where-do-i-begin/"},
{"title": "why cannot I tag one of my answers as answering my own question?", "time": "2011-03-10 04:07:52 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I had the case today where I asked a question and found the answer myself a few hours later.\nI then posted the solution by answering my own original question.", "Now I'd like to mark my original question as \"answered\" by clicking my solution as answering my original question. But the system insists that I cannot accept my own answer for my own question.", "I am afraid that with time, a lot of questions might be left unanswered and will pile up on the main page of ros answers.", "So here is the question: is this behaviour wanted?", "Raph"], "answer": [" ", " ", "This seems to be a useful (and definitely wanted) feature. I've opened an enhancement ticket for it on the ROS trac: ", ".", " ", " ", " ", " ", "Actually I just had the same problem and wanted to add a new thread about this. As it is not olimpic games we're having here, I think the risk of people actually trying to boost their karma by creating artificial questions is really low. I think no one sane would risk his/her reputation to do that. ", " allows accepting your own questions after 24 hours, I think this is good enough.", "I've just posted an answer to my own question, and would like to close the case so nobody else wastes her/his time to read my question. I'm not expecting any karma for that. Isn't there an option to not add karma for marking your own question right?", "Regards, Tom.", " ", " ", "I think the idea is you want one outside source to confirm the answer. I had a similar circumstance a day ago, and someone else came in and checked the \"answer\" to my own answer to the problem.", "Looking at it a little differently from a security perspective. You want to prevent someone from artificially \"answering\" all his questions and boost his karma. Since this is a self-moderated site, you want to prevent some lone guy from reeking havoc by editing/deleting comments since that's an ability you get once you've earned a high enough karma. Obviously, you don't want just anybody to have this power.", "-Shark"], "url": "https://answers.ros.org/question/9370/why-cannot-i-tag-one-of-my-answers-as-answering-my-own-question/"},
{"title": "Conversion accuracy from (latitude, longitude) to (x,y) by utm_odometry_node", "time": "2011-03-24 08:11:29 -0600", "post_content": [" ", " ", " I wrote node for subscribing to the odom topic(nav_msgs/Odometry) at which utm_odometry_node publish results of conversion from (latitude, longitude)(degres)to (x,y)(meters). So my question is is that conversion to (x,y) valid? \n(I use C Turtle.Code of node who is sunscribing to odom topic is here in answer of my orevious question:\n ", " ) ", "Example: \nThat gives gpsd_client: (coordinates in Croatia, nortwest) \nlatitude: 46.3954 \nlongitude: 16.448 \nThat gives utm_odometry_node: \nx:611320 \ny:5.139e+06 \nI think that in x coordinate one digit is missing. Well if someone is working with utm_odometry_node and know something about conversion(accuracy, problems, etc.),please let write answer. Thanks"], "answer": [" ", " ", "That is probably a correct output.", "You have to remember that UTM is in reference to some fixed point on a grid.  If you are significantly closer in the x direction, then it is totally reasonable that the x coordinate will be powers of 10 smaller than the y coordinate.", "As a quick sanity check, I use the Geotool: ", "This gives you UTM coordinates: 33T 611320 5138999, which seems to be in line with the ones that you got with the UTM odometry node.  The first digits represent the zone that you are in, which you can look up on a map like the following.  The next numbers describe your position, in reference to a central meridian plus a \"false northing\" and \"false easting\" to insure that negative numbers will never be a possibility.  ", "You may want to read up a bit more on UTM, ", " has a really good explanation of the reasoning behind UTM."], "url": "https://answers.ros.org/question/9558/conversion-accuracy-from-latitude-longitude-to-xy-by-utm_odometry_node/"},
{"title": "Exploration stack on p3dx [closed]", "time": "2011-03-16 02:06:10 -0600", "post_content": [" ", " ", " ", " ", "I am setting up exploration stack on p3dx with gmapping.", "When I launch the exploration node I get the following warning repeatedly", "The base_scan observation buffer has not been updated for 3.05 seconds, and it should be updated every 0.20 seconds.", "How should I resolve this? The robot rotates in the same position and after a while the exploration stops. Any problem with the base_scan topic here? Is this problem causing exploration to stop early?", "Just to verify what I did. \n1) I set static_map to false and rolling_window to \"true\"\n2) I am not running amcl node for localization\n3) Set the expected_update_rate for base_scan to 0.2", "Am I right?", "The \"base_scan not updated\" warning comes from the explore node. ", "The following warning comes from move_base node", "The robot's start position is off the global costmap. Planning will always fail, are you sure the robot has been properly localized?", "Should I post all my configuration files?", "global_costmap: ", "\n      global_frame: /map ", "\n      robot_base_frame: base_link ", "\n      update_frequency: 5.0 ", "\n      static_map: false   ", "local_costmap:\n      global_frame: odom ", "\n      robot_base_frame: base_link ", "\n      update_frequency: 5.0 ", "\n      publish_frequency: 2.0 ", "\n      static_map: false ", "\n      rolling_window: true ", "\n      width: 6.0 ", "\n      height: 6.0 ", "\n      resolution: 0.05   ", "TrajectoryPlannerROS: ", "\n      max_vel_x: 0.45 ", "\n      min_vel_x: 0.1 ", "\n      max_rotational_vel: 1.0 ", "\n      min_in_place_rotational_vel: 0.4 ", "\n      acc_lim_th: 3.2 ", "\n      acc_lim_x: 2.5 ", "\n      acc_lim_y: 2.5 ", "\n      holonomic_robot: true   ", "obstacle_range: 3.0 ", "\n       raytrace_range: 3.5 ", "\n       footprint: [[-0.2, 0.2], [-0.2, -0.2], [0.2, 0.2], [0.2, -0.2]] ", "\n       inflation_radius: 0.55 ", "\n       observation_sources: laser_scan_sensor ", "\n       laser_scan_sensor: {sensor_frame: /laser, data_type: LaserScan, topic: base_scan, marking: true, clearing: true, expected_update_rate: 0.5}   ", "move_base log", "[roscpp_internal] [2011-03-19 18:07:31,567] [thread 0xb6906740]: [DEBUG] XML-RPC call [searchParam] returned an error (-1): [Cannot find parameter [footprint_padding] in an upwards search] ", "\n    [roscpp_internal] [2011-03-19 18:07:37,936] [thread 0xb6906740]: [DEBUG] Publisher update for [/odom]:  already have these connections: ", "\n    [roscpp_internal] [2011-03-19 18:07:37,962] [thread 0xb6906740]: [DEBUG] XML-RPC call [searchParam] returned an error (-1): [Cannot find parameter [controller_frequency] in an upwards search] ", "\n    [ros.base_local_planner] [2011-03-19 18:07:37,962] [thread 0xb6906740]: [INFO] Sim period is set to 0.05 ", "\n    [roscpp_internal] [2011-03-19 18:07:38,002] [thread 0xb6906740]: [DEBUG] XML-RPC call [getParam] returned an error (-1): [Parameter [/move_base/TrajectoryPlannerROS/backup_vel] is not set] ", "\n    [roscpp_internal] [2011-03-19 18:07:38,003] [thread 0xb6906740]: [DEBUG] XML-RPC call [getParam] returned an error (-1): [Parameter [/move_base/TrajectoryPlannerROS/escape_vel] is not set] ", "\n    [roscpp_internal] [2011-03-19 18:07:38,010] [thread 0xb6906740]: [DEBUG] XML-RPC call [getParam] returned an error (-1): [Parameter [/move_base/TrajectoryPlannerROS/y_vels] is not set] ", "\n    [ros.costmap_2d] [2011-03-19 18:07:39,407] [thread 0xa869fb70]: [WARN] The base_scan observation buffer has not been updated for 0.79 seconds, and it should be updated every 0.50 seconds. ", "\n    [ros.costmap_2d] [2011-03-19 18:07:40,407] [thread 0xa869fb70]: [WARN] The base_scan observation buffer has not been updated for 1.79 seconds, and it should be updated every 0.50 seconds.      ", "robot_configuration.launch"], "answer": [" ", " ", " ", " ", "Can you tell which node is throwing this warning? You can use rxconsole for that.", "I think one of your node expects laser scans at 5 Hz and your hardware is somehow not powerful enough to deliver the scans on time.\nI would be on the ", " and the parameter ", ", but I am not sure.", "check for example local_costmap_params.yaml:", "If I am right, check the documentation of ", " and this navigation ", ", it explains how to set your parameters.\nIf you find which node throws this warning, you can look at its parameters in the wiki.", "Relative to the rotation behavior, have a look at these posts on the ros user list. They discuss the problem of rotating robots.", "Please post your findings, I am curious. :)", "Raph", " ", " ", "Hello Aradvindhan,", "May I ask a few more questions, exploring Eric's suggestion:", "Which laser scanner are you using?\nCan you actually see your laser scans in rviz in a simple setup?\nWhat frame are they published in (are you sure their frame is ", "?", "When you run ", " while your nodes are running, it generates a tf graph with the publishing frequencies of each transform.\nHave a look at the transform tree from your laser scanner (/laser) to the robot frame (/base_link), what are the publishing rates for these transforms?", "Raph"], "answer_code": ["local_costmap:\n  global_frame: odom\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 6.0\n  height: 6.0\n  resolution: 0.05\n"], "url": "https://answers.ros.org/question/9440/exploration-stack-on-p3dx/"},
{"title": "Is it better/prefered to combine nodelets programmatically or in a launch file?", "time": "2011-05-04 15:38:09 -0600", "post_content": [" ", " ", " ", " ", "Is there an advantage to combining nodelets programmatically? I noticed that stereo_image_proc does this and was surprised. ", "I'm attempting to chain image processing using nodelets to avoid the serialization overhead, and want to combine with stereo_image_proc, but was confused as to whether there was any reason I shouldn't just load up all of the constituent nodelets in a launch file."], "answer": [" ", " ", " ", " ", " is right, there's no advantage, infact if you do it in code you have to rewrite all of the remapping logic done by roslaunch, thus it's actually noteably not recommended.  ", "The image processing work was done to work around ", " It's been fixed on trunk and should be out in the next release, allowing the image pipeline work to be simplified.  ", " ", " ", "There's no advantage that I'm aware of in terms of performance.  Combining via launch files is definitely part of the power and flexibility that nodelets provide."], "url": "https://answers.ros.org/question/9894/is-it-betterprefered-to-combine-nodelets-programmatically-or-in-a-launch-file/"},
{"title": "What is the actual size of point cloud from a kinect.", "time": "2011-05-08 23:05:48 -0600", "post_content": [" ", " ", " ", " ", "Hello, all.", "As far as I know a kinect provides 640x480x3 size of 2D image data (widthxheightxRGB) and\n640x480x(sizeof a point in 3D). ", "Is there anyone who knows the actual size of a point cloud?", "640x480x4x3 ? widthxheightx(float)x(x,y,z) ?", "Cheers."], "answer": [" ", " ", " ", " ", "The data payload is 640 x 480 x 8 x sizeof(float) bytes = ", "Plus some bytes for auxiliary information like origin, timestamp etc.", "A good way to check out the ", " is ", "I store them as pcl::pointcloud. The data for one point is described in ", ", although it is somewhat cryptic. PointXYZRGB is described as follows:", "PCL_ADD_POINT4D and the union below will add 4 floats each. While that is overkill memorywise, it is required to benefit from some cpu optimizations. Therefore for a pcl::pointcloud<pcl::PointXYZRGB> from the kinect, you store ", " bytes.", "I wonder why rgb is in a separate struct though. Does anybody know this.", "For a PointCloud2 you can look at the message with ", ".\nThe interesting parts are:", "Therefore, also here you have 640 x 480 x 32, where 32 is 8 x sizeof(float)", "A colleague of mine modified the openni_camera driver and rgbdslam to communicate the color value via the unused forth dimension of the pointXYZ. ", " It is a little hackish, but if anyone wants the patch, you are welcome to contact me. Keep in mind though, that linear algebra operations with Eigen might be affected by the color value or overwrite it. Haven't experienced that yet though.\nThe bandwidth reduction shown by ", " ", " ", "It's probably worth pointing out that the Kinect itself does ", " output point clouds, but rather a depth map. That is, you can think of the Kinect output as being a 640x480 array (that's 307,200 entries) where each entry contains the Z-axis value of the point in the world corresponding to that entry. IIRC this Z-value is encoded with 11 bits but stored in 16 bits, so the size of the (depth, excluding colour) dataset coming off of the sensor is 307,200*2 = 614,400 bytes. At 30 fps that's about 18 MB/sec. However, when you project these depth values to 3D to get 3 floats per entry in the array the size of the dataset of course expands greatly.", "The point is, depending on your application, you might not need ", " of the depth information projected to 3D coordinates, at the full 30 fps. I don't know what your application is but here's a made-up example. Say you want to grab 3d models of faces that are within the Kinect's view. Your first stage is to use the (RGB) camera data to detect bounding boxes of faces. Then you want to find out the 3D points that are associated with those bounding boxes. This means that you really only want to project the depth data that is associated with the bounding boxes, not all of it.", "BTW with the openni_camera driver there is a way to do this; in the current (Diamondback) driver you can set the ", " parameter, and then publish ", " messages to ", ". The pointclouds output on ", " will then only contain projected points corresponding to the indices you asked for (i.e. the ones that correspond to the bounding boxes in the example above). This is what I did for my quadrotor kinect demo, to cut down the amount of 3D data to a reasonable level so that the onboard CPU could handle the calculations at the full 30 Hz.", "The newer openni_camera driver (openni_camera_unstable) looks like it provides more flexibility still -- you can subscribe directly to the raw depth information and do whatever you want with it.", " ", " ", " ", " ", "I can understand very easily.", "Almost 9Mbyte per frame...", "It's quite big data and need many computational power.", "9.83*30=294MBytes / sec...", "Thank you again.", "PS: How can you put source code on the web page?", " ", " ", " ", " ", "Could you please tell me how can I access color data?", "Is this right?", "where\npt,rgb,ptr and image1 are defined the following.", " ", " ", " ", " ", "Thank you Felix.", "I could get a this bandwidth by merging openni_nodelet.cpp given 320x240 configuration.", "The code is about changing from PointXYZRGB to PointXYZ and some modification of it.", "rgbd code that you sent me has compile error on node.cpp", "It looks simple but cut down half size of pcl.", "I would like to ask one more question.", "Why the size of PointXYZ is not 3", "float(16bytes)?", "Is this because of 16bytes alignment?", "Thank you again.", "Cheers."], "answer_code": ["rostopic bw", "$ rostopic bw /camera/rgb/points\nsubscribed to [/camera/rgb/points]\n(...)\naverage: 295.37MB/s\n    mean: 9.83MB min: 9.83MB max: 9.83MB window: 100\n", "00204 struct PointXYZRGB\n00205 {\n00206   PCL_ADD_POINT4D;  // (...)\n00207   union\n00208   {\n00209     struct\n00210     {\n00211       float rgb;\n00212     };\n00213     float data_c[4];\n00214   };\n00215   EIGEN_MAKE_ALIGNED_OPERATOR_NEW\n00216 } EIGEN_ALIGN16;\n", "rostopic echo -n 1 /camera/rgb/points| less", "height: 480\nwidth: 640\npoint_step: 32\n", "rostopic bw", "average: 148.35MB/s\n        mean: 4.92MB min: 4.92MB max: 4.92MB window: 100\n", "use_indices", "pcl/PointIndices", "camera/depth/indices", "camera/depth/points", "Thank you Felix.\n", "pct cloud;\nrgb.float_value=cloud.points[y*cloud.height+x].data[3];\n            ptr[3*x+1] = rgb.Red;\n            ptr[3*x+2] = rgb.Green;\n            ptr[3*x+3] = rgb.Blue;\n", "typedef pcl::PointXYZ        pt; \ntypedef pcl::PointCloud<pt> pct;\n\ntypedef union\n{\n  struct /*anonymous*/\n  {\n    unsigned char Blue;\n    unsigned char Green;\n    unsigned char Red;\n    unsigned char Alpha;\n  };\n  float float_value;\n  long long_value;\n} RGBValue;\n\nIplImage *Image1;\nuchar* ptr = (uchar*) (Image1->imageData + y * Image1->widthStep);\n", "/opt/ros/freiburg_tools/rgbdslam/src/node.cpp:820: error: \u2018class ParameterServer\u2019 has no member named \u2018getMinMatches\u2019\n", "rostopic bw /camera/rgb/points\nsubscribed to [/camera/rgb/points]\naverage: 35.81MB/s\n    mean: 1.23MB min: 1.23MB max: 1.23MB window: 21\naverage: 29.98MB/s\n    mean: 1.23MB min: 1.23MB max: 1.23MB window: 42\n"], "url": "https://answers.ros.org/question/9922/what-is-the-actual-size-of-point-cloud-from-a-kinect/"},
{"title": "Kinect with Beagleboard", "time": "2011-07-04 20:04:05 -0600", "post_content": [" ", " ", " ", " ", "Can Kinect run on Beagleboard using ROS?"], "answer": [" ", " ", " ", " ", " ", "I haven't tried it myself, but I think that the BeagleBoard is not powerful enough to keep up with the Kinect data rate.  I've heard of a few people having gotten the Kinect working on the beefier PandaBoard."], "url": "https://answers.ros.org/question/10478/kinect-with-beagleboard/"},
{"title": "Is visualization OK on laptops with Optimus+Bumblebee?", "time": "2011-06-25 11:06:10 -0600", "post_content": [" ", " ", "I'm mostly interested in whether rviz will work if I install ", " on a laptop that has NVIDIA graphics and Optimus (in which the NV card sends its output through the Intel graphics hardware, which sends everything out to the display).", "Has anyone tried this?"], "answer": [" ", " ", "The Optimus on the TurtleBot laptops does not currently work.  The Nvidia card cannot be used.  ", " ", " ", " ", " ", "My T410 Thinkpad has an optimus card. In order to get the NV drivers working under Ubuntu 10.10 I had to disable the Intel graphics in BIOS, after which everything worked fine (unfortunately, you don't get the battery savings that optmius integrated graphics would offer). I've had no problems with rviz + kinect point clouds on my laptop. ", " ", " ", "I have a Dell XPS15 with an Nvidia card. I'm running Ubuntu 11.04. Installing the proprietary Nvidia drivers would cause my laptop to not boot up. When I ran rviz without the Nvidia support, it would go very slow. I installed optimus+bumblebee, and right now I get excellent performance. ", "1 thing to note - you need to play around with the color parameter in optirun (rgb, yuv, proxy etc) to see what gives you best performance", " ", " ", "On Ubuntu 11.04 on a Dell XPS 15 L502X everything is working well for me. However, on another laptop (same model) it does not work for me well, with Ubuntu 10.04 and cturtle. "], "url": "https://answers.ros.org/question/10397/is-visualization-ok-on-laptops-with-optimusbumblebee/"},
{"title": "Using bumblebee camera for turtlebot", "time": "2011-06-15 14:52:01 -0600", "post_content": [" ", " ", "I'm trying to set up turtlebot but I'm using a bumblebee camera instead of a kinect. I was wondering if anyone has done this before and how to go about doing it.", "So far what I have done is commenting out the kinect node in robot.launch and adding a bumblebee node in there which looks like this", "When I start turtlebot, I will get a Bad Gyro Callibration under Sensors Error in Diagnostics from the turtlebot dashboard. Another anomaly I observed is that [kinect_breaker_enabler-8] process has finished cleanly \nin the terminal where I'm running robot.launch (Well, I did comment out the entire kinect node so this might happen but would this lead to any problems? and how do you solve it?)\nLogfile attached:", "(change .jpg to .log after download, they won't let me upload log files)", "I'm still quite new to this and I do not understand how openni is used in turtlebot(it seems to appear several times in the launch file, so I'm not sure if changing the camera would require me to reconfigure some things in openni) ", "I would be really grateful to anyone who could help me with this. "], "answer": [" ", " ", "To get rid of the bad gyro calibration error you need to see the has_gyro param to False since you do not have a gyro. If you would like to add a gyro you can follow the directions here: ", "The kinect_breaker_enable is a boot strapping thing we did to turn on the breaker to power the kinect. you can remove it from the base.launch file. But it shouldn't effect anything you're doing all it does is come up and request the breaker to turn on and then exits.", "On a side note, you should be aware that if you do not get good depth information from the bumblebee2 you may not be able to navigate well. Also not having a gyro will greatly diminish your ability to navigate. ", " ", " ", "edit the driver.launch file to include the has gyro param"], "question_code": ["  <!-- Bumblebee2 -->\n  <node pkg=\"bumblebee2\" type=\"bumblebee2\" name=\"bumblebee2\"\n    output=\"screen\" respawn=\"true\">\n    <param name=\"video_mode\" value=\"FORMAT7_3\" />\n    <param name=\"fps\" value=\"15\" />\n    <param name=\"gain\" value=\"auto\" />  \n    <param name=\"brightness\" value=\"auto\" />\n    <param name=\"whitebalance\" value=\"auto\" /> \n    <param name=\"shutter\" value=\"auto\" />   \n    <param name=\"bayer_pattern\" value=\"BGGR\" /> \n    <param name=\"bayer_method\" value=\"NONE\" /> \n  </node>\n  <env name=\"ROS_NAMESPACE\" value=\"bumblebee2\" />\n  <node pkg=\"stereo_image_proc\" type=\"stereo_image_proc\" name=\"stereo_image_proc\" />\n"], "answer_code": ["<node pkg=\"turtlebot_node\" type=\"turtlebot_node.py\" name=\"turtlebot_node\" output=\"screen\" respawn=\"true\" args=\"--respawnable\">\n  <param name=\"bonus\" value=\"false\" />\n  <param name=\"has_gyro\" value=\"false\" />\n  <param name=\"update_rate\" value=\"30.0\" />\n</node>\n"], "url": "https://answers.ros.org/question/10278/using-bumblebee-camera-for-turtlebot/"},
{"title": "prosilica intrinsincs parameters failed to load", "time": "2011-06-13 21:40:23 -0600", "post_content": [" ", " ", " ", " ", "Hey All,", "I'm experiencing problems with a GC1380C prosilica camera. The camera is connected through a fast Ethernet interface, which I know is not optimal but still the SampleViewer works. I've access to different parameters, I can modify them, I can see the video stream etc...\nHowever, whenever I'm trying to use the camera as follow : roslaunch prosilica.launch, I'm getting error :", "The IP address in prosilica.launch is the good one. Any idea what could be the origin of this error ? ", "Thank you very much,"], "answer": [" ", " ", " ", " ", "Thank very much for your help. It tried both methods. I actually have dropped packets. \nThe log is :", "From the Sample Viewer, I tune the packet size to 1500 which seems to work. However, it seems that the driver tries to optimize this size and as a result change it a larger value, which cause the Sample Viewer to crash too. When I use the SDK taken from Prosilica website, I do not encounter such issues using the same fast Ethernet interface. I can't actually change to network interface as I'm using a laptop. I'm currently checking if there is a problem in the driver code.", "Hi\nDid you find the problem in the end? I am also having problems with the prosilica_camera. I also get \"failed to load intrinsics from camera\"", " ", " ", "OK, that's interesting. The driver does try to optimize the packet size; the relevant code is in ", " in ", ":", "8228 is a Jumbo Frame size, and the camera default on power up. ", " is supposed to determine the maximum packet size supported by the system, and configure the camera to use that value or ", ", whichever is lower.", "It sounds like there's something kooky about your network card or system that's causing it to report an incorrect max packet size.", "Can you try changing ", " to 1500, recompile prosilica_camera, and see if that works?", "For reference, ", " instead does:", "Which seems a bit odd. That code will never increase the packet size, only decrease it.", " ", " ", "Well, since you suggested it, I have been trying to do the calibration step as written in the troubleshooting page. The exception actually cause the calibration to fail as I'm not able to access the video stream. ", " ", " ", "Have you tried manually setting the StreamBytesPerSecond configuration in the Prosilica program? and restarting in Linux to run ROS?\nIt seems like a failure because of the fast ethernet (switching back to default gigabit because of unset value).", " ", " ", "Adjusting the MTU only works if your entire network setup supports this, otherwise a router or NIC might chop them up again in smaller frames.", " ", " ", " ", " ", "We've seen this sort of thing before, e.g. ", ". Usually it's because of too many dropped packets between the camera and computer. Can you please post the diagnostics - before starting the Prosilica, do:", "Then start the Prosilica driver with ", " (", " starts the camera in polled mode, so it won't automatically stream images). Look at the Packet Statistics component in the runtime monitor. On a properly configured GigE network there shouldn't be any dropped packets. On Fast Ethernet some (warning level) lost packets are expected, but things should still work.", "If the error always happens on the very first diagnostics collection, we'll have to try something else. But hopefully that's not the case.", " ", " ", "Did you run the ", " on this camera? After the camera calibration ran successfully the camera calibration matrices are stored in the internal eeprom of the camera and loaded each time the node starts."], "question_code": ["SUMMARY\n========\n\nPARAMETERS\n * /rosversion\n * /prosilica_driver/trigger_mode\n * /rosdistro\n * /prosilica_driver/ip_address\n\nNODES\n  /\n    prosilica_driver (prosilica_camera/prosilica_node)\n\nauto-starting new master\nprocess[master]: started with pid [9439]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to 1ee20ff0-9668-11e0-92a6-705ab6776520\nprocess[rosout-1]: started with pid [9452]\nstarted core service [/rosout]\nprocess[prosilica_driver-2]: started with pid [9455]\n[ WARN] [1308043480.937092818]: Detected max data rate is 12400000 bytes/s, typical         maximum data rate for a GigE port is 115000000 bytes/s. Are you using a GigE network card and cable?\n\n[ INFO] [1308043480.950345618]: Found camera, guid = 37112\n[ WARN] [1308043480.951187554]: Failed to load intrinsics from camera\nterminate called after throwing an instance of 'prosilica::ProsilicaException'\n  what():  Couldn't get range of attribute StreamBytesPerSecond: Camera was unplugged\n[prosilica_driver-2] process has died [pid 9455, exit code -6].\nlog files: /home/iis-xs/.ros/log/1ee20ff0-9668-11e0-92a6-705ab6776520/prosilica_driver-    2*.log\n^C[rosout-1] killing on exit\n[master] killing on exit\nshutting down processing monitor...\n... shutting down processing monitor complete\n"], "answer_code": ["[rospy.client][INFO] 2011-06-16 17:00:10,089: init_node, name[/runtime_monitor_11983_1308236410083], pid[11983]\n[xmlrpc][INFO] 2011-06-16 17:00:10,090: XML-RPC server binding to 0.0.0.0\n[xmlrpc][INFO] 2011-06-16 17:00:10,090: Started XML-RPC server [http://iis-xs-Satellite-Pro-L500:34239/]\n[rospy.init][INFO] 2011-06-16 17:00:10,090: ROS Slave URI: [http://iis-xs-Satellite-Pro-L500:34239/]\n[rospy.impl.masterslave][INFO] 2011-06-16 17:00:10,090: _ready: http://iis-xs-Satellite-Pro-L500:34239/\n[xmlrpc][INFO] 2011-06-16 17:00:10,093: xml rpc node: starting XML-RPC server\n[rospy.registration][INFO] 2011-06-16 17:00:10,094: Registering with master node http://localhost:11311\n[rospy.init][INFO] 2011-06-16 17:00:10,191: registered with master\n[rospy.rosout][INFO] 2011-06-16 17:00:10,192: initializing /rosout core topic\n[rospy.rosout][INFO] 2011-06-16 17:00:10,196: connected to core topic /rosout\n[rospy.simtime][INFO] 2011-06-16 17:00:10,199: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic\n[rospy.internal][WARNING] 2011-06-16 17:00:28,642: Unknown error initiating TCP/IP socket to iis-xs-Satellite-Pro-L500:38642 (http://iis-xs-Satellite-Pro-L500:56842/): Traceback (most recent call last):\n  File \"/opt/ros/diamondback/stacks/ros_comm/clients/rospy/src/rospy/impl/tcpros_base.py\", line 472, in connect\n    self.socket.connect((dest_addr, dest_port))\n  File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n    return getattr(self._sock,name)(*args)\nerror: [Errno 111] Connection refused\n", "Camera::setup()", "src/libprosilica/prosilica.cpp", "// adjust packet size according to the current network capacity\ntPvUint32 maxPacketSize = 8228;\nPvCaptureAdjustPacketSize(handle_, maxPacketSize);\n", "PvCaptureAdjustPacketSize", "maxPacketSize", "maxPacketSize", "SampleViewer", "tPvUint32 lMaxSize = 8228;\n\n// get the last packet size set on the camera\nPvAttrUint32Get(lHandle,\"PacketSize\",&lMaxSize);\n// adjust the packet size according to the current network capacity\nPvCaptureAdjustPacketSize(lHandle,lMaxSize);\n", "rosrun runtime_monitor monitor\n", "streaming.launch", "prosilica.launch"], "url": "https://answers.ros.org/question/10248/prosilica-intrinsincs-parameters-failed-to-load/"},
{"title": "Advice ROS Single Board Computer - (Kinect, +other sensors)", "time": "2011-07-23 08:07:24 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have a robotic platform that I wish to put ROS and the Kinect sensor on.", "I would like some advice on the hardware for the robotic platform I am building. My current robot platform has blackfin processors which do not have USB interfaces. I need a board that will be powerful enough to process the Kinect data, and be able to accept all the camera, sensors, etc that I already have. I need ports for I2C, UART, additional USB for webcam etc.", "I looked at the Beagleboard, and it was suggested it would not be powerful enough to handle the Kinect.", "It was suggested to me that a mini-itx board would be a good replacement. ", "How would I integrate the i2C, motor controller, and other components into this type of board? Is there some kind of I2C and various connector expansion pack I can get for this?", "Is there another signle board computer that will work well?\nAny advice is appreciated."], "answer": [" ", " ", "You have two ways of integrating the I/O interfaces:", "Look for an embedded board that supports all your requirements.", "Buy a embedded board with good performance and use a microcontroller or similar (PC104) for additional interfaces.", "I think option 2) will be much simpler. Especially I2C is not necessarily available and if you want to process Kinect data you don't want to compromise on power. The arduino is a comparably easy way to get I2C and depending on the data rate you can get additional UARTs.", "When you look into embedded boards in comparison to a standard board, you will usually find that you get things like multiple UARTs which can be really nice and is fading out in consumer products.", "We have a NANO-PV-D510A and a WAFER-945GSE board and both work well, although they not up 2 date on power.", "I think, the ", " is used quite often.", " ", " ", "Thanks for the response. I have been looking at various boards and really am not sure what will work with ROS and Kinect well. Were you using the boards you mentioned with a robotic application and the Kinect sensor?", " ", " ", "Take a look here:\n"], "url": "https://answers.ros.org/question/10701/advice-ros-single-board-computer-kinect-other-sensors/"},
{"title": "changing binning_x,binning_y", "time": "2011-08-02 17:57:53 -0600", "post_content": [" ", " ", "Hi,", "When I change the binning_x and binning_y of prosilica_driver from reconfigure_gui,\nthe color of image_raw turns to black and white. \nAre there are any ways to keep image_raw colored when I change binnings?", "Thanks in advance,"], "answer": [" ", " ", "No. Color cameras generally work by imposing a Bayer pattern on the image sensor, where each pixel is covered by a filter so that it detects only one color channel (red, green or blue). For example, an RGGB Bayer camera gives you raw images laid out like this:", "This is a one-channel image, where each pixel only has information about a single color. Transforming the raw Bayer image into a normal, 3-channel color image is called debayering. There are numerous debayering algorithms of varying sophistication, but the idea is always to interpolate the missing color data from nearby pixels. In ROS, debayering is performed by ", ".", "Binning changes how the camera accumulates the charge at each pixel. With 2x2 binning, 2x2 blocks of pixels are combined into super-pixels using the summed charges of the 4 actual pixels. In this case, every 2x2 Bayer pattern block gets averaged into a monochrome super-pixel:", "You no longer have distinct samples for each color in the raw image.", "There are some cameras with a \"color binning\" feature, which use more complicated circuitry to accumulate each color separately and thus preserve the Bayer pattern in the binned image. As far as I know there aren't any Prosilica cameras with this feature."], "answer_code": ["RG  RG  RG  RG  ...\nGB  GB  GB  GB  ...\n\nRG  RG  RG  RG  ...\nGB  GB  GB  GB  ...\n", "M M M M ...\nM M M M ...\n\nM M M M ...\nM M M M ...\n"], "url": "https://answers.ros.org/question/10801/changing-binning_xbinning_y/"},
{"title": "Is there an Inverse Kinematics Teleop system available anywhere?", "time": "2011-07-17 22:32:37 -0600", "post_content": [" ", " ", " ", " ", "I've been playing around with the ", " from the upcoming E-Turtle distribution and I must say, it's very cool. :)", "As far as I can see though, it only allows you to preview the results of an IK plan or of posing the joints in a kinematic chain. Ideally what I'd like is to be able to pose my robot in RVIZ using the interactive markers and then press another button that moves my robot to the new pose.", "Has anyone written such a system or does someone with connections to Willow Garage know if such a program is planned for inclusion in the forthcoming E-Turtle release?"], "answer": [" ", " ", "Hi,", "We are currently developing a more comprehensive tool for interacting with Planning Scenes (the stuff that your robot is trying to not run into) - this tool allows you to create planning scenes, save to and load from a database, create different motion plan requests, generate, save, and visualize trajectories, and send those trajectories to controllers and record the results.   It should be a really neat and powerful tool.  We have a working prototype for the PR2 - we haven't yet made this robot agnostic, but that's a near term goal.   ", "This tool will be released with Electric, but not in the arm_navigation stack - we're not ready to label this a 1.0 tool yet.  Once we've got a release out we'll be adding a tutorial.", "And if you have any feedback on tools like the Planning Components Visualizer or the tutorials, we'd love to hear them!"], "url": "https://answers.ros.org/question/10624/is-there-an-inverse-kinematics-teleop-system-available-anywhere/"},
{"title": "Kinect power - Create shutdown issue", "time": "2011-07-27 02:40:08 -0600", "post_content": [" ", " ", " ", " ", "Hi,", " I am having some issues where my create is powering down after the kinect is on for a few minutes. I am worried it has something to do with my circuit I made  ", " . I could have something done incorrectly on the circuit. * The setup I currently have is what that tutorial shows. Coming off the create power outputs. * ", "I was wondering, since I am using a full ATX power supply can I just take a 12volt line and use that for powering my kinect? Is there any drawbacks issues. *My power supply is a DC power supply for car PCs. *", "Thanks"], "answer": [" ", " ", "Experiments showed me the following:", "Kinect can actually handle a small voltage swing pretty well. I had no problems with 11 to 13.5V although I would advice against it. Using a typical 7812 or LM1084-12 is the best option. Also a ATX output provides a stable 12V (sometimes they do need a minimal load).\nThe kinect PSU is rated for 1 or 1.5A, but using the Kinect with camera + pointcloud only, I never got it above 500mA. \nHowever the current peaks can bring the robot down. (both lowering the battery voltage and draining the battery faster)\nThis was not tested on an Create but on a Roomba. \nThis is why we choose to sell our Turtlebots with a battery connection.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base.", " ", " ", "So what is your setup? How is your circuit powered?", "I am assuming you take the regulator circuit from the wiki link and power that by the ATX supply's 12V line?", "That probably won't work as the regulator wants >14.5V to get down to the regulated 12V. If you measure the output voltage I think you will observe something like 10V.", "You can use another regulator (9-18V or 9-36V input voltages are quite common), but that might be more expensive or provide higher input voltage.", "Using the 12V from the ATX might also work. I don't know how nicely regulated the ATX voltage is.", " ", " ", "I'll try the 12v ATX power and see what happens. Its suppose to be about 1.5 amps coming off of a standard atx power cable. I can verify with manufacture.", "This should be the correct power settings for the kinect (12volt 1.5 amps) ?", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "url": "https://answers.ros.org/question/10746/kinect-power-create-shutdown-issue/"},
{"title": "what does the intensity channel of a point cloud refer to?", "time": "2011-08-25 08:33:53 -0600", "post_content": [" ", " ", "From the rviz wiki, it seems intensity is related to colour? What type of information is encoded in the intensity of a point in a point cloud? "], "answer": [" ", " ", " ", " ", "This is typically a sensor-dependent \"brightness\" of return, representing some quality of measurement: energy returned, pixel value, etc.  Sometimes it can look like a greyscale brightness.  See the ", " to view some images produced by a laser scanner returning intensities.", "Just be aware that sometimes it isn't human color dependent.  Some materials reflect IR laser beams very well but appear black to our eyes."], "url": "https://answers.ros.org/question/11020/what-does-the-intensity-channel-of-a-point-cloud-refer-to/"},
{"title": "urdf in rviz lags", "time": "2011-08-26 01:07:29 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am seeing my urdf model, and all tf frames specified by robot_state_publisher, lag behind the actual robot motion (which the transform /odom->/base_footprint appears to track perfectly).  The lag is around 5 seconds.", "I have also tried with a simplified .urdf model that uses only fixed joints (and thus removes /joint_state publications as a potential cause).  No help.", "I have set the ~publish_frequency param of robot_state_publisher to 10 hz, and I have a static_transform_publisher publishing at 10 hz to link the top-level urdf frame (/chassis) to the /base_footprint frame published by the rest of the localization system.", "Any bright ideas?  Thanks!", "PS - using diamondback and Ubuntu 10.10"], "answer": [" ", " ", "Yup, good suggestions!  Indeed, I ", " operating over wifi, and today I don't see the problem (after machines have power cycled and been connected to the internet).", "Diagnosis: must have been a time sync issue.", "Thanks!"], "url": "https://answers.ros.org/question/11027/urdf-in-rviz-lags/"},
{"title": "TurtleBot Calibration -- Still waiting for scan", "time": "2011-09-02 06:22:10 -0600", "post_content": [" ", " ", " ", " ", "I've been following the turtlebot tutorial (", ")\nI am stuck on 5.TurtleBot Odometry and Gyro Calibration", "When I run ", "\n$ roslaunch turtlebot_calibration calibrate.launch, ", "\nI get the following output, and the final message \"Still waiting for scan\" repeats forever.", "Any suggestions? ", "\nI am using the full kit that comes with everything (irobot, kinect, eeepc, etc).", "Thanks!"], "answer": [" ", " ", "It sounds like your TurtleBot software on your TurtleBot might not be up to date:", "then: ", " ", " ", "It sounds like the Kinect is not powered on.  Can you launch the turtlebot dashboard and make sure that the turtlebot is in active mode (by clicking on the gear) and that breaker 0 is green.  If not click on the gear and change it to active mode.  ", " ", " ", "Thanks! That updating the software helped a bit. It now gets past the previous error, and the robot starts rotating. However, before completing a full turn, it stops and the wrench icon on the dashboard becomes red. Dashboard says Power System: Error. Further clicking reveals: Level: Error. Message: Error.", "Below is how it looks on the terminal.", "BTW, you must be Melonee from the Youtube videos. Thanks for the help! I might be asking more questions in the future.", "--David"], "question_code": ["\nturtlebot@turtlepc:~$ roslaunch turtlebot_calibration calibrate.launch\n... logging to /home/turtlebot/.ros/log/ff3cc802-d58c-11e0-8856-485d607f5c5a/roslaunch-turtlepc-13025.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://192.168.22.137:60654/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosversion\n * /rosdistro\n * /scan_to_angle/min_angle\n * /scan_to_angle/max_angle\n\nNODES\n  /\n    scan_to_angle (turtlebot_calibration/scan_to_angle.py)\n    turtlebot_calibration (turtlebot_calibration/calibrate.py)\n\nROS_MASTER_URI=http://192.168.22.137:11311\n\ncore service [/rosout] found\nprocess[scan_to_angle-1]: started with pid [13049]\nprocess[turtlebot_calibration-2]: started with pid [13050]\n[INFO] [WallTime: 1314988172.573716] Estimating imu drift\n[INFO] [WallTime: 1314988172.875424] Still waiting for imu\n[INFO] [WallTime: 1314988173.176998] Still waiting for scan\n[INFO] [WallTime: 1314988173.478575] Still waiting for scan\n[INFO] [WallTime: 1314988173.780582] Still waiting for scan\n[INFO] [WallTime: 1314988174.082496] Still waiting for scan\n[INFO] [WallTime: 1314988174.384073] Still waiting for scan\n[INFO] [WallTime: 1314988174.685638] Still waiting for scan\n[INFO] [WallTime: 1314988174.987211] Still waiting for scan\n[INFO] [WallTime: 1314988175.289460] Still waiting for scan\n[INFO] [WallTime: 1314988175.591138] Still waiting for scan\n[INFO] [WallTime: 1314988175.893684] Still waiting for scan\n[INFO] [WallTime: 1314988176.195267] Still waiting for scan\n[INFO] [WallTime: 1314988176.496867] Still waiting for scan\n[INFO] [WallTime: 1314988176.798562] Still waiting for scan\n"], "answer_code": ["sudo apt-get update\n", "sudo apt-get install ros-diamondback-turtlebot-robot\n", "\ncore service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [7908]\nprocess[openni_manager-2]: started with pid [7909]\nprocess[openni_camera-3]: started with pid [7910]\nprocess[pointcloud_throttle-4]: started with pid [7911]\nprocess[kinect_laser-5]: started with pid [7912]\nprocess[kinect_laser_narrow-6]: started with pid [7923]\nprocess[scan_to_angle-7]: started with pid [7940]\nprocess[turtlebot_calibration-8]: started with pid [7952]\n[INFO] [WallTime: 1314997822.154698] Estimating imu drift\n[INFO] [WallTime: 1314997822.456846] Still waiting for imu\n[INFO] [WallTime: 1314997822.759576] Still waiting for scan\n[ INFO] [1314997822.951332371]: [/openni_camera] Number devices connected: 1\n[ INFO] [1314997822.952506928]: [/openni_camera] 1. device on bus 001:31 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00364A10656048A'\n[ WARN] [1314997822.959441340]: [/openni_camera] device_id is not set! Using first device.\n[ INFO] [1314997823.019817499]: [/openni_camera] Opened 'Xbox NUI Camera' on bus 1:31 with serial number 'A00364A10656048A'\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/2f26eecc-d5a5-11e0-b536-485d607f5c5a/kinect_breaker_enabler-1*.log\n[ INFO] [1314997823.164011946]: rgb_frame_id = 'kinect_rgb_optical_frame' \n[ INFO] [1314997823.181211203]: depth_frame_id = 'kinect_depth_optical_frame' \n[INFO] [WallTime: 1314997833.370081] Still waiting for imu\n[INFO] [WallTime: 1314997833.672666] Still waiting for scan\n[INFO] [WallTime: 1314997833.974910]  ... imu drift is -0.243444 degrees per second\n[INFO] [WallTime: 1314997833.976291] Aligning base with wall\n[INFO] [WallTime: 1314997834.279068] Still waiting for imu\n[INFO] [WallTime: 1314997834.581917] Still waiting for scan\n\n"], "url": "https://answers.ros.org/question/11102/turtlebot-calibration-still-waiting-for-scan/"},
{"title": "navigation stack with errors on launch", "time": "2011-07-17 21:36:31 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am using a Pioneer 3dx with a Sick lms 200 Laserscanner. I wanted to access the robot via ROS and the already supported p2os_driver. Everything works fine including the transformation from base_link to laser_scanner and so on. Now my Problem:", "I did the navigation stack tutorial and created the files like its told in the tutorial. I started all relevant things (p2os driver for odom and tf, laser_wrapper, transformation from laser to baselink and slam gmapping for the map) after that I launched the move_base.launch file and got some warnings and errors:", "[ WARN] [1310979131.605673459]:\n  Planner specifications should now\n  include the package name. You are\n  using a deprecated API. Please switch\n  from NavfnROS to navfn/NavfnROS in\n  your yaml file.", "In which file do I have to adjust the path? I can't find it.", "[ERROR] [1310979131.801336625]: You\n  are using acc_limit_x where you should\n  be using acc_lim_x. Please change your\n  configuration files appropriately. The\n  documentation used to be wrong on\n  this, sorry for any confusion.", "I checked the used yaml-Files and I am using only acc_lim_x variables. The error occurs for x,y and th. Where else can this error come from?", "[ WARN] [1310978768.698218184]: Map\n  update loop missed its desired rate of\n  2.0000Hz... the loop actually took 2.1595 seconds", "Whats that? I am totally confused with all these warnings and errors. ", "Same with that warning:", "[ WARN] [1310979131.859018260]: The\n  backup_vel parameter has been\n  deprecated in favor of the escape_vel\n  parameter. To switch, just change the\n  parameter name in your configuration\n  files.", "Despite of all warnings I started rviz and was happy to see the occupacy grid map but the rest of the navigation didnt work. There are warnings in rviz that the topics dont recieve any data... I tried to set an estimated pose and navigate to somewhere but no chance of moving the bot. Why? What is wrong with my config?", " I read everything about my problem here in the  ", "  but couldn't find anything helpful so i created an own question. ", "thanks for your help.", "Jan", "The code of the config-files:", "navigation.launch-File", "</launch>", "I already tried the diff_amcl.launch in the examples but there are even more errors and the map cant be found...", "local_costmap_params.yaml: "], "answer": [" ", " ", "Ok I solved the problem. I had to set the ROS_IP to my own ip. I am using two Ubuntu machines cause the pc on the bot is not powerful enough for rviz and so on. All I had to do was setting the ROS_IP to my local machines IP and the set_2d_nav_goal of rviz worked! ", "Thank you all for your help", " ", " ", "The parameter dump above looks more reasonable. A couple of questions/comments. ", "You should make sure that your \"inflation_radius\" parameter is set to be larger than the radius of the robot. I'm actually surprised that you're not getting a warning about this because you should be.", "Have you tried to \"rostopic echo cmd_vel\" to see if move_base is trying to publish velocity commands. ", "Why are you running both gmapping and AMCL? It should probably be one or the other.  ", "Does the global plan created by the navigation stack look reasonable in rviz? ", "Have you looked at the ", " to make sure that things are set up more or less correctly for navigation.", " ", " ", "You should make sure that your\n  \"inflation_radius\" parameter is set to\n  be larger than the radius of the\n  robot. I'm actually surprised that\n  you're not getting a warning about\n  this because you should be.", "I get a warning about the inflation_radius but this was not an important warning to me. I set the inflattion to a lower value because there would be no space to move my bot if i do it as big as the robots shape or bigger. This was my first thought to lower the inflation so that it would be possible for the bot to move.", "Have you tried to \"rostopic echo\n  cmd_vel\" to see if move_base is trying\n  to publish velocity commands.", "I did echo the cmd_vel but nothing is publsihed. When I start the simple_navigation_goals there is a speed value published on the cmd_vel topic.", "Why are you running both gmapping and\n  AMCL? It should probably be one or the\n  other.", "I am not sure which topics, nodes and so on to run. I tried it like that. So if I start the gmapping I should not start AMCL with the navigation? Ok I will try this now.", "Does the global plan created by the\n  navigation stack look reasonable in\n  rviz?", "In RVIZ there is no Global or Local Plan created when I set a navigation goal. So I cant tell you if it is a good plan or not. The plan that is created by the simple_navigation_goals function looks pretty good.", "Have you looked at the navigation\n  tuning guide to make sure that things\n  are set up more or less correctly for\n  navigation.", "I will do that now but the things have to be set correctly cause I use the param-Files of the p2os_launch stack.", " ", " ", " ", " ", "Yeah my posted warnings are a bit confusing to myself. Sometimes i got these warnings sometimes other warnings. I tried a lot with the yaml files and got a lot of strange warnings. Not all of them at the same time... sorry. I wanted to show you all errors i get but these are not based on one configuration set. I tried a lot and got a version now without errors and warnings! But I dont know how i did this. ", "The nodes and topics I run :", "The funny thing is, the warnings are gone but it is still not working. I am running 2 Systems. One on the Robot and one I work with. Both are installed with Ubuntu 10.04LTS. ", "This is my param dump", "Are you using a pioneer bot too? Perhaps the problem is the bot itself and not the running software? How can I move the bot from A to B by a command?? I tried the simple_navigation_goals tutorial and it was possible to send the bot a command to move a bit. Normally the bot should move 1 meter but he moves so slow as if there is something completely wrongwith the bot. Normal joystick movement works perfect. Perhaps these information can help you to see whats going wrong with my bot. If you need some more code/config params of the actual setting let me know. ", " ", " ", "Based on those configuration files, I'm surprised at some of the warnings that you're getting. Are you sure that you don't have any stale parameters on the parameter server for some reason? In particular, if you haven't set the \"backup_vel\" parameter, you really shouldn't get a warning about it being set. Also, you've set the frequency for map updates to 5Hz, but the warning you posted complains about it not hitting 2Hz... something there seems strange. Can you post the output from a \"rosparam dump\" so that we can see what parameters are actually set?", "One thing I did notice, is that you'll probably want to set \"static_map\" to true and \"rolling_window\" to false in your global_costmap_params.yaml file if you plan to use AMCL to localize and want to create plans that go across the entire map.", "Also, out of curiosity, what version of navigation are you running? What kind of computer are you trying to run things on?", " ", " "], "question_code": ["<launch>\n<master auto=\"start\"/>\n\n<!-- Run the map server (you can run it here or in another terminal) -->\n<!-- <node name=\"map_server\" pkg=\"map_server\" type=\"map_server\" args=\"path_to_map.yaml\"/> -->\n\n<!--- Run AMCL -->\n<include file=\"$(find p2os_launch)/amcl.launch\" />\n\n<node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"$(find p2os_launch)/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"$(find p2os_launch)/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"$(find p2os_launch)/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find p2os_launch)/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find p2os_launch)/base_local_planner_params.yaml\" command=\"load\" />\n    <param name=\"base_global_planner\" type=\"string\" value=\"navfn/NavfnROS\" />\n    <!--  <param name=\"conservative_reset_dist\" type=\"double\" value=\"3.0\" /> -->\n    <!--  <param name=\"controller_frequency\" type=\"double\" value=\"15.0\" /> -->\n</node>\n", "local_costmap:\nglobal_frame: /odom\nrobot_base_frame: base_link ..."], "answer_code": ["roscore &\nrosrun p2os_driver p2os\nrosrun p2os_dashboard p2os_dashboard & (I enabled the motors)\nroslaunch p2os_launch gmapping_tf_lrf_p2os_teleop_joy.launch &\nroslaunch p2os_launch navigation.launch &\n", "axis_vw: 2\naxis_vx: 1\naxis_vy: 0\ndeadman_button: 5\nrosdistro: diamondback\nroslaunch:\n  uris: {host_flyingdutchman__46266: 'http://FlyingDutchman:46266/', host_flyingdutchman__50524: 'http://FlyingDutchman:50524/'}\nrosversion: 1.4.7\nrun_button: 4\nrun_id: c7d0f6ec-b695-11e0-9e1b-00012e270b37\nsicklms_node: {baud: 500000, port: /dev/ttyUSB0, resolution: 1}\nslam_gmapping: {map_update_interval: 1.5}\n", "amcl: {gui_publish_rate: 10.0, kld_err: 0.050000000000000003, kld_z: 0.98999999999999999,\n      laser_lambda_short: 0.10000000000000001, laser_likelihood_max_dist: 2.0, laser_max_beams: 30,\n      laser_model_type: likelihood_field, laser_sigma_hit: 0.20000000000000001, laser_z_hit: 0.5,\n      laser_z_max: 0.050000000000000003, laser_z_rand: 0.5, laser_z_short: 0.050000000000000003,\n      max_particles: 5000, min_particles: 500, odom_alpha1: 0.20000000000000001, odom_alpha2: 0.20000000000000001,\n      odom_alpha3: 0.80000000000000004, odom_alpha4: 0.20000000000000001, odom_alpha5: 0.10000000000000001,\n      odom_frame_id: odom, odom_model_type: diff, recovery_alpha_fast: 0.0, recovery_alpha_slow: 0.0,\n      resample_interval: 1, transform_tolerance: 0.10000000000000001, update_min_a: 0.5,\n      update_min_d: 0.20000000000000001, use_map_topic: true}\n    move_base:\n      TrajectoryPlannerROS: {acc_lim_th: 3.2000000000000002, acc_lim_x: 2.5, acc_lim_y: 2.5,\n        goal_distance_bias: 0.59999999999999998, holonomic_robot: true, max_rotational_vel: 0.59999999999999998,\n        max_vel_x: 0.80000000000000004, min_in_place_rotational_vel: 0.5, min_vel_x: 0.20000000000000001,\n        path_distance_bias: 0.59999999999999998, sim_time: 2.0}\n      aggressive_reset: {reset_distance: 1.8400000000000001}\n      base_global_planner: navfn/NavfnROS\n      conservative_reset: {reset_distance: 3.0}\n      conservative_reset_dist: 3.0\n      controller_frequency: 15.0\n      global_costmap:\n        footprint:\n        - [0.254, -0.050799999999999998]\n        - [0.17780000000000001, -0.050799999999999998]\n        - [0.17780000000000001, -0.17780000000000001]\n        - [-0.1905, -0.17780000000000001]\n        - [-0.254, 0]\n        - [-0.1905, 0.17780000000000001]\n        - [0.17780000000000001, 0.17780000000000001]\n        - [0.17780000000000001, 0.050799999999999998]\n        - [0.254, 0.050799999999999998]\n        global_frame: /map\n        inflation_radius: 0.10000000000000001\n        laser_scan_sensor: {clearing: true, data_type: LaserScan, marking: true, sensor_frame: laser,\n          topic: scan}\n        map_type: costmap\n        observation_sources: laser_scan_sensor\n        obstacle_range: 3.5\n        raytrace_range: 4.0\n        robot_base_frame: base_link\n        rolling_window: true\n        static_map: false\n        transform_tolerance: 0.20000000000000001\n        update_frequency: 5.0\n      local_costmap:\n        footprint:\n        - [0.254, -0.050799999999999998]\n        - [0.17780000000000001, -0.050799999999999998]\n        - [0.17780000000000001, -0.17780000000000001]\n        - [-0.1905, -0.17780000000000001]\n        - [-0.254, 0]\n        - [-0.1905, 0.17780000000000001]\n        - [0.17780000000000001, 0.17780000000000001]\n        - [0.17780000000000001, 0.050799999999999998]\n        - [0.254, 0.050799999999999998]\n        global_frame: /odom\n        height: 5.0\n        inflation_radius: 0.10000000000000001\n        laser_scan_sensor: {clearing: true, data_type: LaserScan, marking: true, sensor_frame: laser,\n          topic: scan}\n        map_type: costmap\n        observation_sources: laser_scan_sensor\n        obstacle_range: 3.5\n        publish_frequency: 2.0\n        raytrace_range: 4.0\n        resolution: 0.050000000000000003\n        robot_base_frame: base_link\n        rolling_window: true\n        static_map: false\n        transform_tolerance: 0.20000000000000001\n        update_frequency: 5.0\n        width: 5.0\n    robot_description: \"<?xml version=\\\"1.0\\\" ?>\\n<!-- ===================================================================================\\\n      \\ -->\\n<!-- |    This document was autogenerated by xacro from /opt/ros/diamondback/stacks/usc-ros-pkg/p2os/p2os_urdf/defs/pioneer3dx.xacro\\\n      \\ | -->\\n<!-- |    EDITING THIS FILE BY HAND IS NOT RECOMMENDED                \\\n      \\                 | -->\\n<!-- ===================================================================================\\\n      \\ -->\\n<robot name=\\\"pioneer3dx\\\" xmlns:controller=\\\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\\\"\\\n      \\ xmlns:interface=\\\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\\\"\\\n      \\ xmlns:sensor=\\\"http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor\\\" xmlns:xacro=\\\"\\\n      http://ros.org/wiki/xacro\\\">\\n  <!-- Chassis -->\\n  <link name=\\\"base_link\\\">\\n\\\n      \\    <inertial>\\n      <mass value=\\\"3.5\\\"/>\\n      <!--<origin xyz=\\\"-0.025 0 -0.223\\\"\\\n      />-->\\n      <origin xyz=\\\"-0.05 0 0\\\"/>\\n      <inertia ixx=\\\"1\\\" ixy=\\\"0\\\" ixz=\\\"\\\n      0\\\" iyy=\\\"1\\\" iyz=\\\"0\\\" izz=\\\"1\\\"/>\\n    </inertial>\\n    <visual name=\\\"base_visual\\\"\\\n      >\\n      <origin rpy=\\\"0 0 0\\\" xyz=\\\"-0.045 0 0.148\\\"/>\\n      <geometry name=\\\"\\\n      pioneer_geom\\\">\\n        <mesh filename=\\\"package://p2os_urdf/meshes/p3dx_meshes/chassis.stl\\\"\\\n      />\\n      </geometry>\\n      <material name=\\\"ChassisRed\\\">\\n        <color rgba=\\\"\\\n      0.851 0.0 0.0 1.0\\\"/>\\n      </material>\\n    </visual>\\n    <collision>\\n     \\\n      \\ <origin rpy=\\\"0 0 0\\\" xyz=\\\"-0.045 0 0.145\\\"/>\\n      <geometry>\\n        <box\\\n      \\ size=\\\"0.35 0.25 0.14\\\"/>\\n      </geometry>\\n    </collision>\\n  </link>\\n  <gazebo\\\n      \\ reference=\\\"base_link\\\">\\n    <material value=\\\"Gazebo/Red\\\"/>\\n  </gazebo>\\n\\\n      \\  <!-- Top ..."], "url": "https://answers.ros.org/question/10621/navigation-stack-with-errors-on-launch/"},
{"title": "rosbag and real robots?", "time": "2011-09-18 11:49:32 -0600", "post_content": [" ", " ", "So I was using this tutorial to map out the floor in at my school.  The map is beautiful.  ", "However, once I am done with the bag file I want to use the map for the real robot.  I have the robot end, where I plan on starting the robot in the future.  Then I start the Segwary RMP driver and with the laser scan.  First problem occurs gmapping (Note: It HAS NOT been  restarted) will begin mapping in a completely different place.  ", "So I hacked the driver and offset the coordinates by the ending coordinates in /map tf frame.  Gmapping appears to get it correct at this point.  But when I try to move 1 meter forward it will turn toward the wall and essentially run into it.  However, I get the coordinates from goal generated by rviz.  The other weird part is I have to subtract the map coordinates from the goal.  (It has to be in terms of base link.)  Not a huge issue but a weird one.  ", "I am using the move_base find path service.  ", "I have spent a better part of a month trying to get this to work.  At first I tried to play with the ros time and sim time, but that didn't seem to help.  ", "I have also tried amcl but the localizer crashes when it gets even a meter off.  gmapping's localizer for whatever reason works really well.", "So question 1: Can I use a rosbag to create a map in gmapping AND use a real robot on it.\n(It worked great on the simulator)", "Question 2:) How do I get the robot driver to \"start\" at the same location the mapper left off.  Considering it was the same location that the mapper started and is the same location that the robot is currently in.", "If I can't do either of these two things then I am in trouble, thanks for the help in advance.", "Cheers.", "I have similar kind of problem in creating a 2D global map. I have not been able to generate the bag file. But my slam_gmapping node appears in rxgraph output. How you come to know that gmapping started or not? My question is here "], "answer": [" ", " ", " is a tutorial that might help you. The main thing that you're trying to do is to save the map generated by gmapping. The command to do that is: ", "Which will save the map to the current directory. You can then load this map as a static map into move_base, and that should help to resolve your problem. ", " ", " ", "AMCL needs to know where to initialize your particle cloud; that's what the \"Initial Pose\" button in rviz does for you.", " ", " ", "The most common way to do this is to build the map with gmapping and then save the map.  Then use ", " to localize the robot in the prebuilt map.  ", " ", " ", " ", " ", "I tried using the inital pose (I think its called 2D Pose Estimate)  Then I click and point the green arrow where I believe the robot is at.  However, nothing happens.", "\nOr rather the scan stays at the bottom right of the map with no promise of moving. \nIts weird because twice in the last two days I was able to get it to localize.  But when I restart my system or the robot it failed again putting it in the bottom right corner.", "\nsorry didn't mean to double post forgot this thread was open.\n", "\nhas a little more information on my current problem.", " ", " ", "Okay,", "I have tried this.  In fact it was the first method I used.\nWhen I run map_saver I get the yaml file and the image.  When I run the map_server I get the map.  ", "WHEN I RUN AMCL it will not localize at the starting point of my map, in fact it starts its own map elsewhere on the map in rviz.  AMCL is not working for me at all.  Gmapping's localizer does work. ", "I need to know if I can use move_base service \"make_plan\" with gmapping.  I currently manually transfer the current position and the destination position into the map coordinates.  I have to do this step because the /map coordinate may change at any time.  ", "It works great in stage when the map coordinates are not 0,0,0.  ", "I can build a beautiful map of the buildings 2nd floor, but I can't use it.  I have been stuck on this for two months.", "Please help.", "Thanks.", " ", " ", "I figured gmapping creates a dynamic map so I invoked move_base like this:", "</launch>", "Here is a copy of my launch file I am using.  The robot segway driver is running, and the laser is up and running.  Robot driver TFs are functioning as well.  Now the only question I have is can I invoke the \"move_base/make_plan\" service with gmapping as the source of my map?"], "answer_code": ["rosrun map_server map_saver\n", "<launch>\n<node name=\"gmapping\" pkg=\"gmapping\" type=\"slam_gmapping\" respawn=\"false\" args=\"scan:=/scan _delta:=.1\"/>\n<node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\" args=\"_controller_frequency:=60 /static_map:=/dynamic_map\"/>\n<node pkg=\"rviz\" type=\"rviz\" name=\"VISUALIZER\"/>\n"], "url": "https://answers.ros.org/question/11241/rosbag-and-real-robots/"},
{"title": "care-o-bot simulation extremely slow", "time": "2011-09-18 05:05:05 -0600", "post_content": [" ", " ", "I'm trying to simulate care-o-bot in gazebo running electric with ubuntu 10.4.", "Although not all of the cob_dashboard buttons seem to work, the ones that do are _extremely_ slow.  Simply extending the tray takes almost 2 minutes.  Is this to be expected?  Also, can anyone suggest why the other buttons (particularly, the base motions) don't seem to do anything?", "Thanks,\nPaul."], "answer": [" ", " ", " ", " ", "In electric, I am getting about ~0.35Xreal-time on my desktop (i7 quad), if you are getting 0.03xreal-time, something is wrong.  Can you try profiling it with your favorite profiler (e.g. valgrind) and post some results?", "The 0.35Xreal-time should be improvable too, but let start with the 0.03X problem?", "I tried turning off the kinect camera by commenting out following line in the urdf", "and I am getting about 1.4Xreal-time.  Taking a closer look, the current code was using gazebo_ros_block_laser plugin with 160X160 configuration.  A better way to implement this is to use the gazebo_ros_openni_kinect plugin, for example, update your kinect.gazebo.xacro file to look something like this:", "and you should see drastic improvements in speed.", "Aside from performance gains, fundamentally, a depth camera like Kinect is better modeled by a gazebo_ros_camera/gazebo_ros_openni_kinect than gazebo_ros_block_laser.  In gazebo_ros_block_laser, the scan angles are constant as you make a horizontal or vertical scan, but for kinect, you actually want equal size pixels for each ray trace, which means the angle between rays as you scan vertically or horizontally is not constant.", " ", " ", " ", " ", "We do have the same problem here on several machines. Even a powerful 4 core machine with nvidia graphics (ubuntu 10.4 64bit, electric) runs only at 0.03xrealtime. It was at least 10 times faster with diamondback. Interestingly enough the same speed is shown on a i5 laptop with intel graphics. So one could speculate that it is somehow controlled to that speed, not limited by the hardware.", "I talked to some people at IPA and they have seen this also, but only sporadicly. They can get rid of that by starting again. One person remembers to have seen this under diamondback also, but feels that occurecne increased under electric. For us it keeps beeing slow for every start.", "Any ideas how to attac this and narrow down the reason?", "For the base motion: you need to start cob_2dnav. Only then commanding base motion works. You can then move the base also from rviz by setting target goals.", " ", " ", "Does the 0.03Xreal-time issue happen with the plugin substitution suggested above?", "Also, can you try profiling a run using valgrind?  e.g. update your gazebo node to look like this:", "roslaunch cob simulation, let it run for a while.  In your ros log directory (usually ", ") there should be a file that looks like ", ", run kcachegrind on it to see some nice plots.", " ", " ", " ", " ", "Here some new (quite confusing) results (still on laptop, hope to do some desktop tests tomorow):", "After it looked like a good step forward yesterday, today I was back at 0.03 - without changing any configuration file. After quite a while of search I found out I can reproduce the following (after 3 tests each sequence, I don't think thats by accident):\nit makes a big difference if I start roscore and cob_bringup sim.launch (nothing else) ", ". With firefox first I get 0.03 otherwise almost 0.2. If firefox is already running gazebo also starts with 0.2 but then goes down to 0.03 in the first 2-3 seconds.", "Some oprofile results for both states (I tried to go more deeply into python and kernel but did'n manage for now):", "Note that firefox-bin does not eat up the time, but somehow influences and brings up the kernel percentage.", "To see python and kernel to use almost all the time is also something I didn't expect at all. Is this something cob specific? Is anybody of the cob people following this thread?", "strange!", "@PaulOnc: how is your realtime factor (shown at bottom line of gazebo)? Can you reproduce the influence of firefox?", "Result after some more experiments:"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "The difference is made on startup of gazebo GUI. Speed depends on weather firefox is running at that point in time. It keeps slow even after closing firefox! It keeps fast when firefox is opened after gazeboo start.", "For a headless gazebo there is no influence from firefox", "The effect is deterministic and reproducable every time.", " ", " ", " ", " "], "answer_code": ["<xacro:cob_kinect_gazebo_v0 name=\"${name}\" ros_topic=\"${ros_topic}\"/>\n", "<?xml version=\"1.0\"?>\n<root xmlns:sensor=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor\"\n      xmlns:controller=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\"\n      xmlns:interface=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\"\n      xmlns:xacro=\"http://ros.org/wiki/xacro\">\n\n<xacro:macro name=\"cob_kinect_gazebo_v0\" params=\"name ros_topic\">\n\n  <gazebo reference=\"${name}_frame\">\n\n    <sensor:camera name=\"${name}_sensor\">\n      <imageFormat>R8G8B8</imageFormat>\n      <imageSize>320 240</imageSize>\n      <hfov>57</hfov>\n      <nearClip>0.01</nearClip>\n      <farClip>5</farClip>\n      <updateRate>1.0</updateRate>\n      <baseline>0.2</baseline>\n      <controller:gazebo_ros_openni_kinect name=\"${name}_controller\" plugin=\"libgazebo_ros_openni_kinect.so\">\n        <alwaysOn>true</alwaysOn>\n        <updateRate>1.0</updateRate>\n        <imageTopicName>/${ros_topic}/image_raw</imageTopicName>\n        <pointCloudTopicName>/${ros_topic}/points</pointCloudTopicName>\n        <cameraInfoTopicName>/${ros_topic}/camera_info</cameraInfoTopicName>\n        <frameName>/${name}_frame</frameName>\n        <pointCloudCutoff>0.5</pointCloudCutoff>\n        <distortion_k1>0.00000001</distortion_k1>\n        <distortion_k2>0.00000001</distortion_k2>\n        <distortion_k3>0.00000001</distortion_k3>\n        <distortion_t1>0.00000001</distortion_t1>\n        <distortion_t2>0.00000001</distortion_t2>\n      </controller:gazebo_ros_openni_kinect>\n    </sensor:camera>\n\n    <material value=\"Gazebo/Red\" />\n    <turnGravityOff>true</turnGravityOff>\n  </gazebo>\n\n</xacro:macro>\n\n</root>\n", "   <node alaunch-prefix=\"valgrind --tool=callgrind --dump-instr=yes --simulate-cache=yes --collect-jumps=yes \" name=\"gazebo\" pkg=\"gazebo\" type=\"gazebo\" args=\"$(optenv SIMX) $(find cob_gazebo_worlds)/common/worlds/empty.world\" respawn=\"false\" output=\"screen\"/>\n", "~/.ros", "calgrind.out.[pid]"], "url": "https://answers.ros.org/question/11240/care-o-bot-simulation-extremely-slow/"},
{"title": "import org.ros.rosjava.android.MessageCallable error", "time": "2011-09-20 15:07:52 -0600", "post_content": [" ", " ", " ", " ", "Hi all:", "I downloaded source from ", " And I ", ". Then I imported this project into Eclipse. However, I got errors, which are ", "w;", "It seems that the project did not import ", "\nDoes anyone could help me? Thanks!!!!!!!!!"], "answer": [" ", " ", "It sounds like there is a Android library dependency. Check that the project properties has a green check mark next to the android_gingerbread library. You probably have not imported the necessary Android library projects into Eclipse.", " ", " ", " ", " ", "Hi all,", "I have the same problem, my adt plugin is up to date", "but I have some problem with message generation ", "I think it is a related problem because MessageCallable.java is not compiled:", " ", " ", "Have you followed the instructions to set up Eclipse for Android development? "], "answer_code": ["Android Development Tools   12.0.0.v201106281929-138431 com.android.ide.eclipse.adt.feature.group\n", "android_gingerbread/src/org/ros/android/MessageCallable.java\n"], "url": "https://answers.ros.org/question/11262/import-orgrosrosjavaandroidmessagecallable-error/"},
{"title": "TurtleBot USB Connection - No Data", "time": "2011-11-02 22:08:24 -0600", "post_content": [" ", " ", " ", " ", "Hi!", "I'm trying to get a Roomba 555 with Turtlebot power board to work, but keep getting the following message:", "\"Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create.\"", "I connected the Turtlebot to a Dell Inspiron 6400, running Ubuntu 10.04 and ROS diamondback.\nMy serial device looks like this: \ncrw-rw-rw- 1 root mate 188, 0 2011-11-03 10:48 /dev/ttyUSB0", "As listed in lsusb:\nBus 004 Device 006: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC", "I've tried the traditional fixes, such as restarting, unplugging-replugging, looked at dmesg, but no clues.", "Any ideas how I could fix this would be very much appreciated.\nThanks in advance,\nM\u00e1t\u00e9"], "answer": [" ", " ", "Check that the iRobot Create is turned on. ( The green light should be on next to the power button.)", "Hi ROS community !\nI have the same problem here, on an eu turtlebot 1 (based on roomba 521).\nI think is correctly turned on and i can see the ttyUSB0 file in /dev.\nI think is cause of the board, it's an old turtlebot.eu card on which I can't find any information on the internet.", "Thx !"], "url": "https://answers.ros.org/question/11783/turtlebot-usb-connection-no-data/"},
{"title": "Does the roomba 564 has a gyro?", "time": "2011-11-04 03:18:52 -0600", "post_content": [" ", " ", "Hi,\nI'm running the turtlebot, but not ont the iRobot, I'm useing a roomba 564. The thing is that when I try to calibrate it says all the time:\nStill waiting for imu\nObviously there's nothing beeing published on the topics /imu/data or /imu/raw. I thinks this topics are related to the gyro. Is that correct?\nI believe, as a conclusion of what I've exposed, that the roomba that I'm useing doesn't have a gyro. Can somebody confirm that? If that's true, how would you solve that?", "Thanks."], "answer": [" ", " ", "The gyro is part of the Turtlebot power board, not the roomba/create itself. You can find more details about the power board at ", "."], "url": "https://answers.ros.org/question/11799/does-the-roomba-564-has-a-gyro/"},
{"title": "remote turtlebot powerup?", "time": "2011-09-29 05:21:44 -0600", "post_content": [" ", " ", "I am using turtlebot from a remote site and would like to powerup the create without asking someone local to the machine to push the button.  I know that there is a powerup pin on the 25 pin connector, but driving that requires adding an arduino or other sort of I/O board.\nAnyone have ideas on how to remotely (say, via ssh) turn on/off the create without adding another board?"], "answer": [" ", " ", "The create is designed to be able to auto power on given a serial pulse on RTS but this only works if the receiving serial side is powered. In the turtlebot_driver code you'll see a wake command that does this but it only works when the create is plugged/receiving external power. If you have the TurtleBot auto-docking you might be able to get it to work. But other than that you must pulse the power on pin using an external device. ", " ", " ", "You should be able to set it into full mode from the Open interface over the serial port. (see also page six of the open interface detail pdf). \nDo you have the turtlebot dashboard also running remotely? "], "url": "https://answers.ros.org/question/11369/remote-turtlebot-powerup/"},
{"title": "Status on arm navigation tutorials", "time": "2011-10-20 03:52:31 -0600", "post_content": [" ", " ", "I'm a relative newbie to ROS, and I'm struggling with learning how to use the arm navigation stack in electric. I was wondering if there are any plans to update the arm navigation tutorials... ", "I realize that the arm navigation API has not changed between diamondback and electric, and that the major changes were adding the Planning Scene and moving some packages around, so theoretically it should be relatively easy to find the files that have been moved. ", "However, the instructions on how to run a PR2 in simulation seem to have changed. Some of the launch files referenced in the tutorial seem to have been completely removed, as I cannot find them anywhere in the arm navigation stack. Also, I've noticed a couple of cases where new launch files need to be added to run the same tutorial. Some of these have already been asked here, like the ", ", but there a few more cases. For example, ", " sits there waiting for a service that is supposed to be launched by a launch file that no longer exists in electric.", "I'm also having trouble visualizing the robot as described in the tutorials, even after I've identified some of the problems. The errors I'm getting involve certain nodes and services not starting correctly, which implies that I'm still missing some launch files.", "Also, this is my first experience with the arm navigation stack, so it would be very helpful if there was a more detailed example of how to use the Planning Scene Architecture. The ", " Gil posted here yesterday afternoon and the high-level overview of the architecture are very helpful. However, they both hint at a lot more functionality, and it would be very helpful if the tutorials included a more detailed example of how to actually use all of the nifty things that the overview alludes to."], "answer": [" ", " ", " ", " ", "Hi,", "I'm definitely aware that things are out-of-date on the tutorial side, and I'm gradually updating everything.  For instance, I wrote a new tutorial for validity checking that I think you'll find interesting, as it shows the power of the planning scene for letting you use the C++ classes within your own code.  You can find the new tutorial ", ".", "In order for you to actually run this stuff through debs I'll need to release pr2_arm_navigation, which is going to take a little while longer as I want to make sure that everything is ready to go before I do it.  Also, working on tutorials more than a couple hours a day makes me kind of crazy so it's going to take a little while.  I'll aim to release electric versions of all existing tutorials as well as some new tutorials demonstrating features that weren't possible in electric by the end of next week.", "What's the current status of the arm_navigation tutorial update? A couple of times I was also confused by the tutorials and provided information on the wiki too. My biggest problem was to find out which information is actually up-to-date. Here and there, I found wrong or missing information.", "E.g. the launch file \"planning_scene_warehouse_viewer_only.launch\" in the move_arm_warehouse package seems to be missing (Warehouse Viewer tutorial). I do understand that creating and maintaining the wiki is quite some work. Since I am currently working my way into the arm_navigation stack, ...", "I might could help with that job - maybe others, too, who are working with the stack, However, I would like to avoid spending time on improving wiki pages, when they are replaced or updated soon anyway."], "url": "https://answers.ros.org/question/11612/status-on-arm-navigation-tutorials/"},
{"title": "PR2 planning_component_visualizer", "time": "2011-10-10 02:06:40 -0600", "post_content": [" ", " ", " ", " ", "I am trying to run the planning component visualizer on a PR2. I couldn't find a PR2 specific tutorial, so I tried to follow the guidelines on ", ". I have a launch file that reads:", "However the node planning_components_visualizer hangs with the message:", "waitForService: Service\n  [/pr2_left_arm_cartesian_kinematics/get_constraint_aware_ik]\n  has not been advertised, waiting...", "I couldn't find a launch file for this service. Could anybody tell me how to launch the component visualizer on the PR2?"], "answer": [" ", " ", " ", " ", "Lorenzo,", "The planning_components_visualizer is meant to be run in concert with the Arm Navigation Wizard, which will auto-generate all the necessary launch files - they get pretty complicated and will be painful to generate by hand.  If you want to use it with the PR2 I'd recommend running this tutorial - ", " - it will tell you how to run it will a copy of the PR2 urdf, and you'll get an auto-generated application that will contain a planning_components_visualizer, as well as an rviz config.  You can then also, with a bit of additional work, use the warehouse viewer, which is substantially more general and powerful than the planning_components_visualizer.   ", "Note that by running the tutorial your application will contain a KDL-based numeric IK solver which is not as fast as the PR2 custom solver.  You can replace the solver by changing the kinematics solver in your autogenerated constraint_aware_kinematics.launch file from \"arm_kinematics_constraint_aware/KDLArmKinematicsPlugin\" to \"pr2_arm_kinematics/PR2ArmKinematicsPlugin\" ."], "question_code": ["<launch>\n    <include file=\"$(find planning_environment)/launch/planning_environment_visualization_prerequisites.launch\" />\n    <node pkg=\"rviz\" type=\"rviz\" name=\"rviz_planning_components\" args=\"-d $(find a-custom-location)/config/planning_components_visualizer.vcg\" />\n    <node pkg=\"move_arm\" type=\"planning_components_visualizer\" name=\"planning_components_visualizer\" output=\"screen\" />\n</launch>\n"], "url": "https://answers.ros.org/question/11483/pr2-planning_component_visualizer/"},
{"title": "Best PC architecture to run Turtlebot", "time": "2011-09-21 18:12:36 -0600", "post_content": [" ", " ", " ", " ", "Hi there,", "I was wondering if it's worth investing in a high end PC to run Turtlebot, rather than using the advised laptop. ", "I did not find much information on this topic, but if this type of question has already been answered I apologize in advance.\nJonathan", "Did you ever build a souped-up turtlebot? I just installed a Mini-ITX mobo, low power i5 processor, and ssd in the cargo bay. Maybe I'll post some benchmarks", " How does your solution work? How do you power the mini ITX board? Maybe you can post some more details on your project."], "answer": [" ", " ", "The choice of computer to run on the TurtleBot is a tradeoff of battery life for computation and price.  In general more computation is better.  However also in general longer battery life is better.  ", "The choice of the dual core Atom processor in the Asus 1215N for the default processor provides enough processing to do all of the navigation stack and process the Kinect while leaving most of one core available for the user to develop their own functionality.  ", "If you go up to an i7 or other more powerful CPU you can do more processing, but you need to be careful about what you're expected battery life is.  ", "The same goes for the RAM though it's less power hungry.  ", "It is definitely possibly to use the graphics card for certain applications.  Right now the graphics card is most heavily utilized by visualization and simulation libraries which are not especially useful on the TurtleBot laptop.  However there are CUDA bindings in the works for both PCL and OpenCV which could offload a lot of computatoin to the GPU soon.  In Lucid the EeePC's Optimus technology is not supported, but I have heard of drivers working on Natty.  ", "Using the EeePC is also a price optimization.  The EeePC is in a highly competitive product space with many similar models from multiple manufacturers making computers with almost exactly the same specifications.  ", " ", " ", "Thanks a lot for this answer. On the battery life problem, maybe something like this battery pack would help ?\n"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Would it ", " give a boost to run a big CPU (i.e. i72600) or is it throwing money out of the window ?", "Is it possible to use the graphics card processing capacity for some applications ? ", "What is the RAM impact as well ?", "What is the data bitrate output of the Kinect ? "], "url": "https://answers.ros.org/question/11276/best-pc-architecture-to-run-turtlebot/"},
{"title": "Does the turtlebot power/gyro board need to enable the kinect?", "time": "2011-10-10 08:48:13 -0600", "post_content": [" ", " ", " ", " ", "Do I need to 'turn on' the turtlebot power/gyro board somehow to power up my kinect? I see it get power momentarily as I turn on my iCreate but it powers off immediatly after.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "answer": [" ", " ", "Yes, you do. The Create needs to be put in \"full\" mode to enable the breakers. This can be done by using the Mode button on the ", ", or by sending it motion commands (using one of the ", " or your own code).", "If it's still not working when it's in full mode, use the dashboard to check that breaker 0 is set. Breaker 0 should automatically be set, though.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "url": "https://answers.ros.org/question/11487/does-the-turtlebot-powergyro-board-need-to-enable-the-kinect/"},
{"title": "Status \"Stale\" in Turtlebot's dashboard", "time": "2011-11-22 03:56:57 -0600", "post_content": [" ", " ", " ", " ", "I just want a clearer definition of the status ", " on ", "'s ", " (it's not really available in ", "). Does it mean that (a) the battery of ", " is left unused for long so that it has mostly discharged? Or (b) the battery is dying or (c) else?", "Btw, the reason I want to know this is because I often see ", " on my ", ". Sometimes it even happens right after I plug out. In this case I suspect the case (b) applies, but then after some moment ", " changes status to show the battery of ", " is almost fully charged. So this case might be rooted in multiple causes, but first I like to know about ", "."], "answer": [" ", " ", "Stale simply means that the status has not be updated because messages are not coming through -- therefore it carries no information about the actual battery state and instead says there is either some issue with A) your networking or B) the turtlebot_node itself. "], "question_code": ["Stale", "Turtlebot", "dashboard", "iRobot Create", "Stale", "Turtlebot", "dashboard", "Create", "Stale"], "url": "https://answers.ros.org/question/12048/status-stale-in-turtlebots-dashboard/"},
{"title": "Navigation stack on youBot", "time": "2011-09-07 01:54:20 -0600", "post_content": [" ", " ", " ", " ", "Hi everybody,\nI am developing my final thesis using a KUKA youBot.\nI am managing to run the navigation stack on it, after many attempts, everything seems to work (no warning, no errors),\nbut when I give through rviz a navigation goal this is what happens", "The green path is the full plan for the robot (topic: NavfnROS/plan), while the red path is the portion of the global plan that the local planner is currently pursuing (TrajectoryPlannerROS/global_plan). There should be also TrajectoryPlannerROS/local_plan in black, but it is not visible.", "Here are the configuration files I am using:", "costmap_common_params.yaml", "global_costmap_params.yaml", "local_costmap_params.yaml", "base_local_planner_params.yaml", "The navigation is blind because in the task involved will not be dynamic obstacles, the only ones are the\nwork space walls.", "Am I missing something?\nWhat possibly is going wrong?", "Thank you so much for the help", "Lorenzo"], "answer": [" ", " ", " ", " ", "There are a couple of problems that I see in your parameter set:", "1) If you really do intend to avoid obstacles based only on localization and a static map, you'll need to make sure that the local costmap is set up to run with that information. Otherwise, the local planner won't know about those static obstacles. If the map is small, you'll probably just be able to have the local costmap have the same parameters as the global costmap. However, if the map is going to be large, you'll probably want to feed the local costmap tiles of the global map to make sure computing the cost function for the map doesn't get too expensive.", "2) The velocity limits you've set are extremely limiting. It'll be hard for the robot to follow any global path when its rotational velocity is limited to 0.07 radians/second and its translational velocity is limited to 0.07 meters/second. To make those limits work, I'd expect you'll have to play around a lot with the ", ". Personally, I've never run navigation with such tight constraints.", " ", " ", "Thank you so much for the help.\nI changed che configuration files and what I am obtaining now is:", "the robot does not follow the path planned and actually it penetrates the wall, as it was not an obstacle", "My new configuration files are", " ", " ", "The page you linked does not exist anymore. If I am not getting wrong is the one\nexplaining to test odometry during rotation and translation setting laserscan decadence to a high value, right?\nI have done it, translation is ok, rotation shows an error of almnost 10 degree, but I thought nothing can be done about it.", " ", " ", "I've tried to change settings, and configuration files, but nothing improved...\nAny suggestions?", "Thank you so much, I am quite stuck and I need help", " ", " ", "Sorry, the syntax for comments doesn't allow me to post correctly.", "Have you looked at the  ", "? Specifically, section 2.", " ", " ", "A couple of things:", "1) I tried to run in simulation with similar parameters, I'll post them at the bottom of this post, and things worked for me.", "2) Its not clear from the video you posted whether or not the local costmap's obstacles are being updated properly. This ", " shows how to display navigation-relevant information in rviz. Specifically, I'd really like to know whether ", " shows anything.", "Here are the params I used in stage:", "Hope this helps.", " ", " ", "Thank you so much!\nWith your help the new situation is:", "Now everything works, but still it seems there is a problem with amcl, since it constantly changes (incorrectly) the transformation between /map and /odom.", "The green lines are the inflated obstacles, so they are correct, I do not display obstacles because they are too heavy for rviz, but they works.", "I tried to change amcl.launch in order to reduce the corrections, but the problem remains...", "Here are the configuration files", "and this is the amcl launch file:"], "answer_code": ["costmap_common_params.yaml\n\nobstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[0.290,0.190], [0.290,-0.190], [-0.290,-0.190], [-0.290,0.190]]\n#robot_radius: ir_of_robot\n#inflation_radius: 0.55\n\nobservation_sources: laser_scan_sensor\n\nlaser_scan_sensor: {sensor_frame: base_laser, data_type: LaserScan, topic: scan\\\n, marking: false, clearing: false}\n\n#point_cloud_sensor: {sensor_frame: openni_rgb_optical_frame, data_type: PointC\\\nloud2, topic: camera/rgb/points, marking: false, clearing: false}\n\nlocal_costmap_params.yaml\nocal_costmap:\n  global_frame: /map\n  robot_base_frame: /base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: true\n  transform_tolerance: 0.8\n\nglobal_costmap_params.yaml\n\nglobal_costmap:\n  global_frame: /map\n  robot_base_frame: /base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: true\n  transform_tolerance: 0.8\n\nbase_local_planner_params.yaml\n\nrajectoryPlannerROS:\n  max_vel_x: 0.2\n  min_vel_x: 0.05\n  max_rotational_vel: 0.3\n  min_in_place_rotational_vel: 0.05\n\n  acc_lim_th: 4.0\n  acc_lim_x: 3.7\n  acc_lim_y: 3.5\n\n  yaw_goal_tolerance: 0.2\n  xy_goal_tolerance: 0.1\n\n  holonomic_robot: true\n  y_vels: [-0.2, -0.05, 0.05, 0.2]\n\n  dwa: false (using trajectory rollout or dwa leads to the same results)\n", "move_base/local_costmap/obstacles", "costmap_common_params.yaml\n\nobservation_sources: laser_scan_sensor\n\nlaser_scan_sensor: {sensor_frame: base_laser, data_type: LaserScan, topic: scan, marking: false, clearing: false}\n\n#Set the tolerance we're willing to have for tf transforms\ntransform_tolerance: 0.8\n\n#Obstacle marking parameters\nobstacle_range: 2.5\nmax_obstacle_height: 2.0\nraytrace_range: 3.0\n\n#The footprint of the robot and associated padding\nfootprint: [[0.290,0.190], [0.290,-0.190], [-0.290,-0.190], [-0.290,0.190]]\nfootprint_padding: 0.05\n\n#Cost function parameters\ninflation_radius: 0.1\ncost_scaling_factor: 10.0\n\n#The cost at which a cell is considered an obstacle when a map is read from the map_server\nlethal_cost_threshold: 100\n\nlocal_costmap:\n  #Set the global and robot frames for the costmap\n  global_frame: map\n  robot_base_frame: base_link\n\n  #Set the update and publish frequency of the costmap\n  update_frequency: 5.0\n  publish_frequency: 2.0\n\n  static_map: true\n  rolling_window: false\n\n\nglobal_costmap:\n  #Set the global and robot frames for the costmap\n  global_frame: /map\n  robot_base_frame: base_link\n\n  #Set the update and publish frequency of the costmap\n  update_frequency: 5.0\n  publish_frequency: 0.0\n\n  #We'll use a map served by the map_server to initialize this costmap\n  static_map: true\n  rolling_window: false\n\nTrajectoryPlannerROS:\n  #Set the acceleration limits of the robot\n  acc_lim_th: 4.0\n  acc_lim_x: 3.7\n  acc_lim_y: 3.5\n\n  #Set the velocity limits of the robot\n  max_vel_x: 0.2\n  min_vel_x: 0.05\n  max_rotational_vel: 0.3\n  min_in_place_rotational_vel: 0.05\n\n  #The velocity the robot will command when trying to escape from a stuck situation\n  escape_vel: -0.1\n\n  #For this example, we'll use a holonomic robot\n  holonomic_robot: true\n\n  #Since we're using a holonomic robot, we'll set the set of y velocities it will sample\n  y_vels: [-0.3, -0.1, 0.1, 0.3]\n\n  #Set the tolerance on achieving a goal\n  xy_goal_tolerance: 0.1\n  yaw_goal_tolerance: 0.05\n\n  #We'll configure how long and with what granularity we'll forward simulate trajectories\n  sim_time: 1.7\n  sim_granularity: 0.025\n  vx_samples: 3\n  vtheta_samples: 20\n      #Parameters for scoring trajectories \n      goal_distance_bias: 1.5\n      path_distance_bias: 0.6\n      occdist_scale: 0.01\n      heading_lookahead: 0.325\n\n      #We'll use the Dynamic Window Approach to control instead of Trajectory Rollout for this example\n      dwa: true\n\n      #How far the robot must travel before oscillation flags are reset\n      oscillation_reset_dist: 0.05\n\n      #Eat up the plan as the robot moves along it\n      prune_plan: true\n", "<launch>\n<node pkg=\"amcl\" type=\"amcl\" name=\"amcl\">\n  <!-- Publish scans from best pose at a max of 10 Hz -->\n  <param name=\"odom_model_type\" value=\"omni\"/>\n  <param name=\"odom_alpha5\" value=\"0.001\"/>\n  <param name=\"transform_tolerance\" value=\"0.2\" />\n  <param name=\"gui_publish_rate ..."], "url": "https://answers.ros.org/question/11138/navigation-stack-on-youbot/"},
{"title": "Ardu IMU+ V3 drivers", "time": "2011-12-11 05:32:16 -0600", "post_content": [" ", " ", " ", " ", "Hi community,\nHas anybody tried to use an Ardu-IMU with ros ?\nThe new version Ardu-IMU V3 seems to be very interesting and powerful...\nIts xmas-time and I like to buy one for my robot :-)", "Cheers\nChristian"], "answer": [" ", " ", "As the ArduIMU is using an atmega328 and arduino IDE for development, it would probably be pretty easy to modify the provided source code to use rosserial_arduino and publish the data over an FTDI cable to a ROS powered computer.", "For those that stumble upon this page, rosserial cannot publish a full IMU message due to the hardware buffer length so your best bet is to publish data in a flat array and then republish it in the sensor_msgs/IMU message. :-)"], "url": "https://answers.ros.org/question/12306/ardu-imu-v3-drivers/"},
{"title": "Turtlebot arm servos not being recognized [closed]", "time": "2011-11-18 07:08:37 -0600", "post_content": [" ", " ", " ", " ", "I am using a turtlebot arm and just finished getting the ROS sketch uploaded to the ArbotiX board when I ran into an error.  I run the command to search for the servos after having plugged only one into the board and get the following result:", "I have tried multiple cables and multiple servos to see if either was the problem.  Neither of them turned out to be the problem.  I have established that the servos do at least move, since I was able to issue them commands using PyPose.  The only concern that I have is the the red light on the servos first light up when I plug them in, but do not stay lit for more than a second.  I am currently running the arbotix board directly off of a iRobot Create battery (as opposed to going through the power board, since the only port on my power board is being occupied by the create).  I don't think that should cause a problem, but figured I would mention it.  Does anyone have an idea why my computer cannot read the servos, or any recommendations for other troubleshooting steps to take?  Thank you very much!"], "answer": [" ", " ", "What firmware do you have installed on the ArbotiX? It has to be running the ros.pde sketch (the PyPose sketch won't work with that terminal).", "The red lights are normal (they flash for a second when powered on).", "Running directly off the 14.4V NiMH in the Create could seriously damage the servos, especially if subjected to heavy loads. "], "question_code": ["turtlebot@turtlebot-laptop:~$ rosrun arbotix_python terminal.py /dev/ttyUSB0\nArbotiX Terminal --- Version 0.1\nCopyright 2011 Vanadium Labs LLC\n>>  ls\n.... .... .... .... .... .... .... .... .... \n.... .... .... .... .... .... .... .... .... \n>>\n"], "url": "https://answers.ros.org/question/11994/turtlebot-arm-servos-not-being-recognized/"},
{"title": "Kinect does not turn on / turns off on its own", "time": "2011-12-07 13:35:18 -0600", "post_content": [" ", " ", " ", " ", "I am trying to follow the tutorials to understand turtlebot. But it seems like every time the kinect is involved, nothing seems to work.", "I keep getting", "When I restart everything (restart laptop, redo ssh, launch), sometimes I get lucky and hear turtlebot beep and the kinect turns on (red light can be seen from the kinect laser), but this is what I get:", "and then I hear turtlebot beep and the kinect goes off again. I thought it was a battery problem, but the dashboard shows the battery is full.", "anyone has any idea what else I can do to even complete the tutorials (I managed to reach teleop with no issues)?"], "answer": [" ", " ", "Based on ominous sound and jumping battery indicator you have a dead battery.  Make sure the Create is plugged in with the led light throbbing red/orange for at least an hour and try again.  Sometimes the Create does not charge when plugged in.  You have to look for the throbbing red/orange light.  If you don't see that, unplug it, press the power button until you  are confident it's off, then plug it in.  Note: sometimes it appears that the indicator LED turns off even though the robot is still \"on\" try to turn it on and then immediately off to be confident it's off.  Then it should charge when plugged in.   ", " ", " ", " ", " ", "Though I don't know that launch file or binary you're running, make sure ", " is run first time after booting ", ". That takes care of breaker and stuff.", "'s sound that you mention in the comment makes me skeptic that the battery is fully/almost discharged. The way to charge ", " was a little tricky to me too. I recommend you to review ", " again step-by-step."], "question_code": ["[ INFO] [1323314495.901083262]: [/openni_camera] No devices connected.... waiting for devices to be connected\n", "[ INFO] [1323314659.192741563]: [/openni_camera] Number devices connected: 1\n[ INFO] [1323314659.193218090]: [/openni_camera] 1. device on bus 001:11 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00362907481053A'\n[ WARN] [1323314659.197118306]: [/openni_camera] device_id is not set! Using first device.\n[ INFO] [1323314659.255754180]: [/openni_camera] Opened 'Xbox NUI Camera' on bus 1:11 with serial number 'A00362907481053A'\n[ INFO] [1323314659.300121550]: rgb_frame_id = 'kinect_rgb_optical_frame' \n[ INFO] [1323314659.306528300]: depth_frame_id = 'kinect_depth_optical_frame' \n[ WARN] [1323314660.231673257]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.232153416]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.232556400]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.232883467]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.233230857]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.233678749]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.234094444]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.234581657]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.235145905]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.235744794]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.236493772]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.236933702]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ERROR] [1323314660.237591048]: Cannot load nodelet /pointcloud_throttle for one exists with that name already\n[ WARN] [1323314660.238391290]: Publisher on '/openni_manager/bond' destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.238664998]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n[ERROR] [1323314660.239578662]: Cannot load nodelet /kinect_laser_narrow for one exists with that name already\n[ WARN] [1323314660.240480382]: Publisher on '/openni_manager/bond' destroyed immediately after creation.  Did you forget to store the handle?\n[ WARN] [1323314660.240773506]: WallTimer destroyed immediately after creation.  Did you forget to store the handle?\n"], "answer_code": ["turtlebot_bringup kinect.launch", "turtlebot", "iRobot Create", "Create"], "url": "https://answers.ros.org/question/12263/kinect-does-not-turn-on-turns-off-on-its-own/"},
{"title": "Splitting work between ROS and a custom motor controller board", "time": "2011-12-26 08:18:16 -0600", "post_content": [" ", " ", "I'm designing a hobby bartending robot arm around ROS, and I'm wondering what's the best way to implement motor control.", "Some background: I'm using brushed DC motors with homemade encoders controlled by various Atmega microcontrollers + H-bridges. Right now, I'm planning on having them communicate via RS-485, with an Atmega32u4 usb bridge to the computer running ROS.", "I looked at how the PR2 motor controller boards work, and the only onboard control they do is a 100kHz control loop around the motor current. All other control is done in a realtime Linux loop and communicated using EtherCAT (", "). I'm tempted to copy what they've done, but I'm worried that my lower-cost approach means more onboard control (ie, position/velocity) will work better.", "I'm relatively new to this, so any advice on how to implement it, or even recommendations on a better way to do the multi-drop communications would be greatly appreciated. Thanks!"], "answer": [" ", " ", "The main motivation behind running all of the PID control loops for the PR2 on the main computer is to reduce the number of languages and environments that a user has to know in order to write software for the PR2.", "If you're building your own motor control boards and writing the software for them yourself, putting the postion control on the motor board is probably a better idea. You'll have less data on your bus, because you won't need real-time feedback from all of your encoders, and you won't need to run a realtime kernel on your main computer (the real-time kernel on the PR2 is one of its weak points).", "If you're looking for a low-cost system as a model, take a look at the ", " board and drivers. It uses an atmel microcontroller as a bridge between the main computer and an AX servo bus, and the AX servos have a built-in microcontroller that does all of the motor and position control.", " ", " ", " ", " ", "Umm ... well, I am not sure, your description is sort of vague. ", "I assume you have some arms for your bartender, so I would suggest using simple servos, either the cheap ones that take PWM or the more more expensive, smart ones like AX-12. No use re-inventing the servo if you don't have to. One Arduino can drive many simple servos pretty effectively. I have not tried it, but there is a serial bridge for Arduino's in the ROS stacks.", "You mention DC motors, I assume are for movement ... how many? Is it differential drive or omni? Being in a confined space like behind a bar, I would suggest omni (3 or 4 wheels depending). The navigation stack can handle either. Remember more motors means more power required.", "I doubt you really need a real-time linux to do this ... a standard linux distro should do the job depending on your programming skills and linux knowledge.", "What is your bartender robot going to do exactly? Does it mix drinks, grab a user requested bottle of beer, or does it only dispense one thing? Will cameras or a kinect be involved?", "I would strongly suggest starting very simple. ROS will take a while to learn and it is constantly changing. If you tackle too much you will never get done.\nGood luck!"], "url": "https://answers.ros.org/question/12464/splitting-work-between-ros-and-a-custom-motor-controller-board/"},
{"title": "Turtlebot SLAM map building problems", "time": "2011-12-23 06:28:53 -0600", "post_content": [" ", " ", " ", " ", "I have some problems building a map with my Turtlebot. I run the calibration (0.85 for the gyro and 0.93 for odom) multiple times and followed the instructions in the tutorials for starting gmapping. The map looks like a set of scans scattered around, also the orientation shown in rviz is not consistent with where the robot is going. ", "To me it seems like there is either a problem with the odometry/gyro or the localization is not working properly. The gyro seems to work (I see the values changing when rotating), do I need to change anything in the config files (except the odo/gyro cal values)? I should also mention that I am using all the turtlebot hardware except the Kinect mounting hardware. I made my own mounting pole but the kinect is located approximately at the same place as with the standard turtlebot. ", "As recommended I did the tests in ", " and it seems like there is something wrong with the odometry, but only while turning, moving straight seems to be fine. The gyro seems to output good values (see plots) and the calibration was done multiple times. Have a look at the following link for a few screenshots and some more details.", "I did a 180 degree turn and the /odom orientation (yaw) measured 176.8\ndegrees, however the /robot_pose_ekf/odom orientation (yaw) measured only 60.4 degrees. So\nthat leads me to believe it is either the gyro or something wrong with\nthe EKF.", "Also, following is the calibration output (removed a few repeated lines to make the post shorter):", "[INFO] [WallTime: 1324939496.895343]  ... imu drift is -0.621943 degrees per second", "[INFO] [WallTime: 1324939497.197380] Still waiting for imu", "[INFO] [WallTime: 1324939497.499277] Still waiting for scan", "[INFO] [WallTime: 1324939497.801215] Aligning base with wall", "[INFO] [WallTime: 1324939498.103249] Still waiting for imu", "[INFO] [WallTime: 1324939498.405232] Still waiting for scan", "[INFO] [WallTime: 1324939545.778987] Odom error: 2.295941 percent", "[INFO] [WallTime: 1324939545.780867] Imu error: -32.801991 percent", "[INFO] [WallTime: 1324939546.090230] Still waiting for imu", "[INFO] [WallTime: 1324939546.392098] Still waiting for scan", "[INFO] [WallTime: 1324939546.996119] Aligning base with wall", "[INFO] [WallTime: 1324939548.270806] Still waiting for imu", "[INFO] [WallTime: 1324939548.573036] Still waiting for scan", "[INFO] [WallTime: 1324939560.870167] Odom error: -11.098675 percent", "[INFO] [WallTime: 1324939560.872639] Imu error: 19.686330 percent", "[INFO] [WallTime: 1324939561.178241] Still waiting for imu", "[INFO] [WallTime: 1324939561.480022] Still waiting for scan", "[INFO] [WallTime: 1324939561.782138] Aligning base with wall", "[INFO] [WallTime: 1324939565.640311] Still waiting for imu", "[INFO] [WallTime: 1324939565.942336] Still waiting for scan", "[INFO] [WallTime: 1324939573.806552] Odom error: 6.987265 percent", "[INFO] [WallTime: 1324939573.808156] Imu error: 35.610160 percent", "[INFO] [WallTime: 1324939574.111180] Still waiting for imu", "[INFO] [WallTime: 1324939574.413042] Still waiting for scan", "[INFO] [WallTime: 1324939574.715408] Aligning base with wall", "[INFO] [WallTime: 1324939577.046587] Still ..."], "answer": [" ", " ", " ", " ", "I think I figured this out! My sensor/power board has the ADXRS620 gyro and not the ADXRS613, the big difference is that ADXRS613 has a +/- 150deg/s output and the ADXRS620 has a +/- 300 deg/s output. This is important because for the same angular speed the raw outputs will differ by a factor of two.", "Also looking at turtlebot_node/src/turtlebot_node/gyro.py the raw signal from the analog input is multiplied by 150 to convert to deg/sec, when I changed it to 300 it worked fine with the values recommended by the calibration.", "One problem with the higher rate sensor is that the resolution is now halved. As a result I noticed that the drift is very high even with the robot stationary. I added a short averaging filter on the analog input in gyro.py and it helped a lot but I need to experiment with this a bit more.", "To fix this the gyro type could be defined as a parameter of turtlebot_node. I would be happy to fix this myself if I can get access to commit code.", " ", " ", "It seems like I am getting much better odometry when setting turtlebot_node/gyro_scale_correction to 2.55 instead of 0.876864 that is recommended by the odometry calibration.", "Not sure if this is normal!"], "url": "https://answers.ros.org/question/12446/turtlebot-slam-map-building-problems/"},
{"title": "(Turtlebot) Easier way to check power button status of iRobot Create", "time": "2011-12-15 01:42:46 -0600", "post_content": [" ", " ", " ", " ", "What would be easier ways to check the power on/off on ", "?", "If I understand correctly the usage, we often need to check if the power light of ", " is on/off on ", ". For ex. power charging doesn't start while the power on unless in ", ". Also, ", " can't operate it when it's off, which sometimes happen to me even though the light on powering cables for ", " is on. Correct me if I'm doing something wrong here, which could be the best answer to this Q as well.", "Suppose I'm correct, it's a little tedious to crouch down and look into the thin space b/w ", " and ", "'s lowest plate to see ", "s panel every time. ", " is very nice tool, but I think it doesn't provide enough feature yet when it comes to checking if power is on/off."], "answer": [" ", " ", " ", " ", "I have also found that the LED near the power button to not be a reliable power status indicator. It's very easy to add a real power on/off status LED by connecting an LED to the cargo bay connector pin 8 (switched 5VDC) and pin 21 (ground). Be sure to also add a resistor (180ohm to 330 ohm) in series with the the LED to limit current to, and protect the LED.", " ", " ", " ", " ", "Might sound barbaric,but you could remove the Create's powerbutton led and move it to a spot where you can see it.Although if you were teleoperating turtlebot. You probably would have to use a micro controller like arduino to toggle the power botton on the create with a relay then you could also tap into the powerbutton-led voltage to confirm that the Create was on or off. "], "question_code": ["iRobot Create", "Create", "Turtlebot", "passive mode", "ROS", "Create", "Create", "Turtlebot", "Create", "Dashboard"], "url": "https://answers.ros.org/question/12355/turtlebot-easier-way-to-check-power-button-status-of-irobot-create/"},
{"title": "irobot create odometry package", "time": "2011-12-17 16:14:21 -0600", "post_content": [" ", " ", "Hi Everyone,\nI want to provide odometry information for my approach.\nI use irobot_create_2_1 brown driver. There is the topic /odom that i check.\nI recognized that the x value is in the pose is absolute . i mean that it just gives the traveled distance along x axis, it does not give - values for moving backward.\nAngle value is also irrational.\nDo you know any successful implementation of this.\nOr has any of you used odometry package for the irobot.\nThank you all.\nBest "], "answer": [" ", " ", "I just tested the (Brown) driver on a model 4400 Create paired via bluetooth to a Fedora ROS install and wasn't able to reproduce the problem. Moving forward increased X and moving back caused a decrease. Can you give a bit more information about your configuration? I have heard of Creates that simply never report either forwards or backwards movements. Do you have another Create you can try?", "As for other known issues that might cause similar symptoms, I can think of a few:", "1) You can't rely on the Create's \"unpowered\" odometry. The on-board controller uses motor power information when determining the distance and direction traveled. Movement information from, for example, turning a Create over and manipulating the wheels will be inaccurate. This also applies to pushing the robot forward or back. The most common behavior in these cases is that the robot will report uniform movement in its last direction regardless of which wheels are turned and in what direction. ", "2) The odom topic attempts to track the robot's absolute---not relative---position. As a result it is very sensitive to angular error (and the best of the Creates still don't have great angular tracking). Often times the numbers for X will look like nonsense but a visualization will reveal they are relatively (in time and space) accurate and that the reference frame (i.e. what forward is) has-drifted/is-drifting. This is why combining the onboard dead reckoning with a compass or gyro is so popular. rviz or some other way of visualizing the odom information should help you determine if this is your problem. If it is and it doesn't seem due to hardware (see known problem 3 below), you can try attaching the back caster and running the robot on different surfaces and otherwise varying how the Create carries it's load (especially if it is notebook laden). This can sometimes improve things.", "3) The Create's odometry seems to be either calibrated at the factory or somehow tuned to the individual Create's motors. Occasionally, you will have a right or left motor (or both) that either from the get-go or as the Create ages simply will not accurately report information. To make matters worse this problem seems to have speeds at which it will occur or not occur on a given motor. Combined with 1 and 2 above, this can lead to frustratingly complicated failures. Some people have a fleet of Creates for this reason, others find the speed at which their robot reports the most accurate information.", "I noticed that it's not just our implementation that's causing you trouble. Since many of the other drivers take very different approaches to working around the foibles of the Create platform and you're getting identical behavior, I would try another Create if at all possible. I don't normally lean towards hardware failure and I'm not eliminating the possibility of a subtle bug but it seems unlikely that ...", "i recognized somehing. When the battery is not full, it gives erroneous measurements. If the battery is very close to full, it gives usable data."], "url": "https://answers.ros.org/question/12385/irobot-create-odometry-package/"},
{"title": "pcd_viewer crashes after installing electric [closed]", "time": "2011-10-07 13:37:55 -0600", "post_content": [" ", " ", " ", " ", "aa755@abhishekHPUbuntu:~$ rosrun pcl_visualization pcd_viewer fridge.pcd", "Loading fridge.pcd /opt/ros/electric/stacks/perception_pcl_addons/pcl_visualization/bin/pcd_viewer: symbol lookup error: /opt/ros/electric/stacks/perception_pcl_addons/pcl_visualization/bin/pcd_viewer: undefined symbol: _ZN17pcl_visualization28PCLVisualizerInteractorStyle3NewEv", "I have unstable, diamondback and electric installed. but my .bashrc sets the environment variables for electric\nsource /opt/ros/electric/setup.bash", "Closing question as there is no activity from OP"], "answer": [" ", " ", "The viewer works fine, but the viewpoint was off - the object being viewed was way off the edge of the visible windows.  Randomly moving the mouse around, or using the \"r\" key brought it back into view.", " ", " ", "The pcd_viewer and pcd_viewer_simple from opt/ros/electric/stacks/perception_pcl/pcl/bin work fine.  The displayed image is sized to the display window and oriented correctly.  The versions of pcd_viewer and pcd_viewer_simple (and bag_viewer_simple) from perception_pcl_addons/pcl_visualization/bin have the image far off screen and it's rotated around a horizontal axis that's parallel to the display's glass (mirrored top/bottom, mirrored towards user / away from user.", " ", " ", "I think that part of the problem is that both perception_pcl/pcl and perception_pcl_addons/pcl_visualization produce libraries called \"libpcl_visualization.so\".  The linker sees the version from perception_pcl/pcl first, so ignores the one from perception_pcl_addons/pcl_visualization.", "I messed around with renaming the library from  perception_pcl_addons/pcl_visualization to be libpcl_visualization_addons.so.  That fixed the undefined variables, but pcd_viewer and pcd_viewer_simple both produce blank screens.  Well, pcd_viewer_simple has the nice red, blue, green axes that follow the mouse, but that's all.  The same .pcd files (from bag_to_pcd) worked fine on a colleague's CloudViewer from the PCL windows download (I'm on Ubuntu 11.04).", " ", " ", "I am encountering the same problem as well. I get the same error when I try to run the electric pcd_viewer on my .pcd file. I am running Natty Ubuntu 11.04. Any help here?", " ", " ", " did you try the suggested solution to use the pcd_viewer inside pcl package instead of pcl_vidsualization. I had the same problem and this answer helped me", "Otherwise you can also try to rename the pcd_viewer.so to something else, maybe this helps"], "url": "https://answers.ros.org/question/11467/pcd_viewer-crashes-after-installing-electric/"},
{"title": "Grasshopper2 camera doesn't switch off", "time": "2012-01-02 05:54:48 -0600", "post_content": [" ", " ", " ", " ", "The camera1394 driver seems to leave the Point Grey Grasshopper2 (IEEE 1394b version) ", " after it is stopped (roscore included).", "I say it ", " because the camera case is hot after I stop the driver. Yes, that's how I know it remains ", ". If you know, by the way, of any other way to check to power state of the camera (or the firewire bus), please, let me know.", "With ", " the camera case goes to ambient temperature after I exit coriander (actually I press the power ", " button before exit), so it seems it's switching the camera off correctly.", "Does anybody else experienced a similar problem/behaviour?", "How can I fix it? Should I touch the camera1394 (pkg) driver?"], "answer": [" ", " ", "I've successfully applied the easy fix I mentioned above to power OFF the camera on exit. Now, at least for my Grasshopper2, the LED turns red/green (power OFF state) and the camera cools down.", "For more details see this ", " and the patch file attached to it.", "Here you have the output of the diff command I run to create the patch file", " ", " ", " ", " ", "On shutdown, the ", " driver sets video transmission off, stops capturing frames, and frees the ", " resources. It currently does not attempt to power off the device.", "Although ", " documentation says \"few cameras support the on/off feature\", ", " could support it for those that do. If you think it should, please ", " for ", ". There is time to make that change for the coming Fuerte release. "], "answer_code": ["--- dev_camera1394.cpp.bak      2012-01-02 20:56:08.306350703 +0000\n+++ dev_camera1394.cpp  2012-01-02 21:12:34.778337485 +0000\n@@ -350,6 +350,7 @@\n     {\n       format7_.stop();\n       dc1394_capture_stop(camera_);\n+      dc1394_camera_set_power(camera_, DC1394_OFF);\n       dc1394_camera_free(camera_);\n       camera_ = NULL;\n     }\n", "diff -u dev_camera1394.cpp.bak dev_camera1394.cpp > dev_camera1394.cpp.patch\n", "camera1394", "libdc1394", "coriander", "camera1394", "camera_drivers"], "url": "https://answers.ros.org/question/12505/grasshopper2-camera-doesnt-switch-off/"},
{"title": "Turtlebot / Kinect - not publishing any images?", "time": "2012-01-16 04:16:33 -0600", "post_content": [" ", " ", " ", " ", "I've built a mostly-turtlebot. After running the turtlebotbringup minimal and kinect.launch, it seems to run fine. The topics are all created, but no image data or depth data is ever published.", "The turtlebot dashboard doesn't show anything amiss.", "The front of the kinect is flashing it's green light.", "Any clue where to look?", "I tried an alternate power supply, wall power, and an alternate kinect. So it's probably not those issues."], "answer": [" ", " ", "Well, here's my answer for posterity:", "It was a bug in my USB drivers in the kernel.", "Running the freenect demos gave a lot of:\n\"Failed to submit isochronous transfer x: -1\"\nAnd dmesg informed me that usb_submit_urb was returning -28 (ENOSPC).", "Switching up my kernel versions and USB drivers made everything come to life.", "Thanks for the help everyone that tried, I really appreciate it.", " ", " ", "Is the 12V line of the Kinect powered? The green LED will indicate that USB is powered, but if the 12V is not powered then the Kinect will not publish data.", " ", " ", " ", " ", "I've had some problems with the openni_kinect driver not publishing any image messages, and there seem to be others experiencing this problem, too.  There's a bug ticket  at ", "  and another thread dealing more specifically with this issue ", ".  Hopefully they'll fix it in fuerte, but in the meantime, you might try restarting the driver until you get some topics published."], "url": "https://answers.ros.org/question/12651/turtlebot-kinect-not-publishing-any-images/"},
{"title": "file [rospack find turtlebot_navigation/nav_rviz.vcg] does not exist", "time": "2011-11-10 01:57:45 -0600", "post_content": [" ", " ", " ", " ", "Having trouble with the navigation tutorial.  Map is correctly saved but can not be brought up in rviz.  Opening up rviz shows a black map (when it should be grey?).  There is a \"Global Status Error: Target Frame - For frame [/kinect_rgb_frame]: Frame [/kinect_rgb_frame] does not exist\"", "When I quit rviz I notice the following error:", "\"File [rospack find turtlebot_navigation/nav_rviz.vcg] does not exist\"", "Does that mean that rviz was not initialized properly with the map data?", "Not sure if this is related but I am using \"roscore\" to bringup the bot instead of the \"sudo service turtlebot start\"", "Thanks for the help in advance."], "answer": [" ", " ", " ", " ", "The error: ", "means that it could not find the vcg file in the turtlebot_navigation folder. It does not mean it could not load your map. All the vcg file does it help you configure your rviz environment. The file should be located in the turtlebot_navigation package. If not, try updating your software.", "The error:", "means that the frame you are trying to view does not exist. Most likely your fixed frame in rviz is set to kinect_rgb_frame, to get rid of this error click on the fixed frame box and select a different frame from the list like base link. For an overview of how rviz works see here: ", "Most likely the reason your map is not being displayed is because the map display module is not loaded. If you click on the add button at the right bottom of the rviz window, then select map and select ok. Once you have added the map module you must select the topic to listen to for the map, click on the topic field and select the map topic and the map should appear.", "On a side note, you need to use more than roscore to bring up TurtleBot. sudo service turtlebot start actually just launches minimal.launch in turtlebot_bringup. ", " ", " ", "thanks for the tips, they are very helpful.  i had to run the rospack find as a separate statement and then paste the result back into the original statement and my rviz is coming up correctly now.  ", "still not getting bringup to run smooth as i'd like.  not getting the dashboard to light up properly as one of the battery lights never turn green but both batteries are fully charged.  wonder if it has anything to do with the bringup?", " ", " ", "I found why the rospack substitution failed. It is necessary to use the backtick ` rather than the standard single quote '. Hope this helps someone."], "answer_code": ["\"File [rospack find turtlebot_navigation/nav_rviz.vcg] does not exist\"\n", "\"Global Status Error: Target Frame - For frame [/kinect_rgb_frame]: Frame [/kinect_rgb_frame] does not exist\"\n"], "url": "https://answers.ros.org/question/11885/file-rospack-find-turtlebot_navigationnav_rvizvcg-does-not-exist/"},
{"title": "gyro for turtlebot", "time": "2012-01-24 19:52:24 -0600", "post_content": [" ", " ", "Hi,", "I'm looking for a gyro for my turtlbot, but the ones that I've found the ", " are horribly expensive (microstan -> \u00a31592+VAT!). Now I am looking for a normal gyro (arround \u00a350-70), but my question is: will I have to build the drivers or the drivers for the gyro that turtlebot uses by default with iCreate (SparkFun ADXRS613S/623 Breakout Board) will more or less work with any gyro??", "Thanks."], "answer": [" ", " ", " ", " ", "In short, yes you should be able to get away with replacing your turtlebot's gyro with another similar one without having to \"write a driver\" per se--since most cheap gyros operate in the same manner: outputting an analog voltage proportional to the angular rate about the axis of the sensor package it should only require minor modification of the turtlebot's existing gyro.py python module.  ", "A post on how to hook up a rate gyro mimicking the configuration in the turtlebot (its not specific to the ADXRS613 it refers to) can be found ", ". This can be useful if you don't actually have a turtlebot, but rather just a create with a laptop (no turtlebot power/gyro board).", "Unfortunately the main distributor for the ", " was Sparkfun and they've discontinued that breakout due to the chip being end-of-lifed by the manufacturer.  You may be able to find some resellers that still have stock, but I haven't had much success.  ", ": There seem to be a few left ", ", however these will probably dry up soon.", ": It should be noted that ClearPath, one of the distributors of the Turtlebot has apparently ", ". Since they have in-house fab capability, its unlikely they'll produce a breakout for this chip--you'd have to make your own or wait for someone like Sparkfun to make one.", "The MLX90609 breakout might make a suitable replacement (from ", " or ", ").  Again, if you're just operating with a iRobot Create then you can hook up the analog out of the gyro to the same DB-25 pins described in the first linked post.  You may still have to modify \n", " in the src folder of the turtlebot_node package a bit.  Taking a quick glance at that file shows a slightly different formula for calculating orientation than I'm familiar with, so here's the calculation (in C++) that I use on my cheap gyro (from ", "):", "You'll have to figure out from the datasheet what the sensitivity (mV/deg/sec) value is and substitute it in the equation above.  It should be relatively trivial to modify gyro.py's publish method if need be to match the above calculation.  "], "answer_code": ["    double dt = current_time - last_time; // last time we read ADC\n    double maxValue = 1024; // using Create's 10-bit ADC\n    double vRef = 5;  // 5v reference\n    double zeroRateV = avg * vRef / maxValue; // I calculate zeroRateV from a circular buffer during startup, avg is the average of these values\n    double sensitivity = 0.013;  // mV/deg/sec from datasheet\n\n    rate = (gyro_adc * vRef / maxValue - zeroRateV) / sensitivity;\n\n    orientation += rate * dt;\n"], "url": "https://answers.ros.org/question/12745/gyro-for-turtlebot/"},
{"title": "Kinect not detected", "time": "2011-10-06 08:10:50 -0600", "post_content": [" ", " ", "Hi,\nI previously posted an issue regarding errors in openni drivers installation (", "). With the answer to this question i was able to install the drivers successfully.\nThen i ran the openni_camera node's openni_node.launch file, which says the devices is not connected. ", "What possibly could be wrong here?", "Thanks in advance, Karthik"], "answer": [" ", " ", "Reinstalled Ubuntu 11.04 (32bit) installed ROS electric and everything else. Now started getting the pointcloud from Kinect in rviz.\nHope this is not the solution to the question but since I couldn't find much help on this, I had to go this way.\nSo this is one of the solution to the problem.", "Thanks\nKarthik", " ", " ", "Same problem here with ubuntu 10.04, it seems to be an issue with the libusb-1.0-0-dev or the openni-dev libraries...", "The solution was to re-install the dependecies of the openni_kinect stack", "First unistall the OpenNI developer package", "$ sudo apt-get remove openni-dev libusb-1.0-0-dev", "and reinstall it afterwards", "$ sudo apt-get install openni-dev libusb-1.0-0-dev", "Then recompile the package usign the --rosdep-install and --pre-clean options", "$ rosmake openni_ros --rosdep-install --pre-clean", "Let me know if it works for you...", "regards..", "Mario", " ", " ", "Adding my own notes from troubleshooting...", "Sometimes it can also be a power problem.  Check to make sure the connect turlebot adapter (or wall adapter) is plugged in and the green light on the connector is on.  Verify the icreate base is on.", "Try an lsusb.  You should see 3 devices from \"Microsoft Corp\".  If you only see one, the kinect may not have full power.", " ", " ", "I've had this problem before and solved it by killing the XnSensorServer.", " ", " ", "Does it work better if you start each node separately? ", "first", "then, in another terminal"], "question_code": ["aravindhan@rrc-laptop:~$ roslaunch openni_camera openni_node.launch\n... logging to /home/aravindhan/.ros/log/a30405ca-f056-11e0-83fc-5c260a051546/roslaunch-rrc-laptop-22434.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://rrc-laptop:42441/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /openni_node1/use_indices\n * /openni_node1/depth_registration\n * /openni_node1/image_time_offset\n * /openni_node1/depth_frame_id\n * /openni_node1/depth_mode\n * /openni_node1/debayering\n * /rosversion\n * /openni_node1/projector_depth_baseline\n * /openni_node1/rgb_frame_id\n * /openni_node1/depth_rgb_translation\n * /openni_node1/depth_time_offset\n * /openni_node1/image_mode\n * /openni_node1/shift_offset\n * /openni_node1/device_id\n * /openni_node1/depth_rgb_rotation\n\nNODES\n  /\n    openni_node1 (openni_camera/openni_node)\n    kinect_base_link (tf/static_transform_publisher)\n    kinect_base_link1 (tf/static_transform_publisher)\n    kinect_base_link2 (tf/static_transform_publisher)\n    kinect_base_link3 (tf/static_transform_publisher)\n\nauto-starting new master\nprocess[master]: started with pid [22450]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to a30405ca-f056-11e0-83fc-5c260a051546\nprocess[rosout-1]: started with pid [22463]\nstarted core service [/rosout]\nprocess[openni_node1-2]: started with pid [22473]\nprocess[kinect_base_link-3]: started with pid [22476]\nprocess[kinect_base_link1-4]: started with pid [22477]\nprocess[kinect_base_link2-5]: started with pid [22478]\nprocess[kinect_base_link3-6]: started with pid [22479]\n[ INFO] [1317931571.886741090]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931572.887058521]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931573.887393253]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931574.887772578]: [/openni_node1] No devices connected.... waiting for devices to be connected\n[ INFO] [1317931575.888150279]: [/openni_node1] No devices connected.... waiting for devices to be connected\n"], "answer_code": ["killall XnSensorServer\n", "roscore\n", "rosrun openni_camera openni_node\n"], "url": "https://answers.ros.org/question/11447/kinect-not-detected/"},
{"title": "turtlebot gmapping large area", "time": "2012-02-15 01:43:30 -0600", "post_content": [" ", " ", " ", " ", "I have tried unsuccessfully to map a large area using gmapping.  The map creation was working, and yesterday I created smaller maps.  However, today, after about 45 minutes of driving, the Turtlebot stops and I get the errors in the gmapping output: \"could not get robot pose\".  Also, I noticed that the create power LED is off, and in one case I got a power error on the dashboard.  The battery display showed that there was plenty of power.  ", "Then, I turned the create back on and the robot briefly found it's pose. I started driving again, and the robot jumped off the map and I got the \"out of bounds\" errors in gmapping.  It seems it wasn't able to recover from that first stop.", "I was trying to map a rectangular hallway area in an office building. The long edge is about 60 meters, and about 30 meters on the short edge.  I went around about 2 and 2/3 times trying to get the loop to close when the Create stopped.  So, I estimate that it traveled about 500 meters.", "Do the Create's motors get hot and cause the create to shutdown?  What conditions would cause a power off?  Any insight into the practical run-time of the Turtlebot would be appreciated.", "Incidentally, after the first time this happened, I changed the map resolution to 10cm, in case it was running out of map space.", "Thank you.  I was able to get each hallway in separate runs, but I never got the whole floor in a single run without costmap \"out of bounds\" errors or the Create powering itself off (I am still investigating why that happens).  I merged the maps as you suggested (using Gimp) and was able to load it."], "answer": [" ", " ", " ", " ", "Although this isn't a direct solution to the root cause of your issue (", "'s power down), making multiple sub-maps separately for a large area and ", " is a valid work-around for the completion.", "In my case with ", ", after a long-run of ", " (say an hour or so) the map tends to get corrupted (I see it via ", "), which made me split the large area into pieces. As long as the maps share the common \"interfaces\" b/w each other, combining shouldn't be a problem (although I got small pitfalls in editing ", " files, that's another subject)."], "answer_code": ["Create", "turtlebot", "gmapping", "RViz", ".pgm"], "url": "https://answers.ros.org/question/27666/turtlebot-gmapping-large-area/"},
{"title": "Ubuntu 64bit 11.10 + Kinect", "time": "2012-02-13 14:10:27 -0600", "post_content": [" ", " ", " ", " ", "I installed ROS and trying to run RBGDSLAM using Kinect. There was some issues installing drivers (such as changing openjava7 to sun java6 for compiler)\nlsusb does not show camera and audio.", "When I connect usb and run command", "roslaunch openni_camera openni_node.launch", "---output ", "INFO] [1329185066.431671629]: [/openni_node1] No devices connected.... waiting for devices to be connected", "-------output for lsusb", "Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub", "Bus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub", "Bus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub", "Bus 003 Device 010: ID 0409:005a NEC Corp. HighSpeed Hub", "Bus 001 Device 003: ID 0bda:58e5 Realtek Semiconductor Corp. ", "Bus 002 Device 003: ID 046d:c517 Logitech, Inc. LX710 Cordless Desktop Laser", "Bus 003 Device 011: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor", "I am not sure why it is only showing the Motor on usb port. ", "Thank you in advance."], "answer": [" ", " ", "It looks like you might not have the 12V power supplied to the Kinect.  There should be three Microsoft devices listed.  ", "See "], "url": "https://answers.ros.org/question/27559/ubuntu-64bit-1110-kinect/"},
{"title": "kinect issues on different machines", "time": "2012-02-09 23:07:54 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I tried running the following launch script, with my kinect, on two different machines, on the first one (desktop) everything works fine, on the second one (laptop) nothing works:", "on the laptop the output looks exaclty the same like on the desktop:", "no errors, nothing. ", "The problem is that on the laptop there is nothing going one on the topics. e.g. listening to /camera/depth/points or /scan on the desktop I can see messages comming in, on the laptop there is nothing, silence.", "There is a difference in the dmesg output. On the desktop:", "On the laptop:", "USB 2.0 on Asus Eee PC 1005HA. btw. I've added freenect-glview output to the question."], "answer": [" ", " ", "To answer my own question, my Eee PC is  too slow and dropping too many packets. htop is showing CPU usage >97%. That's the reason why ROS is not working at all and libfreenect says \"Lost too many packets\". I've tried it with a \"normal\" Laptop and everything works fine. Mystery solved.", " ", " ", "This might be a stupid suggestion - but the Kinect needs power (12v) either from a Wall Wart or an external 12v supply.  The Kinect will  look like it is powered up because the USB puts out 5v and will let some of the LEDs light up.  Some Kinect cables have a power good LED on the Cable.  If it isn't lit - there's the issue.", "So make sure it is plugged in and getting 12v.  I spent a week trying to figure this out on a Turtlebot, not realizing that on a Turtlebot, the Kinect gets it power from the create's battery, not the USB.  (The suspicious unconnected cable should have given me a clue, but it didn't!)", "I'm not changing the power supply when switching between desktop and laptop. All 3 devices are visible (else openni_node wouldn't connect to Kinect at all and freenect-glview would not work at all)."], "question_code": ["<launch>\n  <!-- kinect and frame ids -->\n  <include file=\"$(find openni_camera)/launch/openni_node.launch\"/>\n\n  <!-- openni manager -->\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"openni_manager\" output=\"screen\" respawn=\"true\" args=\"manager\"/>\n\n  <!-- throttling -->\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"pointcloud_throttle\" args=\"load pointcloud_to_laserscan/CloudThrottle openni_manager\">\n    <param name=\"max_rate\" value=\"2\"/>\n    <remap from=\"cloud_in\" to=\"/camera/depth/points\"/>\n    <remap from=\"cloud_out\" to=\"cloud_throttled\"/>\n  </node>\n\n  <!-- fake laser -->\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"kinect_laser\" args=\"load pointcloud_to_laserscan/CloudToScan openni_manager\">\n    <param name=\"output_frame_id\" value=\"/openni_depth_frame\"/>\n    <remap from=\"cloud\" to=\"cloud_throttled\"/>\n  </node>\n</launch>\n", "SUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /openni_node1/use_indices\n * /openni_node1/depth_registration\n * /openni_node1/image_time_offset\n * /openni_node1/depth_frame_id\n * /openni_node1/depth_mode\n * /kinect_laser/output_frame_id\n * /openni_node1/debayering\n * /rosversion\n * /openni_node1/projector_depth_baseline\n * /openni_node1/rgb_frame_id\n * /openni_node1/depth_rgb_translation\n * /pointcloud_throttle/max_rate\n * /openni_node1/depth_time_offset\n * /openni_node1/image_mode\n * /openni_node1/shift_offset\n * /openni_node1/device_id\n * /openni_node1/depth_rgb_rotation\n\nNODES\n  /\n    openni_node1 (openni_camera/openni_node)\n    kinect_base_link (tf/static_transform_publisher)\n    kinect_base_link1 (tf/static_transform_publisher)\n    kinect_base_link2 (tf/static_transform_publisher)\n    kinect_base_link3 (tf/static_transform_publisher)\n    openni_manager (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n\nauto-starting new master\nprocess[master]: started with pid [4969]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to 34911bc8-53e3-11e1-96ac-0023dfffccb1\nprocess[rosout-1]: started with pid [4982]\nstarted core service [/rosout]\nprocess[openni_node1-2]: started with pid [4985]\nprocess[kinect_base_link-3]: started with pid [4986]\nprocess[kinect_base_link1-4]: started with pid [4987]\nprocess[kinect_base_link2-5]: started with pid [4988]\nprocess[kinect_base_link3-6]: started with pid [4989]\nprocess[openni_manager-7]: started with pid [4990]\nprocess[pointcloud_throttle-8]: started with pid [4991]\nprocess[kinect_laser-9]: started with pid [4992]\n[ INFO] [1328877118.394903193]: [/openni_node1] Number devices connected: 1\n[ INFO] [1328877118.395245674]: [/openni_node1] 1. device on bus 002:30 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00363A03948136A'\n[ INFO] [1328877118.398388308]: [/openni_node1] searching for device with index = 1\n[ INFO] [1328877118.446196137]: [/openni_node1] Opened 'Xbox NUI Camera' on bus 2:30 with serial number 'A00363A03948136A'\n[ INFO] [1328877118.454999825]: rgb_frame_id = '/openni_rgb_optical_frame' \n[ INFO] [1328877118.456160538]: depth_frame_id = '/openni_depth_optical_frame'\n", "usb 2-1.4: new high speed USB device using ehci_hcd and address 35\nusb 2-1.4: configuration #1 chosen from 1 choice\nhub 2-1.4:1.0: USB hub found\nhub 2-1.4:1.0: 3 ports detected\nusb 2-1.4.2: new full speed USB device using ehci_hcd and address 36\nusb 2-1.4.2: configuration #1 chosen from 1 choice\nusb 2-1.4.1: new high speed USB device using ehci_hcd and address 37\nusb 2-1.4.1: configuration #1 chosen from 1 choice\nusb 2-1.4.3: new high speed USB device using ehci_hcd and address 38\nusb 2-1.4.3: configuration #1 chosen from 1 choice\n", "usb 1-5: new ..."], "url": "https://answers.ros.org/question/12973/kinect-issues-on-different-machines/"},
{"title": "Making my Turtlebot with ASUS Xtion [closed]", "time": "2012-03-04 02:09:37 -0600", "post_content": [" ", " ", " ", " ", "I have read ", " etc for making your own turtle bot, I wish to do it using ", "; do I need to use the ", " as it is used when assembling the turtlebot using Microsoft Kinect ? ", "Any first hand experience will be most useful. "], "answer": [" ", " ", "No; the Asus Xtion only needs USB power, and already has a USB connector.", "If you want a bracket, iHeartEngineering makes one that is pretty nice: ", " : Very nice ! makes sense to buy Xtion than Kinect ! ", "You'll also need to launch the asus version of the TurtleBot urdf, if you want the camera to be correct in the visualization", " Thank you ! That never crossed my mind ! ", "If you need it, the alternate URDF is available here. ", "Also, since it is open source the files for making your own are available here.\n", ":16761", " Cheerio ! ", "Your link is broken."], "url": "https://answers.ros.org/question/29051/making-my-turtlebot-with-asus-xtion/"},
{"title": "Turtlebot bringup minimal.launch, cannot find create on dev/ttyUSB0", "time": "2012-03-15 05:44:21 -0600", "post_content": [" ", " ", " ", " ", "Every day I start up my turtlebot, I have the same problem with minimal.launch. Every day, I have to fix it, seemingly through random chance. I get it working, but it takes a significant amount of time everyday. I've looked through every post on the subject many times, and have added rules to give permissions to udev, but every day I have the same problem and simply have to get lucky in reboot.", "Even if the create USB is assigned USB0, sometimes it doesn't work. ", "Can I go in and change the minimal launch so that it's looking for a SYNNAME for create instead of just looking at ttyUSB0?", "Then making a rule in udev/rules.d like this:", "KERNEL==\"ttyUSB*\", SYSFS{idVendor}==\"0403\", SYSFS{idProduct}==\"6001\", SYSFS{serial}==SERIAL_OF_USB_CABLE, SYMLINK+=\"turtlebot\"", "So my real question is if I do this in minimal.launch, are there going to be problems in the code later where instead of looking for turtlebot, the code is simply looking for ttyUSB0.", "Thanks", "Are you stopping the turtlebot service before running minimal.launch?"], "answer": [" ", " ", "The ", " has a parameter for what the port is.  You will have to modify the launch file to set that parameter differently if you want to change the port name.  ", " ", " ", "Thanks. I might just do that and see if it works better. ", "Right now, I'm still just having problems with the minimal.launch working when the create is correctly at ttyUSB0. I mostly just don't understand what I could be missing as every day, exactly no change can either cause it to work, or not work. And I do have other devices plugged into my Turtlebot, but minimal.launch has worked fine with them before. ", " ", " ", "I have this same problem. ubuntu 11.10. ros electric", "tully: you have missed the point with your response. the currect port is there - just not providing data", "These may be different problems. If you have the right port set and nothing is working it might be hardware. I would suggest using kermit to connect to the serial port at 57600 baud and power cycle the create. If the hardware is working it should display something.", "The ckermit connands are \"set line /dev/ttyUSB0\", \"set speed 57600\", \"set carrier-watch off\",\"connect\". After that power cycle the create.", " ", " ", "We are working on some of the serial port issues.", "You may want to try out the config script attached to this patch.\n"], "url": "https://answers.ros.org/question/29788/turtlebot-bringup-minimallaunch-cannot-find-create-on-devttyusb0/"},
{"title": "Gyro for Turtlebot?", "time": "2012-04-05 16:11:44 -0600", "post_content": [" ", " ", " ", " ", "I'm in the process of creating my own TurtleBot since I have most of the parts already. However it seems the sparkfun gyro suggested on turtlebot.com has been discontinued. Can anyone suggest a good alternate gyro or IMU? Preferably the device would be a drop in replacement for the gyro (i.e. will work with the TurtleBot dashboard w/o modification)."], "answer": [" ", " ", "This ", " seems to answer your problem ", " ", " ", "I assume you are aware you could buy the gyro built in with this power board ", " from I heart engineering.  ", "I had not seen that before. Thanks.", " ", " ", "I'd suggest Xens - see ", " or Microstrain - see ", ". ", "Do these interface with the connect the same way the sparkfun gyro does?", "AFAIK, for the Turtlebot one just needs angular rate (gyro) information about the z-axis. It appears to me that full 6/9 DOF XSens and Microstrain IMUs are overkill for this application, especially considering their cost.", " ", " ", "If you're looking for something which can integrate directly into your PC but doesn't have the cost of Xsens or Microstrain, ", " have worked well for us in the past."], "url": "https://answers.ros.org/question/31341/gyro-for-turtlebot/"},
{"title": "Neural network and machine learning ?", "time": "2012-03-11 03:25:36 -0600", "post_content": [" ", " ", " ", " ", "Is there a ROS node/facility to implement machine learning via Artificial Neural Network (ANN) ? I know, I can write my own C++ and Python codes. However since topics as navigation, SLAM and face recognition has some inbuilt resources at ROS, so I was hoping that ANN may also have such a node/package. "], "answer": [" ", " ", " ", " ", "I used ", " to create a reinforcement learning application that was using gazebo simulator with pr2, and had rewards based on the topics published by the simulator and the robot nodes.", "I believe that the ANN doesn't need to be part of ROS, you just need to implement your publishers subscribers. ", "By the way, pybrain is pretty powerful, and the ANN has an ok documentation, but the reinforcement learning classes are awfully documented.", " Are your codes/ideas on the public domain ? ", "No, as it was just an experiment for a course at my AI master, but I'll gladly send it to you by email if you are interested.", " Thank you, please send it to my email arkapravobhaumik at gmail dot com. In case I make any fruitful use of your work, I will cite you/your lab etc. ", " OpenCV also includes a fairly extensive collection of machine learning routines including neural networks.  See ", ".", " Thank you - will check it out ! ", " can I have also copy of your codes/ideas?", "Sure just let me know your email address.\nRecently I've been experimenting with what Arkapravo suggests, and I find opencv Machine Learning library much more powerful than pybrain and better documented. I would go with that one", "Ok, thanks. My email adress is ", " ", " ", "I'd recommend this: ", " could you send me the same files if you do not mind?\nemadmahrousawwad at gmail dot com", " ", " ", " ", " ", "I used ", " and ", " wrapped around with a ros node that subscribes/publishes learning-specific data. But I guess you would need something else if you are to use ANN.", "An up-to-date list that shows what most popular ML tools support can be seen ", ".", " ", " ", "So far as I know, there is not. However, I'd be stunned if there weren't existing python or C++ libraries for these kinds of operations; writing ROS nodes that use such libraries is easy.", "(In fact, making it easy to interface with other libraries is the _point_ of ROS)"], "answer_details": [" ", " ", " ", " ", "Python ", "open source", "good tutorials", "widely used", "effective where needed, for example: using LibSVM under the SVM classes", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/29523/neural-network-and-machine-learning/"},
{"title": "Navigation stack - only in place turning", "time": "2012-04-10 07:25:57 -0600", "post_content": [" ", " ", "Hi there,", "I have a Robot that runs pretty good under ROS. I can teleop it, have a valid URDF-model, I can create maps with the help of the laser scanner.\nThe robot itself is differential driven. It has a rectangular shape with 4 wheels. The motorized wheels are located in the front, where the base_link is located, too.\nWhen I follow the tutorials to get the whole navigation stack running. The Path to a navigation goal is calculated and displayed as well. But the robot does not follow the path. It is performing in place rotations instead of forward driving. The cmd_vel-topic says so, too. The angular.z component is at the maximum set in my parameters and the linear x and y component are zero (y should be 0 as it is non holonomic).\nIs this a known problem? Is it caused by the uncircular shape of the robot?\nCould anybody help me?", "Thanks so far.", "Sven"], "answer": [" ", " ", "hmm it seems like this is a problem of the local planner... This usually happens when the robot thinks it is too close to an obstacle. ", "Are you sure the local and global costmap show no obstacles close to the robot?", "You can also try to use another local-planner. ROS offers the \"base_local_planner\" and \"dwa_local_planner\" (see ", " #4 for more information). ", "But my guess still is that there is something set up wrong with the localization and/or obstacles...", "Couple of Qs: Isn't dwa one of the parameters that can be set in base_local_planner? Then why does it have a different package with different names for the same parameters? \n\nAnd why does Navigation Stack spin like that when it is close to obstacles?", " ", " ", "My guess is that the controller in move_base that is generating cmd_vel has a hard time to find the right control commands. This controller basically takes the odometry message and tries to follow the planned path as close as possible. Verify that your odometry message actually makes sense, in particular the velocity values are important for move_base. Check that they actually match the actual velocities of your robot. Be sure that velocities are actually sent in the robot's base frame, not in the odometry frame. I.e. when the robot is driving straight forward with 1 m/s x velocity should be exactly 1 and y zero, no matter in which direction the robot is moving in the odometry frame. Try plotting the velocities with rxplot, drive forward, turn 90 degrees and drive forward again. Both times, only the x velocity must be 1.", "Could you explain what you mean by the controller \"takes the odometry message\"? Does that mean the tf b/w odom and base_link or something else? Could you explain the control in more detail?", " ", " ", "I once had the same problem. I resolved it in a bizarre way, and I do not know why it worked, but it worked and I was happy. When I had everything prepared, before giving a goal for the robot to follow, I launched", "rosrun dynamic_reconfigure reconfigure_gui", "And in the pop-up button, I opened the navigation configuration, and after that everything worked. For more info on this, check this ", " I made.", "cheers", " ", " ", "\nThe laser scanner is defined in the costmap_commons and displayed in rviz, and the costmaps are, too.", "\nIn fact I never really tested if the robot is driving the wanted speed. I will check that tomorrow, as i am not in the university today.\nThe other stuff should be working. The odometry seems to be right (moving 1 meter in rviz means moving 1 meter in reality) and slow rotations are recognized correctly, too. I limited the navigation stack to slow rotations as they are no problem.", "hi, welcome to ", ". please, have in mind that this website is not a forum, so you should not post answers responding to answers, and so on. if you have updates, you should update your question, if you want to answer to suggestions, do so in the \"post comment\" in the respective suggestion", " ", " ", " May be the question is a bit stupid, but how do I change the local planner? I thought the only change I could make using the navigation stack was switching dwa on or off?!", "as I tried to tell you before, this is not a forum, you should NOT post answers as it were a forum. if you have a new question, create a new question. if you have updates, update your current question, if you want to comment on answers, use the \"post a comment\".", "i will be glad to answer how to change the local planner as soon as you post it in the proper way. right now this page should be devoted to your \"in place rotation\" problem only. each new post should be a direct answer to your first question. i hope you understand. cheers", "When launching move_base, you can set rosparams that define which local planner should be used. As Proc\u00f3pio said, it would be nice if you could open a new question, then i can post an example move_base.launchfile... ", " ", " ", "I don\u00b4t think this problem is caused by the shape of the robot, it seems more like your robot is performing some kind of recorvery behaviour (e.g. turning in place). I recommend to visualize all costmaps and laser scans to check if it is a problem of your localization and if your sensors are correctly configured (e.g. did you define the laser scanner (or another sensor) in the costmap_commons.yaml)?", " ", " ", " today the robot should move a straight path and followed it (with the accurcy I told him) and afterwards he should drive an other path (shown in the screenshot) ", "The green path is the global path. The yellow obstacles are the inflated obstacles by the local planner. The robot moved a bit from its starting position to where the footprint is in the picture and then performed this recovery behavior...\nMy inflation radius is set pretty low (0.30) in the costmap_common_params.yaml and in the base_local_planner_params.yaml dwa is set to true.", "And the link you posted does not exist ;)", "But thanks untill here!", "Ups sorry, i meant that link: ", "Did you try to use another local planner?", " ", " ", "Hi,", "I compared the cmd_vel-topic with the odom-topic and the speeds are neartly the same.\nI found out, that the navigation_stack only generates really short local paths. And one time the robot reached the goal, an other time it had to drive a path, drove a bit, performed the in place turnings and then aborted, saying ", "Aborting because a valid plan could not be found. Even after executing all recovery behaviours", "And the robot only hab to drive a straight path for lets say 70 centimeters...\nSo I think now I can say that the tourning really is a recovery behaviour, but I dont't know why if the planned global path has no obstacles in its way...", "I have had similar problems. Did you resolve them in any way?"], "url": "https://answers.ros.org/question/31580/navigation-stack-only-in-place-turning/"},
{"title": "how to convert the world coordination to robot coodination in PR2 ros", "time": "2012-02-26 14:03:15 -0600", "post_content": [" ", " ", " ", " ", "I am try to control pr2 simulator to grasp some object on the table. The robot is fixed as it in the beginning, now I want to know how to convert the object coordination to robot coordination, then we can use Cartesian controller to drive the right arm to reach the object , and grasp the object.\n  Normally we know the object coordination on the table, so how we set the target for arm Cartesian controller coordination to reach the object.\n  Also how to keep the orientation of the gripper vertical to the table, thus make the grasp easily.\n  Besides, is there an documentation for the coordination of pr2 simulator and pr2 arm."], "answer": [" ", " ", "I guess ", " is the library you are looking for. Tutorials ", ". It is the ROS way to transform between different coordinate frames. For instance, if you have the pose of your object as a PoseStamped (that's a pose that also contains the coordinate frame it is relative to and a time stamp) you can use TF to transform the object's pose to the robot's base. I actually believe that the cartesian arm controller internally also uses TF to transform poses into the correct frame, i.e. passing it any valid PoseStamped should work. ", "If you need more complex arm motions, I suggest that you have a look at ", ". It provides a pretty powerful interface for arm navigation. You can find tutorials ", " and ", "."], "url": "https://answers.ros.org/question/28571/how-to-convert-the-world-coordination-to-robot-coodination-in-pr2-ros/"},
{"title": "Transform from base_link to odom failed", "time": "2012-01-12 01:29:08 -0600", "post_content": [" ", " ", "If someone can shoot some wisdom my way I would be much obliged.", "I have the gyro installed, showing up on dashboard, all green.\nWhen I am driving around performing the mapping, I get the error in terminal from gmapping.", "Transform from base_link to odom failed", "The below is what I get from dashboard.", "Node: /slam_gmapping Time:\n  1326381589.218725027 Severity: Error Location:\n  /tmp/buildd/ros-electric-slam-gmapping-1.2.4/debian/ros-electric-slam-gmapping/opt/ros/electric/stacks/slam_gmapping/gmapping/src/slam_gmapping.cpp:laserCallback:496\n  Published Topics: /rosout, /tf,\n  /slam_gmapping/entropy, /map,\n  /map_metadata", "Thanks.", "Is this a turtlebot or a turtlebot clone? What is the system load while running gmapping? Does this happen immediately, or only after gmapping has been running for a while?", "It is a turtlebot clone. It doesn't happen immediately, say if I spent 15mins driving around a space mapping, I would get the error 4 or 5 times. It can then be seen that the position of the turtlebot jumps in Rviz. I suspect that the inconsistent odom values then play havoc on closing the loop.", "From what it seems it looks like a problem with the tf. ", "What are the specs on your computer? What is the system load (uptime, top) while getting this error?"], "answer": [" ", " ", "On a system that is slightly underpowered, you may see this error occasionally. As long as it's happening infrequently, perhaps only once every few minutes, you should be able to ignore it.", "Alternatively, you could capture tf and scan data as a bag file and run gmapping offline, when other nodes aren't competing with it for CPU and memory. See: ", "(2f)Tutorials(2f)MappingFromLoggedData.html", "If you're seeing this more than once per second, it means that there's something wrong with your odometry source.", " ", " ", "Do you have an odometry source running? On a normal turtlebot, this is part of the minimal.launch file.", "This is still a persistent problem.", "From what it seems it looks like a problem with the tf. "], "url": "https://answers.ros.org/question/12614/transform-from-base_link-to-odom-failed/"},
{"title": "Turtlebot: Unstable", "time": "2012-03-14 01:41:18 -0600", "post_content": [" ", " ", "We recently started working with a turtlebot using the ros tutorials for the same.", "We do not know when it is switched on in full mode and when in passive mode or safe mode. Immediately after we switch on the iRobot Create, the red light flashes and there is a small beep and the green light comes on. But in a few seconds the green light goes off with another beep. This doesn't happen all the time, but very often. Also, the moment we switch on the Create, the kinect cable's led is ON. But only when the red light flashes, the led is on. After the beep, after the green light comes on, this kinect cable's LED goes OFF. And sometimes the output from minimal.launch says, \"check if the Create is plugged in and powered on\", when the connections are perfect and after i have turned the create on.", "Will minimal.launch get the turtlebot service on by default? Because if I start the service and then launch minimal.launch, it says \"shutdown request: New node registered with the same name.\"", "Dashboard doesn't always work from the workstation. Sometimes it works:Sometimes not. It works fine on the turtlebot. On the workstation it says POWER SYSTEM ERROR. Though we do not know what that means. This also happens only sometimes. ", "Sometimes starting up minimal.launch powers the kinect. (May be the bot is pushed to full-mode). Sometimes the same thing happens with teleop. We are not always able to push it to full mode from the dashboard, which should work.", "We still managed to calibrate the bot and build a map. The map was nowhere near the real room. Later we found out that when the actual bot turned 90degrees the bot on the map turned very less (about 45degrees). We manipulated the parameters odom_angular_scale_correction and gyro_scale_correction, but to no luck.", "What we know for sure.\n1. The network connection is perfect. For teleop and ssh work great.\n2. The env variables, ROS_MASTER_URI and ROS_HOSTNAME are correctly set.\n3. All mechanical connections on the turtlebot are correct.", "Please help us out."], "answer": [" ", " ", " ", " ", "okay...", "The create doesn't always display it's power state accurately using the LED. Typically the green LED will be on and then go off when the ROS driver connects. Decide whether you're going to use minimal.launch or the turtlebot service but do not use both. They do the same thing essentially and will conflict with each other causing nodes to startup and shutdown repeatedly.", "to start the service type:", "to stop the service type:", "If the service isn't working or not starting up properly look at the dashboard and it should display the problem. Or make sure that you have the correct interface setup for the service. Check out this tutorial for details ", "most likely the reason the dashboard isn't working is because you don't have the ROS_HOSTNAME set on the workstation machine in the terminal that you ran the dashboard from. Make sure that the terminal you are running from has the correct ROS_HOSTNAME and ROS_MASTER_URI.", "minimal.launch will never power up the kinect. There is no method for doing it. most likely what is happening it the robot received a cmd_vel msg, the TurtleBot node will put the TurtleBot in full mode when it receives a cmd_vel msg, when the breaker service is called, or when the TurtleBot is set into full mode. ", "Are you sure that you calibrated the gyro properly? If you run the calibration repeatedly it should get approximately the same number every time. Also check to see what the angular rate of the gyro is 150 deg/sec or 300deg/sec. TurtleBots from clearpath now have 300deg/sec gyros and you must set the ~gyro_measurement_range parameter for the gyro type. You can also use dynamic reconfigure to do this as well. Also make sure that after you calibrate the TurtleBot you update your launch file either minimal.launch or turtlebot.launch in /etc/ros/electric(or your current distro). Then relaunch minimal.launch or restart the TurtleBot service. ", " ", " ", "@ mwise_wg: Ours is a willow garage robot. So our gyro range is set right at 150deg/sec. But that didn't work. The problem that we have been facing with map building is that it builds a double map. We also tried altering the odom_angular_scale_correction param to about 10 - just to check if it alters anything. But that doesn't seem to cause a single difference. The robot on the map still make only half the turn that the real robot makes. The robot turns 90 degrees but thinks it has turned only 45 degrees.", "how are you setting the parameter? are you using dynamic reconfigure? The TurtleBot driver only reads the odom_angular_scale_correction at startup or when set in dynamic reconfigure.. Are you editing  /etc/ros/electric/turtlebot.launch then restarting the TurtleBot service? ", "No. We were simply using 'rosparam set' ... the tutorials say nothing about dynamic reconfigure. We do just what is given in the tutorials.", "thanks for the info, I updated the TurtleBot calibration tutorial to be correct. ", " ", " ", "WillowGarage turtlebots actually have 3 different types of Gyros according to this page: ", " ", "The one that was at my university was an old willowgarage turtlebot and hence had a gyro that has a measurement range of 150deg/sec. Whereas the other three that were shipped to us very recently have 250deg/sec accuracy. Where is this mentioned?? The willow garage site only says 150deg/sec accuracy. Why don't they specify it clearly on the specifications page? This patch, on the page mentioned above, was the one we used and we calibrated the turtlebot 3-4 times after which the parameters converged. Then, we could build a neat map. The old one is a XRS613 Gyro whereas the new one is the XRS652. ", "Also the folder /etc/ros is generated only when we run the 'sudo install.bash <wireless interface=\"\">' in the 'turtlebot_bringup/upstart' folder... But ", " it says 'OR'... 'OR' means we can skip one, right? But both seem very necessary.", "Willow Garage TurtleBots have a gyro rate of 150deg/s (as described above), however Clearpath TurtleBots have different gyro rates. The Willow Garage site only lists the specification for TurtleBots produced by Willow Garage. (I am the TurtleBot co-creator, I know what hardware is on which robots)", "Also when using the upstart install you want to use sudo service turtlebot start not minimal.launch ", "But clearly, there are two different gyros on the tow robots sent to us from WILLOWGARAGE. Not Clearpath. The old one has a red chip and it says XRS 613. The new ones don't have a red chip and they read XRS652. And the patch works. Only after the patch it works!", "Please have a look... \nThis is what we have on the gyroboard. The number says ADXRS652 and that is this... ", "The chip clearly says Copyright WillowGarage 2011 ", "the board states that because it was designed by Willow Garage, However the individual manufacturers stuff different gyros, because the original part was at end of life. The board design allows for part substitutions. If the plates on the turtlebot say clearpath then it was made by clearpath. ", "this is what an iheartengineering turtlebot looks like: ", " and this is what a clearpath turtlebot looks like: ", "/ ", "Also all TurtleBot power boards manufactured by Willow Garage have bar code on them with a Willow Garage part number on them.", " ", " ", "The Diamondback version doesn't create a \"/etc/ros/diamondback\" folder? Dynamic Reconfigure is also not working. There is no file such as the python file you had mentioned in the tutorials. Today we found someone at the University who has worked on a turtlebot and built a very neat map. He controlled the bot from his DELL Laptop-WS. But When I controlled the same turtlebot from my ThinkPad, I got the same map-building error. The robot thinks it has turned 45 when it has turned 90 in reality. I know this is weird but please bear with me and help me! :( This happens with all three turtlebots we have! :(", "Sorry, this was too long to become a comment.", "From his DELL Laptop the map is neat and clean without the calibration. ", "It also says ERROR: Transform from baselink to odom failed!", "please upgrade to electric, TurtleBot development has come a long way since diamondback. ", "It is not a diamondback problem. It is a problem with the gyro that people at WillowGarage did not communicate properly. My friend found a patch from the net, now it works fine. will write in detail in a short while."], "answer_code": ["sudo service turtlebot start\n", "sudo service turtlebot stop\n"], "url": "https://answers.ros.org/question/29678/turtlebot-unstable/"},
{"title": "ASUS Xtion and Open NI documentation", "time": "2012-03-27 07:21:23 -0600", "post_content": [" ", " ", "I have come across lot of documentation and books on Microsoft Kinect;", "However, I have not found much on the ASUS Xtion or on Open NI. Being a Xtion rookie, where should I start ? Any reference to forums, websites, books, tutorials etc would be most useful. ", "Any first hand experience on how to marry the Xtion (as of now without any robot) with ROS ? Do I have to use OpenNI, or ROS has any way around ? "], "answer": [" ", " ", "The power of OpenNI is that it is a \"standard\" interface to many devices (currently the older Primesense SDK5 devices, Asus XTION and the Kinect). It works just fine with the ROS OpenNI drivers. "], "url": "https://answers.ros.org/question/30580/asus-xtion-and-open-ni-documentation/"},
{"title": "Using the Turtlebot gyro without ROS", "time": "2012-03-10 10:28:40 -0600", "post_content": [" ", " ", "Hi,", "We're using the iRobot Create for a project with a laptop mounted on board, like the turtlebot. However, we're using Windows rather than ROS. After some testing, we saw the dead reckoning of the iRobot is pretty poor (most problems from angle rotated), and found the turtlebot's gyro sensor and power board, which looks very convenient!", "Is it possible to access and make use of this gyro sensor through the serial port to improve on the navigation within our C# program?", "Is there any documentation or anywhere someone can point me to that would show me how to interface with the sensor and perform any necessary navigation-correcting calculations?", "Thanks!"], "answer": [" ", " ", "The gyro on the power board provides input to the \"user analog input\" pin on the Create. Your Windows API will likely provide a method for reading this. If it doesn't, check out ", ".", "The next step you'll have is to correct for sensor error. There are two primary types you should worry about:", "After you do those two corrections, you're left with incorporating that data into your position estimates. You can start by integrating it to determine chassis rotation in your current dead-reckoning approach, and improve it further by adding Kinect data via an ", ". I'm not sure what direction you want to go in for this latter point, but hopefully the above helps...1.", "I second all of the above; at this point, though, you've just reinvented a bunch of stuff that ROS does for you, in a well-debugged, well-integrated manner. I'm told there's some rudimentary ROS support on windows; probably enough to let a linux machine do the work, and a windows do the controls."], "answer_details": ["Scale error. The gyro may provide an analog value for a given rotational rate which doesn't quite match up with the spec. The ", " node can automatically determine this error by comparing a prediction based on the gyro with an actual observation of motion from the TurtleBot's Kinect sensor.", "Drift. The analog value corresponding to a zero rotational rate drifts over time. The TurtleBot is constantly recalibrating what the \"zero value\" is by averaging gyro sensor values when both wheels are not moving, and using that value as an offset.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/29501/using-the-turtlebot-gyro-without-ros/"},
{"title": "Robot_Pose_EKF filter TF fails", "time": "2012-03-12 20:54:25 -0600", "post_content": [" ", " ", "When running the Robot_Pose_EKF filter the transforms start to fail (the model in rviz flashes )\nOriginally  this was a long post that I solve before posting while the solution is self evident in hindsight it has taken a while to find.", "I had to disable the TF transform that was published by my base controller  for it to work \n reference this post ", "If someone could add this to the wiki it might save others some time"], "answer": [" ", " ", "Problem solved by deleting TF published by base controller", ", Can you mark this answer as accepted by checking the green checkmark at the left?"], "url": "https://answers.ros.org/question/29612/robot_pose_ekf-filter-tf-fails/"},
{"title": "Has anyone worked with the Clearpath Robotics Husky A200?", "time": "2011-03-28 09:57:44 -0600", "post_content": [" ", " ", " ", " ", "I am wondering if anyone has worked with the Clearpath Robotics Husky A200 platform under ROS. ", "\nSpecifically I would like to know: \n1. Does the ROS API give you enough flexibility?\n2. What hardware do you use to interface with it? (i.e. laptop, netbook, microATX board)\n3. What additional sensors have you used with it?\nThanks!"], "answer": [" ", " ", "I haven't personally used it, but it appears that the ROS API is well-written and well-supported by the actual manufacturer/designer.", "It appears that all of the information from the robot base is brought out on various message types.  I don't know what kind of \"flexibility\" that you are looking for, but you could probably use the documented API and extend it to anything that you could possibly need.", "I would imagine that any computer supported by ROS and clearpath would serve to work with the platform.", " ", " ", "In case anyone would like more details in the future:", "Support-wise, we're also on these forums a lot :)", " ", " ", "Old topic, might still be useful: I can answer these questions for the A100, the predecessor to the A200.", "The API is very simple to use, and provides all the hooks you'll need to interface the chassis. The update rate is flexible and can be set very high (20Hz +). The hardware interface is straightforward (velocities, odometric readings, system and power status, battery levels, etc), all published cleanly on separate topics. There are no services that I know of, but they aren't necessary for this type of hardware interface. No Complaints.", "The joystick demos work out-of-the-box and will help you get started.", "We interface from laptop to chassis directly via USB-RS232 interface. We've had no problems with this interface at all, other than the port-detect feature in the vanilla A100 ROS API can interfere with the gpsd_client port detect. To avoid this, set the \"port\" rosparam for the chassis.  Their packaged launch file can be edited directly to do this.  This is a problem with GPSD, not clearpath, per se.", "Our package was GPS over GPSD_Client, compass and telemetry equipment via a custom board, interfaced with our own ROS nodes, Husky A100 with Clearpath_base ros node, and a custom-written EKF-based navigation routine.  ", "See other reply for relevant links.", " ", " ", "These guys seem to be using it with ROS as a base for "], "answer_details": [" ", " ", " ", " ", "We try to expose nearly all relevant controls from our regular API to ROS ", "Most of our users use their own laptops, TurtleBot-style netbooks, FitPCs, or mini-ITX form-factor motherboards", "Most commonly, people use Garmin/NovAtel/uBlox GPS units, SICK/Hokuyo LIDARs, Kinects, AXIS cameras, CH Robotics/MicroStrain IMUs, Point Grey stereo cameras", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/9588/has-anyone-worked-with-the-clearpath-robotics-husky-a200/"},
{"title": "Transform PoseWithCovariance", "time": "2012-02-15 22:54:15 -0600", "post_content": [" ", " ", "Is there any funcionality in the tf library which allows transforming PoseWithCovariance msgs to a different frame? It should consider not just the pose transformation but also the covariance eigen vectors rotation.", "I talk a bit more about this problem in this post of my blog: ", "/ . I'm sorry it is in spanish but It may be useful for some people.", "You might want to read into \"S. Su and C. Lee, \u201cManipulation and propagation of uncertainty and\nverification of applicability of actions in assembly tasks,\u201d Systems,\nMan and Cybernetics, IEEE Transactions on, vol. 22, no. 6, pp. 1376\u2013\n1389, 1992.\""], "answer": [" ", " ", "Hi Pablo, ", "The new package ", " in the ", " stack does exactly what you're looking for. ", "It includes ", " with a rigorous treatment of uncertainty, even when it's given as a 6x6 covariance matrix in yaw-pitch-roll, which is much harder to implement than e.g. as a 7x7 with unit quaternion representation.", "The package is actually a wrapper of functionality in the lib ", ", which has dozens of unit tests regarding uncertainty transformations so it should be quite reliable. Much more of ", " functionality is expected to be wrapped in the future. ", "Check the ", " for references to a technical report explaining all the internal equations.", "Cheers.", "This is a really really interesting package. AFAIK ROS lacked of this functionality. Neither TF, KDL, Bullet nor BFL provide this functionality. Specially interesting is the case of changing a pose to an uncertain frame. ", "In fact I would like to see TF system based of this uncertainty handling. I think this would be a much more powerful system allowing even a transformation graph instead of a transformation tree thanks to the possibility of a bayesian fussion of two estimation of the same frame. ", "Pablo: Building a transformation tree is \"easy\", given you correctly handle covariance transformations. But graphs, which may have loops, are much harder! They become in fact an entire new problem on its own: graph-slam ;-)", "I can't seem to find a git link to download either pose_cov_ops or mrpt_common. Are there any instalation instructions I'm not aware of?", "Instructions can be found here: "], "url": "https://answers.ros.org/question/27743/transform-posewithcovariance/"},
{"title": "reduction in image quality in ROS", "time": "2012-03-22 03:30:31 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI am working on iball 12MP camera in ubuntu . The quality of image is good when i view the video using guvcview software.\nWhen i launch the camera.launch file and view raw image, the video quality is degrading badly in the raw image.\nHas anyone faced the same problem? \nI'm working on face detection and recognition program, the input video quality is very bad, since the video goes into ROS and OpenCV. The final output also degrades.\nSo please post your views. ", "my launch file", "i have attatched 2 images from guvcview and uvc_cam driver below", " ", "What ROS camera driver are you using? What driver parameters?", "i'm using the driver given in the link ", "Please update your question to include the camera launch file you are using. When you view it in guvcview, what are the parameters set to (e.g. fps, width, height, auto_exposure) and are they the same in the camera launch file?"], "answer": [" ", " ", "Since you are not setting the camera parameters manually in the launch file, they are getting set to the default in the driver. Note that the driver's defaults are NOT initialized to the current settings on the camera (they are specified either in the driver code or in the UVCCam.cfg file), so whatever parameters you set in ", " are not what is being set by ", ". This could result in very dark images, as that is the desired behavior for the use case the driver forked for.", "See my answer to ", " for details on how to \"undarken\" your image when using my ", " driver.", "Your best bet is to find the set of parameters you like using the guvcview program, record them, set them with ", "'s ", " while the ", " driver is running (or in the launch file before starting up) and then any values you changed from default you should set in your launch file. Note that setting any unsupported modes in the ", " may crash the driver or cause the camera to lock up and need power-cycled (Usually this is only if you try to change the resolution, as the ", " doesn't wait to get both a width and height pair before reconfiguring the driver. I suggest setting the width and height in the launch file beforehand.)", "If your problem is not a very dark image, then I suggest you include both an image from while guvcview is running and an image of the same scene with the ", " driver so that we can understand the type of degradation you are seeing.", "Eric, thank you. soon i will attatch those 2 images for u to compare...", "From your launch file, you aren't setting things like saturation, gain or contrast. I believe the defaults for those result in crappy color images like the ones you see. Try using the reconfigure_gui to play with the values or just set them in the launch file to whatever it's set to in guvcview.", "Thank you Eric! I will do", "is it possible to launch stereo view in uvc_cam?", "uvc_cam only supports streaming from one camera at a time, per instance of the driver. You could run two uvc_cam nodes, each connected to a different camera (with each pushed down into a namespace) to view two cameras at the same time. However, there wouldn't be any explicit synchronization.", " ", " ", "this is my launch file.\n<launch>\n        <node name=\"uvc_cam_node\" pkg=\"uvc_cam\" type=\"uvc_cam_node\" output=\"screen\">\n            ", " ", " ", " ", "\n        </node>\nyes all the parameters are same.", "You should edit your original question to reflect this new information instead of posting it as an answer. Answers may be reordered by votes or age, so it will not necessarily make sense to the next person with this problem.", "ok. i have put the launch file with all the parameters. i don know why it is not getting displayed. i have updated my question by your advice. my parameters(width-640, ht-480, fps-60) are not getting displayed there also..", "You needed to put them in a \"code\" block in the question (otherwise I assume they were being interpreted as HTML or something). I edited your question to fix that."], "question_code": ["<launch>\n<node name=\"uvc_cam_node\" pkg=\"uvc_cam\" type=\"uvc_cam_node\" output=\"screen\"> \n    <param name=\"device\" value=\"/dev/video0\" /> \n    <param name=\"width\" value=\"640\" /> \n    <param name=\"height\" value=\"480\" /> \n    <param name=\"frame_rate\" value=\"60\" /> \n</node>\n</launch>\n"], "answer_code": ["guvcview", "uvc_cam", "uvc_cam", "dynamic_reconfigure", "reconfigure_gui", "uvc_cam", "reconfigure_gui", "reconfigure_gui", "uvc_cam"], "url": "https://answers.ros.org/question/30235/reduction-in-image-quality-in-ros/"},
{"title": "beginner robot", "time": "2012-02-24 04:28:54 -0600", "post_content": [" ", " ", " ", " ", "I would like an affordable beginner's robot that can be used for learning and exploring ROS. The ones that are compatible with ROS mentioned on the ROS website, are a little out of my budget at the moment. Are there any cheaper alternatives?", "A simple mobile platform would be sufficient to begin with and then I can add more components as my proficiency with ROS increases. I tried search for other similar questions on the site, but it turned up naught...", "The iRobot create would not be an option because I am from outside the US.. unless, they changed the rules that allows foreigners to buy iRobot creates..", "Thanks"], "answer": [" ", " ", "Where are you located? You might be able to work off of a Roomba instead of a Create.", "For example, I think that's what ", " does. (Though they're not accepting orders currently).", "Im in Asia. Either way, a turtle bot is too expensive I feel. I would simply be happy with a mobile platform without any other sensors. Something with motors, wheels and a base that can be controlled using ROS...", "If you use a serializer MCU and 2 gearmotors with encoders, wheels will cost you about $250. But you have to assemble it your self, Pirobot  (", ") has ROS software to make work", "By a \"serializer MCU\" you mean a microcontroller that has a serial port and can communicate serially with the main computer?", "Yes but most use a USB port (", ") With the above setup you will have a basic robot that can be controlled with ROS the built in motor driver has enough power to carry a small laptop", "How about the arduino? Would I be able to do the same thing with arduino? Because, it is a lot cheaper and there is a \"rosserial\" stack with a \"rosserial_arduino\" package which enables communication between the arduino and ROS. Ofcourse, I will probably have to buy a separate motor driver...no probs", "Yes here is a link (", "/) my own robot is based on a lot of this code here is the link to my blog (", "/) it's a holomonic rob0t and uses a arduino mega", "Hi ", ".  The Serializer MCU is now called the Element and is sold by cmRobot at ", "!element-.net.  "], "url": "https://answers.ros.org/question/28413/beginner-robot/"},
{"title": "turtlebot kinect power", "time": "2012-04-21 18:19:38 -0600", "post_content": [" ", " ", "Hello everyone. I was trying to test out the board on my turtlebot (I don't have ROS setup yet but I should have access to my computers soon). I am only using serial commands over realterm. I used a crimping tool, cut out the power-adapter from the Y-splitter and connected the red/brown wires as shown in the manual and plugged them in (as ", " ", ")", "I then set the create to Full mode via realterm but the kinect isn't powered on at all (blinking green light). Can anyone help me power the kinect using just OI (I don't have a laptop to run ROS on atm and I would really like to see if I have any defective hardware.", "Thanks,\nShriphani P", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base."], "answer": [" ", " ", "The Kinect requires both USB and 12V power to operate.", "You should get a multimeter and measure the voltage on the power connector to confirm that it is producing 12V.", "I am having problem while connecting the kinect power using DB25 connector of Irobot create base. The connector circuit is designed by clearpath robotics to provide 12V DC power to kinect by taking power from pin 10 and 14 of DB25 connector of irobot create base.", "please ask a new question.", "I have asked the question with more detailed information.... need you help.\npls visit question here ........ "], "url": "https://answers.ros.org/question/32322/turtlebot-kinect-power/"},
{"title": "minimal.launch > constantly restarting vol2.", "time": "2012-03-29 05:41:49 -0600", "post_content": [" ", " ", " ", " ", "Hi, I'm aware of the previous topic about it, but still know what to do. When I execute \n$ roslaunch turtlebot_bringup minimal.launch", "I'm getting:", "..."], "answer": [" ", " ", "I think I got rid of it by not executing roscore!", "What happens now is that dashboard opens with the first 2 buttons green and Breaks and batteries grey. when I click on breaks, it says: Cannot control breakers until we have received a power board state message", "Also it gives the following log:", "File \"/opt/ros/electric/stacks/turtlebot_viz/turtlebot_dashboard/src/turtlebot_dashboard/power_state_control.py\", line 111, in set_power_state\n    self._char_cap = 0.8", "float(msg['Charge (mWh)']) ", "\nKeyError: 'Charge (mWh)'\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/wx-2.8-gtk2-unicode/wx/_core.py\", line 14636, in <lambda>\n    lambda event: event.callable(", "event.kw) )\n  File \"/opt/ros/electric/stacks/turtlebot_viz/turtlebot_dashboard/src/turtlebot_dashboard/turtlebot_frame.py\", line 274, in new_dashboard_message\n    self._power_state_ctrl.set_power_state(battery_status)\n  File \"/opt/ros/electric/stacks/turtlebot_viz/turtlebot_dashboard/src/turtlebot_dashboard/power_state_control.py\", line 111, in set_power_state\n    self._char_cap = 0.8", "float(msg['Charge (mWh)']) ", "\nKeyError: 'Charge (mWh)'\nTraceback (most recent call last):", "Any ideas? please help! :)", " ", " ", "I think the most likely cause of this is that the iRobot Create was in a bad state.  The driver will try to soft reboot if the robot is discharged and plugged in to be able to recharge the batter.  However sometimes the Create doesn't work as expected.  "], "question_code": [" * /robot_description\n * /diagnostic_aggregator/base_path\n * /robot_pose_ekf/freq\n * /robot_pose_ekf/publish_tf\n * /app_manager/interface_master\n * /robot_pose_ekf/vo_used\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/analyzers/digital_io/startswith\n * /diagnostic_aggregator/analyzers/power/path\n * /robot_pose_ekf/output_frame\n * /diagnostic_aggregator/analyzers/mode/path\n * /diagnostic_aggregator/analyzers/digital_io/type\n * /diagnostic_aggregator/analyzers/mode/startswith\n * /rosversion\n * /diagnostic_aggregator/pub_rate\n * /robot_state_publisher/publish_frequency\n * /diagnostic_aggregator/analyzers/sensors/startswith\n * /diagnostic_aggregator/analyzers/power/startswith\n * /turtlebot_node/bonus\n * /robot/type\n * /diagnostic_aggregator/analyzers/nodes/type\n * /diagnostic_aggregator/analyzers/nodes/contains\n\nNODES\n  /\n    appmaster (app_manager/appmaster)\n    app_manager (app_manager/app_manager)\n    turtlebot_node (turtlebot_node/turtlebot_node.py)\n    turtlebot_laptop_battery (turtlebot_node/laptop_battery.py)\n    robot_state_publisher (robot_state_publisher/state_publisher)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[appmaster-1]: started with pid [20615]\nprocess[app_manager-2]: started with pid [20616]\nprocess[turtlebot_node-3]: started with pid [20617]\nprocess[turtlebot_laptop_battery-4]: started with pid [20618]\nprocess[robot_state_publisher-5]: started with pid [20619]\nprocess[diagnostic_aggregator-6]: started with pid [20620]\nprocess[robot_pose_ekf-7]: started with pid [20621]\n[ WARN] [1333035260.990533573]: The root link base_footprint has an inertia specified in the URDF, but KDL does not support a root link with an inertia.  As a workaround, you can add an extra dummy link to your URDF.\nturtlebot_apps.installed\nloading installation data for [turtlebot_apps.installed]\n[INFO] [WallTime: 1333035261.490848] Starting app manager for turtlebot\n[INFO] [WallTime: 1333035261.524692] Waiting for foreign master [http://localhost:11312] to come up...\n[INFO] [WallTime: 1333035261.529719] Foreign master is available\n[INFO] [WallTime: 1333035261.559937] Registering (/turtlebot/app_list,http://localhost:42480/) on master http://localhost:11312\n[INFO] [WallTime: 1333035261.582940] Registering (/turtlebot/application/app_status,http://localhost:42480/) on master http://localhost:11312\nturtlebot_apps.installed\n[INFO] [WallTime: 1333035261.613521] Registering service (/turtlebot/list_apps,rosrpc://localhost:44179) on master http://localhost:11312\n[INFO] [WallTime: 1333035261.617852] Registering service (/turtlebot/start_app,rosrpc://localhost:44179) on master http://localhost:11312\n[INFO] [WallTime: 1333035261.635923] Registering service (/turtlebot/stop_app,rosrpc://localhost:44179) on master http://localhost:11312\n[INFO] [WallTime: 1333035261.898413] serial port: /dev/ttyUSB0\n[INFO] [WallTime: 1333035261.898585] update_rate: 30.0\n[INFO] [WallTime: 1333035261.898699] drive mode: twist\n[INFO] [WallTime: 1333035261.898814] has gyro: True\n[INFO] [WallTime: 1333035261.958912] self.gyro_measurement_range 150.000000\n[INFO] [WallTime: 1333035261.959174] self.gyro_scale_correction 1.350000\n[INFO] [WallTime: 1333035268.441169] going into soft-reboot and exiting driver\n[INFO] [WallTime: 1333035275.948409] exiting driver\n[INFO] [WallTime: 1333035282.372672] going into soft-reboot and exiting driver\n[INFO] [WallTime: 1333035289.884078] exiting driver\n[INFO] [WallTime: 1333035296.320293] going into soft-reboot and exiting driver\n[INFO] [WallTime: 1333035303.827759] exiting driver\n[INFO] [WallTime: 1333035310.267098] going into soft-reboot and exiting driver\n[INFO] [WallTime: 1333035315.026299] Setting turtlebot to full mode.\n[INFO] [WallTime: 1333035317.770321] exiting driver\n[INFO] [WallTime: 1333035324.199584] going into soft-reboot and exiting driver\n"], "url": "https://answers.ros.org/question/30770/minimallaunch-constantly-restarting-vol2/"},
{"title": "rosbridge /rosjs/typeStringFromTopic does not work", "time": "2012-04-19 02:16:55 -0600", "post_content": [" ", " ", "I call this service (/rosjs/typeStringFromTopic) with the following code:", "var types=[]; ", "\n  con.callService('/rosjs/typeStringFromTopic', '[/mav2/fcu/mag]', function(rsp)\n  {\n      ", "\n          alert(\"here\");\n          ", "\n      alert(rsp);\n          ", "\n      types.push(rsp);\n  });", "My topic is not empty but I \"here\" is never alerted. There is the following error in my rosbridge terminal", "Problem Problem Problem Problem Problem Problem Problem Problem Problem Problem ", "\n  Traceback (most recent call last):", "\n    File \"/data/my_ros/rosbridge/bin/rosbridge.py\", line 39, in handleFrameHelper\n      call = json.loads(frame)", "\n    File \"/usr/lib/python2.7/json/__init__.py\", line 326, in loads\n      return _default_decoder.decode(s)", "\n    File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n      obj, end = self.raw_decode(s, idx=_w(s, 0).end())", "\n    File \"/usr/lib/python2.7/json/decoder.py\", line 384, in raw_decode\n      raise ValueError(\"No JSON object could be decoded\")", "\n  ValueError: No JSON object could be decoded", "\n  Problem Problem Problem Problem Problem Problem Problem Problem Problem Problem", "Thank you for your help :)", "Beno\u00eet"], "answer": [" ", " ", " ", " ", "It looks like your issue is likely how you're passing the topic argument. ros.js can take proper JSON strings or (in newer versions) pure Javascript objects, which it will convert for you. Unfortunately, your current syntax is somewhat in-between. Either:", "or the simpler and much preferable:", "should work.", "Notice how answer.ros.org's highlighting turns the first example fully green because it's a string. The second example is preferred because its harder to mess up. It's just an array containing a single string. Also remember that Javascript has two types of quotes. So:", "and:", "are also fine.", "This ", " shows reading in all of the currently published topics and displaying their types.", " ", " ", "Thank you a lot"], "answer_code": [" '[\"/mav2/fcu/mag\"]'\n", "  [\"/mav2/fcu/mag\"]\n", "  \"['/mav2/fcu/mag']\"\n", "  ['/mav2/fcu/mag']\n"], "url": "https://answers.ros.org/question/32194/rosbridge-rosjstypestringfromtopic-does-not-work/"},
{"title": "react on diagnostics (eg shutdown on low battery)", "time": "2012-04-26 05:37:41 -0600", "post_content": [" ", " ", "I saw the ", ", but is there a package that actually reacts on those diagnostic messages in terms of issuing a given message or shell command?", "I'd like to shutdown the robot PC if the battery goes low (which the OS isn't aware of), and thought using ", " would be the right way.", "Is there a prepared solution to that, or do I have to write a special node?"], "answer": [" ", " ", "What I do on my project is have a backing message format (just a few strings and integers) to represent system state for things like power and temperature.", "I then have a separate node that bridges these messages into the diagnostics message format, so that I can get a human readable log of robot diagnostics.", "To make decisions based on battery capacity or state, I can use the backing message.", " ", " ", " ", " ", "You can use the ", " with the ", " to display that and other diagnostic information in a human-readable format. For hardware status monitoring, those packages are quite good.", "The ROS diagnostic tools are for human notification (see: ", "). Automatically shutting things down should probably be handled differently.", "why use a separate channel? Why not create a not that would listen to the diagnostic messages, filter them based on their data, and take action when appropriate? I've read before that it's not recommended (maybe from you) but I can't see why. Could you elaborate?", "I was merely paraphrasing the REP-0107 recommendation. The link to that document provides more details, with reasons. ", " [REP] That's difficult when it gets to autonomous robots. Maybe the core problem is that ", " [REP] that needs complex parsing no one really wants to do. ", "The parsing could be skipped, though, when reacting solely on the level flag.."], "question_code": ["diagnostics"], "url": "https://answers.ros.org/question/32736/react-on-diagnostics-eg-shutdown-on-low-battery/"},
{"title": "New to ROS as well as LINUX.", "time": "2012-05-04 01:20:20 -0600", "post_content": [" ", " ", "Hello everybody,\n                This is Shailendra singh from India.I am very new to ROS+LINUX.Till now i was working with windows,embedded system and simple manual robotics using AVR series 8 bit micrcontrollers.But to get myself updated according to the latest trends of RObotics,i started learning ROS.I started with installing ubuntu10.10.Then i installed ROS using command $ sudo apt-get install ros-fuerte-desktop.this installed ros of 728 mb in my system.Than i started working with the help of ", " but i find that i am not getting anything.Is there any tutorial availiable with the help of which i can start step by step.I know i am asking very silly question,but need help in that.Its quite confusing.\ni) I understand the use of ROS,its basically used to simulate robots rather than testing them in real time.\nii) Wheres the coding part?\niii) And how should we start?", "if someone can give those answers i think it will be too great for new webbies to understand things too easily."], "answer": [" ", " ", " ", " ", "You can find everything you need to know in the form of tutorials on the official ROS site here:", "I don't actually know why you started with website you mentioned on your question, but this is apparently about how to configure ROS Diamondback with the Ganzebo simulator and several other packages and NOT about how to get started with ROS. I also recommend you to install the newest ROS release Fuerte like it is explained here:", "Maverick (Ubuntu 10.10) is not supported in Fuerte. If ", " really wants to stick with Maverick, Electric is the newest compatible ROS distribution.", ":- Well i do have electric installed to in my UBUNTU10.10.But i am not getting what to do next,because there are many confusing tutorials in that.Means after that how should i continue with simulators as obiviousaly i dont have harware to work with,what about the coding to be done for simulation", "You should first go through the tutorials ( ", " )  to know how ROS works. After completing those, you can go to the ROS package of the Gazebo simulator and start working. Having done the tutorials, you will know how to get this started...", " ", " ", "hey michikarg, Thanks a lot for your reply.But in that website where we had to install ROS diamondback,everything is given,what to do after installing diamond back where as in ROS tutorials given on ROS website,i didnt find anything clearly,so its a bit dificult.Can you help me how to get started with that?\nMeans what i asked,how can we simulate any of the robot,bcz i think its all regarding getting everything done related to actual robot as a simulation rather than doing it in real time using hardware.", " If you want to run simulations of robot,maybe you can see this pages(", ") and find tutorials to follow~", "I agree with your view.I lead 9 people writing some applications with ROS and I found it is really not easy to follow tutorials and really found what to do can really understand ROS and use the full power of ROS. When I just look,I found that ROS doc seems provide everything,but hard for self learn.", ":- yup you are right,like in easy to understand way,we do install a software,we do have a editor,we write code,we compile code and if we had simulator we can see the compiled code simulation in the simulator ,you can take example of ANDROID sdk", "so if someone can help us to understand thing in a proper manner,like which package to be installed,then where to do coding,then where to see simulation,then i will be obliged.", "I have no way to add info to ROS docs. So if you have questions, you just feel free to open a thread to ask. It is better to ask the questions(with text and keyword) when you really read the ROS tutorials you don't understand. If you ask good questions, maybe can make ROS tutorials more clearly."], "url": "https://answers.ros.org/question/33263/new-to-ros-as-well-as-linux/"},
{"title": "Running ROS in pure simulation [closed]", "time": "2012-04-12 02:51:20 -0600", "post_content": [" ", " ", " ", " ", "Hi, ", "I would assume ROS can run in pure simulation? I'm trying to build a SLAM map of simulated environment while navigating through it (with path planning and obstacle avoidance) using the simulated TurtleBot. However most of the TurtleBot tutorials assume you have the TurtleBot hardware. ", "FYI, I've already got the TurtleBot simulator working via:", "However I'm unsure how to:", "FYI, I would of thought I could do (5) via", "But it doesn't move the TurtleBot in the simulator - I assume its for the real hardware only?", "Any ideas?"], "answer": [" ", " ", "If the simulation is working properly the answers to your questions should be:", "2-5. Exactly as in the real robot tutorials!", "The commands you have posted for 5. work fine for me. Did you use the correct buttons in the focused terminal to control?", " ", " ", "You don't need to calibrate the simulation (nor can you, in fact).", "To view the output of the simulation, you can use rviz. I recommend:", "(taken from ", ")", "Regarding driving the simulation - can you echo /cmd_vel when you're running keyboard_teleop and verify that it's publishing the right data?", " ", " ", "Hi McMurdo,", "How did you get teleop working, as the above command doesn't work for me - I've been RTFM, but not luck as yet ;) - any ideas?", "What about viewing the Kinect sensor output in simulation, or path planning, or SLAM output? - Any ideas?", "Cheers,", " ", " ", " ", " ", "I'm new to ROS (however I was the co-author of a similar project DROS at ", "), so please excuses my newbie questions about ROS. Anyway:", "1) any good simulator should allow you to simulate the calibration steps. E.g. most DMU's have a roll bias, so the simulator should have a hard coded roll bias. Could you please point me to the tutorials to calibrate a robot in pure simulation?", "2) domhege, I'm sure they are, but the tutorials are not the most easiest to find (they seem rather split across pages, and sometimes are missing vital prerequisite), can you please point me to the tutorials I should be following...", "3) re drive the robot around. I've just started up the empty world simulation as shown above, then run that keyboard teleo command above, and when I press \"q\" a few times, the robot in the simulator doesn't not move. Any ideas what I maybe missing? Ryan, how do I \"echo /cmd_vel\"? can you point me to the tutorial on this?", "4) Ryan, when I run that command it opens up, but I just see a black screen. Any ideas how to see the particles, kinect output, etc. And the output of \"roslaunch turtlebot_navigation gmapping_demo.launch\", keeps saying \"[/openni_camera] No devices connected.... waiting for devices to be connected\" - which seems to be looking for the real Kinect hardware - so how do I tell it to use a simulated Kinect?", " ", " ", " ", " ", "Personally I think a pure simulation feature should be mandatory for ROS (and its packages such as turtlebot), so researchers/engineers can see the best outcome (i.e. with zero errors), and then to basically switch on the simulated noise to get a better idea of how the solution will work in a real world with real robots.", "1) One suggestion would be to have a one-page howto for newbies, that contains a simple list of commands (and small description - with links to the full data), to get ROS up and running with (or without) a robot, to show the major features (i.e. sensor output, localisation output, mapping output, etc)", "2) Also the TurtleBot tutorials seems to assume you have all the hardware. Is there a ROS tutorial to show you how to setup everything without the hardware. Basically I have a heap of personal robots I would like to get running with ROS, and I was hoping to base it off the TurtleBot - but since I don't have the TurtleBot hardware, most of the tutorials keep leading me to dead ends (since the hardware is required for most steps).", "I see, I thought that hitting \"q\" alone would move the robot forward. I see need to use i, etc - so that is now solved for me.", "Yeah here is the following things in red, after running \"roslaunch turtlebot_navigation gmapping_demo.launch\" and \"rosrun rviz rviz -d ", "/nav_rviz.vcg\"", "And \"roslaunch turtlebot_navigation gmapping_demo.launch\" keeps saying:", "How do I give it a map? And the kinect error is simply because I don't have a physical kinect attached (I'm currently waiting on a power/comms cable). Any thoughts on how to deal with this errors for a pure simulation method?", "Cheers,", " ", " ", "I have tried this once. I think you should be able to teleop the turtlebot quite easily. I have done that on the simulator. All simulators - gazebo, stage, etc - try to trick ROS into believing that the data has originated from real sensors. You should be able to do all this by following the TurtleBot tutorials exactly. However, there might arise small problems which you must be able to solve with little difficulty.", "I think an empty_world will be of no help in doing anything. You need a wall for calibration. And lots of landmarks for SLAM. Search for .world files on the internet. I do not know where you can get one. But I am sure there will be a lot of world files for the simulator.", "Good Luck!", " ", " ", "Though it's possible to add errors to the Gazebo simulation, the ", " package interfaces directly with the ADC values generated by the hardware, which is not yet simulated (really, I think this has been the first time it's been mentioned as a deficiency in the TurtleBot package).", "All of the tutorials for TurtleBot are listed in order ", ", under the \"Getting Started\" heading. Do you have some ideas for how they can be more clearly laid out for new users such as yourself?", "My version of the keyboard teleop has \"q\" increasing max speed only, and driving with i/j/l/,/etc. Could you let us know what you have? Echoing cmd_vel is done with:", "Does rviz show any errors? They're usually highlighted in red on the left side", " ", " ", "Hi, any thoughts for my previous questions?"], "answer_details": ["You probably don't need this.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Robot Model -> Status: Error -> This lists a heap of errors such as: No transform from [base_footprint] to [/map]", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", "calibrate the simulated TurtleBot", "see the output of a simulated Kinect", "see the output of SLAM (and ideally the particles in the particle filter)", "see the output of the path planning", "drive the robot around (or set way points)"], "answer_code": ["rosrun rviz rviz -d `rospack find turtlebot_navigation`/nav_rviz.vcg\n", "rospack find turtlebot_navigation", "[ INFO] [1334647393.669410590, 120.900000000]: [/openni_camera] No devices connected.... waiting for devices to be connected\n[ INFO] [1334647394.352812232, 121.676000000]: Still waiting on map...\n", "rostopic echo /cmd_vel\n"], "url": "https://answers.ros.org/question/31704/running-ros-in-pure-simulation/"},
{"title": "rosbridge image (uri-data)", "time": "2012-05-21 05:20:14 -0600", "post_content": [" ", " ", "Hello,\nI try to deal with a topic which type is sensor_msgs/Image. I want to display those messages in a webpage using rosbridge. As it's written ", "\nthe json obejct contain an uri field.\nI created a canva and then drawed an image object containing this \"uri field value\" as source.\nI succeeded to connect to the topic because my handler is activated but i cant display the picture. Somebody already did something similar?", "Benoit"], "answer": [" ", " ", "Have you considered trying to use mjpeg_sever to stream images to your web page? This package takes advantage of browser optimizations by streaming ROS images in binary form. Take a look at the following wiki page which explains how to get it up and running:", "I second this recommendation for general in-page image use. Treating images special has been deprecated (and thanks to you removed from the documentation). If you really want to still handle images \"purely\" with JS and rosbridge, you should look at wsview.js in the scripts directory.", "Hello, \nTjay, I looked your script. Just to clear my mind, your img argument in the display function is the json_object received by a rosbridge handler?\nI found a code handling the mono8 encoding; \n/trunk/experimental/ROSDojo", "The object the display method expects is indeed the \"raw\" JSON object. Also, make sure to take a look at the rgb8 version. It's in the scripts directory of rosbridge itself.", "OK It works !!!! I will try the mpeg_server because this solution is quite power consuming. (50-60% of the CPU if i ask for a topic at 200Hz).  ", "Oh yeah. It's pretty terrible. Again, I recommend mjpeg_server unless you can get away with a tiny low framerate stream. I've had good luck with a 160x120 stream at 8HZ that I use the wsview.js script to upsize. It'll still hog your CPU more than a bit though.", "My picture are 7752*480 published at 30 Hz. Both strategy works and own quite high CPU consuming.(50 %)\nThanks you for your advices :)"], "url": "https://answers.ros.org/question/34462/rosbridge-image-uri-data/"},
{"title": "publishing and subscribing Gyro msgs in a Turtlebot", "time": "2011-12-07 20:16:11 -0600", "post_content": [" ", " ", "Hello, ", "Does anybody know how exactly the Gyro messages (imu/data; imu/raw) are subscribed/published in the turtle-bot node? ", "I have an ICreate Robot that doesn't have an inbuilt Gyro..But when I launch the turtlebot dashboard, it displays some random values in place of Gyro messages! I am not sure why that happens! I am trying to use an external Gyroscope (it publishes IMU messages) to publish the Gyro messages instead of the default ones published from the turtlebot node... Can anyone tell me how to go about doing it? ", "I realized a simple \"remapping\" of messages wouldn't work in this case!", "Would really appreciate any kind of help that you can provide!", "Thanks.\nDivya"], "answer": [" ", " ", " ", " ", "The TurtleBot power board has a gyro attached to the analog in. The create reads the data from the analog input and publishes it on imu/data. Set the has_gyro parameter to false and then it should not publish gyro data anymore. ", "To publish your own gyro data to ROS you need to either use an existing ROS driver for your gyro or write your own ROS drive to publish an ", " msg on the imu/data topic.", ", i suppose that the answer you provided is specifically for move_base roomba only? do you have any idea how can i do this for kobuki?", " please be more specific about what \"this\" is. All Kobuki's have the gyro so I'm not sure how this is relevant.", ", i have created a question for this @ ", ". please help.", " ", " ", "For those interested in adding a cheap angular rate gyro to their robot designs for helping with odometry calcs, here's some pseudo code for calculating the orientation that might help:", " ", " ", "Hi,\nI echoed the topic /imu/raw and found out that the z and w keeps on changing. When w increases the z decreases. I kept the turtlebot at the same position , w decreases and then after some time it started increasing .", "The data from the gyro was never stable. ", "Is it normal or is there something wrong in the gyro?", " ", " ", "This is an answer to ", ".  Gyro data will drift. I found this link to be quite helpful. ", " ", " ", " Thanks a lot for the link. ", "I forgot to mention one thing the z and w under the orientation heading is unstable.", "It starts decreasing from 0.999 to 0.00 and the again goes up.", "Is this normal?", "When i publish the analog input it is stable at 514 . So i think the gyro is working fine. "], "answer_code": ["calibration = [] // array for building calibration offset\nloop() {\n  gyro_adc = get_gyro_value_from_analog()\n  if (robotIsNotMoving) {\n      calibration.push_back(gyro_adc)\n      total = 0;\n      for each( float reading, calibration )\n      {\n        total += reading;\n      }\n      cal_offset = total / calibration.size()\n  }\n\n  this_reading_time = get_current_time()\n  dt = this_reading_time - last_reading_time\n  max_value = 255; // 8 bit ADC \n  v_ref = 5; // arduino is a 5V ref\n  zero_rate_v = cal_offset * v_ref / max_value\n  sensitivity = 0.013 // get from gyro data sheet\n\n  rate = (gyro_adc * v_ref / max_value - zero_rate_v) / sensitivity\n\n  orientation += rate * dt\n  // do something with orientation (put it in an IMU msg, etc.)\n  // ...\n  last_reading_time = this_reading_time\n}\n"], "url": "https://answers.ros.org/question/12267/publishing-and-subscribing-gyro-msgs-in-a-turtlebot/"},
{"title": "Turtlebot dying when launching kinect", "time": "2012-05-17 09:59:26 -0600", "post_content": [" ", " ", " ", " ", "Everytime I try to launch the kinect it gets stuck after setting the depth frame id and then the power system dies shortly after.", "turtlebot@turtlebot:~$ roslaunch turtlebot_bringup kinect.launch\n... logging to /home/turtlebot/.ros/log/e892ee8a-a055-11e1-97ca-9cb70d1ffd03/roslaunch-turtlebot-17123.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS\n * /kinect_laser/max_height\n * /openni_camera/depth_rgb_rotation\n * /rosdistro\n * /openni_camera/image_mode\n * /openni_camera/image_time_offset\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /openni_camera/depth_mode\n * /kinect_laser_narrow/max_height\n * /openni_camera/depth_frame_id\n * /openni_camera/depth_time_offset\n * /kinect_laser/output_frame_id\n * /rosversion\n * /openni_camera/depth_registration\n * /openni_camera/debayering\n * /openni_camera/depth_rgb_translation\n * /kinect_laser/min_height\n * /pointcloud_throttle/max_rate\n * /openni_camera/shift_offset\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id", "NODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)", "ROS_MASTER_URI=", "core service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [17144]\nprocess[openni_manager-2]: started with pid [17145]\nprocess[openni_camera-3]: started with pid [17146]\nprocess[pointcloud_throttle-4]: started with pid [17147]\nprocess[kinect_laser-5]: started with pid [17150]\nprocess[kinect_laser_narrow-6]: started with pid [17159]\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/e892ee8a-a055-11e1-97ca-9cb70d1ffd03/kinect_breaker_enabler-1*.log\n--GETS STUCK HERE and then the power system goes down (Green led on iCreate cable goes out)--"], "answer": [" ", " ", "Well the stuck issue was simply me not realizing that the program was just running. The power down issue is that I swapped to a new laptop_battery.py and it seems to not show the correct charge for the iCreate battery now. Will have to just keep tabs on charging it until I have time to look into it."], "url": "https://answers.ros.org/question/34239/turtlebot-dying-when-launching-kinect/"},
{"title": "Gazebo real and simulation time", "time": "2012-06-05 02:30:32 -0600", "post_content": [" ", " ", "Hello,", "As running gazebo, I noticed the lower toolbar with RMS Error, xReal Time, Real time, etc. I'm wondering how exactly are computed all this times and why, when I'm roslaunching lots of nodes, xReal Time is decreasing (as the simulator is slower and the computer also) ?", "If I missed some doc about that, please link."], "answer": [" ", " ", "In your ", " file, you can specify the desired update rate for Gazebo using the ", " tag. ", "The value of ", " has three configurations:", "UPDATE_RATE < 0: Gazebo will attempt to run the simulation in real time", "UPDATE_RATE = 0: Gazebo will attempt to run the simulation as fast as possible", "UPDATE_RATE > 0: This number will represent the desired physics update frequency. In this case, Gazebo will run STEP_TIME*UPDATE_RATE times reality.", "If Gazebo is not able to update at your desired rate, it will update as fast as it can. It's based on the system's computing power, so more nodes = less CPU time for Gazebo. ", "Thanks for answering. Is there a default value if the tag isn't defined (if real time is 1.0, I think the default value is negative) ?", "I don't know what the default value is. Gazebo always runs at sub-real-time for me (about 0.75 real-time). ", "Okey, I'm gonna experience some values to check that. Thanks for help ! :)", "Has this been tested to work in Fuerte? I am setting the parameter as you suggest but with no luck (the simulation in my case is running too fast, I would like real time)", "This is for the Electric version of Gazebo. Fuerte migrated to Gazebo 1.0. I don't know how to do this in that version"], "answer_code": [".world", "<updateRate>", "<physics:ode>\n   ...\n   <stepTime>STEP_TIME</stepTime>\n   <updateRate>UPDATE_RATE</updateRate>\n   ...\n</physics:ode>\n", "<updateRate>"], "url": "https://answers.ros.org/question/35699/gazebo-real-and-simulation-time/"},
{"title": "\"No devices connected...\" error with Turtlebot", "time": "2012-05-17 08:57:19 -0600", "post_content": [" ", " ", "I am getting a no devices connected error when attempting to launch the kinect. I have turtlebot dashboard up and running in full mode with 0 breaker green. My Kinect is plugged in as well and it looks like the other components are coming on except for the camera. I have also tried \"roslaunch openni_camera openni_node.launch\".", "turtlebot@turtlebot:~$ roslaunch turtlebot_bringup kinect.launch\n... logging to /home/turtlebot/.ros/log/30c60d6e-a050-11e1-9212-9cb70d1ffd03/roslaunch-turtlebot-5253.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS\n * /kinect_laser/max_height\n * /openni_camera/depth_rgb_rotation\n * /rosdistro\n * /openni_camera/image_mode\n * /openni_camera/image_time_offset\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /openni_camera/depth_mode\n * /kinect_laser_narrow/max_height\n * /openni_camera/depth_frame_id\n * /openni_camera/depth_time_offset\n * /kinect_laser/output_frame_id\n * /rosversion\n * /openni_camera/depth_registration\n * /openni_camera/debayering\n * /openni_camera/depth_rgb_translation\n * /kinect_laser/min_height\n * /pointcloud_throttle/max_rate\n * /openni_camera/shift_offset\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id", "NODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)", "ROS_MASTER_URI=", "core service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [5273]\nprocess[openni_manager-2]: started with pid [5274]\nprocess[openni_camera-3]: started with pid [5275]\nprocess[pointcloud_throttle-4]: started with pid [5276]\nprocess[kinect_laser-5]: started with pid [5277]\nprocess[kinect_laser_narrow-6]: started with pid [5292]\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/30c60d6e-a050-11e1-9212-9cb70d1ffd03/kinect_breaker_enabler-1*.log", "SOLVED - The power cable down in the bot had wiggled loose.", "Please post the solution as an \"answer\" and then accept it once you are able to (in perhaps a week or so I believe it might let you). This will let others easily see that the question has a solution and find the correct solution. Thanks for helping to keep ROS Answers organized and easy to use!"], "answer": [" ", " ", "The power cable down in the bot had wiggled loose.", "Eric - Thanks for notifying me! I am still getting used to the forum."], "url": "https://answers.ros.org/question/34233/no-devices-connected-error-with-turtlebot/"},
{"title": "Will ROS be able to run on the Raspberry Pi? [closed]", "time": "2011-10-04 09:00:10 -0600", "post_content": [" ", " ", " ", " ", "Will ROS be able to run on the ", "?\nIt uses an ARM1176JZF-S core.\nThe provisional hardware specification is ", ".\nDebian, Fedora and Archlinux will be supported at launch.\nCost is expected to be very low."], "answer": [" ", " ", " ", " ", "Given the specs and that it can run some forms of Linux, I would expect that some level of ROS software can run on the Raspberry Pi. I expect you would be able to run most of the core ROS packages (such as the communications or even some more math heavy packages such as tf at slower rates) pretty well.", "If you try to run complex software such as navigation, the single core will probably start to have trouble keeping up with everything (not to mention any slowdowns due to things like Eigen probably not being very well optimized for ARM11 compared to x86 or x86-64). It may be possible to run something like navigation with appropriate configuration tweaking for the slower CPU.", "All that said, for the cost of an Arduino, you are definitely getting a lot more computing power so it definitely has possibilities.", " ", " ", "Yes - I'm running it now. ", "All the basics seem to be working and TurtleSim's not too bad. ", "Not tried the navigation stuff yet - might be able to have a bunch of Pi's running together each with a bit each :)", "M", "Can you document somewhere what you needed to do to get it running? Maybe just make a \"question\" here on ", " on \"how to install ROS on PI\" or something like that and then answer it.", "Hi Kevin - will do. Later today or tomorrow. Cheers Mark", " ", " ", "The core of ROS does run on ARM based processors.  ", " ", "I know that people run ROS on Gumstix, PandaBoards and BeagleBoards.  Usually on top of Linaro or Ubuntu.  If you use Ubuntu you can use the binary debs linked above, otherwise you can use the compile from source installation method. ", " "], "url": "https://answers.ros.org/question/11406/will-ros-be-able-to-run-on-the-raspberry-pi/"},
{"title": "Openni_Launch on Lion with Xtion", "time": "2012-05-22 05:09:39 -0600", "post_content": [" ", " ", " ", " ", "I'm just returning to ROS after some time.\nHow do you get openni_launch installed in OSX?", "I've been able to consistently get ROS Fuerte installed on Lion. \nusing: ", "I've been able to get VirtualBox with Ubuntu and Fuerte up and running on Lion out of the box.", "I've been able to dual boot and get Fuerte and the Xtion with everything working in Ubuntu on the Macbook Air.", "But what I want is to get At Least OpenNI_Launch and Openni_camera etc up and running on Lion. I can run roscore, and everything is fine. But Openni_Launch is not present in OSX and obviously apt-get is not either. ", "How do I install Openni_Launch in an OSX environment?\nI found: ", "But it seems to be tailored specifically for the kinect even going as far as swapping out usb drivers...", "When I get VirtualBox running on Lion with Ubuntu 64x, everything works except that the camera does not display data either in Rviz or RosTopic.", "running: ", "roslaunch openni_launch openni.launch", "does FIND the camera in the virtualbox vm of Ubuntu, and everything acts like it's about to work, but there is just no picture and on rostopic the camera node is just not displaying data?", "If I could get the camera to work in my VM OR if I could get Openni_Launch to work on Lion this would be sufficient for me. ", "I just want to utilize to objectiveC and the App store for ROS, so I need to be able to get this running on OSX.", "Any Suggestions?\nThoughts??", ">> does FIND the camera in the virtualbox vm of Ubuntu, and everything acts like it's about to work, but there is just no picture and on rostopic the camera node is just not displaying data?\n\nI have a similar problem on Ubuntu guest with Ubuntu host."], "answer": [" ", " ", " ", " ", "The work of Kevin Walchko you found at his github is trying to address this very problem.  I think it is related to problems with the openni_driver on OS X, which has been historically problematic, Kevin can probably comment better here.", "As for visualization in the VM, try switching the display to points from billboards of the PointCloud2 type.  For some reason several of the nvidia CG shader scripts do not work properly on Mac (either directly or via a virtual machine).  This is a known problem and it might be fixed in the future, but the current work around is to avoid billboards.  ", "There is no native obj-c client for ROS nor are there wrappers to the c++ ROS client at the moment, but there is a new special interest group (SIG) for exactly this: ", " ", " ", "Yes, my kinect work in github uses libfreenect and not OpenNI. I can't get OpenNI to work with ros correctly on OSX (tried many times and many ways). Also you cannot use kinect with VB ... doesn't work.", "What I am doing now is running kinect (using libfreenect) to produce depth images and point clouds ros understands. I am right now setting up (just like you did) a virtualbox with Linux and ROS-Fuerte so I can send (over the network) kinect data (from OSX) to virtualbox/Linux and work with RGBD-Slam, octomap, etc. ", "[kinect] -> [OSX/libfreenect] -> [Virtualbox/Linux]", "I have basically given up on an OSX only solution. For the past year, I have spent more time getting ros packages to work on OSX than actually doing any robotics work. I am almost to the point of abandoning OSX and just moving to Linux completely.", "Curious how the performance of Linux/Ros is on a macbook air. Also curious about your thoughts on the Xtion if you have any too.", "Good Luck!", "Linux/Ros on Macbook air works flawlessly. I recommend the tutorial here: ", "Please dont give up on OSX!\nWe just need to get OpenNI_Launch working on the Macbook!", "What success have you had getting OpenNI to port?", "I really really like the Xtion, It's flawless! The best thing about it is that it doesnt require external power; much better for robotics. The worst thing about it is that there are other projects which are Kinect Specific. ", "How can I attempt to install Openni_Launch into ros on OSX? Sorry I'm new.", "Is it: \"rosdep install openni_launch\" ?", " ", " ", "In an effort to be Thorough I've tried everything a couple of times.\nI'm able to see perfect output from the project ", "RGBD_preRelease_0021_osx", "which ran in tech news a few weeks back. This is an OPENNI Project. While recognizing that it's precompiled and prepackaged, It at least lets me know that it's possible to get OpenNI on the MAC Lion. ", "That being said: I've not had time to get OpenNI installed directly from OpenNI. When I installed RGBD I wrongly assumed it was the same as Installing OpenNI... I'll work on that next. ", "That said: Unless I know the process Kevin Walchko is undertaking for the Kinect, I'm not sure if I will be able to port any work I do for OpenNI for the Xtron to ROS. Can I get more information about how to get OPENNI_Launch Ros packages running? Id like ROS to get to the point where it is LOOKING for openni to be there... ", "In reference to your Direction with the VirtualBox solution, I've confirmed that it doesnt work. I've switched Billboard to Points, and every other option in the drop down list. I get nothing. More to the point I've gotten this to work in Ubuntu running on Metal, and not in the VM. I recognize this might be something others have tried, and given up on. But I wouldnt mind trying more things to get it to work, if anyone has any suggestions. ", "Below is the output of my roslaunch openni_launch openni.launch output.\nIt's worth noting that I connected the camera after running openni, THIS TIME... but not every time. I've tried connecting first as well.", "ros@ros-VirtualBox:~$ roslaunch openni_launch openni.launch \n  ... logging to\n  /home/ros/.ros/log/7c0eff3a-a4f9-11e1-b83d-080027a47124/roslaunch-ros-VirtualBox-1902.log\n  Checking log directory for disk usage.\n  This may take awhile. Press Ctrl-C to\n  interrupt Done checking log file disk\n  usage. Usage is <1GB.", "started roslaunch server\n  http://ros-VirtualBox:35053/", "PARAMETERS  *\n  /camera/depth/rectify_depth/interpolation\n  * /camera/depth_registered/rectify_depth/interpolation\n  * /camera/disparity_depth/max_range  * /camera/disparity_depth/min_range  *\n  /camera/disparity_depth_registered/max_range\n  * /camera/disparity_depth_registered/min_range\n  * /camera/driver/depth_camera_info_url  * /camera/driver/depth_frame_id  * /camera/driver/depth_registration  *\n  /camera/driver/device_id  *\n  /camera/driver/rgb_camera_info_url  *\n  /camera/driver/rgb_frame_id  *\n  /rosdistro  * /rosversion", "NODES   /camera/depth/\n      metric (nodelet/nodelet)\n      metric_rect (nodelet/nodelet)\n      points (nodelet/nodelet)\n      rectify_depth (nodelet/nodelet)   /camera/rgb/\n      debayer (nodelet/nodelet)\n      rectify_color (nodelet/nodelet)\n      rectify_mono (nodelet/nodelet)   /\n      camera_base_link (tf/static_transform_publisher)\n      camera_base_link1 (tf/static_transform_publisher)\n      camera_base_link2 (tf/static_transform_publisher)\n      camera_base_link3 (tf/static_transform_publisher)\n      camera_nodelet_manager (nodelet/nodelet)   /camera/\n      disparity_depth (nodelet/nodelet)\n      disparity_depth_registered (nodelet/nodelet)\n      driver (nodelet/nodelet)\n      points_xyzrgb_depth_rgb (nodelet/nodelet)\n      register_depth_rgb (nodelet/nodelet)   /camera/ir/\n      rectify_ir (nodelet/nodelet)   /camera/depth_registered/\n      metric (nodelet/nodelet)\n      metric_rect (nodelet/nodelet)\n      rectify_depth (nodelet/nodelet)", "ROS_MASTER_URI=", "core service [/rosout] found Exception\n  AttributeError:\n  AttributeError(\"'_DummyThread' object\n  has no attribute '_Thread__block'\",)\n  in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\">\n  ignored\n  process[camera_nodelet_manager-1]:\n  started with pid [1922] Exception\n  AttributeError:\n  AttributeError(\"'_DummyThread' object\n  has no attribute '_Thread__block'\",)\n  in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\">\n  ignored process[camera ...", "I forgot to mention that a rostopic for the camera yields no output. ", "Also worth noting: When I call rosrun rviz rviz, from the vm running in lion, I can see the camera turn on. You can see the little red emitter come on. So, it's definitely doing something. It seems like the openni_camera node just isnt able to tap into the Virtualized hardware data.", "Again, try changing the display type in rviz from billboards to points, if the red light is coming from the Kinect when you connect with rviz then it is likely working.  Also, you can do ", " to see if any data is coming over the wire.", "So, the red light I was referring to was the faint glow of the emitter. When I check RosTopic with the command you mention, it says: WARNING: topic [/camera/rgb/points] does not appear to be published yet", " ", " ", "In the Virtual Box Settings for my Ubuntu VM I can configure the USB and I see: ", "Name: PrimeSense Device [0001]", "Vender ID: 1D27", "Product ID:0600", "Revision: 0001", "Manufacturer:PrimeSense", "Product:PrimeSense Device", "Serial No.:", "Port: ", "Remote: NO", "Does anyone know if these should be changed for OpenNI_Launch to see the hardware?\nAlso... since everything is a file in Linux, can I just copy the file that my DualBoot gets and tweak it for the VM?"], "answer_code": ["rostopic hz /camera/rgb/points"], "url": "https://answers.ros.org/question/34571/openni_launch-on-lion-with-xtion/"},
{"title": "Multiple Point Grey Chameleon USB cameras unstable", "time": "2012-06-21 03:50:30 -0600", "post_content": [" ", " ", " ", " ", "Hello.", "This will be a rather fuzzy question and I apologize for that in advance.", "We have two Point Grey Chameleon (", ") cameras connected to our computer and we're using the camera1394 driver to publish them in ROS.", "We've been having instability problems with our setup which seem to be very difficult to debug and analyze.", "We know (now) that the USB 2.0 cameras aren't exactly the best option for robot vision and running two like the Chameleon (with a resolution of 1296x964) might be stretching the limit a little.", "However, the annoying thing is that we've been able to run the cameras without problems. It works often, but not nearly always. Sometimes we have \"rough patches\" where things behave unusually bad (tonight was one such night). But as I said, once the cameras are up, they seem to stay up as long as we don't kill the image_proc process. If, however, the image_proc process reports an error (segfaults), we seem to need to hard reboot the computer and cut off power to the cameras (they are powered through both USB and GPIO since USB power alone didn't seem to cope with two of these).", "So my question is basically whether anybody has experience or tips with running multiple (higher end) USB cameras through the camera1394 driver? Maybe even the Point Grey Chameleon cameras? What can we do to make this less error prone?", "I just found some code that seems to offer a driver for image_proc directly on top of the PGR SDK (", "), I'll try that out tomorrow but I'd like to hear if anybody has experience with that.", "Kind regards,\nStefan Freyr", "\nA bit more information about this.", "I just updated the firmware on both cameras. After that I did a little more structured testing, shutting down and cutting all power to both the computer and cameras. After booting up I tried launching both cameras and that worked. I killed that launch and tried again and got the segfault. Then I tried running each camera individually and that works fine. Then I tried running flycap (the Point Grey image viewer and configuration tool) and that works for each camera individually and (and here's the kicker) it also works to fire up two flycap instances and view both cameras simultaneously! After that I tried running both cameras from a ROS launch script but I get the segfault.", "I'm trying to get more information about the segfault but I can't find anything in the log files. Is there a way to make the log files more verbose for camera1394? Do I need to compile the camera1394 driver again and if so, how do I do that cleanly on a setup using the Ubuntu packages?", "This seems to be some sort of a pickle with the camera1394 since both cameras are ...", "You can turn on Debug logging using the standard rxconsole tool.", "If camera1394 segfaults, please get a stack backtrace and file a bug report at "], "answer": [" ", " ", "Ok this is really weird!", "I tried changing the order of the cameras in the launch file and that seems to do the trick. Now they both kick in even after killing and relaunching the launch file.", "I'm going to mark this as the answer in case it might help someone else but I'm really frustrated to not know exactly why this is happening! So if anyone has an idea, please post a comment and I'll try to verify it.", "Kind regards,\nStefan Freyr", "And to be really clear, it also matters which USB port you use for the cameras. There is no good way of figuring out what works best, you just have to use trial and error.", "In fact, I tried connecting one camera to a USB 2 plug and another to a USB 3 plug and lsusb reported that they were on different busses but launching the ROS camera stuff failed. I then tried other configurations and the only one that works is when they are on the same bus according to lsusb. grmpf", "I've seen reports of USB 2 cameras not working on USB 3.", ": We have the same issue with devices on separate USB 2 and USB 3 busses that is solved when they are on the same bus.  Of course for contention, we'd like to keep them on separate busses.  Did you ever have any correspondence with Point Grey about this? ", " sorry no we didn't. We never had a USB3 bus though so that may be a different problem. I seem to recall some mention of a fix in PointGray release notes recently concerning USB2 cameras on a USB3 bus.", " so my advice would be to: 1) upgrade to the latest PointGray driver, 2) use external power to the cameras through GPIO and 3) try to switch the order in which you have your cameras defined in your launch script.", " ", " ", " ", " ", "I have no experience using USB cameras with ", ". Some people ", " with them.", "With high-bandwidth cameras like yours, I would worry about USB 2.0 bus contention. Unlike IEEE-1394, USB isochronous bandwidth guarantees may be unreliable. ", "It might help if you can attach the cameras to separate USB controllers."], "answer_code": ["camera1394"], "url": "https://answers.ros.org/question/36974/multiple-point-grey-chameleon-usb-cameras-unstable/"},
{"title": "pre-requisite C programming knowledge for ROS?", "time": "2012-06-21 05:26:28 -0600", "post_content": [" ", " ", "Hi i am new to ROS. i want to know that if advance programming concpts in C programming(e.g inheritance etc) are essential to understand ROS?"], "answer": [" ", " ", "I don't want to sound too critical, but ROS uses C++, not C. The ROS API is strictly object-oriented in nature, so it's essential that you at least understand how to instantiate/use objects. ", "Here's my list of programming concepts that you should understand if you were to use C++ in ROS:", ": If you're going to code using ROSCpp, it's vital that you at least understand how to instantiate, handle, destroy, and pass objects. This applies to using Python with ROS as well. ", ": Any time that you use ROS's pluginlib features, you will be required to utilize interfaces to create your plugins. In terms of C++, this means inheriting from a base class and implementing virtual functions. So if you want to use ", ", you will need to understand inheritance. Inheritance also exists in Python, so it's a good concept to know either way. ", ": Based on your application, you might need to understand the concepts of real-time programming to achieve your goals. ", ": Boost is an incredibly powerful and helpful library, and ROS already includes it as a dependency, so you should learn how to use if you're going to use ROSCpp. ", " ", " ", "Basically you can also use Python as your programming language which i would recommend if you don't have any advanced programming skills in C++, Java or LISP. ", "Nevertheless, programming concepts like inheritance are important and i would strongly recommend to have a look at them regardless of which programming language you want to use..."], "url": "https://answers.ros.org/question/36986/pre-requisite-c-programming-knowledge-for-ros/"},
{"title": "Problems with Pelican control using asctec_drivers", "time": "2011-07-11 05:18:31 -0600", "post_content": [" ", " ", " ", " ", "Hi \nWe recently got the Pelican quadrotor, and I am trying to figure out how to interact with.", "I tried playign with the ros asctec_drivers and I was confused about a few things:", "when i run the asctec drivers using the command: roslaunch asctec_proc asctec_driver.launch \nalthough i do get the quadrotor status in the asctec_mon node, however I get neverending stream of error messages also which look like this:", "does anyone know what is causing it?", "2. How can I send waypoints to the pelican using this ros stack? I do not see any topics for it.", "Thank you."], "answer": [" ", " ", "Hi,", "yogi found a (at least partial) solution ", "For the waypoints, do you want to send GPS waypoints, or do you want to navigate the pelican based on some laser/camera slam or odometry? For the latter case, if you can provide position and yaw, you can have a look ", ".", " ", " ", "Did anyone solve this problem yet? I also have the similar problem with my Pelican. ", "After running the demo-autopilot-sensor on the desktop, it showing: \nPublishing LL_STATUS data \nPublishing IMU_CALCDATA data \nControl Disabled ", "I connect the Atom board with autopilot using direct link from LL Serial 0 of the Autopilot to UART1 of the Atom board. They both power on. ", "Thank you very much for your help.", " ", " ", "Hi Folks, me too am having thesame problem when I run the command:roslaunch asctec_proc asctec_driver.launch ", "Have you been able to figure out where the problem lies ?", " ", " ", "ive also faced the same problem and it was the ftdi chip conneting the imu to the quad, just getting a new one solved it", " ", " ", "I am also having the same problem. Anyone?", " ", " ", "Yogi,", "I am having a similar problem.  Were you ever able to resolve this issue?"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "ANT", " ", " ", " ", " "], "question_code": ["ROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[AutoPilot-1]: started with pid [8567]\nprocess[AsctecProc-2]: started with pid [8575]\n[ INFO] [1310403092.428431980]: Creating AutoPilot Interface\n[ INFO] [1310403092.467485696]: Initializing serial port...\n[ INFO] [1310403092.467751836]: Starting AsctecProc\n[ INFO] [1310403092.480610254]: Successfully connected to /dev/ttyUSB0, Baudrate 57600\n\n[ INFO] [1310403092.482211992]: Publishing LL_STATUS data\n[ INFO] [1310403092.483186079]: Publishing IMU_CALCDATA data\n[ INFO] [1310403092.483218031]: Control Disabled\n[ERROR] [1310403092.795496644]:   Unexpected Data: Flushing receive buffer\n[ERROR] [1310403092.808786791]:     Error Reading Packet Header: Resource temporarily unavailable\n[ERROR] [1310403092.808928489]:     Read (3): ;?\n[ERROR] [1310403092.809003484]:   Read failed\n[ERROR] [1310403092.835423407]:     Error Reading Packet Header: Resource temporarily unavailable\n[ERROR] [1310403092.835498263]:     Read (3): ??\n[ERROR] [1310403092.835534067]:   Read failed\n[ERROR] [1310403092.933538751]: Timeout: 0 bytes available 3 bytes requested\n[ERROR] [1310403092.954336371]:   Unexpected Data: Flushing receive buffer\n[ERROR] [1310403092.964756981]:     Error Reading Packet Header: Resource temporarily unavailable\n[ERROR] [1310403092.964830510]:     Read (3): \n[ERROR] [1310403092.964864269]:   Read failed\n..\n..\n..\n"], "url": "https://answers.ros.org/question/10563/problems-with-pelican-control-using-asctec_drivers/"},
{"title": "Gazebo dead slow on 8x2.7GHz cpu with a NVidia GT650m gpu", "time": "2012-06-13 05:05:40 -0600", "post_content": [" ", " ", " ", " ", "To simulate our robots super fast, hopefully near real time, I ordered a laptop with an 8-core Intel i7-3830QM CPU and a NVidia GT 650m GPU, but with a Intel HD4000 gpu on the motherboard too. I heard NVidia is reasonably well supported under linux nowadays, so NVidia instead of ATi.", "I installed a fresh Ubuntu 10.04 Lucid, and not all was well:\nLong story short, Gazebo runs dead slow this laptop, at 0.01x realtime and 7-8 fps. \nAlso, the Z-buffer is messed up, showing some further surfaces in front of nearer surfaces.\nAny clues on how to fix this, maybe where to get newer drivers for this card under linux etc?", "RViz segfaults directly, but that's something for a different question, I guess.", "EDIT: I did try the latest (the day before yesterday) NVidia drivers for this GPU but that lets gnome run in low graphics mode.", "EDIT2:\nRunning ", " gives this:", "network bottleneck?", "Its not the network, as Gazebo only runs locally and not with ros nodes over a network in this setup for now."], "answer": [" ", " ", "Does that laptop have Optimus? Did you check that the nvidia card is actually running and not the intel card is used?", "If the intel card is running you should switch to the nvidia card, if that is possible.", "Yep, it has Optimus, with a sticker promising great battery life and great performance. I don't care for the battery life and the performance isn't that great, to say the least.", "How do I check if the NVidia-card is being used? EDIT: added lshw output to original question", "you have to install bumblebee or ironhide, and before running anything you want to render with nvidia you have to put optirun. Like this optirun roslaunch ....", "Thanks, I'll try this ASAP.", "Some laptops allow you to turn off Optimus, allowing you to only use the nvidia card. I had terrible luck & stability problems using bumblebee/ironhide, and had a much better experience using a laptop that allowed me to turn off the optimus stuff. ", "We tried that already, but even the advanced settings aren't so advanced. There is nothing useful there.", "Have you been able to run gazebo normally already? My laptop is i7 2620 and with a nvidia gt550m and I can run gazebo with optirun but I had alot a problems but it's possible.", "Yes, I have been able to, but using the onboard Intel HD4000, which is supported under ubuntu 11.10. I havent had the time to try bumblebee, I'm leaving for RoboCup in Mexico tomorrow, lots of other prepping. \nGood to hear it is actually possible. That laptop is now a quadboot, with a working 10.04, a broken one, 11.10 that works and a 12.04 to try some stuff too. "], "question_details": [" ", " ", " ", " ", " ", " ", "pci:0\ndesc: PCI bridge\n*- display UNCLAIMED\n   desc: VGA controller\n   vendor: nVidia", "display,\n  desc: VGA controller\n  vendor: Intel\n  version: 09"], "question_code": ["lshw"], "url": "https://answers.ros.org/question/36430/gazebo-dead-slow-on-8x27ghz-cpu-with-a-nvidia-gt650m-gpu/"},
{"title": "Turtlebot rviz Camera & Point Cloud2 topics not available to choose", "time": "2012-04-24 18:09:28 -0600", "post_content": [" ", " ", " ", " ", "Following \"Visualizing Turtlebot Kinect Data\" tutorial, can add Camera and Point Cloud 2 (Yellow background), but Image Topic depth or rgb choices are not listed. Image window is black. Dashboard Diagnostic & Breaker 0 so Kinect is powered on-blinking green light and red laser is on.", "Workstation roswtf gives this:", "What should I check or edit?"], "answer": [" ", " ", "The problem was fixed after discovering there appears to have been a power problem with my iCreate base. Powering the Kinect with its provided AC supply (rather than the Turtlebot Gyro and Power Adapter) and manually running roscore and kinect.launch result in good Camera and PointCloud 2 operation.  ", " ", " ", "With kinect.launch running, doing rostopic list give a long list of /camera/rgb/image_xxx, /kinect_laser/xxx, /openni_camera/xxx topics. Global Status is OK, Fixed Frame & Target Frame are to /base_link. Adding a Robot shows the small icon &  can apply a grid. Still no Camera or PountCloud2 topics to select. Can you suggest any further diagnostics to run? I can see the Kinect red laser & blinking green led. (and the Android Tab Teleop app works.  ", "Here's the roswtf\nERROR Communication with [/rviz_1335498109296442888] raised an error: ", "ERROR Communication with [/kinect_laser_narrow] raised an error: ", "ERROR Communication with [/openni_camera] raised an error: ", "ERROR Communication with [/master_sync_Herschel_27823_510701832] raised an error: ", "ERROR Communication with [/rosout] raised an error: ", "ERROR Communication with [/diagnostic_aggregator] raised an error: ", "ERROR Communication with [/openni_manager] raised an error: ", "ERROR Communication with [/robot_state_publisher] raised an error: ", "ERROR Communication with [/rviz_1335500211257772167] raised an error: ", "ERROR Communication with [/turtlebot_node] raised an error: ", "ERROR Communication with [/pointcloud_throttle] raised an error: ", "ERROR Communication with [/kinect_laser] raised an error: ", "ERROR Communication with [/app_manager] raised an error: ", "ERROR Communication with [/robot_pose_ekf] raised an error: ", "ERROR Communication with [/turtlebot_laptop_battery] raised an error: ", " ", " ", "Hi,\nI am hoping that you have followed the link ", "\nto do this.\nI guess the problem faced here is due to the communication between the turtlebot(netbook) and workstation.", "If teleop tutorial worked for you earlier to this then the communication may be fine.\nThis could be an issue as you are trying to transfer all the cloud (640x480) over wireless lan. ", "You may confirm where the problem is by directly visualizing the same on the turtlebot(netbook) itself having taken it out of the turtlebot rack and openning the closed lid of the laptop. If you are able to see the cloud in the rviz there. Then its for sure the rate of pointclouds being published from the netbook to the workstation.", "If you are not able to visualize the cloud in the netbook please try to echo the topic of the cloud. It may have to do with the configuration of the laptop you are using on the turtlebot as well.", "I see the above possibilities for this error. ", "Hope this helps,\nKarthik", "yes I am followinng that link. I ran rviz on Workstation and directly on the Netbook with the same result."], "question_code": ["Found 16 error(s).\n\nERROR Communication with [/robot_state_publisher] raised an error: \nERROR Communication with [/master_sync_Herschel_15710_1025976273] raised an error: \nERROR Communication with [/diagnostic_aggregator] raised an error: \nERROR Communication with [/kinect_laser_narrow] raised an error: \nERROR Communication with [/rviz_1335325216462728209] raised an error: \nERROR Communication with [/turtlebot_dashboard_3012_1335324855229] raised an error: \nERROR Communication with [/openni_manager] raised an error: \nERROR Communication with [/robot_pose_ekf] raised an error: \nERROR Communication with [/openni_camera] raised an error: \nERROR Communication with [/kinect_laser] raised an error: \nERROR Communication with [/pointcloud_throttle] raised an error: \nERROR Communication with [/rosout] raised an error: \nERROR Communication with [/turtlebot_laptop_battery] raised an error: \nERROR Communication with [/turtlebot_node] raised an error: \nERROR Communication with [/app_manager] raised an error: \nERROR Communication with [/turtlebot_dashboard_cpp_1335324855385011408] raised an error:\n"], "url": "https://answers.ros.org/question/32525/turtlebot-rviz-camera-point-cloud2-topics-not-available-to-choose/"},
{"title": "ROS_NXT Advanced Capabilites?", "time": "2012-06-25 16:45:38 -0600", "post_content": [" ", " ", " ", " ", "Does ROS for the NXT have the capability to do sophisticated statistical calculations such as matrix multiplication and inversion? I'm looking at using it to do some Qlearning algorithms.", "EDIT: Are the packages created in ROS not stored on the NXT then? Does your computer need to constantly be connected via bluetooth or USB in order to relay the package info to the brick?"], "answer": [" ", " ", " ", " ", "In short: as the ROS nodes for nxt don't run on your brick, you are free to use any code / library you want (as long as you can run it on your laptop/pc).", "Longer answer: The ", " package is 'nothing more' than a wrapper around the ", " Python library. This enables bidirectional communication between ROS and the NXT brick.", "As the rest of your ROS nodes \"don't care\" where their data is coming from or going to, you are free to use any other nodes, as long as their messages are compatible.", "This is actually one of the fundamental tenets of component based development: keep implementation specific details 'locked up' inside your components, while using suitable connectors to exchange the proper messages. Ideally, no knowledge about you using the NXT as an actuation platform is ever present in any of your nodes, apart from the ", " package (cum suis).", "Answer to the edit: yes, your computer/laptop will need to be running all the time. As I wrote, the ", " package is just a wrapper around a library, facilitating communication between your computer and the brick over usb/bluetooth.", "According to ", " page, the brick has a 48MHz ARM7 with 64KB ram. While perhaps ", " could be adapted to run on it, the brick is in no way powerful enough to run a full blown ROS.", "Exploiting this decoupledness: you could do all your ML in ", " or perhaps a DSL, provided it can be integrated with ROS."], "answer_code": ["nxt_ros", "nxt_ros", "nxt_ros"], "url": "https://answers.ros.org/question/37258/ros_nxt-advanced-capabilites/"},
{"title": "subscribe to image topic over WiFi", "time": "2012-06-19 08:25:28 -0600", "post_content": [" ", " ", " ", " ", "Hi guys,", "I have an Asctec Pelican here with a Ueye camera attached to it. We're trying to set it up such that we can subscribe to the topic from a remote PC, and view the video feed in real time. ", "We set up a local WiFi network, and experienced tons of latency and very low framerate. We verified that the images are published at the desired rate on the Asctec itself, so it seems like it's either Asctec or WiFi bandwidth that's causing the issue. ", "Has any of you tried this type of things before?", "Thanks,"], "answer": [" ", " ", "You should try the following:", "...with ", " the image topic you want to display. Using image transport decreases a lot the bandwith required by the streaming, but it requires CPU power to compress the data on your robot.", "The only other option is to stream ", " data: lower resolution, black and white only, etc.", "PS: yes, wifi router usually behaves very badly when they reach theirs limits. Freeze, bad performances, reboot, etc. So what you experience is probably an access point performance issue.", " ", " ", "Hi,", "I am now troubled by the same problem. Have you found a solution yet?", " ", " ", "You can check how much bandwidth your image topic is using by calling:", "In the past, I've noticed quite a bit of latency when using images over distributed ROS networks. One thing that you could try would be to compress the image using ", ". This might help a bit. "], "answer_code": ["rosrun image_view image_view image:=/camera/image _image_transport:=theora", "/camera/image", "rostopic bw /your_image_topic\n"], "url": "https://answers.ros.org/question/36829/subscribe-to-image-topic-over-wifi/"},
{"title": "PR2 sees ghost obstacles in tilting laser", "time": "2012-06-23 01:16:20 -0600", "post_content": [" ", " ", " ", " ", "A PR2 I've been working with a while ago has the problem that the tilting laser reports erroneous obstacles to the right and left of the robot, which leads to \"ghost obstacles\" and messes up navigation. The problem can be seen in the following screenshot:", "Note the straight lines on the ground to the right and left of the PR2. They seem to lie above the ground, so they are not caught by the ground removal. The further the points are from the robot, the higher above the ground they are. It looks as if the Hokuyo reported not a straight line, but a bent one.", "Did anybody else see this problem? Any idea what went wrong?", "Here's some more pictures:", "I don't have a solution but I can tell you that I've seen similar behavior on multiple other PR2s."], "answer": [" ", " ", " ", " ", "I've found the root of the problem. This seems to be a bug in ", ". I tried reproducing this in Gazebo to rule out hardware problems, and this is what I get when visualizing the ", " topic:", "The blue points on the side have an average height of 0.003 m, the red ones in the middle have an average height of -0.018 m. In Gazebo, all points are still removed by ", ", but on the real PR2 I've been working with only the red/yellow/green points in the middle are removed, and the blue ones remain. This is exactly what I am seeing.", "This is clearly a bug in the ", " from ", "; I tried removing everything from the filter chain except the ", ", and the problem is still there. The \"dent\" in the point clouds is not present in the original scans:", " It's not a bug, it's a feature. To disable it, set the parameter ", " of the ", " node to ", ", for example like this (in ", "):", " In Fuerte, the parameter is called ", ".", "This solved the problem on the real (not simulated) PR2.", "The sac_ground_removal node and the whole laser pipeline seems to be pretty out-dated. I guess the only node that hasn't been ported to PCL. In the parsec repo you can find a re-implementation (", "), package parsec_perception. In parsec_navigation you can find a launch file.", "Thank you Lorenz, I wasn't aware of that! For the moment I hope that I have found the solution, but if I have to dig deeper, it's good to know there's a better/cleaner alternative to the laser pipeline.", "According to the laser filter wiki ", ":_scan_to_cloud_filter_chain this \"offset feature\" seems useful for long distance, but not for short range. I'm wondering how you'll reconcile the two?", "of course, i understand the PR2 tilt laser isn't used primarily for long range navigation since it has the base laser. but I'm just wondering if only one tilt laser is sufficient to do both navigation and short range obstacle avoidance", ": I think if you're interested in that, it would be best to open a new question (link to this one if it's relevant).", " ", " ", "I've had this problem for a long time, even on flat surfaces. Many people have reported the same. There are two approaches you can follow:", "You can find one at:\n", " that I've copied from somewhere :-)", "Thanks, we'll try upping the z_threshold filtering. It's rather a workaround than a solution, but it should work. I don't think the pr2_local_nav parameters have anything to do with it, because the errors in navigation are an aftereffect of the incorrect laser readings.", " ", " ", "Check the NTP synchronization between the computers involved in the ROS graph. Sometimes this can cause problems when TF transforms pointcloud data from a quickly moving source.", "Thanks, we'll check the NTP synchronization too and report back here. This looks promising."], "answer_details": [" ", " ", " ", " ", "Use dynamic_reconfigure and change\nsome of the parameters (there's a\nz_threshold filtering one you can\nplay with)  ", "Copy some \"good\" set of parameters.", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["laser_filters", "/tilt_scan_shadow_filtered", "sac_ground_removal", "scan_to_cloud_filter_chain", "laser_filters", "LaserScanAngularBoundsFilter", "roslaunch pr2_gazebo pr2_empty_world.launch\nroslaunch pr2_2dnav_local pr2_2dnav.launch\n", "use_hack", "scan_to_cloud_filter_chain", "false", "pr2_navigation_perception/lasers_and_filters.xml", "<node pkg=\"laser_filters\" type=\"scan_to_cloud_filter_chain\" respawn=\"true\" machine=\"c2\" name=\"tilt_shadow_filter\">\n  ...\n  <param name=\"use_hack\" value=\"false\" />\n</node>\n", "incident_angle_correction"], "url": "https://answers.ros.org/question/37126/pr2-sees-ghost-obstacles-in-tilting-laser/"},
{"title": "Looking for a reference use of PoseWithCovariance: any pkg using it?", "time": "2012-03-30 23:45:40 -0600", "post_content": [" ", " ", " ", " ", "Hi all, ", " has a 6x6 covariance matrix which, according to ", ", corresponds to these variables in this order: (x, y, z, rotation about X axis, rotation about Y axis, rotation about Z axis). With rotation around fixed axes, not like in the \"common\" yaw/pitch/roll various conventions.", "I'm working in a pkg for covariance transformations (", ") so it would be great if I could ", " that output PoseWithCovariance at present. Checks would go in the package unit tests.", " I'm looking for some \"stable\" ROS package which computes covariances in 6D, so I can peek its source code and make sure I'm following the right convention. ", "Any recommendation?", "If there're none, I'll assume the 3x3 rotation matrix is as follows:", "R = R_z(yaw) * R_y(pitch) * R_x(roll)", "where matrices are in the ", " same order than the non-global axis, yaw-pitch-roll convention.", "PS: wouldn't it have made more sense to keep a 7x7 covariance for the unit quaternion form? In general, equations for uncertainty propagation in this form are far simpler than for Euler angles. Is there room for a PoseWith7x7CovarianceStamped yet?", "Just for the records: the numerical values of yaw/pitch/roll in the dynamic-axes (rotating) convention are exactly the same than those in the roll/pitch/yaw fixed-axes convention. So a conversion of covariance matrices between both formats becomes just a permutation. Hope it may help someone else."], "answer": [" ", " ", " ", " ", "AFAIK there is no ROS package which make the transform between two poses with uncertainty (perhaps ", " but I don't think so). Because of this the package pose_cov_ops seems unique. I belive that the MRPT ([[mrpt-ros-pkg]]) in which the pose_cov_ops package is based looks very powerful handling poses with uncertainty and ROS should take advantage of this fact. ", "I think that this package should be integrated in the generic ", " package as a plugin in a similar way that the kdl package was integrated (see ", "). Perhaps the tf2 package should be refactored to support more explicitly the inverse tranformation or at least the unary inverse transform. ", "\nDefinitely I think you can find what you are looking for in the source code of the package ", "Regards.", " ", " ", " ", " ", "uncertain_tf is an extension to tf that allows to maintain uncertainty in the translation and rotation of ", "if you want to add uncertainity to your odometry data in 6D you should check out robot_pose_ekf", "turtlebot and pr2 use uncertainity for their odom datas \ncheck out this example of turtlebot", " ", "here is another example from cwru robotics' package", " ", " ", " ", " ", "There is  ", " that might do what you are looking for.", "I don't locate in the package source code the gaussian error propagation (using Jacobians) produced by the nonlinear tranforms. The resulting uncertainty after a transform is represented by a pose cloud(particles) and not as a PoseWithCovariance. In such case it is not what Jos\u00e9 Luis is looking for."], "url": "https://answers.ros.org/question/30919/looking-for-a-reference-use-of-posewithcovariance-any-pkg-using-it/"},
{"title": "Error reading from SCI port. No data. Turtlebot disconnects when set Full Mode", "time": "2012-07-08 07:39:49 -0600", "post_content": [" ", " ", " ", " ", "Hi, there,", "I have recently upgraded to Fuerte on Ubuntu 12.04. I tried to build SLAM map by following ", " tutorial, but after I execute ", " I receive error - \"Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create.\""], "answer": [" ", " ", "if you have a DMM (multimeter) please measure the voltage across the battery to make sure that it is fully charged. ", "Similar: "], "question_code": ["roslaunch turtlebot_navigation gmapping_demo.launch", "turtlebot@turtlebot-1215N:~$ sudo service turtlebot start\nturtlebot start/running, process 4564\nturtlebot@turtlebot-1215N:~$ roslaunch turtlebot_navigation gmapping_demo.launch\n... logging to /home/turtlebot/.ros/log/0c768eee-c921-11e1-8666-485d60f51088/roslaunch-turtlebot-1215N-5175.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://192.168.1.6:41858/\n\nSUMMARY\n========\n\nPARAMETERS\n * /kinect_laser/max_height\n * /kinect_laser/min_height\n * /kinect_laser/output_frame_id\n * /kinect_laser_narrow/max_height\n * /kinect_laser_narrow/min_height\n * /kinect_laser_narrow/output_frame_id\n * /move_base/TrajectoryPlannerROS/acc_lim_th\n * /move_base/TrajectoryPlannerROS/acc_lim_x\n * /move_base/TrajectoryPlannerROS/acc_lim_y\n * /move_base/TrajectoryPlannerROS/dwa\n * /move_base/TrajectoryPlannerROS/goal_distance_bias\n * /move_base/TrajectoryPlannerROS/heading_lookahead\n * /move_base/TrajectoryPlannerROS/holonomic_robot\n * /move_base/TrajectoryPlannerROS/max_rotational_vel\n * /move_base/TrajectoryPlannerROS/max_vel_x\n * /move_base/TrajectoryPlannerROS/min_in_place_rotational_vel\n * /move_base/TrajectoryPlannerROS/min_vel_x\n * /move_base/TrajectoryPlannerROS/oscillation_reset_dist\n * /move_base/TrajectoryPlannerROS/path_distance_bias\n * /move_base/TrajectoryPlannerROS/sim_time\n * /move_base/TrajectoryPlannerROS/vtheta_samples\n * /move_base/TrajectoryPlannerROS/vx_samples\n * /move_base/TrajectoryPlannerROS/xy_goal_tolerance\n * /move_base/TrajectoryPlannerROS/yaw_goal_tolerance\n * /move_base/controller_frequency\n * /move_base/global_costmap/footprint\n * /move_base/global_costmap/footprint_padding\n * /move_base/global_costmap/global_frame\n * /move_base/global_costmap/inflation_radius\n * /move_base/global_costmap/observation_sources\n * /move_base/global_costmap/obstacle_range\n * /move_base/global_costmap/publish_frequency\n * /move_base/global_costmap/raytrace_range\n * /move_base/global_costmap/robot_base_frame\n * /move_base/global_costmap/scan/clearing\n * /move_base/global_costmap/scan/data_type\n * /move_base/global_costmap/scan/marking\n * /move_base/global_costmap/scan/topic\n * /move_base/global_costmap/static_map\n * /move_base/global_costmap/transform_tolerance\n * /move_base/global_costmap/update_frequency\n * /move_base/local_costmap/footprint\n * /move_base/local_costmap/footprint_padding\n * /move_base/local_costmap/global_frame\n * /move_base/local_costmap/height\n * /move_base/local_costmap/inflation_radius\n * /move_base/local_costmap/observation_sources\n * /move_base/local_costmap/obstacle_range\n * /move_base/local_costmap/publish_frequency\n * /move_base/local_costmap/raytrace_range\n * /move_base/local_costmap/resolution\n * /move_base/local_costmap/robot_base_frame\n * /move_base/local_costmap/rolling_window\n * /move_base/local_costmap/scan/clearing\n * /move_base/local_costmap/scan/data_type\n * /move_base/local_costmap/scan/marking\n * /move_base/local_costmap/scan/topic\n * /move_base/local_costmap/static_map\n * /move_base/local_costmap/transform_tolerance\n * /move_base/local_costmap/update_frequency\n * /move_base/local_costmap/width\n * /openni_launch/debayering\n * /openni_launch/depth_frame_id\n * /openni_launch/depth_mode\n * /openni_launch/depth_registration\n * /openni_launch/depth_time_offset\n * /openni_launch/image_mode\n * /openni_launch/image_time_offset\n * /openni_launch/rgb_frame_id\n * /pointcloud_throttle/max_rate\n * /rosdistro\n * /rosversion\n * /slam_gmapping/angularUpdate\n * /slam_gmapping/astep\n * /slam_gmapping/delta\n * /slam_gmapping/iterations\n * /slam_gmapping/kernelSize\n * /slam_gmapping/lasamplerange\n * /slam_gmapping/lasamplestep\n * /slam_gmapping/linearUpdate\n * /slam_gmapping/llsamplerange\n * /slam_gmapping/llsamplestep\n * /slam_gmapping/lsigma\n * /slam_gmapping/lskip\n * /slam_gmapping/lstep\n * /slam_gmapping/map_update_interval\n * /slam_gmapping/maxUrange\n * /slam_gmapping/odom_frame\n * /slam_gmapping/ogain\n * /slam_gmapping/particles\n * /slam_gmapping/resampleThreshold\n * /slam_gmapping/sigma\n * /slam_gmapping/srr\n * /slam_gmapping/srt\n * /slam_gmapping/str\n * /slam_gmapping/stt\n * /slam_gmapping/temporalUpdate\n * /slam_gmapping/xmax\n * /slam_gmapping/xmin\n * /slam_gmapping/ymax\n * /slam_gmapping/ymin\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n    move_base (move_base/move_base)\n    openni_launch (nodelet/nodelet)\n    openni_manager (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    slam_gmapping (gmapping/slam_gmapping)\n\nROS_MASTER_URI=http://192.168.1.6:11311\n\ncore service [/rosout] found\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[kinect_breaker_enabler-1]: started with pid [5204]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored\nprocess[openni_manager-2]: started with pid [5205]\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading' from '/usr/lib ..."], "url": "https://answers.ros.org/question/38199/error-reading-from-sci-port-no-data-turtlebot-disconnects-when-set-full-mode/"},
{"title": "arbotix_python arbotix servos not recognized", "time": "2012-07-13 12:52:06 -0600", "post_content": [" ", " ", "Hi,\nI have purchased an arbotix robocontroller but the servos are not being recognized when I run arbotix_python.\nI am able to successfully compile and upload the ros.pde firmware without error (although I did originally get complaints about that arduino not being on option for the controller that I had to work around). \nSo no errors of any kind, the board is being powered by an external LiPo, the servo lights up briefly when power is turned on, but again, when I run the arbotix_python, no servos are seen. Any suggestions would be appreciated as I have no errors to give me a lead on what to try next.", "Output:\nscott@wilson:~$ rosrun arbotix_python terminal.py /dev/ttyUSB0\nArbotiX Terminal --- Version 0.1\nCopyright 2011 Vanadium Labs LLC", "ls\n    .... .... .... .... .... .... .... .... .... \n    .... .... .... .... .... .... .... .... .... ", "Attention ", "!"], "answer": [" ", " ", "Have you set IDs for each of the servos? If you have multiple new servos attached, they each ship with ID 1, and none of them will be able to respond.", "That is exactly what I am attempting to do, one servo at a time. The plan being to run ls, see what the servo is currently set to, and them mv it to what I need it to be. The problem is, it cannot recognize what it is set to in the first place, so I can't move it to anything.", "So the core of the issue is, if I am getting no compile errors on the ros.pde and it uploads just fine, then why can't I identify what the current servo ID is when I only have a single servo attached a time? I have tried it with several known good servos and same problem, so not related to the servo", "Hi again Fergs, I may have found a clue. I re-ran the arbotix_python node. When I first power on the board. All lights blink on as expected on the FTDI cable and the Arbotix board.  But when I issue the ls command,the communication light stays on solid.", "Did you ever resolve this issue?  I am having the same problem.  Attention ", "did you guys solve the issue, I have same problem, "], "url": "https://answers.ros.org/question/38776/arbotix_python-arbotix-servos-not-recognized/"},
{"title": "openni_launch problem", "time": "2012-07-25 21:13:12 -0600", "post_content": [" ", " ", "I tried to use the openni_kinect. There is no errors or warnings when I installed openni_kinect as the instructions in ", " but when I tried to use openni_launch by typing", "roslaunch openni_launch openni.launch", "in the terminal, ", "It has many errors as follows:", "Found 23 error(s).", "ERROR Communication with [/camera/depth/metric] raised an error:\nERROR Communication with [/camera/driver] raised an error:\nERROR Communication with [/camera/ir/rectify_ir] raised an error:\nERROR Communication with [/camera_base_link] raised an error:\nERROR Communication with [/camera/depth/metric_rect] raised an error:\nERROR Communication with [/camera/register_depth_rgb] raised an error:\nERROR Communication with [/camera/depth_registered/metric_rect] raised an error:\nERROR Communication with [/camera/depth_registered/rectify_depth] raised an error:\nERROR Communication with [/camera/depth/rectify_depth] raised an error:\nERROR Communication with [/rosout] raised an error:\nERROR Communication with [/camera/rgb/rectify_mono] raised an error:\nERROR Communication with [/camera/depth_registered/metric] raised an error:\nERROR Communication with [/camera_nodelet_manager] raised an error:\nERROR Communication with [/camera/rgb/rectify_color] raised an error:\nERROR Communication with [/camera_base_link3] raised an error:\nERROR Communication with [/camera_base_link1] raised an error:\nERROR Communication with [/camera/points_xyzrgb_depth_rgb] raised an error:\nERROR Communication with [/camera_base_link2] raised an error:\nERROR Communication with [/camera/disparity_depth] raised an error:\nERROR Communication with [/camera/depth/points] raised an error:\nERROR Communication with [/camera/disparity_depth_registered] raised an error:\nERROR Communication with [/camera/rgb/debayer] raised an error:\nERROR The following nodes should be connected but aren't:\n * /camera/depth/metric_rect->/rosout (/rosout)\n * /camera/depth/rectify_depth->/rosout (/rosout)\n * /camera_base_link->/rosout (/rosout)\n * /camera_base_link1->/camera_nodelet_manager (/tf)\n * /camera/register_depth_rgb->/rosout (/rosout)\n * /camera/points_xyzrgb_depth_rgb->/rosout (/rosout)\n * /camera_base_link2->/rosout (/rosout)\n * /camera/disparity_depth_registered->/rosout (/rosout)\n * /camera/driver->/rosout (/rosout)\n * /camera/depth_registered/metric->/rosout (/rosout)\n * /camera/ir/rectify_ir->/rosout (/rosout)\n * /camera/depth/metric->/rosout (/rosout)\n * /camera_nodelet_manager->/rosout (/rosout)\n * /camera/rgb/rectify_color->/rosout (/rosout)\n * /camera/depth_registered/rectify_depth->/rosout (/rosout)\n * /camera_base_link1->/rosout (/rosout)\n * /camera/disparity_depth->/rosout (/rosout)\n * /camera_base_link->/camera_nodelet_manager (/tf)\n * /camera_base_link3->/camera_nodelet_manager (/tf)\n * /camera/depth/points->/rosout (/rosout)\n * /camera/depth_registered/metric_rect->/rosout (/rosout)\n * /camera_base_link3->/rosout (/rosout)\n * /camera_base_link2->/camera_nodelet_manager (/tf)\n * /camera/rgb/rectify_mono->/rosout (/rosout)\n * /camera/rgb/debayer->/rosout (/rosout)", "How to solve this problem?", "I highly appreciate your time and great help!"], "answer": [" ", " ", "I need some more info from you... Are you on a turtlebot? Are you using a kinect camera? Is it powered from the turtlebot? ", "Assuming you have the turtlebot with a kinect: I had some trouble when I plugged the kinect into the USB 2.0 ports. Try it int he USB 3.0. If that doesn't fix it, run this and post the output: ", "I was under the impression that the Kinect did not work with USB 3.0 at all... Did you possibly mean it the other way around?", "Nope. I've always used it through USB 3.0", "Interesting, I don't have USB 3.0 to test it on, but I have seen quite a few posts such as ", " and ", " that say otherwise. ", "Although ", " post has a comment that says with newer kernels it might be possible.  Regardless, I suppose it is a moot point until the @LiliMeng responds."], "answer_code": ["lsusb\n"], "url": "https://answers.ros.org/question/39852/openni_launch-problem/"},
{"title": "Dashboard is unble to check the laptop battey state", "time": "2012-07-15 21:06:36 -0600", "post_content": [" ", " ", " ", " ", "hi, im using turtlebot, when i run the dashboard i get this error: [WARN] [WallTime: 1342422151.301084] Unable to check laptop battery state. Exception: Value 12265 mW did not have \"A\" or \"mA\" .\nI'm using asus laptop.\ncan u please help me out.."], "answer": [" ", " ", "Hi,", "The Turtlebot dashboard displays battery info by parsing the contents of two files...", "1) /proc/acpi/battery/BAT0/info", "On my laptop, the contents of that file are... (note values may vary)", "present:                 yes\ndesign capacity:         4400 mAh\nlast full capacity:      4370 mAh\nbattery technology:      rechargeable\ndesign voltage:          11100 mV\ndesign capacity warning: 215 mAh\ndesign capacity low:     150 mAh\ncycle count:          0\ncapacity granularity 1:  65 mAh\ncapacity granularity 2:  4155 mAh\nmodel number:            AS09A41\nserial number:            2477\nbattery type:            LION\nOEM info:                SONY Corp", "2) /proc/acpi/battery/BAT0/state", "present:                 yes\ncapacity state:          ok\ncharging state:          charged\npresent rate:            0 mA\nremaining capacity:      4370 mAh\npresent voltage:         12493 mV", "If either of these files do not exist, or your battery reports charge in a different way, you will run into issues (until this is fixed). You should check out this answer and see if it applies to you:", "/"], "url": "https://answers.ros.org/question/38848/dashboard-is-unble-to-check-the-laptop-battey-state/"},
{"title": "no device connected turtlebot kinect [closed]", "time": "2012-07-16 23:44:48 -0600", "post_content": [" ", " ", " ", " ", "Im trying to run kinect but it gives the massage no device connected ... im pretty sure that the power works well and there is no problem with that..can u please help me out?", "We'll need some more information in order to help you... ", "How are you trying to run the Kinect?", "every thing was fine until i tried to run rviz . when i run rviz a message came that its failed to load nodelet [openni_camera] so i thought maybe the drivers have not been installed so i did the instruction for openni_camera in ros website, i follow them all but at the stage that i wanted to start-", "start kinect.launch it gives the msg no device connected "], "answer": [" ", " ", "You should also check that you have a file at \n/etc/udev/rules.d/55-primesense-usb.rules", "You may need to add the user running the launch file to the \"video\" group to get permissions on this device.", " ", " ", "doughbot01 : oic, maybe its a silly question, how to be member of video group?", "You need to use the command \"sudo adduser $USER video\". However, if everything was fine I'm not sure if that was the problem. What do you mean by \"everything was fine\" before that... you had received kinect images before then?", "no i haven't receive the image it was giving me the error unable to load nodelet [openni_camera]. but it didn't give the msg no device connected ,", "btw i already became a member but still i get this msg : [INFO] [1342591333.817740897]: [/openni_camera] No devices connected.... waiting for devices to be connected", "Ideas 1) Make sure you're on a USB2 port. I've heard USB3 doesn't work. 2) Verify Kinect lights are on or blinking, indicating power is ok. 3) Read here ", "/", "Also, did you buy a turtlebot pre-made, or you are building it yourself? I'm just curious about how you've wired up the 12V power ?", "i bought it pre-made", " ", " ", "Read your /var/log/messages after you plug in the kinect.  You may not have your kinect sensor drivers setup correctly.", " ", " ", " ", " ", "dougbot01: this is exactly what i get:", "turtlebot@turtlebot-laptop:~$ lsusb", "Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub", "Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub", "Bus 003 Device 002: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC", "Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub", "Bus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub", "Bus 001 Device 016: ID 045e:02ae Microsoft Corp. ", "Bus 001 Device 015: ID 045e:02ad Microsoft Corp. ", "Bus 001 Device 012: ID 045e:02b0 Microsoft Corp. ", "Bus 001 Device 011: ID 0409:005a NEC Corp. HighSpeed Hub", "Bus 001 Device 004: ID 13d3:5711 IMC Networks ", "Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "turtlebot@turtlebot-laptop:~$ roslaunch turtlebot_bringup kinect.launch", "... logging to /home/turtlebot/.ros/log/2cb0993e-d09c-11e1-ba91-94dbc938ebcd/roslaunch-turtlebot-laptop-13670.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.", "started roslaunch server ", "PARAMETERS\n * /kinect_laser/max_height\n * /openni_camera/depth_rgb_rotation\n * /rosdistro\n * /openni_camera/image_mode\n * /openni_camera/image_time_offset\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /openni_camera/depth_mode\n * /kinect_laser_narrow/max_height\n * /openni_camera/depth_frame_id\n * /openni_camera/depth_time_offset\n * /kinect_laser/output_frame_id\n * /rosversion\n * /openni_camera/depth_registration\n * /openni_camera/debayering\n * /openni_camera/depth_rgb_translation\n * /kinect_laser/min_height\n * /pointcloud_throttle/max_rate\n * /openni_camera/shift_offset\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id", "NODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)", "ROS_MASTER_URI=", "core service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [13688]\nprocess[openni_manager-2]: started with pid [13692]\nprocess[openni_camera-3]: started with pid [13693]\nprocess[pointcloud_throttle-4]: started with pid [13694]\nprocess[kinect_laser-5]: started with pid [13701]\nprocess[kinect_laser_narrow-6]: started with pid [13707]\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/2cb0993e-d09c-11e1-ba91-94dbc938ebcd/kinect_breaker_enabler-1*.log", "Ok, I have a guess now. Actually my \"turtlebot\" I built with an ASUS Xtion so maybe I never thought of this. The key here I think is the kinect_breaker_enabler. Check this website on turtlebot tutorials regarding the dashboard ", "You have to set the iRobot create to \"full mode\" to enable breaker 0, which will power the kinect. At least, I'm guessing this could be a problem, but the fact the process has exited is concerning", "dashboard working fine all lights are green with also 0 breaker and the kinect cabel light also is green", "after getting these msges which i showed u it start giving me the msg no device connected", "Sorry, but it's almost midnight here and I have to go to bed. Although this is a silly one, I have to try since you have the exact same error. Good luck! ", "/", "its ok thank u :)"], "url": "https://answers.ros.org/question/38991/no-device-connected-turtlebot-kinect/"},
{"title": "simplest robot with ROS", "time": "2012-08-13 08:06:06 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello,", "By using arduino serial, I have made a tracked vehicle move, read distance sensors, and turn a servo. Right now there is no encoders in the bot, and I understand this is a necessity. ", "What is the simplest robot stack that I can run with my setup? Right now I am working on teleoperation. And I am planning to maybe start from turtlebot sim, and maybe connect it so that turtlesim moves with the robot.", "I understand that my setup is really limited. I wanted to do robotics as a hobby, so I bought bunch of arduinos and sensors, then I quickly understood I can't do the things I want to with the arduino, and then i discovered ros.", "Any ideas/recomendations/help greatly appreciated.", "C.B.", "What, exactly, do you want to accomplish? That is, what's your end goal? Without some sense of where you want to go, this question is very vague.", "ok, for example, i want to go around, and if i sense something turn left. arduino_serial provides convenient way to communicate with hardware. where do we run this code? we have to make a ros stack???"], "answer": [" ", " ", "You should formulate some goals which you want to achieve and what kind of robot you want to create. A vacuum cleaning robot can also move randomly, then you don't need encoders. As encoders are quite expensive to buy you can also build them yourself like in this example: ", " ", " ", " ", "I think the simplest robots consist of 3 main components: the brain, the eyes, and the legs. In your case, you have the Arduino as the brain, your distance sensors are the eyes, and the servos are the legs. So it should be quite straight forward creating your first robot, with or without ROS.", "You might want to look at ", " for some of the simplest robots. Eg. ", " did not even have the kind of computational power we have now, yet was able to demonstrate simple obstacle avoidance (which is a good place to start in robotics)", "Of course, with ROS, you get to tap into the huge resource of higher level algorithms written by others. But just note the different requirements of each algorithm, some of which may require different sensors. For example, the Kinect has been used as a relatively cheap 3D sensor in the ", ", and also localization/mapping algorithms like ", "."], "url": "https://answers.ros.org/question/41279/simplest-robot-with-ros/"},
{"title": "RGBDSLAM display issues", "time": "2012-06-18 05:38:56 -0600", "post_content": [" ", " ", " ", " ", "Hi, I am using RGBDSLAM on a Ubuntu 11.10 machine, with ROS Electric installed.\nI am using a Kinect camera to get the video feed. When I open the rgbdslam GUI using:", "I can see the monochrome image and depth image, but on the top window, I am supposed to see the 'map', right? But what I see is basically the background wallpaper (the desktop) right through it. I am confused as to why this happens.", "I also ran rviz, and so far it does say that I am subscribed to a pointcloud2. However, even in rviz I do not get to see a 'map' or a video feed, just a black background screen as set by default.", "Can anyone help?", " : I couldn't find the use_glwidget parameter in any launchfile. Which launchfile were you talking about specifically? I am in ros/rgbdslam/launch directory.", "Have a look at the sample config: ", " - It should contain all possible parameter in their default configuration."], "answer": [" ", " ", "Sounds like OpenGL problems. \nEither fix these, or use rgbdslam without 3D display by setting the parameter \"use_glwidget\" to \"false\" (e.g. in a launchfile).", " ", " ", "I am at the same stage. \nUsing Kinect. \nUsing Ubuntu 11.10. running \nroslaunch rgbdslam kinect+rgbdslam.launch", "Couldn't see anything in the windows above. Only a black screen with origin lines red and green in the center. ", "Can Anyone help what is the problem?"], "url": "https://answers.ros.org/question/36717/rgbdslam-display-issues/"},
{"title": "P2OS teleop", "time": "2012-06-12 06:54:22 -0600", "post_content": [" ", " ", " ", " ", "I can't get the p2os to function with teleop. I have the p2os board running. When I launch the p2os teleop, it works, but doesn't move (I am aware you have to hold down a button and tried all of them). ", "thanks in advance, ", "-Hunter A. ", "On the pioneer: ", "The p2os successfully initializes. Now the joystick driver: ", "The joystick node is up. On the host computer, I did a topic list: ", "Echoing the joystick node, the following error is presented: ", "Could you please specify the commands you used and be specific on whether you use joystick or keyboard?"], "answer": [" ", " ", "The motor must be enabled with the p2os_dashboard. ", " ", " ", "Hi,\nThis is a general problem people face when they directly use the joystick drivers. \nI would suggest you to find out how the mapping of the joystick buttons done to give the cmd_vel commands. So test the joystick functions against the cmd_vel outputs using.", "and", "Please check the teleop_joy launch file as well for the same.\nThis ", " might be of some help to you to understand about the remapping of the buttons according to the joystick you are using.\nteleop using the keyboard works straight forward but its better to use joystick for the easy usage. ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi Park, I get the same problem with you. When I run the program $ roslaunch p2os_launch teleop_joy.launch, it works, but the pioneer doesn't move.  ", "Blockquote", "... logging to /home/wkk/.ros/log/038d02ea-db28-11e1-bbbc-b8ac6f0e8639/roslaunch-wkk-desktop-7050.log", "Checking log directory for disk usage. This may take awhile.", "Press Ctrl-C to interrupt", "Done checking log file disk usage. Usage is <1GB.", "started roslaunch server http://wkk-desktop:44108/", "SUMMARY", "========", "PARAMETERS", "/rosdistro", "/deadman_button", "/rosversion", "/axis_vw", "/run_button", "/axis_vy", "/axis_vx", "NODES\n  /\n    p2os_teleop (p2os_teleop/p2os_teleop)", "ROS_MASTER_URI=", "core service [/rosout] found", "process[p2os_teleop-1]: started with pid [7068]", "process[pioneer_joy_controller-2]: started with pid [7070]", "Blockquote", "Just like the pre-answers, I did a topic list on the host computer: ", "Blockquote", "wkk@wkk-desktop:~$ rostopic list", "/aio", "/battery_state", "/cmd_motor_state", "/cmd_vel", "/diagnostics", "/diagnostics_agg", "/dio", "/gripper_control", "/gripper_state", "/motor_state", "/pose", "/ptz_control", "/ptz_state", "/rosout", "/rosout_agg", "/sonar", "/tf", "Blockquote", "The problem is the same with you when I test \"rostopic echo /joy\" and \"rostopic echo /cmd_vel\": ", "\"rostopic echo /joy\" works well...but pioneer doesn't work..and \"rostopic echo /cmd_vel\" are blank when I push the joystick buttons. ", " ", " ", " ", " ", "Could you solve this problem? I have the same problems with you.\nI have Pioneer 3-at without built in computer and notebook connected with pioneer using serial to usb cable.", "I run the program (roscore, run p2os_dashboard, run p2os_drive, roslaunch telop_joy) and then enable the motor on the dashboard. rostopic lists are the same as you.", "\"rostopic echo /joy\" works well...but pioneer doesn't work..and \"rostopic echo /cmd_vel\" are blank when I push the joystick buttons. The robot just beeped at regular time period.", "I'd like to know ..what the problem is?"], "question_code": ["allenh1@hmt-lisa:~$ rosrun p2os_driver p2os \n[ INFO] [1339516647.279359983]: using serial port: [/dev/ttyUSB0]\n[ INFO] [1339516647.346416374]: P2OS connection opening serial port /dev/ttyUSB0...\n[ INFO] [1339516647.766984936]: SYNC0\n[ INFO] [1339516647.970936556]: turning off NONBLOCK mode...\n[ INFO] [1339516648.174984688]: SYNC1\n[ INFO] [1339516648.583028887]: SYNC2\n[ INFO] [1339516649.195027317]: Done.\n   Connected to Julie_2, a Pioneer P3DX\n[ INFO] [1339516651.631304804]: resetting raw positions\n", "allenh1@hmt-lisa:/opt/ros/diamondback/stacks/p2os/p2os_launch$ roslaunch teleop_joy.launch \n... logging to /home/allenh1/.ros/log/6ff3ea14-b4a6-11e1-ad58-0025648c1ab0/roslaunch-hmt-lisa-1125.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://129.59.89.250:54917/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro\n * /deadman_button\n * /rosversion\n * /axis_vw\n * /run_button\n * /axis_vy\n * /axis_vx\n\nNODES\n  /\n    p2os_teleop (p2os_teleop/p2os_teleop)\n    pioneer_joy_controller (joy/joy_node)\n\nROS_MASTER_URI=http://129.59.89.109:11311\n\ncore service [/rosout] found\nprocess[p2os_teleop-1]: started with pid [1134]\nprocess[pioneer_joy_controller-2]: started with pid [1135]\n[pioneer_joy_controller-2] process has finished cleanly.\nlog file: /home/allenh1/.ros/log/6ff3ea14-b4a6-11e1-ad58-0025648c1ab0/pioneer_joy_controller-2*.log\n^C[p2os_teleop-1] killing on exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\n", "allenh1@muri-pc7:~$ rostopic list\n/aio\n/base_controller/command\n/battery_state\n/cmd_motor_state\n/cmd_vel\n/diagnostics\n/dio\n/gripper_control\n/gripper_state\n/joy\n/motor_state\n/pose\n/ptz_control\n/ptz_state\n/rosout\n/rosout_agg\n/sonar\n/tf\n", "allenh1@muri-pc7:~$ rostopic echo joy\nERROR: Cannot load message class for [joy/Joy]. Are your messages built?\n"], "answer_code": ["rostopic echo /joy\n", "rostopic echo /cmd_vel\n", "pioneer_joy_controller (joy/joy_node)\n"], "url": "https://answers.ros.org/question/36330/p2os-teleop/"},
{"title": "Kinect modification kit for making turtlebot like robot?", "time": "2012-09-17 04:55:01 -0600", "post_content": [" ", " ", " ", " ", "Hello All,", "I am making the turtlebot from scratch since I already have the irobot create and the kinect sensor. I am confused about the kinect modification kit to power the kinect. How to make one? I have the kinect with an AC adapter. Do I need to modify the Adapter cable part? I cannot find any detailed information on how to make the cable. PS I am not using the Xbox kinect , but the educational kinect that windows sell. Please , if some one can provide detailed information would be helpful.", "Thanks\nAnkit"], "answer": [" ", " ", " ", " ", "I don't know about the windows kinect, but for the normal one it is quite easy.", "Just clip the cable that runs from the AC adapter to the Y-connector. There should be two cables inside (brown and white, if I remember correclty). Those should be powered by 12V from your robot.", "The best way to ensure that is the case and that the polarity is correct independent on cable colors, I'd suggest measuring the power on the clipped AC cable first.", "Also, there is a ", " that describes this.", "Thanks for your reply, in the detailed tutorial that you mentioned , they built a 12 V regulated power supply. I will use the turtlebot board with gyro, it says that the board generates regulated power for the kinect which draws power from the create. So do I still need to make the regulator? ", "If they provide 12V regulated power, then you can use those. I'm sure there are specs for the boards available. In that case, you only need to connect the kinect power cable with the correct plug.", "This question may also help: ", "/", "thanks a lot ryan .. that answered everything.. "], "url": "https://answers.ros.org/question/43959/kinect-modification-kit-for-making-turtlebot-like-robot/"},
{"title": "kinect and turtlebot and xbox", "time": "2012-09-13 15:51:44 -0600", "post_content": [" ", " ", " ", " ", "Hi\nIt seems if I want to DIY to put kinect purchased together with a xbox on turtlebox, I have to modify the original kinect which involves cutting the power cable of kinect. Can the modified kinect be still used with xbox? Anyone has done this?", "Thanks", "Jack"], "answer": [" ", " ", "Check out Pgs. 17-20 of our ", ". It assumes that you've got the AC power adapter for your Kinect (which IIRC is optional). If you've got the adapter, you can modify it in a way where you can put it back together (shown in the manual). If you don't have the adapter, you'll need to get one.", "Thanks Ryan. Very detailed instruction.", "The link provided does not work anymore. The TurtleBot hardware documentation is still available in this archive hosted by ROS:\n"], "url": "https://answers.ros.org/question/43830/kinect-and-turtlebot-and-xbox/"},
{"title": "adding rgb data into a point cloud?", "time": "2012-09-18 06:24:16 -0600", "post_content": [" ", " ", " ", " ", "Hi, i'm currently working on a single line ladar which we've managed to transform into a point cloud via rviz so now i want to add some rgb data from a camera onto this point cloud.", "assuming i have worked out which pixel to add to which point cloud point, is it possible to do this?", "thanks", "*edit:", "im using pointcloud and not pointcloud2 at the moment", "thanks for the answer, but we're currently using pointcloud 1, is it still possible to do this?", "Yes, by using the channels, but I'd recommed switching to pointcoud2, if possible.", "thanks, i'll do that in that case"], "answer": [" ", " ", "yes it is possible and quit common. Just check this ", " and you will find a description of the data type PointCloud2.", " ", " ", "I think that PointCloud2 type is not that super easy to use. I'd suggest you to go over the pcl::PointCloud<pcl::pointxyzrgb> cloud and then convert pcl point cloud back to PointCloud2 using  pcl::toROSMsg() method as suggested here: ", "Also if it is true that sensor_msg::PointCloud is easier to understand, it is still not any longer standard. PointCloud2 is the current standard in ROS. To use the PCL is of course an option, but it takes more load and computational power if you need to convert a lot in between ROS and PCL.", "I was not alluding to sensor_msg::PointCloud at all and fully agree with what you are saying."], "url": "https://answers.ros.org/question/44053/adding-rgb-data-into-a-point-cloud/"},
{"title": "turtlebot with one or two laptops", "time": "2012-09-28 10:54:07 -0600", "post_content": [" ", " ", " ", " ", "Hi, ", "I am new to ROS and Linux. Interested in getting a turtlebot. \nFrom turtlebot's website, it seems it already has a laptop directly connected to the turtlebot via a USB calbe.", "here is my questions. How is the turtlebot typically used?", "a) Do I need to have another laptop/desktop that is connected to the laptop on turtlebot via network, so that instructions can be transmitted from the laptop/desktop to the laptop on the turtlebot via the network?", "b) or can  I just use the laptop on top of the turtlebot, if I feel it is not inconvenient to unplug the laptop from turtlebot, write my program on the laptop, and then plug the laptop to the turtlebot?", "Thank you.", "Jack"], "answer": [" ", " ", "We have used many, many TurtleBots. :)", "When we are doing more basic development work with the TurtleBot (parameter tweaking, minor changes, running long-duration tests), the onboard netbook is sufficient. Whenever we need to do more detailed development work, a second computer is recommended. Two main reasons for this:", "As I said, once we're generally happy with the results and are just tweaking things, we'll usually do so on the netbook.", " ", " ", "I have no direct experience working with turtlebots, so judge my advice accordingly.", "But, I suspect you will find it much more convenient to use a separate (perhaps more powerful) system for development. That could be either a laptop or a desktop.", " ", " ", "Hi Ryan, ", "Thanks for answer. A related question. ", "I have a iRobot Create and a device called BAM (", ").  With the BAM installed,  I can  communicate with the Create as if it were directly connected to my desktop/laptop host with a serial cable. ", "In this case, judged from your experience, can I just use one desktop/laptop, installed with ROS,  with BAM on Create, so that to eliminate the need of another netbook on turtlebot, thus reduce the cost.", "Thank you\nJack ", "PS\nThe only problem I can think of is the data from Kinect. Is Kinect directly connected to the netbook on turtlebot or the data from Kinect can be wirelessly transmitted to the other desktop/laptop, if it can wirelessly, we may not need the  netbook, may we?", "That wouldn't be that convenient. You would need to find some way of plugging the power/gyro board into the BAM, since it usually plugs in where the BAM does. As well, you're correct in assuming that the Kinect normally plugs directly in to the netbook.", "You are right. Ryan. I also have a roo tooth which is almost the same as BAM  and except it does not use the plug BAM uses on the Create. Therefore, I can still use power/gyro board. Again, it is the kinect that I worroed. Does Kinect have to be connected into the netbook? Thanks.", "The Kinect needs to be able to get its data onto the ROS network in some fashion. As well, usually it's recommended to do some post-processing on whatever device the Kinect is directly plugged into, because sending raw point clouds over wireless at 30 Hz will eat up all of your network bandwidth.", "Thanks Ryan.that's what I suspect too, the bandwidth problem if choosing wireless. Have a great weekend."], "answer_details": ["Display/graphics: Properly displaying all of the data that the robot can provide alongside your source code is done best with a good monitor (or monitors).", "Comfort. Long periods of programming at knee height is uncomfortable. You can avoid this by unplugging the robot/uncoiling the cables, but it's nice to be able to rapidly iterate on coding and live tests without having to worry about putting the cables/netbook back together.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/44765/turtlebot-with-one-or-two-laptops/"},
{"title": "rviz: unsaved changes", "time": "2012-10-15 02:27:30 -0600", "post_content": [" ", " ", "Is it possible to somehow prevent rviz from asking to save unsaved changes?"], "answer": [" ", " ", " ", " ", "There is no direct way to do this, but fell free to write a patch :) You are not the only one annoyed be this popup.", "Good practise is not to close rviz at all, only after work with the robot. If you need to restart rviz for some reason, it means there is something wrong with your code or rviz itself (happens too). I use to close rviz from time to time when I am working on battery, it saves some power (opengl likes my cpu).", "As a workaround:\n - ctrl+s and alt+f4 to speed things up, or\n - alt+tab and ctrl+c, when you don't want changes to be saved", "I don't know of any better solution.", " ", " ", " ", " ", "I agree with ", " and your question, it would be nice to be able to turn off this feature.  Perhaps a patch is in order.", "My workaround is to always add ", " to my launch files, as well as a launch argument to not start it if I don't want.  Then I find that when I kill the launch process, ", " closes without asking to save, even if its settings have changed. "], "answer_code": ["rviz", "rviz"], "url": "https://answers.ros.org/question/45885/rviz-unsaved-changes/"},
{"title": "turtlebot.eu power board and kinect", "time": "2012-03-28 02:26:26 -0600", "post_content": [" ", " ", " ", " ", "Hey,", "I have a question regarding turtlebot.eu power board. I don't have any external battery so everything is power through roomba's battery. Kinect is connected on 12v dc power connector (spec at max 350mA) on the power board.", "Everything works normally. I can control roomba, get video feed from kinect cameras. But I do have a problem with pointcloud data. It starts completely normal. I am receiving pointcloud normally for approximately a minute. After that it just stops. No error messages, the image just freezes. Roomba works all the time withouth problem (teleop, dashboard). It seems like that the openni freezes. Is it possible that kinect doesn't get enought power, that I should use external battery? Or maybe is there a problem with openni kinect driver?", "The same problem is when I try to build a SLAM map. It works normally for about a minute then the laser topic stops posting. Since laser is build from pointcloud I think problems are connected.", "Also I notice, when I watch only IR stream from kinect, the IR light on kinect flashes slowly. It is not constantly lit. Is that normal? Like that the IR camera is switching between ON and OFF.", "Does anybody have the same problem? I start turtlebot with ", "and start openni with", "Then I run", "to get the image.", "Thank you for your help."], "answer": [" ", " ", "The problem was with the lack of power for Kinect. You have to connect extra battery to the turtlebot.eu power board. After that it worked normally."], "question_code": ["roslaunch turtlebot_bringup minimal.launch\n", "roslaunch openni_launch openni.launch\n", "rosrun rviz rviz\n"], "url": "https://answers.ros.org/question/30670/turtleboteu-power-board-and-kinect/"},
{"title": "Control Design - PID and Topic Publishing", "time": "2012-10-17 06:09:09 -0600", "post_content": [" ", " ", " ", " ", "I am using 2 phidgets high current motor controllers to operate 4-12VDC motors, using the joy package to interpret my saitek av8ter joystick, with axis scaling for velocity control. With the rxgraph depicting the joy node publishing to a teleop node publishing to a move node, thus commanding the phidget HC controller to move the desired motors with the desired velocity. The callback for the teleop node, that publishes to the move node, listens to the joy topic, interprets the axis change, scales and publishes to the move topic. What I am seeing is a humming of the 12VDC motors during the initial motion of the joystick in the desired direction until a proper velocity to rotate the motor in a sufficient manner. ", "Has anyone experienced this type of behavior with their VDC motors? Should I not be continuously publishing or subscribing to the topic responsible for velocity changes? Is this even considered a problem?", "You should only need to publish when you see a change from the joystick. Probably it's sending some command that is just enough to energise the motors (and thus make them hum) but not enough to overcome the initial inertia ", "Exactly. Thank you for the confirmation. An additional question is if the continual hum would cause any degradation of the motors in the long term."], "answer": [" ", " ", "You'll be wasting power for one thing. As for damage to the motors, no not realy. After all it's not like they're at stall current. ", "But I would suggest that you figure out how to keep them off when not in used."], "url": "https://answers.ros.org/question/46112/control-design-pid-and-topic-publishing/"},
{"title": "Openni_manager service call to manually load Kinect driver", "time": "2012-10-22 06:02:17 -0600", "post_content": [" ", " ", " ", " ", "Hey,", "I am trying (for days now) to manually control the nodelets that run my Kinect camera.", "My scenario is a highly automated robot (turtlebot), with the capability to switch off Kinect just before going onto loading dock. Since the Kinect is powered by the Rommba motor outputs, it is switched off as soon as the Roomba starts reloading.", "So I have to re-enable the Kinect stack after leaving the dock.", "The Openni_manager I am using to run Kinect, provides rosservices (/openni_manager/load_nodelet) in order to achieve this. \nIt is no problem to shut down a nodelet via service, but I am not able to put it back on properly.", "The rosservice call in my program", "loads the driver with an output that looks fine and exactly the same as when I start the nodelet with launch file, but after a few seconds (possibly the timeout declared in the particular /openni_manager/bond message for that nodelet) it is automatically unloaded by the openni_manager!", "What am I doing wrong?", "Hopefully somebody can give some advice.", "Cheers,\nJasper"], "answer": [" ", " ", "I am now using app_manager for this purpose and it is working quite good.\nIf somebody stumbles over the need of loading/unloading the kinect driver remotely (necessary when the kinect is unpowered during opereration) I can recommend this approach."], "question_code": ["load_nodelet_proxy(\"/openni_launch\",\"openni_camera/OpenNINodelet\",[],[], [\"load openni_camera/OpenNINodelet openni_manager\"], \"1\")\n"], "url": "https://answers.ros.org/question/46478/openni_manager-service-call-to-manually-load-kinect-driver/"},
{"title": "Can you use multiple Kinects with openni_launch?", "time": "2012-12-03 13:28:48 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "We are using two kinects on our project and use openni_launch to access them. One program uses openni_launch openni.launch device_id:=#1 camera:=kinect1 and the other uses openni_launch openni.launch device_id:=#2 camera:=kinect2. ", "Both of these programs work separately but when we try to run both in parallel, the second kinect to launch doesn't respond. Is there a way to access multiple kinects at the same time?", "Kind Regards,\nMartin "], "answer": [" ", " ", "Are your devices connected to the same USB hub? I had problems with that before. ", "Make sure that each Kinect is connected to an independent hub on the mother board.", "It appears that way from the outside. It's connected to a laptop and the two kinects are plugged in on opposing sides of the computer. Is there a way to check in the terminal? ", "You can take a look at the kernel messages when you plug in the Kinects. You should look for something like:", "[239890.490560] usb 1-1.3: new high speed USB device number 5 using ehci_hcd", "[239995.822980] usb 5-1.4: new high speed USB device number 3 using xhci_hcd", "If I am not mistaken, this means that one kinect is connected to the bus 1 and the other to bus 5", "Ahhh, yes. I checked out where the kinects were connected to and took this screenshot.(", ") I believe they are connected on a separate bus. Do you have any other ideas? Thanks for your help!", "mmmm yeah, they seem connected to separate buses. Are you trying to visualize both point clouds with rviz? If you are, remember to set a tf transform between both kinects, otherwise you will not be able to visualize the point clouds.", "Thanks, Martin. We discovered that they were connected to two separate buses. Now the two kinects can run on the laptop, slowly, ha ha! ", "Dear MartinW, I've a request to you. I am going to start a bachelor thesis where i've to work on multiple kinect whish has to supported by open source program. can you please write here how you prepare your pc so that you could connect both the kinects. where can i code to chage the functions or develop the system? I highly appriciate your reply. Thank you.", "Hello Asif, I think you just need to make sure the Kinects are running on separate buses and then you can just run openni_launch with the different device IDs to run multiple Kinects. You should be able to do this fairly easily, just make sure you have enough ports and processing power on your comp!"], "url": "https://answers.ros.org/question/49657/can-you-use-multiple-kinects-with-openni_launch/"},
{"title": "Best graphics hardware for large point clouds in rviz", "time": "2012-11-05 08:51:41 -0600", "post_content": [" ", " ", "Hi,", "I am putting together some desktop computers that will be running ubuntu 12.04. I know from experience that rviz gets very slow when displaying a very large point cloud (or a large number of cubes etc). What should I be looking for in terms of hardware to mitigate this problem?", "Is system memory, GPU memory or something else the bottleneck? How about specialized graphics cards such as the nvidia quadro cards (designed for CAD)?", "Thanks in advance,\nHugh", "How large is very large (both in terms of number of points and rough shape/area/volume)?"], "answer": [" ", " ", "Some people know more about this than I do. For good performance you need something that does 3D well on X11 and Linux.", "Various nVidia cards are often reported to work well. I am sure some others do, too, but have no experience to confirm it.", " ", " ", "I have a Lenovo W510 (Quad-core, 8GB RAM, SSD, nVidia Quadro FX 880M) and I can display clouds of 1/2 million to 1 million points without a problem. However, it does take a lot of power and going over 1 million might no be so smooth anymore. I noticed that it eventually becomes harder to move around the virtual scene with the load is too high."], "url": "https://answers.ros.org/question/47514/best-graphics-hardware-for-large-point-clouds-in-rviz/"},
{"title": "How to see (visualize) the robot orientation (Yaw) in rviz", "time": "2012-11-24 17:58:44 -0600", "post_content": [" ", " ", "Hello", "I need a help regarding a robot orientation. I made a node that calculate the orientation of the robot to some object in the map/ So I can get the Yaw from my transformation, but that is just a number. Is there any tool that can visualize that orientation??This the part of the code where I get the YAW", ".... some code..\n    while (node.ok())\n                {", "..... more code", "Any help??"], "answer": [" ", " ", "You can simply ", " for ", ". That way you not only get the orientation, but the full pose as a coordinate frame.", "I can do that through rviz??", "If the /base_link tf is being published (it seems like it is from your code) you can visualize it directly in rviz. Just make sure you choose a proper fixed frame in the global options, such as /map", "But that gives me a quaternion X Y Z W. So what that mean for the robot orientation??  I have for example a Yaw of 80 deg. So what the relation of that to the quaternion X Y Z W?", "The TF display in rviz will show you a coordinate frame (a red/green/blue thing as the ones in the link provided). If you are interested in the numerical value of the angle in degrees it won't help you, but then again if you only want that you don't need rviz. All it takes is a std::cout statement."], "question_code": ["            //Speed and Yaw calculation\n            tf::StampedTransform transform2;\n        if(speed_count==9)\n        {\n                try{\n                    listener.lookupTransform(\"/map\", \"/base_link\",  \n                                ros::Time(0), transform2);\n\n                }\n                    catch (tf::TransformException ex){};\n\n                vel = hypotenuse(xlast,ylast, transform2.getOrigin().x(),transform2.getOrigin().y());\n            if(vel<0.003){\n                yaw = tf::getYaw(transform2.getRotation());\n            }\n\n            speed_count = 0;\n            xlast = transform2.getOrigin().x(); ylast = transform2.getOrigin().y();\n        }\n        else speed_count++;\n"], "answer_code": ["/base_link"], "url": "https://answers.ros.org/question/49045/how-to-see-visualize-the-robot-orientation-yaw-in-rviz/"},
{"title": "RGBDSLAM Installation problem", "time": "2012-01-21 21:47:31 -0600", "post_content": [" ", " ", " ", " ", "Hi folks :-)", "Apologies for the long post but I have been trying for three days now to install, build and test RGBDSLAM without any success and am getting rather frustrated... surely it shouldn't be so hard :-)", "I am quite experienced and have followed the instructions laid out in ", " and ", " etc. to the letter and have tried all sorts of fixes suggested here, and on other sites, including starting from scratch with fresh Ubuntu installs twice, to no avail.", "So, PLEASE could someone run through a fresh install and provide up to date detailed instructions as I see I'm definitely not the first to have build problems.", "This is what Ive done, three times, following predominantly ", " thus far:", "1)  Installed absolutely clean Ubuntu 11.10 64bit version (confirmed with /etc/issue and the output of uname -m)", "2)  Installed ROS Electric as required as per ", " Commands entered in \"Terminal\" as follows:", "a.  sudo sh -c 'echo \"deb ", " oneiric main\" > /etc/apt/sources.list.d/ros-latest.list'", "b.  wget ", " -O - | sudo apt-key add \u2013", "c.  sudo apt-get update", "d.  sudo apt-get install ros-electric-desktop-full", "e.  echo \"source /opt/ros/electric/setup.bash\" >> ~/.bashrc", "f.  . ~/.bashrc", "3)  Installed QT4 (QT Creator) via Ubuntu Software Center", "4)  Setup RGBDSLAM again as per ", "a.  mkdir -p ~/ros", "b.  echo 'export ROS_PACKAGE_PATH=~/ros:$ROS_PACKAGE_PATH' >> ~/.bashrc", "c.  source ~/.bashrc", "d.  svn co ", "e.  svn co ", " ~/ros/g2o", "f.  rosmake --rosdep-install rgbdslam", "This immediately failed with:", "[ rosmake ] Packages requested are: ['rgbdslam']\n[ rosmake ] Logging to directory/home/mike/.ros/rosmake/rosmake_output-20120121-085503\n[ rosmake ] Expanded args ['rgbdslam'] to:[]\n[ rosmake ] WARNING: The following args could not be parsed as stacks or packages:\n['rgbdslam']\n[ rosmake ] ERROR: No arguments could be parsed into valid package or stack names.", "5) So I checked:", "a) sudo apt-get install libglew1.5-dev libdevil-dev libsuitesparse-dev\nNo problem", "b) Tried to fix .bashrc adding various combinations and orders of export and source commands (not detailed in instructions)", "...\n\" enable programmable completion features (you don't need to enable\n\" this, if it's already enabled in /etc/bash.bashrc and /etc/profile\n\" sources /etc/bash.bashrc).\nif [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n    . /etc/bash_completion\nfi", "export ROS_PACKAGE_PATH=~/ros:$ROS_PACKAGE_PATH\nsource /opt/ros/electric/setup.bash\n...", "c) Tried to fix electric setup.sh with other changes (also not detailed in instructions)\n", "\n...\n\"!/bin/sh", "export ROS_ROOT=/opt/ros/electric/ros\nexport PATH=${ROS_ROOT}/bin:${PATH}\nexport PYTHONPATH=${ROS_ROOT}/core/roslib/src:${PYTHONPATH}\nexport ROS_PACKAGE_PATH=/opt/ros/electric/stacks:/home/mike:/home/mike/ros/ros-pkg:~/ros_workspace:$ROS_PACKAGE_PATH\nif [ ! \"$ROS_MASTER_URI\" ] ; then export ROS_MASTER_URI=", " ; fi\n...", "Tried incrementally odd combos of ROS_PACKAGE_PATH - now you ..."], "answer": [" ", " ", "If you followed this steps literally I think the problem is 4d/e. This svn checkouts like this will end up in your home, not ~/ros and thus cannot be found in ROS_PACKAGE_PATH.", "So you should ", " before the ", " calls.", " ", " ", "I rebuilt the whole system, yet again, from scratch on a bootable Ubuntu 11.10 64bit partition and it worked imediately (tho I did have to install GSL independently again Felix).", "Now I think I have a more fundamental issue, perhaps with an easy, but expensive solution. I think my Intel I3 M330 dual 2.13GHz laptop, with 4GB ram, is simply not up to the task.", "The CPU is maxed out whilst Im capturing data and the resultant frame rate is very slow, which in turn is leading (I suspect) to greater deltas between analysed frames, less matching and ultimately, my generated maps are often VERY poor.", "(I have tried moving the camera very slowly but this only helped a little bit)", "Also, the 4GB fills up extremely quickly so I'm prettly much limited to small room maps.", "Thanks you all for your patience and superb advice. ", "A good question, but please ask it separately. This is a Q&A forum, not a discussion forum.", " ", " ", "For something as complex as this, I recommend ", " to manage your source checkouts. ", "There are some other hints ", ", which explains the difference between system dependencies and ROS package dependencies.", " ", " ", " ", " ", "Ahaaa, got the build and Kinect recognition going at last - did a completely fresh install and found I had to also install  ", " for the default build to complete: (sudo apt-get install gsl-bin; sudo apt-get install libgsl0-dev) and then ", " for the pixbuf requirement (sudo apt-get install gtk2-engines-pixbuf) before the GUI would launch.", "But now, RGBDSLAM is now building fine and is recognizing the Kinect on launch but unfortunately still ", ". It just says \"Waiting for monochrome, depth and motion\" information...", "The terminal displays:", "): creating collection for \"writeGnuplot\"\nbool g2o::HyperGraphActionLibrary::registerAction(g2o::HyperGraphElementAction", "\n...\nSo,what do I need to do now? :-)", "Do I need to install some sort of viewer or support package, or specific graphics drivers, or is running Ubuntu as a VM possibly causing problems (unlikely)? ", "So close, all help appreciated :-)", " ", " ", " ", " ", "I think Ive got the build going 100% (reordering calls in .bashrc seems to have worked) but am still stuck right at the last hurdle - I can even see the RGBDSLAM GUI - so am tantalizingly close :-)", "I installed the \"openni_kinect\" package as per ", " using the command \"", "\" - no problem - and have connected the Kinect.", "But still, whenever I launch RGBDSLAM with \"", "\" (or \"roslaunch openni_camera openni_node.launch\" I get the following error:", "This appears to be a common problem and I have tried:", "Killing XnSensorServer (with '", "') at different pre/post launch stages, to ensure its shut down, this had no effect.", "It was suggested somewhere else that I run '", "' to prevent an Ubuntu service from interfering - no luck.", "and tried to set the following in case it would help - but am not sure if I did so correctly:\n", "My USB setup looks fine I think:", "mike@ubuntu:~$ lsusb", "So, what could it be??? Have I forgotten to install something, or set something else somewhere???", "All help hugely appreciated, so close :-)", "Thanks, Mike", "P.S. This is the start of the output when I execute \"", "\"", " ", " ", " ", " ", "Thanks Joq, but surely this shouldn't be all that complex, or obscure, a process as, all I'm trying to do is to get RGBDSLAM working on a clean Ubuntu install. ", "i.e.:\n1) Setup clean OS (Ubuntu 11.10 64bit)\n2) Setup ROS (electric)\n3) Setup and build RGBDSLAM\n4) Setup Kinect drivers\n5) Run RGBDSLAM", "After loads of work, I'm finally up to stage 4, still without being able to contribute anything to further development of RGBDSLAM.", "With clear, tested, environment setup instructions there shouldn't be any missing packages. Can rosinstall help me out here?", "P.S. To install the Kinect drivers I simply ran '", "'. Perhaps I need to build them as per the Source Based Installation in ", "... any thoughts?", " ", " ", "Im having similar problems. Im not using VMWare. Has anyone been able to solve this issue?", "I keep getting:\n", "Welcome to ROS answers! Sorry, but this it not an answer. Please comment instead on the respective question/answer or create a new one!", " ", " ", "Im not using VMWare and I have the same problem. The rgbdslam gui opens up but nothing is displayed. the kinect is connected and powered correctly but the infrared never comes on and the led on the kinect just keeps flashing green. Im using ros-electric on the latest oneiric ubuntu.", "I keep getting:\n", "Has anyone been able to finalize a fix for this? Any help plz :S", "Sorry, but this it not an answer. Please comment instead on the respective question/answer or create a new one! Also, verify that your Kinect is working properly with ROS first."], "answer_code": ["cd ~/ros", "svn co", "rosinstall", "'[ INFO] [1327243304.290757975]: [/openni_node1] Number devices connected: 1\n'[ INFO] [1327243304.291324641]: [/openni_node1] 1. device on bus 001:10 is a Xbox NUI 'Camera (2ae) from Microsoft (45e) with serial id 'A00365807452048A'\n'[ INFO] [1327243304.293948521]: [/openni_node1] searching for device with index = 1\n'[ INFO] [1327243304.297442805]: [/openni_node1] **No matching device found.... waiting for 'devices**. Reason: openni_wrapper::OpenNIDevice::OpenNIDevice(xn::Context&, const 'xn::NodeInfo&, const xn::NodeInfo&, const xn::NodeInfo&, const xn::NodeInfo&) @ /tmp/buildd/ros-electric-openni-kinect-0.3.4/debian/ros-electric-openni-kinect/opt/ros/electric/stacks/openni_kinect/openni_camera/src/openni_device.cpp @ 61 : **creating depth generator failed. ,Reason: The network connection has been closed!**\n", "Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 002 Device 005: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nBus 002 Device 003: ID 0e0f:0002 VMware, Inc. Virtual USB Hub\nBus 001 Device 009: ID 045e:02ad Microsoft Corp. Xbox NUI Audio\nBus 001 Device 010: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\n", "auto-starting new master\nprocess[master]: started with pid [2082]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to c52eab7c-450c-11e1-b5c2-000c291b98c1\nprocess[rosout-1]: started with pid [2096]\nstarted core service [/rosout]\nprocess[openni_node1-2]: started with pid [2100]\nprocess[kinect_base_link-3]: started with pid [2102]\nprocess[kinect_base_link1-4]: started with pid [2104]\nprocess[kinect_base_link2-5]: started with pid [2106]\nprocess[kinect_base_link3-6]: started with pid [2108]\nprocess[rgbdslam-7]: started with pid [2147]\n**[rospack] opendir error [No such file or directory] while crawling /home/mike/ros/ros-pkg**\n[rospack] opendir error [No such file or directory] while crawling /home/mike/ros_workspace\nbool g2o::HyperGraphActionLibrary::registerAction(g2o::HyperGraphElementAction*): creating collection for \"writeGnuplot\"\nbool g2o::HyperGraphActionLibrary::registerAction ..."], "url": "https://answers.ros.org/question/12708/rgbdslam-installation-problem/"},
{"title": "Symbol 'subscribe' could not be resolved", "time": "2012-11-28 03:03:04 -0600", "post_content": [" ", " ", "Hello,", "I am trying to subscribe to a topic, namely nav_msgs::Odometry. \nsubodom = nh_.subscribe<nav_msgs::odometry> (\"RosAria/pose\", 1,  &SensorSonar::callback, this);\nStrangely I get this error message:\n", "Just one line bellow I subscurbe to another Topic without any problem.\n sub = nh_.subscribe <sensor_msgs::pointcloud>  (\"/RosAria/sonar\",1, &SensorSonar::callback, this);", "Any ideas?", "BR"], "answer": [" ", " ", "You are using the same callback for both subscribes. I would guess, that the parameter only fits one of those, so that the signature of the templated method does not match the given callback.", "Arg yes I changed it shortly after writing this question. I was too blind :-) Thanks!", "Glad you solved it. Such errors are really annoying... :) Could you please accept my answer, so that its obvious, that the question is finished? Thanks!", "Ops sorry. I clicked the green checkbox to the left. Is that all I have to do? :-)", "Yes, thats it. :) Thanks"], "url": "https://answers.ros.org/question/49319/symbol-subscribe-could-not-be-resolved/"},
{"title": "Turtlebot follower demo not following", "time": "2012-12-10 14:36:15 -0600", "post_content": [" ", " ", " ", " ", "Hey everyone, month long lurker first time poster here. I'm running ubuntu 10.04 ROS electric turtlebot on the lenovo x130e laptop.", "I've been working on the complete turtlebot kit for awhile now and I've come across an problem that just simply cannot solve. I'll do my best to explain it, so here it goes.", "For quite some time I've had an issue with the follower demo as well as the mapping demo. I believe I can relate the problem between the two as well. Basically whenever I run the follower demo everything loads up but then the turtle bot follows me (generally just a straight line, Similarly when ever I begin mapping with the mapping demo everything loads up correctly and I can visually see on rviz what the robot is seeing but when I begin to move. The robot stops scanning the environment and on the rviz screen just becomes a robot moving around without detecting/scanning anything.", "This was very confusing until I realised that when I ran this code I could personally see the kinect's red laser shining. However I found that after the robot sees me in the follower demo the red laser turns off, I observed the laser turning of during the mapping demo as well after beginning tele-operation or even if the robot is left stationary.", "I would like to point out that the follower demo reports no errors when the robot stops moving, and the mapping demo as far as I know doesn't signal any warning to the scanner/laser not working.", "Also if I simply run the kinect visualizer demo the kinect red laser stays on for the entire demo and I can have full control over that demo.", "Please let me know of any information that I could that could help.", "Thanks everyone,", "-Cody", "Hey tfoote I just had ago at your idea, I think ive had a look at the wheels, front bumper, all usb ports and serial connections especially wires connecting to the kinect. Is there anything else I could be missing?"], "answer": [" ", " ", "This sounds like a hardware issue, with the power being lost when the robot starts moving.  I suggest that you check all your cables and the power board are securely plugged in. And with the kinect powered on poke and prod at all the connectors to find if one of them or a cable is flaky.  "], "url": "https://answers.ros.org/question/50140/turtlebot-follower-demo-not-following/"},
{"title": "Is the Schunk powerball package working? Always get an error.", "time": "2012-11-08 21:24:12 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am trying to get my powerball up and running with a Peak USB to Can module.\nBut when I want to start the modules, I always get this error:  ", "Can someone tell me if this package is working or what I am doing wrong?"], "answer": [" ", " ", "The communication of the new ", " arm is based on CANOpen. The old m5api (Schunk Motion Protocol 5) is not supported!", "There is a very new canopen package https://github.com/ipa320/ipa_canopen (ipa_canopen). I've got the test commands running (ipa_canopen_core/tools/move_device) - still need to configure the rospackage for the SCHUNK arm. Device IDs are 3/4 (Cube1) 5/6 (Cube2), 7/8 (Cube3) and 12 for the Gripper (using WSG-50). The ipa_canopen package only supports PCAN (PEAK CAN) devices at the moment (we use a pcan-usb-can adapter).", "Hope this helps.", "I was told that the ROS wrapper should be available in the next days.", " ", " ", " ", " ", "The -207 error code stands for a CAN write error. This usually means that something is wrong with the driver configuration, the configuration file or with the communication's physical connection.", "As the error appeared in module 5, it should mean that the program was able to reset all the other 4 modules, so your driver configuration should be fine. So, it either means that your robot only has 4 modules or that there's something wrong with your fifth module.", "By something wrong, it could be the physical connection problem I said before: disconnected cables, power supply issues and so on. It could also be caused by a wrong moduleID configuration, in which you tried reaching a moduleID that isn't connected to the CAN Bus.", " ", " ", "Hi there,", "good to know that someone else is also working with a Powerball Arm.\nCould you please point me to the driver package that you are using?\nI can only find the simulation package...\nIs it based on CanOpen? Or CAN? Or the Schunk Motion Protocol?\nThanks!", "(sorry, this is no valid answer, but I could not find the \"commenting tool\")", "Sorry, have not stopped by in a long time ;) But ", " gave you a pretty good answer. I use this driver also and modified it to work with an ethernet to CAN adapter frpm VSCOM."], "question_code": ["initstring = PCAN:/dev/pcan32,500000\nPCanDevice successfully opened on /dev/pcan32\nInitializing pcan device ...\nPcanDevice, init ok\n[ INFO] [1352452510.964341600]: ...initializing powercubes not successful. error:         \nCould not reset module 5, m5api error code: -207\n"], "url": "https://answers.ros.org/question/47864/is-the-schunk-powerball-package-working-always-get-an-error/"},
{"title": "could ompl deal with mobile manipulation?", "time": "2012-11-11 15:09:11 -0600", "post_content": [" ", " ", " ", " ", "dear all, i've finished the object manipulation with the amazing stack arm_navigation. that is, now my robot can grasp objects from a table while the robot base is fixed. however, due to the limited workspace of the arm, the robot can only grasp objects within a small region. it cannot even cover the whole table. so i'm wondering that, if we can plan a location for the robot base while do the arm motion planning? for example, the robot goes to the other side of the table to grasp objects there? according to my understanding about RRT, this is merely an addition of 3 degrees in the searching space, that is, the x,y, and rotation of the base. can we do this with the current planning_description_configuration_wizard? or does anybody have any hit for this task?", "thanks in advance.", "EDIT:\ndear pirate, although you've pointed a direction for me, i'm still confused about the details. do you mean that i need to modify the file MyRobot_planning_description.yaml generated by the wizard? if so, how should i modify? \nin the beginning of MyRobot_planning_description.yaml, it is:", "as you said, i need to add a P-R-P or P-P-R joint, where should i add this information?", "besides, since i'm planning for the arm manipulation in the joint space instead of the task space, i need to first convert the goal pose to the joint space with IK. that means i must first determine the robot base's position, and then do the arm planning. in this way the whole planning is acturally split into two parts: first the planning for the base in SE(3), then the planning for the arm in the configuration space. however, i hope that the planner can treat the whole planning as one part.", "don't know if i've put myself clearly :-( but i would really appreciate it if you could provide more details about your realization.", "thanks again.", "Looks like an intern implemented a similar planner (new Darrt planner): "], "answer": [" ", " ", " ", " ", "Yes and no. ", "As far as I can tell, your understanding of the RRT planner is correct. You can add 3 DOF for the base in your URDF, e.g. prismatic/prismatic/rotational or rotational/prismatic/rotational, modify the arm_navigation config files (output of the wizard) and let the OMPL planner of your choice do its work.", "Actually, I did that and the result looks really fancy. :-) However, depending on your type of mobile base, the output plan may conflict with the constraints of your base. More specific, in my example I have a differential-drive base. The plan I get from the planner does not respect the \"non-holonomic/rolling without slipping/don't move orthogonal to the current direction\" constraint. The planner doesn't care about the transition from one point to the next. It only makes sure, each point its checking fulfils the constraints (e.g. joint limits, collisions).", "In case you have a holonomic base, e.g. omni-wheels, powered casters, mercantum wheels, you might be able to do something useful with the output of the planner.", "I think, ", " will be able to do you what you like to do. So, you could lean back and wait for the guys to do once again an awesome job, as they did with arm_navigation. :-)", "/edit to answer your edit:", "First, I modified my robot description (URDF) and added three fake joints (R-P-R) between a new \"manip_start\" frame/link and my robot's base_footprint link. Afterwards I built my custom arm_navigation from scratch using the arm_navigation_wizard. The only difference this time was, that I set the \"manip_start\" link as my new root (tip link stays the same). In this way the new fake joints get pulled in. Let me know how that goes.", "In my case, I made it work for 8 DOF (3 in the arm, 2 in the torso, 3 in the fake base).", "thanks so much for your quick and helpful answer. i will give that a try. thank god, my base is a holonomic one. i'm also looking forward to the release of moveit!", "dear pirate, i've edited my original post. could you kindly read it and give some hints for my new questions? thanks so much.", "Done. Hope this helps.", "thanks. seems things work fine for my 10 DOF robot(3DOF base+7DOF arm). the only problem is about trajectory filter. when i try to filter the trajectory produced by ompl, the process trajectory_filter_server dies with exit code -6. have you encountered this problem?", "I remember an issue like this (haven't been using this set-up for a while). What happened to me from time to time, is that move_arm was crashing, because the filter couldn't process the trajectory. Seems like a bug in move arm, since it shouldn't crash just because the service call wasn't successful", "I think it should just aboard the current request and then let you call the filter again or try a different trajectory.", "The move_arm issue ", " mentions is ticketed as ", ". Issue contains patch with fix, if you're interested.", "Sweet! Thanks! :-)", " ", " ", "Hi,", "I'm trying make a planning scenary with some attached objects on the table, and try to collect all of them and push into a \"trash\".", "I'm some confused about attached and non attached objects and how integrated it in the scenary. In adition, when I attach a object to the pr2 hand, I don't know how change the scenary for a new request ompl plan.", "Could you help me, with some notions or a \"template scheme\"", "Regards. ", "sorry, i've not used the attached object function. besides, you could ask a new question and see if there is anybody could help you.", "Sanxano, please don't ask new questions in answers, which do not help to solve the original question. Just create a new question yourself. Thanks."], "question_code": ["multi_dof_joints:\n  - name: world_joint\n    type: Fixed\n    parent_frame_id: base_link\n    child_frame_id: base_link\ngroups:\n  - name: left_arm\n    base_link: base_link\n    tip_link: left_link7\n"], "url": "https://answers.ros.org/question/48028/could-ompl-deal-with-mobile-manipulation/"},
{"title": "New to ROS, want to build a robot =)", "time": "2012-12-12 15:31:03 -0600", "post_content": [" ", " ", " ", " ", "I kinda want to imitate this robot: htxp://forums.trossenrobotics.com/showthread.php?4654-Maxwell-A-Mobile-Manipulator ", "But i want my base to be this one: htxp://www.dfrobot.com/index.php?route=product/product&path=37_111&product_id=63#.UMlFRbT3BT5 ", "So i been thinking to add a computer to it.. Will something like this work: htxp://www.newegg.com/Product/Product.aspx?Item=N82E16813121442 (i got ram/HD and will use this PSU: htxp://www.logicsupply.com/products/picopsu_80, i currently have this battery: htxp://www.batterymart.com/p-12v-7ah-sealed-lead-acid-battery.html )", "I want to use a xbox kinetic sensor which connects use to that MB. But i need to use a board to control the servos, ill probably use the arbotix board. ", "Should i worry about getting that MB, or just get a raspberry pi?", "sorry links have an x on them, coulnt post full links", "edit #1:", "Can you recommend a motherboard + CPU that would be good for this type of task? Thanks", "I guess i could go with something like this: htxp://www.newegg.com/Product/Product.aspx?Item=N82E16813186211 and i can pick alot of different types of cpus", "when i try to post links it says i dont have enough karma to post links.", "Please use formatting for links, code etc. Oh, and I don't know why you replaced http with htxp, because this will annoy both readers and the automatic linker macro.", "because it says i dont have enought karma"], "answer": [" ", " ", "I think that even that Atom D525 is going to be slightly under powered when it comes to mapping and controlling arm navigation. So the Raspberry Pi is definitely not going to be suitable for the tasks that Maxwell is capable of. ", "Since you are new to ROS, I would recommend first starting with something like Turtlebot. It has a fair amount of documentation and servo arm upgrades. As writing code for a custom base can become quite complex for the beginner. ", " ", " ", " ", " ", "thxs updated question", "Please read the ", " before posting on this site. Do not create answers for comments or updates. Instead edit your original post or use the comment functionality.", "Sorry didnt know"], "url": "https://answers.ros.org/question/50327/new-to-ros-want-to-build-a-robot/"},
{"title": "How to connect the kinetic sensor to my ROS computer", "time": "2012-12-28 13:08:41 -0600", "post_content": [" ", " ", "So i finally finish building up my robot, with a litle computer built in into it.. And i just purchased a Kinetic sensor but the connector wont fit on one of the USB ports, so im guessing ill need to purchase one of those adapters than convert that kinetic port into USB and power (http://www.amazon.com/Xbox-360-Kinect-Hyperkin-AC-Adapter/dp/B0057K045O/ref=sr_1_4?rps=1&ie=UTF8&qid=1356728403&sr=8-4&keywords=Xbox+360+Kinect+AC+Adapter%2F+Power+Supply)", "Do i need to use that power cable? or the USB cable is good enough, also is the kinetic sensor i got the right one? or they actually sell one that comes already with USB port??", "Thanks", "how is the ASUS Xtion PRO LIVE compared with the kinetc??? "], "answer": [" ", " ", "yes. you need the adapter shown in your mentioned link. and that adapter is enough. you don't need to buy other things.", "but do i have to use the power cable? If so i guess i have to cut the transformer it self and hook it to my robot battery...", "I recall there's a way of connecting a kinect to a Create robot's power supply. If you find a tutorial on how to do that it might be a starting point to use your bot's battery.", "You will have to cut the cable.  The USB port is not enough. ", " ", " ", "Yes you will need to use the power cable aswell and connect it to a 12 vdc power supply"], "url": "https://answers.ros.org/question/51181/how-to-connect-the-kinetic-sensor-to-my-ros-computer/"},
{"title": "Create Semantic Maps using RGBDSLAM + OctoMaps?", "time": "2012-07-01 02:52:17 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone!", "I'm interested in using RGBDSLAM and OctoMaps with the Kinect camera to create 3D maps. Using generated 3D maps and combining it with object recognition code I've written using OpenCV, I would like to create semantic maps of the environment. For clarity, when I say semantic maps, I just mean having maps are able to highlight or pinpoint recognized objects in the environment.", "I hope to do this in 'real-time' - it would be nice if the final product was able to update at a rate of at least 5Hz on my dual-core laptop. ", "I'm new to ROS/ RGBDSLAM/ OctoMaps so I was wondering whether anyone can provide feedback or suggestions? Is there anything I should watch out for? Are there any easier ways of going about this? Is this in fact possible!?", "As far as I can tell from my research, this is the best way to go about this. ", "I'm also open to suggestion for 2D maps as well, although would prefer 3D at this stage.", "Thanks!!!", "I have also posted a similar but slightly more generalized question to this one at "], "answer": [" ", " ", "Regarding the integration of semantic information, here's my thoughts:", "The easiest way (maybe not the most efficient though) to do this, is should be to have your recognition software subscribe to the rgb image and point-cloud, do recognition, recolor the ", ", e.g. red points for class 1, green for class 2, black points for unclassified. then send the rgb image and the recolored cloud to the topics rgbdslam listens to (you can modify them via parameters). Then the point cloud colors will be integrated into the map and can be seen as labels.\nTo make things efficient, you would integrate your software into rgbdslam and call it from the callback methods in openni_listener.cpp (e.g. kinect_callback, noCloudCallback).", "A cleaner way is to use a point cloud type that contains semantic information. Therefore you would need to redefine the point cloud type in rgbdslam (src/parameter_server.cpp) to one that contains your semantic information. Note that, if you omit the color (i.e. the point.rgb field), there will be errors in compiling glviewer.cpp. These should be easily solvable though.", "Then you need to adapt the octomap server to use a voxel leaf that stores semantic information. This has been done, but I don't know how.", " ", " ", "Using Multiresolution Surfel Maps for RGB-D-SLAM \nSee code.google.com/p/mrsmap\nand Random Forests for Object-class segmentation of individual views, Nenad Biresev fused this semantic information to create 3D semantic maps.", "See: \nJ\u00f6rg St\u00fcckler, Nenad Biresev, and Sven Behnke:\n Semantic Mapping Using Object-Class Segmentation of RGB-D Images\n In Proceedings of IEEE/RSJ International Conference on Robots and Systems (IROS), Vilamoura, Portugal, October 2012.\nSee www.ais.uni-bonn.de/papers/IROS_2012_Semantic_Mapping.pdf", " ", " ", "5hz on a dual core laptop might be hard to get. You'll need a GPU, and use SIFTGPU features otherwise detection & extraction of SURF will slow you down too much (~2hz). ORB might be an alternative, but I found them to be less accurate. With two cores, you will definitely need to reduce loop closure search (the ..._candidates settings, see parameter_server.cpp).", "Setting the openni camera driver to QVGA and lower FPS (use dynamic reconfigure) will reduce the cpu load of the driver.", "Producing Octomaps from online-mapping consumes a lot of RAM, because you need to store the clouds. Reducing resolution will also help here.", "Hi Felix, you've highlight some good points on improving the operation of RGBDSLAM but I was more interested in whether it is possible to highlight some pointcloud data which shows an object which has been recognised. Can the RGBDSLAM code be easily modified to do this if it can't already? Cheers."], "url": "https://answers.ros.org/question/37595/create-semantic-maps-using-rgbdslam-octomaps/"},
{"title": "How can I do a Networking between ROS (Linux) and a C++ Program running on Windows?", "time": "2013-01-13 05:32:33 -0600", "post_content": [" ", " ", " ", " ", "The C++ program has been made in Visual C++. How can I establish a network with my program and ROS but without having ROS on Windows?, I mean a directly conexion between my program in VC++ and ROS.", "I know that ROS can run on Windows, but my goal is not do a network between ROS on windows and ROS on Linux. ", "I have already created a Client/Server code in C++, so I think that I can implement that code in ROS (with cpp) and my program  just to send/recieve messages between them. Is it possible just configure a network on ROS? I mean to set up IP, port, protocol.", "I would appreciate your help with tips or example of codes.", "Thanks.", "If I do that \"dbworth\" recomended me, can I use functions of ROS just to obtain values from a device with a ROS driver and then use the TCP connection to send that value to the VC++ program (Computer running on Windows)? I mean send that value to process it in the program in VC++.", "My goal is just send values (that are obtained from a device connected with ROS, for example a sensor) from ROS on Linux to a VC++ program on Windows and process those values in that VC++ program.", "Of course, you can write a little node that subscribes to a topic, then in the callback it writes the data to some variable.  Then in the while loop (mentioned below) it sends the current value out via non-ROS TCP socket.", "Is there a format message to subscribe to a topic? How can I do it?  Thanks.", "Okay, step 1 is to do all the ROS Tutorials. It is important. Then make a new package to read the sensor. Write the CPP code for your new node, and get it to print the sensor data to terminal window. Then you can add the code for creating a TCP socket in Linux, and output that data on a socket."], "answer": [" ", " ", "You can simply run a ROS Node on Linux that receives both ROS messages and messages on a standard TCP/IP socket.", "Google search for TCP sockets programming for Linux, and winsock programming for VC++. You create a simple TCP socket connection and pass data back and forth. In the ROS node, you have a loop like:", "So you put the sockets code in there. The system can be uni-directional (one way communication) of even bi-directional (both ways) works fine.", " ", " ", "Rosbridge might be an answer for your case.\nBasically, you run the Rosbridge server within ROS environment, and implement web socket client within your windows program. For details, you could refer to ROSbridge documentation.", "Which is there: ", " ", " ", " ", " ", "What do you want to do is not too much well explained. But in any case, here you are some important hints about using a heterogeneus network for robotic applications with/without ROS:", "ROS currently has only a basic and experimental support for windows: ", " You will have to test the basic examples of this package. Also make sure that the ROS_MASTER_URI variable is properly set.", "Of course you an alternative is interconecting processes using standar IPC mechanism: from simple sockets to complex web services. You can also use XML-RPC++. \nHowever, if you use these mechanisms you will lose the majority of the runtime ROS benefits as Distributed System: introspection, data standarization in the comunications, parameter server, services, dynamic reconfigure. It also includes the powerful publish/subscribe (topics) that ROS provides: A ROS node/process can comunicate with other process in other machines in a decoupled way. That is, without knowing neither: the IP of the machine nor the proces PID nor the underlaying socket port.", "Also many of the development and deployment ROS features like roslauch/topic remaping, etc."], "answer_code": ["while (ROS::OK) { \n   do stuff in loop \n}\n"], "url": "https://answers.ros.org/question/52107/how-can-i-do-a-networking-between-ros-linux-and-a-c-program-running-on-windows/"},
{"title": "Is there a ros package that can provide GUI for other ros nodes?", "time": "2013-01-01 05:37:56 -0600", "post_content": [" ", " ", " ", " ", "There are a camera and multiple sensors mounted on my robot. And I will write ros nodes for all of them. But is there a ROS package that can provide GUI to show data got my camera and sensors together?", "I use Ubuntu 10.04 and ROS electric. So I think package rqt doesn't work for me.", "Thanks in advance!"], "answer": [" ", " ", " ", " ", "Direct solution to your problem could be that since ", " is distributed system, you could receive messages from your robot (as long as they're published as ", " format) on different computers that run ", " or higher where ", " is available.", "Regarding ", " as ", " suggests, I actually tried using it on ", " recently and my answer is no it's impossible/not worth trying.", "I used ", " branch (that is the oldest branch afaik, and also doesn't use ", " to build) of ", " which also requires ", " and ", " with same branch names on github. When I run ", " I got the following error which seems to depend on a ", "'s function that might not exist in ", ". So, if you dare to workaround even further, ", " may or may not work. I'm curious.", "Used ", "(Added on Jan 12) Another answer based on comments on ", "'s answer, is again, ", ". You can open ", " and ", "'s GUI called ", " at the same time on a single window. Using ", " is ideal though.", "Thanks for the useful information, Isaac.", "BTW, I do not recommend mixing different ROS distros in the same ROS graph. In some cases that may work, but not in general.", "Agreed. Unless mixing distros is the only solution..", " ", " ", " ", " ", "You might want to try   ", " if you are on ", ".", "Hi alfa, rviz is mainly for 3D visualization right? I don't need to plot the data gathered by kinect and laser in 3D. What I need is just a GUI that has several panels to show the information gathered by camera, battery power and temperature sensor.", "It would be better if ros dynamic reconfigure GUI can integrate with it.", "I've limited skill on this, perhaps you can refer to joq's answer, that's better.", " ", " ", "For future reference, the ", ", but that package is not currently available on Electric.", "I have no idea how difficult a back-port to Electric would be. You might consider ", " to see if it seems feasible and whether he is willing to accept such a patch."], "answer_code": ["ROS", "ROS", "fuerte", "rqt", "rqt", "electric", "fuerte-devel", "catkin", "rosmake qt_gui_core", "pluginlib", "electric", "rqt", "Ubuntu Lucid, electric", "/home/rosuserian/ROS/qt_gui_core/qt_gui_cpp/src/qt_gui_cpp/recursive_plugin_provider.cpp:68:   instantiated from here\n/home/rosuserian/ROS/qt_gui_core/qt_gui_cpp/include/qt_gui_cpp/ros_pluginlib_plugin_provider.h:237: error: \u2018class pluginlib::ClassLoader<qt_gui_cpp::PluginProvider>\u2019 has no member named \u2018createInstance\u2019\n", "rqt", "RViz", "dynamic_reconfigure", "rqt_reconfigure", "Groovy", "rviz", "electric"], "url": "https://answers.ros.org/question/51273/is-there-a-ros-package-that-can-provide-gui-for-other-ros-nodes/"},
{"title": "kinect dies when using the follower example on a turtlebot", "time": "2013-01-26 09:39:21 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to test out the follower example on my turtlebot.  Everything seems to work fine, I start the follower app, the kinect comes on, then as soon as the motors move the kinect shuts off.  Sometimes the robot will move a little bit before it shuts off.  Other times it seems like the kinect is still running (red IR light is on) but the robot doesn't respond at all. ", "Here's what I get out of the follower app. ", "[kinect_laser-5] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored\nprocess[kinect_laser-5]: started with pid [5010]\n[kinect_laser_narrow-6] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored\nprocess[kinect_laser_narrow-6]: started with pid [5011]\n[openni_launch-3] process has died [pid 4984, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load openni_camera/OpenNINodelet openni_manager __name:=openni_launch __log:=/home/turtlebot/.ros/log/20535746-67f8-11e2-a10f-00242ca9ad17/openni_launch-3.log].\nlog file: /home/turtlebot/.ros/log/20535746-67f8-11e2-a10f-00242ca9ad17/openni_launch-3", ".log\nrespawning...\n[kinect_laser_narrow-6] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored\nprocess[kinect_laser_narrow-6]: started with pid [5140]\n[kinect_laser-5] process has died [pid 5010, exit code 255, cmd /opt/ros/fuerte/stacks/nodelet_core/nodelet/bin/nodelet load pointcloud_to_laserscan/CloudToScan openni_manager cloud:=/cloud_throttled __name:=kinect_laser __log:=/home/turtlebot/.ros/log/20535746-67f8-11e2-a10f-00242ca9ad17/kinect_laser-5.log].\nlog file: /home/turtlebot/.ros/log/20535746-67f8-11e2-a10f-00242ca9ad17/kinect_laser-5*.log\nrespawning...\n[kinect_laser-5] restarting process\nException AttributeError: AttributeError(\"'_DummyThread' object has no attribute '_Thread__block'\",) in <module 'threading'=\"\" from=\"\" '=\"\" usr=\"\" lib=\"\" python2.7=\"\" threading.pyc'=\"\"> ignored\nprocess[kinect_laser-5]: started with pid [5152]"], "answer": [" ", " ", "Looks like it's a hardware issue.  ", "I brought the kinect up.  Then drove the robot around using the joystick teleop.  When it's moving very slow, the kinect stayed on.  But if it went faster the kinect died.  I verified that the voltage to the kinect is fine so it must be a current issue.  I would think that the battery pack could source plenty of current unless my batteries are going bad?"], "url": "https://answers.ros.org/question/53457/kinect-dies-when-using-the-follower-example-on-a-turtlebot/"},
{"title": "Turtlebot (clearpath) Kinect Visualization Tutorial", "time": "2012-08-19 19:29:09 -0600", "post_content": [" ", " ", " ", " ", "I was able to solve the problem by upgrading the turtlebot laptop to Fuerte.", "Hi,", "I have read through several posts on this particular problem, unfortunately was not able to solve it using the hints given in other threads.\nFirst off, about my setup: I have ROS Fuerte running on my workstation while it's the fresh electric install on the netbook. ROS_HOSTNAME is set to the workstations IP, the master uri to the laptop's IP and corresponding port. ROS_HOSTNAME on the laptop is set to its IP, likewise the master uri (again on port 11311).\nAs for the SDK, I installed the ros-fuerte-clearpath-turtlebot package, because I was unable to fetch the dependencies for the electric SDK using Fuerte (maybe using two different versions is the root of all evil?)", "I can bring up the dashboard from my workstation without any problems, the only warnings I get are about ROS being unable to fetch information in the laptop's battery charge state.\nI can set the robot to full mode and breaker 0 turns green. Also, I have no problem firing up kinect via roslaunch turtlebot_bringup kinect.launch. I have also followed the note in the turtlebot bringup tutorial saying that one should \"roslaunch openni_camera openni_node.launch\" after charging the turtlebot, nevertheless no matter whether I use the former, the latter or roslaunch openni_launch openni.launch, I am unable to get any visualization using the camera input in rviz on the workstation.\nIf I run rostopic list on the workstation there are several camera and kinect_laser topics, nevertheless rostopic echo does not show any messages being sent to the workstation. rosout does not show any broadcasting information with regards to the camera (if there should be any).", "What am I doing wrong? Should I upgrade the turtlebot laptop's ROS to Fuerte as well? What is the corresponding SDK to download? Am I even using the right SDK on the workstation? Why is there absolutely no output if I try echoing the camera topics?", "edit: by the way, the green light on the kinect is flashing. Is it supposed to flash or is this due to some problem that might be related to power shortage (e.g. USB-driver related as some other topic suggests)?", "edit2: the workstation is running ubuntu 12.04 LTS, the turtlebot laptop runs ubuntu 10.04 LTS (the differences on the platforms are not intentional, I had my ROS installed on the workstation beforehand and considering it's not my own turtlebot, I did not want to induce huge changes to the system)."], "answer": [" ", " ", "I had problems with the kinect visualization tutorial as well...", "once you bring up RVIZ you have to add a camera view and go to the topic section and click to the right where there is a drop down menu (that you can't see until you click), there you choose a view such as rgb or laser", "hope this helps!", " ", " ", "I had the same installation configuration:\nUbuntu 12.04 LTS on the workstation and 10.04 on the turtlebot.\nI also had a lot of problems setting the system up. Most of all wireless connection losses and Problems starting the kinect.", "I got the system to work when I used the (non-clearpath) ros-fuerte-turtlebot packages. I didn't test the clearpath packages. I started my kinect with ", "roslaunch turtlebot_bringup\n  kinect.launch", "as far as I know, this will start the deprecated Openni nodelets. ", "The green light on the kinect is flashing as soon you plug into usb, but this is no indication whether there is enough power... The breaker 1 on the dashboard has to be on.", "Sometimes it helped me when I stopped/started the turtlebot service.", "sudo service turtlebot stop", "sudo service turtlebot start", " ", "sudo service turtlebot restart \n  doesn't work ", "Since a few days new apt repositories for ros-fuerte-turtlebot are available. I upgraded both systems and kinect as well as gmapping works. With this configuration you can start the kinect via openni_launch", "For the battery problem you can use the solution here:\nhttp://answers.ros.org/question/30631/turtlebot-unable-to-check-laptop-batterybat0state/", "You create an overlay package for this, to keep the apt-packages unchanged. If you have questions to this, just ask. It is different for electric and fuerte and it took me some time to figure out."], "url": "https://answers.ros.org/question/41829/turtlebot-clearpath-kinect-visualization-tutorial/"},
{"title": "Kinect + Pandaboard + ROS = Fail", "time": "2012-11-14 04:03:29 -0600", "post_content": [" ", " ", " ", " ", "Hi there,", "i tried to run kinect with ROS on pandaboard (ARMv7-processer), but it failed.", "now i am at a point that kinect is running on pandaboard, but not with ROS. I used ", " to get kinect started on pandaboard an it works, the sample \"SimpleRead\" sends data to the terminal with \"fps...\" and \"middlepoint is...\".", "After that i tried to run openni_kinect stack, downloaded it, but cant build. I tried", "rosmake openni_kinect", "but it stops with errors:", "but yaml-cpp is installed.", "What version of gcc are you using?", "i'm not sure, gonna lookup tomorrow, but i believe to use the newest one.", "edit: i am using gcc & cpp version 4.6, no newer one is available."], "answer": [" ", " ", "I managed to get a Kinect working about a year ago on a Pandaboard, however I must warn you that the Pandaboard is not really powerful enough to read out the Kinect at the full framerate and do processing on the side (include encoding and sending it over a network). ", "My idea was to have the Pandaboard send images over Wifi to a laptop with more computational power which would do the processing. I tried many variations of encoding and resizing to get the highest possible framerate, but I could not get more than 20 fps. After a while (few minutes) the cpu would get too hot and shift down, dropping the framerate to somewhere around 10 fps. If you do the processing on the pandaboard you do not need to encode it and send it, so you might get somewhat higher framerate then, but then you are limited to the computational power of the Pandaboard.", "However if you still want to get it to work, here is roughly what I did:", "I hope this is of any help.", "I am interested in your code for the kinect. I am trying to do processing on the Pandaboard, but if I can't your code would be of great help.", " is roughly based on ROS packages, OpenCV code and OpenNI examples. The only thing you should need for the package is OpenCV and OpenNI. There are some things there that you won't need, just comment it out :) (nero_msgs)", "Thank you! I will get started looking though this immediately.", " ", " ", "You will probably need to work on compile flags and possibly patches to get it to compile on arm processors. The ", " would be a good place to get help. "], "answer_details": ["Download the source code for OpenNI", "Compile the source code with the arm flag set (follow the instructions at OpenNI somewhere, I am not sure what I had to do)", "Disable kernel modules for kinect if necessary (I believe they are called gspca_kinect, google it ;) )", "I received some error that it couldn't connect to the Kinect, I had to activate USB BULK type in the OpenNI config", "I didn't use any ros package for reading out the Kinect, I created my own package that in my opinion is more efficient and customizable than the ros one, so I am afraid I cannot help you much there. If you want I can give you a link to the code I used.", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["> make[3]: Verlasse Verzeichnis '/home/panda/ros/image_common/camera_calibration_parsers/build'\n> make[3]: Betrete Verzeichnis\n> '/home/panda/ros/image_common/camera_calibration_parsers/build'\n> Linking CXX shared library\n> ../lib/libcamera_calibration_parsers.so\n> /usr/bin/ld:\n> /usr/local/lib/libyaml-cpp.a(iterator.cpp.o):\n> relocation R_ARM_THM_MOVW_ABS_NC\n> against `a local symbol' can not be\n> used when making a shared object;\n> recompile with -fPIC  \n> /usr/local/lib/libyaml-cpp.a: could\n> not read symbols: Bad value  \n> collect2: ld gab 1 als Ende-Status\n> zur\u00fcck   make[3]: ***\n> [../lib/libcamera_calibration_parsers.so]\n> Fehler 1   make[3]: Verlasse\n> Verzeichnis\n> '/home/panda/ros/image_common/camera_calibration_parsers/build'\n> make[2]: ***\n> [CMakeFiles/camera_calibration_parsers.dir/all]\n> Fehler 2   make[2]: Verlasse\n> Verzeichnis\n> '/home/panda/ros/image_common/camera_calibration_parsers/build'\n> make[1]: *** [all] Fehler 2   make[1]:\n> Verlasse Verzeichnis\n> '/home/panda/ros/image_common/camera_calibration_parsers/build'\n> -------------------------------------------------------------------------------} [ rosmake ] Output from build of\n> package camera_calibration_parsers\n> written to: [ rosmake ]   \n> /home/panda/.ros/rosmake/rosmake_output-20121114-163131/camera_calibration_parsers/build_output.log\n> [rosmake-0] Finished <<<\n> camera_calibration_parsers [FAIL] [\n> 10.40 seconds ]     [ rosmake ] Halting due to failure in package\n> camera_calibration_parsers.  [ rosmake\n> ] Waiting for other threads to\n> complete. [rosmake-1] Finished <<<\n> test_ros [PASS] [ 51.08 seconds ]     \n> [ rosmake ] Results:                  \n> [ rosmake ] Built 53 packages with 1\n> failures.\n"], "url": "https://answers.ros.org/question/48320/kinect-pandaboard-ros-fail/"},
{"title": "Problem running pick and place application tutorial", "time": "2012-04-04 18:06:39 -0600", "post_content": [" ", " ", "Hi", "I am trying to run the tutorial in ", "However, I get an error saying: Call to segmentation service failed", "This occurs when it looks for the tabletop in the very beginning. Any help would be appreciated, thanks in advance"], "answer": [" ", " ", "Is the robot looking at the table/object?  Segmentation won't succeed otherwise.  If you're in Fuerte or Groovy, you can use interactive manipulation to position your robot, see ", "Also, this shouldn't have been a problem back in April 2012, but recently if you were using Fuerte + Kinect, there was a launch misconfiguration that was killing segmentation that is now fixed.", "Sorry not to answer in a very timely fashion.  All my ROS answers emails were getting spam-filtered. :P", "I've got the same problem recently, but there was no such problem before. The default setting is to use the stereo camera and I've write a simple node to make PR2 look down. But the problem still exists. How to fix this problem? Thanks", "You're using the stereo camera and looking at the table and it says Call to segmentation service failed?  Any other error messages around it?  Which ROS distro are you using?", "I've checked the pr2_tabletop_manipulation.launch, and the value of arg stereo is true, which will set thetabletop_segmentation_points_input as narrow_stereo_textured/points2. I put the error messages here: ", " and the brief description is below. My ROS distro is Fuerte", "The object segmentation service can segment out the table. The INFO shows that \"Step 4 done\" and \"Table computed\". But after that, there seemed to be some internal problems which threw out the error message \"Call to segmentation service failed\", and then the tabletop_segmentation node died. ", "Hrm.  That's odd.  Fuerte debians, latest version?  Are you using shadow-fixed or public repos?  What version of Ubuntu?  64-bit or 32-bit?  I just tried using pr2_table_object.launch, both stereo:=true and stereo:=false, and both worked fine on my laptop.  ", "There have been some build-farm compilation issues with compile flags lately, that might be causing the undefined-symbol problems you're seeing.  Try just compiling and using object_manipulation and pr2_object_manipulation from source.", "I'm using Fuerte on Ubuntu 12.04 64-bit and the package is installed using apt-get. I've already upgrade all the package to the latest version. I don't know what is the difference between using shadow-fixed or public repos. Also, what is build-farm compilation issues with compile flags lately mean? ", "I'm guessing you're using the public repos (", ") then.  When ", " shows FbinP64 for perception_pcl having a green box (mouseover the green box should read ros/Public 1.2.4) then the new release of perception_pcl will be done building."], "url": "https://answers.ros.org/question/31265/problem-running-pick-and-place-application-tutorial/"},
{"title": "Alternate Turtlebot Configurations", "time": "2013-02-09 11:27:14 -0600", "post_content": [" ", " ", "Does anyone know of methods for powering and recharging ", " components from\nthe iRobot Create on a ", "? I'd like to get a Turtlebot, but it's hacky power system and inability to auto-recharge, despite the Create's builtin auto-recharge mechanism, is a huge non-starter.", "Specifically, would it be practical to replace the laptop/netbook with a more efficient ", " or ", ", which only need 5V?", "Similarly, has anyone tried replacing the Microsoft Kinect (requiring 12V) with the much smaller and more energy efficient ", ", which only needs 5V over USB?"], "answer": [" ", " ", "If it helps, the ", " has a much better power system and can auto-recharge.", "Other than that:", "1) Yes, it is possible to replace the netbook with a single-board computer. We do not ship with one by default because it's a higher barrier for entry for new Linux & ROS users, but it's quite possible. However, if you're working with Kinect data, we suggest something faster than a Raspberry Pi (even if you're only using pointcloud_to_laserscan and transmitting the scan). I know that ", " (among others) got ROS running on a RasPi, but it can't do much on that board.", "2) Yes, the Xtion is known to work. It's not the \"default\" sensor for TurtleBots because the Kinect is cheaper and we're really trying to make these robots as economical as possible.", "On a side note, why is the Kinect sensor mounted to the rear of the platform where it has to \"look through\" the bot's interior? Wouldn't it have better visibility, and free up all that interior space, if mounted more towards the front?", "The Kinect depth sensor has a minimum effective range of ~0.5 m and a limited horizontal field of view. Pulling it back means that you're able to get closer to objects and see a wider area in front of the robot."], "url": "https://answers.ros.org/question/54840/alternate-turtlebot-configurations/"},
{"title": "Performance of Algorithms doing slam with kinect [closed]", "time": "2012-12-07 09:07:40 -0600", "post_content": [" ", " ", "Hello everyone", "I wanted to ask a question about slam and kinect.", "I will first describe my problem and then talk about some possible problems. ", "If anyone has some insight on whether I might be doing something wrong please give me a hint.", "I have been trying to benchmark this kind of application and so far I have encountered these two packages ", "rgbdslam\n", "modular_cloud_matcher\n", "I was able to test them both running live with 2 different kinects, in different indoor environments.", "From my tests, it seems that the slam does not work very good. ", "In one case, using rgbdslam, one time I was with the kinect facing a wall (with texture), the first point cloud was drawn in a plane, the second point cloud (from the same wall) was drawn in a different plane, oblique to the first.", "Another case, using modular_cloud matcher, I had the kinect standing still looking at a scene, and the output transformation said that the kinect was moving about 5 centimeters everywhere.", "In sum, for some reason I don't seem to be able to get the kind of performance that both these packages show in their demo videos. The performance I get from these packages is considerably lower than for example the performance shown in both these videos. ", "or", "Possible reasons for this, my thoughts were:", "Problem with the kinect (shouldn't be, since I tried out two of them)", "Calibration of the kinect (I thought of this and I calibrated both the rgb and ir cameras using the instructioon from ", " There was no significant change in the performance)", "This is my last guess. My computer is very slow (it actually is a good laptop, 2.5 quadcore, about 3 year old).", "My guess is that since these slam algorithms try to find correspondences to infer the transformation, perhaps because my computer is slow, the frames they compare end up having a small overlap.\nCould this be a problem?\nTo try to solve this, I have tried to move the kinect around very slowly, still with no better results.", "If anyone can give me some help I would appreciate.", "Thanks ", "Miguel"], "answer": [" ", " ", "Yes, it could be your computer. You can easily test this: record a bag file and play is back on a very much lower rate. SLAM in 3D does need some power.", "It could also be that your problem is very hard. SLAM needs some features to match where it is. A flat wall doesn't have many. Also an error of 5cm doesn't seem much to me. Maybe your expectations are different from what the algorithms can deliver.", "It might be a good idea to upload an image of the environment and/or a specific bag file that you want solved. Maybe someone has time to look at that. With the information you have given it is a bit hard to guess what is the actual problem.", "Thanks dornhege. I will make a bag recording or a video and upload it.", " ", " ", "You can also check out ", ", which provides tools for fast visual odometry with RGB-D cameras."], "url": "https://answers.ros.org/question/49930/performance-of-algorithms-doing-slam-with-kinect/"},
{"title": "Speeding up OpenCV on ARM?", "time": "2011-06-15 11:02:29 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi all,", "I'm working with OpenCV on a Gumstix, and it is incredibly slow.  For instance, running the \"GoodFeaturesToTrack\" function in a node runs at 1.5Hz, and requires 85% processor utilization on a Gumstix Overo Tide.  I used a standard download and compile of the ROS vision_opencv package to get these results.  Any ideas on how to optimize?  It seems that even a lowly 700MHz processor should be a bit faster than what I'm getting.", "Thanks!", "I'm looking into Ash Charles' suggestion to add a flag to use the neon coprocessor.  I'm not quite sure how to do this.  I'm assuming that I make a change to my opencv2 makefile, but would appreciate some direction on that.", "I'm working on implementing RahulP's suggestion.  So, in the vision_opencv package, I added a flag to CMakeLists.txt, which now looks like this:", "Then, I ran ", " which was successful.  However, running the code showed no improvement whatsoever in execution speed (as measured by running rostopic hz /output).", "Did I skip a step here?  Or do I need to be looking into profiling?  I tried using oprofile, but didn't really get anywhere productive.  If you have any suggestions, they would be much appreciated. ", "If I were to go to my OpenCV directory, and type in:", "followed by running rosmake, would that have the desired effect?", "I've uploaded all of my code to ", "  The main program that I'd like to optimize is in nodes/find_laser_filtered.py.  Obviously, one easy thing to do is to get rid of filtering lines by angle, that was mostly for debugging purposes on the desktop.", "If you get an opportunity to take a look, please suggest any ideas for improvement you may have.  I'd really like to get this running at 15+ Hz, and I'm currently at about 2 Hz...", "Thanks!"], "answer": [" ", " ", " ", " ", "I agree this seems slow so two thoughts crossed my mind.", "The DSP onboard provides powerful capabilities when used with gstreamer---this may also give some benefit.", " ", " ", "The easiest way to do this is to trick the build setup by doing the modifaction mentioned below in the file CMakeLists.txt. ", "Dont forget to turn ON the USE_O2 options while configuring your build. This modifcation only applies to earlier versions of OpenCV. The latest version of OpenCV (version 2.3.1) supports building OpenCV with NEON enabled directly via CMake", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I have done lot of optimization work on OpenCV on ARM and not ARM. Main gains I got us just looking into OpenCV code and just optimizing it for my particular use. For example often there is a some sort of \"if ( 8 bit color) or ( 16 bit color)\" and I make millions of calls to this function. Of course I never use randomly different color depths, so I just can remove this \"if\". Doing this in as many places possible usually leads with huge boost from some point as compiler realize some way to optimize it after. Mostly with inlining, comptuting constants while compiling etc. All kind of magic with GCC flags usually provides result in range of 1% up to 5%, no more. Of course when -O2 at least in place.\nSo just cut and paste parts of OpenCV you use into own code and then strip them down as much you can. Do not worry too much, usually those algorithms are just few hundred lines.", " ", " ", "I represent a company called Uncanny Vision. Our first product UncannyCV is an OpenCV like image processing/vision library optimized for Cortex A series ARM. We use all kinds of techniques like Neon instruction, algorithm optimizations, cache optimization etc to get a good speed-up. The performance gains vary from algorithm to algorithm. Sometimes its as low as 2x and some others are as high 20x. ", "For good points to track, we get nearly 8x improvement over standard openCV on Cortex A8. On BeagleBoardXm with a single core Cortex A8 running at 1GHz, we can do a VGA frame in 45ms(ofcourse the performance is also a little image dependent), meaning 22 fps. On 720MHz(which is the case discussed above), this would translate to 62.5ms.  ", "For non-commercial purposes(university development projects) we can provide the trial version of the library for free. You can come to our website and drop us an email.", " ", " ", "I would suggest profiling with either ", " or ", " to see what the primary bottle neck might be. Once you see what part is slowing execution the most, you might be able to find specific issues which might be easier to research re compile flags.", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "i will try to use DSP core - ", " "], "answer_details": ["Check what is actually using all the processor time with something like oprofile. The processor shouldn't be doing tons of math or swapping of buffers but knowing what this is a first step.", "Hard-float capabilities are provided by the Neon and vfp extensions.  IIRC, CMake should pick up flags from the environment so try adding '-mfpu=neon' to CXXFLAGS prior to recompiling.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["cmake_minimum_required(VERSION 2.4.6)\ninclude($ENV{ROS_ROOT}/core/rosbuild/rosbuild.cmake)\n\nset(ROSPACK_MAKEDIST true)\nset(EXTRA_C_FLAGS_RELEASE \"${EXTRA_C_FLAGS_RELEASE} -O2 -mfpu=neon\")\n\nrosbuild_make_distribution(1.4.3)\n", "rosmake vision_opencv", "./configure -mfpu=neon\n"], "answer_code": ["# Other optimizations \nif(USE_O2)\n\n    set(EXTRA_C_FLAGS_RELEASE \"${EXTRA_C_FLAGS_RELEASE} -O2 -mfpu=neon\")\n\nendif()\n"], "url": "https://answers.ros.org/question/10273/speeding-up-opencv-on-arm/"},
{"title": "turtlebot  - DB25 connector", "time": "2013-01-14 08:39:29 -0600", "post_content": [" ", " ", "HI all \nIs it Possible to use the \"TurtleCore\u2122 expansion board\" Instead of \"TurtleBot Power and Sensor Board with 250\u00b0/sec gyro\" ,Then Connect a x-tion to one of the usb ports? .Does the \"TurtleCore\u2122 expansion board\" Included Gyro sensor like the \"TurtleBot Power and Sensor Board\"? Thanks."], "answer": [" ", " ", "The two boards are not interchangeable, the TurtleCore board is meant to be runs as a standalone processor or as a means of communicating with the TurtleBot. While the gyro board is meant to provide gryo data and a means of powering the kinect sensor from the create. As far as I know the TurtleCore does not provide a gyro. "], "url": "https://answers.ros.org/question/52215/turtlebot-db25-connector/"},
{"title": "Is a power board really necessary for turtlebot? Or is an din-usb cable enough...", "time": "2012-10-05 14:28:12 -0600", "post_content": [" ", " ", " ", " ", "Hi", "I have an Roomba 780 and the din-usb communication cable from irobot, my question is if this is enough to enable communicate with the roomba with turtlebot? I realise that I will need to provide separate power for the kinect but wondering if there is something else special with the power board?", "(at the moment it seems like turtlebot.eu is not selling any components so cant buy a power board for my roomba, I was thinking to make my own board with the schematics for the board but it appareas to me that the board is basically the usb interface with the FT2232RL chip and extraction of the 12v to the the user, is this correct?)", "Thanks", "I cant see that the powerboard for the roomba has any gyro? Is it possible to read a gyro throught the din7 port of the roomba?", "There is no analog input available on the Roomba, so no option to connect an analog gyro like the ADXRS613 directly. You could use another IMU device that is connected to the PC, e.g. with an USB interface. See ", " for available drivers."], "answer": [" ", " ", "It's not necessary if you have an alternate power source for the Kinect.  The other thing that it has is a small gyro which helps with the odometry, but there are parameters to disable the gyro for the software.  ", " ", " ", "The power board also has a angular rate gyro onboard, which is highly recommended for odometry purposes.", " ", " ", " ", " ", "Hi again", "Many thanks for these answers.", "Yes I have read  about the gyro but as I see it there is only a gyro on the powerboard made the Create (US version). Studying the (european powerboard for roomba) on turtlebot.eu I cant see that there is any gyro on this board ", "   ???", "Have anyone managed to use a gyro with roomba throught the din7 port, I am not sure if the din7 port have the ADC able to read the gyro like the Create has on its DB25 analog serial port?! If that is possible my idea would offcours be to buy the US version of the powerboard and modify the port but little bit uncertain if that is possible...", "Krst", "Generally updates to the question (or replies to answers) are best posted as edits to the original question (or comments on the answers).  Keeps things much tidier and it's easier for people to make sense of in the future.", " ", " ", " ", " ", "I have the same question, have my EU turtlebot running but without gyro and wonder if it helps to add. I have US create version pOwerboard with gyro as well and wonder how to connect to din port. Or if I buy seperate gyro how to have it publishing messages? Maybe via Arduino?", "Please use comments for questions and coimments like this or create a new question. Thank you.", "Ok, will do next time. Note I did not have this comment option before, now I have, something to do with  >10 karma (bug?)", "That's quite possible. The required karma for commenting was recently lowered to 3 (", "). So, it's not a bug, but a setting. :-)"], "url": "https://answers.ros.org/question/45207/is-a-power-board-really-necessary-for-turtlebot-or-is-an-din-usb-cable-enough/"},
{"title": "ROS vs MATLAB...which is best..?", "time": "2013-03-01 00:32:39 -0600", "post_content": [" ", " ", " ", " ", "Often i end up with this question. Both of them are powerful softwares; and has more or less similar functionalists. Other than Opensource is there any preferable advantages of ROS over MATLAB."], "answer": [" ", " ", "There is some overlap of functionality between certain ROS tools and libraries and the MATLAB Robotics Toolbox, for controlling robotic arms in particular. So some research labs around the world use MATLAB to write controllers for robotic arms, while other prefer non-MATLAB code written for ROS.", "The focus of ROS is a bit more towards solving mobile robot problems, rather then only robotic arms in industrial settings. And a benefit of ROS is that new libraries that have been created can more easily be shared with other researchers in the world, and more easily be integrated with other software on a robot.", " ", " ", "I think you misunderstood something completely. ROS and Matlab do NOT have similar functionalities at all. Thus a comparison as to \"what is best\" doesn't make sense.", "Can you narrate...it please ", "I was referring to Matlab as in pure Matlab. Here, ROS serves a very different purpose, i.e. enabling robots. If you meant the context of ROS in comparison to the robotics toolbox, refer to ", "'s answer.", " ", " ", " ", " ", "I don't think that difference between MATLAB and ROS is due to mobile robots or any other functionality, we should look for ", " to get the difference.", "I see MATLAB (Octave or Scilab) as a great tool for the ", " where you are focus on develop control algorithms, in this step a powerful tool that MATLAB is very useful because allow you to focus on the problem itself. ", "After this phase there is the ", " (design and implementation). In this phase you design and implement the code you have prototype in MATLAB and other functionalities that are necessary to make your system work. \nThis can be done in any platform (OS, program language, libraries) you want, your own platform too. However, doing this by yourself would require a lot of time.", "ROS provides this functionality out-of-the-box, and a lot of extra components ready to use. In addition, there is a lot of people around the world using and developing it ."], "url": "https://answers.ros.org/question/56721/ros-vs-matlabwhich-is-best/"},
{"title": "Turtlebot 2 errors", "time": "2013-02-28 22:23:08 -0600", "post_content": [" ", " ", " ", " ", "\nI removed everything from the turtlebot laptop and installed Ubuntu 12.04, installed ros-groovy-desktop and went through debs installation for the TurtleBot.", "I do not get errors with bringup now but I get this warning", "process[app_manager-11]: started with pid [6649]\n[WARN] [WallTime: 1362386890.272856] Battery : unable to check laptop battery info [/proc/acpi/battery/BAT0 does not exist]\n[WARN] [WallTime: 1362386890.277911] Battery : unable to check laptop battery state [/proc/acpi/battery/BAT0 does not exist].", "Now, the dashboard looks like this", "With tele-op, it is still the same. I am not able to move it around. The controls do not work.", "The kuboki base doesn't seem to be recognized. Why is that ? Is there a mistake I am doing ?", "Thanks, Prasanna.", "Can you you see the robot green LED? Sorry, I know this sounds like a stupid question, but Kobuki has a switch to change between firmware updates (no LED) and normal running. Maybe your switch is on \"download\" mode.", "And another basic thing to check is whether port /dev/kobuki exists when the robot is plugged (don't need to be on)", "The warning is probably because you are using a different laptop than the default the turtlebot is configured for. There is some information about netbook batteries on the wiki: ", " The switch is in \"operation\" mode only and /dev/kobuki does not exist. What may be the problem ?", " under /proc/acpi there is nothing named battery. So basically there is no /proc/acpi/battery.", "Did you execute this command after installation?  rosrun kobuki_ftdi create_udev_rules  (you will need root password; and upgrade ros-groovy-kobuki if the script is missing)", " Thanks! I guess that should work once <param> tag bug is fixed. And, I am sorry if this sounds too naive, how do I edit a ros wiki page?", " I tried setting the battery to /sys/class/power_supply/BAT0 but now I am getting a warning tha /sys/class/power_supply/BAT1/info is missing. "], "answer": [" ", " ", "Is ", " relevant for you? If that works, update the battery setup wiki page with a hint and ticket us at "], "url": "https://answers.ros.org/question/56704/turtlebot-2-errors/"},
{"title": "Help with workstation PC vs turtlebot PC", "time": "2013-03-09 06:16:53 -0600", "post_content": [" ", " ", " ", " ", "I have been working on installing groovy on to my Mac book laptop and would that make it my workstation or turtlebot PC?", "Do you have to have two PCs to do the turtlebot or can one piece be both the workstation and the turtlebot PC?"], "answer": [" ", " ", " ", " ", "You need two PCs. One PC which runs the programs that need the turtlebot hardware like minimal.launch, 3d sensors.launch, teleop etc. This PC is the turtlebot PC to which the USB cables from create/kobuki base and kinect are connected. Workstation PC is theoretically not needed, you can run everything on TurtleBot PC. But if you need to run powerful apps for navigation, planning etc, you will only end up burning your turtlebot pc with atom processor (one that comes with the robot usually). In this case you need another powerful laptop which can listen to turtlebot related topics and do the job. Even if you have a powerful laptop, as in your case - an apple PC, imagine you are teleop-ing the robot. You will have to run behind the robot as it moves. Not a good scenario. So, using two laptops is must, TB PC attached to the robot and Workstation PC connected wirelessly to the TB PC.", "I don't think turtlebot packages are available in Windows version of ROS. So, yes you need two laptops both running linux. Or atleast, the one connected to the robot has to run linux and may be the workstation can run windows but I am not sure how far that ll work. So both running linux is best solution.", "If both the laptops are setup properly, you can easily establish a connection going through ", ". Everything is clear there.", "How do you setup the wireless connection between the two PCs and do they have to both be running Linux if one is because the only other PC I have acess to runs windows 7 ", " I have updated my previous answer to accomodate a reply to your comment. please check..", "OT  How do I vote for this question/answer?", " What do you mean ?", ", how do I add a vote (left block of three blocks) for this question/answer?  Alan KM6VV", "You can always ssh into the turtlebot pc from a windows computer though.", " There are up and down arrows beside every question and answer. "], "url": "https://answers.ros.org/question/57601/help-with-workstation-pc-vs-turtlebot-pc/"},
{"title": "turtlebot_bringup error  - invalid <param> tag [closed]", "time": "2013-03-05 05:33:41 -0600", "post_content": [" ", " ", "I installed turtlebot packages from debs. When I run minimal.launch I get the following error", "I am getting this error after running ubuntu updates this afternoon. I did not get it before the update. Is anyone else getting the same error ? ", "Thanks,", "Prasanna", "I never update Ubuntu. This helps a lot ;-)", "I have the same error than you, can you keep me up to date if you solve the problem or if a bug fix is released ? Thanks !", " Your advice to update Ubuntu, wasn't it ;) Sure I ll update..", " It works for me the first time I did it...  But not this time ! Sorry !", "New version of xacro has been released yesterday (now version 1.7.3). Download and test it to see if it solves your problem.", "I downloaded a new version of xacro doing : \"sudo apt-get update\" and then \"sudo apt-get install ros-groovy-xacro\" and it works fine for me now, even with the roslaunch command."], "answer": [" ", " ", "You are probably hitting the bug in the new version of xacro, which has been introduced when catkinizing it.", "Have a look at this ticket for more details: ", "The bug fix should soon be pushed to public.", "Please report back, if you are still having issues after the update.", "Guys, I'm sorry to inform you that you need to wait a bit longer for a bug fix. The bug fix in xacro release 1.7.3 only solves the problem of 'rosrun xacro xacro.py ...', but not the roslaunch issue. Follow this ticket for more details: ", " I installed the new xacro release and now the roslaunch works fine except for the laptop battery warning which I have described here - ", " Thanks for the info. Just triggered my updates ... PS: Since your question/problem looks to be answered/solved, please mark it accordingly. Thank you!"], "question_code": ["while processing /opt/ros/groovy/stacks/turtlebot/turtlebot_bringup/launch/includes/_robot.launch:\nInvalid <param> tag: Cannot load command parameter [robot_description]: no such command [/opt/ros/groovy/share/xacro/xacro.py '/opt/ros/groovy/stacks/turtlebot/turtlebot_description/robots/kobuki_hexagons_kinect.urdf.xacro']. \n\nParam xml is <param command=\"$(arg urdf_file)\" name=\"robot_description\"/>\n"], "url": "https://answers.ros.org/question/57161/turtlebot_bringup-error-invalid-param-tag/"},
{"title": "Microsoft Kinect or ASUS Xtion ? [closed]", "time": "2012-02-05 14:54:02 -0600", "post_content": [" ", " ", " ", " ", "I plan to buy one of the two, Kinect or Xtion for my Roomba 560. The issue is that I am not able to decide; Kinect probably has a better compatibility with ROS while Xtion has better hardware. Any first hand experience/suggestion will be most welcome.", " ", "\nASUS Xtion Pro Live works really nice with ROS Electric, but has issues with ROS Fuerte and ROS Groovy. ", " ", "\nASUS Xtion Pro Live is WOW ! ", " ", " ", "Concerning the question update, what specific issues have you encountered?, and more importantly, are they reported as issues?. We've had a good experience with an Xtion Pro Live and the latest drivers on Fuerte.", "Adolfo Rodriguez T In Groovy, I was getting 'cannot set up USB' while Fuerte had issues with the Open Ni driver. ROS Electric was the easiest and as of now, we are working on Electric. May be 6 months later we may move to Fuerte or Groovy. ", "Adolfo Rodriguez T Can you tell me which packages and commands did you use for Fuerte ? ", "Latest binary packages (ie. libopenni 1.5.4.0, libopenni-sensor-primesense 5.1.0.41). Launch files from openni_launch package. Tested in Ubuntu 10.04 and 12.04, ros Fuerte. ", "Adolfo Rodriguez T Thanks ! ", "I like update 2! :-D", " LOL !!! it is a work in progress ! ;-) ", " I think your robot would look much cooler with a Kobuki base! ;-)"], "answer": [" ", " ", "The Asus models have the distinct advantage that they're USB-only, smaller, and easier to mount than the Kinect. The disadvantage is that you need to pay attention to how you support it when mounting; the mounting tab on the Asus will break in high-stress or high-vibration environments if it isn't properly supported, but this is still easier than mounting a kinect.", "Depth and RGB from both sensors work well with the OpenNI drivers in ROS Electric.", "Note that the Asus Xtion Pro is depth-only, while the Asus Xtion Pro ", " has both depth and RGB.", "I believe the stereo microphones on the Asus Xtion Pro Live are supported with the OpenNI driver, but I don't think there's ROS support for them yet.", " Hi again, I have purchased an Xtion Pro Live - where do I start ? AFAIK ROS doesn't have a Xtion package ! Any hints ? :-) ", "The usual Kinect tutorials and drivers will work; use the openni_kinect package. This is because the Kinect and Asus sensors are both OpenNI devices; the Kinect was first, so most of the packages got named after it, but most things should work with both.", " Cool ! ", " Another tip needed, which OS are you using for the Xtion ? I am using Ubuntu 10.04 LTS - which has serious issues, further Xtion recommends Ubuntu 10.10. ", "That should probably be a new question. I've been using an Xtion with ROS Electric on Ubuntu 10.04 without problems.", " Thank you, I tried with a new question - but no luck ! :-(  ", " ", " ", " ", " ", "I did a bit of a comparison of the Xtion and Kinect. Data was collect with OpenNI's viewer", "Before I got a hold of it I didn't realise the size difference was that significant!", "Horrible music.", " ", " ", "This is a very late answer but i have not used the Xtion but I have been able to with a simple resistor added to the kinect at the power circuit been able to use the kinect on only usb power. Actually even thought people have stated that the minimum power needed should be about 0.7 amps I am able to run it from a single usb 2 port that has 0.5 amp. Just to be sure however I am running mine from two usb ports with a power usb Y cable.", " ", " ", "USB-only is handy, but (if you can get access to raw battery voltage on your Roomba, like you can on a create), it's about thirty minutes and ten dollars of work to pull power from the robot for a Kinect.", " Well, the Roomba (560) is much like create ! ", "Sure, but the Create has pins in the cargo bay for battery voltage (and assorted other things); I don't know if that's easily-accessible on a Roomba."], "url": "https://answers.ros.org/question/12900/microsoft-kinect-or-asus-xtion/"},
{"title": "ros::service::waitForService() blocks infinite", "time": "2013-03-18 08:57:45 -0600", "post_content": [" ", " ", " ", " ", "hi,\nwhen i use ros::service::call() to call e.g. gazebo/apply_joint_effort inside a rostest (c++,gtest) the call sometimes fail and sometimes don't fail ?", "So i thought, using ros::service::waitForService(\"gazebo/apply_joint_effort\", -1)) to wait for \"something\" and then use the call() function, but the waitForService() is blocking endless. But when i call the service with the commandline tool \"rosservice\", the service is available and returns success ?", "is waitForService() for another usage?", "thanks\nflo", "Change 1:\nInserting", "SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread\")\nrosbuild_add_gtest(test/mytest test/mytest.cpp)", "into CMakeList.txt file and running then the binary bin/test/mytest the service call returns success, but the service (force) does not get applied to the model in gazebo ?", "gazebo dies with:", "terminate called after throwing an instance of 'std::runtime_error'\n  what():  Time is out of dual 32-bit range\nAborted (core dumped)", "I got ubuntu-12.04-amd64 installed. Someone knows what this error causes?", "Also if i use:\nrosbuild_add_executable()\nrosbuild_add_gtest_build_flags()\nrosbuild_add_rostest()", "in my CMakeList.txt file instead the two other options mentioned before, and run rostest mypackage mytest the test is run 3 times in a row after building? Why is that so?  Also the rostest lib cannot call the service, but the test-binary can call the service (but gazebo dies) ?", "Is there a way to prevent the test from running after building it with make test ?", "thanks\nflo ", "I think running it is the intended behaviour of make test."], "answer": [" ", " ", "You are passing -1 for the timeout, which means: wait infinite.\nJust pass a real timeout that you want to wait for.", "yes i pass -1 because i want to block so long till i get the service. I know that the service is available because i can run rosservice from console \"before\" and \"after\" i run the rostest with the awaitForService() call.  The question is more: why is it blocking, but the service is available ?", "Maybe you did something wrong in the software. You can just call the service to see, if it works in principle.", "like i wrote in my question, it sometimes works and sometimes fails", "How are you registering the service client/calling the service? Also ros::service::call?", "what does the <2> and the green check-mark right beside your answer mean ?", "yes i use  (ros::service::call(\"/gazebo/apply_joint_effort\", req, res)", "Your call does have a / before gazebo unlike the waitForService. Did you try ", "'s suggestion?", "yes i tried it, like i wrote in the answer to Hansg91  :)", " ", " ", "the call ros::service::waitForService(\"/gazebo/apply_joint_effort\", -1) blocks also infinite ?", "my test is just:", "TEST(MyTest, firstTest)\n{\nros::Time::init();   //used for waitForService\nif(ros::service::waitForService(\"/gazebo/apply_joint_effort\", -1))\n        EXPECT_EQ(1,2) << \"wait for service ok\";\n}", "Are there some other functions i can call to get more info? like what namespace i am in, or something?", "thanks\nflo", "Please do not create answers for discussion or comments. Instead, either edit and append to your original post or use the comment functionality.", " ", " ", "I'm not sure but it might be using your namespace in the check if it is available (and is therefore waiting for /your_ns/gazebo/apply_joint_effort).", "Could you try :", "That might work, because it is then ignoring your namespace. At least for registering and subscribing topics this is the case.", "with the namespaces i must check (later), but i need the service to publish a service request, so registering and subscribing is not enough. i reply if it works", "There are some functions for that in the ros namespace. However, if you use the / this shouldn't matter unless you remapped that."], "answer_code": ["ros::service::waitForService(\"/gazebo/apply_joint_effort\", -1));\n"], "url": "https://answers.ros.org/question/58464/rosservicewaitforservice-blocks-infinite/"},
{"title": "TurtleBot Kinect Problem [closed]", "time": "2013-03-05 16:44:07 -0600", "post_content": [" ", " ", " ", " ", "I am following the tutorial \"Visualizing TurtleBot Kinect Data\", but I cannot see the data from camera...", "roslaunch turtlebot_bringup kinect.launch", "it gives: ", "Then on the Workstation, I type:", "rosrun rviz rviz", "Add camera -> Image Topic -> whatever...", "The rviz window shows No Image", "And the terminal shows Dropped 100% of messages", "Closing rviz, now on the Laptop, I try:", "rosrun image_view image_view image:=/camera/rgb/image", "It just shows a gray window.", "Anyone has any idea what is wrong?", "TurtleBot (Kinect, iRobot Create); Ubuntu 10.04; ROS Electric", "Please help me!! Thank you very much.", "More information: (1) the Kinect shows a flashing green light (not solid light); (2) in rviz, under camera, it says Global Status: Error. Inside it says ... No Transform."], "answer": [" ", " ", " ", " ", "This problem is solved.", "The solution is a bit mysterious... In Rviz, the default setting is Global Options -> Fixed Frame -> base_link and Target Frame -> < Fixed Frame > ", "This setting is correct, but in order to make it work, I need to click them and select them again, and then reboot the workstation and the laptop. I don't know why but it works after then.", "In addition, the distributor told me that the flashing (not solid) green light on Kinect is normal."], "question_code": ["[ INFO] [1362547950.940390473]: [/openni_camera] Number devices connected: 1\n[ INFO] [1362547950.940926034]: [/openni_camera] 1. device on bus 001:10 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00367910023104A'\n[ WARN] [1362547950.950968862]: [/openni_camera] device_id is not set! Using first device.\n[ INFO] [1362547951.051461572]: [/openni_camera] Opened 'Xbox NUI Camera' on bus 1:10 with serial number 'A00367910023104A'\n[ INFO] [1362547951.114948505]: rgb_frame_id = 'camera_rgb_optical_frame' \n[ INFO] [1362547951.127139480]: depth_frame_id = 'camera_depth_optical_frame'\n"], "url": "https://answers.ros.org/question/57231/turtlebot-kinect-problem/"},
{"title": "Dynamic information flow  between ROSJAVA nodes and ROSActivity", "time": "2013-02-18 12:55:14 -0600", "post_content": [" ", " ", " ", " ", "Hi All,", "When using ROSJAVA with android I'd like to pass information from the ROSActivity gui to the ros_node implementing nodeMain. Is there a way to do this without using the normal ROS message traffic?", "If there is, how is it done?", "cheers", "Peter"], "answer": [" ", " ", "There's a lot of options here. You're basically asking how to write a multi-threaded Activity. This tutorial seems to cover a lot of the various concepts:", "I looked at the handlers type of processing but in the tutorial, it mentioned that it must be run in the same scope as the main activity. I attribute my failure to the fact that I'm trying to update a handler inside my class and therefore out of scope. What method do you recommend?", " ", " ", " ", " ", "Based off Damon's answer, I applied the transfer of information between threads by using an application class as described in his link and elaborated ", " ", "To use the application class which contains the data I wanted to transfer. I created a global instance in the mainActivity onCreate() method, then I created my node classes, passing the instance of the application to them like this:", "After the in the init() method after all the nodes had been set running by the nodeMainExecutor, I passed to a function called loop() and placed the following code.", "there's probably a more correct way to implement the loop other than the while loop, but it is effective and the update to ui works! The power of Android UI and ROS!"], "answer_code": ["@Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        rApplication = (ROSApplication)getApplicationContext();\n        interfacenode= new cartesian_interface_node(rApplication);\n        statusupdate = (TextView) findViewById(R.id.textView1);\n    }\n", "protected void loop()\n{\n    while (true)\n    {\n        runOnUiThread(new Runnable() {\n            public void run() {\n                statusupdate.setText(rApplication.Status);\n            }\n        });\n\n        try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n}\n"], "url": "https://answers.ros.org/question/55582/dynamic-information-flow-between-rosjava-nodes-and-rosactivity/"},
{"title": "Package for simple USB web camera in groovy?", "time": "2013-03-06 07:41:02 -0600", "post_content": [" ", " ", " ", " ", "Which package is recommended to use in groovy for a web camera? And how do I install it? I seem to remember there being a package called usb_cam but I cannot find that anymore."], "answer": [" ", " ", " ", " ", "Nothing specific about Groovy, but here's a good overview of most of the available ", ".", "An up-to-date list of all Ubuntu packages available for Groovy is ", ". At the moment, it says camera_umd is available (green) on Oneiric and Precise, but not Quantal (red).", "Well, I cannot find any of them in my groovy installation (full desktop).", "I don't think full desktop installs any drivers. You can look for ros-groovy-DRIVERNAME packages to install, or else build them from source.", "OK, good to know that this is the method. In groovy I seem to never know if I just is missing a deb or if I am supposed to build from source.", "if there is a deb and you do not want to change anything -> use the deb\nif there is no deb or you want to change the code -> checkout git repo and build from source", " ", " ", " ", " ", "I've used ", " not that long ago, although I can't recall if it was under Groovy or not. It appears that the stack containing uvc_camera is building on Groovy, for all except 12.10 (probably an issue with the more stringent compiler). You should be able to install with \"sudo apt-get install ros-groovy-camera-umd\" unless you are on 12.10.", "usb_cam was part of bosch_drivers, which has not been released into Groovy.", "I don't see an Ubuntu package for uvc_camera, but there is a ros-groovy-camera-umd available.", "Thanks Jack, I had reversed the \"camera\" and \"umd\" in the stack name. Fixed above."], "url": "https://answers.ros.org/question/57320/package-for-simple-usb-web-camera-in-groovy/"},
{"title": "OpenNI_Camera on Pandaboard (Fuerte + Ubuntu 12.04)", "time": "2013-02-19 18:01:57 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "As is mentioned in the title, I have a Pandaboard (pandaboard.org) which features a dual core ARMv7 processor. I currently have Ubuntu 12.04 running, as well as have compiled the \"full - desktop\" version of ROS Fuerte for it (wiki here: www.ros.org/wiki/fuerte/Installation/Ubuntu/Source).  This Pandaboard is intended to be the brain of a robot which features a Kinect sensor for SLAM.", "I am currently grappling with compiling OpenNI_Camera for my platform.  I have installed the latest (unstable) OpenNI drivers as per this guide (www.pansenti.com/wordpress/?page_id=1772). I have not been able to ", " any ROS packages on the Pandaboard. Is there a repository I'm missing or something?", "Not sure if that was the right move, but thats what I've attempted so far. I realize this is not ros-fuerte-openni-camera as one would ", " on a normal x86 processor, but I believe it to be a dependency for the openni_camera (ros.org/wiki/openni_camera - fuerte version) package.", "I guess my question/problem is that according to sources like this (kinect-with-ros.976505.n3.nabble.com/Ros-kinect-kinect-on-ARM-td2654041.html), people have openni_camera and openni_launch compiled and running for ARM architectures but I simply cannot find out how to do it, I thought I'd start by compiling its dependencies by hand, but the libopenni-sensor-primesense-dev for example, has no ARM compilation options according to the README (github.com/jspricke/openni-sensor-primesense) as well as a quick peak into the git.", "Any direction or advice for how to do this would be greatly appreciated! This is for an undergraduate senior design course, for those interested. Apologies for the lack of karma and henceforth, real links - I've been lurking around for an answer on the internet for a couple weeks now.", "Having followed the directions provided here:(", ") (openNI and kinect drivers) and here:(", "/) (OpenCV - Hansg91 mentioned it as a dependency, not sure if its necessary) The first link basically outlines what Hansg91 has said below. I have successfully read data from the kinect on the pandaboard, as per the testing listed at the bottom of the first link. This data is not in ROS yet, but will begin to look at Hansg91's source code to see if it can read the data as I have it.", "It is worth noting that to get the test to execute successfully I had to change the default USB interface. Hansg91 recommended BULK, however I needed ISO, as is mentioned under the driver installation (UsbInterface=1) in the first link. I plan on continuing to update this post as I make progress. It is still very possible that his/her code needs BULK usb interface instead (UsbInterface=2)", "Moving on from The OpenNI drivers, we found we would like to use these packages in ccny_rgbd.  Its faster than rgbd_slam and doesn't require openni_camera (but still ...", "I'll follow your question: I'm using fuerte on a Panda too but didn't even try to compile the whole desktop variant.", "Unfortunately there is (at this time) no repository for ROS on ARM.\nSo no debs.", "Yeah, that's what I assumed, hence the installation from source.  Thanks.", "Ah I see you have it working, very good :) You actually don't even need OpenCV, but it makes things a bit easier (and you probably need it sometime anyway). I thought I had changed it to BULK interface, but I could very well have been mistaking, ISO sounds familiar too ;)", "Hello, have you made any progress with ccny_rgbd? I'd be curious to see if it worked on your setup. If you need help recompiling it against a custom pcl, let me know"], "answer": [" ", " ", "Getting a Kinect to work on a Pandaboard is possible, but I will tell you now that it is computationally very expensive for the Pandaboard to read out the Kinect data. As I said in another question, if you have the Kinect working and sending its data to the Pandaboard, there is little computational power left for SLAM, sending it over WiFi is a bad idea as well. I have not tried to use the Kinect on the Pandaboard with just SLAM, but my experience is that the Pandaboard would get hot rather quickly, scale down its CPU and then you have one very slow brain.", "So to sum up: be careful if you want to do this, with what you let the Pandaboard do as calculations.", "I have a Pandaboard with Ubuntu 11.10 and ROS Fuerte installed, indeed from source as none of the packages are available for ARM.", "What I did was compile the OpenNI drivers from source, using (if I remember correctly) these steps:", "I am not sure if the NITE middleware SDK was mandatory or optional, you can try without. Also, I believe the OpenNI had to be set to use the BULK USB interface for ARM devices, otherwise it wouldn't communicate properly.", "If you got this far, try one of the OpenNI samples and see if they work. If they do, it is just a matter of letting ROS know OpenNI is installed and where it is installed.", "Good luck!"], "answer_details": ["Download the OpenNI source from\n", " and\ncompile it using the ARM compiler and\ninstall it ", "Download the NITE middleware source (for which I cannot find the link at the moment?) and compile it using ARM and install it", "Download the avin2 SensorKinect\ndriver\n(", "),\ncompile it for ARM and install it", " ", " ", " ", " "], "url": "https://answers.ros.org/question/55712/openni_camera-on-pandaboard-fuerte-ubuntu-1204/"},
{"title": "NAO or ROMEO aldebran robot in V-rep", "time": "2013-02-03 22:50:18 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello guys,", "I would like an opinion about the state of the art of 3D simulator. \nI'm going to work with aldebaran robots NAO and ROMEO and I have to choose a simulation environment where to simulate those robots. The main candidates are:", "1) Gazebo / MoveIt", "2) V-rep", "3) Webots", "The real choice is between the first two simulators since they are completely open sources. \nI would like an opinion from you about these two simulators and if there are existing models of the two robots above-mentioned. \nIn addition, I have to mention that I have to use those simulators also for motion planning (that I see this feature is included in V-rep while I have to use MoveIt for Gazebo).", "I really appreciate if you give me an opinion with strength point / drawbacks of each of them from your experience.", "Thanks in advance,", "Neostek", "P.S. Obviously, other simulators are welcome!"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello there,", "Since some time now, Aldebaran Robotics is delivering a SDK-simulator allowing you to connect with any simulator.", "If you need more information, I'll be happy to help.", "Best,", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I do not know very well these simulators, as far as we are concerned, HRP-2 users, we are working with the ", ". This is mainly due to the high fidelity of the dynamics simulation. This is the most crucial part when it comes to human-size humanoid robots. This is crucial for us as these robots are big, costly and powerful.", "If you plan on working on control algorithms for humanoid robots then you should first review if other people doing humanoid robotics are using it and how good is the result. Then comes all the other features (ROS integration, sensor simulation, ease of use). The community also is a major factor.", "The fact that it is bundled with Nao/Romeo should not be a major criteria for your decision, it is not that hard to port a model from one system to another (or even patching an open-source project to load a new file format is not that difficult either).", "Long story short, because of the community size, the fact it is used in the Darpa Challenge (", ") and its nice ROS features among these three simulators I would choose Gazebo. You may also want to consider OpenHRP.", "NB: Another nice robotics simulator is ", ". Unfortunately it is not focused on humanoid robots.", "Just adding an fyi, `OpenHRP3` binary package is now available via ROS repository too (you may expect more frequent updates than its original repo): ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Thank you for your answer. OpenHRP simulator seems to be really interesting and I will take a look over it. I really appreciate an opinion on V-Rep since I expect this might be the new simulator for robotics application but I really need someone that already used it for a better understanding about it. Obviously I'm not afraid in developing my own model in any kind of simulator but I'm searching for some advises for humanoid simulation, since I'm a newbie in this field.", "Thank you again,", "Neostek ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "You may be interested to know that we plan to have support for Nao in ", " in a foreseeable future (required for by the MarDI project that has just started at LAAS-CNRS).", "I can not give you any timeframe, but I know it will be worked upon in the coming month (however, it won't simulate accurately dynamics, like OpenHRP, for instance. MORSE is more suited for high-level control).", "Do you have support for Nao in Morse yet?", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "As far as I know, You can download webots-for-nao from ", "/ with 90 days trial. Aldebaran robotics gives webots simulator with appropriate world files and robot models. "], "url": "https://answers.ros.org/question/54165/nao-or-romeo-aldebran-robot-in-v-rep/"},
{"title": "Wireless camera for usb_cam", "time": "2012-05-09 02:01:36 -0600", "post_content": [" ", " ", "I'm looking for a wireless camera solution that could feed what it sees into the usb_cam node. I want to experiment with ar_pose however the camera is to be mounted on a quadcopter and obviously running a cam would be disadvantageous. Is there a camera system that would work over short range (5-10 m or so) that would permit this?", " experimented with a wireless camera at some point."], "answer": [" ", " ", "You can try the Trek AI ball.  They are kinda expensive ($80) and run through batteries quickly, so I would power them externally.", "There's a ROS MJPEG driver here:"], "url": "https://answers.ros.org/question/33596/wireless-camera-for-usb_cam/"},
{"title": "Turtlebot2 power for laptop", "time": "2013-03-15 01:13:10 -0600", "post_content": [" ", " ", "Hello,", "I would like to use the battery of the Kobuki base of my turtlebot to give power to the laptop. I would like the laptop to take power from the Kobuki only when the Kobuki base is on the loading station. For that I need to activate or not the external power for the Kobuki.\nI found a function \"Command Command::SetExternalPower(const DigitalOutput &digital_output, Command::Data &current_data)\" in the file kobuki_driver/src/driver/command.cpp. And if I am right, this function allows us to activate or not the external power of the Kobuki. But I don't know how to use this function. Is there any documentation somewhere ? or examples ?\nOr should I use another function ?", "Thanks for help.\nCaroline"], "answer": [" ", " ", "The 19V laptop power port passes through from the charging dock or power port any time the Kobuki is docked or plugged in.  There's no need for software. "], "url": "https://answers.ros.org/question/58247/turtlebot2-power-for-laptop/"},
{"title": "Can i use Turtlebot Without Laptop by using Android phone/uc board", "time": "2013-03-06 16:36:10 -0600", "post_content": [" ", " ", " ", " ", "Hi I'm new to this turtlebot.\nI was wondered to see a laptop for turtlebot, why can't we use some embedded board like ", " ", "or ", " instead of laptop,\nand one more thing is that can we use ", " directly connected instead of laptop.\nif anyone already done it, can you give me a link to know it.\nThanks in Advance."], "answer": [" ", " ", "Generally, No.", "ROS is targeted towards linux platforms, and binary packages are provided for Ubuntu x86/x86_64. The further you get from that platform, the more trouble you'll have getting things to run.", "All but the fastest ARM boards lack the CPU power to run the OpenNI (Kinect) driver well, and getting the OpenNI and pointcloud tools to compile on ARM is not easy, and takes a long time. The raspberry pi is the least powerful ARM that I would even attempt to use on a turtlebot, and I wouldn't really expect it to work very well.", "Running ROS on Ubuntu on a phone is possible, but in addition to the issues with ARM you'll also have to deal with limited USB bandwidth and special cables to connect USB devices to the phone.", " ", " ", "Hi Thanks for reply", "I'm able to launch turtlebot using raspberry pi wheezy with groovy, and teleop using workstation, but kinect is over burden on processor."], "question_code": ["                   http://www.friendlyarm.net/products/mini2440\n"], "url": "https://answers.ros.org/question/57359/can-i-use-turtlebot-without-laptop-by-using-android-phoneuc-board/"},
{"title": "Turtlebot bringup problems", "time": "2013-03-19 07:22:12 -0600", "post_content": [" ", " ", "Hello,", "I am having problems with the turtlebot_bringup package on Turtlebo 1 (iRobot Create base). ", "I am using ROS Groovy on both the netbook and workstation, and I have edited the minimal.launch file such that TURTLEBOT_BASE=create, TURTLEBOT_STACKS=circles and TURTLEBOT_3D_SENSOR=kinect. I have made sure that the udev/rules.d had the correct specification and the launch files were indeed pointing to ttyUSB0. But when I turn the turtlebot on and run minimal.launch, the turtlebot turns off automatically, and the launch file keeps popping up errors such as \"No data from sci\", \"Please ensure Create is powered on and plugged in\" etc. The dashboard on the workstation reports that everything is \"stale\".", "Any advice on how to proceed would be very helpful."], "answer": [" ", " ", "This usually happens when the battery is very low or going bad. I would get a multimeter and see what the battery voltage is, if it's close to 12V the battery is going bad."], "url": "https://answers.ros.org/question/58588/turtlebot-bringup-problems/"},
{"title": "Best desktop computer option for ROS", "time": "2013-05-07 11:21:34 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I am planning to buy a new desktop specifically to use ROS and Turtlebot application. Is there a best option to go after. It would be nice to have a machine with Ubuntue and perhaps ROS pre-installed and a graphic card that is going to be the best option for Gazebo and rviz.", "Gazebo requires more power than rviz, for this reason I would ask this question to ", " too If I were you. One has to be quite careful with graphic cards since not all of them work in ubuntu easily"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I use a ", " as my desktop, and have been very happy with it.  Enough power to run Gazebo and some control code, and zero problems with nVidia video card.  Preinstalled with Ubuntu 12.04.  I also have a ", ", which I'm similarly happy with, although it weighs a ton.  The seem to have moved to an Intel graphics card for this, though, rather than the nVidia that I have in mine.", "Thanks for your reply. I was wondering how much graphic card is needed, the default is 1GB nVidia, is it enough. ", "I've got whatever the default option is, and it's fine.  A faster/better card will let you render more polygons, but the critical thing, IMO, is the closed-source nVidia drivers.", "I'm quite happy with my System 76 Serval laptop. ZaReazon also makes a good platform as long as they have the discrete graphics cards. ", " ", " ", "For choosing the desktop machine itself, I would pick one of the machines under the Ubuntu Desktop Certified Hardware list for 12.04LTS. You should be able to find it with your preferred search engine.This will ensure that Ubuntu itself will run well on the desktop hardware (no weird driver issues). ", "I don't think there are any Ubuntu installs with ROS already installed. If you follow the groovy install instructions on the main ROS page, it's a pretty quick, painless install using package manager tools. The guide walks you right through it.  ", "Not sure about the graphics requirements for rviz/Gazebo.", "I have made a custom Ubuntu 12.04 with ROS groovy already installed: "], "url": "https://answers.ros.org/question/62350/best-desktop-computer-option-for-ros/"},
{"title": "ar_pose rotate transform without image rotation", "time": "2013-04-27 10:42:16 -0600", "post_content": [" ", " ", " ", " ", "I have an urdf description of a camera on a robot. On the camera I run the ar_pose detector. The image is correct, but detected location is behind the camera. The problem is that if I change the origin in the urdf the image gets also rotated. How can I get the image and the marker position correctly simultaneously?"], "answer": [" ", " ", "In my experiments, the camera points in the direction of its blue axis. So looking at your frames, it looks like your camera is actually pointing upward. Try to correct that in your URDF and then, if your image is rotated, you can always rotate your camera frame ", " the blue axis until you get it right.", "Thanks for your response, but the camera image obtained at the red axis. Rotating the axis so that the blue axis  is in line with the arm means that the camera is pointing downwards like this: ", "actually that image only confirms that the blue axis is pointing in the direction of your field of view. If it was the red axis, you would see the grid as squares, but instead you see them in perspective. Also you can see the axis just in front of your camera in the image (/arm_shoulder_pivot).", "just rotate your camera frame once more, maintaining the direction of the blue axis, until you can see your floor grid in the base of the camera image (in your camera window). I think the green axis must point downwards. after that, check if the marker position displays correctly.", "Thanks for your help. But the camera then points sideways. ", "ok, now you got your camera rigtht (by the position of the floor and by the arm's joints in your cam  image)! I think the problem now is with your Rviz object. Maybe you are creating your Rviz object with the wrong orientation? Check your code when you create the Rviz marker orientation.", "..Got it  thanx ", "How did you solve it then? ", "sorry ", ", do you mind adding the images from the comments here to the question part (upload to answers.ros.org)? in this way the question would become more informative for future reference and we would not depend on your dropbox. thanks!"], "url": "https://answers.ros.org/question/61672/ar_pose-rotate-transform-without-image-rotation/"},
{"title": "No CameraInfo received using Rviz", "time": "2013-05-13 11:52:33 -0600", "post_content": [" ", " ", " ", " ", "I am trying to get camera data from the turtlebot to my workstation.  The netbook is running 10.04 and the workstation laptop is running 10.10. ", "On the turtlebot netbook I configured the IP settings, and ran the teleop successfully. I would like to now grab camera data from the turtlebot kinect.", "On the netbook I ran", "rosrun turtlebot_bringup kinect.launch", "On the turtlebot workstation I run", "rosrun rviz rviz", "I click add, camera, then under the display bar, under Camera, it says \"Status Warning.\" Image Topic I select camera>rgb>Image_raw. I get an error and No CameraInfo received on [/camera_info]", "I have tried not only Image_raw, but a few of the other selections, and non of them seem to work.", "Any suggestions would be appreciated", "http://answers.ros.org/question/29298/turtlebot-kinect-fails-since-dropped-10000-of-messages-and-wireless-crashed/     you can have a try."], "answer": [" ", " ", "That suggests that the cameara is not streaming data.  Make sure it's breaker is powered on and the kinect.launch does not have any errors. ", "You can also explore what's being published using rostopic echo and rostopic hz ", "all breakers are powered on"], "url": "https://answers.ros.org/question/62645/no-camerainfo-received-using-rviz/"},
{"title": "Depth Camera [closed]", "time": "2013-04-28 09:00:04 -0600", "post_content": [" ", " ", " ", " ", "Hi, im about buying a depth camera for a ROS operated(Fuerte,Groovy) robot, and i would like to hear some recommendations , and differences between Xbox Kinect, Windows Kinect and Xtion from ASUS, Or do u have any other option?\nThank you for your time.", "This is a duplicate, ", "/"], "answer": [" ", " ", "Hi,", "if you plan to use ROS, I would recommand that you don't take the Kinect for Windows. Indeed, a lot of things are made using OpenNi, and as far as I know, it doesn't support Kinect 4 Windows. Moreover, unless you plan on developping with Windows SDK, it appeared to me (when I had to make the same choice as you) that a lot of the development tools are made for Kinect Xbox360.", "Please note that there are two models of Xbox360, as I said ", ", the latest model seems to have some sound-driver wrong, mainly using hark. I contacted the people taking care of hark to let them know, and I'm waiting for their solution.", "Regarding Xbox360 VS Xtion Pro Live, as Dornhege, I have never used the later, so I can't tell you if it's better or worse. I just know it's smaller, and have more or less same characteristics.", "Have a good day,", "Bests regard,", "St\u00e9phane", " ", " ", "Basically they are the same sensor (use Xtion Pro Live).", "The Kinect is larger and needs external power. The Xtion Pro Live operates on USB alone.", "One minor detail: AFAIK the Asus doesn't give out a unique serial.", "I'm not sure about Kinect for Windows."], "url": "https://answers.ros.org/question/61708/depth-camera/"},
{"title": "Changing SMACH states on a service request", "time": "2013-04-28 15:53:43 -0600", "post_content": [" ", " ", " ", " ", "I am using SMACH to create a state machine.  I would like to create a state machine that switches state when a service request is received (i.e. the state machine has a service server).  I have a simple GUI with a START/STOP button.  The GUI executes a service request when the button(s) are pressed.  I would like to change from and IDLE state on START and go back to it when the STOP button is pressed.  Is there a preferred way of embedding the service server in a SMACH state machine and what is the best way to trigger state transitions based on a service request event?"], "answer": [" ", " ", " ", " ", "First, have you thought of making your SMACH state machine controlled by the actionlib API with the SMACH ", "? This lets you start and preempt execution of a SMACH container over ", " actions, which are basically preemptable service calls. ActionServerWrapper wraps a SMACH container in an actionlib server, and then you can \"command\" the SMACH plan with the normal actionlib API. ", "If you really must use a service, then I have two thoughts:", "Additionally, this is a really good use case for event-driven SMACH state transitions (something which I think SMACH could really use, and could be implemented without making SMACH much more complex).", "I will have to consider using the ActionServerWrapper or one of the two other options.  I should be able to make them work for my needs.  I am interested in an event-driven approach as this is probably a better approach.  We can discuss this offline, if I decide to take that approach.", "Yeah, I think an exogenous event interface could be really powerful, in addition to the current endogenous outcome events. Feel free to ping me if you want to talk more about it, also I'll be at ICRA and ROSCon.", "Hey, Any progress on this? I would also be very interested in the event-driven approach."], "answer_details": ["To make a SMACH state that responds to service requests with the current API, you can model a custom SMACH state type similar to the SMACH ", ". This state would represent waiting for a service call, so it would block until it receives a service call, or until it's preempted. Then that service call could cause the state to have a specific outcome, which could start execution.", "To create a wrapper that controls a SMACH plan through service calls, you could build something similar to ActionServerWrapper, but use service calls instead of actionlib.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/61714/changing-smach-states-on-a-service-request/"},
{"title": "rostopic echo to webpage", "time": "2013-06-04 03:50:22 -0600", "post_content": [" ", " ", "How can iI export the output from rostopic echo (for example the battery level) to a webpage?"], "answer": [" ", " ", "You can look at ", " for this. \nOr write a node yourself that generates a html file with the level. ", "Excellent! Thank you for this information. I just checked it out. Does it run with ros electric, too? \nalso, Do you have any experience with Json? What the structure of such a file be, for example to get the data from rostopic echo battery?"], "url": "https://answers.ros.org/question/64292/rostopic-echo-to-webpage/"},
{"title": "Small, cheap, off-the-shelf robot integrated with ROS?", "time": "2013-06-01 16:12:48 -0600", "post_content": [" ", " ", "I want to create an inexpensive testbed for multirobot experiments. In order to do this within my budget, I plan to centralize as much of the sensing and computation expense as possible: a single computer communicating with each robot wirelessly, and overhead kinect providing position and obstacle information. ", "For the robots themselves, I'm thinking something like Pololu's ", " coupled with an XBee. My goal is to spend as little time as possible futzing with the hardware getting it to the point where it reacts to geometry_msgs/Twist messages. Does anybody here have any experience (or better yet, tutorials!) integrating a system like this into ROS? Do you know of other hardware that better suits my needs? Requirements:", "I've seen ", " ", " questions, and ", ", but didn't find anything ideal."], "answer": [" ", " ", "Have you looked into ", "?  It is small, somewhat expensive for what you get ($130), but fully controllable out-of-the-box and readily available.", "There are some ROS packages available (", " and ", "), but I do not know the maturity or activity level of either of these projects.", "Interesting! I think my main concern with this would be my ability to localize it sufficiently well. For a wheeled robot, I'd use some combination of odometry / cmd_vel history + motion model / April tag tracked by the overhead kinect. Does anybody know how well Sphero does at this out of the box?", "I think localization based purely on Sphero sensors would be very difficult, but if you have an overhead Kinect, and the ability to control the Sphero illumination color, it may make for a very easy target to track.", "Oh! duh! I'm very wary of the robustness of color-only tracking algorithms, but combined with extracting spheres from the depth image that should allow tracking multiple spheros at once =) Anyways - I've ordered one, and plan to play with it. (I'm also still in search of better options!)", "You also may be able to just get an Arduino powered platform with the zigbee, especially if you aren't planning on doing to much sensing on board.", " 1) Post this as an actual answer? 2) Do you have a particular platform in mind? (My goal is to absolutely minimize hardware hacking, and I haven't seen anything yet, but there's a lot out there)", "Are there any \"real\" tech specs for Sphero? That webpage is the worst marketing junk I've ever seen. All it seems to be is a glorified bluetooth receiver attached to a couple of motors. Without sensors of any kind, how is it useful for robotics?", "Have you checked the ", "? Yes, it's simplistic, but I think multi-robot control using overhead camera and different-colored spheros would be pretty easy to jump in and get started.  Like ", ".", " ", " ", "I guess if your whole goal was to minimize unit cost, I would consider something like this: ", "It has motors and encoders, which is he minimum that you need to implement closed loop motor speed control on-board.  Includes an ATMega328, and has a spot for zigbee."], "question_details": [" ", " ", "Small: < 6\" diameter", "Cheap: < $200 each", "off-the-shelf: I'm not really interested in designing my own robot and hacking it together with an arduino or Raspberry Pi, but that's an option of last resort \u2026"], "url": "https://answers.ros.org/question/64142/small-cheap-off-the-shelf-robot-integrated-with-ros/"},
{"title": "Workstation as Master [Turtlebot]", "time": "2013-06-18 11:10:39 -0600", "post_content": [" ", " ", " ", " ", " ", "Here is the error I get on the turtlebot commandline:", "[ERROR] [WallTime: 1371589364.129064] Failed to contact device with error: [device reports readiness to read but returned no data (device disconnected?)]. Please check that the Create is powered on and that the connector is plugged into the Create.", "[ERROR] [WallTime: 1371589370.596000] Failed to contact device with error: [device reports readiness to read but returned no data (device disconnected?)]. Please check that the Create is powered on and that the connector is plugged into the Create.", "[ERROR] [WallTime: 1371589377.007524] Failed to contact device with error: [device reports readiness to read but returned no data (device disconnected?)]. Please check that the Create is powered on and that the connector is plugged into the Create.", "[ERROR] [WallTime: 1371589383.468197] Failed to contact device with error: [[Errno 11] Resource temporarily unavailable]. Please check that the Create is powered on and that the connector is plugged into the Create."], "answer": [" ", " ", "This usually means your turtlebot battery needs to be charged. Or that the connection from turtlebot to the laptop is loose.", "Thank you for your reply. I do not get these errors when turtlebot is the master. So, I don't think that the robot is not charged or the connection is loose. "], "url": "https://answers.ros.org/question/65437/workstation-as-master-turtlebot/"},
{"title": "Best tablet to run ROS", "time": "2013-06-19 14:12:59 -0600", "post_content": [" ", " ", "We're thinking of buying a tablet to run a small number of ROS nodes (primarily rviz), to communicate with the ubuntu machine on the robot. I've heard some painful stories about ROS on tablets, so would like to ask if anyone has recommendations on which tablet to get. Are the Android platforms the way to go? Has anyone tried the Ubuntu Tablet? Thanks a lot! "], "answer": [" ", " ", " ", " ", "I've actually been interested in this myself. In it's current state, ROS and Android are not as mature as ROS on Ubuntu Desktop (x86). There is an implementation of the ROS client library and a ROS master in the form of rosjava, but there are no ports of apps like rviz or the ROS nav stack yet for an Android environment. If you rooted your Android device, in theory you could crosscompile and run C++ nodes using roscpp. However it'll likely be a pain as all dependencies that we take for granted in the Ubuntu repositories (e.g. OGRE for rviz) have to be compiled manually. Your app will also not work on a stock Android device. You could also turn your ROS code into C++ libraries and invoke functions from a Java shim via the Android NDK without rooting the device, but that's another world of hurt.", "I would recommend instead getting an x86 based tablet with an i3, i5, or i7 processor and intel hd graphics (e.g Acer Iconia W700). Ubuntu's Unity environment has pretty good multitouch support. The power draw is of course going to be more than an ARM-based tablet, but you'll get much higher performance. Be wary of Atom based tablets based on the Atom Z2760 processor. These use Intel's Cloverfield architecture which does not work with Linux due to the use of a proprietary PowerVR video chipset. The next generation of Atom based tablets should support Linux but that likely won't come out till end of this year.", "Also, I'm not sure if Ubuntu Tablet would help. To my understanding, it's currently only a Developer's Preview. I'm not sure how much more different their environment is from desktop Ubuntu and if their repositories will be as comprehensive. In the future, it may well be the way to go for ROS development on ARM tablets, but I think it's way too early to consider it.", "Thanks a lot for the recommendations ", "! Have you used Acer Iconia W700 for running ROS?", " I agree to every word of ", ", however it may be worth a try to install ROS barebones on a Ubuntu Tablet. I have heard that Ubuntu is installable on various Android tablets, but have not attempted such. ", " On rosjava etc, you can find some ROS apps on play.google (i.e. ", ")", "I have not yet tried the w700, but I have seen youtube videos of people running stock ubuntu on it without any need for hacks. The Intel HD 4000 has fully open source drivers -- though not as beefy as nvidia and amd graphics cards, it should be fine for rviz. "], "url": "https://answers.ros.org/question/65550/best-tablet-to-run-ros/"},
{"title": "Microstrain 3DM-GX3-45 with imu_drivers", "time": "2012-07-23 21:51:47 -0600", "post_content": [" ", " ", "Hi.\nI have Microstrain 3DM-GX3-45, and testing with imu_drivers stack.\n", "However, the error message below shown. Have anyone suffered similar problem?", "Make sure the IMU sensor is connected to this computer. (in microstrain_3dmgx2_imu::IMU:receive) You may find further details at ", "I'm sure that there is no other process using ttyUSB1", "Do you receive any errors in your /var/log/messages when plugging it in?"], "answer": [" ", " ", "The data communications protocol for the various MicroStrain 3DM inertial sensors is different. The 3DM-GX2, 3DM-GX3-25 and 3DM-GX3-35/45 each have a separate data communications protocol. All of these can be found at: ", " The error certainly could mean that the communication cable and/or power is disconnected/broken/mis-wired/etc. A number of other reasons could also cause the error including 1) the command being sent is not the correct command for the particular inertial sensor, 2) the serial port is not set correctly, 3) the command being sent is not structured correctly, 4) the sensor is busy with a previously instructed command, 5) the sensor is damaged, and so forth. A terminal program is often useful to observe the actual TX and RX traffic and troubleshoot the problem. ", " ", " ", "I started coding a new driver based on boost asio for this sensor. Code can be found on ", ". It's not very capable yet - at the moment it can ping sensor and run self-test. Any cooperation will be highly appreciated!", "Driver can now publish some topics... Wiki documentation coming soon ;)", "Driver documentation: ", ".", "Hi, does the package support the ROS indigo now? Thanks", " I haven't tried it under Indigo yet. Our robots run on Hydro. Btw, there is some new API from Microstrain:  ", "  -> it would be great to build ROS driver on top of this. "], "url": "https://answers.ros.org/question/39600/microstrain-3dm-gx3-45-with-imu_drivers/"},
{"title": "Machine Learning and ROS", "time": "2013-06-26 15:42:41 -0600", "post_content": [" ", " ", "Hello", "Im collecting lots of data from my robot runs and I wonder if possible some implementation of some Machine Learning techniques in ROS. Or if I can use some machine learning techniques like Support Vector Machine Classifier or some other in correlation with ROS? I would like to do like shared control of the Joystick of my platform so I thought Machine Learning is a good approach. ", "Thanks"], "answer": [" ", " ", "You can use OpenCV or Matlab with ROS. OpenCV is not so powerful in comparison wit Matlab, but you can find SVM in OpenCV.", "ok.. Any examples or tutorial regarding machine learning with Matlab and ROS??"], "url": "https://answers.ros.org/question/66060/machine-learning-and-ros/"},
{"title": "How to realize image processing with ROS and opencv", "time": "2013-05-28 12:13:13 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I try to realize image processing with ROS and opencv, and I follow the tutorial of cvBridge. Then I really want to know, whether it is true that we can only do the image processing in the callback function of subscribe message? Is there another way?", "As for my experiment, I have realize the image processing with the camera on my PC, then I try to wotk with ROS image,however, there are always problems. So if there is other ways? I don't want to use the callback function to realize image processing....."], "answer": [" ", " ", "ROS is a callback-based system, so on that level, the answer is that you have to do all of the processing in the callback.  You can, of course, work around this by using the callback to store the image somewhere, and the working on it later, or in another thread, or something like that.", "What would help people answer your question, though, is an explanation of ", " you don't want to do the processing in the callback function.  Without knowing what you want to do, it's hard to offer concrete advice.", "In the other question it sounded like there were callback for user input as well as callbacks from the image message that were confusing the OP ", "Didn't see that bit.  I can reopen the other question, if you think it's suitably distinct.", "I think it's essentially the same question, just a little detail came out in the comments.", "Thank you for your answer! In my program, I don't want to deal image processing each time receive a image and the image process for each frame maybe 100ms. So when I try the webcam, I grab the image when I want to use, now I am trying to do that with ROS\u2026 Thank you!", "I am going to be crazy and so worry about my program. Compared with my program of webcam, there only difference in getting the image data. In order to say clearly, let me make an example. I try to draw several boxes in the window of image.", "When I deal with webcam, I can draw several boxes in the window and name them: \"0\", \"1\", \"2\"...When I deal with ROS, I cam only draw the box \"0\" and suddenly it will disappear, then I draw again, it will show box \"0\" again and disapper again.It seems it will refresh. The problem it's like this way.", "Hello Anthony, you can choose the rate at which the camera messages are published. You can also choose when you update the images that you're merging (I assume that you're using something like ", ")", "I have tried several ways with it all the day, and I will try to set the loop_rate as you said! Moreover, if you'd like, you could see my code(http://www.zlzdesign.com/pet_drone.cpp) and help me to find the problem! Anyway, you really do me a favor! I will make effect to make it not refresh!", " ", " ", "It seems that it is a little complicated to process images with ROS and opencv.\nI'm a green hand on this field. But as for myself, I used to process image with the help of a third party image processing SDK.\nIt is just one of many but I do appreciate its simple way of processing.\nEven though I only tried its free trial package and didn\u2032t check the cost and licensing conditions, it works great for me. \nShare with you.\nI hope you success.", "Best regards,\nArron"], "url": "https://answers.ros.org/question/63763/how-to-realize-image-processing-with-ros-and-opencv/"},
{"title": "Can't access turtlebot through ttyUSB0", "time": "2011-09-12 07:33:31 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I think that ROS can't access my Create... Is not possible see informations about the robot in turtlebot_dashboard, like battery and sensors status, but is possible to see netbook battery status, then I can conclude that the connection Workstation-netbook is ok, the problem could be between the netbook and Create. The status of all sensors in turtlebot_dashboard is Stale, except the netbook battery.", "I'm using a serial-usb converter, because my Create uses a serial-serial cable. Could it be the problem?", "Using \"dmesg\" and \"lsusb\" commands, I can see that my converter is properly connected at ttyUSB0. And I've tried to change the default_port in turtlebot_node.py to another USB and serial ports, but the error continues.", "Thanks in advance,\nLucas"], "answer": [" ", " ", "I think I had the same problem with the ATHEN USB serial cable.\nHere is how I solved it:", "Add the following line in the /etc/udev/rules.d/52-turtlebot.rules:", "ATTRS{idProduct}==\"2008\",ATTRS{idVendor}==\"0557\",MODE=\"666\",GROUP=\"turtlebot\"", "2008 and 0557 is either the vendor or product id. You can find that out from lsusb.\nThe problem is that the ttyUSB0 does not have the approprite permission.", "I had a similar problem. I followed the instructions here, and got my turtlebot working for a day. I shutdown everything at the weekend. On monday,  I was back to zero. I am getting \"Failed to open port /dev/ttyUSB0\" What can cause this? ", "Hi is GROUP=\"xxx\" referring to the login account?", "Bus 003 Device 005: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\n\nWhat shall be my required code ? and could u tell me the parameters for GROUP and MODE ?", " ", " ", "I had a similar problem, and I'd like to add something to dvbot's extremely helpful response.  I couldn't communicate to the my turtlebot netbook, and my Turtlebot Dashboard was all gray.  Under Diagnostic, I had the error:\n", " ", "When trying to implement dvbot's recommendation, I realized my turtlebot netbook didn't have the file /etc/udev/rules.d/52-turtlebot.rules, and I did not have permission to create it.  I went ", ", and found good instructions from tredegar (halfway down the page) in how to overcome the problem.  For your convenience, I am recopying some of tredegar's recommendation.  To become root issue the command:", "Then to create the file you can use the command", "or use your favorite editor.  Once you populate the content with ", ", restart udev:", "I'm still very new to this, but I do know this worked for me: once I restarted udev, my Turtlebot Dashboard went from gray to greens and reds.  Both my workstation and turtlebot netbook are running Ubuntu 11.10 and ros electric.   Hope this is useful.", " ", " ", " ", " ", "If you are still having this problem, I resolved it by changing adding the turtlebot user to the 'dialout' group. That group owns /dev/ttyUSB0 so by being a member of that group you are added to the ownership. It doesn't affect any other permission at least so that U can detect at this point. I'll keep you posted, unless you already fixed it using another method. Just sharing the info as I find it.", "how to do it?", "how to add the turtlebot user to the 'dialout' group", " ", " ", "You could check the permissions of ttyUSB0", "If the perms are not 666 or a+rw then you need to boot with usb that has permission to access. Basically you need to create a new rule in /etc/udev/rules.d:", "Create a new file with your specific permissions (higher numbers override existing rules):", "Add the following lines for every usb device:", "To find the product id and vendor id, do lsusb -v, I think if you just do regular lsusb its vendor:product. Shift + PgUp to view previous parts of the cli.", "finally reboot", "I have a similar issue with a Create2.\nI found /etc/udev/rules.d/99-com.rule\nWhat GROUP should be added to #serial usb line?"], "answer_code": ["sudo -i\n", "gedit /etc/udev/rules.d/52-turtlebot.rules\n", "/etc/init.d/udev  restart\n", "ls -la /dev/\n", "cd /etc/udev/rules.d/\ntouch 80-turtlebot.rules\nsudo pico 80-turtlebot.rules\n", "#serial usb\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"6001\", ATTRS{idVendor}==\"0403\", MODE=\"0666\", GROUP=\"turtlebot\"\n", "sudo reboot\n"], "url": "https://answers.ros.org/question/11185/cant-access-turtlebot-through-ttyusb0/"},
{"title": "Failed to open port /dev/ttyUSB0", "time": "2012-10-25 06:28:51 -0600", "post_content": [" ", " ", " ", " ", "hey, i got some problems with getting my turtlebot started.", "to be a bit mor clear i tried ", " whitch presents me this message as well as the Dashoard as well as if i type ", "4.i tried to solf this problem using with this possible solutions", "but nothing changed", "again more clear i changed the chmod of /dev/ttyUSB0, i tried udev rules, i have tested if there is an connection to this port with ", " and ", " as well ass  ", " ", ". ", "All confirm me my serial device converter (needed for the connection from pc to rommba cleaner) is attached to ttyUSB0", "My anybody got some advices for me?", "edit: ", "First, post the output of the commands. Second, try opening the port in gtkterm or similar (or even cat first) to see if it can be opened in principle and send some chars.", "i am dont know how i format the terminal output properly .. how do i do this?", "gtkterm or here? Here, you can use <pre> tags or for code indent by 4 spaces.", "I assume gtkterm is similar to minicom, but I always try minicom first.", "To me it looks like everything is OK. Can you try just opening it using gtkterm or similar to see if you can open the port with the same parameters. If that works the only explanation I have is that the error msg might be misleading, i.e. the port could be opened, but commnucation fails (bad cable, protocol, etc.)", "According to the error msg you added, there is a comm error. So, I would check at the other end, i.e. is there only one USB device (i.e. might it just try to connect to something else), is cables, hardware, etc. OK.", "im not sure how i have to use gtkterm to open it how i do this? ... no there are no other usb devices connectet.", "at this problem ", "/ they say they \"recommend to check roomba stack\" .. what does this mean?"], "answer": [" ", " ", " ", " ", "Did you reload udev rules after adding new ones? Are you sure rules were propertly added? There is still something wrong with permissions.", "Post output of ", " and ", "First you need to be in dialout group, then make sure tty is owned by this group.", "To add yourself to group:", "Then relog / restart X server / reboot to make sure", "udev rule:", "crw-rw---- 1 root dialout 188, 0 May 30 17:05 /dev/ttyUSB0", " ", " ", "You could check the permissions of ttyUSB0", "If the perms are not 666 or a+rw then you need to boot with usb that has permission to access. Basically you need to create a new rule in /etc/udev/rules.d:", "Create a new file with your specific permissions (higher numbers override existing rules):", "Add the following lines for every usb device:", "To find the product id and vendor id, do lsusb -v, I think if you just do regular lsusb its vendor:product. Shift + PgUp to view previous parts of the cli.", "finally reboot", " ", " ", "I fixed it by comment out code like below.", "PySerial opens port in constructor, so you need not open explicitly.", " ", " ", " ", " ", "thanks to all of you for your help i think the problem changed somehow from ", "failed to open prot devttyusb0 to: failed to contact device with error [Error reading from SCI port. No Data ] Please check if create is powered on and the connector is plugged into into the create ", "so i think this problem is solved i will open a new question and accept this on as correct ", "ps: udev rule looks like yours, the user owns the group dialout", "ls-al for /dev/ttyusb0  crw-rw---- 1 root dialout 188,0 ...."], "question_details": [" ", " ", " ", " ", "have and ubuntu 11.10 running with ros on asus eeepc.", "started with the turotials at ", "at  the tutorial turtlebot_bringup i got the error Failed to open port /dev/ttyUSB0"], "question_code": ["rostopic echo /diagnostics", "roslaunch turtlebot-bringup minimal.launch", "lsusb", "dmesg | grep usb", "cd /dev/serial/by_id/", "ls -al", "lsusb", "Bus 004 Device 003: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC\n", "dmesg|grep usb", "[  130.917744] usb 4-1: FTDI USB Serial Device converter now attached to ttyUSB0\n", "cd /dev/serial/by_id/``ls -al", "lrwxrwxrwx 1 root root 13 2012-10-29 15:07 usb-FTDI_FT232R_USB_UART_A900Y9QC-if00-port0 -> ../../ttyUSB0\n", "rostopic echo /diagnostics", "roslaunch turtlebot_bringup minimal.launch"], "answer_code": ["ls -la /dev/ttyUSB*", "groups", "gpasswd -a yourUserName dialout\n", "ATTRS{idProduct}==\"6001\",ATTRS{idVendor}==\"0403\",MODE=\"666\",GROUP=\"dialout\"\n", "ls -la /dev/\n", "cd /etc/udev/rules.d/\ntouch 80-turtlebot.rules\nsudo pico 80-turtlebot.rules\n", "#serial usb\nSUBSYSTEM==\"usb\", ATTRS{idProduct}==\"6001\", ATTRS{idVendor}==\"0403\", MODE=\"0666\", GROUP=\"turtlebot\"\n", "sudo reboot\n", "$ roscd create_driver/src/\n$ diff create_driver.py create_driver.py.org \n213c213\n< #self.ser.open()\n---\n>     self.ser.open()\n"], "url": "https://answers.ros.org/question/46790/failed-to-open-port-devttyusb0/"},
{"title": "Can I access the absolute or parent namespace of a node from within a launch file?", "time": "2011-11-27 04:36:21 -0600", "post_content": [" ", " ", "Hi!", "I'm wondering if there is a way to determine a node's absolute or parent namespace from within a launch file at run-time.  The reason why I would like to be able to do this is as follows.", "First, I use many nested launch files, which contain groups with their own namespaces.  For this to work, my launch files need to avoid explicitly setting absolute namespaces, which maximises launch file reuse.  (Note that reading absolute or parent namespaces at run-time, on the other hand, would not reduce reuse, so it's not a problem.)", "Second, my nodes always read and write parameters and topics within their own private namespaces.  This allows intuitive grouping of parameters and topics, and since all nodes always do the same thing it's easy to keep track of things.  But most importantly, this avoids polluting the namespace in which the node was started; in particular, this allows multiple nodes to publish identically named topics for the same robot or namespace (without requiring me to look for and remap all potential conflicts).", "For example, imagine we have a robot named Marvin, with its own group/namespace \"marvin\".  Now image that marvin uses a robot base controller node called \"base_controller\", and an IMU-based position estimation node called \"imu\".  The odometry topic produced by the base_driver might be \"/marvin/base_controller/odom\", and the IMU node might also produce an \"odometry\" estimate \"/marvin/imu/odom\".  Without the private namespace we'd have to rely on remapping to achieve unique topic names like \"base_controller_odom\" and \"imu_odom\", which doesn't scale and gets very messy (what about the second IMU?).  Namespaces seem to be a perfect way to avoid this.", "The problem arises, of course, when I want to connect two nodes to the same topic.  Since both publishers and subscribers will interpret all names as private by default, I can only pass topic names between them if I use absolute paths, or paths which are relative to a common parent node.", "For example, if a navigation node, which uses its own \"/marvin/navigation/\" namespace by default, wants to read from Marvin's base controller's odometry topic \"/marvin/base_controller/odom\", I either need to tell the navigation node to read \"/marvin/base_controller/odom\", or I need to tell both to use something like \"/marvin/odom\".  In either case, the desired topic needs to be specified as a global name, or it needs to use a common parent, in order to override the private namespaces used by both nodes.", "...", "Solution 1.  For now I solve this in what I guess is the standard way, by not using private namespaces for topics.  I still use the node's name internally to put everything in the private namespace by default anyway, but I can then remap topics relative to the node's parent namespace in the launch file.  But this causes a bit more work for each node, and I really like the idea of every node using its own namespace ..."], "answer": [" ", " ", "The recommended solution is to define parameters in each node's private namespace, but define topics in the ", ". While parameters are typically specific to each node, topics are shared between multiple nodes. Publishers and subscribers   generally do not need to know who provides or consumes the data, the topic name is sufficient.", "So, in your example, ", " should provide ", " for Marvin. Each node will access it using the relative topic name, \"odom\". This is automatically be resolved in the appropriate namespace without any special handling in the nodes or launch files.", "I do think that OP had a point. Imagine two sub-modules, with their own launch files, that I want to include in separate namespaces into a another module launch file, especially in simulation. Remapping between these sub-namespaces is impossible right now, since the absolute names are not known.", "Thanks for your belated support. ;)  Your example is the type of thing I was getting at.  Launch file reuse and topic remapping could be much more powerful than they are.", "If you are convinced you need this feature, please open an enhancement request for roslaunch. You can link it to this page to avoid repeating everything.", " might be related.", " ", " ", " ", " ", "A current hacked solution we came up with: Keep track of the global namespace by always passing down a ", " to every included launch file, and include a ", ". For example, with a simple two-subnode case, being called from an upper launch file.", "The upper level launch:", "And then in a lower level launch:", "In this way, you are always using the global-remap method, which always works, and you can reference the global namespace without actually knowing it a priori. This allows you to build up launch files with multi-level includes, and \"cleanly\" remap at the appropriate points. The best place to remap then is always at the lowest namespace where two topics are common, and no higher."], "answer_code": ["/marvin/odom", "nav_msgs/Odometry", "<arg name=\"ns\" ...>", "<arg name=\"ns\" default=\"\"/>", "<launch>\n  <arg name=\"ns\" default=\"\"/>\n  <remap from=\"$(arg ns)/node1/chatter\" to=\"$(arg ns)/node2/chatter\"/>\n  <include file=\"talk_sub.launch\" ns=\"node1\">\n    <arg name=\"ns\" value=\"$(arg ns)/node1\"/>\n  </include>\n  <include file=\"listen_sub.launch\" ns=\"node2\">\n    <arg name=\"ns\" value=\"$(arg ns)/node2\"/>\n  </include>\n</launch>\n", "<launch>\n  <arg name=\"ns\" default=\"\"/>\n  <node name=\"talker\" pkg=\"play\" type=\"talker.py\"/>\n</launch>\n"], "url": "https://answers.ros.org/question/12105/can-i-access-the-absolute-or-parent-namespace-of-a-node-from-within-a-launch-file/"},
{"title": "Kinect extrinsic calibration (between depth and built-in cameras)", "time": "2013-05-15 10:33:29 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I am having some problems with this tutorial: ", " ", "I calibrated both the RGB and the Depth camera on the Kinect, however, when I try to do this tutorial. I always get an error which says \"Timed out waiting for checkerboard\".  I tried to find this error, however, I don't see anyone else having this problem.", "I'm not sure if it is because the tutorial is broken for ROS fuerte.  Anyone else having problems getting this to work?", "Or if there is a better calibration method out there on ROS to calibrate the kinect.  Note:  I'm trying to get a better calibration than the manufacture.  This is because I need more accuracy (millimeter range) than what is given (centimeter range).", "EDIT:\nI tried to use the Contrast Augmentor using this command:", "rosrun contrast contrast_augmenter image:=/camera/ir/image_raw", "Then the input to the calibration was:", "roslaunch camera_pose_calibration calibrate_2_camera.launch camera1_ns:=/camera/rgb_bag camera2_ns:=/camera/ir_augmented", "Its the standard checkerboard so I should not need to give rows and columns.  I did not have to when I did the calibration for the RGB Camera and IR Camera.", "I still get the error on the screen \"Timed out waiting for checkerboard\".  Am I missing something here?", "EDIT_2:", "I tried adding in the Checkerboard size and the number of columns and rows.  Still can't be seen.  ", "EDIT_3:\nimage:", "on both RGB and IR?\nWhat version of ROS are you using?", "I'm using fuerte. I'm only running the Contrast Augmentor on the IR image. Also I'm getting the error message on both the RGB and IR image.", "strange. Yes, the Contrast Augmentor is only for IR. This packaged seams dead and without a substitute. I tried to use it on groovy (compiled from the source) and there is a error with cv_bridge on a conversion.", "Well if the package is dead.  I was able to get the intrinsics.  Still need the extrinsic though.", "Can you try to put checkerboard more closer and horizontal. It seemed a bit small to me.", "That was just one instance.  When I calibrated separately, that seemed like the best and stable position in both cameras.  I moved it closer and change its position, still the same."], "answer": [" ", " ", " ", " ", "The problem was solved by reading this website: ", ".", "The most important part from this sequence was this line: \n\" At the time of writing, the camera_pose_calibration package doesn\u2019t work for us. There is a patched version that can be gotten from: ", " \"", "For some reason, the code-base on git-hub has some fixes which are not included in the version on ROS.", "Is this still the case, that it is not updated?", " ", " ", "Try using this: ", "The theory is that there is a bug somewhere on the image pipeline (there was a topic explaining that), if i'm not mistaken the imageshow is using the 8 LSB and the checkerboard is using the 8 MSB, so you need to shift the bits to transform the LSB in MSB. The node that i liked does that for you.", "Don't forget to make that the new input to the camera calibration program", "Thank you for the tip.  I'll try this out and report the results.  I think the tutorial should be updated if this is a common problem.", " ", " ", "Hello there! Sorry for bringing an old thread back to life, but we just calibrated some of our Kinects and also had kind of a bad time (Ubuntu 12.04 + ROS Fuerte from packages). \nThis thread helped though, the contraster turned out to be required.", "We have put all necessary steps into a wiki page available here:\n", "\nHope it helps the community!", "Hi, thanks for pointing out your site. it helps me a lot. I followed your steps for extrinsic parameters but i'm stuck at the optimization phase. camera_pose can detect the chessboard in both images, both bars become full green but no optimization starts...hence no extrinsic_bag file is generated. What i'm missing?? Could you help me? Thanks a lot.", "Sorry, I saw the answer late.\nI had the same problem using Groovy instead of Fuerte. Try with Fuerte then.\n\nAnother solution is the IT crowd meme: \"Have you tried turning it off and on again?\" : try restarting the whole calibration process."], "url": "https://answers.ros.org/question/62834/kinect-extrinsic-calibration-between-depth-and-built-in-cameras/"},
{"title": "How to use multiple kinects on the same USB 2.0 bus", "time": "2012-04-12 21:12:36 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi everybody, I got two questions about kinect driver (OpenNI,freeKinect under Ubuntu or even Kinect SDK under windows, what ever works. Kinect here refers to Kinect for Xbox, not Kinect for windows which is more expensive but support up to 4 kinect on same PC) ", "Al though I got 3 independent USB 2.0 BUS, when I connect 2 Kinects to my PC, they always goes to the same USB BUS", "1 is it possible to shut down Kinect_1 temporarily, to get IR streams from Kinect_2 on the same USB bus? For example, I want to do the 3D 360 degree scan, and I need 3 kinects to work together, but I only need them to work in 3 frames (which lasts about 0.1 second), each gives out 1 frame of point cloud.", "2 is it possible to reduce the information that sent from kinect to PC, so that I can use several kinect at the same time without any one of them using up the bandwidth? For example, to do the 3D 360 degree scan, I only need 3 different point clouds from 3 different angles of the same object, but not anything else like RGB stream.", "part of my lsusb results(to make sure I really had 3 usb 2.0 bus on my laptop): "], "answer": [" ", " ", "As far as I know, this would require modifying some stuff in openni_camera. Perhaps eventually USB3.0 will be supported and we won't have to worry about saturating a single USB bus anymore.", "If you do end up modifying the driver to meet your needs in a reusable way, make sure to submit a patch. I'm sure this sort of thing would be useful to others.", " ", " ", "Each Kinect needs a separate USB bus dedicated to it. That's not USB ports it actual circuits. I've tried powered USB hubs to see if that helps but it's the bus addressing that's the key.  I'm running two Kinects on my macBook Pro (2006).  THe only other thing might be to get a good PCI-E USB card running USB 3. That might work on a PC. But it needs separate bus addresses for each camera. Remember you can have a USB hub with a lot of ports on one bus. That doesn't help. ", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "To my knowledge, the USB bandwidth allocation is handled between the driver of the respective device and the host OS. If you plug a device in, the driver asks for a certain amount of bandwidth. Based on the currently available bandwidth (total bandwidth minus bandwidth reserved for already connected devices, overhead, ...), the host OS either grants the bandwidth request of the new device or denies it, in which case it won't work.", "EDIT: For additional information about bandwidth allocation, see ", " and ", ". And a ", " on Kinect on USB 2.0 and 3.0.", "So for solution 1, you'd need to dynamically disconnect one Kinect (unload driver, free bandwidth) in order to connect the next one. Solution 2 won't work without modifying the driver itself.", "Regarding your laptop, it's possible that all external USB ports are internally connected to the same root hub.", " ", " ", "Update to Ubuntu 12.10, in which USB 3.0 is solved... It is, in my opinion and experience, the fastest way to solve your problem.", "Let us know about the results.", "Bests regards,", "St\u00e9phane", "I don't think this will solve the problem if he's only dealing with USB 2.0 root hubs?", "For me, it made the difference, even on usb 2.0...\nBut if phillip is right (and I'm sure he is), then maybe both the gestion of usb bandwith and the driver  have been changed in 12.10 ?... To be continued..."], "question_code": ["Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\n"], "url": "https://answers.ros.org/question/31735/how-to-use-multiple-kinects-on-the-same-usb-20-bus/"},
{"title": "Where to start programming with Kinect or Asus Xtion Pro in ROS?", "time": "2013-08-09 02:07:02 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hello,", "My problem is I need to use Xtion Pro live to get gesture data on ROS fuerte.", "What would you suggest me to try?", "1) PCL is powerful blah, but I am very confused when it comes to understand PCL tutorials, as if they are meant to be used directly, instead of making an example to programmers. Besides, how to convert PCl tutorial to ROS environment is a bit unknown, even after following ROS PCL tutorial.", "2) ROS supports OPENNI. Though, I could not see any example on \"how\". Besides, sources on programming with OPENNI is a bit limited. Does anyone know a good source or where to start. ", "3) There is a very very good book on programming in SimpleOpenNI. Though, examples are coded in Java. Does ROS support Java? I assume it does, but does it also support SimpleOPenNI as well? How?", "Actually I have seen many questions regarding this Kinect-Ros-Openni-PCL issue.Though I did not see a clear answer. I wish there was a good tutorial on those. If someone could answer, I am sure you will not me only helping me. Thanks in advance.  "], "answer": [" ", " ", "as far as i know, the pure pcl library has different kinect driver with ros.\nso using pcl_ros package is the best choice if you try to use ros. You may examine the the perception_pcl, inside which there are some basic functionalities like filters and segmentation, though it will not be powerful as the orginial pcl libraray.", "for how to get started with pcl in ros, you may refer to ", ", which may help"], "url": "https://answers.ros.org/question/71947/where-to-start-programming-with-kinect-or-asus-xtion-pro-in-ros/"},
{"title": "Rosserial and Arduino DUE", "time": "2013-03-13 12:27:58 -0600", "post_content": [" ", " ", "Hi,", "I tried to compile a .ino for my Arduino DUE with rosserial package and Arduino IDE-1.5.2, but this warning happens :", "And that is surely why I can't communicate with serial port when I use ROS :", "However, the code compiles and works with an Arduino MEGA.", "Does anyone had this problem before ?", "Have you found a resolution to your problem?"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "It is necessary to use UARTClass or USARTClass (which are subclasses of HardwareSerial). See ", ". I haven't gotten Rosserial communicating yet, but my progress notes are here: ", "Got it working. See also ", " .", " ", " ", "The Arduino DUE is based on a different chip than the other Arduinos (ARM Cortex M3 rather than AVR). There are several tickets open for other ARM processors on limitations with in the rosserial framework that are affecting them, so there could be a variety of issues.", "I have a chipkit max32, that is powered by a pic32 chip using Mpide 23. Is this the reason that rosserial says  \"Lost sync with device\"? ", "Very possible, I don't use PICs and so I can't speak to the architectural differences in the pic32. "], "question_code": ["ArduinoHardware.h:68: warning: undefined reference to `HardwareSerial::begin(unsigned long)'\n", "Lost sync with device\n"], "url": "https://answers.ros.org/question/58053/rosserial-and-arduino-due/"},
{"title": "Navigation: oscilation on local planner", "time": "2013-08-15 15:59:00 -0600", "post_content": [" ", " ", "Dear community,", "I am working on a ", " (differential drive), using a ", " and the ", " stack (ROS Groovy on Ubuntu 12.04) for autonomous navigation.", "I have ", " of the robot and I can get a fairly ", " of the environment using ", ". Once the map is saved, I can joystick the ", " around and it ", " on the map. ", "Then I use Rviz to ", " to the robot, the ", " does its job and ", ", the robot starts to move and ", ", but it does so with something that can only be described as \"flair\" (and a lot of it!!)", "I have read carefully all the documentation on ", " and especially the ", ".", "I suspect that the problem might be in the configuration parameters of the local planner (maybe the acceleration and speed limits, but I am using the values that are supposed to be appropriate from my robot) and ", " a bit less \"flairly\" by ", ", from the default 6 meters, ", ". But ", ", the robot ", ".", "You can see ", "Is there any way to solve this?   As I mentioned, I read all the documentation from head to tail and still can figure it out. Posting this question is my last resort, so I would deeply appreciate any advice or answers.", "Thank you so much in advance."], "answer": [" ", " ", "From your video, it seems you are having problem with localization, as your laser scans (in red) are moving a lot. Although you say you have good localization, as far as I understood, when you test it you are not using the navigation stack. That puts an extra load on your hardware, which may result in bad localization, when everything is running together (I had similar problems running simulations, that would go away when using more powerful machines, without changing any parameters).", "If you can use a more powerful hardware, I would give it a try. If not, try to relax some of the navigation stack constraints, specially the ", ". Try reducing the simulation time, the number of samples and the loop frequency. ", "I hope this helps... ", "Thanks a lot! Yes, when I test the odometry the navigation stack is not running and the hardware I am using is not powerful at all. I will try to get my hands on a kick-ass machine, but first I will try your suggestion on relaxing some navigation constraints.", "You were right, relaxing those constrains (specially the loop frequency), the robot could navigate smoothly along the global path.", "Hi, I have two doubts; 1. I think first one is your move_base.launch file. Why you have included \"remap from=\"cmd_vel\" to=\"base_controller/command\"\n\n2. Instead of move_base_params.yaml; I have added base_local_planner_params.yaml which have similar parameters except first four lines (controller_frequency, oscillation_timeout etc). Is it make a huge difference in autonomous behavior? Where you have set the FORWARD SIMULATION PARAMETER?\n\nPlease Reply..", " ", " ", "I would play around with the first couple ", " (esp. pdist_scale and gdist_scale)", "David, thanks for your suggestion, I will play with it and update with the findings"], "question_code": ["<launch>\n\n  <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <remap from=\"cmd_vel\" to=\"base_controller/command\"/>\n    <rosparam file=\"$(find beego)/config/LRF/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"$(find beego)/config/LRF/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"$(find beego)/config/LRF/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find beego)/config/LRF/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find beego)/config/LRF/move_base_params.yaml\" command=\"load\" />\n  </node>\n\n\n</launch>\n", "obstacle_range: 3.5\nraytrace_range: 4.0\nfootprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.35, 0.0], [0.25, -0.25]]\ninflation_radius: 0.65\n\nobservation_sources: laser_scan_sensor\n\nlaser_scan_sensor: {sensor_frame: /laser, data_type: LaserScan, topic: /scan, marking: true, clearing: true, expected_update_rate: 10.0}\n", "local_costmap:\n  global_frame: /map\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 1.0\n  height: 1.0\n  resolution: 0.05\n", "global_costmap:\n  global_frame: /map\n  robot_base_frame: base_link\n  update_frequency: 5.0\n  publish_frequency: 2.5\n  static_map: true\n  width: 0.0\n  height: 0.0\n", "controller_frequency: 10\ncontroller_patience: 15.0\noscillation_timeout: 10.0\noscillation_distance: 0.5\n\nTrajectoryPlannerROS:\n  max_vel_x: 0.45\n  min_vel_x: 0.1\n  max_rotational_vel: 1.0\n  min_in_place_rotational_vel: 0.4\n\n  acc_lim_th: 1.0\n  acc_lim_x: 0.5\n  acc_lim_y: 0.5\n  path_distance_bias: 50.0\n  goal_distance_bias: 0.8\n  holonomic_robot: false\n"], "url": "https://answers.ros.org/question/73195/navigation-oscilation-on-local-planner/"},
{"title": "how to use ethzasl_icp_mapping with velodyne?", "time": "2013-08-28 23:53:42 -0600", "post_content": [" ", " ", " ", " ", "Hi all,\nI am using a .pcap file to playback velodyne HDL 64E_2 data using the velodyne ros package. I would like to obtain some odometry data using the velodyne and the ethzasl_icp_mapping package seems to be a good start. I am able to register pointclouds using a kinect, but with the velodyne I have no output on /point_map. My launch file is as below. I am not using any filters or icp config file and just want to try out the package with default parameters. Is there anything wrong here?", "If there are any other techniques that can do laser odometry, kindly suggest"], "answer": [" ", " ", " ", " ", "I have not tried that combination. Several questions and comments:", "Are you using a recent version of the driver? It has fixes for the 64E S2. Are you running Hydro or Groovy?", "You should verify the speed of your device. I've seen examples at 300 RPM, but not 200 RPM, I don't think that is an option. Velodyne provides a procedure for configuring the speed, consult the manual if you need or want to change it.", "Using ", " rather than nodes would probably reduce your CPU overhead noticeably.", "You should use the correct angles calibration for your device, ", " is for an older model than your S2. The ", " script will convert the ", " provided by Velodyne with your device into the YAML format used by the velodyne_pointcloud package.", "The cloud node publishes data in the \"/velodyne\" frame to the ", " topic, ", " ", ". Use ", " or ", " to verify what is actually being published. Also, ", " will tell how often messages are actually sent.", "The driver does ", " publish a transform from \"/velodyne\" to \"/odom\". How could it know that? You do need to provide one yourself.", "Since your device is moving, something needs to transform \"/velodyne\" points into the \"/odom\" frame. Normally that would be done via the ", " instead of the cloud node. It transforms each packet as it's received, to minimise \"smearing\" of data as the device revolves. Unfortunately, that presumes you have a separate odometry source, publishing that transform for the conversion.", "I am not sure what to recommend for the circular odometry dependency: the driver needs odometry for transforming data into a fixed \"/odom\" frame, but that's what you are trying to generate. If the vehicle is moving slowly enough and the device is rotating quickly enough, you may be able to get away with assuming the point cloud generated for each rotation is \"instantaneous\". ", "I am open to suggestions for how to change the driver to better support your use case. Perhaps we could write a different transform node and nodelet that would better suit your requirements. It is not clear to me how that would work, however.", "I would expect other rotating LIDARs to exhibit similar problems. I would appreciate suggestions from people with experience doing similar things with those devices. Heading changes tend to cause more trouble than forward motion.", "Hi, first of all, thanks a lot for the work on the driver. I do have issues running some pcap files (e.g. ", "), but thats off topic. Yes, I normally run it at 900 rpm and it looks fine. I get errors on \"unable to open calibration file\", the 64e_utexas is the only working one i have. I am using the hydro package in ubuntu 13.04.", "Thanks for the suggestion on the rostopic. Thats probably what is wrong. Will try it out. I think the driver as is, serves its purpose; and no feature enhancement is required.", "I hope you're right. I am interested in knowing the results of your work.", "The latest Hydro version 1.1.1 provides a `params/64e_s2.1-sztaki.yaml` configuration file. That S2.1 is more similar to your S2 than our old 64E from 2007. I would try that, instead.", "The manual says your model supports spin rates from 300 to 1200 RPM. That higher, 20Hz, speed should help minimize rotational \"smearing\" for your application.", "Will post any updates here. Regarding the calibration file, I cannot load 64e_s2.1-sztaki.yaml. I will verify if I doing it correct 2moro.", "The problem was with the wrong ros topic as you suggested. It should just be /velodyne_points. Thank you.", " ", " ", "We didn't try yet the node ethzasl_icp_mapper with a Velodyne 64 (and never used pcap format) but we have some appropriate configuration files for the Velodyne 32. You can have a look at the folder ./launch/artor for some examples or directly \n", ".", "Some of the results can be seen here:", "Once you confirm that your topic names are correctly links, I'm foreseeing some other challenges at the registration level:", "The default configuration of the node might have some problem due to the concentric circle generated by the 64 beams. I really suggest you to start with the config files in ./launch/artor", "The point cloud that you will give to the topic \"cloud_in\" should optimally be from an assembled point cloud of 360 degrees. If you want to do it well, you would need an estimate of the platform motion. For a quick implementation, you can assume that the platform is static during one Velodyne revolution.", "The bandwidth use by that sensor is very large. Don't expect real-time capability at the beginning so you should replay your data at a slow rate to see what is going on.", "I hope that will put you on the good track!", "Very cool! Was your work done using the ROS Velodyne driver?", "I used the same parameters and filters used for artor, but with a 64E_2 laser. The program ran, but I did not see any associations or map built. This could be due to vehicle speed or my overlap parameters. Here is my data log with a \"no sensor noise or normals found\" warning. ", ": the link you provide doesn't seem to work.\n\n\n@ joq: I'm only supporting the platform for the 3D mapping but I think they use their custom version of the driver that is floating around the lab since some years. I will direct them to your node so they can have a look.", "It works: ", ". Unfortunately this page adds a (/p) to the link. So plz remove it if this is the case.", "Thanks to all for your support. I can get odometry with artor configuration with no problem. Due to the speed of the velodyne (1200rpm) and speed of the vehicle(10kmph), the map is lost, Nevertheless I was able to get fairly good odometry information. This was tested on a quadcore i7. I guess for higher speeds, a more powerful pc is required.", "I am using groovy  and velodyne 32E for ethzasl_icp_mapper, but it keep on showing this error\nERROR: cannot launch node of type [point_cloud_converter/point_cloud_converter]: point_cloud_converter\nartor file in launch folder i.e. artor_mapper.launch mentions this <remap from=\"cloud_in\" to=\"/velodyne/assembled_cloud2\"/> which really doesn't exist.", "Yes, the launch file that you are using is specific for a platform that we have in our lab and not intended to be used out-of-the-box. I suggest you to copy the launch file and adapt what your need.", "[follow up] In its current configuration, Artor produces PointCloud msgs and not PointCloud2, which explains the extra node point_cloud_converter. You will also need to remap cloud_in to your own topic.", " ", " ", "Hi,\nI have used ethzasl_icp_mapping with tilting laser-scanner. But I have not worked with velodyne. Do the velodyne nodes you are launching provide a ", " from the \"/sensor_frame\" to \"/odom\" frame? ", "In your case \"/odom\" frame and \"/sensor_frame\" are both velodyne. ", "The node ethzasl_icp_mapping requires the transform \"/sensor_frame\" to \"/odom\" frame. ", "It basically requires some initial guesses for odometry. The package provides uses this guesses and provides corrected odometry using ICP as far as I know.", "Hi, I do not have any odometry. I also tried giving a zero tf from /velodyne to /odom, but dint help. The openni example in the stack works without any odometry or tf though", "Hey, is odometry input (wheel, GPS, IMU) really necessary for node ethzasl_icp_mapping to work? I tried the node with velodyne 32E pcap file without any odometry input. But it couldn't output direction or move forward for more than 5m. After which it stopped at a point and kept adding points at the same place."], "question_code": ["  <node pkg=\"velodyne_driver\" type=\"velodyne_node\" name=\"velodyne_driver\">\n    <param name=\"model\" value=\"64E_S2\"/>\n    <param name=\"pcap\" value=\"/home/hope/Downloads/velodynepcap/S227_target.pcap\"/>\n    <param name=\"read_once\" value=\"false\"/>\n    <param name=\"read_fast\" value=\"false\"/>\n    <param name=\"repeat_delay\" value=\"10000.0\"/>\n    <param name=\"rpm\" value=\"200.0\"/>\n    <param name=\"frame_id\" value=\"velodyne\"/>\n</node>\n\n<node pkg=\"velodyne_pointcloud\" type=\"cloud_node\" name=\"cloud_node\">\n    <param name=\"calibration\" value=\"$(find velodyne_pointcloud)/params/64e_utexas.yaml\"/>\n    <param name=\"min_range\" value=\"3.0\"/>\n    <param name=\"max_range\" value=\"130.0\"/>\n</node>\n\n<node name=\"mapper\" type=\"mapper\" pkg=\"ethzasl_icp_mapper\" output=\"screen\" >\n    <remap from=\"cloud_in\" to=\"/velodyne/velodyne_points\" />\n    <param name=\"subscribe_scan\" value=\"false\"/>\n    <param name=\"subscribe_cloud\" value=\"true\"/>\n    <param name=\"odom_frame\" value=\"/velodyne\" />\n    <param name=\"map_frame\" value=\"/map\" />\n    <param name=\"useConstMotionModel\" value=\"false\" />\n    <param name=\"localizing\" value=\"true\" />\n    <param name=\"mapping\" value=\"true\" />\n    <param name=\"minOverlap\" value=\"0.8\" />\n    <param name=\"maxOverlapToMerge\" value=\"0.9\" />\n    <param name=\"tfRefreshPeriod\" value=\"0.01\" />\n    <param name=\"vtkFinalMapName\" value=\"finalMap.vtk\" />\n    <param name=\"useROSLogger\" value=\"true\" />\n    <param name=\"minReadingPointCount\" value=\"1000\" />\n    <param name=\"minMapPointCount\" value=\"10000\" />\n    <param name=\"inputQueueSize\" value=\"1\" /> \n</node>\n"], "answer_code": ["params/64e_utexas.yaml", "db.xml", "rostopic list", "rosgraph", "rostopic hz"], "url": "https://answers.ros.org/question/75433/how-to-use-ethzasl_icp_mapping-with-velodyne/"},
{"title": "Problem with connecting to Husky via USB port", "time": "2013-08-14 04:09:33 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "I am trying to run Husky and using the instruction for husky in fuerte ", "However, when I use this command \"roslaunch husky_bringup base.launch port:=/dev/ttyUSB0\" it gives me the following error:\nconnecting error on /dev/ttyUSB0. Will retry every second", "Does anyone have any idea what is the source of problem and how can I solve it?", "Thanks,"], "answer": [" ", " ", "We're working on this directly with ", " via our support line at the moment, but to make sure the answer is out here: We believe that the root cause is that there is more than one ROS instance trying to access the serial port.", " ", " ", "Just in case you haven't checked: are you sure that the Husky is connected to USB0? (You can check the latest output from dmesg if you are not sure.)", "I guess I connected it to the right USB. The communication indicator on Husky turns green but the connection goes on and off", "I don't have personal experience with the Husky platform, so that was the only thing I could think of.\nI hope someone else as a better idea."], "url": "https://answers.ros.org/question/72838/problem-with-connecting-to-husky-via-usb-port/"},
{"title": "pi_face_tracker problem", "time": "2013-08-23 10:33:42 -0600", "post_content": [" ", " ", " ", " ", "Hi \ni have installed your package. i am trying to use it ,but i haven't succeeded.\nfirstly i run cheese because without it nothing works. then i run the two commands . \nfrom the first one \ni get a : [ WARN] [1377289686.323464310]: Reconfigure callback failed with exception unable to allocate buffers.\nthe other one works fine . so i get a  : [INFO] [WallTime: 1377289898.645841] Starting pi_face_tracker. ", "the problem is that when i type \nrostopic echo /whatever gets published \nthen nothing appears on my screen, not even my \n/camera/image_raw.", "the cheese program works fine .\nwhat should i do", "edit: i just saw the with my laptops camera it works fine. with my msi star cam clip it does not.\nand when i run the first command i get "], "answer": [" ", " ", "Since the pi_face_tracker package works with your laptop camera, it sounds like your msi star cam clip is not supported by the uvc_cam driver.  You might try the ", " driver instead as follows.  I see you are using ROS Fuerte so you can install the camera_umd package using the commands:", "I have added a suitable launch file for this driver to the pi_face_tracker package.  To get the update run the commands:", "You can now try this launch file with your msi camera which is probably /dev/video1 on your laptop.  Assuming the camera is plugged in to a USB port, run the command:", "Then test to see if you have an image by running the command:", "If you do not see an image, it is possible you camera is not uvc-compatible.  If you do see an image, you can try pi_face_tracker.  Ctrl-C out of the image_view process then run:", "--patrick", "hi\nthanks for the answer, \nit worked half way,\ni get an image from my web cam but it just 'green' noise.\nso my camera is not uvc?", "It sounds like that might be true but I am not a Linux video expert.  I've had good luck with Logitech cameras and a Philips camera but have not tried other brands.", "ok , in 3-4 days i will be able to test it with a kinect \nand i hope that it will be able to give me the exact location (xyz) of the face \nvia the /target_point ."], "question_code": [": Invalid argument\n[ERROR] [1377291803.163531318]: Problem setting exposure. Exception was unable to get control\n\nsetting control 9a0902\nunable to get control\n: Invalid argument\n[ERROR] [1377291803.163756417]: Problem setting absolute exposure. Exception was unable to get control\n\nsetting control 98091b\ncurrent value of 98091b is 90\nnew value of 98091b is 142\nsetting control 98090c\nunable to get control\n: Invalid argument\n[ERROR] [1377291803.164985903]: Problem setting white balance temperature. Exception was unable to get control\n\nsetting control 980913\nunable to get control\n: Invalid argument\n[ERROR] [1377291803.165156665]: Problem setting gain. Exception was unable to get control\n\nsetting control 980902\ncurrent value of 980902 is 25\nunable to set control\n: Numerical result out of range\n[ERROR] [1377291803.165319116]: Problem setting saturation. Exception was unable to set control\n\nsetting control 980901\ncurrent value of 980901 is 20\nnew value of 980901 is 50\nsetting control 980900\ncurrent value of 980900 is 128\nnew value of 980900 is 66\nSegmentation fault, stopping uvc camera driver.\n[ERROR] [1377291803.302588952]: Segmentation fault, stopping uvc camera driver.\n[uvc_cam_node-2] process has died [pid 15237, exit code -11, cmd /home/alexandros/fuerte_workspace/sandbox/uvc_cam/bin/uvc_cam_node __name:=uvc_cam_node __log:=/home/alexandros/.ros/log/71731024-0c37-11e3-a02f-001de0d62475/uvc_cam_node-2.log].\nlog file: /home/alexandros/.ros/log/71731024-0c37-11e3-a02f-001de0d62475/uvc_cam_node-2*.log\n"], "answer_code": ["$ sudo apt-get install ros-fuerte-camera-umd\n$ rospack profile\n", "$ roscd pi_vision\n$ svn update\n$ rospack profile\n", "$ roslaunch pi_face_tracker camera_node.launch device:=/dev/video1\n", "$ rosrun image_view image_view image:=/camera/image_raw\n", "$ roslaunch pi_face_tracker face_tracker_uvc_cam.launch\n"], "url": "https://answers.ros.org/question/74578/pi_face_tracker-problem/"},
{"title": "TurtleBot bringup error - \"Robot not connected\"", "time": "2013-05-16 04:57:02 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to run the Bringup Tutorial for Turtlebot \nGroovy. But I always get ", "I have recently upgraded ROS to Groovy and I'm using a Gateway computer for Turtlebot.\nI have done all the tutorials for Installation and Network Setup. The Turtlebot is plugged in and the lights are on.", "At this point, the Base beeps and goes off. The iRobot light keeps on.\nThen I turn the base back on:", "Any ideas?", "I have the same problem and I'm very confused because it wasn't happening before.", "Same problem here. But cannot fix it!!"], "answer": [" ", " ", " ", " ", "I modified minimal.launch and I don't seem to get the error anymore (after 5 minutes, so I'm not sure that's the fix, but hopefully it is). So I changed robot/name for robot_name and robot/type for robot_type.", "Nevermind... it occurred again, but I fixed it by recharging the battery. I think it was almost empty, even though the dashboard was indicating that it was full and despite the fact that I had let the robot charge for a very long time. ", " ", " ", "If you're using a non-standard laptop you will need to verify that the device ports are valid in the launch files.  It looks like /proc/acpi/battery/BAT0 is invalid.  And it is probably looking for the USB port on the wrong device as well.  ", "I'm a newbie to Create. I've heard that the battery could be a problem. I'm currently charging to make sure that's eliminated. But how would I change the ../battery/BAT0 files to include a non-standard laptop?.\nCould you also please elaborate how I would modify the launch files (minimal.launch?) to include the correct ports", "You should start a new question for this...info you can find at "], "question_code": ["[WARN] [WallTime: 1368713745.246504] Create : robot not connected yet, sci not available\n", "[WARN] [WallTime: 1368713742.227517] Create : robot not connected yet, sci not available\n    [WARN] [WallTime: 1368713744.380423] Battery : unable to check laptop battery info [/proc/acpi/battery/BAT0 does not exist]\n    [WARN] [WallTime: 1368713744.388681] Battery : unable to check laptop battery state [/proc/acpi/battery/BAT0 does not exist]\n    [WARN] [WallTime: 1368713745.246504] Create : robot not connected yet, sci not available\n    [turtlebot_laptop_battery-10] process has finished cleanly\n    log file: /home/turtle2/.ros/log/07609262-be33-11e2-8571-d0df9aaae785/turtlebot_laptop_battery-10*.log\n    [kinect_breaker_enabler-6] process has finished cleanly\n    log file: /home/turtle2/.ros/log/07609262-be33-11e2-8571-d0df9aaae785/kinect_breaker_enabler-6*.log\n", "[WARN] [WallTime: 1368713868.243731] Failed to read sensor package. 5 retries left.\n[ERROR] [WallTime: 1368713868.498948] Failed to contact device with error: [Distance, angle displacement too big, invalid readings from robot. Distance: 29.73, Angle: 58.26]. Please check that the Create is powered on and that the connector is plugged into the Create.\n[ERROR] [WallTime: 1368713878.887388] Failed to contact device with error: [Error reading from SCI port. No data.]. Please check that the Create is powered on and that the connector is plugged into the Create.\n"], "url": "https://answers.ros.org/question/62893/turtlebot-bringup-error-robot-not-connected/"},
{"title": "ROS determines the size of an array itself? Really", "time": "2013-08-28 00:40:42 -0600", "post_content": [" ", " ", " ", " ", "Following the ros code in ros msg format:", "which is equivalent of this in c:", "Now I have heard that ros determines itself the size of an array. One does not have to give it size.", "Now for the C code inorder for me to iterate in the array I need to know the size of the array but I believe in ros, I dont.", "So if I am writing a C code and I want to access the msg format and I want to iterate through the array eph, how can I do it without knowing the size of eph"], "answer": [" ", " ", "There is a simple answer: ROS doesn't support C (in this way).", "ROS messages are C++ and in that case the [] is translated to an std::vector, which has a size.", "C-ports like for rosserial usually do have an additional size value.", "oh cool, so I can use vector::begin() and vector::end() to navigate in the array. I will incorportate my C code into C++ then. Just one question. I know that in ROS one does not need to declare size of an array so according to your analogy, I can infer that a dynamic vector is created everytime. If this is true than it means that ROS is powerful enough to know the size of any array itself.", "Yes, in C++ any [] in a message is a vector. This might be different for fixed size arrays.", "http://www.ros.org/wiki/msg?action=show&redirect=ROS%2FMessage_Description_Language#Fields gives a description of how messages are translated into python and C++."], "question_code": ["Ephemeris[] eph\n", "Ephemeris *eph;\n"], "url": "https://answers.ros.org/question/75184/ros-determines-the-size-of-an-array-itself-really/"},
{"title": "ROS installation on Ubuntu taking too long", "time": "2013-06-01 21:48:18 -0600", "post_content": [" ", " ", " ", " ", "I am a ROS as well as Ubuntu newbie. I am trying to install ROS on Ubuntu 12.10 (quantal).Its been more than 6 hrs since I started install Step 1.4 (Desktop-Full Install) described on the Ubuntu installation page on ROS.org. All I see on the Ubuntu terminal is this ....", "\"Selecting previously unselected package \"...insert package name here...\" ", "\nUnpacking \"........\"                                                                   (it is doing this selecting and unpacking of various packages one after the other)Seems to be never ending. Is this normal or am I doing some wrong."], "answer": [" ", " ", "That is the normal outputs.  The length of time is surprising, often that suggests you have issues or hardware that is not powerful enough. What hardware are you running? ", "It was the hardware.... had to get a new computer to be able to work with ROS.\nOld system-Dual boot PC, intel T2400 1.83GHz,RAM 2GB, ATI X1300 ...", "Glad to hear you got it working.  Please accept the answer at the left using the checkmark so others know your question is solved."], "url": "https://answers.ros.org/question/64148/ros-installation-on-ubuntu-taking-too-long/"},
{"title": "How to draw a personalized path line in rviz (for example 30cm wide) [closed]", "time": "2013-09-12 01:28:40 -0600", "post_content": [" ", " ", " ", " ", "Is there a possibility to draw the path in a hector-map in rviz with a userdefined wide?\nThe reason is I'm tracking a VR100 vacuum cleaning robot (aka neato) cleaning my house. Because the wide of it's brushes is 30cm I want to draw the path in the map in the same wide to see, which areas were cleaned and which areas ware skipped due to obstacles to remove them. Nice to have were also a configurable alpha value of its color, which will be increased when a path is drawn more then once to see overlapping cleaned areas."], "answer": [" ", " ", " explains how to publish ", " that RViz can visualize. What you need is a LINE_STRIP with scale.x=0.3, I think.", "Thank you. I inspected display_path.cpp to find out how to get the path data to draw the line_set (in the example the data was generated by the marker itself). This is so tricky, that I'll better use the time to get a consistent map out of my lidar data.", "I'm not sure what you mean... the tutorial I propose shows quite straightforwardly how to show a path. Anyway, can you mar the question as solved if that is the case? Maybe can help others in your same case.", "I couldn't find any hints how to get the data for drawing these line-strips out of the slam system and not by calculating demo data by sin/cos-functions. But because neither gmapping nor hector_mapping are able to process my lidar data into a usable map and path this question anyway is obsolete.", "I tried to mark this question as \"Solved\" but I'm problably blind and can't find a button or link for it:\n", "\n\nAnd there were no informations in the help or the FAQ how to mark a question as \"Solved\". So I'll mark this question as \"Closed\".", "Just click the \"check\" symbol below the answer's votes counter. It gets green."], "url": "https://answers.ros.org/question/78771/how-to-draw-a-personalized-path-line-in-rviz-for-example-30cm-wide/"},
{"title": "MoveIt! Rviz Plugin - No interactive markers visible/available", "time": "2013-03-09 07:19:02 -0600", "post_content": [" ", " ", " ", " ", "Hello to all,", "I created a Robot-Config via MoveIt! Setup Assistant. The Robot is a simple 6 Dof Robot.", "When I start it with the Rviz Moveit Plugin the interactive markers are not displayed. (After I click on \"Interact\"). I suspect the End-Effector is not correctly set by me.", "Screenshots of the MoveIt Setup Assistant:", " ", " ", "I set for both plannings groups the \"KDLKinematicsPlugin\" as Solver on default values. \nI followed ", " for the pr2-robot.", " See here the ", ".", " Removing and adding \"MotionPlanning\" inside Rviz didn't help.", "\nHere you can download my ", ".\nHere you can download my ", ".", " If I set the \"fixed_frame\" to a link of the robot itself, interactive markers for joint a4 show up. (See picture below) But I need interactive markers in joint5. If I let \"fixed_frame\" set on \"base_footprint\" or \"odom_combined\" (as it should be, I think) and \"plan and execute\" for a random query MoveIt! crashes (", ")", " Now the interactive markers work correctly.\nMany Thanks to all supporters.", "viovio", "Please mark an answer as correct, so the question is flagged as \"answered\".  Glad it's working for you!  Consider looking into ", ", which has several tutorials and templates for controlling your industrial robot through ROS.  We'd love to add support for Staubli to ROS-I !", "did you tried to connect that to real robot or the simulator ?"], "answer": [" ", " ", " ", " ", "To me, it looks like the issue is that your joint_state_publisher is crashing due to a malformed robot URDF.  See the following snippet from your error log:", "I was able to reproduce the error using the ", " supplied in your links.  The issue here is with the ", " section in your URDF.  The joint_state_publisher uses a python minidom XML parser, which is more picky about XML syntax than the TinyXML C++ parser used in much of the rest of ROS (including the check_urdf test you ran earlier).  Even though the TinyXML parser can handle your URDF, the minidom parser requires a little bit more help...", "To fix the error, you have two options:", "Using these changes, I was able to get the joint_state_publisher to run without errors.  I expect this will also fix the issue you were having with missing interactive markers.", "Thx for your answer. I updated the links, so it must work now. My urdf file seems to be fine according to check_urdf. ", "In the meantime I also noticed this. I removed it and the joint_state_publisher starts correctly. But this doesn't solve the problem. But I noticed something interesting: When setting the fixed_frame from odom_combined  to a link of the robot itself, interactive markers appear for the a4-link!", "I think you are seeing the correct behavior.  What are you expecting to see?  For a fixed robot, you shouldn't need to define odom_combined.  It is appropriate to set the fixed_frame in rviz to base_link of your robot.", "Similarly, at4 is the last link in your robot_arm group.  That's where the markers should be shown.  If you prefer to include the at5 link, then you need to add it to your robot_arm group.  This may require you to use a dummy (fixed) joint for the EE group, if you don't have a true gripper.", "Thx. I followed your comment and the interactive markers are now on their right spot. ", " ", " ", "If anyone else comes across this issue, you can get more debug info with the following steps:", "This helped me debug that my issue is related to the robot being in a namespace. (A quick fix in this case is to manually publish the transform with \"rosrun tf static_transform_publisher 0 0 0 0 0 0 1 /world /{robot_namespace}/world 10\")", "Your error is similar to mine, but when I try your fix it doesn't work for me.  Could it be a problem simply with having slashes?", "what's the error that you see? the frames with/without a slash in the error message that I mentioned have significantly different meaning: '/world' is a global frame and 'world' is a local frame. Probably whatever was publishing it as a local frame (I forget what) should have been doing it as global", "Ah okay! I was revising a different deprecated MoveIt! tutorial but I found the issue once the global/local difference was made clear.  Thanks for your help.", "glad to hear that helped", " ", " ", "Have you tried disabling and then re-enabling the MoveIt plugin? I've noticed in the past that if RVIZ starts before MoveIt is really up and running, the markers don't work. ", "Not yet, how can I do that?", "On each plugin there is a checkbox next to it to enable/disable it.", "Unfortunately it does not work. I guess you mean the checkbox next to MotionPlanning in Panel \"Displays\".", "I confirm that disabling/enabling the MotionPlanning panel does fix the issue for me.", " ", " ", " ", " ", "I have same problem, but nothing help me. After launching demo.launch it writes one error: \n\"[ INFO] [1378302606.416030588]: No root joint specified. Assuming fixed joint\nprocess[mongo_wrapper_ros_romesy_desktop_2173_3455346243933734730-4]: started with pid [2228]\"", "\"[ INFO] [1378302606.461072650]: rviz version 1.9.32\"", "\"[ INFO] [1378302606.461179802]: compiled against OGRE version 1.7.4 (Cthugha)\"", "\"[ INFO] [1378302606.653216609]: OpenGl version: 2.1 (GLSL 1.2).\"", "[ERROR] [WallTime: 1378302606.779255] Execution failed: [Errno 2] No such file or directory", "\"[ INFO] [1378302606.887842389]: Publishing maintained planning scene on 'monitored_planning_scene'\"", "Rviz starts, everythings works, but i can't see and use interactive markers and also have problem with connecting to Warehouse.", "Is the move_group node getting started? Does the planning library show in red no planning library loaded?", "move_group is running (visible in rqt_graph) and libary is loaded(green OMPL)"], "answer_details": ["remove the ", " section from your URDF", "or update the URDF to a \"more correct\" format:\n", "move the ", " section ", " the ", " section", "add ", " qualifiers to the robot tag as shown:", " ", " ", " ", " ", "In the Displays panel of RViz, click Add > By topic > select InteractiveMarkers under /rviz_moveit_motion_planning_display", "In my case I see the error \"Error getting time of latest transform between /world and world: (error code: 1)\" for this interactive marker in the Displays panel.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["<launch>\n\n<include file=\"$(find tx60l_final3)/launch/upload_tx60l.launch\"/>\n\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"odom_broadcaster\" args=\"0 0 0 0 0 0 odom_combined base_footprint 100\" />\n\n<node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\">\n <param name=\"/use_gui\" value=\"false\"/>\n</node>\n\n<node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" respawn=\"true\" output=\"screen\"/>\n\n  <include file=\"$(find tx60l_final3)/launch/move_group.launch\">\n    <arg name=\"allow_trajectory_execution\" value=\"false\"/>\n  </include>\n\n<include file=\"$(find tx60l_final3)/launch/moveit_rviz.launch\"/>\n\n</launch>\n"], "answer_code": ["xml.parsers.expat.ExpatError: unbound prefix: line 3, column 4\n[joint_state_publisher-3] process has died\n", "tx60l.urdf", "<gazebo>", "<gazebo>", "<gazebo>", "<robot>", "xmlns", "<robot name=\"TX60L\">\n       xmlns:controller=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\"\n       xmlns:interface=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\">\n  <gazebo>\n    ...\n  </gazebo>\n  <link name=\"base_link\">\n    ...\n</robot>\n"], "url": "https://answers.ros.org/question/57611/moveit-rviz-plugin-no-interactive-markers-visibleavailable/"},
{"title": "How to retrieve data from Kinect using rviz", "time": "2013-10-06 06:18:49 -0600", "post_content": [" ", " ", " ", " ", "Hello community", "I am trying to retrieve image and point cloud data from kinect, which is connected to the ASUS 1025C netbook. The green LED on kinect is blinking, which I am assuming confirms that it is getting power from turtlebot 2 Kobuki base.", "All sensors are reported okay on Kobuki dashboard.", "At step 3.1.3 (3D Visualization) of turtlebot bringup groovy instructions for installing and setting up turtlebot2 and workstation, when I run ", "the display GUI comes up, where turtlebot's position with respect to grid map updates as I manually move turtlebot's base. Laser or Image on the other hand, do not provide any information on this GUI. Selecting ", " from the display list brings up the Image window with ", " written in it. ", "What are other checks that can be performed to make sure the kinect \"service\" (not sure what ROS term for this would be) is running and to see if the connection is okay?"], "answer": [" ", " ", "To see if the Kinect is publishing RGB images, use:", "rostopic echo /camera/rgb/image_color", "Since rviz isn't displaying the image though, this will most likely fail too. You can also try:", "rosnode list", "and check for the \"openni_camera\" node.", " ", " ", "Your problem may be related to this post\n", "/", "They discuss using  Avin2 GitHub snapshot of SensorKinect drivers to solve that problem.  It may be worth trying.  Instructions for downloading and installing Avin2 can be found here:", "/", " ", " ", " ", " ", "Thank you for the pointers. ", "did not had openni_camera node listed in it. Following is the output of rosnode list after I ran roslaunch openni_launch once:", "Based on ROS documentation, I launched ", "It gives the following output. What seems odd to me is where it printed ", " ", "I believe in ROS fuerte a breaker (breaker 1 possibly?) had to be enabled to have the turtlebot base power the kinect, but I'm not sure if this is the case in groovy and later.\nAlso, the Kinect doesn't work with USB3.0 so make sure it is plugged into a 2.0 port.", "When you launch \"turtlebot_bringup 3dsensor.launch\", does it report any errors", "Please see updates in the answer above. Also, to eliminate doubts about power supply, I just tested powering kinect from its AC power adapter and got same results. Thanks.", "Try running the command \"lsusb\" on the computer that the Kinect is plugged into. There should be three USB devices listed that are associated with the Kinect.", "Yes, three devices do show up for kinet as NUI audio, motor and camera. I've added the screen print in answer above.", "Try: \"killall XnSensorServer\" then \"roslaunch openni_launch openni.launch\" and see if you get the same error as before.", "XnSensorServer: No process found", "I'm out of ideas. Those are all the issues that I've ran into with the Kinect. The issue is ROS detecting the Kinect it seems though."], "question_code": ["> roslaunch turtlebot_bringup 3dsensor.launch\n> roslaunch turtlebot_rviz_launchers view_robot.launch\n"], "answer_code": ["turtlebot@turtlebot-1025C:~$     rostopic echo /camera/rgb/image_color\nWARNING: topic [/camera/rgb/image_color] does not appear to be published yet\n\nrosnode list\n", "turtlebot@turtlebot-1025C:~$ rosnode list\n/app_manager\n/bumper2pointcloud\n/camera/depth/metric\n/camera/depth/metric_rect\n/camera/depth/points\n/camera/depth/rectify_depth\n/camera/depth_registered/metric\n/camera/depth_registered/metric_rect\n/camera/depth_registered/rectify_depth\n/camera/disparity_depth\n/camera/disparity_depth_registered\n/camera/driver\n/camera/ir/rectify_ir\n/camera/points_xyzrgb_depth_rgb\n/camera/register_depth_rgb\n/camera/rgb/debayer\n/camera/rgb/rectify_color\n/camera/rgb/rectify_mono\n/camera_nodelet_manager\n/cmd_vel_mux\n/diagnostic_aggregator\n/master_sync_turtlebot_1025C_2026_1107254408\n/mobile_base\n/mobile_base_nodelet_manager\n/robot_pose_ekf\n/robot_state_publisher\n/rosout\n/rviz_1381089680931013393\n/turtlebot_laptop_battery\n/zeroconf/zeroconf_avahi\n", "roslaunch openni_launch openni.launch\n", "turtlebot@turtlebot-1025C:~$ roslaunch openni_launch openni.launch\n... logging to /home/turtlebot/.ros/log/92afcf1a-2ec0-11e3-abb1-94dbc9a2c311/roslaunch-turtlebot-1025C-13826.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server (*EDITED* since I don't have previlidges to post URLs)TURTLEBOT:55616/\n\nSUMMARY\n========\n\nPARAMETERS\n * /camera/depth/rectify_depth/interpolation\n * /camera/depth_registered/rectify_depth/interpolation\n * /camera/disparity_depth/max_range\n * /camera/disparity_depth/min_range\n * /camera/disparity_depth_registered/max_range\n * /camera/disparity_depth_registered/min_range\n * /camera/driver/depth_camera_info_url\n * /camera/driver/depth_frame_id\n * /camera/driver/depth_registration\n * /camera/driver/device_id\n * /camera/driver/rgb_camera_info_url\n * /camera/driver/rgb_frame_id\n * /rosdistro\n * /rosversion\n\nNODES\n  /camera/depth/\n    metric (nodelet/nodelet)\n    metric_rect (nodelet/nodelet)\n    points (nodelet/nodelet)\n    rectify_depth (nodelet/nodelet)\n  /camera/rgb/\n    debayer (nodelet/nodelet)\n    rectify_color (nodelet/nodelet)\n    rectify_mono (nodelet/nodelet)\n  /\n    camera_base_link (tf/static_transform_publisher)\n    camera_base_link1 (tf/static_transform_publisher)\n    camera_base_link2 (tf/static_transform_publisher)\n    camera_base_link3 (tf/static_transform_publisher)\n    camera_nodelet_manager (nodelet/nodelet)\n  /camera/\n    disparity_depth (nodelet/nodelet)\n    disparity_depth_registered (nodelet/nodelet)\n    driver (nodelet/nodelet)\n    points_xyzrgb_depth_rgb (nodelet/nodelet)\n    register_depth_rgb (nodelet/nodelet)\n  /camera/ir/\n    rectify_ir (nodelet/nodelet)\n  /camera/depth_registered/\n    metric (nodelet/nodelet)\n    metric_rect (nodelet/nodelet)\n    rectify_depth (nodelet/nodelet)\n\nROS_MASTER_URI=*EDITED* since I don't have previlidges to put URLs TURTLEBOT:11311\n\ncore service [/rosout] found\nprocess[camera_nodelet_manager-1]: started with pid [13866]\nprocess[camera/driver-2]: started with pid [13878]\n[ INFO] [1381090122.820232930]: Initializing nodelet with 4 worker threads.\nprocess[camera/rgb/debayer-3]: started with pid [13926]\n[ INFO] [1381090123.256421081]: No devices connected.... waiting for devices to be connected\nprocess[camera/rgb/rectify_mono-4]: started with pid [13942]\nprocess[camera/rgb/rectify_color-5]: started with pid [13967]\nprocess[camera/ir/rectify_ir-6]: started with pid [13983]\nprocess[camera/depth/rectify_depth-7]: started with pid [13998]\nprocess[camera/depth/metric_rect-8]: started with pid [14016]\nprocess[camera/depth/metric-9]: started with pid [14034]\nprocess[camera/depth/points-10]: started with pid [14055]\nprocess[camera/register_depth_rgb-11]: started with pid [14073]\nprocess[camera/depth_registered/rectify_depth-12]: started with pid [14088]\n[ INFO] [1381090126.256891688]: No devices connected.... waiting for devices to be connected\nprocess[camera/depth_registered/metric_rect-13]: started with pid [14104]\nprocess[camera/depth_registered/metric-14]: started with pid [14130]\nprocess[camera/points_xyzrgb_depth_rgb-15]: started with pid [14145]\nprocess[camera/disparity_depth-16]: started with pid [14160]\nprocess[camera/disparity_depth_registered-17]: started with pid [14179]\nprocess[camera_base_link-18]: started with pid [14202]\nprocess[camera_base_link1-19 ..."], "url": "https://answers.ros.org/question/87631/how-to-retrieve-data-from-kinect-using-rviz/"},
{"title": "nav stack missing from UbuntuARM install", "time": "2013-10-27 06:33:01 -0600", "post_content": [" ", " ", " ", " ", "I am following this page:\n", "\nand have it working, but cannot find the install package for navigation.\napt-cache search ros-groovy\ndoes not show up a nav package other than nav_msgs.\nSo, of course,\nsudo apt-get install ros-groovy-navigation\njust returns this:\nUnable to locate package ros-groovy-navigation", "How do I get programs like amcl, move_base, etc. installed?"], "answer": [" ", " ", " ", " ", "Looking at the build farm status and the navigation packages are all in red (missing) for groovy.\n", "However, good news, they are all green for hydro.\n", "So I am moving to Ubuntu 13.04 and hydro. "], "url": "https://answers.ros.org/question/94791/nav-stack-missing-from-ubuntuarm-install/"},
{"title": "primitive object recognition (Hydro)", "time": "2013-10-21 13:37:51 -0600", "post_content": [" ", " ", " ", " ", "Hi again,", "in case that my other question regarding object recognition with only a 2d laser scan available cannot be answered easy enough... ", "How about any sort of object recognition using vision sensors like from Kinect through TOF cams and all the way down to simple web cams? Are there any close to ready built stacks/packages in ", " that I could make use of? Up-to-date tutorials about that topic?", "Thanks again!", "Cheers,\nHendrik"], "answer": [" ", " ", "OpenCV is a really powerful computer vision library (", ") and has a whole bunch of tutorials for doing all sorts of interesting things:", "ROS has a wrapper/bridge built for communicating with this library (", ").", "You can find some tutorials on communicating between the two ends here:"], "question_code": ["ROS Hydromedusa"], "url": "https://answers.ros.org/question/92971/primitive-object-recognition-hydro/"},
{"title": "Optimize control loop?", "time": "2013-09-21 04:55:46 -0600", "post_content": [" ", " ", "Hi there!", "I'm constantly getting warnings like ", "Control loop missed its desired rate of 5.0000Hz... the loop actually took 0.2028 seconds", "using ", " to drive my robot around. Obviously something is taking pretty long within the control loop. What do you think, how could I optimize it to reduce the time a control loop cycle takes? ", "This is what my current ", " looks like: ", "My personal assumption is that there are too many nodes between /MoveBase and /udpOutgoingInterface. ", "Thanks for your help!", "Cheers,\nHendrik", "Did you compile the code, or are you using the debs?", "I'm using the debs. Do you think, compiling the stack explicitly with compiler optimizations for my arch would make that much of a difference?", "The debs should be correct, I think."], "answer": [" ", " ", "Hello,", "There are some advices about lack of processing power on ", "some advices may be useful for you "], "question_code": ["move_base", "rosgraph"], "url": "https://answers.ros.org/question/81866/optimize-control-loop/"},
{"title": "Real PR2 plug and unplug procedure", "time": "2013-09-20 06:20:33 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "We now have a PR2 in our lab and we are cautiously reading the tutorials and learning how to use it the right way. But in our search, we were unable to find a clear information about the way to plug and unplug the PR2 to the mains. I read that it is recommended to let the robot plugged when not used, but what about the plug and unplug procedures ? Are there particular things to know about this point ?", "Thanks for reading,"], "answer": [" ", " ", "From the PR2 Manual (", "):", "Plugging a live AC cord into the back of the robot can cause arcing, which can deteriorate the power inlet on the PR2.", "It should be noted that that's actually happened, so it's not just some theoretical safety advise.", "Thanks for your answer and sharing your experience. I finally found this quotation in the manual after asking the question. I think It should be more visible on the online documentation (as it is a safety procedure and should not appear only there : "], "answer_code": ["When plugging in the PR2, always attach the power cord to the AC inlet on the robot before attaching it to the AC outlet on the wall. Unplug the cord from the wall before unplugging from the robot.\n"], "url": "https://answers.ros.org/question/81566/real-pr2-plug-and-unplug-procedure/"},
{"title": "Using subscriber/callback function inside of a class C++", "time": "2013-12-09 05:55:39 -0600", "post_content": [" ", " ", " ", " ", "For the life of me I cannot seem to figure out why this code is not working and throws up a lot of errors:", "I have initialised the ", " inside of the class members but cannot seem to figure out why it's giving me an error.", "Any ideas?", "EDIT:", "Here are the error messages: ", "What exact errors? When are you getting them? During compile or when running? Initial guess: Try adding a this to the subscribe call an specify the classname instead of this-> for the subscribe."], "answer": [" ", " ", "Callbacks with class member functions are a little tricky. \"Regular\" functions are merely a pointer to some code in memory. Class member functions have additional state information, namely the object instance they belong to, so you cannot just plug a member function into a regular function pointer and expect it to work.", "There are two easy ways to solve this. Firstly, the ROS developers anticipated this problem and provided a neat alternative ", " method that accepts member functions and the corresponding object like this:", "The ", " is the function pointer to the member function, and ", " is the object instance for which you want to have the callback called.", "But even if they had forgotten, the second option is to use ", ", a very powerful tool to bind arguments to arbitrary functions, which supports class member functions as well:", "The syntax is slightly more complicated, but it is much more versatile (read the Boost documentation for details).", "I have the same problem , I used the boost::bind,but I have new error information", "the code:\n_sub_object = _nh.subscribe(\"/perception/object\",1,boost::bind(&MotionCore::_callback_from_perception_obstacle,this));", "the error:\nIn file included from /home/westeast/git/enmodel/src/trajectory/nodes/final", "You forgot the placeholder _1 for the argument that will be passed to your callback", "If using ", ", the ", " have a useful note not mentioned here: ...", "when using functor objects (like boost::bind, for example) you must explicitly specify the message type as a template argument, because the compiler cannot deduce it in this case."], "question_code": ["class Example {\n\npublic:\n\n    Example(ros::NodeHandle n )\n    {\n        sub = n.subscribe(\"/camera/depth_registered/points\", 1000, &this->callBack);\n\n    }\n\n    void callBack(const sensor_msgs::PointCloud2Ptr& msg)\n    {\n\n\n    }\n\nprotected:\n\n    ros::Subscriber sub; \n};\n", "ros::Subscriber sub", "error: no matching function for call to \u2018ros::NodeHandle::subscribe(const char [32], int, <unresolved overloaded function type>)\u2019\n/src/talker.cpp:11:71: note: candidates are:\n/opt/ros/hydro/include/ros/node_handle.h:379:14: note: template<class M, class T> ros::Subscriber ros::NodeHandle::subscribe(const string&, uint32_t, void (T::*)(M), T*, const ros::TransportHints&)\n"], "answer_code": ["subscribe", "sub = n.subscribe(\"/camera/depth_registered/points\", 1000, &Example::callBack, this);", "&Example::callBack", "this", "boost::bind()", "sub = n.subscribe(\"/camera/depth_registered/points\", 1000, boost::bind(&Example::callBack, this, _1));", "boost::bind"], "url": "https://answers.ros.org/question/108551/using-subscribercallback-function-inside-of-a-class-c/"},
{"title": "gmapping with user defined laser scan and position data", "time": "2013-02-19 02:42:51 -0600", "post_content": [" ", " ", " ", " ", "actually i want each robot to build two local maps , one which is build by its own laser scan and position while the other map from the laser scan and position achieved by other robot through wifi. now what i think , this require two gmapping algorithm having different laser messages . am i rite? is this a rite approach for global mapping? . i,m doing this for global mapping. if yes , how to modify gmapping algorithem for different laser message and position data.", "thankyou", "actually i want each robot to build two local maps , one which is build by its own laser scan and position while the other map from the laser scan and position achieved by other robot through wifi. now what i think , this require two gmapping algorithm having different laser messages . am i rite? ", "yes, for that you have to run two gmapping algorithms. The stuff with the frame_ids, that shade explained, is important here, too. The result of the two gmapping algorithms will be two maps, which are not aligned, though...", "thanks Achim for the response . can you tell me how to do this i have no idea of this. all i want to make user defined laser message and modify gmapping with this messag. "], "answer": [" ", " ", "Multiple laser message can be published but u have to make sure that they have different namespace. By default the topic name is for laser scan is \"scan\". You can prefix it with robot namespace like \"/robot0/scan\" and \"/robot1/scan\". Secondly for each laser scan the frame_id should be defined correctly w.r.t to respective robots. Gmapping most probably does not support laser scan from multiple sources so, if you want to build a map with multiple scan either you can write your own mapping algorithm or transform messages from one frame_id to other and then publish scan message on 1 single topic only.", "Hope it helps.. ", "thanks for help shade. can you tell me how to change topic name and modify gmapping for it. ", " ", " ", " ", " ", "I'm not sure what you try to achieve. Build one single map from two laser scanners? I don't think that gmapping node in ros can handle that (look here for example: ", "/ ). So you would need to write a new node with a different mapping approach anyway.. in that node you can then register to multiple laser_scan topics, easily. :)", "Otherwise, if you just want to use the cpu-power of a different computer to process the laserscans from the robot (which has only a slow cpu, maybe), you can adjust the ROS_MASTER_URI on one of the computers and run only one master. Then the setup is just the normal setup, i.e. remap the laser topic on which the driver of the laserscanner publishes so that gmapping listens to it and so on... ", "As an alternative to the ROS_MASTER_URI you can try ", " best in combination with node_manager_fkie (there you only need to press one button to enable the sync).", " ", " ", "According to the final comment in this thread, ", ", slam_gmapping should work with multiple lasers as long as you publish them both to the same topic. Gmapping takes care of the transforms itself.", "This will most likely not work. Laser parameters in GMapping are determined from the first received message. After that it will always use that frame, scan-count, angle increments etc... So no, using GMapping with multiple laser scanners is impossible or at least extremly difficult."], "url": "https://answers.ros.org/question/55626/gmapping-with-user-defined-laser-scan-and-position-data/"},
{"title": "Kinect or Asus for a movement base", "time": "2013-11-27 21:46:29 -0600", "post_content": [" ", " ", " ", " ", "Hello everybody.", "I am developing a mobile base with wheels for a robot and I am thinking different sensors for calculate distances, track people and avoid obstacles.", "I found two alternatives: Kinect for Windows (I think that its better that Kinect 360 but I dont know exactly) and Asus Xtion PRO LIVE.", "Searching in the web, I dont found a lot of differences between the two: Asus have USB power supply and better RGM image and Kinect are more complet, with a lot of functions (tracking skelletons, near mode, ...) but I dont know what it will be better to my work.", "Anyone knows which is better or which fits better with my project?", "Thanks!"], "answer": [" ", " ", "First, AFAIK there is a difference between Kinect and Kinect for Windows regarding driver support and so on. Be aware of that.", "Second, if comparing Kinect and Asus Xtion Pro Live: They are essentially the same. The internal hardware is the same and driver and software support are also the same. The main difference is that the Xtion is USB powered, while the Kinect needs an additional 12V power supply. In addition the Kinect is larger.", "So, really, its the same except that the Kinect is 12V powered and Asus is USB no? They have the same features (skeleton tracking, measure depth and distances, etc... right?", "Software and sensors are the same.", "Hello, I know this thread is old but I have 1 question regarding the same topic. I want to use Asus with gmapping to do SLAM. I prefer the minimum bells and whistles, so does an Asus Xtion Pro (without RGB) do the job (direct Depth --> Laser conversion) or do i have to have RGB too for that to work?"], "url": "https://answers.ros.org/question/105571/kinect-or-asus-for-a-movement-base/"},
{"title": "ls: cannot access /dev/ttyACM0: No such file or directory", "time": "2012-08-08 15:10:20 -0600", "post_content": [" ", " ", "Hi, \n   I have installed the Hokuyo UTM-30LX driver as followed by ", "\n    And then I typed $ls -l /dev/ttyACM0 in the terminal, but it displayed that  ls: cannot access /dev/ttyACM0: No such file or directory.\n    Why? Any help is appreciated!!!"], "answer": [" ", " ", "There is no device. Is it plugged in and powered up properly?", "The device might also be any like /dev/ttyACM*.", "Are there any error messages when you type ", "?\nTo check what is relevant, unplug the device, ", ", replug it, ", ", and see what changes. There should be either some messages reported the new device (also giving the device) or error messages.", " ", " ", "Hi all. Thank you for your advice! I have solved the problem by following: ", " ", "I think the main problem is that the port permission is denied, and we could get access by typing the ", "$ sudo chmod a+rw /dev/ttyACM0", "to get access. ", "Hopefully it also works to you guys:)", "This doesn't solve the original problem. If it was a permission problem you would have seen an error message similar to: Permission denied, not No such file or directory."], "answer_code": ["dmesg", "dmesg", "dmesg"], "url": "https://answers.ros.org/question/40934/ls-cannot-access-devttyacm0-no-such-file-or-directory/"},
{"title": "Real time control of the platform without encoders", "time": "2013-05-06 21:21:24 -0600", "post_content": [" ", " ", "Hello", "I would like to do some real time control of the platform that does not contain wheel encoders. So is possible to do some real time control of the motors of that platform (like the position and velocity), but without encoders. I heard that it is a possible to use hard real time control with ROS. I need like ROS real time rosrt and Orocos RTT. Is that true??", "Any help would be welcome. "], "answer": [" ", " ", "If you have hard realtime requirements (e.g. can\u00b4t miss a deadline, otherwise system failure) you can either use a hard realtime Linux approach using a RT-PREEMPT Kernel (or alternatives like Xenomai and RTAI) with rosrt or Orocos RTT. The other option is offloading the hard real-time critical part to a microcontroller and talking to that using rosserial or another approach.\nAs microcontrollers get more powerful and are increasingly easier to integrate with ROS, I\u00b4d recommend the second option. Dealing with hard realtime Linux systems can be very frustrating as they are not very widely used and debugging problems can become very hard.", " ", " ", "From ROS-I FAQs:", "Q: Is ROS-Industrial suitable for real-time control?", "A: Like ROS, ROS-I nominally runs on Ubuntu Linux, which is not a real-time OS. ROS-I is fast enough to run closed-loop with perception systems for industrial applications, but (at least for now) ROS-I must be used as a high-level controller in conjunction with a low-level real-time controller (usually the one from the OEM), which closes servo feedback loops and provides safety behavior (e.g. an E-stop).", "This is also valid for ROS. Hard real time control is not possible using only ROS. You will have to implement (or buy) your own low-level real time controller and connect it to your high level ros nodes, e. g. via ROS serial."], "url": "https://answers.ros.org/question/62281/real-time-control-of-the-platform-without-encoders/"},
{"title": "Upload packages to respositories", "time": "2014-01-19 09:53:23 -0600", "post_content": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi all! \n Im working on a Pharo library for writing ROS nodes, also in some tools based on this technology and some commands for infraestructure. ", "I am wanting to start to use apt-get for installation because of facility. I was thinking in upload in ubuntu developer, but then im quite sure it will be better to have it here. ", "How do i get a space for uploading my work? I searched through site and google without cool results, there is any tutorial? there is any standar process? ", "Thank you very much!"], "answer": [" ", " ", " ", " ", "ROS has nice and powerful releasing envionment.", "\"Releasing\" your package allows you to install your package via debian binary (ie. via ", " on Ubuntu). You can start from looking at \"Get Involved\" wiki page and specifically at ", " section, where its ", " is linked from.", "Before starting that tutorial, you have to have at least your package publicly accessible on a distributed revision control system such as github, ", ", bitbucket etc. Being comfortable with ", " tools (eg. git, svn, hg) would be very helpful (or almost required) if you're not yet.", "Thank you very much! It's very usefull information!"], "answer_code": ["apt-get"], "url": "https://answers.ros.org/question/119376/upload-packages-to-respositories/"},
{"title": "Kinect + Udoo Quad?", "time": "2014-01-01 03:15:15 -0600", "post_content": [" ", " ", " ", " ", "I am wondering if it would be possible to get Kinect to work with Udoo board (Quad).  I have found that there is now support for ", ". Also saw a question asked about ", " but it would really be great if it could be possible for Kinect+Udoo. I wish someone could give some insights on this matter. Thanks. "], "answer": [" ", " ", "If the Xtion works, the Kinect will probably work as well.", "Most of the ARM boards I've seen aren't quite powerful enough to handle the bandwidth of the Kinect, but I think the i.MX6 in the Udoo can probably handle it. I think it's worth a try.", "If you do give it a try, please let me know how it goes.", "Thanks, I will take a look at workarounds done for Beagleboard and RaspberryPi and see if similar steps would apply for Udoo. But looking at the complexity, it would've been great if someone experienced could tackle this challenge.", "The combination of ROS+Kinect experience and ownership of an Udoo board will be difficult to find. I could probably come up with an answer in half a day if I had an Udoo, but I don't have the money or the time to experiment with all of the ARM boards on the market.", "For a start, I would try to install the ros-hydro-openni-camera package and see if it works.", "Small addition: According to an embedded board developer the USB controllers on those ARM boards are often another limiting factor. E.g. on some boards the same controller is in charge of handling both Ethernet and USB connections, what can significantly lowers the available bandwidth.", "The Freescale i.MX6 has separate USB and gigabit ethernet controllers on-chip, so it should be more capable of high-bandwidth USB than most ARM boards. Udoo block diagram: ", " ", " ", "Hi, the kinect now is working with the UDOO. I tested it in the UDOO quad.\nJust follow this steps: ", "Regards "], "url": "https://answers.ros.org/question/114241/kinect-udoo-quad/"},
{"title": "Raspberry Pi and Turtlebot", "time": "2013-02-08 05:35:23 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am very sorry if my question has an obvious answer but:", "I am new to Raspberry Pi and TurtleBot and ROS. I know I can get ROS on the Raspberry Pi, I found a tutorial for that. But before I go through the steps to get it all set up, can I use a Raspberry Pi as the TurtleBot Netbook? Is there anything extra I will need? Is there an example somewhere?", "Also, I'm a fan of Groovy, will TurtleBot even work with ROS Groovy right now because the installation instructions are blank?", "Any help would be greatly appreciated! ", "-Kelly", "I was hoping someone already did this! I am working with some folks who think the best approach for ROS on an RPi is to use ARCH -  supposedly it is faster.  I don't know if it will run as a ROS master host.", "I feel sure that TurtleBot will work on Groovy, once all the problems have been solved. :-)", "DrBot- would you be willing to try to work together on this? I am going to start on the process tomorrow, either way I will post the results here. I am hoping to process the kinect data, or at least send it to the work station. ", "Goal is the RPi will launch the robot and kinnect, but RVIZ will be run on a wifi connected laptop or other mobile device.  I tried to build he openni drivers on Rpi Arch, but the buld failed on the 'clean'.", "BTW  were able to capture data from both a XV-11 neato node,  and a Kinnect using the turtlebot kinnect launch on an ubuntu laptop using fuerte. This is all work associated with the Ubiquity robotics project.", "Hi I'm able to launch turtlebot using raspberry pi wheezy with groovy, and teleop using workstation, but kinect is over burden on processor, were  you able to process kinect data?", "Hi i dont know what im doing wrong, I've installed ros groovy on the raspberry pi and trying to move the kobuki motors with the kobuki node and the keyop but the turtlebot doesn't move. I hope anybody can help me"], "answer": [" ", " ", "Are you trying to just control the motors, or are you trying to do the Kinect image processing?  I think the Pi is powerful enough for the former but not for the latter.", "The Rpi will capture the data and Rviz will be run on a remote computer.", " ", " ", "We are able to run a roscore on Raspian with groovy and use it to control an Arduino.", "The build takes forever on a Pi.  Once we get a working build, we burn SD cards.", "http://youtu.be/6SezZAJZmn4 (youtube)  More info in google groups ubiquity_robots", "check out this link, it also provides a sd image with the mobile and perception preinstalled", " ", " ", "I am working on getting the kinect to work with my raspberry pi but I have not seen anyone getting a pointcloud from a kinect. So no, right now it is not possible to replace the netbook with a raspberry pi and I doubt that it is possible at all.", " ", " ", "If you use 7805 power regulators to feed your rpi, make sure you use heatsinks on them. That took a while to debug....  Also, just use one of the rpi images rather than compiling yourself and save yourself a week of frustration. "], "url": "https://answers.ros.org/question/54761/raspberry-pi-and-turtlebot/"},
{"title": "Turtlebot Netbook Battery Error", "time": "2013-02-13 12:21:54 -0600", "post_content": [" ", " ", " ", " ", "Hi!", "While launching the turtlebot_bringup minimal.launch that generally uses the \"info\" and \"state\" files to check the turtlebot laptop battery I get the following Error: ", "[WARN] [WallTime: 1360800362.124919] Battery : unable to check laptop battery info [[Errno 2] No such file or directory: '/proc/acpi/battery/BAT0/info'] ", "However in Ubuntu Quantal 12.10 these files no longer exist. Now they've broken the data into several different files located in /sys/class/power_supply/BAT0.", "So I'm wondering if there is an updated script (which I haven't been able to find) or if the laptop_battery.py file needs to be changed in order to get it working, however I'm kind of new at this and that would take longer. =S", "Thanks!"], "answer": [" ", " ", "laptop_battery.py has evolved over time, as laptop vendors and driver developers don't follow any sort of standard for how they expose those stats. If you aren't using an official turtlebot laptop from either iHeartEngineering or Clearpath Robotics, chances are that your specific laptop isn't exactly supported. I would suggest posting a ticket on github.com/turtlebot with as much details as you have about the layout of your ACPI -- and of course, if you do get around to fixing it, please submit a pull request with your changes!", " ", " ", "Hi!", "I experienced the same problem and was now able to solve it (at least for my configuration).\nSince I'm no Python programmer and the changes are really quick and dirty, I don't feel able to post a ticket. But maybe this helps some people.", "Setup: ROS Fuerte on Laptop Lenovo Thinkpad X130e (which was delivered with my TurtleBot) running Ubunto 12.04 (fresh install). My battery information is located in /sys/class/power_supply/BAT1.\nI found some information here: mjmwired.net/kernel/Documentation/power/power_supply_class.txt\nMy system provided power_now, energy_now and voltage_now that I could use to calculate the values. Since all such entries just contain a single value, you don't need to parse anything, just devide it by 1.000.000 because values are supplied in micro-A/W/V. Also remember to change the path to the battery in minimal.launch!", "Attached see my changed laptop_battery.py (you can identify modified lines by the commented original lines next to them)."], "answer_code": ["from __future__ import division\n\nimport roslib; roslib.load_manifest('turtlebot_node')\n\nfrom   collections import deque\nimport threading\nimport copy\nimport yaml\nimport math\nimport rospy\n\nfrom turtlebot_node.msg import LaptopChargeStatus\nfrom diagnostic_msgs.msg import DiagnosticStatus, DiagnosticArray, KeyValue\n\ndef _strip_Ah(raw_val):\n    if 'mAh' in raw_val:\n        rv = float(raw_val.rstrip('mAh').strip()) / 1000.0\n    elif 'Ah' in raw_val:\n        rv = float(raw_val.rstrip('Ah').strip())\n    elif 'mWh' in raw_val:\n        rv = float(raw_val.rstrip('mWh').strip()) / 1000.0\n    elif 'Wh' in raw_val:\n        rv = float(raw_val.rstrip('Wh').strip())\n    else:\n        raise Exception('Value %s did not have supported units. (mAh,Ah,mWh,Wh)' % raw_val)\n    return rv\n\ndef _strip_V(raw_val):\n    if 'mV' in raw_val:\n        rv = float(raw_val.rstrip('mV').strip()) / 1000.0\n    elif 'V' in raw_val:\n        rv = float(raw_val.rstrip('V').strip())\n    else:\n        raise Exception('Value %s did not have \"V\" or \"mV\"' % raw_val)\n    return rv\n\ndef _strip_A(raw_val):\n    if 'mA' in raw_val:\n        rv = float(raw_val.rstrip('mA').strip()) / 1000.0\n    elif 'A' in raw_val:\n        rv = float(raw_val.rstrip('A').strip())\n    elif 'mW' in raw_val:\n        rv = float(raw_val.rstrip('mW').strip()) / 1000.0\n    elif 'W' in raw_val:\n        rv = float(raw_val.rstrip('W').strip())\n    else:\n        raise Exception('Value %s did not have supported units. (A,mA,W,mW)' % raw_val)\n    return rv\n\ndef slerp(filename):\n    f = open(filename, 'r')\n    data = f.read()\n    f.close()\n    data = data.replace('\\t', '  ')\n    return data\n\n#/proc/acpi/battery/BAT0/state\ndef _check_battery_info(_battery_acpi_path):\n    o = slerp(_battery_acpi_path+'/energy_full_design')\n\n#    batt_info = yaml.load(o)\n    design_capacity = float(o) / 1000000.0\n    o = slerp(_battery_acpi_path+'/energy_full')\n    last_full_capacity = float(o) / 1000000.0\n#    design_capacity    = _strip_Ah(batt_info.get('design capacity',    '0 mAh'))\n#    last_full_capacity = _strip_Ah(batt_info.get('last full capacity', '0 mAh'))\n\n    return (design_capacity, last_full_capacity)\n\nstate_to_val = {'Full':     LaptopChargeStatus.CHARGED, \n                'Charging':    LaptopChargeStatus.CHARGING, \n                'Discharging': LaptopChargeStatus.DISCHARGING }\n\ndiag_level_to_msg = { DiagnosticStatus.OK:    'OK', \n                      DiagnosticStatus.WARN:  'Warning',\n                      DiagnosticStatus.ERROR: 'Error'    }\n\ndef _check_battery_state(_battery_acpi_path):\n    \"\"\"\n    @return LaptopChargeStatus\n    \"\"\"\n    o = slerp(_battery_acpi_path+'/status')\n\n#    batt_info = yaml.load(o)\n\n    rv = LaptopChargeStatus()\n\n#    state = batt_info.get('charging state', 'discharging')\n    state = o\n    rv.charge_state = state_to_val.get(state, 0)\n    o = slerp(_battery_acpi_path+'/power_now')\n    rv.rate = float(o) / 1000000.0\n#    rv.rate     = _strip_A(batt_info.get ..."], "url": "https://answers.ros.org/question/55151/turtlebot-netbook-battery-error/"},
{"title": "Single board PC for ROS", "time": "2012-10-15 17:44:36 -0600", "post_content": [" ", " ", " ", " ", "Hey", "Which Single board computers are recommended/better suited for running ROS (Fuerte and Ubunutu 12.04 or other recommendation)? ", "This has been asked a few times before, but not recently as far as I could find anwyays, sorry if double up though.", "My application will be running with a Hokuyo URG LIDAR (", "/) and using an Arduino to do all the motor/servo controls.", "All low level control such as SLAM will done on board so will need to be capable of processing everything. ", "It will also be sending and receiving high level information from a main computer so will need to be network capable as well, we do currently have a Wireless modem on the robot itself so the SBC does not need built in wireless. ", "Cheers for any advice ", "Henry"], "answer": [" ", " ", "From my point of view, one of the best ways to find out which SBC to use is to take a look at what other people have published regarding their experiments with ROS. As an example, in ", " wiki page, there are some references to published works that include technical details about a working SBC based flying robot doing Visual SLAM and navigation. ", "From my own experience, you need a SBC with least a 1.0Ghz modern CPU + 2GB of RAM. If power consumption is not a concern, I would recommend Intel Core CPUs over Atoms.", "hey cheers", "Yea been trying to find SBC that other people have used, so many of them though haha", "Useful recommendation information, will keep that in mind", "hey cheers", "Yea been trying to find SBC that other people have used, so many of them though haha", " ", " ", "It sounds like you mainly want to process point cloud data?  therefore, perhaps a better question is \"how much PC do i need to run PCL?\". You could ask directly on the PCL (Point Cloud Library) mailing list?", " ", " ", " From my own experience, you need a SBC with least a 1.0Ghz modern CPU + 2GB of RAM. If power consumption is not a concern, I would recommend Intel Core CPUs over Atoms.\nSome good ones here  ", " . Never tried though so am not the voice of experience ...h ", " ", " ", " ", " ", "Some good ones here ", ". Never tried though so am not the voice of experience ... "], "url": "https://answers.ros.org/question/45950/single-board-pc-for-ros/"},
{"title": "My kinetic camera stop working (no device detected)", "time": "2014-01-07 07:36:30 -0600", "post_content": [" ", " ", " ", " ", "update: assuming there are no USB 2.0 to 3.0 adapters since 3.0 was to be compatabile does ubuntu 13.10 updated kernel off any hope. I am running 13.04", "update: tried to reinstall everything then run freenect launch. I get the following first waiting message then I plug the device in and get what follows. The USB ports are 3.0", "Update: There seems to be some dissusion on a share lib causing some issues: I looked for it on my machine and it looks a bit confused any thought this.", "Update: There is a CD with my kinetic. Is there anything useful in this to address this issue?", "Update: If I unplug the device and run glview again I get: From what I can see this is a known problem with the linux kernel since 2012 but little has been done to address it. I tried several solutions in the blogs but non seem to work", "Update: I tried the following utility", "Update: lsusb -v"], "answer": [" ", " ", "It sounds like you have USB 3.0 problems, since you can't use the Kinect on a USB 3.0 port (in general, sometimes it just works, sometimes it doesn't). Is it possible to disable USB 3.0 in your BIOS on a Mac? If not, try changing ", " in /etc/openni/GlobalDefaults.ini to some other value (just try the four other options).", "Oh wow, I didn't even see that the first time.  But he said that it had worked before I think.\n\nAnyways, yes. 3.0 = no bueno.", "Yeah I don't know why it would suddenly stop working...that happened to me once, I never found out the root problem, but it was fixed by changing the UsbInterface field. Maybe some kind of Kernal update?", "The USB line from my Mac GlobalDefaults.ini\n; USB interface to be used. 0 - FW Default, 1 - ISO endpoints, 2 - BULK endpoints. Default: Arm - 2, other platforms - 1\n\nAny one look better then the others?", "A USB 3.0 to 2.0 adapter won't work, I've tried that with a USB 2.0 hub. I suggest just trying each one of those interfaces, the GlobalDefaults file is loaded when OpenNI is run (ie with roslaunch openni_launch or your own variant) so just modify the file, try to start it, repeat.", "Your update with the most recent output tells me the interface (from GlobalDefaults.ini) might be the problem: it seems able to identify the camera on the bus, but won't get its serial number so it can't talk to the camera.\n\nCan you disable USB 3.0 in OpenFirmware (see: ", "Thanks Tim. I have been scanning for any info on how to turn off/on usb in the BIOS. Not something I normally do and would be great to see instructions or a demo. But I can not seem to find anything. Any idea where I can look?", "I've never owned a Mac so I have no clue..I only use one for video editing :) I'll ask around in my lab and see if anyone knows. It might not be possible. Did the UsbInterface thing not work?", "I came across this which is encouraging but does not tell me how to do this on a mac.\n", ".\nThank you .", " ", " ", " ", " ", "I just use @zero so that it will default to the correct number.  It works for me with openni_launch, you might be experiencing an openni2 problem.  Try this:", "Your ", " looks pretty normal, I think the computer is detecting a camera fine.  Make sure you run ", " after you've connected but before you try roslaunch openni_launch to ensure you're on the proper bus.", "Edit: Could you post a picture of your rqt_graph picture and roswtf results to see if there are any messages that aren't communicating properly?  Also I'd check to see if there is anything else currently on the bus you're trying to use.", "Thanks for looking....but still not working.", "How do I check to see what else is on bus...camera plugin it to usb port. Also this was working a month or  so ago.", "1) Next time you insert an image I'd suggest uploading to imgur or a similar site and link so that it doesn't blow up the page.\n2) You can see the list with the lsusb command like you used before.  It appears from your previous listing that only the Kinect is on Bus 3\n3) Is the Kinect plugged in?", "Yes...its plugged in has green LED is on. I issue lsusb and use the port the camera is on in the above eg...that would be 3@4. Not the audio which is ", "Can you please modify the image so it doesn't fill the screen?  Also, two other things.  What computer are you using?  And have you tried using bus 1 or 2?  I noticed in all of your responses you keep trying usb ports on bus 3.  If there is enough bandwidth, try another bus. Might be hardware.", "I an new mac running Ubuntu in dual boot. I've tried both ports. And as I said it worked in the past. I tried uninstalling ros and re-installing did not work.  I do not update the data on the question that is why the port is always the same. I have tried different ports.", "Ok, you've tried different ports but are they both still on bus 3?  In other words, when you use -lsusb-, will they list it as Bus 003 each time?  Other than this, I don't know what else it could be.", "yes...I post lsusb with verbose in case it is helpfull"], "question_code": ["[ INFO] [1390668700.050552116]: No devices connected.... waiting for devices to be connected\n[ INFO] [1390668703.054328712]: Number devices connected: 1\n[ INFO] [1390668703.054588759]: 1. device on bus 000:00 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id '0000000000000000'\n[ INFO] [1390668703.056447295]: Searching for device with index = 1\n[ INFO] [1390668703.095462065]: No matching device found.... waiting for devices. Reason: [ERROR] Unable to open specified kinect\n", "sudo glview\nglview: error while loading shared libraries: libfreenect.so.0.2: cannot open shared object file: No such file or directory\nviki@viki:~$ glview\nKinect camera test\nNumber of devices found: 1\nCould not open device\n\n\nsudo find / -name libfreenect.so -ls\n15341498    0 lrwxrwxrwx   1 viki     viki           18 Jan 11 13:08 /home/viki/catkin_ws/src/KinectLibs/libfreenect/build/lib/fakenect/libfreenect.so -> libfreenect.so.0.2\n12716955    0 lrwxrwxrwx   1 viki     viki           18 Jan 11 13:08 /home/viki/catkin_ws/src/KinectLibs/libfreenect/build/lib/libfreenect.so -> libfreenect.so.0.2\n\n\n11141534    0 lrwxrwxrwx   1 root     root           18 Oct  9 21:08 /opt/ros/hydro/lib/fakenect/libfreenect.so -> libfreenect.so.0.1\n10883699    0 lrwxrwxrwx   1 root     root           18 Oct  9 21:08 /opt/ros/hydro/lib/libfreenect.so -> libfreenect.so.0.1\n9438521    0 lrwxrwxrwx   1 root     root           18 May 25  2012 /usr/lib/x86_64-linux-gnu/fakenect/libfreenect.so -> libfreenect.so.0.1\n7355096    0 lrwxrwxrwx   1 root     root           18 May 25  2012 /usr/lib/x86_64-linux-gnu/libfreenect.so -> libfreenect.so.0.1\n\n7480477    0 lrwxrwxrwx   1 root     root           18 Jan 11 13:08 /usr/local/lib64/fakenect/libfreenect.so -> libfreenect.so.0.2\n7357524    0 lrwxrwxrwx   1 root     root           18 Jan 11 13:08 /usr/local/lib64/libfreenect.so -> libfreenect.so.0.2\nviki@viki:~$\n", "Kinect camera test\nNumber of devices found: 0\n", "freenect-glview \nKinect camera test\nNumber of devices found: 1\nCould not open device\n", "Bus 003 Device 021: ID 045e:02ae Microsoft Corp. Xbox NUI Camera\nDevice Descriptor:\n  bLength                18\n  bDescriptorType         1\n  bcdUSB               2.00\n  bDeviceClass            0 ..."], "answer_code": ["UsbInterface", "roslaunch openni_launch openni.launch device_id:=3@0\n", "lsusb", "lsusb"], "url": "https://answers.ros.org/question/115581/my-kinetic-camera-stop-working-no-device-detected/"},
{"title": "Run laserscanner in Android", "time": "2014-02-03 21:23:23 -0600", "post_content": [" ", " ", " ", " ", "Hello, I want to driver my Laserscanner under Android system (a smartphone or a tab). I am using Hokoyo Laserscanner UTM-30LX.  Does anyone have experience?", "How to supply the Laserscanner from smartphone or tab? Do I need an adapter to give the scanner appropriate Voltage?", "Normally the USB in smartphone or tab has a mini USB. How can I use a Y cable to connect the Laserscanner and the smartphone or tab?", "For your answer my heartfelt appreciation!"], "answer": [" ", " ", "I don't think this is going to work. You'll need 12V. Technically you can build a voltage converter that gives you 12V from USB voltage, but I think a smartphone USB will be way too less power for a UTM-30LX. It's definitely too much for USB specs. Do NOT use a Y cable!!!", "In addition, you'll need a USB host port. My guess is most smartphones don't have that."], "url": "https://answers.ros.org/question/125851/run-laserscanner-in-android/"},
{"title": "EKF dosen't work perfectly", "time": "2014-02-07 03:52:29 -0600", "post_content": [" ", " ", "when i launch exf for my ", " data , the results are the same of input\nthis is my lunch file :", "all of my topices are:", "first of all, i launch ekf file and then use this ", " command to run my dataset."], "answer": [" ", " ", "I'm not quite sure what your question is. If you're only data source is odometry, which is how your launch file is setup, then the EKF will basically just be able to spit that odometry back at you with an increasing uncertainty.", "thank you, my gole is fusing /odom and /vo , but i read kalman could increase accuracy of data even for one sensor, so i started for /odom, but the result of /odom_combined is the same of /odom. do you have any idea?", "That's not how Kalman filters work.  With only one data source, at best, you get the same data out.  At worst you can achieve a \"smoothed\" laggy output version of your original data (depending on your update model and process/sensor noise values).", "That's assuming you're filtering on variables you're directly measuring.  Kalman filters are also very good at measuring \"hidden\" values.  For instance, calculating accelerations from GPS or velocity commands.  Or estimating model parameters from output data (battery time until full while charging)."], "question_code": ["/odom", "<launch>\n\n<node pkg=\"robot_pose_ekf\" type=\"robot_pose_ekf\" name=\"robot_pose_ekf\">\n  <param name=\"output_frame\" value=\"odom_combined2\"/>\n  <param name=\"freq\" value=\"50.0\"/>\n  <param name=\"sensor_timeout\" value=\"1.0\"/>  \n  <param name=\"odom_used\" value=\"true\"/>\n  <param name=\"imu_used\" value=\"false\"/>\n  <param name=\"vo_used\" value=\"false\"/>\n  <param name=\"debug\" value=\"false\" />\n  <param name=\"self_diagnose\" value=\"true\" />\n</node>\n\n</launch>\n", "/base_odometry/odometer\n/base_odometry/state\n/base_scan\n/clock\n/odom\n/robot_pose_ekf/odom_combined\n/rosout\n/rosout_agg\n/tf\n/tilt_scan\n/torso_lift_imu/data\n/torso_lift_imu/is_calibrated\n", "rosbag play 2011-04-11-07-34-27.bag  /base_odometry/odom:=/odom --clock"], "url": "https://answers.ros.org/question/127171/ekf-dosent-work-perfectly/"},
{"title": "slam map inconsistent", "time": "2014-03-02 22:40:18 -0600", "post_content": [" ", " ", "Hello, I was building a map using hector slam, the result was good but I found map inconsistent when the robot U - turn from a dead end. I don't have odometry on the robot. The first picture shows the robot is going in to a dead end. The second picture shows the robot go back to the corridor that it has scanned before. There is some inconsistency on the mapping which it seems like the robot don't 'remember' the corridor and mapping a new area. May I know how could I solve this problem? Or do I just avoid dead end at any cost? Thank you.", "This is the parameter for the hector mapping"], "answer": [" ", " ", "Try with this parameters:", "But the map also depend to the soft moves and the power of your CPU."], "question_code": ["<node pkg=\"hector_mapping\" type=\"hector_mapping\" name=\"hector_mapping\"    output=\"screen\">\n\n<param name=\"use_tf_scan_transformation\" value=\"true\" />\n>param name=\"use_tf_pose_start_estimate\" value=\"false\" />\n<param name=\"scan_topic\" value=\"scan\" />\n\n<param name=\"pub_map_odom_transform\" value=\"true\"/>\n<param name=\"map_frame\" value=\"map\" />\n<param name=\"base_frame\" value=\"base_link\" />\n<param name=\"odom_frame\" value=\"base_link\" />\n\n<!-- Map size / start point -->\n<param name=\"map_resolution\" value=\"0.075\"/>\n<param name=\"map_size\" value=\"512\"/>\n<param name=\"map_start_x\" value=\"0.5\"/>\n<param name=\"map_start_y\" value=\"0.5\" />\n<param name=\"laser_z_min_value\" value=\"-2.5\" />\n<param name=\"laser_z_max_value\" value=\"3.5\" />\n\n<!-- Map update parameters -->\n<param name=\"update_factor_free\" value=\"0.4\"/>\n<param name=\"update_factor_occupied\" value=\"0.9\" />    \n<param name=\"map_update_distance_thresh\" value=\"0.2\"/>\n<param name=\"map_update_angle_thresh\" value=\"0.4\" />\n<param name=\"scan_subscriber_queue_size\" value=\"25\" />\n<param name=\"map_multi_res_levels\" value =\"2\"/>\n</node>\n"], "answer_code": [" <launch>\n <node pkg=\"hector_mapping\" type=\"hector_mapping\" name=\"hector_mapping\"    output=\"screen\">\n\n <!-- Frame names -->\n <param name=\"base_frame\" value=\"base_link\" />\n <param name=\"odom_frame\" value=\"base_link\" />\n <param name=\"output_timing\" value=\"false\"/>\n\n <param name=\"map_frame\" value=\"map\" />\n\n <!-- Tf use -->\n <param name=\"pub_map_odom_transform\" value=\"true\"/>\n\n <!-- Map size / start point -->\n <param name=\"map_resolution\" value=\"0.025\"/>\n <param name=\"map_size\" value=\"6000\"/>\n <param name=\"map_start_x\" value=\"0.5\"/>\n <param name=\"map_start_y\" value=\"0.5\" />\n <param name=\"map_pub_period\" value=\"1\"/>\n <param name=\"laser_max_dist\" value=\"20\" />\n <param name=\"laser_z_min_value\" value=\"-2.5\" />\n <param name=\"laser_z_max_value\" value=\"7.5\" />\n <param name=\"update_factor_free\" value=\"0.4\"/>\n <param name=\"update_factor_occupied\" value=\"0.7\" />    \n <param name=\"map_update_distance_thresh\" value=\"0.2\"/>\n <param name=\"map_update_angle_thresh\" value=\"0.06\" />\n\n </node>\n </launch>\n"], "url": "https://answers.ros.org/question/134906/slam-map-inconsistent/"},
{"title": "Kinect for Windows Install Ubuntu 13.04 w/Hydro", "time": "2014-02-20 20:52:54 -0600", "post_content": [" ", " ", " ", " ", "Good day everyone,", "I've been trying different settings to achieve this, with no success. My settings are as follow:", "\nUbuntu 13.04", "\nROS version Hydro Medusa", "\nfreenect_stack install as in (can't insert link) the install instructions of the wiki", "\nKinect for Windows 1517 fully functional (tested with Microsoft SDK)", "\nI've tried in two different computers, same settings, the difference between them is:", "So far, what I've read is that, as I'm using Ubuntu 13.04 I don't need to do the kernel update to fix USB 3.0 (can't insert link yet) . Nevertheless the result is exactly the same wheter I use the 3.0 ports or the combo one. After using the command ", " I get the following error:", "[ INFO] [1393001100.077038281]: No devices connected.... waiting for devices to be connected", "[ INFO] [1393001103.079356057]: No devices connected.... waiting for devices to be connected", "Using the command ", " I get the following:", "Bus 001 Device 002: ID 8087:8008 Intel Corp. ", "\nBus 002 Device 002: ID 8087:8000 Intel Corp. ", "\nBus 003 Device 008: ID 045e:02c2 Microsoft Corp. ", "\nBus 003 Device 002: ID 05af:1023 Jing-Mold Enterprise Co., Ltd ", "\nBus 003 Device 003: ID 046d:c52b Logitech, Inc. Unifying Receiver", "\nBus 003 Device 004: ID 22b8:710f Motorola PCS ", "\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub", "\nBus 003 Device 011: ID 045e:02be Microsoft Corp. ", "\nBus 003 Device 012: ID 045e:02bf Microsoft Corp.", "My guess is that the Microsoft Corp devices are the Kinect Audio, Camera and Aux (The ID 045e corresponds to it).", "Any help, suggestion, recommendation with this? It's really driving me crazy. Let me know if I can give you any more information. Thank you!", "Regards", "Will openni_launch, support kinect xbox (model 1414) for  ubuntu 14.04 lts, ros indigo?"], "answer": [" ", " ", "Good day everyone. I've been working so far with this problem and I've got a answer. Maybe it's not the most convenient one, but I got results.", "So far, everything I've been reading says only Kinect, never specifing which type of Kinect. In my lab we had the previously described one. Today, I went to a friends house and borrowed his Kinect for Xbox 360 (model 1414). I tried all the previous instructions, and checked as instructed by expelliarmus the ", " page. Finally, Kinect is working with Linux, is being recognized and I can start testing ASAP.", "Summarizing, use a Kinect for Xbox 360. ", "However, take into consideration that this version does not include a USB, so you need to use an adapter which supplies power to the Kinect and also enables the USB (e.g. The one supplied with Kinect for Windows, which I'm using ATM)", "Will openni_launch, support kinect xbox (model 1414) for ubuntu 14.04 lts, ros indigo?", " ", " ", "How about taking a look at ", " page. ", "Thank you. I already check it. Whenever I try to start the **roslaunch openni_launch openni.launch\n** I receive an error that reads:", " [openni.launch] is neither a launch file in package [openni_launch] nor is [openni_launch] a launch file name", " \nI also tried ** roslaunch freenect_launch freenect-xyz.launch**, but still receive the \"No devices connected...\" error.", "You do not openni package installed. Try this command and see:\n\nsudo apt-get install ros-hydro-openni-launch", "Sorry for taking so long to try this. As you said, I didn't have the package installed. Nevertheless with the roslaunch openni_launch openni.launch I still get the \"No devices connected...\"\nI also tried the  roslaunch freenect_launch freenect-xyz.launch with same results.\nI think the problem is that lsusb shows the Kinect is pluged in, but it just recognizes it as Microsoft Corp. I don't know if the version of my kinect has something to do with it (Kinect for windows 1517)"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "XPS Dell - 2 USB port 3.0 + 1 USB/eSATA combo port.", "Assembly computer with 6 USB port 3.0"], "url": "https://answers.ros.org/question/131671/kinect-for-windows-install-ubuntu-1304-whydro/"},
{"title": "URDF alternative of SDF revolute2 joint type", "time": "2014-03-11 11:31:25 -0600", "post_content": [" ", " ", " ", " ", "For our robot we had a Gazebo model built in SDF. The real robot has two unpowered omniwheels. In the SDF model, these omniwheels were modelled as a ball joint: a sphere with the radius of the omniwheel and two axes of rotation around the X and the Y axis. This worked perfectly and allowed the robot to move freely in simulation as it does on the real robot.", "SInce we are now trying to integrate with MoveIt, the need has risen to use URDF instead of SDF. However, the revolute2 joint type is not supported by URDF, for reasons unclear to me. The alternative that is usually suggested is to use two chained revolute joints by the use of a virtual link.", "First of all, the virtual link needs physical properties: if it does not have a <inertial> part with a <mass> element, it will just be completely omitted by gzsdf. What physical properties can I attribute to a non-existing virtual joint?", "Then, if I just make up some value for mass and set up the joint to have 2 chained axes of rotation, it kind-of works. But not completely, and not equal to how the revolute2 joint did. The problem is that whenever a movement is initiated in a direction that does not align with one of the two chained axes of the omniwheels, it will behave like a caster-wheel: it will move sideways a bit until it is in the appropriate orientation. This means unpredictable movement that does not occur on the real robot. This also did not occur when using the revolute2 joint types in Gazebo.", "So now I'm stuck. I really do not feel like maintaining an SDF and an URDF version of the model. However, URDF seems to have discarded revolute2 as a valid joint type (even if it is directly supported by the SDF format to which it converts, which is the only thing that I want as the only thing that matters here is avoiding the friction or imbalance by fixing or leaving out the omniwheels).", "One possible, but very complex method would be to make an accurate model of the omniwheels and add a separate joint for each roller. However, this would result in an additional 40 joints (2 2-layered wheels with 10 rollers) and I doubt this will be beneficial for the performance of Gazebo.", "Any insights on how to fix this would be greatly appreciated."], "answer": [" ", " ", "\"Unpowered omniwheels\" are not really going to be useful to MoveIt (or ROS even), and so you probably only need them for simulation. Within MoveIt, you probably only need rough collision data (and if your arm can't hit the wheels, then even that may not be necessary).", "I think what you are actually looking for with MoveIt is to create a virtual link which is \"planar\" type for the mobile base itself. Presumably you have a controller which, given a plan for where to move the mobile base, can control the powered wheels.", "Now, as to how you would do this without maintaining both the URDF and SDF, I'm not entirely sure. In the past, I have used XACRO to add simulation-only things (such as sensor plugin XML blocks) to an existing URDF, thereby creating the \"simulation-ready\" URDF. I'm not sure how the URDF->SDF converter actually works -- perhaps it will simply pass a SDF block through without trying to convert it? It would be a bit of a hack, but it might work...", "Thanks. It indeed a nice solution to wrap an entire <link> in a <gazebo></gazebo> element so that it gets passed through as-is. This makes it so that it is completely ignored in MoveIt!, which indeed has no reason to have it. In Gazebo I can now use the revolute2 joint ype again!", "I'm in a similar situation. I am trying to make joints for a neck and arms that would require at least 2 DOF. But when I try to wrap a link in a gazebo element, gazebo cannot see it at all and the simulation fails to load. Maybe since I'm using xacro? Any help would be greatly welcomed.", " I don't know why it does not work for you. However, we are also using xacro for the omniwheels. The omniwheels are contained in a separate file, omniwheels.urdf.xacro, of which I have posted the contents on:  ", "Hope this helps!", "Thanks for the help, but it still seems the link doesn't appear at all. Gazebo also recognizes the next link as a new root so the model can't load. I'll keep trying.", "Hi, when i use the revolute2 joint type, i got confused that how can i control axis1 or axis2 rotate? I use the URDF file and reference as gazebo tags to define the joint , and then i try to use gazebo-ros-control to control the revolute2 joint, but how can i define which axis i want to control?"], "url": "https://answers.ros.org/question/138736/urdf-alternative-of-sdf-revolute2-joint-type/"},
{"title": "terminate & launch other node from code", "time": "2011-12-07 10:30:19 -0600", "post_content": [" ", " ", " ", " ", "Is there a ROS way to terminate and launch another node from one node's code?", "For launching example in python, it's possible by doing something like this w/o using ROS:", "Any way to do the same in an official way in ROS? ", "How about terminating as well? If a node run as \"respawn = true\" receives \"shutdown\" msg/service request then shuts down itself? Better idea?", "Update) I thought I'd better describe a situation when I need this...There is a node that gets to consume higher CPU in a process of time (eventually CPU load reaches 12.0 or above although it's not supposed to do so). And I found that restarting makes it stay calmer and still functions. Besides debugging that node itself, it would be nicer if I can just work around the issue for now by restarting the node.", "Update-2) Found ", " and ", " might be good as well for launching from ", " (I'm sure the argument for ", " is ", " instead of ", " as in ", ")."], "answer": [" ", " ", "Yes, of course, you can start a node from another node using your favorite process management API.\nIn ROS, you also have ", " which will provide this kind of service.", "However, it seems to me that what you want to achieve does not really fit well with the ROS framework. When using component oriented frameworks, the general idea is to start all the services at the beginning (using a roslaunch file for instance) and stating nodes relations and configurations using the ROS parameters.\nThen no nodes should be killed before the whole system is shutdown.", "Then monitoring tools and/or interactive tools should just be launched from outside, using rosrun.", "can you tell me how it will be done in rosjava? i mean which api in rosjava is used to run another node?", " please don't use comment section to ask a whole new question. Open a new one.", " ", " ", " ", " ", "We've encountered this problem as well. On our robot, we have ROS set to autoboot on power-up. At this point, it boots into a \"minimal\" state in which only a few nodes are running until we receive a connection to an external network. At that point, it will launch the rest of the ROS nodes. Why we do this isn't really important, but here's HOW we do it... Just a warning, it's fairly complex. ", "Our whole autoboot/control system is based on a Bash/C++ interface. You are able to pipe the output of a C++ program to a Bash script and read the output. To do this, your C++ program must simply output to stdout and Bash can read it. Here's a quick example of what I mean:", "control.cpp", "control.bash", "In the above example, \"/home/robot/scripts/control\" represents the location of the c++ executable on the filesystem. The reason it's wrapped in a Bash function is because Bash doesn't allow you to pipe an executable directly in to the function. I.E. \"", "\" is not allowed. That's why the executable is wrapped in the Bash function cpp_func.", "This allows us to execute any functions we want (start, kill, restart, etc.) on any node we want. As long as you expect an output in your Bash script, you can handle any ROS commandline processes from that one control.bash script. Just a note, you cannot send commands back to the c++ program using this method. ", "I can't understand \"/home/robot/scripts/control\", is this the path where \"control.cpp\" locates or some other path?", " ", " ", "I just had the same problem and was stunned to see how difficult it was to find a simple example for this basic task.\nSo I added a minimal code example to the roslaunch wiki page:", "Is there a way not just to start a single node, instead start an whole launch file?", " it looks like there might be api entries for that but they aren't currently implemented:  ", "Though you can start a launch file bypassing the api ", "- but it blocks and doesn't have the stop/is_alive methods."], "question_code": ["import os\nos.system('roslaunch turtlebot_navigation_ours amcl_customized.launch')\n", "roslaunch.scriptapi", "python", "ROSLaunch.launch", "roslaunch.core.Node", "roslaunch.Node"], "answer_code": ["#include <iostream>\n#include <string>\n#include <unistd.h>\n\nusing namespace std;\n\nint main(int argc, char** argv)\n{\n    while(1)\n    {\n      if(guiNotConnected())\n      {\n        cout << \"Connected\" << endl;\n      }\n      sleep(1);\n    }\n}\n", "#!/bin/bash\n\nfunction cpp_func {\n\n    /home/robot/scripts/control\n\n}\nwhile read line; do\n    if [ \"$line\" == \"Connected\" ]; then\n        roslaunch robot_launch rest_of_ros.launch\n    fi\ndone < <(cpp_func)\n", "done << /home/robot/scripts/control", "import roslaunch; args = ['roslaunch', 'my_package', 'foo.launch']; roslaunch.main(args)"], "url": "https://answers.ros.org/question/12260/terminate-launch-other-node-from-code/"},
{"title": "understanding velodyne_pointcloud package [closed]", "time": "2014-03-26 01:32:58 -0600", "post_content": [" ", " ", "I have read and test the manual of this package in hydro but I have several questions I hope you can help.", " and nodelet do? There is no documentation in the package.", " file call the driver and the conversor in the same code but it still require the intern messages in topic /velodyne_packets (VelodyneScan.msg). Can I fuse these processes without the intern messages? Fusing the two nodelets in one?", "And now several questions about the nodelet TransformNode executing with ", ".", " /hydro/lib/libtransform_nodelet.so ? or /shared/velodyne_pointcloud/launch/transform_nodelet.launch?", " such as frame_id? or the parameter are fixed?", " Is not a field in the message. But it is required for visualization with rviz.", "Thanks in advance"], "answer": [" ", " ", " ", " is mostly just a demo of how to get the ring number for each laser. It assigns rainbow colors to each successive laser, allowing you to figure out what laser generates each point in the rviz display.", " The two nodelets run as separate threads in the same nodelet manager process. The /velodyne_packets messages flow between them using zero-copy pointers. I do not recommend trying to combine them into a single nodelet.", " ", " Parameters are specifically provided so you can change them.", " Transform node is a different implementation, but the resulting point clouds should be the same.", " ", " is a field in the message header, i.e: ", ". See ", ".", "In the 4th, you mean you can or can't change them?", "Yes, you can change them. That is the purpose of making something a parameter.", "How can I check if there is messages with zero-copy pointers between the two nodelets inside the 32e_points.launch file as you have said? zero-copy messages means INTRAPROCESS instead of TCPROS transport communication? I have seen there are the same rostopic list -v , and the same rosnode info velodyne_nodeleT_manager communications (intraprocess and tcpros) than if I use the 2 nodelets separately  (roslaunch velodyne_driver nodelet_manager.launch and roslaunch velodyne_pointcloud cloud_nodelet.launch).", "Both the driver and the point cloud conversion run in the same \"velodyne_nodelet_manager\" process. You can verify that by reading the launch files. If you believe this question was answered, please click the green check-mark.", "After using velodyne_pointcloud,is it need to pointcloud processing and  registration?\nAnd then, can I get pcap file from velodyne gazebo model (not real velodyne)?"], "answer_code": ["RingColors", "/hydro/lib/libtransform_nodelet.so", "frame_id", "msg.header.frame_id"], "url": "https://answers.ros.org/question/144566/understanding-velodyne_pointcloud-package/"},
{"title": "beaglebone and turtlebot", "time": "2014-04-01 02:45:12 -0600", "post_content": [" ", " ", "Since I had to offload most of the computation done for a project I'm working on to another computer, I wanted to substitute the netbook I'm using right now on my homebrew turtlebot with a system on a chip, like the beaglebone black. I wanted to know if it was enough to handle pretty much just the kinect and the serial interface to the Create, or if I had to consider other systems. From what I've read, it seems like it would be better to use a dual core SoC, like the pandaboard, but it would cost me more than using two other \"smaller\" SoCs, so another possibility I was considering was to use more than one board and further distribute the nodes among them (eg, one board dedicated to the kinect and the other to the create). Can anyone offer some advise?", " might have something to say about it?", "I agree with ", " :)"], "answer": [" ", " ", "The general consensus from the people who have tried is that the Beaglebone and other small ARM boards don't have enough CPU power or USB bandwidth to handle the full-resolution data stream from the Kinect well.", "This is made even worse on a number of boards where the ethernet interface is attached to the USB bus, requiring the kinect and the network port to share bandwidth. The kinect uses something like 90% of the theoretical USB bandwidth, ethernet is another 20%, and then there's overhead in the protocol to be accounted for as well; the numbers just don't add up."], "url": "https://answers.ros.org/question/146926/beaglebone-and-turtlebot/"},
{"title": "Turtlebot_bringup[WARN]Create : robot not connected yet, sci not available", "time": "2014-05-05 01:59:57 -0600", "post_content": [" ", " ", " ", " ", "Hi, every body.\nWhen I use the command line \"roslaunch turtlebot_bringup minimal.launch\", this \"[warn]Create: robot not connected yet, sci not available\" always occurs. I use the Hydro edition in Ubuntu 12.04. And the turtlebot was bulid by myself with a Lenovo laptop and iRobot Create.", "I have tried several solutions on the web, but none works.\nSome of them are:", "Change the permission of /dev/ttyUSB0 using  \"chmod 777\"", "Change the type of TURTLEBOT_BASE in file minimal.launch", "Install the driver on the page ", "\nBut this conflicts with the driver in the turtlebot package.", "Change the code in turtlebot_node.py:\ndef set_operation_mode(self,req):\n      if not self.robot.sci:\n          raise Exception(\"Robot not connected, SCI not available\")", "It is definitely related with self.robot.sci, but I still have no idea of what causes the warning\u3002 ", "Any ideas? Thanks."], "answer": [" ", " ", "Usually that means that the Create is not turned on or the serial cable is not plugged into the robot fully. ", " There was also an issue if the battery is not charged:  ", "In fact, I checked this page before, but it seemed useless.\nI'm sure the Create is turned on and the serial cable is plugged into the robot fully, because I can even control it using the keyborad.\nThe problem is that the warning still occurs.", "If you're driving something around with the keyboard it suggests that you are already running the create driver. Thus you shouldn't be launching another copy. Or does it work with this warning appearing?", "Yes, it works with this warning. I just wonder why the warning occurs. Or maybe I should ignore it.\nThank you so much. :)"], "question_code": ["> turtlebot@turtlebot:~/turtlebot/src/irobot/irobot_create_2_1/src/irobot_create_2_1$\n> roslaunch turtlebot_bringup\n> minimal.launch  ... logging to\n> /home/turtlebot/.ros/log/45c9215e-d42b-11e3-9c80-001e6557542c/roslaunch-turtlebot-14349.log\n> Checking log directory for disk usage.\n> This may take awhile. Press Ctrl-C to\n> interrupt Done checking log file disk\n> usage. Usage is <1GB.\n> \n> started roslaunch server\n> http://192.168.1.104:48516/\n> \n> SUMMARY\n> ========\n> \n> PARAMETERS  *\n> /cmd_vel_mux/yaml_cfg_file  *\n> /diagnostic_aggregator/analyzers/digital_io/path\n> * /diagnostic_aggregator/analyzers/digital_io/startswith\n> * /diagnostic_aggregator/analyzers/digital_io/timeout\n> * /diagnostic_aggregator/analyzers/digital_io/type\n> * /diagnostic_aggregator/analyzers/mode/path\n> * /diagnostic_aggregator/analyzers/mode/startswith\n> * /diagnostic_aggregator/analyzers/mode/timeout\n> * /diagnostic_aggregator/analyzers/mode/type\n> * /diagnostic_aggregator/analyzers/nodes/contains\n> * /diagnostic_aggregator/analyzers/nodes/path\n> * /diagnostic_aggregator/analyzers/nodes/timeout\n> * /diagnostic_aggregator/analyzers/nodes/type\n> * /diagnostic_aggregator/analyzers/power/path\n> * /diagnostic_aggregator/analyzers/power/startswith  * /diagnostic_aggregator/analyzers/power/timeout\n> * /diagnostic_aggregator/analyzers/power/type\n> * /diagnostic_aggregator/analyzers/sensors/path\n> * /diagnostic_aggregator/analyzers/sensors/startswith\n> * /diagnostic_aggregator/analyzers/sensors/timeout\n> * /diagnostic_aggregator/analyzers/sensors/type\n> * /diagnostic_aggregator/base_path  * /diagnostic_aggregator/pub_rate  *\n> /robot/name  * /robot/type  *\n> /robot_description  *\n> /robot_pose_ekf/freq  *\n> /robot_pose_ekf/imu_used  *\n> /robot_pose_ekf/odom_used  *\n> /robot_pose_ekf/output_frame  *\n> /robot_pose_ekf/publish_tf  *\n> /robot_pose_ekf/sensor_timeout  *\n> /robot_pose_ekf/vo_used  *\n> /robot_state_publisher/publish_frequency\n> * /rosdistro  * /rosversion  * /turtlebot_laptop_battery/acpi_path  *\n> /turtlebot_node/bonus  *\n> /turtlebot_node/port  *\n> /turtlebot_node/update_rate  *\n> /use_sim_time\n> \n> NODES   /\n>     cmd_vel_mux (nodelet/nodelet)\n>     diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n>     kinect_breaker_enabler (create_node/kinect_breaker_enabler.py)\n>     mobile_base_nodelet_manager (nodelet/nodelet)\n>     robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n>     robot_state_publisher (robot_state_publisher/robot_state_publisher)\n>     turtlebot_laptop_battery (linux_hardware/laptop_battery.py)\n>     turtlebot_node (create_node/turtlebot_node.py)\n> \n> ROS_MASTER_URI=http://192.168.1.104:11311\n> \n> core service [/rosout] found\n> process[robot_state_publisher-1]:\n> started with pid [14370]\n> process[diagnostic_aggregator-2]:\n> started with pid [14391]\n> process[turtlebot_node-3]: started\n> with pid [14474]\n> process[kinect_breaker_enabler-4]:\n> started with pid [14475]\n> process[robot_pose_ekf-5]: started\n> with pid [14479]\n> process[mobile_base_nodelet_manager-6]:\n> started with pid [14517]\n> process[cmd_vel_mux-7]: started with\n> pid [14587] [WARN] [WallTime:\n> 1399283836.636816] Create : robot not connected yet, sci not available\n> process[turtlebot_laptop_battery-8]:\n> started with pid [14669] [WARN]\n> [WallTime: 1399283839.642510] Create :\n> robot not connected yet, sci not\n> available [kinect_breaker_enabler-4]\n> process has finished cleanly\n> \n> log file:\n> /home/turtlebot/.ros/log/45c9215e-d42b-11e3-9c80-001e6557542c/kinect_breaker_enabler-4*.log\n"], "url": "https://answers.ros.org/question/161321/turtlebot_bringupwarncreate-robot-not-connected-yet-sci-not-available/"},
{"title": "How to mount a Kinect with my laptop without any AC Adapter [closed]", "time": "2011-05-02 04:43:08 -0600", "post_content": [" ", " ", " ", " ", "Hi team,", "I would like to use Kinect without any AC Adapter. Turtlebot use use energy from roomba, but I would like to use other platform to navigate so I need to learn something similar to this image:", "Does anyone any reference to give energy to Kinect?", "Cheers", "Notes, ", "\nI have seen it stated in a university tutorial that the kinect will operate reliably down to 8V, ", "\nThis could alow regulated power from a 12v battery, ", "\nHaven't tried it myself. ", "\nThe original turtlebot power gyro board uses a regulator with a enable pin to alow turning it off for charging"], "answer": [" ", " ", " ", " ", "Depending on what voltage battery you have I suggest getting a linear voltage regulator. You should not hook the power cable from the kinect directly to the battery, you will get variations in voltage which will break your kinect. This tutorial will show you how to wire up a voltage regulator for a kinect, make sure to buy a voltage regulator that will down convert from you battery voltage, and then connect the power side pigtail to your battery: ", ".", " ", " ", "This is not an answer, but a warning.", "A 12v lead-acid battery of that size has the strength to literally melt your face. The amperage it can put out makes it very dangerous. It is likely able to spot weld metal if you drop something across the contacts, and poses an explosion hazard. If you don't know what you're doing with soldering/wiring/managing power, do not learn with trial and error.", "I'm not sure how familiar you are with these things, but please, be safe :)", " ", " ", "You might want to use the Asus Xtion sensor.", "It only needs USB power.", " ", " ", " ", " ", "You will need 12V Power from somewhere.", "Take the kinect splitter cable and cut the part connected to the power supply.\nThe safest way is to measure the voltage to determine which is positive in case yours has different cabling then ours. For ours there is a brown and a light-grey cable inside, where brown is +12V.", "It might be a good idea to put a plug in between so you can connect the kinect to either the on-board power source or the original power supply.", " ", " ", "There is a tutorial on the Adept/Mobile Robots wiki on ", ". ", "The concept is essentially the same here, except you should at the very least a voltage regulator between your battery and the sensor. In fact, all power used by your robot should probably be regulated. Since the Pioneer robots already have a dedicated power board with that and more, that isn't mentioned in the wiki.", " ", " ", "Rainer Hessner used in his project about ", " the following product to use Kinect:", " ", " ", "Hi Melonee,", "Tutorial is great and many thanks by the comment about voltage regulator.", "I am trying to find some batteries for Kinect.", "On ebay I found 3 kind of batteries:", "Gel battery: 12v 33Ah\n  ", " Lead battery: 12V 18 Ah\n  ", " Li-Ion battery: 12V 4800mAh\n  ", "With your experience what is better?\nIn my personal opinion, Li-ion battery seems to have less weight.\nWhat is effect about Ah values? Previous products have different values.", "In relation to voltage regulator, I will search a electronic friend to build a homebrew voltage regulator", "Cheers", " ", " ", "Did the Li-ion battery 12v 4800 mAH work with the Kinect?", "Please use the comment functionality if you don't answer but comment on sth.", ": You're right, but the comment functionality is only available when you have > 10 karma.", "Oh, that again, sorry. But now he might.", "I have it now working with a miniGorilla lithion battery connected to the Kinect, works fine.", " ", " ", "Hi dornhege,", "Do you know other batteries in the market with better features and price?", "Cheers"], "url": "https://answers.ros.org/question/9868/how-to-mount-a-kinect-with-my-laptop-without-any-ac-adapter/"},
{"title": "detect object with color", "time": "2014-04-19 21:24:58 -0600", "post_content": [" ", " ", " ", " ", "Hi\nI am using the tabletop object detector to determine some object on point cloud database. ", "I was wondering  if I would able to somehow also extract color information about the objects. I want to know that  can I use color information or RGB image by tabletop library? If not ,does anybody know of a tool that does anything similar???"], "answer": [" ", " ", "Hey,\nThis is what you need - Voxelized Shape and Color Histograms (VOSCH) ", "Thank's Adreno I think it's good. but I got problem in VOSCH installation. I follow this link ", " ,but when I run :\n rosrun vosch example_vosch `rospack find vosch`/data/sample_cone_green.pcd\n I got the Error that Couldn't find executable named example_vosch below /opt/ros/fuerte/stacks/mapping/vosch. what is the problem?", "Did you rosmake ? You have to build the package to generate the executables..."], "url": "https://answers.ros.org/question/155211/detect-object-with-color/"},
{"title": "How do I build a package as a library in catkin?", "time": "2014-05-09 14:20:41 -0600", "post_content": [" ", " ", " ", " ", "I'm trying to build a c++ package as a library so that other packages can link to it.   I've been working off the page ", " but no luck.   It seems gcc is not getting the ", " flags etc.    ", "catkin_make is giving me \"undefined reference to `main'\" and also is failing to link to basic ros classes even though roscpp is specified in all the usual places.    ", "All worked well when the cpp and .h files were in the same package as an another cpp file which called classes in the file I want to make a library.   Can I separate the .cpp files into separate packages and make the one without main() a library?"], "answer": [" ", " ", "Since you want to build a library, that is the wrong how-to page.", " Try this one:  ", "If that is not enough detail to fix your problem, please edit your question and add the relevant parts of your CMakeLists.txt.", "Right - yes I was using your link but posted the wrong one.    I'll have to wait until Monday to post the CMakeLists.txt file --- thanks.", "Thank you - on further checking CMakeList.txt and matching it carefully to the link in your answer, it seems to work now!", "If this answer works for you, please click on the green check mark, so others will know this question has been resolved."], "url": "https://answers.ros.org/question/163381/how-do-i-build-a-package-as-a-library-in-catkin/"},
{"title": "Segway Driver", "time": "2014-06-02 09:29:36 -0600", "post_content": [" ", " ", "I was able to install the Segwayrmp driver and get it to work properly. But now it is showing a 'checksum mismatch' error.\nAny solution to this problem?"], "answer": [" ", " ", "If I remember correctly this occurs when there is invalid data sent from the segway device and typically this means some sort of problem with the usb device. I remember debugging this once and finding lots of messages about usb failures in ", ". If you are using USB to connect to the segway, try passing it through a powered USB hub or rethinking your power system to try and identify places where your could have different grounds.", "Additionally. If the checksum mismtach error only shows once, don't worry about it. That's typical. If it shows up more than once, then you probably have a problem."], "answer_code": ["dmesg"], "url": "https://answers.ros.org/question/172597/segway-driver/"},
{"title": "tum_ardrone autopilot", "time": "2014-05-28 00:39:53 -0600", "post_content": [" ", " ", " ", " ", "Hi ", "I downloaded and was able to compile tum_ardrone on Ubuntu 12.04. I'm using ROS Hydro.", "As the README.txt explain for Autopilot section we should do the following:", "type command \"autoInit 500 800\" in top-left text-field", "=> drone will takeoff & init PTAM, then hold position.", "But when I do the same, the drone don't take off. If I click takeoff and then clear and send \"autoInit 500 800\" command, the drone try to go to the target but can't work properly and don't hold the position. How can I fix this problem? Any idea?", "I wonder if I should calibrate the camera and initialize PTAM by myself before sending the commands. Is it necessary?", "How much the structure of environment influence the accuracy of the algorithm? Which objects is better to be in the field of view of the camera? Can the AR.Drone work properly in small scales and bounded environments?"], "answer": [" ", " ", "Hey,", "First of all, it has been over a year when I last used the tum_ardrone package, so my response might not be accurate. From afar I can't answer your first question, but I remember I had some trial and error also regarding to when the drone actually took off, sometimes my message-queue for the drone wasn't empty and the drone took off after inserting a new battery and things like that. You could post some logfiles, maybe I'll see something.", "It is hard to tell what you mean with \"can't work properly and don't hold the position\". The onboard controller of the drone tries to keep the drone in one place via the bottom camera when no commands are sent. If you have a floor without any visible edges, like a grey carpet, this sometimes results in the drone moving into one direction because of the optical illusion that the floor is moving. You can switch to the output from the bottom camera to see for yourself that it is really hard. This isn't a faul of tum_ardrone but rather from the onboard controller. To avoid this, you can place highly distinctive markers (or even just sheets of paper) on the floor or maybe use one of those carpets for children with the city drawn on it ", "In regards to the more general questions: I managed to get the drone to work pretty well in smaller environments, like offices or hallways. It helped to have objects about 1-5 meters away from the drone. Again, the better the color-contrasts and the more edges visible, the better the algorithm worked.", "I hope this helped a little,", "Rabe", "Rabe, many thanks for the reply. It was very helpful.\n\n  1. In regards to the first question, you are right. I will try it and post the log files.\n\n   2. For tracking ground targets reasons, a front camera of the AR.Drone was taken out and mounted on the bottom of the drone looking downwards. Does the tum_ardrone package work in this situation or I should mount the camera on the first position?", "1) Post a gist or a pastebin, I'll have a look 2) My guess is, it won't work per default, you'd definetly have to change some lines of code, since the whole orientation changed. Also, why don't you just use the bottom camera? Because of the resolution, or because of tag-detection?", "Maybe, you could just tell me the concept of your idea (or PM it to me, if you don't want it to be public) and I could give my 2 cents. Cheers, have a nice weekend!"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "click Clear and Send (maybe click Reset first)", " "], "url": "https://answers.ros.org/question/170766/tum_ardrone-autopilot/"},
{"title": "How can I calibrate low resolution cameras?", "time": "2011-12-02 03:00:15 -0600", "post_content": [" ", " ", "I'm having great difficulties calibrating the intrinsic parameters of a low resolution monocular camera. The camera is a thermal imager with 160x120 pixels, but with a good setup (two high power lamps and big calibration target) I get a pretty good monochrome image. It however appears that camera_calibration struggles with the low resolution of the camera. Over the last hours, I succeeded twice in getting all but the 'X' bar in the calibration wizard to a 'green' state, with over 120 successfully collected sample images in both cases. The 'X' bar however, refuses to become green, despite all my attempts to specifically sample points at the left and right edges of the image. I also copied the camera_calibration package and changed the code so my image gets upscaled (line 357 and following), but that didn't make calibration work significantly better. Any ideas are welcome :)"], "answer": [" ", " ", "Ok, so I finally managed to succeed (after no less than 5 hours of trying). What I found out:", " ", " ", "Thanks for the report. I've committed a couple changes to ", " that should make this easier in the future. The X/Y threshold is now 70%; 80% did seem too finicky in practice. Also the GUI will now permit you to calibrate with 40+ samples, even if not all the bars are green.", " ", " ", "I'm doing the camera calibration with  a very low resolution just like yours, may I ask how can you  calibrate accurately\uff1f"], "answer_details": ["Try to get as good image quality as possible (major factor for me as I used a thermal cam)\n", "Do not try to accumulate hundreds of images, it is very likely that calibration will never enable the calibration button if you have over a hundred or so. My successful try was with 59 samples. It is better to restart often than to stick to a failed calibration try.\n", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["camera_calibration"], "url": "https://answers.ros.org/question/12188/how-can-i-calibrate-low-resolution-cameras/"},
{"title": "Any experience with iRobot ATRV-JR?", "time": "2014-06-24 14:28:57 -0600", "post_content": [" ", " ", "I am building a robot testbed at my community college and I inherited an iRobot ATRV-JR platform, but no documentation or other information.  Does anyone here have experience or know who I can contact.", "Thanks,\nJonathan West"], "answer": [" ", " ", " ", " ", "These robots have a proprietary board inside called an rFlex bus that communicates with the sensors and motors. I've been trying to communicate with the rFlex board on my ATRV-Jr for a while now with no luck so far. ", "I found that the Washington University robotics group has written some ROS software that communicates with rFlex based robots, but it didn't seem to work for me. But I haven't had time to troubleshoot deeply yet. It's not my main project. ", "Here is a manual for the iRobot/RWI software that runs on the onboard PC.  It's terribly old not supported anymore, so I did not find it useful.  ", "I have been able to drive mine around the lab using an old wired joystick and walking behind it.  I can talk you through that if you'd like to do that. Let me know if you're interested.", " ", " ", "I am somewhat jealous of you... That thing has SONARS ! (Well, the rescue version anyway)", "Found something else here:", "It has a Pentium 3 inside. If the communication with the sensors/controllers is not proprietary, you could try replacing the mainboard with something more powerful. ", " ", " ", "Thanks for the information.  Yes, it has the sonar array which I would like to use, but all of the interface is apparently proprietary and quite outdated.  My goal is to get a good educational platform up and running so spending a lot of time on old, proprietary interfaces may not be in my best interest. ", "I was hoping that the interface boards would just be a serial communication protocol that I could implement, but otherwise I may end up stripping it down to the motors and hardware and then replacing the electronics with a laptop and some more modern interfaces.", "I will look into the links provided and see if I can make anything work before I give up on the existing system.", "Thanks again", "J West"], "url": "https://answers.ros.org/question/174825/any-experience-with-irobot-atrv-jr/"},
{"title": "Should I use Hydro or Indigo for my new project?", "time": "2014-07-15 22:49:27 -0600", "post_content": [" ", " ", "Ok, I have downloaded and tried many of the tutorials and I am sold on using ROS/Gazebo for my robotics project at my community college. ", "I am starting everything from scratch so I would just like some advise on whether I should install Ubuntu 12.04/Hydro or 14.04/Indigo.", "I prefer Ubuntu 14.04 but I don't want to commit to Indigo if there are likely to be big problems down the road.  So basically I need to know if Indigo is stable enough for me to commit to using it.  Since I am at a school, I don't mind doing a bit of Beta testing for the new release, but I can't afford to be fighting with a lot of broken or missing features.", "Thanks,\nJonathan West\nSouthwest Indian Polytechnic Institute."], "answer": [" ", " ", "The choice between ", " and ", " is yours.\nIf both distros have all the ROS packages you need for your project you basically trade better tested with longer support.", "I would ", " recommend ", " at this point. As soon as ", " is released (which will likely be within a week) ", " will be ", " and therefore not receive any kind of updates / fixes anymore.", "Thanks.  My project is going to be running for several years, so I would favor a long term solution.  If Indigo is about to be released it sounds like it may be my best best.\n\nAre there currently any major features missing from Indigo that I should know about?", "Well, \"missing\" is not the word, but important change: OpenCV - if I remember well - is no longer part of the deal (cannot really say \"distribution\"), having to be installed separately. This might affect your plans for the rovers.", "opencv is not longer packaged with ROS in indigo since it is available as a normal Ubuntu package on trusty, so that is not a limitation.", "Well, yeah, for ubuntu users and x86 architecture, it may not be a limitation, and, since the main devices here, on this question, are notebook-driven, your point is valid. Still, in a ROS-related discussion, yes, it becomes a limiting factor for several other platforms and \"quick\" implementation.", ", fair enough, point taken that it might not be as seemless. However, you should be able to (I haven't tried this) just build and install the hydro variant of opencv, if you want even using caktin_make_isolated, and then build indigo on top.", "My point being opencv should not be an argument against indigo.", " ", " ", " ", " ", "Hello, Jonathan.", "I was going over a few of your previous posts, and your project starts...well, NOW. July.", "There are still a few things pending on Indigo, as far as i could see, and there is a small chance it could jam your project, just because it is not solved in time.", "Solution here is not the one my heart would love (bleeding edge latest, because new is better, right ? ) but the sensible one instead: rock solid performance, on stable, well known platform.", "I would go for 12.04 and Hydro. At least you have a lot of people with experience to support you here.", "Besides, U 14.04, with the the graphical-rich GUI, is using more resources than previous versions. You might want to consider Lubuntu, or other light-weigh variations (of 12.04). Or even run w/o GUI at all.", "On the flip side, how long will this project last ? If it will go on and on for years, then it might be worth taking your chances with 14.04 and indigo, but always keep in mind: new software usually takes more resources, and you seem to have limited resources. But, then again, it is a good way to simulate a mission to mars. (What a dream project).", "Note: for the implementation of network latency and bandwidth, I would suggest a FreeBSD server with the \"pipe\" implementation on ipfw, the native firewall, per IP address (of the bots/rovers). Fixed, and locked to the MAC address, just in case you have some little hacker among your students ;) ", "Thanks for the input.   \n\nTo clarify, I will have 2 platforms: \nThe rovers will have small laptops and will only communicate via network, so GUI-less would be fine there.  They will not run Gazebo or anything but ROS and whatever is needed to interact with the hardware.\n\nThere will also be a couple of powerful server computers which will be running ROS and Gazebo.  These machines will also be running LAMP servers to allow remote connections and interactions.  So I would prefer to have these running the latest ubuntu version.\n\nOf course for maintenance reasons I would rather them all be running the same versions and since this is planned to be a multi-year project, I don't want to have to do a major upgrade any time soon!", "Hydro will be maintained until J-turtle is released which is expected in about one year. Indigo on the other hand will receive long term support - expected are 5 years matching the support timeline of Ubuntu Trusty.", "Geeezzz, I meant Hydro and not Groovy. Changing the main post for clarity, and making my apologies public."], "url": "https://answers.ros.org/question/186722/should-i-use-hydro-or-indigo-for-my-new-project/"},
{"title": "face animation display", "time": "2014-07-16 22:07:09 -0600", "post_content": [" ", " ", "I do not want to reinvent the wheel on this one.  What I have in mind is a program developed that shows a simplified face maybe a little better that eyes,nose, mouth (maybe a smiley face at first) in cartoon style accompanied with voice or text message shown on a tablet.", "I would like to develop a robot head that really is only a tablet mounted vertically that can locate, turn, and interact using on board camera.  The user can either via voice recognition or touch screen interact with this stationary friend. I am sure some of this is already done.  If someone (I have limited robotic and programming experience) can organized in modules so the user can start with:", "Porting Linux to a spare tablet such as the old Viewsonic tablet that  is collecting dust right now.", "Interfacing with the tablet throughthe micro usb port which most tablets have using an arduino, raspberry pi,\npropeller board.  ", "Motor driver,gearing hardware and mounting of the tablet .  ", "Accepting input through microphone, touch screen, or keyboardand camera.", "Above mentioned display output via cartoon face and simple response boxes on the touch screen or\nactual artificial voice output.", "The need of using a tablet is so it can be portable or eventually mobile.  Basically the tablet is the \"head\" and \"face\" of the robot friend. I think the neatest thing would be the interactive face that not as complex as the various AI programs that tend to be propriety and expensive.  The whole point of this would be an inexpensive, open source robotic friend especially for old folks and childern. \nThis is not exactly fleshed out but a starting point for a discussion about a desktop robot friend."], "answer": [" ", " ", "Well, as far as discussions go, and sharing my shallow knowledge of ROS, here are a few collected thoughts to start steering this.", "Voice or speech recognition is a very new and unexplored field; I've followed a recent discussion on the list and these are still early days. Might help understanding commands, but not simple instructions for instance. This was what the author himself said.", "I am not 100% sure you will need to have Linux on the tablet, since Android tablets are essentially Linux and there are modules developed on ROS that allow some sort of interaction with the robot. Now, using your tablet's CPU to run the entire robot would be unrealistic, with the level of refinement you are expecting. ", "Regarding \"head\" movement, well, this is a somewhat easier field; Supposing that you have some sort of support for the tablet (you will have to be creative here), you can use the famous \"pan and tilt\" mechanism, driven by servo motors, to move the head up and down, and to the sides. Not very human-like, but coherent with the \"budged specified\".", "Now, facial expressions, mimicking the human expressions or not, well, I personally think this is too much to ask from a tablet or regular ARM processors. You might want to re-think your hardware requirements. I have not heard of anything like this yet (well, I am new here), but this is something that the Kinect software and hardware did, with games. This is complex stuff, IMO.", "Back to the tablet interface with ROS or whatever hardware you have in mind, well, from my experience tablets don't play well with the USB port, naturally. You might want to consider bluetooth or WIFI. OF course this supposes the tablet will run the face software and some other light stuff, and some other stronger hardware (beaglebone black perhaps ?) would run the power-hunger parts of the robot, including image processing (input from camera and processing your smiley face, sending the result to the tablet.", "The project is nice. It will keep you entertained for days, but I am afraid it will require some degree of programming skill. If you are a beginner like me, you might want to look into coding in python, at least to start, until you get the hang of things. It is common sense that if you want serious, quick and powerful stuff you should go to C and its variations.", "For camera input you have OpenCV, and also OpenNI. Those will help you with image processing. ", "Now, your intention, of creating a sort of companion, is brilliant. I had an idea like this, simpler, tough, to be used to monitor the health of debilitated patients. A dog-shaped robot that would have some degree of cute interaction, and be able to collect readings like body temperature, blood pressure, pulse, and other simple metrics, monitoring them and eventually sending them elsewhere, via wifi.", "Now, let's \"face\" it. A head is a complex thing by ...", " ", " ", "Thanks for the input. I have seen robots running with far less computing power than a tablet.  I was thinking of converting to linux to remove the constrictions that an android operating system.  Too bad someone has not figured out how to make the tablet act as just a smart monitor taking input from another computer.  The problem I  see is that the necessary computing power of a robot requires a big form factor which is not advantageous to something small on a desk. I am sure someone can come up with an \"app\" that produces a moving face.  Anyway it is a thought for someone to go with."], "url": "https://answers.ros.org/question/186872/face-animation-display/"},
{"title": "BeagleBone Black & Kinect", "time": "2014-07-04 01:07:11 -0600", "post_content": [" ", " ", "Hi Everybody,", "Does anybody know how complex of a process it is to get a Ubuntu, Kinect and ROS running on a BeagleBone Black? I would like to do this for my humanoid robot project.", "Thanks.", "https://roboticsclub.org/redmine/projects/quadrotor/wiki/Installing_ROS_and_the_Kinect_stack_on_the_Pandaboard\n\n"], "answer": [" ", " ", "Don't bother. The BeagleBone Black doesn't have enough processing power to handle a Kinect.", "Some of the newer, more powerful ARM boards such as the Odroid U3 can handle a Kinect or an ASUS Xtion fairly well."], "url": "https://answers.ros.org/question/183037/beaglebone-black-kinect/"},
{"title": "map transform 90 degrees off when using amcl", "time": "2014-06-06 14:40:39 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I've been building a custom robot in gazebo. This robot has two laser scanners, one on each side facing outward (not forward). When running amcl, the laser scans are 90 degrees off of where they should be. If I set the 2d nav position to the correct position, the laser scans are 90 degrees off. If I set it 90 degrees offset, the laserscans are correct. The odd thing is, no part of my urdf is rotated 90 degrees (except the wheels). Here is my process:", "I start up gazebo and load up my model. Here are the urdf laser scan links and the laser scan plugin. There are no rpy rotations at any links above the laser scans.", "I then run robot_pose_ekf, robot_state_publisher.", "Lastly amcl:", "If you visualize the TF frames in rviz, do the laser scans line up with their origin frames properly? Which frame are you sending your position estimate in? A screenshot might be helpful."], "answer": [" ", " ", " ", " ", "Found the solution.", "The robot was actually built entirely 90 degrees off center. Swapping all of my x,y values in origin xyz and rpy, as well as the x,y axis values fixed everything.", "Thanks for the help.", " ", " ", "Your right scanner is pointing forward and your left scanner is pointing backwards; is that what you intended? Note that in ROS, X is forward; this corresponds to the green axes in rviz.", "In your URDF, you've specified the yaw of the left scanner as ", " (180 degrees) and the yaw of the right scanner as ", ". This doesn't match your description of your robot; you probably want the yaw for the left scanner to be ", " and the yaw for your right scanner to be ", ".", "Those numbers are an artifact of me trying to figure out what was going on. I changed the yaw to see if the map tf frame would rotate with the scanner. Your yaw numbers along with 0 to 3.14 degree min/max angle results in the same laser scan as my numbers with -1.57 to 1.57. I just double checked and the same 90 degree error exists. If I change the fixed frame to the right_scanner, and visualize the right scans, they are in the right spot. It seems like the tf transform for the map is the problem.\n\nSo what creates the map tf transform? Map_server publishes it. It's likely computed in gmapping. So the problem must be with that somehow. This is where I am stumped.", "I'm not sure I understand why you're adjusting the laser min and max angle.", "If you set the fixed frame to 'odom' or 'base_link' in rviz, you should be able to visualize your laser scans with respect to the rest of your TF tree, without running AMCL at all.", "I was adjusting the yaw of the lasers, which made me need to adjust the min and max angle.\n\nIf I set my fixed frame to odom or base_link in rviz, the laser scans are accurate. The transform between map and odom is the problem.\n\nThis didn't solve my problem, but the answer is related: ", "\n\nI also updated the question with a hack-fix that might shed light on the problem."], "question_code": ["  <link name=\"right_scanner\">\n    <visual>\n      <origin xyz=\"0.0 0 0\" rpy=\"0 0 0\"/>\n      <geometry>\n        <box size=\"0.05 0.05 0.1\"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.05 0.05 0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1e-5\" />\n      <inertia ixx=\"1e-3\" ixy=\"0\" ixz=\"0\" iyy=\"1e-3\" iyz=\"0\" izz=\"1e-3\" />\n    </inertial>\n  </link>\n\n  <joint name=\"right_scanner_joint\" type=\"fixed\">\n    <axis xyz=\"0 0 0\" />\n    <origin xyz=\"${0.05/2} -${chair_length/2 - 0.05/2 + chair_length/8} ${chair_height/16 + 0.05}\" rpy=\"0 0 0\"/>\n    <parent link=\"right_bar\"/>\n    <child link=\"right_scanner\"/>\n  </joint>\n\n  <link name=\"left_scanner\">\n    <visual>\n      <origin xyz=\"0.0 0 0\" rpy=\"0 0 0\"/>\n      <geometry>\n        <box size=\"0.05 0.05 0.1\"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.05 0.05 0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1e-5\" />\n      <inertia ixx=\"1e-3\" ixy=\"0\" ixz=\"0\" iyy=\"1e-3\" iyz=\"0\" izz=\"1e-3\" />\n    </inertial>\n  </link>\n\n  <joint name=\"left_scanner_joint\" type=\"fixed\">\n    <axis xyz=\"0 0 0\" />\n    <origin xyz=\"-${0.05/2} -${chair_length/2 - 0.05/2 + chair_length/8} ${chair_height/16 + 0.05}\" rpy=\"0 0 -3.14\"/>\n    <parent link=\"left_bar\"/>\n    <child link=\"left_scanner\"/>\n  </joint>\n\n    <gazebo reference=\"right_scanner\">\n    <sensor type=\"gpu_ray\" name=\"head_hokuyo_sensor\">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>40</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-1.57079633</min_angle>\n            <max_angle>1.57079633</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.10</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </ray>\n      <plugin name=\"gazebo_ros_head_hokuyo_controller\" filename=\"libgazebo_ros_gpu_laser.so\">\n        <topicName>/wheelchair_lasers/right</topicName>\n        <frameName>right_scanner</frameName>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n   <gazebo reference=\"left_scanner\">\n    <sensor type=\"gpu_ray\" name=\"head_hokuyo_sensor2\">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>false</visualize>\n      <update_rate>40</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-1.57079633</min_angle>\n            <max_angle>1.57079633</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.10</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </ray>\n      <plugin name=\"gazebo_ros_head_hokuyo_controller2\" filename=\"libgazebo_ros_gpu_laser.so\">\n        <topicName>/wheelchair_lasers/left</topicName>\n        <frameName>left_scanner</frameName>\n      </plugin>\n    </sensor>\n  </gazebo>\n", "<node name=\"map_server\" pkg=\"map_server\" type=\"map_server\" args=\"$(find wheelchair_mapping)/map.yaml\" />\n<node name=\"amcl ..."], "answer_code": ["-3.14", "0", "1.57", "-1.57"], "url": "https://answers.ros.org/question/172974/map-transform-90-degrees-off-when-using-amcl/"},
{"title": "ClamArm using Dynamixel Motors: No Motors Found", "time": "2013-09-03 14:00:01 -0600", "post_content": [" ", " ", " ", " ", "For those of you who are brave enough to try to help me solve this problem, I will grant you 3 wishes. Here we go. ", "I'm using an arm called the ClamArm ", "Seen here", "I just got done building the arm. All motors are powered up. I plugged in both Dynamixel2USB cables from the arm into my computer's USB ports and I run the lowlevel launch file to connect to the arm. (7 motors on the arm total, 3 motors to 1 USB, 4 motors to the other).", "Seen here\n(lowlevel.launch)", "I get this error. Apparently, no motors are found!", "No Motors found on port_rs485. I look at the launch file and see that it has an embedded launch file called clam_controller.launch that is actually responsible for connecting to the motors. ", "Seen here (clam_controller.launch)", "Sorry for the off-topic comment but I'm building the clamarm and the road narrows kit doesn't exist anymore:", "I think I have found all the servos but am unsure on the brackets - if you have a chance could you let me know what you ordered?", "Hey there, I'm the creator the ClamArm website. I don't have time to maintain the project anymore, but if any of you guys figured out problems or made improvements, I'd be happy to improve the documentation."], "answer": [" ", " ", "I've bought  CLAM arm too, but you've gotten further along than me in assembly. ", "My understanding is that the motors don't have the ID numbers programmed in already, and you have to manually set them. There were a couple of scripts in ROS stacks for dynamixel maintenance, setting ID numbers, what's on the bus, etc. I believe it's a matter of plugging each motor in individually, setting its ID, then powering down and moving onto the next. \n", "Once all the motors are programmed, ROS might be able to see them. ", "Hope this helps,\nGav", "Thanks Gav! It seems all of those scripts change the Motor ID of the servo based on it's previous ID as a parameter. I don't know my motors IDs. Is there one that lets you set a motor ID?", "I think the 'info_dump' script might be able to tell you something, but I haven't got that far myself. Although I quickly tried running the script and didn't get any info previously. At the time I figured that my motors weren't powered up right.", " ", " ", ", did you ever figure out your problem. Gav's answer does not help me. Please let me know."], "question_code": ["<!-- -*- mode: XML -*- -->\n<launch>\n\n  <!-- debug capability -->\n  <arg name=\"debug\" default=\"false\" />\n  <arg unless=\"$(arg debug)\" name=\"launch_prefix\" value=\"\" />\n  <arg if=\"$(arg debug)\" name=\"launch_prefix\" value=\"gdb --ex run --args\" />\n\n  <!-- chmod the usb ports -->\n  <node name=\"change_usb_port_permissions\" pkg=\"clam_bringup\" type=\"change_usb_port_permissions.sh\" />\n\n  <!-- Startup the arm controller, dynamixel manager, hardware interface, etc -->\n  <include file=\"$(find clam_controller)/launch/clam_controller.launch\">\n    <arg name=\"debug\" value=\"$(arg debug)\" />\n  </include>\n\n  <!-- Send robot XML description to param server -->\n  <param name=\"robot_description\" command=\"cat $(find clam_description)/urdf/clam.urdf\" />\n\n  <!-- Publish the robot state -->\n  <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" />\n\n</launch>\n", "setting /run_id to 389151fa-13fb-11e3-ae2c-20c9d0bd73cd\nprocess[rosout-1]: started with pid [4772]\nstarted core service [/rosout]\nprocess[change_usb_port_permissions-2]: started with pid [4784]\nprocess[dynamixel_manager-3]: started with pid [4790]\n[ INFO] [1378145546.539716442]: port_rs485: Pinging motor IDs 0 through 7...\n[change_usb_port_permissions-2] process has finished cleanly\nlog file: /home/lucid/.ros/log/389151fa-13fb-11e3-ae2c-20c9d0bd73cd/change_usb_port_permissions-2*.log\nprocess[clam_controller_spawner_ttl-4]: started with pid [4820]\nprocess[clam_controller_spawner_rs485-5]: started with pid [4821]\nprocess[arm_trajectory_controller_spawner-6]: started with pid [4824]\nprocess[joint_state_aggregator-7]: started with pid [4827]\n**[ WARN] [1378145546.941467380]: port_rs485: No motors found.**\nprocess[clam_gripper_controller-8]: started with pid [4854]\n[ INFO] [1378145547.037642567]: ClamGripperCommand action server ready\n[dynamixel_manager-3] process has died [pid 4790, exit code -11, cmd /home/lucid/ros/clam/devel/lib/dynamixel_hardware_interface/controller_manager __name:=dynamixel_manager __log:=/home/lucid/.ros/log/389151fa-13fb-11e3-ae2c-20c9d0bd73cd/dynamixel_manager-3.log].\nlog file: /home/lucid/.ros/log/389151fa-13fb-11e3-ae2c-20c9d0bd73cd/dynamixel_manager-3*.log\n[INFO] [WallTime: 1378145547.097725] clam_controller_spawner_ttl: waiting for controller_manager clam_controller_manager to startup in / namespace...\nprocess[robot_state_publisher-9]: started with pid [4887]\n[INFO] [WallTime: 1378145547.140146] clam_controller_spawner_rs485: waiting for controller_manager clam_controller_manager to startup in / namespace...\n[INFO] [WallTime: 1378145547.173755] arm_trajectory_controller_spawner: waiting for controller_manager clam_controller_manager to startup in / namespace...\n", "<!-- -*- mode: XML -*- -->\n\n<launch>\n\n  <!-- Debug Info -->\n  <arg name=\"debug\" default=\"false\" />\n  <arg unless=\"$(arg debug)\" name=\"launch_prefix\" value=\"\" />\n  <arg     if=\"$(arg debug)\" name=\"launch_prefix\" value=\"gdb --ex run --args\" />\n\n  <!-- Start the Dynamixel motor manager to control all clam servos -->\n  <node name=\"dynamixel_manager\" pkg=\"dynamixel_hardware_interface\" type=\"controller_manager\"\n        launch-prefix=\"$(arg launch_prefix)\" output=\"screen\" >\n    <!-- Load all the servo properties -->\n    <rosparam file=\"$(find clam_controller)/config/dynamixel_ports.yaml\" command=\"load\"/>\n  </node>\n\n  <!-- Load joint controller configuration from YAML file to parameter server -->\n  <rosparam file=\"$(find clam_controller)/config/clam_controller_configuration.yaml\" command=\"load\"/>\n\n  <!-- Start all ClamArm ..."], "url": "https://answers.ros.org/question/76243/clamarm-using-dynamixel-motors-no-motors-found/"},
{"title": "Packaging ROS with homebrew", "time": "2014-02-17 23:58:36 -0600", "post_content": [" ", " ", " ", " ", "Are there plans or have there been attempts to package ROS as homebrew formulas in a similar way to the debs on Ubuntu?", " For future reference: With pointers from William a proof of concept has been created, but there are a few issues that need to be addressed:  "], "answer": [" ", " ", "I have looked to do it for a long time now, but I have never had the time to setup everything. For now, all the time I can afford to spend on it is to keep it building from source on OS X.", "From my perspective, the main challenges to getting this working are as follows:", "I think that generating Formulae for packages is the only sustainable way to approach this problem. There are nearly 800 packages in Hydro and 200+ in desktop-full. Packaging them by hand is error prone and unmaintainable. So coming up with a bloom generator for the formulae would be crucial. I started that work, but didn't get very far at all:", "The CI system for OS X has also been a challenge. I spent some time trying to setup a diskimage with COW for OS X which would behave like chroot/pbuilder for linux but ran into a lot of issues. The other thing I tried was using a VM and snapshots through a Python API, but that required Parallels which is not free and was slow. The Homebrew guys just clear ", " with ", " between builds for their \"homebrew brewbot\" system ( ", " ). I believe we (OSRF) are setting up a similar setup using jenkins on our build farms: ", "If that is successful, I might try to use it for packaging ROS.", "Finally, for bottle support (which would be awesome), we would need to also build each formula with the ", " option, create the bottle, upload it somewhere (like download.ros.org/bottles) and then update the formula again, probably pushing the formula to ", ".", "So I have looked into it before, but as always progress on this is subject to my free time, which is in pretty low supply at the moment.", "As always thanks for the comprehensive answer. At least the current state of affairs is out there now. I don't think I will manage to do it just now, but maybe me or otgera can work towards it in the future. I fully agree that automatic bottle creation is a must. I can wounder if we can get support", "from the homebrew guys for things like CI and bottle creation. However they might not be happy about 800+ incoming formulae. In any case publishing at least the core packages of a base install would be a gigantic leap for OSX support.", "Upstream ticket to track progress on this: "], "answer_details": ["Generating Homebrew formulae for packages using bloom", "Setting up a CI system on OS X which has a clean starting point for each build", "Building bottles, pushing them to some storage (like download.ros.org/bottles), and updating the formula which uses the bottle", " ", " ", " ", " "], "answer_code": ["/usr/local", "git clean -fdx", "--build-bottle", "https://github.com/ros/homebrew-hydro/"], "url": "https://answers.ros.org/question/130346/packaging-ros-with-homebrew/"},
{"title": "groovy-devel, hydro-devel, different git branches architecture question", "time": "2014-06-21 10:42:29 -0600", "post_content": [" ", " ", "Maybe this is a really general question about git and branches and i could put it somewhere else (stackoverflow) but I would like to know for the developers that are using ", " and ", " (for example) how do you synchronize changes one branch and another. Do you use something like ", " to select exactly what files to synchronize? Can you even use the cherry-picking for ", "??? So, what's the usual standard that you use? the question is mainly because when you develop something for some branch and you want to put that utility to the other branch, (i think) you won't do rebase, you will try to merge only some specific files which are not going to mess up some others.", "Thanks!"], "answer": [" ", " ", "Yes, the standard way to port code from one development branch to another is to use cherry-pick. Well constructed commits will usually cherry-pick betwen branches relatively easily. ", "You cannot use merge unless you want all changes. Generally merging is only done to bring feature branches/pull-requests into a development branch and never for crossing between forked development branches. ", "rebase is super powerful and can be used in many ways. However if the content is already available to others in a public branch it should not be used. It's really for cleaning up local branches/development before pushing content. ", " ", " ", " ", " ", "There should be many people in ROS community who can explain more thoroughly, but here's what I know:", "To synchronize branches, I think ", " and ", " (use either one depending on the purpose) is generally preferred to ", " (", "). ", " is useful particularly when you want to sync the specific commits.", "In ROS, branching per distro seems to be a common practice, e.g. ", " as you pointed out. Note, however, that you may create a branch for a new ROS distro ONLY when there's a necessity; for instance, while the latest ROS distro is hydro, the latest development branch of ", " remains ", " since all the code there is expected to run on both distributions and thus it reduces the maintainer's effort. This kind of branching policy can vary depending on repositories (and for ", ", it's ", ")."], "answer_code": ["git merge", "git rebase", "git cherry-pick", "cherry-pick", "{ groovy, hydro, indigo }-devel", "groovy-devel", "rqt"], "url": "https://answers.ros.org/question/174369/groovy-devel-hydro-devel-different-git-branches-architecture-question/"},
{"title": "I changed the buffer size and it didn't change ... for a while", "time": "2014-06-24 19:56:09 -0600", "post_content": [" ", " ", " ", " ", "I have some ROS nodes that take joystick data and translate it into CAN bus commands to command motor controllers on my robot. The motor commands buffered up too much because after I stopped moving the joystick, it would take several seconds for the final motor commands to stop the motors to occur. So, logically I reduced the buffer sizes on both the publisher and subscriber sides (mostly because I don't understand the difference or why both sides are configurable) but nothing seemed to change in responsiveness. ", "I stopped all nodes, performed a catkin make and started them all again and the heavy buffering continued. So I took all the buffers all the way down to 1 in all my ROS code. Stopped all nodes , catkin make , started all nodes. No joy. So I'm confused and frustrated at this point and starting to wonder what could be buffering up these motor commands if not ROS. ", "So I go into my CAN node code and add a debug message so I can see how quickly or how slowly the CAN messages are being sent. And all the sudden it's no longer buffering. Then I remove the debug message and it's still acting exactly as I wanted. As soon as I stop the joystick, the motors stop. No buffering. ", "So why didn't the buffer size of 1 take initially?  Is there some sort of caching that I encountered?  It ended up fine but it really threw me for a loop for a while. ", "P.S. I'm using ROS Hydro on Mint 13 Maya."], "answer": [" ", " ", "It sounds like you might not have successfully recompiled the node in question in previous cycles. If you do not have your dependencies correctly declared in CMakeLists.txt it may not rebuild if it doesn't think the files have changed. ", "Well, every time I do a catkin_make, I always look for the red message showing the appropriate library (the file I just modified) being linked. But I don't recall if I saw the green message showing the .o file being made. The library wouldn't get linked if the .o file wasn't changed, though, right?"], "url": "https://answers.ros.org/question/174860/i-changed-the-buffer-size-and-it-didnt-change-for-a-while/"},
{"title": "Importing ROS Messages in Python", "time": "2014-06-30 16:25:35 -0600", "post_content": [" ", " ", "I'm using ROS as middleware to connect my model (in Python) and my view (in JS), so I want to keep my Python code as separate as possible from ROS-related code. My directory structure looks like:", "I'm trying to import ", " from my ", " script, but Python doesn't work too well with relative imports.", "I have two questions:"], "answer": [" ", " ", "Turns out the problem was with calling ", " myself. The ", " explains issues with calling ", " manually screws things up.", "Instead, just uncomment ", " in ", " and follow the instructions in ", ".", " ", " ", ".msg files are not importable by python. Python modules will be generated when you build your package (with catkin or rosbuild), and placed in your package's ", " directory. They will be added to your pythonpath (either automatically with catkin, or with the ", " command with rosbuild)", "Hmm - it doesn't look like I'm getting cops_and_robots.msg added as a submodule when I examine it using pkgutil even though I think I've configured my CMakeLists and my package.xml properly (see ", " ). rosmsg show cops_and_robots/battery works, though."], "question_details": [" ", " ", " ", " ", " ", " ", "Do I need to add an ", " to my ", " folder to be able to import ", "? I don't entirely know pythonic standards, but it seems odd that I'd need to designate ", " as a module.", "Is this an acceptable way to structure my package? I'm trying to be standards-compliant, but I don't know if I am!"], "question_code": [".\n|____CMakeLists.txt\n|____cops_and_robots\n| |______init__.py\n| |____Cop.py\n| |____Map.py\n| |____MapObj.py\n| |____Robot.py\n| |____test\n| | |____unit\n| | | |______init__.py\n| | | |____moveTest.py\n|____msg\n| |____battery.msg\n| |____cmd.msg\n|____package.xml\n|____README.txt\n|____scripts\n| |____status_talker.py\n| |____websocket_interface.py\n|____setup.py\n", "battery.msg", "status_talker.py", "__init__.py", "msg", "battery.msg", "msg"], "answer_code": ["setup.py", "setup.py", "catkin_python_setup()", "CMakeLists.txt", "src/PKG_NAME/msg", "load_manifest(PKG_NAME)"], "url": "https://answers.ros.org/question/180069/importing-ros-messages-in-python/"},
{"title": "Problems with install hector quadrotor", "time": "2014-07-15 14:04:29 -0600", "post_content": [" ", " ", "I want to install Quadrotor outdoor flight demo and just go through everything step by step as the tutorial.Unlucky, the mistake appears after catkin_make.I am a beginner of ROS and sorry for my English at the same time.could anyone kindly help me? If possible, I wish to know how to solve the problem.  Many thanks in advance!", "CMake Error at /opt/ros/hydro/share/catkin/cmake/catkinConfig.cmake:75 (find_package):\n  Could not find a configuration file for package joint_limits_interface.", "Set joint_limits_interface_DIR to the directory containing a CMake\n  configuration file for joint_limits_interface.  The file will have one of\n  the following names:", "Call Stack (most recent call first):\n  gazebo_ros_pkgs/gazebo_ros_control/CMakeLists.txt:5 (find_package)", "-- +++ processing catkin package: 'smartcar_description'\n-- ==> add_subdirectory(smartcar_description)\n-- +++ processing catkin package: 'using_markers'\n-- ==> add_subdirectory(using_markers)\n-- Configuring incomplete, errors occurred!\nmake: ", " [cmake_check_build_system] Error 1\nInvoking \"make cmake_check_build_system\" failed", "In hydro system and 12.04 Ubuntu"], "answer": [" ", " ", " ", " ", "these code are so powerful nd iI have solve my problem.", "thank you, this is the unique solution"], "question_code": ["joint_limits_interfaceConfig.cmake\njoint_limits_interface-config.cmake\n"], "answer_code": ["cd ~/catkin_ws    # or whereever your workspace resides\nrosdep install --from-path src --ignore-src     # This will install all required packages.\ncatkin_make\n"], "url": "https://answers.ros.org/question/186679/problems-with-install-hector-quadrotor/"},
{"title": "Gmapping stops updating map", "time": "2014-06-23 07:19:55 -0600", "post_content": [" ", " ", " ", " ", "Hey,", "after finally getting deeper into ROS and understanding most of the things which are necessary for using the Gmapping-SLAM ( basics like tf, odom etc ) im now at the point of testing the whole thing. There is the Problem that Gmapping stops updating the map after serval scans. But first of all i want to give you a short overview:", " Im trying to use a simulated 2D-Scanner in VREP as source for Gmapping.", " I use Groovy on a Ubuntu 12.04 LTS on a old IBM X41 Laptop (1.5Ghz Centrino SinglecoreCPU)", "3 . I use a static tf transform between base_link and LaserScanner_Frame: ", "This results in the following tf-tree:\n", "I use Gmapping with the following Settings:", "But if i use these settings, there is the problem that Gmapping stops updating the map after serval scans. If i look at the SLAM-Debug-Messages there are serval odd messages:", "Are you maybe mixing times from different sources? Is there a /clock topic or/and maybe a simulation time?"], "answer": [" ", " ", "The fully resolved frame id warnings are not problematic. It's basically ", " vs. ", ", where if you just send ", " it's gonna make it to be ", ". This is usually what you want.", "What you should look at is the \"Out of the back of Cache Time(stamp: 1403512072.685 + cache_length: 10.000 \" and \" Time jumped forward by [5.379792] for timer of period \". This seems like something is wrong with the messages you are sending. All messages should be arriving continuously with a nicely increasing timestamp and laser and tf should fit together. It seems that something in your data doesn't.", "Hey domhege, thank you for your answer! You can see a new EDIT at the question.\n\ngru\u00df aus Berlin", "Hi julled,\n\nsorry for the off-topic but i think we try to do pretty much the same system. Could you send me a mail to andreas.gerken (at) thi.de so we can join powers?\n\nThanks Andreas\n\nGru\u00df aus Ingolstadt :)"], "question_details": [" ", " ", " ", " ", " ", " ", "I transfered the /scan and /odom-Topic from VREP to ROS.", "I wrote some code (odom_baselink_tf.py) for tf of /base_link to /odom:"], "question_code": ["def metadata_odom(data):\n    global p, q, info\n    info = data\n    p = data.pose.pose.position\n    q = data.pose.pose.orientation\n    rospy.loginfo(Received odom)\n...\nbroadcaster.sendTransform(\n  (p.x, p.y, 0), \n  (q.x, q.y, q.z, q.w),\n  rosnow,\n  base_lin,\n  odom\n  )\n", "rosrun tf static_transform_publisher 0 0 0 0 0 0 base_link LaserScanner_Frame 100\n", "    <param name=\"map_update_interval\" value=\"2\"/>  <!--2-->\n    <param name=\"maxUrange\" value=\"5.5\"/> \n    <param name=\"sigma\" value=\"0.05\"/>\n    <param name=\"kernelSize\" value=\"1.0\"/>\n    <param name=\"lstep\" value=\"0.05\"/>\n    <param name=\"astep\" value=\"0.05\"/>\n    <param name=\"iterations\" value=\"5\"/>\n    <param name=\"lsigma\" value=\"0.075\"/>\n    <param name=\"ogain\" value=\"3.0\"/>\n    <param name=\"lskip\" value=\"0\"/>\n    <param name=\"srr\" value=\"0.01\"/>\n    <param name=\"srt\" value=\"0.02\"/><!--1.0-->\n    <param name=\"str\" value=\"0.01\"/>\n    <param name=\"stt\" value=\"0.02\"/>\n    <param name=\"linearUpdate\" value=\"0.1\"/>     \n    <param name=\"angularUpdate\" value=\"0.1\"/>  \n    <param name=\"temporalUpdate\" value=\"-1.0\"/> \n    <param name=\"resampleThreshold\" value=\"0.5\"/>\n    <param name=\"particles\" value=\"50\"/>  <!--80-->\n    <param name=\"xmin\" value=\"-50.0\"/>\n    <param name=\"ymin\" value=\"-50.0\"/>\n    <param name=\"xmax\" value=\"50.0\"/>\n    <param name=\"ymax\" value=\"50.0\"/>\n    <param name=\"delta\" value=\"0.03\"/>\n    <param name=\"llsamplerange\" value=\"0.01\"/>\n    <param name=\"llsamplestep\" value=\"0.01\"/>\n    <param name=\"lasamplerange\" value=\"0.005\"/>\n    <param name=\"lasamplestep\" value=\"0.005\"/>\n", "update frame 7\nupdate ld=0.0495583 ad=0.179583\nLaser Pose= 13.7771 3.96148 -0.695156\nm_count 7\n[DEBUG] [1403512083.321312113]: TF operating on not fully resolved frame id base_link, resolving using local prefix \n[DEBUG] [1403512083.321624443]: TF operating on not fully resolved frame id odom, resolving using local prefix \nAverage Scan Matching Score=164.656\nneff= 49.8545\nRegistering Scans:Done\n[DEBUG] [1403512083.931338640]: scan processed\n[DEBUG] [1403512083.933940439]: new best pose: 13.752 3.943 -0.644\n[DEBUG] [1403512083.935263651]: odom pose: 13.777 3.961 -0.695\n[DEBUG] [1403512083.936265942]: correction: -0.025 -0.018 0.051\n[DEBUG] [1403512083 ..."], "answer_code": ["base_link", "/base_link", "base_link", "/base_link"], "url": "https://answers.ros.org/question/174585/gmapping-stops-updating-map/"},
{"title": "Computer vision in ROS [closed]", "time": "2014-07-29 05:48:18 -0600", "post_content": [" ", " ", " ", " ", "I am new in computer vision but I need to implement a robot with these kind of abilities.", "I imagine that ROS has its own version of OpenCV for these cases. \nWhat I have to do is a pretty simple task but I have no idea of image processing.", "Any idea? \nWhere can I find libraries and documentation explaining them?", "(I am working with ROS Hydro)", "Thank you"], "answer": [" ", " ", "Check out ", ". This converts ROS images to OpenCV images. From then on it's straight-forward open cv code.", "Is there any library to process images that belongs to ros?\n(without needing to convert it with CV Bridge)", "Something as powerful as Open CV: No. I would consider Open CV the image library for ROS.", "thank you very much ;)"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "It has to identify objects with a particular color and size.", "For this purpose I have a RGB camera which gives me a topic of the type: "], "url": "https://answers.ros.org/question/188138/computer-vision-in-ros/"},
{"title": "any good param to build my map?", "time": "2014-08-07 05:39:40 -0600", "post_content": [" ", " ", " ", " ", "im working on gazebo simulator , i have created my world sdf file and lunch file , but when i build the map of y world i got bad map not similar to my world ( not accurate map ) , although i played with the param but i still facing the same problem . any one has a good param i can use?\nthnaks in advance", "How did you build your map gmapping or hector mapping ? Better share your map build so far, for better info.", "i used gmapping demo"], "answer": [" ", " ", "It all depends on the computational power of your computer. If you have a fast computer, you can set your mapping parameters to lets say, higher resolution, larger map area, as well as update rate. If you are working on a arduino or a embedded system, you wont be able to set those parameters high because it will slow down the rest of your system.", "as ", " asked, please post your current parameters.", " , ", " can you tell what file shall i share with you ? , because im new in ros , by the way the power of my computer is very high", "you can see the parameters for gmapping in launch file. Here is descriptions about "], "url": "https://answers.ros.org/question/189302/any-good-param-to-build-my-map/"},
{"title": "The most compatible single boards with Kinect", "time": "2014-08-18 05:44:01 -0600", "post_content": [" ", " ", "Hi!", "I am looking for a single board which ", "I have found pandaboard, beaglebone and udoo, but I need suggestion for which one is the best card or another recommended sbc.", "Thanks.", "\"Kinect\" and \"image processing\" are two VERY different things. Can you be a bit more descriptive about your target application?", "I will get depth and rgb images from kinect sensor and I'm going to use image processing for re-identification."], "answer": [" ", " ", "It sounds like you don't really know how much processing you'll be doing (that's OK!), which means that you should probably aim for the most powerful CPU you can afford.", "In order of increasing price and power, I would recommend:", "The Intel NUC is a standard 64-bit processor and will be supported by standard Ubuntu and ROS, so it will be the most compatible with existing packages. It's also the most expensive. The Odroid and the Jetson both have enough processing power to receive camera and depth information from the Kinect, but you will have limited processing power to run computer vision research.", "I don't have any experience with the UDOO. They have a dedicated installation page on the ROS wiki, so they're probably well-supported, and the board has good specs.", "I would stay away from the BeagleBone; I know several people who have tried to use the Kinect with it and failed because it doesn't have enough CPU or USB bandwidth.", "The PandaBoard is getting old at this point. There are newer, more powerful boards on the market now, and I don't think TI is making the OMAP4 processor any more, so support may be limited.", " ", " ", " ", " ", "Thanks so much for suggestions, I will refer to recommendations.  ", "I have found cubieboard2 (Dual Core A20 ARM Cortex-A7), ECS (Intel D2550 Dual Atom 1.86GHz) and  Minnowboard (Intel\u00ae Atom\u2122 Processor E640). They can be good.  ECS caught my attention, but it has issues about installing ubuntu. Do you have any idea about them?"], "answer_details": ["Odroid U3 or XU", "Nvidia Jetson TK1", "Intel NUC", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "is able to run on ubuntu 12.04 or never version", "is supported from ros", "is capable with kinect", "mainly can perform image processing."], "url": "https://answers.ros.org/question/190452/the-most-compatible-single-boards-with-kinect/"},
{"title": "Would Java be a good option to use as a programming language for ROS?", "time": "2014-08-15 08:35:41 -0600", "post_content": [" ", " ", "I am not well versed with C++ so would using RosJava would be a better option?"], "answer": [" ", " ", "Python would be the preferred language for ROS imho. It's very easy to learn and very powerful as well. Maybe that would be an option?", "I agree with ", ". Try python if find C++ difficult. It's very easy and robust programming language!", "I'd like to add that I find C++ somehow closer to Java than python so since the poster is a Java expert, it might be easier for him to start with C++.", " ", " ", "I am not well versed with C++ so would using RosJava would be a better option?", "While ", " is certainly an option, I would recommend starting with ", " or ", " (as the other posters have written). Once you get acquainted with ROS terminology and concepts, ", " will probably start to make more sense. It is far more asynchronous and harder to 'wrap your head around' than any of the other client libraries (although if you are proficient enough in Java that asynchronicity should not really matter).", " ", " ", "Yes, Thanks guys.. I am a Java expert so very scared to go to C++. Python I hope would be a better option", "pls don't answer as a comment"], "answer_code": ["rosjava", "roscpp", "rospy", "rosjava"], "url": "https://answers.ros.org/question/190320/would-java-be-a-good-option-to-use-as-a-programming-language-for-ros/"},
{"title": "What is displayed in an RVIZ map?", "time": "2014-08-03 21:52:54 -0600", "post_content": [" ", " ", " ", " ", "I am looking at a Map display in RVIZ, using the ", " file from the ", " package (in ROS Hydro).  The map is created from the ", " node fed from the ", " nodelet, fed from a Kinect sensor.  My robot is stationary at the moment.  (It has to be, it's a table :-), but I publish a static TF frame showing that it is stationary.)", "I have disabled all of the displays except Map and Grid, and have removed (i.e. clicked on the \"Remove\" button) the Global Map and Local Map displays (more on that in a minute).", "I couldn't upload a screenshot of what I saw when I first wrote this (because I was a newbie and didn't have enough karma to include images), but ahendrix bumped my karma, so I can now include a screenshot.  I'll leave my original textual description in place still.", "Here is what I see:", "I can see the grid (of course).", "I can see black lines which match the walls that my Kinect can see.  These match the  documentation on the wiki at /rviz/DisplayTypes/Map) as being cells that are occupied.  So far this makes sense.", "But I can also see a greyish green transparent horizontal bar that is about 1.5 grid cells tall, whose lower left corner is at (-1, -1) and which extends to the right off the grid.  What is this?", "I can see a light grey area, shaped like a pie wedge, extending from the origin to the wall markers.  This would seem to be cells for which the occupancy is \"unknown\" since it is grey.  This seems strange to me that the cells have unknown occupancy, since the (fake) laser can see all the way through them right to the solid wall.  Perhaps they are supposed to be \"white\" according to the documentation.  It just doesn't look very white to me.  [EDIT: it _is_ white if I change the alpha to 1.0].  OK, this makes sense to me as well.", "If I don't remove the Global Map and Local Map displays (even if they are disabled), there is yet another greyish greenish 1x1 square whose left corner is at the origin.  It goes away if I remove both the Global Map and Local Map displays.  [EDIT: Ahhh I just noticed that the Map shows up as a 1x1 square if I delete it (and the other 2 maps) and re-add it.  It turns back into the display I described above once I subscribe to the /map topic.  Furthermore, I notice that I don't have anything publishing ", " and ", " topics, so I expect the 1x1 square is some sort of default visualization for the Global and Local Map displays that is shown in the absence of anything rational to be displayed.  OK, I suppose that makes sens now too.]", "Finally, when I read the documentation, I see that the Map display has ...", "I've bumped your karma. You should be able to post links and images now."], "answer": [" ", " ", "It sounds like things are working properly.", "The light, pie-shaped wedge you see if probably the free space between your sensor and the wall.", "The 1x1 squares you see at the origin are the default visualization of maps. The fact that these show up is probably a bug, but I haven't had time to file a proper bug report or propose a fix. They usually disappear if you disable the map displays.", "The extra map parameters you describe aren't documented, but they're fairly self-explanatory. Most of them, such as width, height and orientation, are not editable. The rest allow you to adjust how the map is draw, whether it's in front of or behind other things at the same depth, and the color scheme. Feel free to play with them to get a feel for how they work.", "I'm not sure what the greyish-green transparent bar you see is. A screenshot would probably help figure that one out.", "Hello ahendrix,\n\nThank you for your prompt response and for bumping my karma.  I didn't mean to imply that things weren't working properly, only that my understanding wasn't working properly :-).  I have updated my post to include the image.\n\nMore to follow...", "The 1x1 squares only disappear if I remove the maps.  Simply disabling them doesn't make them disappear.  As you note, that's probably a bug.  Now that I understand more of what's going on, I can work around that.", "I now see that the extra map parameters aren't parameters at all, they appear to be from the MapMetaData included in the OccupancyGrid message.  OK, that makes sense as well.", "Finally, I suspect that the greyish-green transparent bar (now shown in the screenshot) is likely to be the \"undecided\" cells reported by the mapper.\n\nThanks for helping me figure all of this out.  Stay tuned for my next (set of) question(s) regarding TF and odometry frames :-).", "Yep; the grayish area on the map in your screenshot is the \"unknown\" space."], "question_code": ["view_navigation.launch", "turtlebot_rviz_launchers", "slam_mapping", "depthimage_to_laserscan/DepthImageToLaserScanNodelet", "/move_base/global_costmap/costmap", "/move_base/local_costmap/costmap"], "url": "https://answers.ros.org/question/188752/what-is-displayed-in-an-rviz-map/"},
{"title": "Any chance of getting the gazebo2 and the collada-dom packages added to the armhf archive?", "time": "2014-06-17 16:25:49 -0600", "post_content": [" ", " ", " ", " ", "gazebo2 and collada-dom can't be found in any of the ubuntu repositories and are typically found in the ros repo for other architectures. These will be useful for bootstrapping any of the armhf full builds.", "PCL and sbcl are also missing, but I can build these in a PPA."], "answer": [" ", " ", " ", " ", "I have collada-dom-dev builds for most of the Ubuntu armhf variants, and PCL builds for 12.04 and 13.04. Which ones are you missing?", "I haven't tried to build gazebo... it doesn't really make sense to try to run gazebo on a low-powered ARM cpu, and I don't want to spend time on something that won't be useful. (The same line of reasoning goes for rviz).", "If you can manage to do builds of SBCL for armhf, you will be my hero.", "I've added builds of collada-dom-dev and pcl to my repository. I'm currently working on full builds of ROS.\nI also have a build of SBCL in the works. Fingers crossed.", "BeagleBone Black supports 14.04.\nSome of those arm processors aren't that low powered. For example the exynos processors have 8 cores and nvidia has a board with cuda built in.\n\nI'm trying to figure out how adam conrad bootstraped sbcl for ppc. Then I'll have a ppa with sbcl."], "url": "https://answers.ros.org/question/173857/any-chance-of-getting-the-gazebo2-and-the-collada-dom-packages-added-to-the-armhf-archive/"},
{"title": "Extrapolation Error and AMCL", "time": "2013-12-01 14:47:27 -0600", "post_content": [" ", " ", " ", " ", "Its my first time trying to get the navigation stack up and running. After fiddling with the .yaml files, I've gotten it to move with RVIZ. There are some issues I'm running into. ", "The robot is having issues localizing. My set up follows the ", " tutorials, but only have a laser scanner as an input.", "Here's the error generated by move_base.launch.", "After reading up on some other posts on answers.ros, to see what could be the problem, I took a look at my transforms, and I don't think thats it.", "Here is the tf_monitor report.", "I also ran ", "But there wasn't anything that threw a red flag at me.", "There were three of those errors thrown when I was running the navigation stack. The room I'm using was fairly large. I used rviz to set an initialpose. After I did that, sending a it a goal didn't go too well. It moved a lot farther than it was suppose to, and I had to shut it off before it had a chance to try to complete the path. It also did a lot of extra rotating even though it was sent a straight path.", "What additional information would be helpful to debug this issue? I'm not too sure where to look myself. ", "Thank you!", "Edit: \nAfter reducing the size of the local cost map back down to the original size of 6x6 @ 0.5 resolution, the extrapolatoin error went away.", "There weren't any grid cells with ..."], "answer": [" ", " ", " ", " ", "After making sure the odom message is being published is correctly, the main issue on getting it to run smoothly was the laptop.", "To make sure the CPU didn't stall, I essentially lowered the local/global maps and move_base controller update frequency to a point where the navigation wouldn't work well at all. For me, the map updating/publishing rates were down to 3Hz and 2Hz, respectively. The move_base frequency was also down to 3-4Hz. On  top of this, I was running rviz on the same laptop on an integrated graphics card.", "Some of the behaviors I had while running with the above setup was: ", "The local planar detected an obstacle. As it moves around it, the obstacle was dragged across the local map, enclosing the robot an completely unmovable space.", "This lead to the robot getting lost, a lot. I would see it moving to the goal designated, but when it gets there, it would spin to get the correct orientation. While this happens, the laser scan would slowly move out of place of the global map.", "Here is a couple general rule of thumb if you want to run the navstack. ", "Use gnome-system-monitor to see if the CPU is close to stalling. If you are, try splitting some of the cpu intensive nodes across different machines. ", " Or run rviz on different machine and just have the navstack run on the local machine.", "Don't run the map/controller frequency lower than the ros nav stack guide.", "I upgraded to a laptop with an i7-4th gen (~2.2Ghz), and an nvidia graphics card. I run the map publish rates and controller publish rates to twice the ros tutorial's values. It can run at higher velocities and smoother motions!", "Hopefully this post will help someone run the navstack successfully for the first time(like me). If any of this is incorrect, please comment and I'll edit the post!", "Edit: Naturally, those errors went away after the upgrade and the CPU stopped stalling/running so slow.", " ", " ", "I had the same problem with an youbot (ROS Hydro), that mounts a no powerful pc. I solved this problem changing the global frame of the local costmap from odom to map. The error disappeared and the navigation behavior became more smoothly and promptly with no drawback until now."], "question_code": ["[ERROR] [1385945596.417775629]: Extrapolation Error: Lookup would require extrapolation into the future.  Requested time 1385945596.400712013 but the latest data is at time 1385945596.309387000, when looking up transform from frame [odom] to frame [map]\n\n[ERROR] [1385945596.417830096]: Global Frame: odom Plan Frame size 305: map\n\n[ WARN] [1385945596.417853423]: Could not transform the global plan to the frame of the controller\n", "RESULTS: for all Frames\n\nFrames:\nFrame: /base published by /base_ctrl_odom Average Delay: 0.000454951 Max Delay: 0.000718333\nFrame: body published by /robot_state_publisher Average Delay: -0.499598 Max Delay: 0\nFrame: bracket published by /robot_state_publisher Average Delay: 0.000529436 Max Delay: 0.000995636\nFrame: bracket_90 published by /robot_state_publisher Average Delay: -0.499596 Max Delay: 0\nFrame: hokuyo published by /robot_state_publisher Average Delay: -0.499597 Max Delay: 0\nFrame: hokuyo_90 published by /robot_state_publisher Average Delay: -0.499597 Max Delay: 0\nFrame: left_wheel published by /robot_state_publisher Average Delay: 0.000526063 Max Delay: 0.000993213\nFrame: odom published by /amcl Average Delay: 0.0103003 Max Delay: 0.0867516\nFrame: right_wheel published by /robot_state_publisher Average Delay: 0.000528483 Max Delay: 0.000995021\nFrame: servo published by /robot_state_publisher Average Delay: -0.4996 Max Delay: 0\n\nAll Broadcasters:\nNode: /amcl 41.432 Hz, Average Delay: 0.0103003 Max Delay: 0.0867516\nNode: /base_ctrl_odom 5.82941 Hz, Average Delay: 0.000454951 Max Delay: 0.000718333\nNode: /robot_state_publisher 60.1176 Hz, Average Delay: -0.420926 Max Delay: 0.000994623\n", ">rosrun tf tf_echo odom map\n"], "url": "https://answers.ros.org/question/106261/extrapolation-error-and-amcl/"},
{"title": "Motion planing for Yaskawa Motoman MH5f", "time": "2014-09-06 07:14:41 -0600", "post_content": [" ", " ", " ", " ", "Hi everybody,", "I got from Yaskawa the MotoPlus runtime package and I installed the ", " packages. \nI also used the ", " to move the robot and wrote my own c++-Application sending the robot (publishing it in the ", " topic) a ", " defined by a simple ", ". \nThis works quite good. I have also some experience IkFast to make the Ik ond FK- Transformations.", "But now I want to be able to send an whole path to the robot and to plan the path.", "I have basically no idea how I could do this. I think I should use MoveIt!, but there are no specefications on the MoveIt! Homepage for my robot.", "Could you explain me which packages I may need to install for this and which kind of message I have to publish whereever? Or is there an example or Tutorial for this?", "Thank you!"], "answer": [" ", " ", "I have basically no idea how I could do this. I think I should use MoveIt!, but there are no specefications on the MoveIt! Homepage for my robot.", "The 'beauty' of ROS (or any other similar component based system) is that once components implement a certain interface, you can exploit the power-of-abstraction: motion planning for a Motoman MH5F is not (or rather: should not be) any different as for an ABB IRB 2400 or a Fanuc LR Mate 200iD. The fact that ", " does not matter: they are all serial, 6 axis robots, and all the ROS-Industrial drivers implement the same interfaces to control them (in fact: all MoveIt compatible robots implement the same interface, so they can all be controlled in a similar way).", "Could you explain me which packages I may need to install for this [..]", "This should not be any different from any other use of MoveIt with a compatible manipulator. You normally require (at least) a driver, a 'robot 3D model' and a MoveIt Configuration package. You already have the ", " package. The ", " package could provide you with the '3D model' (", ". I'm not sure how similar the MH5F is to the MH5).", "Unfortunately, a MoveIt Configuration for the MH5(F) has not been made available yet, but it isn't too hard to create one: see the ", " tutorial for that. First though, be sure to try and ask the people on the ", ", they might already have one.", "[..] which kind of message I have to publish whereever? Or is there an example or Tutorial for this?", "You already mentioned MoveIt, which is currently ", " motion planning framework for ROS. As a playing-around and debugging interface, I've always found the ", " very nice to work with. For direct API access, you can use the ", " or ", " interfaces to the ", " API. Configured correctly, this should allow you to plan complete trajectories for your MH5F (or again: any other MoveIt compatible robot).", "All of the above links are from the ", " page, which also lists some more advanced uses of the available functionality.", "In addition, you might be interested in the ROS-Industrial ", " which should cover your questions above."], "question_code": ["motoman_driver", "move_to_joint.py", "joint_path_command", "JointTrajectory", "JointTrajectoryPoint"], "answer_code": ["move_group"], "url": "https://answers.ros.org/question/192084/motion-planing-for-yaskawa-motoman-mh5f/"},
{"title": "Using xv-11 lidar", "time": "2014-09-17 05:47:50 -0600", "post_content": [" ", " ", "Is anyone using the xv-11 lidar unit with their robot?  I am in the process of incorporating the lidar unit with my bot and would like to share information.", "Thank you"], "answer": [" ", " ", "I have, Here's a rundown of what I've discovered.  I was never able to get GMapping to draw a good map with it.  HectorMapping did a decent job, but the maps were always slightly to big and the navigation stack never seem to work with them, my robot kept getting lost.  ", "I ended up pulling out the tape measure and using gimp to draw a map of my house.   Using my hand drawn map the xv-11 does a real good job of keeping my robot localized, I'm please with it.  ", "Have you checked if the lidar measurements are biased and show higher distances than they should?", "They are correct when stationary, I suspect it has something to do with the 4 hz laser data and the minimum speed of my robot (0.1 m/s).  I think the neato can go much slower than that.", "What voltage are you powering your xv-11 at?  I'm powering xv-11 at 5v and motor at 3v.  I took mine apart when I wasn't getting good results and on the power pin of the pcb board was +5v.  Something to check, also look at the firmware argument.", "Yes, it works well with the navigation stack. ", "Two things to check, are you using the firmware version 2 option? ", "and what is the average rpms from the following:\n", "I found this (link below), the firmware version 1 is suppose to be powered with 3.3 volts, could be something. ", "I did get the newer driver to work properly.  The rpm of the lidar unit was adjusted to 300 RPM.", "What did you have to change in the launch file to get a good scan?"], "url": "https://answers.ros.org/question/192852/using-xv-11-lidar/"},
{"title": "bash problems on pr2, robot command not found", "time": "2014-09-24 10:42:37 -0600", "post_content": [" ", " ", " ", " ", "Hey guys, I was having some issues with the kinect on the pr2 and so I had to uninstall and reinstall openni. I now get the kinect working (roslaunch / ros commands in general work), but somehow I think I messed up the bash configuration on the Pr2 machine. Now all commands robot start/stop, robot users etc, are not found.", "I tried source again /opt/ros/groovy/setup.bash but nothing seems to work.", "Also, right when I ssh into the pr2 machine AND when I type /bin/bash, I get the following:", "-bash: /usr/bin/check-ssh-keys: No such file or directory\nSourcing /etc/ros/setup.bash\n-bash: /etc/ros/setup.bash: No such file or directory ", "Do you guys know any workaround for this issue?"], "answer": [" ", " ", "You didn't just mess up bash. It sounds like you uninstalled most of the system management utilities that are required on the PR2. Did the package manager ask you something like:", "(You should never answer yes to that prompt without consulting a PR2 support representative first).", "You should be able to get back to mostly working by reinstalling the pr2-core-groovy and pr2-network packages:", "You should probably follow that up by running a systemcheck to make sure things are functioning properly.", " ", " ", " ", " ", "Yeah it was my fault. I got everything working with above commands, but now joystick doesn't work:", "I have the robot commands now but because of the previous issue, joystick doesn't work and some stuff is missing on diagnostics. for example:", "I tried to reinstall joystick drivers then, and I got the following:", "So I did apt-get -f install and I think it installed livudev-dev and everything.", "So I tried installing again the drivers and all went well:", "But  diagnostic still says joystick is stale and Joystick driver is missing so I cant control the Robot ...", "there are some weird ...", "Have you tried running ", " as it suggests?", "Given that you uninstalled and had to reinstall the joystick drivers, you probably need to restart the ps3joy service: ", "I tried all above, No work.", "I then tried apt-get install --reinstall ros-groovy-joystick-drivers\nand no work. I tried reinstall pr2-core-groovy and network, stilll no work. i dont what else to do.", "You should probably contact PR2 support.", "Yeah, I just installed pr2-* and ros-groovy-pr2-* and now my diagnosis is all green. Thanks a lot Hendrix!", "That's almost certainly a bad idea, because it pulls in many packages, and may even try to install conflicting packages, or packages that shouldn't be installed on a PR2. It would be much better to look at the set of packages that weren't installed, and only install them if they're necessary.", "This is why I suggest that you contact PR2 support; they understand which packages are necessary and can walk you through the debugging process."], "answer_code": ["You are about to do something potentially harmful.\nTo continue type in the phrase 'Yes, do as I say!'\n", "sudo apt-get install pr2-core-groovy pr2-network\n", "Power system: error\nIBPS 0 to 3 are missing, smart battery all missing. the only ok is the power board 1056.\n    More examples:\nDevices: error\nJoystick: Stale\nJoysting driver status: missing\n", "$ sudo apt-get install ros-groovy-joystick-drivers\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nros-groovy-joystick-drivers is already the newest version.\nYou might want to run 'apt-get -f install' to correct these:\nThe following packages have unmet dependencies:\n ros-groovy-oculus-sdk : Depends: libudev-dev but it is not going to be installed\nE: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).\n", "$ sudo apt-get install ros-groovy-joystick-*\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'ros-groovy-joystick-drivers' for regex 'ros-groovy-joystick-*'\nros-groovy-joystick-drivers is already the newest version.\nThe following packages were automatically installed and are no longer required:\n  config-package-dev libpcl-surface-1.7-dev\n  ros-hydro-opencv2 syslinux dnsmasq-base\n  libnet-ssleay-perl ros-hydro-rosbag ros-hydro-cv-bridge\n  chrony libgraphicsmagick3 ckermit pr2-grant\n  libpcl-surface-1.7 libpcl-features-1.7-dev\n  libcxsparse2.2.3 octave3.2 ros-hydro-roscpp\n  ros-hydro-rosgraph libfile-copy-recursive-perl\n  ros-hydro-image-proc ros-hydro-roscpp-serialization\n  ros-hydro-smclib ros-hydro-roslaunch libglpk0 libarpack2\n  ros-hydro-rospack libpcl-outofcore-1.7\n  libpcl-recognition-1.7 ros-hydro-message-runtime\n  pr2-ctr350 libcholmod1.7.1 libxml-parser-perl liburi-perl\n  unionfs-fuse ros-hydro-message-filters ros-hydro-roslib\n  libpcl-geometry-1.7-dev libhtml-parser-perl\n  ros-hydro-rosnode update-inetd\n  libpcl-registration-1.7-dev pr2-sendhwlog libv8-3.7.12.22\n  libhttp-daemon-perl ros-hydro-rosmsg rlinetd\n  ros-hydro-tf2-msgs ros-hydro-rosout\n  ros-hydro-camera-info-manager screen ros-hydro-pcl-msgs\n  libfont-afm-perl pr2-netconsole libhttp-negotiate-perl\n  libfile-listing-perl libhtml-form-perl\n  libpcl-features-1.7 libfftw3-3 syslinux-common\n  libvtk5.8-qt4 python-urlgrabber bluez-hcidump\n  ros-hydro-nodelet ifplugd ros-hydro-roswtf\n  ros-hydro-actionlib libaprutil1-ldap apache2-mpm-prefork\n  cdbs libpcl-tracking-1.7 netperf\n  ros-hydro-camera-calibration-parsers apache2-utils\n  ros-hydro-topic-tools libpcl-tracking-1.7-dev\n  libccolamd2.7.1 ros-hydro-cpp-common\n  libpcl-search-1.7-dev apache2 tcsh libhtml-tree-perl\n  ros-hydro-gencpp intltool libencode-locale-perl\n  libhttp-date-perl ros-hydro-actionlib-msgs\n  libmailtools-perl snmpd ros-hydro-diagnostic-msgs\n  pr2-chrony liblwp-protocol-https-perl apache2.2-common\n  ros-hydro-rostest libnetfilter-conntrack3 timelimit snmp\n  libpcl-keypoints-1.7 pr2-sysros libpcl-apps-1.7\n  ros-hydro-tf2-ros ros-hydro-rostime libparpack2\n  ros-hydro-rosgraph-msgs libhttp-cookies-perl\n  libpcl-keypoints-1.7-dev libhttp-message-perl\n  ros-hydro-geometry-msgs python-svn\n  ros-hydro-message-generation ros-hydro-rosservice\n  smartmontools pr2-ckill python-scour ros-hydro-genmsg\n  libaprutil1-dbd-sqlite3 libflann1 ros-hydro-class-loader\n  libapache2-mod-python libcolamd2.7.1 ros-hydro-rosclean\n  nodejs libnet-http-perl apache2.2-bin ros-hydro-xmlrpcpp\n  ros-hydro-rosmaster libgraphicsmagick++3 octave3.2-common\n  ros-hydro-rosunit libpcl-octree-1.7-dev libcap-dev\n  ros-hydro-genlisp libpcl-people-1.7 nfs-kernel-server\n  libhtml-format-perl ros-hydro-image-geometry libev4\n  libpcl-common-1.7-dev pr2-wrt610n ros-hydro-catkin\n  ros-hydro-rosbag-storage ros-hydro-bondcpp\n  libpcl-search-1.7 libpcl-sample-consensus-1.7-dev\n  libpcl-common-1.7 ros-hydro-roscpp-traits pr2-repo\n  pr2-netboot dnsmasq libdaemon0 libsocket6-perl libc-ares2\n  ros-hydro-tf zsh ros-hydro-bond executable-selector\n  pr2-stress libpcl-filters-1.7 pr2-repo-pr2\n  ros-hydro-image-transport ros-hydro-dynamic-reconfigure\n  libpcl-kdtree-1.7-dev dh-translations libhtml-tagset-perl\n  libpcl-filters-1.7-dev ros-repo ros-hydro-rospy libkms1\n  libwww-perl ros-hydro-nodelet-topic-tools fuse-utils\n  libvtk5-qt4-dev ros-hydro-rosbuild ros-hydro-tf2-py\n  libflann-dev ros-hydro-rosconsole ros-hydro-std-msgs\n  libpcl-registration-1.7 ros-hydro-genpy ros-hydro-tf2\n  libio-socket-ssl-perl libqrupdate1 libpcl-1.7-doc distcc\n  libpcl-sample-consensus-1.7 libwww-robotrules-perl\n  liblwp-mediatypes-perl libpcl-segmentation-1.7\n  libpcl-octree-1.7 ros-hydro-rosparam\n  ros-hydro-console-bridge ros-hydro-sensor-msgs\n  libpcl-kdtree-1.7 ros-hydro-rostopic\n  ros-hydro-diagnostic-updater texinfo pr2-systemcheck\n  libio-socket-inet6-perl atftpd\n  libpcl-segmentation-1.7-dev pr2-bios-bmc-images\n  ros-hydro-pluginlib\nUse 'apt-get autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 252 not upgraded.\n", "apt-get -f install", "sudo service ps3joy restart"], "url": "https://answers.ros.org/question/193393/bash-problems-on-pr2-robot-command-not-found/"},
{"title": "Compile roscore for ARM board", "time": "2014-08-23 11:43:55 -0600", "post_content": [" ", " ", " ", " ", "I have previously used ROS only on PC but found the use of rosbags for offline debugging to be convenient. I would now like to use ROS to record sensor data on an embedded ARM board running Linux. I wonder if it is possible to do this without doing a full ROS install on the board?", "All I would need are nodes running on the board which can publish their sensor readings to topics and then a laptop on the same network can do the rosbag recordings.", "I have a cross-toolchain set up for compiling executables for the board. The following document explains how to use a toolchain to compile libraries and ROS for ARM but it is not clear how to install it on the board once it's all compiled.\n", "If I manage to cross compile boost, apr, apr-utils, log4cxx and ROS (as mentioned in the above document) what do I get and how do I move it (and where?) to the ARM board?", "Can I then later cross compile my ROS nodes, copy them to the board and simply run them, provided roscore runs on a laptop?", "I hope this makes sense.", "Update:\nI am using a Wandboard with custom compiled Linux version 3.10.17.", "If your board runs Ubuntu, you can try the binary installs of ROS ", " or ", " for ARM. Note that you don't have to do a full install; you can choose to install only the things you need.", "The board does not run Ubuntu unfortunately...", "Could you provide more information on what board you're using, and what OS it runs?", "I added some info about board and OS at the end of the question."], "answer": [" ", " ", " ", " ", "I realize this answer is coming probably a month too late for you, but if you're still working on cross compiling ROS, here's a few tips:", "Don't follow those instructions you found, because they are very old and outdated. I started with that document, but quickly found it wasn't relevant. Basically, you want to cross-compile a bare-bones ROS from source and then cross compile your own ROS packages. I have done this exact thing with ROS Hydro, targeting an Odroid U3 running Ubuntu 12.11. I can outline the steps for you here, but since you have a different system, details will change. The way I figured things out, you basically need to have cross-compiled versions of the dependencies for anything you want to cross-compile. At some basic level, you will install a toolchain, which should include cross-compiled standard libraries. Many dependencies you will need to build yourself, however.", "Here's what you're going to need to do:", "Step 1. \nInstall your cross compiling toolchain. I don't know anything about the Wandboard, but it may use the same cross compiler toolchain that I used for the Odroid; I know the beaglebone uses the same one (g++-arm-linux-gnueabihf)", "Step 2. \nCross compile ROS dependencies. For bare-bones ROS Hydro, I needed the following dependencies: boost (1.56.0), Python (2.7.3), Bzip2, Poco, uuid, libtinyxml. Depending on what ROS packages you need, you might have different dependencies.", " Step 3. \nAfter you cross compile all of those packages, you need to cross compile ROS. You basically just follow the building ROS from source instructions ( ", " ). The only major difference that you will have is  when you build, you want to pass in a toolchain.cmake file, which will instruct cmake to use the cross compiling tools instead of your typical system tools. For a bare-bones ROS, use the following rosinstall_generator and wstool commands to get the necessary source code (modified for your needs, of course): ", "$ rosinstall_generator ros_comm <ros_pkg_1> <ros_pkg_2> ... <ros_pkg_n> --rosdistro hydro --deps --wet-only --tar > hydro-my_ros_config-wet.rosinstall", "$ wstool init -j8 src hydro-my_ros_config-wet.rosinstall", "Now, when you build ROS, you should use a rostoolchain.cmake file that looks something like this:", "This file just tells cmake how to look for things it needs. Think about CMAKE_FIND_ROOT_PATH as the starting point for your system. Cmake won't search down any path that doesn't begin with CMAKE_FIND_ROOT_PATH. So if you have boost libraries installed on your system under /usr/lib and you also cross compiled boost to /home/user/crosscompile/boost/. If your CMAKE_FIND_ROOT_PATH is set to /home/user/crosscompile, then it can find the cross compiled boost, which is what you need if you want to cross ...", "  I want to cross compile my custom ROS packages.Is it must cross compile ros,at first? My board can run roscore,but it achieve not by cross-compiler.Can I cross compile my own package wthout cross-compiling roscore?", "Alice, your custom ROS packages depend on ROS libraries, so you need the ARM version of those libraries to cross compile your packages. If you don't want to cross compile roscore, you could probably copy the ARM version of ROS to your build machine and link to that. I haven't tried that myself.", " Hello again! I'm now at this again (one year later), but this time with an Odroid board. I posted a follow up question to this ", "Thanks for your considerate guideline\nHave a nice day :)", "Do you cross compile Python as well?", " ", " ", " This page has a number of Ubuntu images for the Wandboard:  ", "  - It should be relatively easy to follow the UbuntuARM instructions and get ROS up and running on top of one of those within a few hours. ", " on the Wandboard mailing list has a few more suggestions about Ubuntu images to try.", "Note that there are only builds of ROS for armhf - all of the recent ARM chips have hard-float support and armel was too slow to do anything interesting, so it has been phased out.", "Thanks, but I wish to install ROS without changing the OS on my board, so this does not really answer the question...", "Given that the Wandboard has a nice i.MX6 processor, it should be powerful enough to compile ROS onboard. It takes a little longer, but the setup is much simpler.", "Or, if you're already building your own custom Linux, you should be able to use that toolchain to cross-compile ROS."], "answer_code": ["#File rostoolchain.cmake\nset(CMAKE_SYSTEM_NAME Linux)\nset(CMAKE_C_COMPILER arm-linux-gnueabihf-gcc)\nset(CMAKE_CXX_COMPILER arm-linux-gnueabihf-g++)\nset(CMAKE_FIND_ROOT_PATH /path/to/cross/compile/build/environment)\n\n# Have to set this one to BOTH, to allow CMake to find rospack\nset(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM BOTH)\nset(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)\n"], "url": "https://answers.ros.org/question/191070/compile-roscore-for-arm-board/"},
{"title": "the rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'", "time": "2014-10-14 13:05:19 -0600", "post_content": [" ", " ", "I get this message when I SSH into Baxter, but I do not have sudo access so cannot fix this error message. Is there a way to remove this warning or run ", " without sudo? I tried and it says:", "Thanks!"], "answer": [" ", " ", "A sudo-less option for rosdep has been talked about for while now, but nothing has ever gotten rolled out.", "This pull request addresses this by allowing you to set a ", " which can be in your home folder or whatever:", "But there are substantial changes to the behavior of rosdep in that pull request and without more people pushing on testing it out and getting all the docs and tutorials updated, it won't make it into master.", "We have addressed it in xylem, but that doesn't help you right now.", "Currently there is no way to use rosdep without sudo that I know of, but you could manually modify rosdep's source code to install and look for those files in a different place, to which you can write. This isn't very sustainable because you would need to do this every time a new rosdep comes out, but it would work for now.", "Thanks for the details. As I just commented on that Github Issue, I think a sudo-less option is important, but I cannot get into working on that feature myself.", " ", " ", " ", " ", "Would using ", " perhaps be an option?", "I cannot really find any 'official' documentation for it, but from what I've understood (from ", " answer from ", " on the buildbot-ros-sig, and from ", " in ", " on ROS Answers) you should be able to just create a copy of the rosdistro files that normally go into ", " anywhere your user does have write permission, and then update ", " to point to that location.", "I have no idea whether that environment variable is used throughout all bits that deal with dependency resolution, but it might be worth a shot.", "Edit: the xylem ", " also mentions ", ":", "[..]", "[..]"], "answer_details": [" ", " ", " ", " ", "Do the API calls respect the XYLEM_PREFIX environment variable or need explicit setting of a prefix parameter? I think the latter.\n  ", "Dirk: For rosdistro we actually do the first approach - the environment variable ROSDISTRO_INDEX_URL is also used for API calls (if not overridden by passing a custom index url). I think that approach has the advantage that any tool using rosdistro will use the custom url when it is defined in the environment.", " ", " ", " ", " "], "question_code": ["rosdep init", "ERROR: cannot create /etc/ros/rosdep/sources.list.d:\n    [Errno 13] Permission denied: '/etc/ros'\nPerhaps you need to run 'sudo rosdep init' instead\n"], "answer_code": ["ROSDEP_PREFIX", "ROSDISTRO_INDEX_URL", "/etc/ros", "ROSDISTRO_INDEX_URL", "ROSDISTRO_INDEX_URL"], "url": "https://answers.ros.org/question/194963/the-rosdep-view-is-empty-call-sudo-rosdep-init-and-rosdep-update/"},
{"title": "ROS + Intel Galileo + RPlidar not working, but gives OK health status", "time": "2014-10-05 15:01:11 -0600", "post_content": [" ", " ", "Heya all, hopefully some of you can help me with this frustrating problem.", " I am working with the project where I need to get indoor navigation data using RPlidar. The lidar is working fine with ROS when I'm connecting it to the development PC, but the idea is to get it running on Intel Galileo. RPlidar ROS package is installed with the instruction here:  ", "Currently, Intel Galileo has Debian running without problems (installation instruction from the Wiki) and RPlidar is connected to the board with GPIO pins. The lanch file has been modified that serial_port is changed from default ", " -> ", " and all GPIO pins have been configured with the instructions shown here:  ", "ROS is running fine and RPlidar power led is on (but the actual motor is NOT spinning the lidar, so it is not working). When I'm launching the rplidar node with ", " there is no problems - health code is 0 and everything seems to be working. However, when I try to get data with ", " - nothing happens. No errors, no warnings, nothing. And as I stated in previous command, everything seems to be normal - except from the motor is not running.", "I tried to search for the solution without success, but I have absolutely no clue why it does not run. Have I done something fundamentally wrong or is there some major issues with rplidar package and Intel Galileo?", "Looking forward to the solution and your respected help."], "answer": [" ", " ", "I had a similar, but not exactly the same, issue running rplidar on a raspberry pi B+, using ros hydro I built from source.", "My problem was resolved by using a powered USB hub for the rplidar.  So try disconnecting the rplidar from the usb on the galileo, connect a powered usb hub to galileo, and the rplidar to the powered usb hub.", "My issue also was led was on and system indicated good health, but no data.  Although in my case, the motor was spinning!.", "For me, perhaps either the power supply I was using for the B+ pi board wasn't big enough, or the B+ board itself has an issue with current needs of the rplidar, perhaps regarding starting the motor.", "Hi, \nDo you have the procedure, or the links to the tuto that you use to install the RPlidar on your raspberry ?\nBecause, I tried to install it on my rapsberry, but I have problem with instalation of ROS..", "thank you in advance", " ", " ", "You may already be aware of this: The motor is enabled by the RS232 DTR signal (so I read in another post). Have you checked it?"], "url": "https://answers.ros.org/question/194223/ros-intel-galileo-rplidar-not-working-but-gives-ok-health-status/"},
{"title": "ROS-I Computer - Recommended RT - Operating System", "time": "2014-11-22 11:15:40 -0600", "post_content": [" ", " ", " ", " ", "As far as I found out there are three ways to send data to an industrial robot:", "The third point as I understood needs collision detection already before downloading the trajectory. ", "The first and the second points are quite equal i guess. ROS-I on my PC can do on-the-fly-visualization and collision detection in ", ".\n", "\nIs RTAI necessary or strong enough? Or is there any recommended RT-upgrade to any specific linux system?", "\nI guess Ethernet via TCP/IP won't do. Would EtherCAT be usable?", "Regards", "Edit: My Imagination of realtime is yet hard to explain. But to be not too specific, I would rather like to know how realtime ROS-I is. What update rates are possible with ROS-I?"], "answer": [" ", " ", "It is my understanding that to use ROS-I you don't need any realtime guarantees. From what I've heard, typical ROS-I applications don't exceed update rates of 50Hz. You should be able to deliver that with decent latencies with a stock Linux kernel.", "Something that is not clear from the question is what realtime means for you. At some parts of the question it seems that \"realtime\" means an update frequency fast enough to allow online visualization, which is in the order of tens of Hz. You typically start requiring some form of realtime scheduling when you need determinism at update rates of hundreds of Hz (or more).", "That's some good information! Thank you! I am waiting a little more, may there are some more people who'd like to give me feedback, before closing this question.\nI now added on my imagination of realtime by editing my Question in the last line.", " ", " ", "Thanks Adolfo for your answer.  Just an additional point of clarification, the ROS-I interfaces as originally conceived would allow for individual point streaming.  At a low level some driver work this way (i.e. points are queued on the PC side and sent to the controller one at a time).  Other drivers send the whole trajectory all at once.  This difference is primarily driven by the controller.  Not all controllers were designed to take points one at a time.\nUnfortunately, at a ROS level, point streaming is not well supported.  Much of ROS is built around calculating a full trajectory before sending it to the controller.  Trajectories can be stopped during execution, but altering them was never really supported.  All this means that dynamically changing the trajectories on the fly is only available on those controllers that support trajectory streaming and only at a code level (not a ROS message level).", " ", " ", " ", " ", "A lean C++ TCP/IP app on standard Linux can achieve  latency in the order of 100 micro-seconds without a lot of jitter (only caveat is you need all power mgt disabled in BIOS).  With the large amount of processing needed for ROS/visualization/collision detection, a standard version of Linux will not likely be the limiting factor.  If you need sub 100 micro-second, you will need an RT OS."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Point Streaming", "Trajectory Streaming (like Point Streaming but with velocities added to the stream)", "Trajectory Downloading"], "url": "https://answers.ros.org/question/198003/ros-i-computer-recommended-rt-operating-system/"},
{"title": "What is the difference between Asus Xtion and Microsoft Kinect? [closed]", "time": "2014-11-25 20:56:28 -0600", "post_content": [" ", " ", "What is the difference between Asus Xtion and Microsoft Kinect?\nCan both of them produce RGB-D point clouds in hardware? Does Kinect have IMU?"], "answer": [" ", " ", " ", " ", "They both use the same 3D sensor from Primesense and produce RGB-D pointcoulds. ", "The Kinect has an embedded tilt motor and needs an external power source. ", "The Asus is smaller than the Kinect and can be powered from the USB current. ", "None of them has an IMU."], "url": "https://answers.ros.org/question/198321/what-is-the-difference-between-asus-xtion-and-microsoft-kinect/"},
{"title": "what are the differences between \"name\" and \"type\" in roslaunch?", "time": "2013-05-02 03:55:52 -0600", "post_content": [" ", " ", " ", " ", "what are the differents between \"name\" and \"type\" in roslaunch for nodes?"], "answer": [" ", " ", " ", " ", "Type is the file you want to launch in the package  whereas the name is a unique identifier for your node.", "In your example it will launch static_transform_publisher in package tf with name of the node as fake_localize.", "tnx for your link. for complete the answre, for example in my launch file >> the \"fake_localize\"(NAME) node is uniqe arbitrary name, (you can see in rxgraph!) using the \"static_transform_publisher\" (TYPE) executable from the \"tf\" package.", "Yes that's right!", " ", " ", "The easiest way to find out what to initialize in the type is to go to your CMakeLists.txt. Find the line that says something like add_executable(blah_blah src/whatever.cpp) then blah_blah is the type. Another way is to cd to your /devel/lib/ folder, dig into your package and there you will see the executables(probably, in green)."], "question_code": ["<launch>\n  <master auto=\"start\"/>\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"fake_localize\" args=\"0 0 0 0 0 0 map odom 10\"/>\n</launch>\n"], "url": "https://answers.ros.org/question/61983/what-are-the-differences-between-name-and-type-in-roslaunch/"},
{"title": "\"apt-get source\" for packages.ros.org?", "time": "2011-12-28 07:21:02 -0600", "post_content": [" ", " ", "Where are the sources (*.orig.tar.gz, *.diff.gz, and *.dsc files) for the debs on packages.ros.org?", "Specifically, how to make \"apt-get source\" do the right thing for packages from this repository?  Adding the obvious deb-src line to ros-latest.list doesn't seem to help (see below).  Poking around on the packages.ros.org http interface, it looks to me like the sources have not been uploaded.", "Packages I care about include ROS dependencies like assimp, yaml-cpp, and eigen.  For example, "], "answer": [" ", " ", " ", " ", " Just a follow-up; with the current package repository of  ", "  (Sep 2014), you can obtain the source of the released packages by simply  ", ". ", " I documented it ", ".", "Update 12/10/2014) ", " seems to be required. I updated the link above too.", " ", "You just need to explicitly specify deb-src entry in the apt sources:", "For example update ", " as follows:", "This assumes precise substitute your applicable ubuntu distro. ", "This downloads from the server all the files in the released version of the package (i.e. things not installed in the installation rule (e.g. CMakeLists.txt) are also included).", "This does not seem to work for me. I'm running Hydro on 12.04, e.g., both", "and", "return E: Unable to find a source package for ros-hydro-slam-gmapping.", "Am I missing something?", " I've updated my post above.", " that's not an appropriate place to put that information. Placing that at the end of the procedural instructions will distract/confuse beginner users as it's not necessary/useful for them to get started. It should be documented for power users, where I think this thread is pretty good.", "I would also highly recommend using rosinstall generator instead of apt-get source for fetching packages as you will get a version controlled source.", " I pulled the full content in here and promoted this answer. It's already the top search result for apt-get dsc ros  ", " Very much agreed.", " ", " ", "You will find the source for all the ros released stacks in the release repository ", "The existing infrastructure for 3rd party packages are quite ad hoc at the moment.  We are redesigning the system to make the whole process easier.  ", "In the mean time you can find the stacks you are looking for in gitbuildpackage repos:\n * Eigen is from debian upstream ", " cloned at ", "\n * yaml-cpp ", "\n * assimp is pulled from launchpad ", " ", " ", "Looks the same for me.", "This doesn't exactly fix your problem, but you can usually download the source code from each packages wiki entry and overlay the installed version with the sources by putting the local source directory to the front of the ROS_PACKAGE_PATH."], "question_code": ["$ apt-cache policy libassimp2\nlibassimp2:\n  Installed: 2.0.0.6-ubuntu1~lucid1\n  Candidate: 2.0.0.6-ubuntu1~lucid1\n  Version table:\n *** 2.0.0.6-ubuntu1~lucid1 0\n      500 http://packages.ros.org/ros/ubuntu/ lucid/main Packages\n      100 /var/lib/dpkg/status\n$ cat /etc/apt/sources.list.d/ros-latest.list \ndeb http://packages.ros.org/ros/ubuntu lucid main\n$ apt-get source libassimp2\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nPicking 'assimp' as source package instead of 'libassimp2'\nE: Unable to find a source package for assimp\n$ sudo vi /etc/apt/sources.list.d/ros-latest.list\n$ cat /etc/apt/sources.list.d/ros-latest.list\ndeb http://packages.ros.org/ros/ubuntu lucid main\ndeb-src http://packages.ros.org/ros/ubuntu lucid main\n$ sudo apt-get update\n...\nIgn http://packages.ros.org lucid/main Packages\nIgn http://packages.ros.org lucid/main Sources\nHit http://packages.ros.org lucid/main Packages\nGet:1 http://packages.ros.org lucid/main Sources [20B]\nFetched 20B in 1s (14B/s)\nReading package lists... Done\n$ apt-get source libassimp2\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nPicking 'assimp' as source package instead of 'libassimp2'\nE: Unable to find a source package for assimp\n"], "answer_code": ["apt-get source", "deb-src", "deb-src", "/etc/apt/sources.list.d/ros_latest.list", "  deb http://packages.ros.org/ros/ubuntu precise main\n+ deb-src http://packages.ros.org/ros/ubuntu precise main\n", "$ apt-get source ros-hydro-laser-pipeline\n", "apt-get source ros-hydro-gmapping\n", "apt-get source ros-hydro-slam-gmapping\n"], "url": "https://answers.ros.org/question/12478/apt-get-source-for-packagesrosorg/"},
{"title": "Could not resolve 'packages.namniart.com' [closed]", "time": "2014-12-13 13:50:52 -0600", "post_content": [" ", " ", " Following the ros installation tutorial for ARM devices, you need to add  ", "  to your sources.list.\n ", "However, it seems to have been down for at least 24 hours. Does anyone know if this was planned downtime, if it was permanently brought down, or if they are just having technical issues? Being unable to install deb packages for ros arm devices is pretty inconvenient..."], "answer": [" ", " ", "Sorry; I'm doing some maintenance on my packages server. It should be back online in a few days.", " As per  ", "  , I'm aware that the domain is expired and I'm working on renewing it. My provider is here in California, and they had a pretty significant power outage last week, so they're not responding quickly. ", "The standard ROS binaries are compiled for x86, so it isn't possible to use them on ARM.", "I've renewed the domain and the DNS records are updated. It may take up to 24 hours for the DNS changes to propagate.", "Your time and effort is much appreciated, thank you."], "url": "https://answers.ros.org/question/199429/could-not-resolve-packagesnamniartcom/"},
{"title": "Does ROS support this scenario off-the-shelf?", "time": "2014-12-11 23:52:19 -0600", "post_content": [" ", " ", " ", " ", "I'm building a mobile robot which is going to search for and collect small items in easily traversable environment with approximate map known beforehand. Robot will be equipped with a simple 2 DOF manipulator to pick, hold and deposit an item into a single stationary storage box. I will have my own vision software to detect the items to collect and course obstacles, and I will have a ROS driver for my wheeled platform and manipulator. Is there an off-the-shelf module in ROS, which allows for high level solution to this scenario? Do I have to write a lot of glue code and use multiple ROS modules: planner, navigation, mapping, etc.?"], "answer": [" ", " ", "One of the core assumptions of ROS is the understanding of ROS as a toolbox, not framework. Therefore ROS provides you with many powerful tools that can help you to quickly create solutions for various problems/applications that can be thought off. ROS is flexible and most tools are not created for a specific application. However, this includes that ROS does not provide solutions for a specific application of-the-shelf. You as a ROS user are still the one to create the solution.", "For your scenario I think ROS could help a lot, I think you won't have to write to much glue code for the existing ROS modules but you will still have to provide some input (sensors? map?) and for the interaction between navigation and manipulation etc. I think you'll also be moreless on your own."], "url": "https://answers.ros.org/question/199343/does-ros-support-this-scenario-off-the-shelf/"},
{"title": "Trying to get the XV-11 Lidar to work", "time": "2014-10-14 16:17:50 -0600", "post_content": [" ", " ", "Hey there!", "I already asked this over at the Trossen Robotics Forum but did not get a reply yet. Maybe someone here can give me a hint at what might be wrong with my XV-11 Lidar Setup.", "I bought the laser scanner from Ebay and am trying to get it to work in ROS.", "What I have done so far:", "Connected the Lidar to ROS Indigo running Ubuntu in Virtualbox on a Windows Machine.", "For the power to the motor I am using a 3V regulator that gets its power form USB (5V).", "The sensor is powered by an Arduino with 3.3V for now.", "Instead of the Sparkfun FTDI board I am using a standard RS-232 to USB Adapter that is getting its data from the Lidard's Tx. The adapter gets accepted by Ubuntu. Dmesg shows: ", "usb 1-2: FTDI USB Serial Device converter now attached to ttyUSB0", "Running", "rostopic echo /scan", "I get this:", "header: \n  seq: 34\n  stamp: \n    secs: 1413106818\n    nsecs: 470585788\n  frame_id: neato_laser\nangle_min: 0.0\nangle_max: 6.28318548203\nangle_increment: 0.0174532923847\ntime_increment: 0.00059940997744\nscan_time: 0.0\nrange_min: 0.0599999986589\nrange_max: 5.0\nranges: [0.25600001215934753, 8.192000389099121, 4.928999900817871, 13.229000091552734, 13.300999641418457, 13.274999618530273, 10.946999549865723, 0.37700000405311584, 14.380999565124512, 15.883000373840332, 15.381999969482422, 0.25600001215934753, 8.960000038146973, 6.848999977111816, 13.114999771118164, 13.0649995803833, 13.126999855041504, 13.109000205993652, 0.36899998784065247, 14.380999565124512, 15.883000373840332, 15.378000259399414, 0.5120000243186951, 1.2799999713897705, 6.880000114440918, 13.14900016784668, 10.869000434875488, 9.96500015258789, 10.836999893188477, 0.37599998712539673, 14.348999977111816, 15.873000144958496, 15.847999572753906, 16.128000259399414, 11.008000373840332, 6.90500020980835, 11.003000259399414, 10.888999938964844, 10.954999923706055, 4.479000091552734, 0.33799999952316284, 15.279000282287598, 14.972999572753906, 7.293000221252441, 0.23399999737739563, 8.019000053405762, 8.29699993133545, 15.732999801635742, 15.833000183105469, 7.519999980926514, 11.776000022888184, 8.960000038146973, 6.888999938964844, 11.795000076293945, 13.74899959564209, 13.708999633789062, 11.914999961853027, 0.4320000112056732, 11.229000091552734, 16.277000427246094, 16.277000427246094, 9.472000122070312, 8.704000473022461, 3.884999990463257, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 8.295999526977539, 15.189000129699707, 16.277000427246094, 13.991999626159668, 9.472000122070312, 0.0, 3.628000020980835, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 7.690999984741211, 15.189000129699707, 16.277000427246094, 15.187999725341797, 9.472000122070312, 7.935999870300293, 3.627000093460083, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 8.399999618530273, 15.189000129699707, 16.277000427246094, 13.991999626159668, 9.472000122070312, 7.679999828338623, 3.625999927520752, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 8.295999526977539, 15.189000129699707, 16.277000427246094, 13.991999626159668, 9.472000122070312, 7.423999786376953, 13.097000122070312, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 8.406000137329102, 15.189000129699707, 16.277000427246094, 16.37700080871582, 15.871999740600586, 11.520000457763672, 14.026000022888184, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 0.43700000643730164, 16.35700035095215, 16.37700080871582, 16.356000900268555, 15.871999740600586, 4.0960001945495605, 14.001999855041504, 16.381999969482422, 16.381999969482422, 16.381999969482422, 16.381999969482422, 0.43700000643730164, 16.047000885009766, 16.37700080871582, 16.327999114990234, 15.871999740600586, 8.192000389099121, ", "and so on...", "So there seems to be data coming in.", "But then here's my problem. In RVIZ the data looks like noise and it seems like the Laser is detecting ..."], "answer": [" ", " ", "The header is generated on the computer side, not the lidar, so it's quite possible that your data is corrupt even though the header looks correct. TTL and RS-232 use different voltages and different polarities, so that's probably causing problems.", "I would start by finding a proper USB to TTL serial converter. It doesn't have to be the specific Sparkfun model, but it should definitely be a 5V level and non-inverted (RS-232 is inverted).", " ", " ", " ", " ", "Hey ahendrix,", "thanks for the quick response!", "I noticed before that connecting the Lidar's Tx to the RS232 Rx did not deliver any data. So I connected Tx to Tx on the RS232 plug, which seems kinda fishy.", "I ordered a ", " now and it's on its way. I'll update this post as soon as I have tested it.", "Thanks again.", "EDIT:", "The issue is resolved and the XV-11 is working beautifully with ROS and hector_slam.", "There were a couple of things I had to change. First the Serial converter, I am now using a FTDI Breakout from Sparkfun to translate the signal to USB. Obviously these RS-232 converters speak a different language and that messed up the signal.", "Then I switched the sensor power from 3.3V to 5V. On the inside of the XV-11 on the board it says 5V right next to the power cable.", "Next I want to add the Razor 9dof IMU to the setup to get some attitude data, will see how that goes..."], "url": "https://answers.ros.org/question/194977/trying-to-get-the-xv-11-lidar-to-work/"},
{"title": "Can RosAria and p2os both be used simultaneously for pioneer robots?", "time": "2014-12-03 03:47:01 -0600", "post_content": [" ", " ", "Hi,", "I am currently using RosAria package for the pioneer p3dx on ROS-Hydro Ubuntu 12.04, 64 bit system. \nMy question is can I use the p2os package as well on the same robot or there will be compatibility and other issues?", "If yes, what are the benefits of using either one of the package. I know for a fact that RosAria is a newer , but there are more tutorials for p2os on the internet. \nIf I can install both, should I install it in the same workspace and catkin_make?\nI am new to ROS so any help would be much appreciated. ", "Thanks\nAlex"], "answer": [" ", " ", "You can install both and switch between them, but only one node at a time can run and connect to the robot controller, both nodes cannot run at the same time.   ", "When either node (any software actually) disconnects from the robot controller any temporary state (acceleration settings, pid settings, etc.) is reset within the robot controller.", "ROSARIA will work with more of the ActivMedia/MobileRobots/AMR robots especially the newer ones, but p2os will certainly work for Pioneer 3 and Amigobot.   p2os has been around longer, has more supporting infrastructure built around it like URDF, launch files, examples, and also doesn't require also installing the ARIA library.    I am personally biased towards ROSARIA since its easier for me to support, but recognize that p2os has advantages as well.   They have fairly similar interfaces (I think that the essential stuff at least is identical like cmd_vel, pose, sonar topics) so I think you should be able to switch from one to the other if necessary, by changing topic name references or remapping the topic names.  I don't have lots of experience with p2os though.  ", "Hi, Thanks for your reply. I am presently having no problems using the RosAria package and it works well with the hokuyo laser range finder. I am however unable to monitor the battery and robot status. Are there any tools to monitor them. Also do you have an idea for powering sensors using Aux point", " You can use the battery_voltage topic to check battery use.  Some robots (Seekur, LX) also publish battery_state_of_charge also.  There is no \"dashboard\" for RosAria like there is for p2os.  For power, see robot manual and ask  ", "  if questions. ", "Do the guys at mobilerobots give help regarding ROS for pioneer as well? I mean by the support email.", "Use ROS Answers and the sig-ros-pioneer mailing list to ask ROS-specific questions.  For questions about the robot or its behavior (using either ARIA or ROS software), ask aria-users, pioneer-users or AMR support. AMR robot-related questions there are higher priority for us but I also check here.", "Thanks ", ". One more thing, we dont have a gyro installed on our p3dx. Which economical external gyro can be used with p3dx?"], "url": "https://answers.ros.org/question/198802/can-rosaria-and-p2os-both-be-used-simultaneously-for-pioneer-robots/"},
{"title": "raspberry pi with ROS Indigo ( or any new Release of ROS)", "time": "2014-09-09 04:59:24 -0600", "post_content": [" ", " ", "Hi ROS Users,", "The ROS community has grown tremendously over the last 2 years, and it is good to be part of this growing community. Previously I tried to install ROS (Groovy) on Raspberry Pi using ", " tutorial and it was successful (Debian installation). I also understand that there is ", " (installation from Source) which requires lot of time for compilation. So now My questions are, ", "it is also much appreciated if someone can give pointers to a debian installation of Hydro on Raspberry pi. Thanks again, and thanks to the whole community,", "Best Regards,", "Murali ", "It would be nice if there was a collection of images somewhere for common hardware such as the Raspberry Pi and Beaglebone Black with Ubuntu + ROS preinstalled.", "This has already been discussed on other topics. Turns out the cost (money and man hours) to keep this kind of images AND maintain binaries+repositories is high. ", "Not practical in the end. Community goals do not lean that way. (which is a nice way to say that those platforms are not that popular)."], "answer": [" ", " ", " ", " ", "I think I've worked out many of the problems with getting Indigo installed on the Raspberry Pi. I've written up some installation instructions here: ", ".", "If you have a chance to try it out, I'd appreciate if you let me know if there are any problems and I'll try to keep it updated.", "Sure, will try it out over the weekend ... thank you for the update.", " Hi, Just installed bare-bones version of Indigo on Rpi, I did not overcome any problems during the installation. Truly nice work ... Thank you. Could you also leave an update as to how to install individual packages from source and also how to uninstall the same? Thanks again.", " Great! I added a section at the end on adding individual packages to the workspace.  I'm actually not sure the best way to uninstall a single package - to uninstall ros completely, you can just delete /opt/ros/indigo.", "I followed it, and it works fine until step 2.2.2 (rosdep install). It fails installing python-catkin-pkg and thus dependants, saying :\n\"python-catkin-pkg : Depends: python:any (>= 2.7.1-0ubuntu2) but it is not installable\"\nOf course Python is here (2.6, 2.7.3 and 3). Thanks in advance for any clue.", " Hmm, I'm not sure why I didn't run into that, but I would try to install catkin_pkg using pip (sudo pip install -u catkin_pkg) and then rerun rosdep.  If there's still an error, add -r to the rosdep command to ignore install errors and that may work.  What variant are you installing?", " Thanks a lot for your quick reply. I could bypass the problem by removing the \"python:any\" dependencies in /var/lib/dpkg/status and the whole process went fine. I'm installing the basic ros_comm variant. Could it be some change in the ubuntu repo the \"python:any\" depends clause points to?", " Glad you got it working. I think ", " was very recently added and the version of apt on wheezy doesn't handle it ( ", " ). So, your fix is fine, or it can be ignored because those packages are installed on pip. ", " I couldn't find any doc on the meaning of the ", " suffix BTW. Maybe the procedure described in this article should mention to install these packages using pip and not letting rosdep trying to do the job. Or am I misunderstanding something ?", " ", " ", " ", " ", "Hi There,", "Well, I tried this long, confusing, procedure and I was able to get Hydro on my RPi wheezy image. below are the steps I followed: (Please correct me if I did something wrong)", "Resizing the RPi image, well I used a 16Gb SD-card and was able to expand the file system on my Rpi hardware, using the ", " utility. (My Idea was, to boot with as much memory as I could get).", "Getting the expanded RPi image on to my PC (Ubuntu 14.04). with the command ", " (The Idea again is, not to use RPi hardware to perform source installation--->as it takes very long time)", "To boot the copied .img file in step 2 with QEMU (ARM Emulator). As mentioned in the previous answer above. (encountered a small problem here with the latest RPi image dated 09-09-2014). But I was able to boot into the RPi desktop.", "Following the ROS Hydro Source installation.I was able to complete the ROS installation and also could run the roscore on the QEMU terminal. (But with the exception that I was not able to install separate new packages and further steps were unsuccessful).", "The Last step is to get the image with ROS Hydro back onto the SD-Card with command, ", " ( step-2 reversed ). And ... I was NOT able to get the image run properly on Rpi hardware. ", "So, requesting ROS-Pi users who have already done this successfully to shed some light on this procedure, mentioned above. I spent the whole week with the emulation thing and could not get it to run on the actual hardware.", "Thanks again,", "Murali", "  I am using ", " Hardware, many old distributions won't even boot on the new hardware. During the boot, I also encountered a problem of \"missing kernel (ERROR) modules\" with the old dist of wheezy(after performing above steps). Guess, this info might be helpful. ", " Hydro installation (", ") is available, I am getting dependency error when trying to perform this installation. to be more specific, I am not able to fully resolve the dependency with the command `", "rosdep install --from-paths src\n  --ignore-src --rosdistro hydro -y -r --os=debian:wheezy", "`\nAnyone else who have done this please share your experiences. Also please let me know if you were able to do this successfully.", " ", " ", " ", " ", "I have a partial answer for you, but that can come in VERY handy, especially when it comes to installing from source.", "This is my personal experience, and I had limited iterations with it (tried just a few times) but worked well.", "Start with a standard debian SD image;  Run this image using QEMU, on a regular (powerful) computer.", "Before you start, make sure you have enough space on that image. Two GB can be too small. Four GB just right for the basics. I would recommend 16 GB, if the price is right for you. Keep in mind that later you will install this image on an ACTUAL SD card.", "Instead of creating an image with 16 GB, create it with 15900 MB. It is the never ending discussion about 1024 vs 1000, and sometimes the actual available space on a storage device will not be the ACTUAL reported space on the label.", "There are a few tricks you have to apply in order to get your ARM image to run on a qemu environment, but it works well. download, install and compile all you want on that image ", "I no longer have the links that I used to create my images, but here are the key points:", "Note: I did it for Hydro. Not yet for Indigo, but it should work.", "Cheers.", "++++++++++++++++++++++++-----------  LONG ADDITION -------------+++++", "Well, would you look at that. I found some of my notes I meant to transform into a WIKI/HOWTO but never finished.", "Before following or applying my steps, refer to the original links posted here. I recommend you understand the story rather than just repeating command lines.", "WARNING ! THESE INSTRUCTIONS WERE TESTED AND WORK, BUT IN THE END, I FOUND EASIER WAYS AND DID NOT UPDATE THE DOCUMENT.", "CONSIDER THIS A GUIDELINE AND ", " - I REPEAT ", " - A RECIPE FOR A SUCCESSFUL OPERATION.", "SORRY FOR THIS. It was my best effort.", "Well, like some people out there, I am taking my first steps in the ROS world, and my weapons of choice are a Raspberry PI (rev b, with 512 MB), an Ubuntu 12.04 running on VMWare Fusion, to be used as the \"base station\", and an idea in my head.", "Anyone trying their luck with a RPI knows that installing ROS takes time. Some brave souls out there created images for SD card with ROS already installed, and that is a brilliant job they've done.", "I had problems with those images, so I had to start my own tests.", "Now, the very first obstacle was the processing power of the RPI, so, like many, I decided to go virtual, using QEMU to install and compile everything. ", "The other \"little problem\"  I ran into was, the default Debian ...", "Hi, Thank you for replying. The points you have mentioned do make sense, It did give me a hint as to where I should start. Will be back with results...", "I had a dependency issue trying to get indigo on rasbian but hydro works fine.", ", you are right about the \"resize operation\", It is very tricky. I would like to point out here that, I am using ", " on 14.04 ubuntu (", ". it would be nice if you could elaborate a bit more on the resize operation, meaning what needs to be done when re-sizing? thnx", " you are right. Ii was going over my notes and I realize not only it lacks details, but also that it is confusing. I mixed parted instruction with the later use I made of gparted itself. Give me a few days. I'll repeat the operation and change my post.", " ", " ", " Hi,\nfind a Raspberrry Indigo for downloading here:\n "], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "resizing an SD image (tricky. Look into solutions that include gparted, and mounting the image as a loop device).", "Running the image with quemu (relatively easy, but you will be \"limited\" to 256 MB ram)", "transferring the final image to the SD card.", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "how should one proceed to install the latest version of ROS (For eg. ", " )  on Raspberry Pi from source? ", "how to create a ROS Indigo debian release, after installing from source (I would like to try this)? "], "answer_code": ["python:any", ":any", "sudo raspi-config", "sudo dd if=/dev/sdx/ of=/<path-to-my-image.img>", "sudo dd if=<path-of-the-image-file-with-ROS Hydro> of=/dev/sdx"], "url": "https://answers.ros.org/question/192250/raspberry-pi-with-ros-indigo-or-any-new-release-of-ros/"},
{"title": "ROS on the embedded boards", "time": "2015-01-07 23:27:32 -0600", "post_content": [" ", " ", " ", " ", "Dear All,", "I intend to buy a mini ubuntu- based system to install ROS on it. There are lots of boards to chose but I am not sure\nwhich one is the best for installing ROS on it. For example, Cubiboards support Lubuntu (not Ubuntu) which is not suitable for ROS installation. Do you have any suggestion?"], "answer": [" ", " ", "With the data you've provided, it's hard to make any recommendations.", "In particular, you don't say anything about the applications that you want to run. What you want to do with your board will dictate how many cores you need, how fast they need to be, which interfaces and peripherals you need, etc.", "There are a few types of research that you can do to help narrow down your search:", "Dear ", " I want to prepare a minimum ROS-based core system for Robotic purposes. Except some standard sensors such as encoders, IMU etc., it would be great if the board could support cameras like Kinect.", " ", " ", "Lubuntu is Ubuntu but instead of Unity as the desktop environment, it uses LXDE.  It can support ROS.  The board depends on your needs.  If you just need communication and basic processing, something like the Raspberry Pi or Beaglebone Black would work.  If you have more processing, you might want to look at dual or quad core boards like the Odroids, Nvidia Jetson TK1 or Udoo.  Some board have certain features that may be better suited towards your needs. This tough to answer since it really depends on your individual needs.", "I strongly discourage people from using the raspberry pi for robotics; it isn't supported by Ubuntu, it's difficult to get ROS installed, and it doesn't have a lot of computing power when compared to other boards.", "I totally agree, the Odroid-C1 is better value.  However, if you have one laying around, it could be some use like a WiFi connection to a microprocessor.  I will note that I was able to compile ROS indigo from the directions for ROS robot with no hiccups.", "Thanks for comments"], "answer_details": ["determine what sort of sensors and devices you need to plug in, and make sure the board you pick supports them", "search the internet for other people doing similar projects, and look at which computers they're using", "look at the ROS installation instructions and search the internet for guides on installing ROS on each board that you're considering. skim through each guide to get a sense of how well polished it is and how complex it is. Poorly-polished or complex guides indicate that it will take more work to get ROS running on that particular board.", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/200575/ros-on-the-embedded-boards/"},
{"title": "Transmission and track chain [closed]", "time": "2015-02-12 01:44:16 -0600", "post_content": [" ", " ", "I added a \"transmission\" block in the URDF to describe the model of the tracks of my robot... \nMy tracks are triangular, with an active sprocket and two passive sprockets. The active sprocket is rotated according to joint state message in my \"control node\".\nI expected that this kind of description resulted in a automatic joint state update of the passive sprockets in Rviz, but I think to have not understood very well how \"transmission\" works.", "Does transmission work only in Gazebo?"], "answer": [" ", " ", "If you mean ", "'s transmissions, only simple reducers are supported in Gazebo.", "It sounds like you need to implement a custom transmission type. Is this what you are trying to do?. If so, what is exactly the behavior you want to produce: That the active sprocket triggers motion of the two passive ones, or do you also want to move some sort of track geometries?.", "I would like to move the passive sprockets according to the angular position of the active respecting the radius ratio propagated by the track chain.\nI was going to create a subscriber to the joint_state message submitted by the \"active\" sprockets, but it does not seems a good solution to me.", "If you just want the passive sprockets to move as a linear function of the active one, consider using ", " (look for ", " element documentation).", "Great!!! It is just what I was searching for!!!\nThank you very much", "Glad to hear!. Please mark the question as answered if it is indeed the case.", "Hey, for the record. To mark a question as answered, you don't need to close it and provide a reason. There's a checkmark icon to the left of every answer. Once you click on it, it becomes green, and the question is considered answered. You can only accept one answer (when there are more than one).", "Done... I thought that closing it was the most complete way. Thank you again"], "answer_code": ["ros_control", "<mimic>"], "url": "https://answers.ros.org/question/202976/transmission-and-track-chain/"},
{"title": "Raspberry Pi 2 or Odroid U3 enough performance to run ROS and AR-Tag Pose Estimation?", "time": "2015-03-17 13:16:31 -0600", "post_content": [" ", " ", " ", " ", "Hello experts,", "the title says it all. I want to implement a Pose Estimation of AR-Tags on a Raspberry Pi 2 / Odroid U3 or similar board. But I have no clue, if these boards have enough performance to deliver a \"smooth\" estimation... \nA visual output is not required or can be attained through a ssh access from my pc (to save performance).", "Would be nice if someone could share his experiences with a Raspberry Pi 2 / Odroid U3 (or similar board) and ROS...", "Thanks for your help!", "Greetings Johannes"], "answer": [" ", " ", "I have not got Ondroid U3 or RPi2, however I have run ", " AR-tag node with ", " node on ", "I got the satisfactory results. Messages with marker's pose was published each 100 ms (10 Hz) for 320x240 resolution. \nFor resolition 640x480 I have got only 2-3 Hz. CPU loading was about 60% for 320x240 and about 90% for 640x480.\nI think that Ondroid U3 and RPi2 are more powerful, so you can use them for marker detection.", "Best Regards,\nAlex", "good news :) if it works on a single core, the RPI2 or Odroid U3 quad cores should have enough power (even for 640x480 (or higher)). Thanks for your help!", "It is truth, if the AR-tag detector supports several cores.", " ", " ", "I have only gotten simple programs to work on the rpi2 all of the computation is done on my workstation.", "Thanks for your answer. So computer vision (with ar-tags) isn't possible with a rpi2? Do you also use other boards like cubieborads oder odroids?", " ", " ", "Even though the raspberry pi might not be powerful enough to run the vision stuff on-board, it should be fairly easy to run it from the workstation. I was able to get the kinect kind-of working on the raspberry pi and publishing data.", "Thanks for your hint. Your solution is my plan b. But I want to use the camera and stuff on my mobile robot. Therefore I want to avoid a soultion including a workstation."], "url": "https://answers.ros.org/question/205221/raspberry-pi-2-or-odroid-u3-enough-performance-to-run-ros-and-ar-tag-pose-estimation/"},
{"title": "Raspberry Pi +Hydro + Openni/freenect", "time": "2014-09-29 11:48:38 -0600", "post_content": [" ", " ", "Hi All,", "I am very new to ROS, raspberry pi and linux. I already install Hydro into my raspberry pi. I can run roscore. ", "Next I want to run freenect_launch. I can'y sudo apt-get install freenect_stack. It say Unable to locate package freenect_stack. I check with apt-cache search freenect. It didn't have anything about freenect_stack/freenect_launch", "When I apt-cache search openni there is nothing. Should I add source.list? how do i do that? ", "How to i apt-get freenect_stack and use roslaunch freenect_launch freenect.launch ? ", "Sorry I am very new to all this. Do please link me if already have answered. ", "Are you trying to connect a Microsoft Kinect to a raspberry pi? What linux are you using in rpi?", " Debian. Thanks for your reply. I follow this tutorial to install hydro.  ", "If you are trying to connect Microsoft kinect to raspberry pi, it is not possible due to limitations in bandwidth on the rpi.", "Are you sure? I search the net, there are some successful project on kinect and raspberry pi", "I've got indigo running on a rpi2B (raspbian:jessie) and the kinect works with the freenect drivers. I've read that you can compile openni from github and that works too."], "answer": [" ", " ", "I tried interfacing kinect with a beaglebone black, I could not get the data stream from kinect. Try Odroid it is much faster. If you still want to use kinect with raspberry pi, you must either overclock it or  ", ", doing it from scratch. ROS +rpi + kinect cannot be done as it requires more processing power. Most of the projects that people have done would include a desktop pc to which they interface the kinect and stream the control data to rpi to control some actuators. ", "thank you for your explanation. I think i need to shift my work to using a laptop to process data from kinect and use raspberry pi as a controller. ", "Do you mind to share me some link about the project that kinect with PC + raspberry", "I was able to do it with freenect + indigo + rpi2B", " ", " ", "I battled with the Kinect on my Pi for a while and gave up. I suggest using the ASUS Xtion . I managed to get that working without too much difficulty. Remember to use OpenNI2 though.", "Mark", "I wish i already had a kinect ready. So just thought of using it for a project.", " ", " ", "I battled with the kinect and never gave up! I have a \"working\" implementation as a .img image and some code in ", " . It eventually outputs an 8 bit depthimage as a 16-bit mm-encoded ROS depthimage. The intention was kinect --> raspi --> laptop", "As far as I can tell, it's one of the few working ROS projects using kinect with Rasperry Pi 1 Model B+ \nMy implementation uses the librekinect driver, but has big limitations:\n6fps frame rate\n640x40 resolution\nConnection often drops for 1s periods", "Try it out. I just bought a raspberry pi 2, which can probably handle OpenNI, so I'll be trying that next as my current implementation is sub-par", "hey... I am also working on a similar project using rpi2. I got the freenect working. Did openni work?", "For any reading this: Yeah I got Openni working but NOT Openni2\nWould highly recommend using a different depth camera, not Kinect", " ", " ", " ", " ", "I was able to get the Openni driver and a Xtion working on a Raspberry Pi with ROS. I detailed all the process ", " if you want to do it. It took me quite a few weeks to get it right. I might be able to send a image in a couple of weeks if you need it just send me a private message.", "About the limitation in bandwidth I must say it is kinda very slow. It was more to say it's working than to use it in the end that I did this project. Just so you know."], "url": "https://answers.ros.org/question/193741/raspberry-pi-hydro-opennifreenect/"},
{"title": "Long term turtlebot storage and care", "time": "2015-03-04 11:18:00 -0600", "post_content": [" ", " ", "Does anyone have any experience with maintaining the netbook and Kobuki batteries during long term (6 months - 1 year) of storage? Do they need to be intermittently recharged? "], "answer": [" ", " ", "If you charge the batteries and then unplug them, they should survive 6 months to a year. Open the bottom of the Kobuki and unplug the battery pack. And remove the battery from the laptop, like it was shipped. ", "You might want to check the batteries every couple months, and recharge them if they're low. But as long as they're unplugged they are unlikely to be damaged in that time period. I don't know that anyone has paid attention to this sort of thing so some real data would be great. "], "url": "https://answers.ros.org/question/204239/long-term-turtlebot-storage-and-care/"},
{"title": "Dynamixel problem : No motors found", "time": "2013-06-18 22:11:00 -0600", "post_content": [" ", " ", "Hi !", "I just got a new computer on which I installed ros groovy.", "I am trying to use Dynamixel motors but I have an error. This error doesn't appear each time I launch the script so it's why I don't understand what's happening.", "Below is the error :", "Here is my launch file :", "The strange thing is that I can launch the script several times and got the error, and the second after (without changing anything) I launch it again and it works perfectly well. So the motors are connected and recognized by the computer.", "Does someone have an idea about why I have this problem ?\nThanks,", "Caroline"], "answer": [" ", " ", " ", " ", " I had the same Problem. I've solved it by downloading the Dynamixel RoboPlus software  ", " . It provides the Dynamixel Wizard which will search for any motor attached. You can then change all of its settings. ", "In my case the baud rate was way off, like Teja Krishna already guessed.", "I had this problem with my MX-64AR, and I even roboPlus doesn't help. I use usb2dynamixel and a power box to support..... it makes me crazy now....", "Hi just to make sure: Have you checked if the usb2dynamixel adaper ist set to the right protocol? And if it is, is one of the red led's flashing when you search for motors?", "Hi, Alex, I am sure the adaptor is set to RS485 (in the middle position), and red led is flashing.", " ", " ", "The order of launching using roslaunch is kinda of random so it could be that the tilt joint controller is started before the dynamixel manager is started (or not ready yet). Do you also have the problem when start the two components separately? /dev/Motors is populated each time?", "Thansk for your answer, but I have the same problem if I launch only the dynamixel manager... So it's not related to the order of launching.\nWhat do you mean by /dev/motors is populated each time ??", "There should be a device listed there if the Dynamixels are correctly connected. ", "Yes I can see that the Dynamixels are correctly connected, for example if I try the command \"udeavm info\" on the port /dev/motors I can see the connector USB2AX. ", "I have problem with detecting my servos, I spend many days to solve it, but I failed, please check it out ", " ", " ", "First check whether the motor id's are distinct or not. If not, try to set the id's with the help of the dynamixel_driver package. In this package there is a node called change_id to set distinct id's. If at all the baud rate for the motors is not the same set the baud rate to be the same by using set_servo_config node in the same package. This worked for me. Try this!"], "question_code": ["[INFO] [WallTime: 1371628756.187555] pan_tilt_port: Pinging motor IDs 1 through 25...\n[INFO] [WallTime: 1371628756.271966] pan_tilt_port controller_spawner: waiting for controller_manager dxl_manager to startup in global namespace...\n[FATAL] [WallTime: 1371628758.257885] pan_tilt_port: No motors found.\n================================================================================\nREQUIRED process [dynamixel_manager-2] has died!\n", "<launch>\n    <node name=\"dynamixel_manager\" pkg=\"dynamixel_controllers\" type=\"controller_manager.py\" required=\"true\" output=\"screen\">\n        <rosparam>\n            namespace: dxl_manager\n            serial_ports:\n                pan_tilt_port:\n                    port_name: \"/dev/Motors\"\n                    baud_rate: 1000000\n                    min_motor_id: 1\n                    max_motor_id: 25\n                    update_rate: 20\n        </rosparam>\n    </node>\n\n    <!-- Start tilt joint controller -->\n    <rosparam file=\"$(find emox_dynamixel)/tilt.yaml\" command=\"load\"/>\n\n    <node name=\"tilt_controller_spawner\" pkg=\"dynamixel_controllers\" type=\"controller_spawner.py\"\n          args=\"--manager=dxl_manager\n        --port=pan_tilt_port\n        base_tilt_controller\n        pico_tilt_controller\n        pan_controller\n        tilt_controller\"\n          output=\"screen\"/>\n</launch>\n"], "url": "https://answers.ros.org/question/65478/dynamixel-problem-no-motors-found/"},
{"title": "Finding the relative pose of a robot gripper", "time": "2015-02-17 05:10:40 -0600", "post_content": [" ", " ", "I have a robot arm with a gripper. I know the gripper pose (relative to the robot base coordinate system) at any moment. At startup, I record the pose of the gripper and set this as the original pose ", ". Then, the gripper moves to its new pose ", ", again in the robot base coordinate system. What I want to calculate, is ", ", the relative pose of the gripper ", " ", ", rather than in the coordinate system of the robot base, which would just be ", ".", "So, ", " = original pose, ", " = new pose, ", " = relative pose", "From my calculations:", " * ", " = ", "Therefore:", " = ", " * ", "However, from my observations, ", " is the pose of the hand relative to the original pose, but in the coordinate system of the robot base. I want it to be in the coordinate system of the original pose. How do I get this?"], "answer": [" ", " ", "If you're asking how to implement the solution in ROS, ", " is a powerful library that should exactly suits your need.", "If you're asking general robotics ideas, this forum is not the place for it. I recommend to ask on ", "."], "question_code": ["O", "N", "R", "O", "N", "O", "N", "R", "O", "R", "N", "R", "O^-1", "N", "R"], "url": "https://answers.ros.org/question/203267/finding-the-relative-pose-of-a-robot-gripper/"},
{"title": "Controlling robots in gazebo without using gazebo plugins [closed]", "time": "2015-03-21 00:47:18 -0600", "post_content": [" ", " ", "Is there a way to control robot models in gazebo by using external controller? I know usually it is controlled using plugins which we are adding in urdf file.", "I mean, without using a gazebo control  plugin is there a way to control robot model?", " I'm interested in the answer too. I would ask on Gazebo's forum  "], "answer": [" ", " ", "You can do non-physical movements like this:", "It's okay for setting model positions while time is stopped but intersecting two things will explode the sim.  It might be better to set velocities instead of positions but the model state has both, so any publish is going to overwrite both (in my example the velocities are going to be zero by default).  More advanced stuff can be done with GazeboJS, but C++ plugins are going to be the most powerful.", "The rawest controller plugins are the effort ones, in order to achieve a position you would have to write your own controller around them.  Are those not usable for your application?", "Thank u lucasw. It is useful.", "For reference, wth gazebojs and gazebo8 it would the equivalent would be:"], "answer_code": ["rostopic pub -1 /gazebo/set_model_state gazebo_msgs/ModelState '{model_name: testbot, pose: { position: { x: -0.32, y: 0, z: 2.1 }, orientation: {x: 0.0, y: 0.0, z: -0.766, w: 0.643 } }, reference_frame: world }'\n", " gazebo.publish('gazebo.msgs.Model', '/gazebo/default/model/modify', {name: 'testbot, pose: { position: { x: -0.32, y: 0, z: 2.1 }, orientation: {x: 0.0, y: 0.0, z: -0.766, w: 0.643 } } })\n"], "url": "https://answers.ros.org/question/205507/controlling-robots-in-gazebo-without-using-gazebo-plugins/"},
{"title": "arduino robot", "time": "2015-02-07 07:51:36 -0600", "post_content": [" ", " ", "Hello there !I would like to make a robot with arduino but using ros .Do you know any book or tutorials that i am able to read?Because i found some examples but i dont really get some parts!  Also have i to write python or c/c++ is good? \nThank you in advance! "], "answer": [" ", " ", " ", " ", "The package ", " is probably what you are looking for if you want to combine Arduino and ROS. See also related blogpost by the author ", ". ", "Also, I suggest picking up ", " as a general introduction to ROS. While not focused on Arduino, it is the best ROS introduction I know (and accidentally, is written by the same author as ros_arduino_bridge).", "EDIT: \nI should also point out that ros_arduino_bridge is basically a ROS driver for Arduino - it allows you to control your Arduino board from within the ROS framework. You will still need an additional, higher-end system to run the actual ROS nodes (Arduino hardware is not sufficiently powerful for that).", "Thank you very much! :)"], "url": "https://answers.ros.org/question/202650/arduino-robot/"},
{"title": "Geo-Referenced Mapping interface for ROS/RVIZ", "time": "2013-03-06 06:33:34 -0600", "post_content": [" ", " ", " ", " ", "Hi Guys,", "I found this video on Youtube of a QuadCopter mapping a building on a Georeferenced Map:", "The interface looks to be RVIZ, with the map laid across the ground plane.", "Does anyone know how to import a georeferenced Map, such as a geotiff, into RVIZ to get this same effect???", "I am looking for a mapping interface that can take geo-referenced maps of satellite imagery and plot the robots path and sensor information on top of it, just like the video.  I would also like to select GPS waypoints on the map, and send those to the robot to navigate too.", "This needs to be done without connection to the internet.", "The only other things I have found are:", " - ", ": World Wind is not supported very well in Ubuntu", " - ", ": Related to QuadCopters, and is no longer being updated", ": ", ": Does not zoom in very good and is no longer being updated", ": ", ": No high resolution Satellite images.", ": Google Earth/API has a lot of limitations and there licenses strictly forbids its use with autonomous vehicles. See section 10.2.C at ", ": It displays map features from Open Street Maps into rviz. OSM though does not provide satellite imagery unfortunately.", "Looking for your guys thoughts/ideas.", "Thank you", "Some packages to maybe look into for implementation into rqt are: OSSIM (", ") a QT geo app. Here is an example of their viewer: ", " . Also some others: "], "answer": [" ", " ", "Maybe qGIS would be an option (", ") too. It's not the easiest to use, but pretty powerful if you are familiar with GIS techniques and other GIS software (it uses GRASS internally to implement may of its geographic processing tools).  It's been around a while and is pretty mature and stable. It's similar in purpose to ArcGIS, and maybe OSSIM (but probably with more of a vector map emphasis than imaging); it's a general GIS toolbox  not just a viewer.", "How did you use qgis?  Did you have to write your own rviz plugin?", " ", " ", "Although my answer might be just fyi since integrating with ", " isn't an option and this might require internet connection, I'm asking opinions and putting together requirements for ", " plugin that plots ", " msgs.", "rqt would also work, and would just use rqt_rviz (", ") to embed rviz into it.  I have also looked into Google Earth/API, and found that it has a lot of limitations, also there license forbids any systems or functions for automatic or autonomous control of vehicles", " ", " ", "I just found this plugin."], "answer_code": ["RViz"], "url": "https://answers.ros.org/question/57313/geo-referenced-mapping-interface-for-rosrviz/"},
{"title": "New RoS setup", "time": "2015-04-08 10:16:51 -0600", "post_content": [" ", " ", "Hello everybody,\nI'm totally new to RoS, so I need a little help to understand if I'm planning correctly what I want to do:", "I would like to develop a simple bot that carries a camera and is remotely controlled (both using WiFi atm) with the idea of having a remote server image analisys (ideally OpenCV + cuda)", "The current setup would be: ", "\n 1. arduino for controlling the hardware (motors) ", "\n 2. raspberry for controlling the arduino and video streaming ", "\n 3. my laptop as remote server ", "For my understanding I need to install RoS on my laptop and on the Raspberry Pi, is this correct ? ", "\nwould would be the ideal structure ?", "Thank you very much, ", "\nCesare", "You might want to make sure that all the video stuff works with the raspberry pi you're using.  Not a lot of processing power on there."], "answer": [" ", " ", "The setup you describe sounds quite reasonable. As you said, you will need to install ROS on both the Raspberry Pi and the laptop. They will talk to each other via wifi. The Raspberry Pi could talk to the arduino any number of ways, but you could start by taking a look at ", " for some examples of getting communications up and running between Arduino widget and ROS. ", "You could run the ROS master (aka \"roscore\") on either side of the Wifi link, since you're teleoperating the robot and will be developing code actively for a while. If you eventually want to have the robot run autonomously and launch everything at power-up, many people run the ROS master on the robot to ensure that the robot's internal nodes connect regardless of the network status, but that's not terribly important when you are in the initial phases of hacking the system together."], "url": "https://answers.ros.org/question/206881/new-ros-setup/"},
{"title": "Working with ROS and Robot Arms [closed]", "time": "2015-04-14 11:32:46 -0600", "post_content": [" ", " ", "Hello!", "I am  working for my Bachelor Degree with ROS and I have to setup and implement a Robot System.", "The final goal is to grab a mug while the robot is driving next to it.", "Now my question is, since I am not so good with ros right now and I am at the beginning, what packages there exist to use Robot-Arms and which Robot-Arms are supported?", "Is it possible to solve this Problem with ros?", "It would be nice if anybody could answer me these questions :) greez"], "answer": [" ", " ", "There is a piece of software, ", ", that is for manipulation motion planning and execution.", "What are the specs of your arm? MoveIt supports 6DOF arms very well. However, for lower DOF arms one has to tweak the system a little bit, and do things like generate your own inverse kinematics solvers. There is a little bit of a learning curve.", "I'm currently working on integrating a simple 2DOF arm with it, not without challenges, but once you have it setup it can be very powerful. ", "  Sir How you integrate your arm with Moveit ? is it based on Arduino ??", " ", " ", "You can use the pick and place example from ROS by example 2, add a move base action server and have it do this\n", " ", " ", " ", " One solution is to use the ROS package - turtlebot_arm.  This uses Kinect for vision and works with the PhantomX robot arm.  You can see a pick and place / block sorting demo here - \n( ", " )  "], "url": "https://answers.ros.org/question/207124/working-with-ros-and-robot-arms/"},
{"title": "A problem in communicating with raspberry pi.", "time": "2015-02-07 10:25:58 -0600", "post_content": [" ", " ", "hello,"], "answer": [" ", " ", "If you want to use wifi it probably makes sense to install ROS on the pi and leverage the power of that. If you want a simple serial connection from the pi to ROS running on a different computer then a different radio infrastruture would make more sense - like zigbee, or one of the many pairs of radio modules that you send serial in one, and it comes out of the other."], "question_code": ["I want to use rosserial package to communicate with raspberry pi by wifi . Do I have to install ROS on raspberry pi? Is there a better way? Please help me, thanks. :))\n"], "url": "https://answers.ros.org/question/202654/a-problem-in-communicating-with-raspberry-pi/"},
{"title": "Kobuki : Timed out while waiting for serial data stream [/mobile_base] on Indigo", "time": "2015-04-13 20:41:38 -0600", "post_content": [" ", " ", "Using a turtlebot 2 on 14.04.2", " I ran through the install section at  ", "  and the network setup. ", "When running:", "roslaunch turtlebot_bringup minimal.launch", "The kobuki base makes it's startup noise, then, a few seconds later it makes it's shutdown noise. With the following error message being produced on the console:", "I get the same error when I try the kobuki node by itself:", "$ roslaunch kobuki_node minimal.launch", "setting /run_id to 0d2810f0-e247-11e4-8a15-0024d76aae74\nprocess[rosout-1]: started with pid [22493]\nstarted core service [/rosout]\nprocess[mobile_base_nodelet_manager-2]: started with pid [22510]\nprocess[mobile_base-3]: started with pid [22511]\nprocess[diagnostic_aggregator-4]: started with pid [22546]\nThis turtlebot was previously working in Furte. ", "Any ideas?"], "answer": [" ", " ", "In the end, powering off, then powering back on my turtlebot laptop seemed to be the fix. ", "The suspicion was usb bus issues with the kinect and the kobuki on the same usb port. "], "url": "https://answers.ros.org/question/207099/kobuki-timed-out-while-waiting-for-serial-data-stream-mobile_base-on-indigo/"},
{"title": "Contacting Turtlebot Distributor (I Heart Engineering).", "time": "2015-04-14 14:09:57 -0600", "post_content": [" ", " ", "Hello,", "I am having an issue with some of the equipment that my organization purchased through I Heart Engineering. Apparently I Heart Engineering has closed down and I am having a hard time getting in contact with them for warranty and support.", "Is anyone else having this issue?\nWhat did you do to get in contact with them?\nOr does anyone have any suggestions for me?", "The problem was not user error, the devices we ordered and received came with Asus x200 laptops. Those laptops have defective batteries, we didn't know they didn't work because we neglected to test them 6 months ago when we received them for the first time.We took the equipment out last week for a test drive to come and find out the batteries don't work. So now we have about two laptops without batteries that came through I Heart Engineering. ", "Thank you."], "answer": [" ", " ", "Laptop batteries need to be periodically topped up to keep them from falling below their minimum voltage due to self-discharge.", "You may be able to recover them by leaving them on a charger for a few days, but don't expect full capacity from them.", "The only problem with that logic is that we had a total of 4 that were in storage for a while and there is only 2 that have this battery issue. I did some research on the model of asus laptop that was shipped to us and other people had lemons. Therefore it leads to a defective conclusion.", "If the laptops themselves are faulty, you may want to contact ASUS directly for a replacement."], "url": "https://answers.ros.org/question/207130/contacting-turtlebot-distributor-i-heart-engineering/"},
{"title": "Smach Tutorial", "time": "2015-04-12 12:23:13 -0600", "post_content": [" ", " ", "First of is there a good alternative to Smach? If not then is there a better set of tutorials than the official ROS option?"], "answer": [" ", " ", "The Book ", " has a pretty good tutorial on Smach.  Plus it also has an alternative approach call pi_trees (behavior tree)  ", " ", " ", " ", "To my knowledge there is no alternative to Smach. But I might be wrong.", "Yep, the tutorial are good but they don't cover all that is possible with Smach. There is quite a few question around here that explain how to do more stuff with it (but I can't seem to find them again) and I have a repo on Github ", " if you'd like to see but I'm not sure it's going to be really useful.", ". I know there is a good trick into this that made Smach more powerful but I used it quite a long time ago and I don't quite remember what the trick is :P.", "It's pretty sad there is not more tutorial about it. ", " ", " ", " ", " ", "I am pretty sure the book(ROS by Example Vol 2) is aimed towards hydro, would it still be a worthwhile pickup?"], "url": "https://answers.ros.org/question/207041/smach-tutorial/"},
{"title": "Nav2d call getmap service failed", "time": "2015-04-10 10:05:59 -0600", "post_content": [" ", " ", "Hello,", "I already asked a question ", ". I received no answer but I found a way to work around the problem. I'm currently still trying to use the exploration service provided by nav2d. I solved my problem of map building by changing the mapper node by the gmapping one.\nBut now I'm still not able to call the StartMapping and StartExplore services. I got this message: \"Could not get a map.\"\nI looked at the code to find the not respected condition and I found that:", "Have you any idea of what to do when a service doesn't work? And particularly this one?", "Thank you in advance for any answer! "], "answer": [" ", " ", " ", " ", "Have you checked whether the service names match? Gmapping names the service \"dynamic_map\", while the Navigator defaults to \"get_map\". You can use the Navigator's \"map_service\" parameter to tell it what service to call for the map.", "Edit:\nCan you read my comments? Something seems broken...\nIt helps to visualize both global map and local costmap in RVIZ to see what is actually happening.", "Can you post a sceenshot from RVIZ with the map, costmap and the green/blue movement indicators?\nHowever, if your laser reports an obstacle, the Operator will try to avoid it. There is probably little you can do about that from the costmap-side. Maybe you can pre-filter the laser scan?", " ", " ", " ", " ", "Thank you very much, this solved my problem. But now the robot is easily stuck because of the obstacles. Which parameters can I change in the costmap  and navigator configuration files to decrease the obstacle influence?", " edit: I can only read your edit text and I see no comments. I used RVIZ to visualize the map as you said and I noticed two things: the turtlebot does a lot of circle because sometimes it isn't able to detect part of the map without obstacle:  ", "  ( it's because of the bad laser quality) and it's not able to go between two obstacles (1m). The blue navigation line is good but not the green corrected line. I think it's because the costmap is too large for obstacles. ", "edit_2: Finally I changed of robot and I use an Hokuyo laser instead of the Kinect.  All the nav2d packages are now working well and I'm able to use the default mapper node to build a map. Thank you very much for your answers."], "question_code": ["ros::ServiceClient mGetMapClient = robotNode.serviceClient<nav_msgs::GetMap>(std::string(\"get_map\"));\nnav_msgs::GetMap srv;\nif(!mGetMapClient.call(srv))\n{\nROS_INFO(\"Could not get a map.\");\nreturn false;\n}\n"], "url": "https://answers.ros.org/question/207001/nav2d-call-getmap-service-failed/"},
{"title": "Minimum computer specs for buying a used computer", "time": "2015-04-25 16:36:35 -0600", "post_content": [" ", " ", "So I tried to setup my laptop for school work to dual boot windows and linux only to nearly fuck it up due to windows 8 complicating everything, so I've decided not to take the risk and instead by a cheap used laptop. ", "maximum price is $80 (Im from Denmark, so please do not send me offers) so ofcause the laptop will not be great, but what is the minimum requirements for running linux\nand ROS? "], "answer": [" ", " ", "The core ROS libraries will run on just about anything, particularly if it can run Ubuntu.", "Which packages you want to use and what you want to do with your robot will have a much more significant impact on the CPU and memory requirements.", " As suggested elsewhere ( ", "  ,  ", "  and  ", "  ), look for a laptop with an Nvidia graphics card. ", "Given your extremely limited budget, just buy the most powerful laptop you can, and hope it's enough.", " ", " ", "Thanks!\nI've decided to see if I can boot ubuntu from an USB pen instead and use that. have anyone had problems with that before? \nIs that a sustainable solution? ", "The Ubuntu live environment usually boots well from a USB thumb drive, but it will only be severely limited by the speed of your thumb drive. If there is a local Linux Users Group in your area, you may want to contact them and see if they can help you install Ubuntu."], "url": "https://answers.ros.org/question/207896/minimum-computer-specs-for-buying-a-used-computer/"},
{"title": "Failed path planning Moveit+RIVZ", "time": "2015-04-30 00:13:40 -0600", "post_content": [" ", " ", " ", " ", "I am trying to learn of to do path planning with RVIZ+MoveIt for a UR5 arm. For now I'm trying to simulate only.\nI use Gazebo4, RVIZ+MoveIt and universal_robot packages that I built from source for ROS Indigo.", "I run these 3 command in 3 different terminal, as written in the universal_robot's README :", "I have several issues and I don't really now what is the origin.\nI tried to do path planning from RVIZ, on a simple path from home to up position and from up to home. Sometimes I have \"Failed\" message in the GUI, sometimes it works. But most of the time it didn't. I struggle to find proper documentation to use the motion planning panel", "I have the following error messages", "Terminal #2 :", "Terminal #3:", "If I use the Moveit command-line commander", "And try to \"go goal\" with goal from \"rec c\" then modifiy joint angles. Sometimes it works, but I still have this bug sometimes :", "I run it in VirtualBox and I can have 8 FPS in RVIZ.", "EDIT : I notice than even if I ask for simple joint movement", "I have the error", "Are you using the pkg from apt-get, or github? If you were the one who created the ", " you're using, I would say you just have to create controller configs (see ", ".", "from apt-get for all of them except universal_robot packages, I build it from indigo branch of the git repo,\nThe arm move most of the time, it means that the controller is configured.\nI have the errors messages but not always.\nMoreover I don't have any message related to wrong controller config."], "answer": [" ", " ", "I moved to a powerful PC. I installed Ubuntu 14.04 and all needed packages.\nThe problem is gone. Seems that the controller was not able to compute fast enough, because before it was very slow in VM.", "I am not sure of why this thing happened. ", "Is there no way to just slow down the execution of the planned trajectory so as to give the computer more time for the calculations? \nI ran into the same kind of issues and don't really have the opportunities to switch over to a more powerful computer..."], "question_code": ["roslaunch ur_gazebo ur5.launch\n\nroslaunch ur5_moveit_config ur5_moveit_planning_execution.launch sim:=true\n\nroslaunch ur5_moveit_config moveit_rviz.launch config:=true\n", "[ WARN] [1430368642.447147735, 185.001000000]: Controller  failed with error code PATH_TOLERANCE_VIOLATED\n[ WARN] [1430368642.447324161, 185.001000000]: Controller handle  reports status ABORTED\n", " [ WARN] [1430368548.820347974, 107.047000000]: Fail: ABORTED: Motion plan was found but it seems to be invalid (possibly due to postprocessing). Not executing.\n", "rosrun moveit_commander moveit_commander_cmdline.py\n", "manipulator> go goal\n[ INFO] [1430370662.457107039, 2110.983000000]: ABORTED: Solution found but controller failed during execution\nFailed while moving to goal\n", "  group.set_joint_value_target([1.5, 0, 1.5, 0, 0, 0])\n  group.go()\n", "[ WARN] [1430368642.447147735, 185.001000000]: Controller  failed with error code PATH_TOLERANCE_VIOLATED\n", "moveit_config package"], "url": "https://answers.ros.org/question/208208/failed-path-planning-moveitrivz/"},
{"title": "hector slam does not work! [closed]", "time": "2015-05-05 09:55:36 -0600", "post_content": [" ", " ", " ", " ", "I received below error after running", "\"", "\" ", "\" ", "I could see the tf in rviz , but there is no map!\nhow could I define transform between frames /map and scanmatcher?\nand why does it call terminate?\nAny idea?", "can you post complete log of error.", "that was the complete log of error"], "answer": [" ", " ", "To know how the tf works see the following tutorials :", " ", "An easy solution would be to manually starting a tf publisher that publishes the needed transform from the base_link (localized) to the laser scanner frame. See the example launch file below:", "and you should have your hector_mapping parameters like this :", " ", " ", "Thank you AlexR, I changed my computer and I  used my personal laptop which has more powerful graphic card, and I run the launch file with the bag file and it worked perfectly. \nI used the parameters below which give me a reasonable map.", "Glad it helped. Please accept the response as an answer by clicking on the \"check\" symbol."], "question_code": ["roslaunch hector_slam_launch tutorial.launch", "\"rosbag play Team_Hector_MappingBox_RoboCup_2011_Rescue_Arena.bag --clock", "terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'\n  what():  boost: mutex lock failed in pthread_mutex_lock: Invalid argument\n[hector_mapping-2] process has died [pid 9809, exit code -6, cmd /home/zargol/catkin_workspace/devel/lib/hector_mapping/hector_mapping __name:=hector_mapping __log:=/home/zargol/.ros/log/644b3e9a-f334-11e4-adf6-0025649a63d3/hector_mapping-2.log].\nlog file: /home/zargol/.ros/log/644b3e9a-f334-11e4-adf6-0025649a63d3/hector_mapping-2*.log\n[ WARN] [1430837517.680715839, 1310298004.093839447]: No transform between frames /map and scanmatcher_frame available after 20.001973 seconds of waiting. This warning only prints once.\n"], "answer_code": [" <node pkg=\"tf\" type=\"static_transform_publisher\" name=\"base_to_laser_broadcaster\" args=\"0 0 0 0 0 0 base_link laser 100\" />\n", "  <param name=\"pub_map_odom_transform\" value=\"true\"/>\n   <param name=\"map_frame\" value=\"map\" />\n   <param name=\"base_frame\" value=\"base_link\" />\n  <param name=\"odom_frame\" value=\"base_link\" />\n", " <arg name=\"base_frame\" default=\"base_link\"/>\n <arg name=\"odom_frame\" default=\"nav\"/>\n"], "url": "https://answers.ros.org/question/208584/hector-slam-does-not-work/"},
{"title": "Help with ROS", "time": "2015-06-22 08:13:01 -0600", "post_content": [" ", " ", " ", " ", "Hi all. ", "I really want to get into ROS but I am having quite a few problems. I only know the basic of programming, but my interest is much more in mechanical design and construction. That's also the reason why I am so much interested in ROS. ", "Is there a place where you can have a forum log, so it will be easier to get help than asking a question everytime. I am on my own with this in my spare time. ", "Does any one have a project which makes you connect an arduino to ROS, in order to control a robot with two motors connected to a h-bridge, just as a beginner project to get started?", "Also \nI'd liked to make the Follow me robot with the kinect sensor, but so far I can't make it work, but the kinect is connected since I can access the dual screen view with $freenect-glview and therefore I think I'll need to reinstall everything from what I can read from another guy having the same problem.  is it common that ROS packages gives a lot of trouble installing or is it because of all the other drivers the kinect uses making it much more complex? "], "answer": [" ", " ", "ROS is an incredibly powerful system, but also complex.  I'd recommend starting slow and simple.  Go through the beginner tutorials (you can get through them pretty quickly). Then start on a simple project.  Installing Kinect is one of the trickier things to do.  I would post your arduino/ h-bridge question separately with its own title to improve chance of finding someone who's done that. ", "Thank you corb. \nI've gone through all the beginner tutorials before I started trying to get the Kinect working. ", "Would it be better to invest in a Raspberry Pi or another controllerboard (Which one then) in order to use ROS for Robotic Platform development and design?", "There are basically 3 options for hardware:\n1. RaspPi-2 (or ODROID C1) - $36 -  this will be fine for basic ROS things.  It will not have enough power to run RVIZ.   Kinect may be a stretch.\n2. Nvidia TK1 - $200 - very fast ARM system with Kepler GPU.\n3. Cheap x86 system - will run all ROS pkgs"], "url": "https://answers.ros.org/question/211849/help-with-ros/"},
{"title": "Kinect v2 \"No devices connected...\" I've tried everything.", "time": "2015-04-12 15:13:23 -0600", "post_content": [" ", " ", "I'm at the point, I've tried literally everything for weeks now to get any kind of visualization and no dice because of No devices connected error.", "I'm on an intel i7 w/ hd graphics 4k\nrunning ubuntu trusty 14.04lts\ntrying to get visualization on ros indigo", "First I tried getting it all to run in a virtual machine, I thought that was my big problem that Virtualbox would not recognize USB 3.0 devices. So I abandoned that and partitioned my drive & installed ubuntu. Went through a bunch of stuff trying to get openni and openni2 to work, eventually finding out that that is for the 360 device. So now I tried libfreenect2 & freenect but still can not get passed the no devices connected error. ", "My lsusb lists the kinect in 3 places as it should. I don't know where to go from here.\nI tried 'protonect' from libfreenect2 but I get an error about openCL on there.", "Can someone walk me through it? ", "When you download libfreenect2 there's a rules folder.\nDid you copy the file 90-kinect2.rules in your /etc/udev/rules.d/ folder ?\nYou can run Protonect with cpu from the bin folder, to test if you can visualize the data :", "I your Kinect a model 1473? If so this question may help you: "], "answer": [" ", " ", " ", " ", "Hi,", "It seems that usb 3 is not well recognized on linux kernel < 3.13 so it doesn't work on every machine.", "EDIT: try georg l answer before, this will tell you directly if you have a permission issue or if the problem is elsewhere. Otherwise here is how I sovled it on my laptop.", "I've installed ubuntu14.04.2 which uses the newer kernel versions and kinect2 is working fine on it. \nI followed the steps  listed ", ".", "note: if you use 14.04.2, you'll need to install a bunch of X libraries, most of them are listed on the ", ".", "\nnote2: the registration of the kinect2 point clouds is done on the computer, which take a huge amount of computing power, I'm trying to figure out how to make the GPU perform the registration. On an i7 laptop the framerate drops to 5~6 fps when you perform the registration.", " ", " ", " ", " ", "I had the same  issue with a kinect v1 and the freenect stack. Try to change the permissions of the usb bus", "This helped me with ubuntu mate 16.04.2 with ROS Kinetic on Raspbery pi 3. Thanks..", " ", " ", " ", " ", "You need to create a rule for your user in udev. Create a file named ", ", put the following in it # ", "ATTR{product}==\"Kinect2\"\n  SUBSYSTEM==\"usb\", ATTR{idVendor}==\"045e\", ATTR{idProduct}==\"02c4\", MODE=\"0666\"\n  SUBSYSTEM==\"usb\", ATTR{idVendor}==\"045e\", ATTR{idProduct}==\"02d8\", MODE=\"0666\"\n  SUBSYSTEM==\"usb\", ATTR{idVendor}==\"045e\", ATTR{idProduct}==\"02d9\", MODE=\"0666\" ", "and save it in ", ". But do lsusb and make sure the id of your kinect corresponds to the ones above otherwise substitute the id's above with your kinect id. If you do copy it and all goes well, that should make you happy =)"], "question_code": ["./Protonect cpu\n"], "answer_code": [" sudo chmod 777 -R /dev/bus/usb\n", "90-kinect2.rules", "/etc/udev/rules.d"], "url": "https://answers.ros.org/question/207047/kinect-v2-no-devices-connected-ive-tried-everything/"},
{"title": "Localization based on a laserscan", "time": "2015-06-20 12:08:15 -0600", "post_content": [" ", " ", "Hello,", "i have a question regarding the localization of a mobil robot. Unfortunately my robot is not able to offer wheel odometry. I am looking for an approach to localize my robot in a given map just based on a laserscan.\nI read that hector_mapping can be used in this case, as it includes a laser-scanmatcher, which works well as stand alone. Is something similar avialable without the whole SLAM algorithm behind? I just need the \"L\" ;).\nMy robot has the constraint of relativly low computing power, why i also can't use visuel odometry.", "Thx for your answers"], "answer": [" ", " ", " ", " ", " Try this\n ", "Not a good answer, only for fuerte not for indigo..", "The wiki page is just saying that documentation is only generated for electric and fuerte. I have used laser_scan_matcher in hydro with no issues.", " ", " ", "Hi, ", "you can try amcl and use scan to scan registration to \"replace\" wheel odometry, for example discussed ", ".", "Alternatively, you can adjust hector slam to add functionality to save and load maps, this shouldn't take longer than a couple of hours..", "Best,\nJoscha ", " ", " ", " I would recommend to use Hector. I am using Hector for everything, you have to edit your hector to just use the laserscan and localize but not publish a map.\n ", " ", "From where do you get your map?", " \nAMCL should also work for you cause AMCL is publishing a pose and fake-odometry.  ", "I create the map with an other system. Its much more powerful and i am using gmapping for it. I will go on and test which approach needs less computing power. Thx for your tips."], "answer_details": ["roscd ;", "source devel/setup.bash", "cd src ;", " git clone  ", "apt-get  install libgsl0-dev ", "cd ..", "catkin_make", "Start IT.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/211789/localization-based-on-a-laserscan/"},
{"title": "Kinect isn't visible from Odroid when battery powered (turtlebot v2)", "time": "2015-01-31 15:23:57 -0600", "post_content": [" ", " ", "I have a problem with my turtlebot 2 that is driven by an odroid U3. The problem is that the Kinect isn\u2019t visible from the odroid. The odroid is powered from the 12V/5A connector on the kobuki base. There is a LM2596, 5V voltage regulator between. If I disconnect the voltage regulator and istead drive the odroid with a wall power supply is works perfectly. The current from the 12V source at the base is is measured to 1.5A, so current should be enough. To test I have also driven the odroid with an external 5V powerpack (charging unit for mobile devices 12000mAh, max current 3.4A), that didn\u2019t work either. With not working I mean that the Kinect isn\u2019t visible from the odroid, everything else works perfectly.", "There are a lot of other stuff on as well; screen, 11 IR sensors, mic and camera. Everything works well regardless of power source, except the Kinect that doesn\u2019t work when the odroid is battery driven. Everything except the things in the figure below are now disconnected not to interfere.", "I have no clue and I have tested and measured everything I can think of, any ideas?"], "answer": [" ", " ", " ", " ", "After some more measurements I see that when running on battery there are loads of USB error messages during startup. Then the voltage on the USB port becomes 4.2 V, while power from a DC/DC step down regulator fed into odroid is 5.25V. Still all other USB devices works well (WiFi, camera), but not the kinect. Current to kinect is 0 A.", "[Later] It didn't work with better (at least more expensive) regulators. First I thought it worked then I realized that the charging cable to the kobuki was attached. So it still doesn't work when running kobuki on battieries but it works when charging is attached. I use 2 5V/3A regulators to the 12V/5A kubuki port.", "Could it be a ground problem??", "Yeah, in my testing with other embedded boards I have made sure there's at least 3A available at 5V. 1.5A is enough for smaller peripherals but as you're observing can lead to brownouts with more/larger loads.", "Yes, that makes sense. However, the hardkernel power supply is the only soruce that works and it is 5V/2A. Maybe it is some transient thing. I will use better DC/DC and make sure 3A is possible. Let's see what happens.", "Did swapping the regulator work for you? Our turtlebot with Odroid U3 keeps on dying, and we're having a hell of a time debugging why. I'd imagine it's because we're using 2A regulators for the 5V and 12V rails, but we haven't been able to work it out.", "Our own problems were fixed by using an external battery for our Odroid. We emailed the Create support team, asking for specs, and they didn't provide anything helpful except to say that the DB25 connector on the Create was limiting our current output. Might be the same issue on the Kobuki?", "Changing regulators didn't help. I have also tried with a odroid U2 and same issue. However, I have found that it happens only when the battery is running low. It could be that the power to the kinect isn't enough for it and that the odroid works perfectly. Everything else works for quite some time."], "url": "https://answers.ros.org/question/202176/kinect-isnt-visible-from-odroid-when-battery-powered-turtlebot-v2/"},
{"title": "rosaria failed to connect to robot", "time": "2015-01-28 05:34:19 -0600", "post_content": [" ", " ", "Hi,", "I want to use the RosAria package in order to send commands and interface with my robot. I have managed to successfully install and build this rosaria package and I have also tested it multiple times on simulation (by using MobileSim by MobileRobots). ", "Now I want to connect rosaria to the actual robot, Powerbot (a pioneer robot by Adept MobileRobots). I have successfully connected to the robot ONCE by running", "Then I killed the node (^C) but when I tried to connect to it once again by re-running the same line (above), I got this error:", "Now $USER is a member of the ", " group and I'm sure that the robot connection worked because:\n(a) I had connected to the robot through rosaria ONCE during the first time and\n(b) Before I installed rosaria I was working with the ARIA library directly and everything worked brilliantly.", "In the error it is suggesting that I check that ~port parameter is correct, but I'm sure it is since I ran the same command previously.", "Can somebody help me understand this please? Thanks!", "I have had this problem before a dozen times or so in the past year. A dirty fix that worked for me was to hit the reset button. Then power off the P3AT, and repower P3AT. I may have restarted my GUI computer in one of the steps to regain the connection.", "It worked thanks!", "remember that you will have to run the sudo chmod command every time you start the robot. You can define the sudo permissions in your launch file for ease.", "can you mark it as an answer if it helped you. Thanks", "The reset button for Powerbot is in the panel on the top-rear of the robot. Newer firmware it will make periodic beep to indicate that it is still expecting software communication but data has stopped (reset maybe required in this case).  Sound can be turned on/off in firmware configuration though.", "I cannot connect to pioneer robot using rosaria for several days. Although I was able to connect when I installed first. I tried powering of and turning power on of the robot but nothing changed. I also used restart button that also did not work. can any one help?", " you can either follow the above comments, or check that the port you are using is valid. How are you connecting to rosaria?", "I am connecting P3 using a serial cable. I am running the following command line\n$ rosrun rosaria RosAria _port:=/dev/ttyS2\nS2 because I know this port worked well with aria example codes those are provided by Mobile robot.\nSo basically I am using a USB serial converter to connect to the robot."], "answer": [" ", " ", "I'd first check that rosaria (or other software connected to the robot) isn't still running. This can prevent ARIA from resetting the connection.  You can use the robot reset button as Orso says, but you shouldn't need to power off the whole robot.   I'll put in a request to check/fix rosaria to make sure it always disconnects correctly on control-c. ", "If you were using a USB-serial converter that could potentially be a problem, though it doesn't look like that applies in your case  (since you are using /dev/ttyS0)."], "question_code": ["rosrun rosaria RosAria _port:=/dev/ttyS0\n", "powerbot67@powerbot67-desktop:~/catkin_ws/src/rosaria$ rosrun rosaria RosAria\n[ INFO] [1422444061.117081691]: RosAria: using port: [/dev/ttyS0]\nCould not connect to simulator, connecting to robot through serial port /dev/ttyS0.\nSyncing 0\nNo packet.\nSyncing 0\nNo packet.\nTrying to close possible old connection\nSyncing 0\nNo packet.\nSyncing 0\nNo packet.\n Robot may be connected but not open, trying to dislodge.\nSyncing 0\nNo packet.\n Robot may be connected but not open, trying to dislodge.\nSyncing 0\nNo packet.\nCould not connect, no robot responding.\nFailed to connect to robot.\n[ERROR] [1422444067.613784823]: RosAria: ARIA could not connect to robot! (Check ~port parameter is correct, and permissions on port device.)\n[FATAL] [1422444067.613882512]: RosAria: ROS node setup failed...\n", "dialout"], "url": "https://answers.ros.org/question/201936/rosaria-failed-to-connect-to-robot/"},
{"title": "Turtlebot not appearing on RViz during mapping", "time": "2011-11-06 21:47:32 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm trying to follow ", " to build a map with the turtlebot. The thing is that I can not see anything on the rviz. If I look at the topics on the left column I see that it has two problems: Pose Array and Path. Both have the same problem: No message received.\nWhen I have a look at the diagnostics on turtlebot dashboard I get this message:", "My turtlebot doesn't have a gyro. Can this be the problem?", "Thanks"], "answer": [" ", " ", "From my experience Gmapping without the gyro will give the same results as you mentioned. Turtlebot doesn't move around the map. I believe this is due to the fact it obviously needs the gyro data to localize it self properly . I assume you could use RXGraph to find where the missing data link in gmapping is and possibly write your own code to create data to determine your robots location but, if I were you I would just invest in the recommended gyro.", " ", " ", "checkout ", " You will find a roomba560_node.cpp. For mapping you have to add a joint state publisher, just like turtlebot_node does.", " ", " ", "I've solved part of the problem and I only get the first half of what I posted. It only happends that nobody is publishing at the topics Pose Array and Path.\nI set the gyro as false, because I don't have one, useing: ", "What are this topics which I have the problem? I realised that when I run rviz the robot doesn't move arround the enviroment as it's suposed to do. What I mean is tat when I try to create a new map the robot only tourns and prints the map all at the same point so at the end I get a mess not a map. I think this has to do with one of the topics which is suposed to \"know\" where the turtlebot is.", "What would you recomend me to cheeck to see why is nobody publishing?", "Thanks", " ", " ", "Problem almost solve, I've changed the odometry in turtlebot_node.py for the one in roomba560.cpp and it's working quite well. Now it only needs a little bit of calibration...", " ", " ", "I've solved the problems. The two topics are not supposed to be published while mapping, they're only for the autonomous navigation.\nThe other thing is that I'm still not able to create maps, that's because of the odometry calculated on turtlebot_node.py is not correct for the roomba564. We're working on a new odometry for that model of robot, and hopefully when the new odometry works the mappgin will also work.", " ", " ", "Hi,", "I get the same transform problem and I have been scratching my head for some time to find out why.\nI have a gyro, it is present on the turtlebot_dashboard etc. I don't have a roomba base like in your case, I have the actual Create base.", "It remains a mystery.", "Have you tried using rxgraph to look for any broken links? Are you sure your gyro is fully functional and installed correctly? Are you receiving any data from the gyro ?", "Dashboard says so, also when running minimal.launch it specifies measurement rate of 150. It is all there. I'm getting a lot of skipping, ", " . ", " graph is complete. I've swapped out the Create base with another and still get the problem.", "Looking back at some of your other posts I noticed you were using a Samsung NC110. I think this netbook might not be enough power to run gmapping along with rviz together. Are you running gmapping and rviz on this one machine? Your cpu might be lagging and causing some problems. ", "Hi, Rviz is running on a workstation like it should. Point cloud throttling is set to 10.0 and I get a load average of 0.8 to 1.5 on the netbook, it is a dual core Atom N570, meaning no system overload.", " ", " ", "MichiReip, I have this package and I know it's working. but I can not work with both at the same time. They can't read both from the same USB port. that's why we want to change the odometry on the turtlebot_node.py for the one on the roomba560.cpp"], "url": "https://answers.ros.org/question/11824/turtlebot-not-appearing-on-rviz-during-mapping/"},
{"title": "Kinect for windows(first version) with ROS", "time": "2015-07-24 14:38:30 -0600", "post_content": [" ", " ", " ", " ", "I am try to use kinect for windows(first version) with ROS on a jetson", "I tried both freenect_stack and openni_kinect. In the both the cases I got the same error. Please let me know if you have any leads.", "Thanks\n-P", "No devices connected.... waiting for devices to be connected", "Bus 002 Device 010: ID 05d5:6781 Super Gate Technology Co., Ltd \nBus 002 Device 008: ID 05e3:0608 Genesys Logic, Inc. USB-2.0 4-Port HUB\nBus 002 Device 038: ID 045e:02c2 Microsoft Corp. \nBus 002 Device 006: ID 05e3:0608 Genesys Logic, Inc. USB-2.0 4-Port HUB\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub"], "answer": [" ", " ", "I know that I had to do a bit of fiddling with some USB settings when I first set mine up. It was a while  ago so I don't remember everything, but I know that I have to run this command before I launch the freenect_stack: ", "sudo bash -c 'echo -1 > /sys/module/usbcore/parameters/autosuspend'", "In addition to that, if you still see no devices connected, try unplugging the kinect from your laptop and plugging it back in and waiting about 5 seconds. Does the kinect for windows have a power adapter? If it does make sure that you have that plugged into the wall too or it won't work. The USB on your computer is not enough to power all the hardware in the kinect.", "Thanks! It worked!", "No problem, glad I could help!"], "url": "https://answers.ros.org/question/214520/kinect-for-windowsfirst-version-with-ros/"},
{"title": "What is the difference between acceleration and effort? [closed]", "time": "2015-09-01 20:58:52 -0600", "post_content": [" ", " ", " ", " ", "In ros_control what is \"effort\"? How is it different from acceleration? Can I use a value meant to be acceleration in place of effort?", "For context I'm using moveit and ros_control with a stepper motor controller that moves the joint given a target position, a maximum velocity, and an acceleration. The controller accelerates the joint up to the given max velocity, travels a bit, and then decelerates until it stops at the target position.", "I currently am using joint_trajectory_controller with just a PositionJointInterface, but the arm is very poor at travelling along a cartesian path. The end effector deviates from the path by several centimeters (it's a small arm about 15cm in length) My hope is that if I specify a PositionJointInterface, VelocityJointInterface, and EffortJointInterface (using acceleration) for each joint with joint_trajectory_controller I'll see the end-effector follow a cartesian path more closely."], "answer": [" ", " ", "Effort is the type of control of your joints where the commands that you specify for your joint are for example voltage or power. For example you could have a power controlled joint where the controller of joint motor can receive commands from 0 to 1000 (when controller receives 0 the motor would not move and when controller receives 1000 the motor of that joint would move at its maximal power).", "If you had a stepper motor controller that can be only controlled using effort you would then need to specify a EffortJointInterface and the joint_trajectory_controller would internally run a PID loop to control the position of your joints using effort.\nYour case is different you already have a stepper controller that can be controlled using target position. You therefore have a PositionJointInterface and in this case the joint_trajectory_controller only works by forwarding the target positions directly to stepper motor controller. You can read more about different modes of joint_trajectory_controller ", ".", "So specifying the VelocityJointInterface and EffortJointInterface in your case does not make sense.", "Now in terms of trying to improve the cartesian path there are several things you could try doing. First would be to try tweaking the parameters of the stepper motor controller. This is highly dependent on the actual motor controller that you have. If this controller has a PID loop internaly you might have to tweak PID parameters etc.", "That explains a lot. Thanks! I'll work on tuning the motor controller instead."], "url": "https://answers.ros.org/question/216930/what-is-the-difference-between-acceleration-and-effort/"},
{"title": "nav2d - Problems getting it working on a real robot", "time": "2015-08-12 16:39:39 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm trying to get nav2d to work. I have a launch file on my real robot which looks like:", "and a launch file on the PC to do all the nav2d work which looks like:", "I assume (correct me if the assumption is incorrect) that if I bring both these up and use to joystick to point the robot fwd then it will just start roaming around building a map, as it does in the Stage tutorials?", "Two issues I am seeing:", "1) I am getting a TF tree disconnect between the map and the laser - showing in rviz as:", "My TF tree is here:\n", " ", "2) Console is reporting:"], "answer": [" ", " ", " ", " ", "Please note that the topics \"/cmd\" and \"/cmd_vel\" are different (and should be so). Your remapping on the Navigator is most likely wrong. I assume this causes your type-mismatch.", "As a side note: I don't know how much ressources you have on the robot and how your robot and PC are connected, but you should consider moving only the Operator over to the robot and then use the /cmd topic (of type \"nav2d_operator/cmd\") on your ros-bridge. This should cause much less traffic on your bridge and also move the (somewhat critical) obstacle avoidance directly on your robot.", "Edit:\nGetMap, SetGoal and Explore are helper nodes to call the navigator's actions.You need SetGoal if you want to send goal positions with RVIZ.", "If your costmap is not updated, it's always good to check the sensor height first. The local costmap has a min_obstacle_height and a max_obstacle_height (", "), and the sensor height must be in between. (Use RVIZ to check the height of the laser scan line.)", "Thanks I will create a new message to the bridge to accept these messages and let you know. The robot's core is a Raspberry Pi 2 - should have enough power to move the Operator across. Will try that once I get the rest working", "Ok dumb question - how do I measure the laser scan height in rviz. In reality it's 22cm of from the floor. I've not touched the costmap.yaml file. Defaults are:", "obstacle_range: 4.0\nmin_obstacle_height: 0.0\nmax_obstacle_height: 2.0\nraytrace_range: 4.5", "Thanks", "As a simple check, you can just turn the camera a little so you can see if the laser points are roughly 22cm above the ground. You do not worry about one or two cm, but whether your scans are maybe 22cm beneath the floor. This is often caused by errors in the transformations.", "Okay - that's what I though. At the moment it's on the floor. Will check the transforms and revert", "That's done the trick - the static transform was incorrect. cost map has started picking up the data...now to get the robot to actually think about moving!", " ", " ", "Two issues:", "Thanks - The bridge is supposed to be sending out an odom transform - not sure what's happened to it. Will check"], "answer_details": [" ", " ", " ", " ", "Your tf tree is disconnected, into the robot frames on the left and the navigation related frames on the right. There should be a connection between base_link and odom (see ", ") that normally is provided by either your mobile base controller directly, or via another node like ", ". The latter case if frequently used when the odometry transform is estimated from multiple sources of information, such as encoders plus gyros.", "Your ros_bridge expects a standard ", " message type for the \"cmd_vel\" topic, but the publisher it is connected to appears to publish a ", " message type instead. This can be fixed by making sure that your publisher also publishes a ", ".", " ", " ", " ", " "], "question_code": ["<launch>\n<node name=\"toeminator_ros_bridge\" pkg=\"toeminator_ros_bridge\" type=\"toeminator_ros_bridge_node\"/>\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"static_transform_publisher3\" args=\"1 0 0 0 0 0 1 base_link base_laser_link 100\"/>\n<node name=\"depthimage_to_laserscan\" pkg=\"depthimage_to_laserscan\" type=\"depthimage_to_laserscan\" > \n    <remap from=\"image\" to=\"/openni2_camera/depth/image_raw\"/>\n    <param name=\"output_frame_id\" value=\"base_laser_link\"/>\n</node>\n<node name=\"openni2_camera\" pkg=\"openni2_camera\" type=\"openni2_camera_node\" />\n</launch>\n", "<launch>\n        <!-- Some general parameters -->\n        <param name=\"use_sim_time\" value=\"false\" />\n        <rosparam file=\"$(find nav2d_tutorials)/param/ros.yaml\"/>\n\n        <!-- Start Stage simulator with a given environment -->\n        <!--node name=\"Stage\" pkg=\"stage_ros\" type=\"stageros\" args=\"$(find nav2d_tutorials)/world/tutorial.world\">\n    <param name=\"base_watchdog_timeout\" value=\"0\" />\n        </node-->\n\n        <!-- Start the Operator to control the simulated robot -->\n        <node name=\"Operator\" pkg=\"nav2d_operator\" type=\"operator\" >\n            <remap from=\"scan\" to=\"base_scan\"/>\n            <rosparam file=\"$(find nav2d_tutorials)/param/operator.yaml\"/>\n            <rosparam file=\"$(find nav2d_tutorials)/param/costmap.yaml\" ns=\"local_map\" />\n        </node>\n\n        <!-- Start Mapper to genreate map from laser scans -->\n        <!--node name=\"Mapper\" pkg=\"nav2d_karto\" type=\"mapper\">\n            <rosparam file=\"$(find nav2d_tutorials)/param/mapper.yaml\"/>\n        </node-->\n\n        <!-- Start the Navigator to move the robot autonomously -->\n        <!--node name=\"Navigator\" pkg=\"nav2d_navigator\" type=\"navigator\">\n            <remap from=\"cmd\" to=\"cmd_vel\"/>\n            <rosparam file=\"$(find nav2d_tutorials)/param/navigator.yaml\"/>\n         </node-->\n\n        <!--node name=\"GetMap\" pkg=\"nav2d_navigator\" type=\"get_map_client\" /-->\n        <!--node name=\"Explore\" pkg=\"nav2d_navigator\" type=\"explore_client\" /-->\n        <!--node name=\"SetGoal\" pkg=\"nav2d_navigator\" type=\"set_goal_client\" /-->\n\n        <!-- Start the joystick-driver and remote-controller for operation-->\n        <node name=\"Joystick\" pkg=\"joy\" type=\"joy_node\" />\n        <node name=\"Remote\" pkg=\"nav2d_remote\" type=\"remote_joy\"></node>\n\n        <!-- Pioneer model for fancy visualization -->\n        <!-- Comment this out if you do not have the package 'p2os' available! -->\n        <include file=\"$(find p2os_urdf)/launch/pioneer3at_urdf.launch\" />\n\n        <node name=\"front_left_wheel\" pkg=\"tf\" type=\"static_transform_publisher\" args=\"0 0 0 0 0 0 p3at_front_left_hub p3at_front_left_wheel 100\" />\n        <node name=\"front_right_wheel\" pkg=\"tf\" type=\"static_transform_publisher\" args=\"0 0 0 0 0 0 p3at_front_right_hub p3at_front_right_wheel 100\" />\n        <node name=\"back_left_wheel\" pkg=\"tf\" type=\"static_transform_publisher\" args=\"0 0 0 0 0 0 p3at_back_left_hub p3at_back_left_wheel 100\" />\n         <node name=\"back_right_wheel\" pkg=\"tf\" type=\"static_transform_publisher\" args=\"0 0 0 0 0 0 p3at_back_right_hub p3at_back_right_wheel 100\" />\n\n        <!-- RVIZ to view the visualization -->\n        <node name=\"RVIZ\" pkg=\"rviz\" type=\"rviz\" args=\" -d $(find nav2d_tutorials)/param/tutorial1.rviz\" />\n\n      </launch>\n", "Transform [sender=unknown_publisher]\nFor frame [base_laser_link]: No transform to fixed frame [map]. TF error: [Could not find a connection between 'map' and 'base_laser_link' because they are not part of the same tree.Tf has two or more unconnected trees.]\n", " [ERROR ..."], "answer_code": ["geometry_msgs/Twist message", "nav2d_operator/cmd", "geometry_msgs/Twist message"], "url": "https://answers.ros.org/question/215763/nav2d-problems-getting-it-working-on-a-real-robot/"},
{"title": "Robot rotates before or as it reaches goal", "time": "2015-09-29 14:30:48 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "I have a question regarding my robot rotating often while using frontier exploration, also it rotates just before it reaches the goal?", "also the costmap does mark and clear with the laser scan (asus xtion- depthimage_to_laserscan) but the ultrasonic sensor does not mark or clear the costmap?", "(based on husky exploration)", "Is there anything wrong with the configuration??", "costmap_common.yaml", "costmap_exploration.yaml", "local_costmap.yaml", "global_costmap.yaml", "planner.yaml"], "answer": [" ", " ", "I Solved the issue, the acceleration limits in the trajectory planner were too high. Once they were decreased and find tuned to give the best robot movement robot does not rotate anymore. ", " ", " ", "have the robot the target orientation of the  target? same orientaion as the green goal Arrow?  if not the robot rotatis until it is so."], "question_code": ["footprint: [[-0.230,-0.16],[-0.230, 0.16],[0.230,0.16],[0.23,-0.16]]\nfootprint_padding: 0.01\n\nrobot_base_frame: base_link\nupdate_frequency: 3.0\npublish_frequency: 2.0\ntransform_tolerance: 0.5\n\nresolution: 0.099\n\nobstacle_range: 1.5\nraytrace_range: 1.5\n\n#layer definitions\nstatic:\n    map_topic: /map\n    subscribe_to_updates: true\n\nobstacles_laser:\n    observation_sources: laser\n    laser: {data_type: LaserScan, clearing: true, marking: true, topic: scan, inf_is_valid: true}\n    expected_update_rate: 0.07\n\ninflation:\n    inflation_radius: 0.05\nrange_sensor_layer:\n expected_update_rate: 0.125\n clear_threshold:     0.1\n mark_threshold:      0.1\n no_readings_timeout: 0.2\n topics: [\"/ultrasound1\"]\n", "track_unknown_space: true\nglobal_frame: map\nrolling_window: false\n\nplugins: \n- {name: external,            type: \"costmap_2d::StaticLayer\"}\n- {name: explore_boundary,    type: \"frontier_exploration::BoundedExploreLayer\"}\n#Can disable sensor layer if gmapping is fast enough to update scans\n- {name: obstacles_laser,     type: \"costmap_2d::ObstacleLayer\"}\n- {name: inflation,           type: \"costmap_2d::InflationLayer\"}\n\nexplore_boundary:\n  resize_to_boundary: false\n  frontier_travel_point: middle\n  #set to false for gmapping, true if re-exploring a known area\n  explore_clear_space: false\n", "global_frame: odom\nrolling_window: true\n\nplugins:\n  - {name: obstacles_laser,           type: \"costmap_2d::ObstacleLayer\"}\n  - {name: range_sensor_layer, type: \"range_sensor_layer::RangeSensorLayer\"}\n  - {name: inflation,                 type: \"costmap_2d::InflationLayer\"}\n", "global_frame: odom\nrolling_window: true\n\nplugins:\n  - {name: obstacles_laser,           type: \"costmap_2d::ObstacleLayer\"}\n  - {name: range_sensor_layer, type: \"range_sensor_layer::RangeSensorLayer\"}\n  - {name: inflation,                 type: \"costmap_2d::InflationLayer\"}\n", "recovery_behaviour_enabled: true\n\nNavfnROS:\n  allow_unknown: true # Specifies whether or not to allow navfn to create plans that traverse unknown space.\n  default_tolerance: 0.5 # A tolerance on the goal point for the planner.\n\nTrajectoryPlannerROS:\n  # Robot Configuration Parameters\n  acc_lim_x: 15.0\n  acc_lim_theta:  15.0\n\n  max_vel_x: 0.15\n  min_vel_x: 0.8\n\n  max_vel_theta: 0.5\n  min_vel_theta: -0.5\n  min_in_place_vel_theta: 0.7\n\n  holonomic_robot: false\n  escape_vel: -0.2\n\n  # Goal Tolerance Parameters\n  yaw_goal_tolerance: 0.2\n  xy_goal_tolerance: 0.3\n  latch_xy_goal_tolerance: false\n\n  # Forward Simulation Parameters\n  sim_time: 1.0\n  sim_granularity: 0.02\n  angular_sim_granularity: 0.02\n  vx_samples: 6\n  vtheta_samples: 20\n  penalize_negative_x: true\n  controller_frequency: 7.0\n\n  # Trajectory scoring parameters\n  meter_scoring: true # Whether the gdist_scale and pdist_scale parameters should assume that goal_distance and path_distance are expressed in units of meters or cells. Cells are assumed by default (false).\n\n  occdist_scale:  0.01 #The weighting for how much the controller should attempt to avoid obstacles. default was0.5\n\n  pdist_scale: 0.4  #     The weighting for how much the controller should stay close to the path it was given . default 0.6\n\n  gdist_scale: 0.8 #     The weighting for how much the controller should attempt to reach its local goal, also controls speed  default 0.8\n\n  heading_lookahead: 0.25  #How far to look ahead in meters when scoring different in-place-rotation trajectories\n\n  heading_scoring: true  #Whether to score based on the robot's heading to the path or its distance from the path. default false\n\n  heading_scoring_timestep: 0.8   #How far to look ahead in time in seconds along the simulated trajectory when using heading scoring (double, default: 0.8)\n\n  dwa: false ..."], "url": "https://answers.ros.org/question/218425/robot-rotates-before-or-as-it-reaches-goal/"},
{"title": "2D point to 3D transformation(using Kinect) programmed by openCV", "time": "2015-06-08 14:30:10 -0600", "post_content": [" ", " ", " ", " ", "enter code here I am working on something related to 2D point to 3D transformation, which is new to me. Could anyone provide me an outline about doing this? better with explanation. Thank you!"], "answer": [" ", " ", "I recommend taking a look at the ", " wiki page and learning how the ROS handles transforms.  You can also take a look at ", " to learn a bit more detail than the wiki page provides.  ", "Finally, you can take a look at Introduction to Autonomous Mobile Robots by Roland Siegwart and Illah R. Nourbakhsh.", " ", " ", " ", " ", "Can you be a little more specific? If you are already using the Kinect, then you already have a 3D point cloud.", "Edit:", "It's very easy to perform calibration in ROS, using the ", " package. The idea is that you match the VGA pixel to the DepthPoint of the Kinect IR (3D) camera. You can read more ", ".", "You can also perform extrinsic calibration, following ", " tutorial. In general, look at the ", " to understand how to proceed.", "I am doing object tracking. I use 2D because it is faster to run energy function compared to run it on 3D, but I want to have 3D as an initialization. Thank you.", "Well, since that's the case, then all you need is to perform ", ". That way you have the mapping of the 2D VGA camera of the Kinect with the 3D point cloud of the IR.", "How could I map the 2D point to 3D? There are lots of information that I need, for example principle point, focusing length, radial distortion and trangential. Are they same for all the Kinects? Thank you."], "question_code": ["I solved this problem.\n1. read rgb image and depth image from kinect approximately, using http://wiki.ros.org/message_filters/ApproximateTime.\n2. get the formula to calculate the value(x,y) from rgb image and z from depth image into 3D point.x, point.y, point.z\n"], "url": "https://answers.ros.org/question/210822/2d-point-to-3d-transformationusing-kinect-programmed-by-opencv/"},
{"title": "how can i simulate obstacle avoidance for serial manipulators using potential field method", "time": "2015-10-19 14:08:54 -0600", "post_content": [" ", " ", "i need to simulate obstacle avoidance in case o serial manipulators"], "answer": [" ", " ", "Start thinking that the spatial derivative (or gradient) of the potential field represents a force. By applying this force at the robot end-effector you can use operational (or cartesian) space control (see O.Khatib) or translating this force in joint torques by means of Jacobian transpose. ", "This paper is one of the milestones in the artificial potential field:", "Basicly, it explains how to model both attractive potential field (for the desired pose) and repulsive potential field (for the obstacles). This two potential field have always different signs. You can make use of quadratic function, e.g. ", "which is positive, continuous and differentiable function which attains zero when x=xc and where xc is the vector position center of the modelled obstacle or of the goal position. Once all objects have been modelled using this kind of functions you have simple to differentiate all the U obtaining respective forces F. This forces should be added together for calculating the overall force at the robot tip.", "If you have a simulator accepting joint torques as input, lets say V-REP, all that you have to do is multiply the resulting force for the Jacobian transpose matrix that is function of the joint positions.", "Obviously, there are several ways to model obstacles, using different functions, and several ways to keep the stability of the simulation. For further details have a look into the literature or don't hesitate to ask again!", "Good luck ;)", "thank u sir, for your response, can you suggest me a simulator that can demonstrate the same which can sensor values about the obstacle", "As far as I know, potential field, though very interesting method is quite old and led, sometimes, to unrealistic behaviour (when total potential reach a local minimum the robot can stall). Recent simulators use more sophisticated collision detection and avoidance algorithms, see V-REP for instance.", "sir, will you help in implementing the potential field in the simulator, or any related docs sp that i can follow it...", "It depends on what you want to do. As I stated above, new simulators make use of powerful calculation modules for path planning and obstacles avoidance. However, if you want implement your own method from scratch I suggest to use Matlab with Robotics tool and a simulator, like V-Rep, as visualizer."], "answer_code": ["U = \u00bd (x - xc)^2\n"], "url": "https://answers.ros.org/question/219421/how-can-i-simulate-obstacle-avoidance-for-serial-manipulators-using-potential-field-method/"},
{"title": "hydro installing on pandaboard es", "time": "2015-10-26 01:35:58 -0600", "post_content": [" ", " ", "hello\nI have installed ros-hydro-ros-base on the platform pandaboard es(the system is ubuntu 12.04 precise) following the instructions of official website. But some common tools packages such as rqt and rviz haven't been installed on the platform. It's difficult to build them from sources to install them on the platform .Because there are many dependencies which i have to install at the same time. \nso i want to know weather the ros-base is enough for the platform as a operating environment?\nI install a same distro hydro on computer and make computer as a development environment for example I deal with some sensors data and debug some projects on the computer then generate some executed files and I put these executed files on the platform pandaborad to execute them?\nCould I do as above? ", "i am confused by this question"], "answer": [" ", " ", "Yes, it is recommended to run things like rqt and rviz offboard from an embedded board like a Panda Board. The small embedded boards do not have much computational power to provide an interactive environment. ", "ros-base is probably not enough to run your robot, but most of the tools you need are likely available or at least do not need the large set of external dependencies. "], "url": "https://answers.ros.org/question/219863/hydro-installing-on-pandaboard-es/"},
{"title": "Control loop missed its desired rate", "time": "2015-09-18 02:38:12 -0600", "post_content": [" ", " ", " ", " ", "I tried to run move_base navigation to a goal on Odroid C1+ board.\nBut the result is bad, the robot didn't move smoothly.\nHere is the result screen recording:", "And this is my move_base parameter files:", "The strange thing is, if I run the same code on my desktop PC. The result is pretty smooth.", "I found the difference is, on my PC, the control loop frequency can reach to 2.5Hz as specified, but it can only reach to ~2.2Hz on Odroid C1+.  I even tried the same test on Raspberry Pi 2, the result is even worse...  ", "Any idea to make it better??", "I also tried XU3, the control loop rate can reach >=3 and the result is much better."], "answer": [" ", " ", "The problem is with the cpu and the processing power. As ", " said I do not think it is related with move_base.", "Maybe you should try lowering the costmap resolution, and/or costmap sizes etc. Therefore your navigation stack will require less processing power and memory. However your navigation performance may degrade as a result. ", "Thanks for your reply. I tried modify the (both global & local) costmap resolution to 1.0 and local map width x height to 1.0x1.0.  The control rate did slightly increased to 2.45... but it's still not good enough. Any more recommendation??", "You may try increasing sim_time, sim_granularity and angular_sim_granularity a bit.", "BTW, I found the system is not very busy in overall. It's a quad-cores CPU and only one core is almost 100% when running move_base.", "Yes, I tried increasing sim_time=1, sim_granularity=0.5 and angular_sim_granularity=0.5, the result is still not god.  Now I'm afraid that Odroid C1+ is just not powerful enough for move_base....", " ", " ", "Usually control loop missing its desired means that something is taking longer that you anticipated.  I have no experience with Odroid C1+, how do the specs compare your PC? I think the issue has nothing to do with ", " in this case"], "answer_code": ["move_base"], "url": "https://answers.ros.org/question/217842/control-loop-missed-its-desired-rate/"},
{"title": "Battery Usage Simulation in Gazebo", "time": "2015-10-06 09:39:44 -0600", "post_content": [" ", " ", "Hi All", "What is best practice in simulating a robot's battery usage in gazebo?", "I have a robot, and therefore a general \"model\" plugin for it.  Should I be crating a sensor plugin to simulate battery usage? OR Should I be creating a model plugin for the same robot?  Can a robot have multiple model plugins?", "The goal is to have this information be generated and output to battery status topics to the ROS world.", "It would be nice to have the code reusable for other robots. ", "Thanks", "Hello GGabria,\ni'm searching the same in python. Did you find it?"], "answer": [" ", " ", "This is a Gazebo plugin that simulates an open-circuit battery model. This is a fairly extensible and reusable battery plugin for any kind of Gazebo compatible robots: "], "answer_code": ["https://github.com/pooyanjamshidi/brass_gazebo_battery"], "url": "https://answers.ros.org/question/218715/battery-usage-simulation-in-gazebo/"},
{"title": "What should I expect from Turtlebot calibration? / How do I know if my Turtlebot is broken?", "time": "2014-09-14 12:45:28 -0600", "post_content": [" ", " ", " ", " ", "I'm having a hard time debugging my Turtlebot because I don't know what a functioning Turtlebot should look like. Now that I've read the source code for ", " I have some better understanding of what to expect, but I'm still not sure if my bot is mechanically broken.", " Here's a video showing my bot doing calibration (I know it over-turns right now so the numbers aren't correct, I just want to show you how it stops for long periods of time making weird noises, e.g. t=0:50):  ", " We can collaboratively put together an answer here.", "\nSeriously? No one is going to even give me a ", " of whether they think my video looks normal or not??", "If you are following this question then you might also want to upvote it.", "Not the answer you're looking for but you can always calibrate the Turtlebot manually. Oh and it took me around 5 minutes to auto calibrate mine and the results are bad."], "answer": [" ", " ", "The CREATE base on the earlier versions of turtlebot has some drawbacks when it comes to hardware (IMU and Battery). If the Ni-Mh battery is old and used over many charging cycles, it will tend to create power fluctuations in your power distribution board. You will need to do a power reset (switch off, unplug cables from turtlebot, remove battery, press and hold power button for a few seconds, connect battery, plug in cables and then turn on). ", "Moreover, the IMU used is a cheap odometry estimation gyro, which is not reliable for mapping complicated environments (few features, many turns). The calibration process done on a clean power supply can still give you values close to each other, if done multiple times, however, it does not converge to a precise particular value for which the delta yaw (heading error) is close to zero.", "In the calibration process, the turtlebot is supposed to rotate 720 degrees first, followed by three 360 degree rotations at varying speeds. It is supposed to point at the wall, or the configuration at which you started the calibration. If this doesn't happen, try changing your odom parameters using rqt_reconfigure. ", "There is no documented calibration routine, hence it is hard to guess how the gyro_correction factor and the odom_angular_correction factor change the calibration routine.", "If you are facing trouble over a long time, you can do the following:\n1. Check pin out at the power distribution. Between pins 8 and 14, a steady 5V DC should be present when powered on.\n2. Change battery, or get an alternative LiPo power supply (4 cell battery with higher mAh would do)\n3. Use a custom IMU\n4. Get a kobuki base. It is fairly accurate and factory calibrated.", " ", " ", "Maybe you could restart the robot. Sometimes I have this problem, but when I restart my turtlebot, it's gone. It's nomal If your turtlebot turn 720\u00b0 continuously in a routine, where each routine begins with \"Aligning base with wall\" and ends with IMU and odom error output.", " ", " ", "Quoting ", ":", "Odometry is expected to be accurate within 1% after calibration in both linear and angular movement.", "So dead reckoning should be very good indeed. This doesn't answer all the questions posed, but it does give a target to shoot for. I never tried manually calibrating the gyro so I can't be sure that my Turtlebot actually had a hardware issue, but I do imagine it should have been much better than that, even with a suboptimal calibration."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Is it normal for Tbot to stop for long periods of time during the calibration routine?", "Is it normal for calibration to take 7-10 minutes, or more?", "What are some examples of reasonable values for ", " and ", " (for a ", " of 150)? Is a gyro value of 0.3 completely unreasonable? What about 3.4? Should it be much closer to the default?", "What is a normal error range for odom and IMU? Even on a good run I might see an odom error of >10% and IMU error >40%.", "Is it normal for the bot to drift out of place during calibration? (i.e. it doesn't turn in place perfectly, but drifts a few centimeters over the course of the routine; one wheel must be spinning faster than the other, I guess)", "Also, what kind of error is acceptable overall (from the EKF's odom_combined) to be able to run gmapping? If I rotate 360\u00b0 while watching the output of ", ", should the yaw delta be within 1-2\u00b0? Or can I accept errors of more like 10-20\u00b0 and still be able to run SLAM?"], "question_code": ["turtlebot_calibration", "gyro_scale_correction", "odom_angular_scale_correction", "gyro_measurement_range", "rosrun tf_echo odom base_footprint"], "url": "https://answers.ros.org/question/192672/what-should-i-expect-from-turtlebot-calibration-how-do-i-know-if-my-turtlebot-is-broken/"},
{"title": "robot_localization: erroneous filtered GPS output", "time": "2015-10-04 21:34:28 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am quite new to the robot_localization package and am facing a number of difficulties in using it. I am currently trying to fuse data taken from an Android device's GPS and IMU using this node. To achieve this, I have extracted the GPS and IMU log data and have read it into a bag file, which I then play back to the ekf_localization_node and navsat_transform_node to try to fuse the data.", "I have a few difficulties. My GPS doesn't have altitude information, so I estimated the altitude at around 240 and set the field to that value. I notice that while the altitude stays quite constant for maybe 15-20 seconds, its value starts exploding and becoming erratic soon after.", "Also, I have input a value of around -79 degrees in the longitude field. However, the filtered GPS topic inexplicably maintains the longitude field in the range of 179-180 degrees instead. I'm not sure what's going on here, and have attached the launch file I am using as well as the bag file I am reading from below. ", "Would greatly appreciate some feedback on what I am currently doing wrong! Thanks in advance!", " Bagfile:  ", " Launch file:  ", "UPDATE: I have downloaded and tried out the latest version of robot_localization, and it does indeed resolve all issues with the GPS output from the navsat_transform node.", "I have also tried to follow up on Porti's suggestion that my frames are not correct, but as far as I can tell this is not the case. I am using the IMU on an Android-powered camera, and have followed the developer's API to process the data coming from the camera. As far as I can tell, I am taking the accelerometer values and inputting them so that x is forward (direction of motion of base_link), y is to the left when facing forward, and z is upward. I understand this to be the standard required by REP 103.", "I have also tried exchanging the x and y axes and attempting to zero out any effects of gravity (from the possibility that the camera is skewed). However, all these attempts yield only paths similar to what Porti has plotted. I have enclosed MATLAB graphs of the path output by the node for the various cases listed above. All these graphs have the same jagged, discontinuous shapes as Porti's graph for IMU data. Would anyone be able to suggest any possible causes for this phenomenon?", "Thank you in advance for your help!", " MATLAB graphs:\n ", " ", " ", "I'll have to look at the bag files. Your IMU does have a compass, correct? It could be trouble with acceleration biases."], "answer": [" ", " ", "Hi jll,", "The issue with the longitude being way off was actually a regression that I've just fixed. Would you mind grabbing the latest source and trying again? Thanks!", " ", " ", " ", " ", "The problems are in the data of imu, you are fusing this data of sensor with gps, you need know if the imu data are corrected, I paint your rute and have the next ", " green is the path filtered and rose is the position gps, i created a tools for visualization. \nThe imu has follow the rep 103, try if the frame of imu is corrected, launch only the imu and test it. The x axis of the base_link frame should point in the direction of advance of the robot. The angular_velocity and lineal acceleration of imu not is consistent with vehicle movement", "I changed your .launch", "hi, dear ", " , how to make a tool to visual the gps position? can you share with me? thank you very much.", " Hi, i did a plugin for rviz which can plot a Odometry like a path, so you plot odometry/gps\nDownload this  ", " , compile in your workspace and then you can plot this msg in rviz. If you was think in plot the GPS (NavSatFix)  you have to convert it "], "answer_code": ["<launch>\n\n    <node pkg=\"rosbag\" type=\"play\" name=\"rosbag_play\" output=\"screen\" args=\"--clock /home/jorge_j/catkin_ulises/src/pruebas/covar_2015-10-05-10-02-06.bag -d 3\"/>\n    <!-- <node pkg=\"rosbag\" type=\"play\" name=\"rosbag_play\" output=\"screen\" args=\"clock /home/Downloads/filtered.bag -d 3\"/> -->\n    <node pkg=\"tf2_ros\" type=\"static_transform_publisher\" name=\"bl_imu\" args=\"0 0 0 0 0 0 1 base_link imu_link\" />\n\n\n <param name=\"odom0_queue_size\" value=\"10\"/> -->\n\n    <!-- Global (map) instance -->\n    <node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization_global\" clear_params=\"true\">\n      <param name=\"frequency\" value=\"30\"/>\n      <param name=\"sensor_timeout\" value=\"0.1\"/>\n      <param name=\"two_d_mode\" value=\"true\"/>\n\n      <param name=\"map_frame\" value=\"map\"/>\n      <param name=\"odom_frame\" value=\"odom\"/>\n      <param name=\"base_link_frame\" value=\"base_link\"/>\n      <param name=\"world_frame\" value=\"odom\"/>\n\n      <param name=\"transform_time_offset\" value=\"0.0\"/>\n\n      <!-- <param name=\"odom0\" value=\"/jackal_velocity_controller/odom\"/> -->\n      <param name=\"odom0\" value=\"/odometry/gps\"/>\n      <param name=\"imu0\" value=\"/imu/data\"/>\n\n      <!-- <rosparam param=\"odom0_config\">[false, false, false,\n                                      false, false, false,\n                                      true, true, true,\n                                      false, false, true,\n                                      false, false, false]</rosparam> -->\n\n      <rosparam param=\"odom0_config\">[true, true, false,\n                                      false, false, false,\n                                      false, false, false,\n                                      false, false, false,\n                                      false, false, false]</rosparam>\n\n      <rosparam param=\"imu0_config\">[false, false, false,\n                                     true,  true,  true,\n                                     false, false, false,\n                                     true,  true,  true,\n                                     true,  true,  true]</rosparam>\n\n      <!-- <param name=\"odom0_differential\" value=\"false\"/> -->\n      <param name=\"odom0_differential\" value=\"false\"/>\n      <param name=\"imu0_differential\" value=\"false\"/>\n\n      <!-- <param name=\"odom0_relative\" value=\"false\"/> -->\n      <param name=\"odom0_relative\" value=\"false\"/>\n      <param name=\"imu0_relative\" value=\"false\"/>\n\n      <param name=\"imu0_remove_gravitational_acceleration\" value=\"true\"/>\n      <param name=\"print_diagnostics\" value=\"true\"/>\n\n      <!-- <param name=\"odom0_queue_size\" value=\"10\"/> -->\n      <param name=\"odom0_queue_size\" value=\"10\"/>\n      <param name=\"imu0_queue_size\" value=\"10\"/>\n\n      <param name=\"debug\"           value=\"false\"/>\n      <param name=\"debug_out_file\"  value=\"debug_ekf_localization.txt\"/>\n\n      <remap from=\"/odometry/filtered\" to=\"/odometry/filtered/global\"/>\n   </node>\n\n   <!-- navsat_transform -->\n   <node pkg=\"robot_localization\" type=\"navsat_transform_node\" name=\"navsat_transform\" respawn=\"true\" output=\"screen\">\n      <param name=\"frequency\" value=\"30\"/>\n      <param name=\"delay\" value=\"3\"/>\n      <param name=\"magnetic_declination_radians\" value=\"0.190240888\"/>\n      <!-- <param name=\"magnetic_declination_radians\" value=\"-0.161617489\"/> -->\n      <!-- <param name=\"yaw_offset\" value=\"1.570796327\"/> -->\n      <param name=\"yaw_offset\" value=\"1.570796327\"/>\n      <param name=\"zero_altitude\" value=\"false\"/>\n      <param name=\"broadcast_utm_transform\" value=\"true\"/>\n      <param name=\"publish_filtered_gps\" value=\"true\"/>\n      <param name=\"use_odometry_yaw\" value=\"true\"/>\n      <param name=\"wait_for_datum\" value=\"false\"/>\n\n      <remap from=\"/odometry/filtered\" to=\"/odometry/filtered/global\"/>\n      <!-- <remap from=\"/gps/fix\" to=\"/navsat/fix\"/> -->\n    </node>\n\n</launch>\n"], "url": "https://answers.ros.org/question/218626/robot_localization-erroneous-filtered-gps-output/"},
{"title": "turtlebot bad gyro calibration", "time": "2015-11-23 03:28:36 -0600", "post_content": [" ", " ", " ", " ", "rostopic echo  /diagnostics :", "header: \n  seq: 2238\n  stamp: \n    secs: 1448269540\n    nsecs: 681997060\n  frame_id: ''\nstatus: "], "answer": [" ", " ", "Remove the power distribution circuit board from your CREATE Base. On a full battery charge, ensure that there is a constant 5V power supply between pins 8 and 14, as shown in the photo.", "If there isn't you'll have to use an external gyro or maybe change the base.", "I'm using iRobot Create 2 as the base. I see there is an encoder near the wheel. Furthermore, I noticed that \"iRobot_Roomba_600_Open_Interface_Spec_0908.pdf\" shows us Roomba Open Interface Sensor Packets, and we can read the distance and angle from the encoder. The 'gyro' in your message means that?"], "question_code": ["level: 0\nname: TurtleBot Node\nmessage: RUNNING\nhardware_id: ''\nvalues: []\n", "level: 0\nname: Operating Mode\nmessage: Full\nhardware_id: ''\nvalues: []\n", "level: 0\nname: Battery\nmessage: OK\nhardware_id: ''\nvalues: \n  - \n    key: Voltage (V)\n    value: 15.613\n  - \n    key: Current (A)\n    value: -0.344\n  - \n    key: Temperature (C)\n    value: 32\n  - \n    key: Charge (Ah)\n    value: 2.297\n  - \n    key: Capacity (Ah)\n    value: 2.696\n", "level: 0\nname: Charging Sources\nmessage: None\nhardware_id: ''\nvalues: []\n", "level: 0\nname: Cliff Sensor\nmessage: OK\nhardware_id: ''\nvalues: \n  - \n    key: Left\n    value: False\n  - \n    key: Left Signal\n    value: 2475\n  - \n    key: Front Left\n    value: False\n  - \n    key: Front Left Signal\n    value: 2561\n  - \n    key: Front Right\n    value: False\n  - \n    key: Front Right Signal\n    value: 2606\n  - \n    key: Right\n    value: False\n  - \n    key: Right Signal\n    value: 2609\n", "level: 0\nname: Wall Sensor\nmessage: OK\nhardware_id: ''\nvalues: \n  - \n    key: Wall\n    value: False\n  - \n    key: Wall Signal\n    value: 61\n  - \n    key: Virtual Wall\n    value: False\n", "level: 2\nname: Gyro Sensor\nmessage: Gyro Power Problem: For more information visit http://answers.ros.org/question/2091/turtlebot-bad-gyro-calibration-also.\nhardware_id: ''\nvalues: \n  - \n    key: Gyro Enabled\n    value: True\n  - \n    key: Raw Gyro Rate\n    value: 0\n  - \n    key: Calibration Offset\n    value: 0\n  - \n    key: Calibration Buffer\n    value: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ..."], "url": "https://answers.ros.org/question/221362/turtlebot-bad-gyro-calibration/"},
{"title": "Mini mobile robot with mini PC", "time": "2015-12-30 06:41:38 -0600", "post_content": [" ", " ", "I want to make a mini mobile robot with a mini PC which can run ubuntu.", "Is there any opensource hardware product?"], "answer": [" ", " ", "I think a convenient solution is using a Raspberry Pi 2 for your robot. ROS can now be installed using Ubuntu 14.04 for ARM (see ", " and ", ").", "Using this approach, the amount of fiddling with toolchains, compilation etc. is fairly minimal. The system is very lightweight and small and can for instance be powered conveniently via a powerbank during experiments.", "Thank you so much. Could you recommend a mini mobile robot platform?\nI want to use Arduino Robot, but the product is end-of-life.", "I'm not quite sure what you want to achieve, but you can get a \"2WD arduino robot\" chassis (google it) starting at about ~15 US$. You can also order a Arduino clone (~10 US$) and motor shield clone (~10 US$) from China. This is about the cheapest option that comes to mind :)", "Thank you so much. I want SLAM and navigation using RP-Lidar. A good wheel encoder is a must, which will be used for localization(amcl) although mapping algorithms, such as hector_slam, do not need it."], "url": "https://answers.ros.org/question/223269/mini-mobile-robot-with-mini-pc/"},
{"title": "problem with running rgbdslamV2", "time": "2015-11-23 04:43:44 -0600", "post_content": [" ", " ", "My computer configuration: a quad-core CPU with 4GB of memory.", "When I run rgbdslamV2,  Memory usage reaches 100%\u3002", "How can I do to reduce memory usage? Or I must extend memory\uff1f", "I suppose rgdbslam is not very memory efficient. In my experiments I switched from 16 to 32 GB.", "Thank you for your answer!"], "answer": [" ", " ", "Hope it's not too late for some tuning advise. If you don't need the point cloud map increase the ", " to a higher power of two. If you do not have fast motions increase ", ".\nYou could also experiment with the options ", " and ", ", but I suspect that they don't work anymore."], "answer_code": ["cloud_creation_skip_step", "data_skip_step", "create_cloud_every_nth_node", "store_pointclouds"], "url": "https://answers.ros.org/question/221371/problem-with-running-rgbdslamv2/"},
{"title": "7-8\" Tablet for Turtlebot?", "time": "2015-10-26 23:42:25 -0600", "post_content": [" ", " ", "I'm looking to replace the bulky laptop on my roomba-based Turtlebot with a tablet that has a full-size USB port, so that I can do telepresence audio/video with the same machine that does all the mapping & bot/ROS logic. I've been fiddling with a TW700 that I got really cheap, and it's ", " working, but it has a lot of landmines. Oh, and I might have just bricked it.", "Any suggestions for tablets that will run Ubuntu, with good device support, and have a full USB port? "], "answer": [" ", " ", "I'm in the same boat as you--we need a lightweight tablet that can run Ubuntu 14.04.3 and ROS indigo and preferably a full-size USB port.", "After doing a lot of research I think we have narrowed it down to used Microsoft Surface Pro 1 or 2 tablets or Dell's low-quality 2-in-1 Inspiron 11 3157 (that has terrific battery life and is Ubuntu-certified).", "If you are willing to step up from 7-8\" to 10\" or 11.6\", those may work for you. The Surface Pro 1 is actually fairly small. There is some word that the Dell Venue 8 Pro can run Ubuntu but I have not found the full foolproof instructions to do so.", " ", " ", "Another feasible approach would be using a small single board computer (SBC) as the main robot controller (using the Raspberry Pi2 with ROS for instance is pretty painless by now). The footprint of such SBCs is very small and you are then free to either add a display to the SBC directly, or via Ethernet connect a tablet/small netbook/whatever with whatever OS suits you (for instance having a UI running in a ", "). The advantage would be that you could avoid experiments with installing Ubuntu/ROS on a tablet this way."], "url": "https://answers.ros.org/question/219948/7-8-tablet-for-turtlebot/"},
{"title": "on board kinect use for navigation", "time": "2015-09-10 20:27:28 -0600", "post_content": [" ", " ", "Hi, the kinect for xbox one or kinect for windows 2, which one is better for uav navigation? I know the Kinect for windows is discontinued right now. I want to use odroid xu4 as onboard computer to connect kinect. \nThanks"], "answer": [" ", " ", "I am using kinect 2 for windows. According to ", " and ", " , I can process data successfully in odroid xu4 board.", "How many FPS does it produce? Just I read for 30 FPS Kinect need very powerfull board.", " ", " ", " ", " ", "The most important difference between Kinect for XBOX One and Kinect for Windows 2 is the USB3 plug. I bought my Kinect for XBox One for just 30\u20ac. I needed one afternoon to mount a USB3 connector (and another connector for 12V), so I saved 170\u20ac.", " But I think you should not use either of the two. The first problem will be to handle the grater amount of data (USB3!), than the power supply ( ", " ). This can bee solved. ", " But the new Kinect suffers from noise and reflections. See my post at PCL-users for more details: \n ", "Handling the noise is possible. A simple solution is to do not move for 1sec, take 30 images and compute the average. Reflections can be maybe \"solved\" by restrictions to the environment (and the materials). But for example in the laboratory in the university the new Kinect is entirely useless for navigation at the moment, because of the flooring, the wall paint and some furnishing.", "For robot navigation I'm using still the old Kinect. It is very challenging to solve this problems (An experienced researcher can spend months and more.) and the bad resolution of the old Kinect is still enough.", "really appreciate for your reply. any ideas you have or you saw before to solve this kind problems. Thanks, Michaelkorn", "Like I wrote \"An experienced researcher can spend months and more.\". But with odroid xu4 and the new Kinect you will have enough problems to just grab the data. There are no reserves for processing. Which algorithm do you want to use for navigation?", "3D SLAM algorithm, thanks."], "url": "https://answers.ros.org/question/217453/on-board-kinect-use-for-navigation/"},
{"title": "move_base acting weird with nodes running on different hosts.", "time": "2016-02-08 04:56:04 -0600", "post_content": [" ", " ", " ", " ", "Hello All,", "Using examples from \"ROS by example\" to get move_base working, the trajectory is erratic when the code, apart from rviz, is running on ANOTHER host. It gets there in the end, but is moves very strange, as if it is very \"drunk\"...", "The commands I use are:", "When all commands are run on my regular PC (PC, i5, ubuntu 14.04, indigo), all is well.\nBut when the  2 roslaunch commands are running on another host the strange paths occur.", "That other host is a quad core i.MX 6Quad from Freescale, 1GHZ, 1GB, running ubuntu 14.04 with indigo.\nThere is no swapping, and nothing else is running there.", "Communication is via a 5GHz Wifi link that can sustain 15MB/sec.", "I would assume that for this simple setup the performance of the arm-system and wifi-link is not the problem.\nWhat could be the cause of this, and how can I proceed in nailing this down further?", "EDIT:", "roswtf gives the SAME output in both situations. It does give the following error:", "but that probably is not relevant now.", "move_base often does not start properly, the initialization normally ends with \"odom received!\", but sometimes it takes a long time (20 seconds) for that last line to appear.\nIt also sometimes does not come at all. I already asked about that in my question \"remote core with move group\".\nWhen I stop the hung program it complains about: Failed to contact master.\nBut it uses it all the time....", "When I use the move_base parameters that come with this example, I get the following warnings when the controller and move_base are running on the arm-board.", "I find it strange that calculation one loop takes that much time, given that both global and local maps are empty.\nAm i missing something?", "If I lower the rate to 1 Hz it basically works, but it has hard time following the track, and most of the time it looks like an highly unstable system. After changing some more parameters I can get it  better, but only a little bit: not usable.", "If move_base now only gives a single /cmd_vel per second, I can imaging that it is difficult to keep it on track.\nWouldn't it be better if move base gave a complete trajectory to the final goal, and update that each second?\nThen the rates of move_base and execution the trajectory could be decoupled.", "In summary, two questions. Why does the \"odom received!\" sometimes not come or after a long delay?\nAnd ...", "Communication is via a 5GHz Wifi link that can sustain 15GB/sec.", "Are you sure that is correct? That is 15 Gigabyte per second. What kind of technology are you using?", "Oops. 15MB/sec of course, Changed it.", "do you get any useful feedback when you run roswtf ?", "I see no differences, please see the edit in my question."], "answer": [" ", " ", " ", " ", "I've figured it out. The control loop on the arm-board takes about 0.7 seconds, so you have to set the rate accordingly.\nIf you do that it just works, which will include the weirdness.\nThis is due to the design of move_base in that it only sets a new /cmd_vel after each loop.\nI now realise that this is exacly what I see.", "So the question is basically answered, but I want a better behaviour of course.\nI see several possibilities.", "First, and that is what I will use as a workaround, is running move_base on the main PC.\nAnd this works perfectly.\nBut because we anticipate to have 10 of these robots running around, I really want the code to run on the arm-board. ", "Getting a more powerful computer on the robot would be defeat, especially because my intuition says that it should be possible with the current processor.\nAnd a more powerful computer would need more power etc.", "So in the end I probably need to make move_base faster.\nAs I already wrote in my question a possibility would be to let move_base deliver the complete path, instead of /cmd_vel messages.\nThen a separate node could use these paths to create /cmd_vel messages at a higher rate.\nThe complete path is already available, as seen in rviz.\nAt some future point I will design this, or does something like this already exists?", "Thanks for the help, Sietse", "How did you install ROS on the i.MX? Was that a from-source install? If so, did you build everything with optimisations turned on?", "I used the regular ROS repository for the arm architecture. Anything to gain by a source install?", "For the ubuntu install I used the recipe from ", "Anything to gain by a source install?", "No. The release binaries are already built with optimisations turned on. It's just that ppl sometimes forget to enable them when doing a from-source installation.", " ", " ", "Here are a few typical checklist items for a multiple machine setup:", "Is the network setup correct (i.e.\ndoes communication work\nbidirectionally)? If it is wrong, it\nsometimes is possible to list topics\non a machine for instance, but no\ndata can be received when\nsubscribing.", "Is there a timesync offset? If clocks\nbetween machines are not synced all\nsorts of weird things can happen due\nto components waiting for tf much\nlonger than they should (or failing\ncompletely).", "Is CPU consumption on one of the\nmachines much too high (A low power\nARM CPU could be overburdened and\nunable to keep up)?", "Is bandwidth good enough so\nall data can be transfered (If the\ntransmitted bandwidth is close to\nmaximum there's a pretty good chance\nof comms dropouts).", "Also, I think you refer solely to move_base here? If so, you should probably edit your question to replace all references to \"move_group\" (which is part of MoveIt! arm motion planning and unrelated to move_base).", "Thanks. Network, time and bandwidth are all ok I think. Only CPU load is a bit high.The core that does the move_base hovers around 80% when running. But that probably doesn't explain why the initialization of the move_base command often takes so long or doesn't finish"], "question_code": ["  roscore\n  roslaunch rbx1_bringup fake_turtlebot.launch\n  roslaunch rbx1_nav fake_move_base_blank_map.launch\n  rosrun rviz rviz -d `rospack find rbx1_nav`/nav.rviz\n", "ERROR The following nodes should be connected but aren't:\n * /move_base->/move_base (/move_base/global_costmap/footprint)\n * /move_base->/move_base (/move_base/local_costmap/footprint)\n", "[ WARN] [1455012218.435531811]: Map update loop missed its desired rate of 3.0000Hz... the loop actually took 1.0281 seconds\n[ WARN] [1455012218.953750145]: Control loop missed its desired rate of 3.0000Hz... the loop actually took 0.5194 seconds\n"], "url": "https://answers.ros.org/question/226036/move_base-acting-weird-with-nodes-running-on-different-hosts/"},
{"title": "slam gmapping real time on rviz", "time": "2016-01-25 20:56:35 -0600", "post_content": [" ", " ", "Hi all,", "I am doing slam gmapping on my robot. i run rviz so that I can visualise the map real time. However, the map seems to take a long time before appearing. ", "rostopic echo /scan provides data quickly. \nrostopic echo /map_metadata about 40seconds before data is being churned.\nrostopic echo /map  gives a huge array of -1", "since map_metadata has a delay 40 seconds, i am guessing this is the cause for the delay in the visualisation of map in rviz. Is there a parameter where i can tune to fasten map output?", "Thank you."], "answer": [" ", " ", " ", " ", "Check out ", " (maybe you want to have a look at wiki pages before asking questions about parameters) and the parameter ", " which is by default set to 5.0.", "Note that the visualization(!) of the map is actually not when scans are integrated, but only when a new gridmap is assembled and sent out over the topic. Also note, that the robot needs to move for scans to be integrated.", "Well, if the maps are distorted and it still takes a long time, you probably have some problem with the mapping process itself. Maybe also not enough processing power?\nParameters that might lower computational requirements: ", ", ", ", ", ", ", ", ", ".", "However, note that some of those parameters might result in even worse maps. ", "Hi mig, thank you for your answers. I know of the parameter map_update_interval and have already lowered it to 1.0. But the processing still takes a long time and the subsequent maps produced are always distorted. Hence i thought that there may be some other parameters that I can tweak."], "answer_code": ["map_update_interval", "throttle_scans", "maxUrange", "linear/angularUpdate", "temporalUpdate", "particles"], "url": "https://answers.ros.org/question/225048/slam-gmapping-real-time-on-rviz/"},
{"title": "ros servo control arduino fails", "time": "2016-01-31 15:51:34 -0600", "post_content": [" ", " ", "I want to get the control of a servo under ROS, for that I am using arduino UNO, following this ", ".\nWhen I run the rosserial_python serial_node.py at the moment when I publish the angle I got this error:", "Could anyone help me please? I don't understand why this is happening", "does your user have permission to access the serial port?", "did you try specifying the port to use? ", "rosrun rosserial_python serial_node.py /dev/", "or", "rosrun rosserial_python serial_node.py /dev/", "Yes @gary-servin I changed the permission with chmod and at the moment to run the rosserial_python serial_node.py I gave the port parameter (in my case /dev/ttyACM0). At the beginning it looks like works, but when the servo begin to move the node crashes.", "can you try disconnecting the servos and running the serial node again? If it works without the servo, then it should be a problem related with the power supply, the regulator on the arduino can't provide enough current to the servo and it resets the arduino when you try to move the motor", "Many thanks that was the problem there isn't enough current. With an external power supply it works fine"], "answer": [" ", " ", "The problem as @gary-servin said below it was for insufficient current. That solves with an external power supply.", "external power supply for arduino or motor?", "I had the same problem and it was an excess of demanded current from the servo, thanks for the solution"], "question_code": ["Traceback (most recent call last):\n\n\n File \"/home/jcardenasc93/catkin_ws/src/rosserial/rosserial_python/nodes/serial_node.py\", line 82, in <module>\n    client.run()\n\n\n\nFile \"/home/jcardenasc93/catkin_ws/src/rosserial/rosserial_python/src/rosserial_python/SerialClient.py\", line 495, in run\n    self.requestTopics()\n  File \"/home/jcardenasc93/catkin_ws/src/rosserial/rosserial_python/src/rosserial_python/SerialClient.py\", line 392, in requestTopics\n    self.port.flushInput()\n  File \"/usr/lib/python2.7/dist-packages/serial/serialposix.py\", line 500, in flushInput\n    termios.tcflush(self.fd, TERMIOS.TCIFLUSH)\ntermios.error: (5, 'Input/output error')\n"], "url": "https://answers.ros.org/question/225490/ros-servo-control-arduino-fails/"},
{"title": "How to solve Segmentation fault (core dumped) when rviz on raspberry pi 2?", "time": "2016-01-07 09:18:56 -0600", "post_content": [" ", " ", " ", " ", "As title , how could I solve segmentation fault when rviz?", "My raspberry pi 2 img is using ubuntu 14.04 with ros indigo.", " I tried to rmove ros-indigo-ros-model and git clone from github to my catkin workspace.\n( ", " ) ", "Here is my steps.", "But it just works on tegra tk1 not on raspberry pi 2.", "Thanks,\nAlyson"], "answer": [" ", " ", " ", " ", " From  ", "\"Using RVIZ", "It is not recommended to run rviz on most ARM-based CPUs. They're generally too slow, and the version of OpenGL that is provided by the software (mesa) libraries it not new enough to start rviz.", "'IF' you have a powerful board with a GPU and vendor-supplied OpenGL libraries, it might be possible to run rviz. The IFC6410 and the NVidia Jetson TK1 are two such boards where rviz will run, although neither is fast enough for graphics-heavy tasks such as displaying pointclouds.", "Note that rviz will segfault if you have the GTK_IM_MODULE environment variable set, so it's best to unset it in your ~/.bashrc:\"", "Do you mean raspberry pi 2  can't use rviz?\nbut github group shows this problem has been solved recently.", "I just don't know how to do and what are the steps to solve it. :(", "The page linked to has instruction, missed off copying it ", "\nadd ", " to your .bashrc file\nYou may be able to get it to run, but performance is likely to be poor, as detailed in that quote - tegra is much more powerful, especially for graphics with better openGL implementation", "I do add ", " to ~/.bashrc but it doesn't work.\n ", "I would suspect that ubuntu on the pi has less optimisation for the graphics of the board compared with raspbian. You might get rviz to run on that, but i think it will be disappointing performance even if you do get it to launch", "YingHua: You found the fixed bug--did you apply one of the two fixes? Either you upgrade libpcre3 or you upgrade collada-dom to the versions containing the fixes. It should fix the segmentation fault right away.", " ok!I will try to upgrade collada-dom. I tried to upgrade libpcre3,it still shows segmentation fault.", "Upgrading libpcre3 to 8.35 definitely fixed the segmentation fault for me. You should not have to upgrade collada-dom if you did upgrade libpcre3 to high enough a version. It is an \"either or\" fix.", " ", " ", "I met this question. The reason is that I used the version of ros-indigo-base. When I use the ros-indigo-desktop instead of it, I can use the rviz. "], "question_code": ["sudo apt-get remove ros-indigo-robot-model \ncd ~/catkin_ws/src \ngit clone https://github.com/ros/robot_model.git \ncd ~/catkin_ws\ncatkin_make \nsource devel/setup.bash\nrospack profile\n"], "answer_code": ["unset GTK_IM_MODULE", "unset GTK_IM_MODULE"], "url": "https://answers.ros.org/question/223637/how-to-solve-segmentation-fault-core-dumped-when-rviz-on-raspberry-pi-2/"},
{"title": "catkin version of the hector_slam package: mapping issue", "time": "2016-02-01 10:03:21 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I am new to ROS. I have been trying to work with ", " tutorial for logged data and I noticed that the ", " version of the ", " does not give good mapping results as compared to the ", " installation using package manager (", "). There is a significant difference in the scan matching .I am using the default ", " given in the github without any change. The trajectory of the robot is also different, which I believe is due to the scan mismatch. ", "Am I missing something here or is this a known issue? I also tried replacing the ", " executable in the ", " directory from the ", " and it works fine.", "To build the catkin package I followed:", "I also tried the ", " method described ", ". But the result does not improve. I dont have enough points to attach the images.", "update: I am using Dual core CPU @2.60GHz with 4 GB RAM with 60 GB disk space (dual boot with windows). OS: 64 bit Ubuntu 14.04.I had the same issue with other machine with i7 processor 4GB RAM(dual boot). I am building without optimization (catkin_make).", "Thanks in advance,"], "answer": [" ", " ", "What kind of machine are you using? If it has low CPU power (such as a Raspberry Pi2) and you accidentally build without optimization, this might explain the issues observed. If you could edit your question with additional information that would be helpful.", "For reference, with optimization on the tutorial ran with 10% CPU consumption on  a single CPU on a RPi2.", "To build with optimization, run", "I tried building it with optimization on  and it worked well. The CPU consumption was around 50- 60 % with rviz consuming 30-35 % of the CPU and hector_mapping(5%-10%). In case of Debug build the CPU consumptions remains at 100%", "If the answer was correct, please mark it solved to indicate that no further action is required to others."], "question_details": [" ", " ", "clone the ", " catkin stack from ", " into ", " folder", "Source the setup file in the workspace\u00b4s ", " folder", " the ", "Play the bag file provided in the tutorial ", "Save the geotiff node."], "question_code": ["hector_slam", "catkin", "hector_slam", "hector_slam", "sudo apt-get install ros-indigo-hector-slam", "tutorial.launch", "hector_mapping", "catkin_HS/devel/lib/", "/opt/ros/indigo/lib/hector_mapping", "hector_slam", "src", "catkin_make", "devel", "roslaunch", "tutorial.launch", "wstool"], "answer_code": ["catkin_make -DCMAKE_BUILD_TYPE=Release\n"], "url": "https://answers.ros.org/question/225561/catkin-version-of-the-hector_slam-package-mapping-issue/"},
{"title": "No mattter what the initial position I set, the position x and y subscribing from topic \"/g500/pose\" and \"/uwsim/girona_500/odom is still 0, and z is increasing.", "time": "2016-02-25 08:35:43 -0600", "post_content": [" ", " ", "No mattter what the initial position I set, the position x and y subscribing from topic \"/g500/pose\" and \"/uwsim/girona_500/odom is still 0, and z is increasing. But actually, I can see that the vehicle is exactly at the position which I set."], "answer": [" ", " ", "Hi,", "I guess you are using the uwsim underwater_vehicle_dynamics package  as the vehicle is \"falling\" (z increasing). When using the vehicle_dynamics package the initial position must be stated in it (no matter what you set in the uwsim XML config file) as that package is the one who sets the vehicle position in uwsim. Just set the position in the ", " (g500/dynamics/initial_pose: ). We are still working to integrate the dynamics inside UWSim so everything is easier...", "You also have to send messages to the thrusters_input, if you send velocities or positions to UWSim expect weird results as two nodes will be sending different information to UWSim.", "Thank you. Then I set the position in the yaml file:g500/dynamics/initial_pose: [1, 1, 1]\nBut the result is still the same.", "Oh I understand, I should set the position like this g500/dynamics/initial_pose: [1.0, 1.0, 1.0, 0, 0, 1.57]", "Hi Javier, I run into another problem for the multibeam sensor. When I set to have 10 increase between -20 & 20 degree for it, it has 5 readings with the 1st one \"NaN\" and the last one \"0.0\". Also, in the GUI, I only see 4 green beams rather than expected 5 beams. How should I use the readings?", "Hi, try to post a new question next time. I've seen there is a bug rounding numbers and eventually x/x != 1 so \"NaN\" results appear. I'm solving it and will upload it, but remember multibeam interpolates distances so you will get bad simulation with 10\u00ba increase, try using virtualrangesensors", "Thank you so much for your rely. It is really helpful."], "url": "https://answers.ros.org/question/227563/no-mattter-what-the-initial-position-i-set-the-position-x-and-y-subscribing-from-topic-g500pose-and-uwsimgirona_500odom-is-still-0-and-z-is-increasing/"},
{"title": "rviz - video recording", "time": "2013-07-10 02:32:39 -0600", "post_content": [" ", " ", " ", " ", "Hi all!", "Is it possible to make a video recording from rviz displays?", "Thanks for your help."], "answer": [" ", " ", "It's possible. Just use a screen capture utility. For Ubuntu, you can use something like ", ". ", "Run it, then select the part of the window you wish to record. ", " ", " ", "  captures a parameter defined area of the screen and publishes it as a ros image.  From there the image can be recorded to a bag and then exported to a video  ", "  .   ", " The other approach is publishing an image from a camera within rviz (if it isn't necessary to capture the user interface, or annoying to adjust the window position and screen capture settings to match each other).  There isn't a good way to do that yet but there are some possibilities that might not take too much development  ", " ", " ", "just for reference, here's some other desktop video recording options in ubuntu I use to record graphical stuff (e.g. rviz, gazebo, etc):", ", and ", " has a script that lets you click on a window to record it:", " ", " ", " ", " ", "In ", " package that is available via apt in some ROS distros, there's ", " plugin.", "I only get a huge video that shows only green pixels along with some noise.", "VLC can fix that. \ufffd\ufffd"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " and piping ", " to a file or some other media.", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["sudo apt-get install kazam\n", "alias recordmywindow='window_id=`xwininfo | grep \"Window id\" | sed -e \"s@.*Window id: \\([0-9a-fA-Fx]*\\).*@\\1@\"`; recordmydesktop --workdir /home/hsu/videos --no-sound --windowid $window_id'\n", "--sout"], "url": "https://answers.ros.org/question/66988/rviz-video-recording/"},
{"title": "How to use OpenCV to process image", "time": "2016-03-07 02:49:36 -0600", "post_content": [" ", " ", "Hi! Everyone! ", "I'm a beginner of ROS, and now I've been focusing on imaging processing on ROS, My robot is Turtlebot, and I use the kinect as the camera, so basically what I need to do is to find the changes in the same background, so I followed the tutorials and started to use Gazebo now, and I tested the kinect all is fine. So my question is, I am pretty lost since there is no example or demo for image processing on wiki, I am wondering what is my first step? and what should I do after that. ", "may I know how you are getting images? are you using pictures or video stream? let me know about these and then I'll guide you for ROS and OpenCV image processing.", "I got the video stream from Kinect, and I need to capture one pic using an order, every time I sent an order, one pic is captured. So what I need to do is to do the rest job upon the pic."], "answer": [" ", " ", " ", " ", "ROS and OpenCV are two totally independent projects. All you need to do in ROS is to transform the image message into an opencv image using ", ". Once you have this image, you can do whatever you want with it using OpenCv functionalities. Check the extremely extensive OpenCV documentation and tutorials either in ", " or ", ". ", "Thank you very much! I know what you mean, It's like I need to write a node using OpenCV library to change ROS image first and I can do the image processing after this step in the node right? I prefer C++, so which means all of these should be done by cpp file is that right? ", "I'm assuming you're using the image transport system and a camera to get your images. You'll need to use the ROS cv_bridge object to convert the image into a cv::Mat image. From here you'll be able to use the significant power of openCV to process your image. All of this can be done in C++", "Thank you so much!!!! You saved me!"], "url": "https://answers.ros.org/question/228407/how-to-use-opencv-to-process-image/"},
{"title": "how to integrate kobuki auto docking command in roscpp or python", "time": "2015-06-03 15:05:29 -0600", "post_content": [" ", " ", "Hello", "I am using Turtlebot with Kobuki base.\nI would like to drive the robot to a goal position close enough to the docking station using C++ client in ROS, then when the robot reaches the goal, I would like to call the actionlib command that would take the robot to its docking station. \nI could not find any pointer to this. The only tutorial on ROS wiki I found only shows how to test autodocking by starting two launch files, but I could not find any way to code this either in python or C++.", "Any help would be appreciated\nAnis"], "answer": [" ", " ", "I know this is an old question, but I thought I would post my answer since I just had to figure this out for myself. ", "In the above code the node for the action client, and the goal type are incorrectly defined. The node should be ", " and the goal type should be ", ". Below is a working version of some code to call the docking action:", "Hopefully someone can find this useful.", "Someone found it useful! =D", "how to make the kobuki to perform auto docking program automatically when it detects that the battery is low? then how to add that program to my current go to specific coordinate?", " ", " ", " ", " ", "Thanks for the pointers. Actually, I know and did all these tutorials. I wrote the following code", "But I got a compilation problem in the line \n", "In actionlib client tutorial, the same command above is used, but in my current example, did not want to compile.\nIs there any error in the code?", "what is the error message when the code fails to compile ?", "it gives an error message saying that sendGoal was called with different parameters as it must have three additional callback functions in parameters (doneCallback, feedbackCallback, statusCallback), but the problem is that the same function sendGoal works with one parameters in ROS tutorials.", " ", " ", " Did you do all the tutorials from  ", "  Also you might want to the the action-server, action-client Tutorial from the ROS wiki. \nThey are a great start and there is actually a file which already does exactly what you want to do. \nHave a look at the coffe_bot.py script here: ", "That code sends the action to the Docking server and gives you a result if the docking was successful or not. "], "answer_code": ["dock_drive_action", "kobuki_msgs::AutoDockingGoal", "  // Create the client\n  actionlib::SimpleActionClient<kobuki_msgs::AutoDockingAction> docking_ac(\"dock_drive_action\", true);\n  docking_ac.waitForServer();\n\n  // Create goal object\n  kobuki_msgs::AutoDockingGoal goal;\n\n  // Assign initial state\n  actionlib::SimpleClientGoalState dock_state = actionlib::SimpleClientGoalState::LOST;\n\n  // Send the goal\n  docking_ac.sendGoal(goal);\n\n  ros::Time time = ros::Time::now();\n\n  // Monitor progress\n  while (!docking_ac.waitForResult(ros::Duration(3))) {\n\n    dock_state = docking_ac.getState();\n    ROS_INFO(\"Docking status: %s\",dock_state.toString().c_str());\n\n    if (ros::Time::now() > (time+ros::Duration(10))) {\n      ROS_INFO(\"Docking took more than 10 seconds, canceling.\");\n      docking_ac.cancelGoal();\n      break;\n    }// end if\n  }// end while\n", "  actionlib::SimpleActionClient<kobuki_msgs::AutoDockingAction> ac(\"AutoDockingAction\", true);\n        ac.waitForServer();\n        kobuki_msgs::AutoDockingActionGoal goal;\n        ac.sendGoal(goal);\n\n\n        //wait for the action to return\n        bool finished_before_timeout = ac.waitForResult(ros::Duration(30.0));\n\n        if (finished_before_timeout)\n        {\n            actionlib::SimpleClientGoalState state = ac.getState();\n            ROS_INFO(\"Action finished: %s\",state.toString().c_str());\n        }\n        else\n            ROS_INFO(\"Action did not finish before the time out.\");\n", " ac.sendGoal(goal);"], "url": "https://answers.ros.org/question/210527/how-to-integrate-kobuki-auto-docking-command-in-roscpp-or-python/"},
{"title": "need explanation on sensor_msgs/LaserScan.msg [closed]", "time": "2014-12-03 12:09:09 -0600", "post_content": [" ", " ", "Hi, i am new in ROS, the ", " is as follows:-"], "answer": [" ", " ", " ", " ", " has explanation for all the variables.  ", "Looking at the code ", " will gives good picture of variables.", "laser scanner view of  ", " ", "red,green are x,y-axes", "Values for above scanner ", "Obstacle distance calculation is easy", "if r, \u03b8 are range and angle from one scan laser scan obstacle is at (x,y) = ( rcos\u03b8, rsin\u03b8 )  in laser frame.", "To make sense about \u03b8 , here imagine your laser has 180 deg view and now looking towards x-axis and ray along postive y-axis is +90 degress and negative y-axis is -90 degree.", "hi ", ", i have gone through ", ", but i could not properly understand what those variable holds in them eg range_max, ranges, intensities ect, that's why I've posted this question for a brief explanation.", " , Max reading depends on what laser you are using , it can be 5m or 30m or different depending on laser you are using. Min reading is zero. To understand Intensities , if a laser beam hits reflective surface like glass it will have intenstiy 1.", "And if beam hit some suface which absorbs laser , then intensity is zero. and middle values are different sufaces with relective nature. ", "Please let me know what exactly you are trying to understand.", "thank you so much ", ", can you please explain me about all the variables, thanks in advance. :)", "hi ", ", i have a P3AT spawned in Usarsim Simulator, i want measure its distance with wall or any other obstacle, for this i need to subscribe laser_scan topic which will give the data in the laser_scan.msg format which has these variables, thats why i wanted to know about these variables. Thnks", "if you are looking for lms200 values specifically, go through ", "Thank you very much ", ", now i've understand the things. :)", "Wow. (pi/3.14) Won't they be just 1 and should be eliminated instead."], "question_code": ["std_msgs/Header header\nfloat32 angle_min\nfloat32 angle_max\nfloat32 angle_increment\nfloat32 time_increment\nfloat32 scan_time\nfloat32 range_min\nfloat32 range_max\nfloat32[] ranges\nfloat32[] intensities\n"], "answer_code": ["    angle_min= -135 * (pi/180); //angle correspond to FIRST beam in scan ( in rad)\n    angle_max= 135 * (pi/180); //angle correspond to LAST beam in scan ( in rad)\n    angle_increment =0.25 * (pi/180); // Angular resolution i.e angle between 2 beams\n\n   // lets assume sensor gives 50 scans per second. i.e every 20 milli seconds 1 scan with 1081 beams.\n   // Each beam is measured in  (20 ms/ 1081 ) ~ = 0.0185 ms\n\n    time_increment  = (1 / 50) / (1081); \n\n    scan_time = ;   // scan is collected at which time\n    range_min =0 ; // in meters\n    range_max = 20; // scan can measure upto this range\n    // ranges is array of 1081 floats for each laser beam\n    ranges[0] = //distance measure corresponds to angle -135 deg\n    ranges[1] = //distance measure corresponds to angle -134.75 deg\n    .\n    .\n    .\n    ranges[1080] = //distance measure corresponds to angle +135 deg\n\n\n    //To understand Intensities \n    //if a laser beam hits reflective surface like glass it will have intensity 1. \n    //And if beam hit some surface which absorbs laser , then intensity is zero. \n    //Middle values are different surfaces in between.\n", "Header header            # timestamp in the header is the acquisition time of \n                         # the first ray in the scan.\n                         #\n                         # in frame frame_id, angles are measured around \n                         # the positive Z axis (counterclockwise, if Z is up)\n                         # with zero angle being forward along the x axis\n\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\n\nfloat32 time_increment   # time between measurements [seconds] - if your scanner\n                         # is moving, this will be used in interpolating position\n                         # of 3d points\nfloat32 scan_time        # time between scans [seconds]\n\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\n\nfloat32[] ranges         # range data [m] (Note: values < range_min or > range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units].  If your\n                         # device does not provide intensities, please leave\n                         # the array empty.\n", "agnle_min = -90    \nagnle_max = 90\n"], "url": "https://answers.ros.org/question/198843/need-explanation-on-sensor_msgslaserscanmsg/"},
{"title": "Code Arduino to ROS", "time": "2016-03-28 09:10:57 -0600", "post_content": [" ", " ", " ", " ", " Hi, \nI have my robot Arduino and i want to command it with ROS.\nI install ROS and Rosserial_arduino in my raspberry and i want to command my Robot.\ncan you help me please to add the instructions of ROS in my code arduino?\nmy code is in this page:\n ", " \nThanks ", "Please post the code here. Otherwise the answer will become useless for others if the link to the blog does not work anymore."], "answer": [" ", " ", "#include <afmotor.h>\n#include <servo.h> \n#include <newping.h>", "#define TRIG_PIN A4 // Pin A4 on the Motor Drive Shield soldered to the ultrasonic sensor\n#define ECHO_PIN A5 // Pin A5 on the Motor Drive Shield soldered to the ultrasonic sensor\n#define MAX_DISTANCE 200 // sets maximum useable sensor measuring distance to 200cm\n#define MAX_SPEED 180 // sets speed of DC traction motors to 180/256 or about 70% of full speed - to get power drain down.\n#define MAX_SPEED_OFFSET 10 // this sets offset to allow for differences between the two DC traction motors\n#define COLL_DIST 10 // sets distance at which robot stops and reverses to 10cm\n#define TURN_DIST COLL_DIST+10 // sets distance at which robot veers away from object (not reverse) to 20cm (10+10)\nNewPing sonar(TRIG_PIN, ECHO_PIN, MAX_DISTANCE); // sets up sensor library to use the correct pins to measure distance.", "AF_DCMotor motor1(1, MOTOR12_1KHZ); // create motor #1 using M1 output on Motor Drive Shield, set to 1kHz PWM frequency\nAF_DCMotor motor2(4, MOTOR12_1KHZ); // create motor #2, using M2 output, set to 1kHz PWM frequency\nServo myservo;  // create servo object to control a servo ", "int pos = 0; // this sets up variables for use in the sketch (code)\n  int maxDist = 0;\n  int maxAngle = 0;\n  int maxRight = 0;\n  int maxLeft = 0;\n  int maxFront = 0;\nint course = 0;\nint curDist = 0;\nString motorSet = \"\";\nint speedSet = 0;", "//-------------------------------------------- SETUP LOOP ----------------------------------------------------------------------------\nvoid setup() {\n  myservo.attach(9);  // attaches the servo on pin 9 (SERVO_2 on the Motor Drive Shield to the servo object \n  myservo.write(90); // tells the servo to position at 90-degrees ie. facing forward.\n  delay(2000); // delay for two seconds\n  checkPath(); // run the CheckPath routine to find the best path to begin travel\n  motorSet = \"FORWARD\"; // set the director indicator variable to FORWARD\n  myservo.write(90); // make sure servo is still facing forward\n  moveForward(); // run function to make robot move forward\n}\n//------------------------------------------------------------------------------------------------------------------------------------", "//---------------------------------------------MAIN LOOP ------------------------------------------------------------------------------\nvoid loop() {\n  checkForward(); // check that if the robot is supposed to be moving forward, that the drive motors are set to move forward - this is needed to overcome some issues with only using 4 AA NiMH batteries\n  checkPath(); // set ultrasonic sensor to scan for any possible obstacles\n}", "and i have also all function of this code", "Please edit your question instead of posting something as an answer. What have your tried so far? What do you want? Where is your problem?", "i want to add instruction of ROS into my code", "That's a bit inexact. Why? What kind of instructions? What do you want to achieve?", "i have arduino robot with the code all is ok. i want to command it with arduino. so i installed ROS in raspberry and rosserial.\nnow, i want to converte my code arduino to code arduino with ROS.", "But where is your problem? This is a forum where you can ask for help if you are stuck and no place where others are doing the work for you."], "url": "https://answers.ros.org/question/230317/code-arduino-to-ros/"},
{"title": "Basic understanding of MAVROS, Quadrocopter, UAV", "time": "2016-04-16 13:46:03 -0600", "post_content": [" ", " ", " ", " ", "Hey, ", "I just want to ask if I understood that right:\nMAVROS or better the MAVLink communication protocol con be used to communicate between a companion computer(with ROS installed on an Ubuntu OS) conntected via serial to an autopilot board(e.g: PX4, ArduPilot APM Mega 2.5). Moreover MAVLINK Protocol can be used to communicate over the autopilot connected hardware e.g:  3DR Radio modul 433MHz to a Ground Control System (e.g: mission Planer 2 etc.)", "Now what I would like to know is:\n(1) What kind of Companion Computer do you suggest? (e.g: Erle Copter just uses an ARM Cortex A8) but I would like to use an Intel nuc board instead? Do you think this is too heavy?", "(2) Would you suggest controlling the UAV via SSH via Wifi Connection via intel nuc board wifi card instead of using the MAVLINK and the 3DR Radio modul? (this is probably a matter of range is it not?) ", "(3) What is the difference between MAVLINK and MAVROS? I thought MAVROS is dependend on MAVLINK but what is MAVROS then?", "(4) Ah if somebody knows the Erle Brain 2 powering a Erle copter ist just an autopilot or? Or is it kind of both a companion computer and a autopilot board which are communicating via MAVLINK Protocol?", "Hope you can help me in understanding this :)\nBest Markus", "Edit: Hey Marco, \nWow Thank you so much for your explained answer. I just want to make sure I got everything right:", "So generally you can you say MAVLInk is the communication protocl used for the communication between a Groud Station and an Autopilot. Whereas Mavros can be used if you have a companion pc connected to your autopilot to communicate (publish subscribe topics etc.) with your autopilot. Ok I think I got that!", "(1) Yeah we are just thinking about a companion computer using an intel nuc but as far as I know I only saw Rasperi Pis, Banana Pis arduino boards, beagle Boards but almost never an nuc because of weight probably. ", "(2) Wow that is really helpfull and good to know what datalink is preferable. Yeah I also read that e.g. Ascending Technologies uses a XBee Link. See more last point what I found out about datalinks:", "(4) Yeah what I do not get the Erle Brain is kind of both autopilot and companion Computer or? I mean it is both on one board is it not?!", "Best Markus", " Additional: Datalinks:\n ", " ", "When applying the 2G and 3G networking services in the \ufb01eld of UAVs, the use of a\nregular (commercial) cellular network may not be practical due to potential coverage\nproblems in remote areas and high operational costs [8]. However, it is possible to\nimplement the cellular concept with a self-created network [9] in which a cellular base\nstation such as an IP.Access nanoBTS [10] module represents the \ufb01xed infrastructure\nside of the network. In combination ...", ", please don't post ", " yourself, unless you are actually ", " the question. For updates or follow-up questions, please edit your original question. You can use the ", " link/button for that. I've already merged your answer into your OP, but please keep it in mind.", "Ok thanks :)"], "answer": [" ", " ", "Hi Markus,  I have been working a little bit with Mavlink, Mavros and the ", " and I think that what you said is a little bit confusing:", " is a communication protocol between a ground station application and an autopilot board. Mavlink does not require ROS to exchange messages between these two agents. In fact, you can use any autopilot and ground station you want without having ROS installed in your PC. On the other hand, if you want mavlink messages to be published/taken in/from ROS, then you need to run Mavros on your pc. In this case Mavros directly \"talk\" with your autopilot, using mavlink, and publishes/subscribes to determined topics. ", "About your questions:", "1) I've only used the PX4FMU board with no companion computer, so I can't really suggest you which one is the best.", "2) What do you mean by \"controlling\"? For safety reasons I'd say all the controllers (attitude, position, etc) should run on the computer/autopilot in your drone. You can \"control\" the drone remotely in the sense of changing parameters and settings, send commands, etc. In this latter case, I saw that WIFI can be problematic (delays, high packet loss, etc...), especially if there are many people around. You can use 3DR module, or maybe a ", ".", "3) I hope I answered this question in the above introduction.", "4) Erle Brain is mainly composed by a raspberry pi, so you should have enough computational power both for autopilot and executing other processing (in fact Erle Robotics sells a onboard camera that can be attached to their brain).", "Marco."], "url": "https://answers.ros.org/question/232076/basic-understanding-of-mavros-quadrocopter-uav/"},
{"title": "Laser scanner choose", "time": "2016-04-19 03:12:27 -0600", "post_content": [" ", " ", "Hello, i am working on an AGV Project with 2D SLAM. Now, I survey different sensors like SICK LMS-100, and Hokuyo UTM-30LX. i can't tell distinct different between them from the spec and the price are also similiar. but i notice that in many cases, AGV prefer SICK and Small Robot using Hokuyo. Except for the size, Is there any specific reasons? Thanks!!"], "answer": [" ", " ", "I think it's mainly the size; as you wrote, the price + specs for the SICK LMS-100 and the Hokuyo UTM-30LX are roughly similar. However, many smaller robots use the Hokuyo URG-04LX, which is completely different: cheaper, has lower power consumption but also much worse specs than the other two. By the way, SICK has recently added a line of small (Hokuyo-like) scanners, the SICK TiM 5xx line, that are available in a range of different prices and specs.", "Thanks for your reply Martin. The point i am curious about is that if the price and spec are similiar, why AGV prefer LMS-100 but not UTM-30LX(maybe i was wrong) as it is much smaller. Is there any advantage of LMS-100 when it applied on AGV  ?", "I don't really know, sorry. Perhaps you should make a list of requirements (angular resolution, range, aperture, safety class (resistance against dust/water), resistance against ambient light etc., and then ask SICK and Hokuyo directly why their scanner is better than the other. :)", "Thanks Martin, i think you might come out some good point that i can ask."], "url": "https://answers.ros.org/question/232235/laser-scanner-choose/"},
{"title": "Fixed frame does not exist on Rviz", "time": "2016-04-26 07:14:50 -0600", "post_content": [" ", " ", "hi,\ni have a problem on rviz, fixed frame not existe. do you have any solution of tutorial for create fixed frame please?"], "answer": [" ", " ", "Look for a link in ", " that has a 1 or more answer and a green check mark on it.", " (Is there a way to make the answer to the question a sticky post on the top of the  ", "  front page?) ", "Is there a way to make the answer to the question a sticky post", "Would be nice :). But it would also be nice if people would use the search and / or read the RViz documentation. That would save us a lot of questions like this one."], "url": "https://answers.ros.org/question/232912/fixed-frame-does-not-exist-on-rviz/"},
{"title": "how can I install vicon_bridge?", "time": "2016-04-28 10:16:49 -0600", "post_content": [" ", " ", "I am using Ubuntu 14.04, and ros indigo and I am a new user.  When I run \"sudo apt-get install ros-indigo-vicon-bridge\" in command line to install the vicon_bridge package it says \"unable to locate package\".  I have tried some variations on that command but I can't figure out what I am doing wrong and I can't seem to find any instruction.  Please help"], "answer": [" ", " ", "If you look at the ", " wiki page, you'll notice the only green status indicator at the top is \"Documented\" --- there is no \"Released\" or \"Continuous integration\" as compared to, for example, ", ". This was my first indication that this package isn't released on Indigo. I then checked ", " to verify that the package isn't available on ", " (which is exactly what your error says). So the basic problem is the package you want to install has not been released to the ROS build farm. Thus, you can't install a pre-built binary from ", ". So, we need to build this package from source:", "Those commands will clone the package into your workspace (you may need to adjust the path), verify you have system dependencies installed, and then compile the package in your workspace. Now you should be able to run and launch nodes in this package (assuming you've sourced your workspace's setup.bash).", "Thank you!"], "answer_code": ["apt-cache search vicon", "apt-get", "apt-get", "cd ~/catkin_ws/src/\ngit clone https://github.com/ethz-asl/vicon_bridge\nrosdep install -r --from-paths vicon_bridge/\ncd ~/catkin_ws/\ncatkin_make\n"], "url": "https://answers.ros.org/question/233177/how-can-i-install-vicon_bridge/"},
{"title": "Failled to install MAVLink on Raspberry Pi. How I do?", "time": "2016-04-12 14:18:53 -0600", "post_content": [" ", " ", "Hello all!", " I have some problems. I have been trying to install Mavros on Raspberry Pi fallowing the next link:\n ", "But I can't to achieve it. When I want to install it, i write the next command line:", "show me the next image.\n", "I'm running a ssh since ubuntu.\nROS - Indigo", "Someone have any idea? How to install mavros - mavlink on Raspberry Pi?"], "answer": [" ", " ", " ", " ", "Hi,", "I suggest you follow the instructions at ", " in the \"Source Installation\" section. There is also a green box at the bottom of the page regarding Raspberry Pi.", "Regards,", "Marco.", "Yes, the question explicitly states that the OP is trying to install mavros.", "yeah, sorry for that. Let me try to be more polite :)", " ", " ", "The odd thing to note here is that libboost-system1.54.0 is not installable. That's a package that's normally available on Ubuntu, so it's unusual for it not to be installable.", "You can try to debug by attempting to install it manually. (Be sure to read the prompts carefully if it wants to uninstall another package):", "You may also want to check that you have the main, universe and multiverse repositories enabled in your apt sources. (boost is normally a part of the main repository)."], "question_code": ["sudo apt-get install ros-indigo-mavros ros-indigo-mavros-extras\n"], "answer_code": ["sudo apt-get install libboost-system1.54.0\n"], "url": "https://answers.ros.org/question/231769/failled-to-install-mavlink-on-raspberry-pi-how-i-do/"},
{"title": "rospy sensor_msgs.msg.BatteryState is not found", "time": "2016-04-12 14:45:15 -0600", "post_content": [" ", " ", " For some reason I'm not able to import the BatteryState message with rospy. This is the only message I've encountered that seems to be missing.  ", "I'm using ros-indigo. Anyone else have this problem?"], "answer": [" ", " ", "The BatteryState message was added to Indigo in the most recent release of sensor_msgs, and it has not yet reached the public apt repository.", "If you really need the battery message, you can use sensor_msgs from source, or you can keep an eye on the ros-users mailing list for the announcement of the next sync, which will be made when the updated version of the package reaches the public apt repo.", "How might I install sensor_msgs for source for rospy?", " Have a look at the answer here:  ", "  , but replace the git URL for descartes with the git URL provided on the sensor_msgs wiki page. ", "Thanks. I actually ended up installing BatteryState as a custom message in my package, copy/pasted from github."], "question_code": ["zac@nuc-2:~$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from sensor_msgs.msg import Imu\n>>> from sensor_msgs.msg import BatteryState\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name BatteryState\n>>>\n"], "url": "https://answers.ros.org/question/231771/rospy-sensor_msgsmsgbatterystate-is-not-found/"},
{"title": "launch multiple Rviz instances on the same machine", "time": "2016-04-25 21:17:47 -0600", "post_content": [" ", " ", "Hi,", "Is it possible to launch multiple independent Rviz instances on the same machine?"], "answer": [" ", " ", " ", " ", "Yes. Assuming you have enough memory/CPU/GPU power you can start as many rviz instances as you like. The only \"gotcha\" that I can think of is that each instance should have a unique ROS name (just like every other ROS node). This happens automatically with ", " and can easily be implemented in ", " with the ", " substitution arg.", "If by \"independent\" you mean they shouldn't be able to talk to each other via ROS mechanisms. Then, once again, yes. That is possible. You would just need to set the ROS_MASTER_URI different for each instance (and have a corresponding roscore running).", " ", "If the second paragraph is ", " what you want, there are certainly a few ways this could be done, but none of them seem great. If you already had two ros masters running on different ports (could be started with ", " see ", ") then you could use the ", " tag to set the ROS_MASTER_URI for each rviz instance. For example:", "I verified this works, but it obviously isn't great as it requires you to manually start one ROS master independently of the call to ", " (the second master could get started automatically by ", "). Here are a few potential solutions to this problem:", "Use a shell script that starts the ROS cores for you. It might be tricky to make sure that they are both up and running before calling ", ". You will also need to be careful that everything shuts down properly.", "In principle, you could edit the nodes that get started when calling roscore by editing ", ". I don't think this is a great idea though.", "Thanks ", ", I was able to launch independent Rviz instances by setting different ROS_MASTER_URI and GAZEBO_MASTER_URI in separate terminals. However, when I tried to launch them from one single launch file, one Rviz starts properly, while the other exits on exception.", "here is my config\n <node name=\"rviz2\" pkg=\"rviz\" type=\"rviz\" args=\"-d $(find scene_plugin)/launch/test2.rviz\" required=\"true\"/>\n  <node name=\"rviz3\" pkg=\"rviz\" type=\"rviz\" args=\"-d $(find scene_plugin)/launch/test3.rviz\" required=\"true\"/>\nDo you have any idea why this would happen?", "There should be nothing wrong with two rviz instances in a single launch file. However if you are trying to launch two rviz instances that talk to different masters, that may take a bit more effort. See my edited answer above."], "answer_code": ["rosrun", "roslaunch", "anon", "roscore -p", "<launch>\n  <node name=\"rviz_instance1\" pkg=\"rviz\" type=\"rviz\">\n    <env name=\"ROS_MASTER_URI\" value=\"http://localhost:11311\" />\n  </node>\n\n  <node name=\"rviz_instance2\" pkg=\"rviz\" type=\"rviz\">\n    <env name=\"ROS_MASTER_URI\" value=\"http://localhost:11312\" />\n  </node>\n</launch>\n", "roslaunch", "roslaunch", "roslaunch"], "url": "https://answers.ros.org/question/232877/launch-multiple-rviz-instances-on-the-same-machine/"},
{"title": "rosbag playing 100hz lidar data", "time": "2016-04-22 02:32:17 -0600", "post_content": [" ", " ", " ", " ", " Hi I'd recorded four lidar(Sick  ", " ) with sicktoolbox_wapper2 and rosbag record(lidar1/scan, lidar2/scan, and so on). ", "Once I tested my code in the lab using rosbag play XXX.bag, I noticed my callback function actually manipulating quadruple data at once! \nCallback takes not 10ms for each lidar data but it takes 40ms at first data and about 0ms at others. even the program subscribe 1 of 4 lidar data(lidar1/scan).", "Here is my test code to compare recoded timestamp & callback time", "and its results with network IO graph using wireshark are below:", "using rosbag play XXX.bag. Highlighted text shows called time(left with bracket) and sensor timestamp(right)\n", "using rosbag with -i option\n", "no bag used(using actual sensor), ignore wireshark result... it's same as upper one.\n", "In view of the results, I think this problem is a bug of rosbag play, not my code related with ROSTCP and ROSCPP.\nHow can I handle this problem? Is there anyone who suffered similar problem?"], "answer": [" ", " ", " ", " ", "It took me a while to figure out what the question is that you're asking, so let me rephrase it: You recorded a rosbag at 100 Hz, and when you replay it, rosbag sends the messages in bursts of 4. You would instead like rosbag to play each single message at once. Correct?", "First: I don't think this behavior by rosbag is a problem or a bug. Do you really absolutely need to get the messages as fast as possible? Since everything is timestamped in ROS, I can not come up with a reason why you need that, especially when playing back a rosbag (there is no interaction with the external world in that case, so it doesn't really matter if there is a 30 ms delay in producing the answer).", "But if you really must try to change this, read on. Your first wireshark screenshot shows that it is indeed the publisher inside rosbag that buffers the messages, not the subscriber inside your test node. Perhaps it's because ", " is not set? Try enabling it in ", ". This should reduce the delay, but be less efficient.", "P.S.: Some more comments:", "Yes it corrects. I can modify program to use ros timestamp. but I want to clarify why it happen although it actually have power to doing more accurately.\nI used -i option to test communication performance between rosbag and my node. I think this results shows that the communication is not problem.", "and thank you for your kind advices! I used \n    --clock \noption when that screenshot was taken but I didn't realize that\n    rosparam set use_sim_time true\nis needed!", "Glad I could help! Just to clarify: If you use ROS time (and you should), the results will be as accurate as possible, no matter if the messages are buffered or not."], "answer_details": [" doesn't disable the buffer; instead, it makes rosbag speed up replay time as fast as it can, instead of running at the same speed as when the rosbag was recorded.", "When playing back a rosbag, you should usually run ", " right after starting the roscore, and call rosbag with the ", " option. For the screenshots you made it was correct not to do that (because it actually produced more useful log output), but in general you should always do it.", " ", " ", " ", " "], "question_code": ["#include <ros/ros.h>\n#include <sensor_msgs/LaserScan.h>\nclass laser_rate_test_node{\nprivate:\n    ros::NodeHandle nh;\n    ros::Subscriber laser_sub;\npublic:\nlaser_rate_test_node(ros::NodeHandle _nh):nh(_nh){\n    laser_sub = nh.subscribe<sensor_msgs::LaserScan>(\"lidar1/scan\",10, &laser_rate_test_node::laserCallback, this, ros::TransportHints().unreliable());\n}\nvoid laserCallback(const sensor_msgs::LaserScanConstPtr& scan){\n    ROS_INFO(\"%lf\",scan.get()->header.stamp.toSec());\n}\n};\nint main (int argc, char **argv)\n{\n    ros::init(argc, argv, \"laser_rate_test_node\");\n    ros::NodeHandle nh;\n\n    laser_rate_test_node laser_rate_test_node(nh);\n    ros::spin();\n}\n"], "answer_code": ["rosbag -i", "rosparam set use_sim_time true", "--clock"], "url": "https://answers.ros.org/question/232580/rosbag-playing-100hz-lidar-data/"},
{"title": "Good Path following controller for ROS!", "time": "2016-05-17 08:26:34 -0600", "post_content": [" ", " ", "Hello people,", "I am trying to generate collision free trajectory using specific planning algorithm and i got my trajectories from my planner and now i need a path-following controller to make the car-like robot to follow the concern trajectory.", "So it there any good in-built path following controller code in ROS that i can use directly? Or any suggestions which controller that i can use? ( i am already trying to built pure-pursuit controller on my own, but i am bit lost!)", "Any help will be much grateful!!"], "answer": [" ", " ", " ", " ", "Hi vignesh,", "according to your description I guess that you are looking for a path controller rather than a tracking controller.\nHowever, your specific planning algorithm returns a trajectory? The question is now: trajectory tracking (following the desired time profile) or path following.\nAnd it is further important to know, if your planner returns trajectories that are already feasible for your car-like robot or if it is just a coarse reference path without taking kinodynamic constraints into account.", "Depending your problem setup you have multiple options:", ": ros-control provides generic interfaces for controllers. Package ", " already provides some common implementations (for robotic arms, and for a diff-drive robot). You could implement your controller here (e.g. your pure pursuit). Or try one of the existing controllers. Since you have a car-like robot, a simple PID controller should not fit well I guess. The choice probably depends on your reference path / trajectory. ", ": You can wrap your specific planning algorithm into a global planner plugin in order to be compliant with move_base (refer to navigation tutorials). Afterwards you can implement your pure pursuit algorithm as local planner plugin. Using this approach you can benefit from localization, maps etc.\nSince you are asking for existing packages for path following:\nYou could try ", " which supports car-like robots and also provides a path following mode in which the distance to the reference path is minimized. The reference path may also be really coarse. However, since it is an optimization based approach, the computational burden is quite large. There is a ", " on path following. But you need the planner version 0.4 which is not yet on the public repositories. I guess a sync will not take place before kinetic is released next week. But you can compile the package from source. You might also refer to this ", ".", "EDIT: I found a standalone pure pursuit implementation ", ". The source is \nfor ROS hydro, but it should work (with minor modifications) on newer distributions as well. But I have never used the planner and hence cannot tell anything about it.", "I hope that helps.", "Cheers", "Thanks for the much detailed answer croesmann. My planner will give trajectory according to kinodynamic constraints of car-like robot. (but i have also taken those trajectories poses and stored as way points with nav_msgs::Path message type !). My controller just have to follow feasible path!", "ros-control seems to be good option to proceed on but there are lot of existing controllers already in it! Do u know which controller will be feasible for car-like robot?", "ros navigation won't adapt for my environment i guess and i will look into pure pursuit code!", "Hi Croesmann, How about robot_controller package developed by Fetch Robotics where there were some safety control features powered by Laser sensor"], "url": "https://answers.ros.org/question/234491/good-path-following-controller-for-ros/"},
{"title": "Unable to compile rviz on ubuntu armhf", "time": "2013-01-13 01:17:28 -0600", "post_content": [" ", " ", " ", " ", "I am building ROS Groovy Desktop on a Pandaboard running Ubuntu 12.04 Precise using armhf. As ROS is not currently released for armhf I have had to compile Groovy from source. I am attempting to compile RVIZ and it crashes part-way through with errors related to \"undefined reference to 'vtable for Assimp::IOSystem' \"  ", "See below for the full output from running Make within the RVIZ directory:", "[ 61%] Built target rviz\nLinking CXX executable /home/user/ros_catkin_ws/devel_isolated/rviz/lib/rviz/rviz\n[ 92%] Built target default_plugin\n[ 92%] Generating moc_image_view.cxx\nScanning dependencies of target rviz_image_view\n[ 92%] Building CXX object src/image_view/CMakeFiles/rviz_image_view.dir/image_view.cpp.o\n/home/user/ros_catkin_ws/devel_isolated/rviz/lib/librviz.so: undefined reference to ", "typeinfo for Assimp::IOSystem'\ncollect2: ld returned 1 exit status\nmake[2]: ", " [all] Error 2"], "answer": [" ", " ", " ", " ", "I had the same issue and I solved it by patching mesh_loader.cpp by adding the next lines after the assimp include lines:", "This is basically the same patch as here applied for the collada_urdf package: h++ps://groups.google.com/forum/#!msg/ros-sig-embedded/26XlDtZhyNs/OexZAx6BCBcJ</strings.h>", "Did the trick to Odroid running Ubuntu Linaro 12.11 and ROS Hydro ;)", "Give this man a cookie.", "Spot on. As small of a change this is without it a build of rviz is a no go. :D", " Following is the path to the file \"/home/pi/ros_catkin_ws/src/rviz/src/rviz/mesh_loader.cpp\" if you are following the setups on  ", " ", " ", "RVIZ is pointless to have on the pandaboard.  It is not powerful enough to run it and does not have the required video drivers to make it work.  Just remove the package and anything that depends on the rviz package.", "Interestingly if you compile it, this means you can compile packages that depend on rviz, and then disable / not actually use rviz when you are running them.", "I wanted the Rqt stuff and the Rviz was just a dependency (along with VLC player?!?!). So if you want to use and develop something like the Rqt dashboard this is definitely a must. As for using Rviz on such a platform - yeap, it's like shooting yourself in the kneecap.", " ", " ", "The Indigo builds for ", " include a patched version of ASSIMP that no longer requires patches to all of the packages that depend on it.", " ", " ", "the problem keeps unsolved after patching on jessie(Raspberry Pi 2) ", "seems error with the \"ifdef _arm_\" . deleting it and \"endif\" makes the error retreats", " ", " ", "Based on an installation tutorial for ROS Groovy on Raspbian Pi:", "h++p://www.ros.org/wiki/groovy/Installation/Raspbian/Source", "I created a tutorial for ROS Groovy running under Ubuntu 12.04 on a PandaBoard (armhf):", "h++p://www.ros.org/wiki/groovy/Installation/PandaBoard/Source", "It's recommended to use the robot installation, this will have all neccessary messages but no grafic tools.", "Hope this will help somebody.", " ", " ", "I encounter same problem, and solved by add \n      #  ifdef __arm__                 // fix for ARM build\n      #include <strings.h>\n      bool Assimp::IOSystem::ComparePaths(const char *p1, const char *p2) const\n      {\n             return !::strcasecmp(p1, p2);\n      }\n     #  endif\nin to src/rviz/mesh_loader.cpp"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Does anyone know how I can get past this error and get RVIZ to compile properly?"], "question_code": ["vtable for Assimp::IOSystem'\n/home/user/ros_catkin_ws/devel_isolated/rviz/lib/librviz.so: undefined reference to", "vtable for Assimp::IOSystem'\n/home/user/ros_catkin_ws/devel_isolated/rviz/lib/librviz.so: undefined reference to"], "answer_code": ["#  ifdef __arm__                 // fix for ARM build\n#include <strings.h>\nbool Assimp::IOSystem::ComparePaths(const char *p1, const char *p2) const\n{\n    return !::strcasecmp(p1, p2);\n}\n#  endif\n"], "url": "https://answers.ros.org/question/52098/unable-to-compile-rviz-on-ubuntu-armhf/"},
{"title": "P2os and rosaria not connecting to Pioneer P3-AT", "time": "2016-06-08 07:36:19 -0600", "post_content": [" ", " ", "I have pioneer ", " robot and a separate laptop with Ubuntu ", " and Ros ", " properly installed in it.", "I downloaded ", " from  ", "  and  ", " from  ", "I am connecting robot to laptop via ", " converter with RS-232 cable", "But every time I run command ", "sudo usermod -a -G dialout $USER", "Then I logged out and again logged in", "sudo chmod 777 -R /dev/ttyUSB0", "rosrun rosaria RosAria _port:=/dev/ttyUSB0", "But I get this error", "shivam@shivam-Inspiron-3542:~$ : rosrun rosaria RosAria", "[ INFO] [1422444061.117081691]: RosAria: using port: [/dev/ttyS0]", "Could not connect to simulator, connecting to robot through serial port /dev/ttyS0.", "Syncing 0", "No packet.", "Syncing 0", "No packet.", "Trying to close possible old connection", "Syncing 0", "No packet.", "Syncing 0", "No packet.", "Robot may be connected but not open, trying to dislodge.", "Syncing 0", "No packet.", "Robot may be connected but not open, trying to dislodge.", "Syncing 0", "No packet.", "Could not connect, no robot responding.", "Failed to connect to robot.", "[ERROR] [1422444067.613784823]: RosAria: ARIA could not connect to robot! (Check ~port parameter is correct, and permissions on port device.)", "[FATAL] [1422444067.613882512]: RosAria: ROS node setup failed...", "And when I run ", " it shows this ", "[ERROR] [1349087058.764888277]: Error reading packet header from robot connection: P2OSPacket():Receive():read():", ".", ".", "[ERROR] [1349087114.446749752]: Error reading packet header from robot connection: P2OSPacket():Receive():read():", "[ERROR] [1349087114.447846009]: p2os setup failed...", "[p2os-1] process has died [pid 12089, exit code 255]", "Robot is working properly. I tried it with Aria Library which comes with p3-at.\n", " \nbecause I am connecting it using the serial port of the robot. And no where it mentions that robot's OS is important"], "answer": [" ", " ", "When you run RosAria or p2os you can specify the port via a ", " parameter. e.g.  ", " or ", ".     When you tested it with ARIA, did you alse specify the ", " as the port (e.g. ", ")?     ", "You can turn off the internal onboard computer with the switch on the side. This will give you extra battery life and prevent any possible interference with anything on that computer also using the serial port.", "I am using the port /dev/ttyUSB0, I am supplying it as a parameter in RosAria and p2os both. \"Turn Off the on-board computer\" - the serial port is on the on-board computer, I think if I turn it off then serial port won't work. Or is there any other serial port inside the robot", "Yes you should use the HOST/SERIAL port on the left side of the robot if you are using an added laptop, that is the connection to the robot's controller.   (Which is also connected to the internal computer as well.)", "So you mean I need to open the robot's top plate to access the Host Serial Port because I don't See it in P3-AT on the side", "Sorry on the P3AT its on the top, next to TX, RX, STATUS leds, reset button, battery led, etc.  STATUS should show if the controller is ready, communicating to software etc. TX and RX will show if there is any communication through the serial connection.", "Could you please tell me if I need to give any commands on the robot to connect via serial port. I know you said turn off the onboard computer and I did, but it still could not connect. So we don't need to give any permissions, just connect the serial cable to the robot and run the command on laptop", "Sir I Know its too many questions? but Sir please, its urgent for me, I have to complete my work in 1 month at my college", " ", " ", " ", " ", "In the example you posted, it shows that you used /dev/ttyS0 instead of /dev/ttyUSB0 in your attempt to connect, so make sure that you use the correct syntax.  I use an AmigoBot, not a P3-AT, but it sounds like the connection process is similar when using the serial port.  I usually use the following process to connect with RosAria.", "-Turn on the robot and laptop and connect them with the RS-232 cable", "-Run 'roscore' on laptop terminal (1)", "-Run 'sudo chmod 666 /dev/ttyUSB0' on a new laptop terminal (2)", "-Run 'rosrun rosaria RosAria _port:=/dev/ttyUSB0' on laptop terminal (2)", "Additionally, if you are using an RS-232 extension cable, you should verify that it is the correct type.  The AmigoBot uses a straight through, not a null modem.  Null modem cables have a different wiring configuration;  I'm not sure which the P3AT uses, but you should double check.  Additionally, some cables don't have all of the pins connected, so that could also be causing problems.", "I used the port parameter to specify the port as /dev/ttyUSB0 , Is there any possibility that even after specifying the port it used the port /dev/ttyS0 ?", "You included the command that gave the error message.  ", "shivam@shivam-Inspiron-3542:~$ : rosrun rosaria RosAria", "I don't know if this was just one of several attempts, or if you didn't include the port parameter on any of your attempts.  I don't think it would have used an unspecified port."], "answer_code": ["port", "rosrun rosaria RosAria _port:=/dev/ttyUSB0", "rosrun p2os p2os_driver _port:=/dev/ttyUSB0", "/dev/ttyUSB0", "demo -rp /dev/ttyUSB0"], "url": "https://answers.ros.org/question/236426/p2os-and-rosaria-not-connecting-to-pioneer-p3-at/"},
{"title": "Warning \"Map update loop missed its desired rate\" keeps on comming", "time": "2014-10-21 15:25:52 -0600", "post_content": [" ", " ", "hello, i am setting up Navigation stack as explained in the ", ", My my configuration files are as follows:-", "I have met the same problem,the warning always occured when i tried to change the value of controller_frequency in both larger and smaller conditions. But it seems only a little harm in  the running process.I also expect someone who can point out the real problem it is ,with much appreciation.", "What system are you running on and what sensors and global map size do you get? 3 seconds for an update seems really slow. Does this happen only once?", " i am using ros-fuerte on ubuntu machine. \n& a p3at in USARSim (on windows machine).", "MAP SIZE: 4000, 4000", "Subscribed to Topics: laser_scan_sensor", " Thanks for you reply.", "i have change update frequency from 2.0 to 0.25, now no such kind of warning appears.", "but getting a new warning", " to view the terminal output", "You just changed the parameters to suppress the warning without solving that it is slow. The other warning could be due to the robot/navigation setup now or still caused by the same problem."], "answer": [" ", " ", "This means that your map cannot be updated as fast as you've parameterized it. You can choose to use a smaller or lower resolution map or provide more computational power. ", "@rfoote then what should be the new value, & where i need to improve. please explain.", "There's a ", " ", " ", " ", " ", " I think it's a potential bug in navigation stack. please see:\n ", "comment r.sleep() will solve the issue.", "where should the r.sleep() be added?"], "question_code": ["    TrajectoryPlannerROS:\n    # for details see: http://www.ros.org/wiki/base_local_planner\n    max_vel_x: 0.2\n    min_vel_x: 0.05\n    max_rotational_vel: 0.5\n    min_in_place_rotational_vel: 0.1\n\n    acc_lim_th: 3.2\n    acc_lim_x: 2.5\n    acc_lim_y: 0\n\n    holonomic_robot: true\n\n    # goal tolerance parameters\n    yaw_goal_tolerance: 0.1\n    xy_goal_tolerance: 0.2\n    latch_xy_goal_tolerance: true\n", "  obstacle_range: 2.5\n  raytrace_range: 3.0\n  footprint: [[0.305, 0.278], [0.04, 0.193], [-0.04, 0.193], [-0.282, 0.178], [-0.282, -0.178], [-0.04, -0.193], [0.04, -0.193], [0.305, -0.278]]\n #footprint: [[0.075, 0.178], [0.04, 0.193], [-0.04, 0.193], [-0.282, 0.178], [-0.282, -0.178], [-0.04, -0.193], [0.04, -0.193], [0.075, -0.178]]\n #robot_radius: ir_of_robot\n inflation_radius: 0.6\n\n observation_sources: laser_scan_sensor\n\n laser_scan_sensor: {sensor_frame: openni_depth_frame, data_type: LaserScan, topic: scan, marking: true, clearing: true}\n", "  global_costmap:\n  global_frame: /map\n  robot_base_frame: /base_footprint\n  update_frequency: 2.0\n  static_map: true\n", "  local_costmap:\n  global_frame: /map\n  robot_base_frame: /base_footprint\n  update_frequency: 2.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 6.0\n  height: 6.0\n  # origin_x: -0.115\n  origin_x: 7.5\n  origin_y: 7.5\n  resolution: 0.05\n", "controller_frequency: 10\ncontroller_patience: 15.0\noscillation_timeout: 10.0\noscillation_distance: 0.5\n\nTrajectoryPlannerROS:\nmax_vel_x: 0.45\nmin_vel_x: 0.1\nmax_rotational_vel: 1.0\nmin_in_place_rotational_vel: 0.4\n\nacc_lim_th: 1.0\nacc_lim_x: 0.5\nacc_lim_y: 0.5\npath_distance_bias: 50.0\ngoal_distance_bias: 0.8\nholonomic_robot: false\n", "<launch>\n\n  <!-- Run the map server -->\n  <node name=\"map_server\" pkg=\"map_server\" type=\"map_server\" args=\"$(find my_robot_name_2dnav)/map/map.pgm 0.05\"/>\n\n  <!--- You can see original move_base.launch -->\n  <!--- Run AMCL -->\n  <include file=\"$(find usarsim_inf)/launch/usarsim.launch\"/>\n  <include file=\"$(find amcl)/examples/amcl_omni.launch\" />\n\n\n\n  <node pkg=\"move_base\" type=\"move_base\" respawn=\"false\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"$(find my_robot_name_2dnav)/launch/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"$(find my_robot_name_2dnav)/launch/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"$(find my_robot_name_2dnav)/launch/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find my_robot_name_2dnav)/launch/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"$(find my_robot_name_2dnav)/launch/move_base_params.yaml\" command=\"load\" />\n  </node>\n\n\n  <node pkg=\"gmapping\" type=\"slam_gmapping\" respawn=\"false\" name=\"slam_gmapping\" args=\"scan:=lms200\" >\n  </node>\n\n  <node pkg=\"rviz\" type=\"rviz\" respawn=\"false\" name=\"rviz\">\n  </node>\n</launch>\n", "  [ WARN] [1413922749.722348193]: Map update loop missed its desired rate of 2.0000Hz... the loop actually took 2.9248 seconds\n", " Aborting because a valid plan could not be found. Even after executing all recovery behaviors\n"], "url": "https://answers.ros.org/question/195523/warning-map-update-loop-missed-its-desired-rate-keeps-on-comming/"},
{"title": "Kobuki 12V 1.5A connector?", "time": "2016-06-08 08:13:42 -0600", "post_content": [" ", " ", "According to ", " describing the Kobuki power connectors, the 12V 1.5A connector is similar to ", ".  The connector on the robot is single-row 1-circuit but all I can find is a two-row version.  Is it possible there is a different Molex part number that matches?", "Thanks!", "\npatrick", "The connectors you link to are a two-pin Mini-Fit (2 rows, 1 column). I'm reasonably certain that's the connector that's on the Kobuki", "Thanks Austin.  I got myself confused with the ", " that shows only two-row connectors.  However, it's still not clear to me which mating connector I need..."], "answer": [" ", " ", "This ", " like the correct mating connector, but I haven't bought one to verify it:  ", " ", "Molex makes many versions of the mating connector and crimp pins, with variations in temperature resistance, corrosion resistance, material and cost. These can usually be found through the \"Mating Product\" and \"Associated Product\" sections on the part page on DigiKey.", " The company where I work has a Molex Mini-Fit crimper ($300+), so I borrow that when I need to make cables, but it doesn't scale well for hobbyists. If you have access to one, this housing should be reasonable:  ", "  . Depending on your wire size you can go from that to individual crimp pins. ", "If you buy crimp pins for hand crimping, ", ", not tape. Bulk connectors come pre-separated, tape connectors come attached to a metal tape and need to be cut off before you can use them in the hand crimper.", "Many thanks Austin! I really appreciate the time you took to find that part.  I have ordered one from DigiKey and will report back here if it works.", "I just received ", " listed above and unfortunately it does not fit the 12V 1.5A jack.  It ", " fit the 12V 5A jack.  Sure would be nice if Yujin Robot could post links or sell the connectors...", "I went from the part number you linked and didn't read the Yujin page carefully; they do clearly state that the 5566-02B2 (mini-fit) is for the 12V@5A connector.", " I don't see a similar pre-assmebled cable for the 43045-0224 (micro-fit). DigiKey does sell the housings:  ", "  and crimp pins; but the crimper is still about the same price :( ", "Thanks again Austin.  That micro-fit connector looks right so I've ordered one from DigiKey and some crimp pins.  I have a cheap crimp tool that I've used before so hopefully that will get the job done.", "Yep, that's the one.  Just received it from DigiKey and it fits the 12V 1.5A jack.  It is DigiKey part number WM1783-ND and manufacturer (Molex) part number 0430250200.", "Follow-up question - so I'm attempting the same task of getting power out of the 12V 1.5A socket in Kobuki. ", "I understand that I need to order this \"housing connector\" (DigiKey part number WM1783-ND ", "). However I'm a newbie in electronics. May I ask what I need to do next, so that it connects with a power cable where the other end is a standard DC male plug? ", "I understand I need the \"crimp pins\" and a \"crimp tool\", but I don't know exactly what to do and don't know how to connect it with a DC male plug.. Please help!", "I'm trying to power a small external LCD screen like ", "Instead of buying crimp pins and a crimper, you can also buy pre-crimped wires with the micro-fit crimp pins from digikey: ", " . Digikey sells these in a variety of lengths and colors. Then you just stick these in the two-pin housing, and hook the other end up to the DC plug for your screen."], "url": "https://answers.ros.org/question/236432/kobuki-12v-15a-connector/"},
{"title": "How to stream RGB and Depth at 30Hz simultaneously?", "time": "2016-05-23 20:13:50 -0600", "post_content": [" ", " ", " ", " ", "How to stream RGB and Depth at 30Hz simultaneously?", "I wrote two nodes which subscribe rgb and depth images. They can work at 30Hz when I run only one of the nodes.\nHowever, the performance reduces to 20Hz when I launched the two nodes together.", "How to solve the problem? Note that the openni_launch publishes the rgb and depth images at 30Hz all the time."], "answer": [" ", " ", "Hi,", "Transferring two ", " (such as RGB and Depth) at 30Hz can be a ", " and that is probably why the ", " to 20Hz. You can confirm this by launching your nodes and checking how much CPU is being consumed, it will likely be close to 100%.", "If performance is an issue for you, implement your nodes as ", " and register them into openni's nodelet manager to avoid the costly memory transfer between processes.   ", "I hope this helps.", "Thank you for answer. But I have a 16-core i7 CPU, it is so powerful that the CPU load is far from 100%. The subscribed images are written to SSD.\nDo you have any nodelets sample code? I am new to this area."], "url": "https://answers.ros.org/question/235049/how-to-stream-rgb-and-depth-at-30hz-simultaneously/"},
{"title": "subscribing rgb and depth simutaneously very slow", "time": "2016-05-25 10:26:29 -0600", "post_content": [" ", " ", "I am subscribing the rgb and depth images published by openni_launch at the same time.\nHowever, my problem becomes very slow and can only run at 15Hz.", "How to solve the problem?", "Please add more detail. What camera are you using, which node is the subscriber, etc...", "I am using the tum ", " code.\nI am using Asus Xtion."], "answer": [" ", " ", " ", " ", "Assuming you launch the openn_launch without throttling parameters on the same machine as rgbd_demo, my best guess would be that your computer is too slow to reproject the point clouds with more than 15 hz.", "In that case, one of your cpus should be near 100% while running rgbd_demo.", "Since you have exactly 15Hz (i.e. exactly half of the maximum), you might also have enabled some throttling parameter. Maybe you can configure them differently in the openni_launch launch-file or use dynamic_reconfigure to set it.", " Try increasing the subscriber queue size for each ", " (in lines 90 to 93). 2 should be sufficient, but 5 won't hurt either.", "I test the code on a powerful PC. When subscribing only RGB or depth, it can achieves 30Hz. However, when subscribing the two topics simultaneously, the rate drops to about 25Hz.", "The subscription is not computationally expensive, it's the reprojection that's done by rgbd_demo."], "answer_code": ["message_filters::Subscriber"], "url": "https://answers.ros.org/question/235201/subscribing-rgb-and-depth-simutaneously-very-slow/"},
{"title": "Which RGB-D sensor is better with ROS packages?", "time": "2016-07-06 21:06:00 -0600", "post_content": [" ", " ", "While looking for specifications of Kinect and Xtion sensors, I found various versions such as Kinect Version 2, Kinect for Xbox one, Kinect for Xbox 360, Xtion Pro Live and WAVI Xtion. I wanted to know which one of these sensors is better to use especially when using them with ROS."], "answer": [" ", " ", "I've only used the Xtion Pro Live and it works pretty well. I've seen a lot of people using Kinect v2 cameras, so there will be a lot of support for these. They are also higher resolution than the Xtion Pro live and the v1 so will be a bit better. I personally wouldn't buy one of the original Kinects, but it's up to you as they are pretty cheap.\nIf I was to buy one today I would buy a Kinect v2, because they have good resolution and a good community of support.", "Also note that the original kinects needed a separate power supply; the Xtions draw enough power from USB."], "url": "https://answers.ros.org/question/238961/which-rgb-d-sensor-is-better-with-ros-packages/"},
{"title": "invert an coordinate axis with tf", "time": "2016-06-22 21:45:06 -0600", "post_content": [" ", " ", " ", " ", "The coordinate of my robot and the bundled API is", "I want to map such that my base_link is the following", "(I think this is the most common base_link setup)", "How do I setup tf::Transform to invert the z axis? Do I use a 3x3 matrix to setup my transform to invert the axis? Possible to use the following 3x3 Matrix for the transformation? (tried using .setBasis but I got a bunch of tf warnings).", "My UAV uses an internal Left Handed frame (because they thought positive values for z for altitude was more intuitive). I want to switch it to Right Handed frame so that it works with ROS tf.", "The UAV is left handed axis is the following.", "x -> North\ny -> East\nz -> Up", "A right handed setup would have z-> Down"], "answer": [" ", " ", "According to ", ", all coordinate frames are right-handed. The \"standard\" ", " setup actually has x forward, y left, and z up. That's what I would suggest transforming your frames to.", "AFAIK, ", " has no support for left-handed frames. Some of the libraries with geometry support that are common in ROS (bullet, Eigen, transformations.py, KDL, etc.) may have some support for transformations with rotations not strictly in SO(3) (such as mirroring transformations), but unless you really have some good reason for getting into this, I think it's best to avoid.", "Since tf has no support for left-handed frames, how will I go about transforming my Robot's internal left-handed frame setup?", "Btw, my robot is drone. Typically z-axis should be pointing down, but the SDK has the z-axis pointing up (since they though negative values for height feels odd).", "Are you sure that the coordinate system for your UAV is left-handed? The description in the question makes it sound like the native system is right-handed, and the system you want to transform it to is left-handed.", " The UAV is from DJI and here is the frame setup  ", "Notice the \"*\", they switch the frame to left handed by inverting the height/z-axis. ", "I need transformation so I can switch the internal left handed frame to work with ros tf.", "Convention would say the green axis is the \"y axis\" not the \"z axis\". It looks to me like tf will not be able to natively handle any data coming from the API. I'd recommend manually converting any information into a right-handed coordinate system before broadcasting transforms.", "As explained in the document you can just reverse their change which is to make the Z direction positive downward by multiplying by -1 in Z only to make it right handed again. ", "The diagram does use odd colors standard is X red, Y green, Z blue.", "Currently I'm just inverting the z-axis data from the API before they get publish. So at least all the topics being published into ROS are right handed. However, I have to manually keep track of making API since the API are still left handed.", " ", " ", "You can build a rotation matrix or a quaternion that represents the change in orientation from one reference frame to the other reference frame. Then all you do is rotate your vector [x,y,z] with the quaternion or matrix.", "Cheers", "Problem is you can't invert an axis with rotations. My robot internally is internally mapped on left handed axis and I need to remap it to a traditional right handed axis. Go from [x y z] to [x y -z]. Only method I can think right now is manually add a \"-\" to whatever z data the robot spits out.", "Oh! Good! I was about to point out that your are asking for a left to right handed axis transformation. Ok, be careful with your reference frames. I would suggest to change your perspective to:", "x: North\ny: East\nz: Down", "Georeferenced systems used that reference frame (drones, rovers, planes).", "Can I use tf to convert a left handed frame to a right handed frame? I tried using tf with \"-\" for the z axis transformation between frames, but it only corrects my base_link. My odometery z-axis is still flipped. I'm thinking of editing the SDK to switch z to point down if possible.", "North-East-Down is a right handed frame! No doubt about it. The image you shared from DJI displays the drone from its back x axis pointing towards the front. X CROSS Y = Z, so it is a right handed frame. Use a rotation to convert between frames.", " Here is the documentation of the frame setup for the drone:  ", "Unfortunately the ground frame is left handed and tf doesn't seem to have a way to handle left handed frames. I'm leaning towards modifying the SDK to be right handed."], "question_code": ["x -> forward\ny -> right\nz -> down\n", "x -> forward\ny -> right\nz -> up\n", "tf::Transform transform;\n//setup transform\n\ntf_broadcaster.sendTransform(tf::StampedTransform(transform, time, \"base_link\", \"my_robot\"));\n", "[[1,0,0],\n [0,1,0],\n [0,0,-1]]\n"], "answer_code": ["base_link", "tf"], "url": "https://answers.ros.org/question/237828/invert-an-coordinate-axis-with-tf/"},
{"title": "How can I help AMCL deal with catastrophic wheel slips?", "time": "2016-06-19 01:52:00 -0600", "post_content": [" ", " ", "My robot uses an ASUS Xtion for a fake laser scan and odometry from encoders on the wheels. It basically is a big version of a turtlebot.", "For the most part it navigates fine, but sometimes a wheel will catch on something, like a lip on a rug or a threshold and spin badly. AMCL immediately thinks the robot has rotated and RVIZ shows that the map is in clear disagreement with the laser scan image of the surrounding walls.", "Is there some setting in AMCL that will help it to \"reset\" when the visual input is very different from the model?\nIn short, I'd like AMCL to assume the laser scan is right and the odometry is wrong when there is a huge disparity.\nAs it is now, if it gets too bad I often just have to shut down AMCL and start over.", "Note, this is for AMCL, where the map has been created and is not being updated.", "This picture might help explain what the issue is:\n", "You can clearly see how the corner of the room as seen by the 3D sensor is off from the corner of the map."], "answer": [" ", " ", " ", " ", "I used AMCL on a robot with Mecanum wheels, which have an enormous amount of slip. I used an IMU along with the laser_scan_matcher package in order to simulate the odom and then fed that to AMCL. ", "If you want to try with straight odometry, one thing you could try is increasing the covariance values in the odometry message. Covariance is a measure of uncertainty. If the uncertainty is high then AMCL should weigh the measurement less.", "[EDIT] I did not know that AMCL doesn't use covariance, so I guess that's out. ", "In regards to laser_scan_matcher, yes and no. Yes, you are using the laser scans twice, but you are using them for different things. AMCL uses scan matching to compare the current laser scans against a map to try and determine the sensor's pose on that map. Laser_scan_matcher also uses scan matching, but between successive scans in order to determine the amount of translation and rotation there is between scans. This is then converted into a transform between a user specified fixed frame and base frame. \nTLDR: amcl uses scan matching to find pose on a map, laser_scan_matcher uses scan matching to find pose relative to its initial position.", "Based on ", " and the ", ", I don't think that AMCL actually uses the covariance on the odometry topic. Am I wrong", "When you combine the IMU with the laser_scan_matcher and then feed it to AMCL, does that mean you are using the Laser Scan twice? This always confused me.", "Well, that I did not know. Oh well, there goes the covariance idea.", "My attempt to use the laser_scan_matcher resulted in a huge mess:\n", "I've never seen the green arrows spread so far and wide! I must be doing something very wrong.", " ", " ", "A practical way of doing this in any state estimation filter (which is what AMCL is) is to increase the process noise (noise in motion) relative to measurement noise (sensor noise). That's not as straightforward in AMCL. The way I've been doing it is by increasing the value of the odometry parameters (", ", etc.) But that affects the behavior of the filter at all times, not just when there's wheel slippage.", "Another way would be to fuse your raw odometry with another sensing modality, such as an IMU. If that's not an option, you can feed odometry into the ", " and have that broadcast the ", " to ", " transform (see the wiki page). I haven't tried this, but I'm planning to.", "Looking forward to other people's answers.", "Isn't feeding the odometry into the laser_scan_matcher and then using AMCL essentially using the laser scan twice?\nI guess if it works, there is nothing wrong with it.", "Yes, it's a bit weird. But they are using the LaserScans differently. One is doing laser scan matching and the other Particle Filtering using an inverse sensor model for the laser distance sensor."], "answer_code": ["odom_alpha_1", "odom", "base_link"], "url": "https://answers.ros.org/question/237453/how-can-i-help-amcl-deal-with-catastrophic-wheel-slips/"},
{"title": "How does MSF estimate heading ?", "time": "2016-07-11 15:54:35 -0600", "post_content": [" ", " ", "Hi All, ", "I am relatively new to MSF and am trying to set it up for estimating the pose(absolute) of a ground vehicle. There are two update pose sensors (in addition to IMU for prediction step). One pose sensor provides absolute pose measurements in the world frame and the second one provides a relative pose (odometric data). The setup seems to work okay but the heading seems to be off. More precisely, the movement of the robot seems to be interpreted along the global frame and not the local frame. To clarify, a ", " velocity translates the robot along the global ", " axis and not the local ", " axis. \nI know this is not a lot to debug the problem, but is there something that I am doing wrong ? If some information/links could be provided as to the part where heading is estimated and how it is being estimated, it would be really helpful. Thanks.", "-Abhishek", "It would help to have a little more information about your system - ROS distro, launch files, bag files etc. But first, check that your local and global frames have the same origin and orientation - if they don't the odometry can be very different in each one.", "Distro: Indigo, \nlaunch file being used: position_pose_sensor.launch (supplied with MSF).\nThe data is composed of: pose data from wheel encoders on a simulated robot and absolute pose data from vicon. \nThe part about the global and local frames having same origin and orientation is satisfied."], "answer": [" ", " ", " ", " ", "This isn't a proper answer sorry but it's too long an explanation for comments and it may give you some leads to look into.", "I don't have any personal experience with ethzasl (I'm using the ", " package). But what I can say is that I see similar behavior quite a bit, and it's usually because sensor data is being converted to odometry in a reference frame, and somewhere along the way an angular offset is added to the data. ", "For example if you line up your robot with the 'X' axis of your global frame and drive it in a straight line but your odometry in that frame shows the robot travelling at an angle to the 'X' axis. E.g:", "My global, local and GPS frames line up with the 'X' and 'Y' axis, but my global (red) and GPS (green) data is reported at an angle. ", "Is this similar to what you're seeing? I suggest you try something similar, drive your robot in a straight line outside and plot the odometry in each frame in Excel. ", "If this looks familar it's usually because your IMU heading has been distorted (nearby metal objects will do that) or because yaw-related parameter has been set incorrectly. Is one of your sensors a GPS? If so then it will almost certainly be doing some sort of geodetic conversion to produce an absolute pose - and for such conversions it's critical that parameters such as your magnetic declination and yaw offset are correct (as per ", " and ", ", most IMU's read 0 when pointing North, whereas ROS expects them to read 0 pointing East). If you're using a provided launch file your robot may not report sensor data in the way the launch file expects it to.", "I'm sorry I can't give you more specific advice, but hopefully this will give you some things to check. And in general, the more information you can provide about the problem the easier it is for others to debug - if you could post some bag files or graphs of your odometry similar to the one above that would help."], "answer_details": ["I would make sure ethzasl sensor fusion works on Indigo because from the ", " it looks like Groovy is the latest distribution that's supported, so there may be compatibility issues (hopefully someone more familiar with ROS than me can tell you). ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/239277/how-does-msf-estimate-heading/"},
{"title": "Troubles with autonomouse mapping with nav2d and turtlebot 2 - StartExploration Error", "time": "2016-07-15 15:43:02 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I just started with ros and don't have much experience.\nI already installed the the nav2d package.", "Like in the tutorial described, I started the command rosservice call /StartMapping 3 and the robot drives 1 meter forward and makes a 180 turn. I don't know if that is correct. \nThen I start rosservice call /StartExploration 2 and then the following error is given:", "Failed to compute odometry pose, skipping scan (\"base_laser_link\" passed to lookupTransform argument source_frame does not exist. )", "\n  [ERROR] [1468626044.630942697]: You must specify at least three points for the robot footprint, reverting to previous footprint.", "\n  [ERROR] [1468626142.281016628]: Is the robot out of the map?", "\n  [ERROR] [1468626142.281070197]: Exploration failed, could not get ent position.", "In my ", " I can't find the base_laser_link.", "You can find my rqt_graph ", ".", "I start my TurtleBot with minimal.launch and I also start my 3D sensor. After that, I start the following launch file.\nMy tutorial3 launch file is:", "There is also a problem with the rviz virtualization. currently it looks like ", ".", "I hope somebody is able to help me because other questions and answers in this forum didn't help me.", "Edit 25. July 16:\nThe new (error) messages are:", "The new tf-tree looks like ", ". The map and offset frame disappeared.", "\nHere you can also find my ros.yaml file:", "Edit 26. July 2016 ", "\nIn my config files aren't configuration regarding the footprint.\nThe footprint error is really strange because it doen't appear at the first launch of minimal.launch, 3dsensor.launch and tutorial3.launch. At ...", "Sadly, your error message is not there.", "oh sorry, I added it.", " As developer of the nav2d package maybe you can help me."], "answer": [" ", " ", " ", " ", "You have to set all frame-names according to your robot setup. In your tf-tree the laser frame is called \"laser\", so you have to set the parameter \"laser_frame\" in the ros.yaml parameter file from nav2d to just \"laser\".", "It looks like you are defining a footprint somewhere that is not correct. As the operator can only handle robot radius (as defined in costmap.yaml), you should check your yaml files and remove this definition. \"rosparam list\" might also be helpful.", "But actually I don't think that this is really your problem. This all happens within the Operator and from your screenshot it seems to be fine. (blue and green trajectory indicator are there) Are there any other error messages? Have you visualized the /scan topic to see if it contains useful data?", "In general, I would suggest to follow the tutorials step by step, so:", "Thanks for your answer ", ". I changed the laser_frame to laser but it seems that this kills the map and offset frame. --> I added the new tf-tree to the question.\nI think that's the reason for not receiving a map.", "\nI edited my question regarding your edit.", "When I run ", " the robot drives through the room without stopping. Normally the robott should do a 360 turn as described in your tutorial.", "I started rviz with the rviz config file of the gmapping demo and got a costmap as you can see in the picture.", "Almost all your laser points are outside your costmap, this cannot work. You should really setup your system step by step and test everything before you continue with the next one. You cannot debug the whole system at once.", "Should I follow the 3 steps you supposed or do you mean to set up the complete robot?", "I mean setting up the robot. Add all the components step be step and verify each before you go on. And if you then run into any troubles, you can ask here,"], "answer_details": ["Add Operator, check that costmap is there and the robot can move safely.", "Add Mapper, check that a global map is build and localization is good", "Add Navigator, check navigation and autonomous exploration", " ", " ", " ", " "], "question_code": ["<param name=\"use_sim_time\" value=\"false\" />\n<rosparam file=\"$(find nav2d_tutorials)/param/ros.yaml\"/>\n\n<!-- Start the Operator to control the simulated robot -->\n<node name=\"Operator\" pkg=\"nav2d_operator\" type=\"operator\" >\n    <!-- <remap from=\"scan\" to=\"base_scan\"/> -->\n            <remap from=\"cmd_vel\" to=\"mobile_base/commands/velocity\"/>\n    <rosparam file=\"$(find nav2d_tutorials)/param/operator.yaml\"/>\n    <rosparam file=\"$(find nav2d_tutorials)/param/costmap.yaml\" ns=\"local_map\" />\n</node>\n\n<!-- Start Mapper to genreate map from laser scans -->\n<node name=\"Mapper\" pkg=\"nav2d_karto\" type=\"mapper\">\n    <!-- <remap from=\"scan\" to=\"base_scan\"/> -->\n    <rosparam file=\"$(find nav2d_tutorials)/param/mapper.yaml\"/>\n</node>\n\n<!-- Start the Navigator to move the robot autonomously -->\n<node name=\"Navigator\" pkg=\"nav2d_navigator\" type=\"navigator\">\n    <rosparam file=\"$(find nav2d_tutorials)/param/navigator.yaml\"/>\n</node>\n\n<node name=\"GetMap\" pkg=\"nav2d_navigator\" type=\"get_map_client\" />\n<node name=\"Explore\" pkg=\"nav2d_navigator\" type=\"explore_client\" />\n<node name=\"SetGoal\" pkg=\"nav2d_navigator\" type=\"set_goal_client\" />\n\n<!-- RVIZ to view the visualization -->\n<node name=\"RVIZ\" pkg=\"rviz\" type=\"rviz\" args=\" -d $(find nav2d_tutorials)/param/tutorial3.rviz\" /></launch>\n"], "url": "https://answers.ros.org/question/239616/troubles-with-autonomouse-mapping-with-nav2d-and-turtlebot-2-startexploration-error/"},
{"title": "usb_cam exit code-11 on arm firefly rk3288", "time": "2016-07-19 05:26:13 -0600", "post_content": [" ", " ", " ", " ", "i launch the example usb_cam-test.launch on my computer it works, but when it turns to the arm firefly rk3288 i got the error:", "can somebody help with this problem? In fact i have reflash the arm and reinstaller all , but i still got the same error", "This is a duplicate of ", ". Could you close either this one, or the other question?", "i ve deleted the other one, thank you", "I am facing the same problem. Did you fix it?", "Well, I gave up using this package. I was not able to use my cameras. So, I tested the GSCAM and it worked flawlessly. Anyway, thank you for your response!", "in fact i found that not a pb of current , it is because i changed a camera , the camera i use for now is logitech, can u pls tell me you use gscam with indigo or other?", "I downloaded the GSCAM github files, added them to my catkin workspace, and compiled using catkin_make. I am using ros Indigo and it is working fine. I am using two Logitech cameras."], "answer": [" ", " ", "yes, its because i used a usb switch which is connected to the arm and connected the keyboard the camera and etc on the usb switch -> to the arm, so it leads to the current of arm not enough, the solution is use a usb hub which is connected to other power supply, sry for bad english , hope it helps"], "question_code": ["firefly@firefly:~$ roslaunch usb_cam usb_cam-test.launch \n... logging to /home/firefly/.ros/log/9ab8650a-4d99-11e6-8ca2-00904c112233/roslaunch-firefly-2096.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://firefly:38621/\n\nSUMMARY\n========\n\nPARAMETERS\n * /image_view/autosize: True\n * /rosdistro: indigo\n * /rosversion: 1.11.20\n * /usb_cam/camera_frame_id: usb_cam\n * /usb_cam/image_height: 480\n * /usb_cam/image_width: 640\n * /usb_cam/io_method: mmap\n * /usb_cam/pixel_format: yuyv\n * /usb_cam/video_device: /dev/video0\n\nNODES\n  /\n    image_view (image_view/image_view)\n    usb_cam (usb_cam/usb_cam_node)\n\nROS_MASTER_URI=http://localhost:11311\n\ncore service [/rosout] found\nprocess[usb_cam-1]: started with pid [2114]\nprocess[image_view-2]: started with pid [2115]\n[ INFO] [1468923724.026820452]: Using transport \"raw\"\n[ INFO] [1468923724.095676535]: using default calibration URL\n[ INFO] [1468923724.095989785]: camera calibration URL: file:///home/firefly/.ros/camera_info/head_camera.yaml\n[ INFO] [1468923724.096226618]: Unable to open camera calibration file [/home/firefly/.ros/camera_info/head_camera.yaml]\n[ WARN] [1468923724.096343285]: Camera calibration file /home/firefly/.ros/camera_info/head_camera.yaml not found.\n[ INFO] [1468923724.096473660]: Starting 'head_camera' (/dev/video0) at 640x480 via mmap (yuyv) at 30 FPS\n[ WARN] [1468923724.172862035]: unknown control 'white_balance_temperature_auto'\n\n[ WARN] [1468923724.180684535]: unknown control 'focus_auto'\n\n[usb_cam-1] process has died [pid 2114, exit code -11, cmd /opt/ros/indigo/lib/usb_cam/usb_cam_node __name:=usb_cam __log:=/home/firefly/.ros/log/9ab8650a-4d99-11e6-8ca2-00904c112233/usb_cam-1.log].\nlog file: /home/firefly/.ros/log/9ab8650a-4d99-11e6-8ca2-00904c112233/usb_cam-1*.log\n\nand i tried to run the node usb_cam_node, the result is segment fault:\nfirefly@firefly:/opt/ros/indigo/lib/usb_cam$ rosrun usb_cam  usb_cam_node \n[ INFO] [1468923818.447067913]: using default calibration URL\n[ INFO] [1468923818.447372997]: camera calibration URL: file:///home/firefly/.ros/camera_info/head_camera.yaml\n[ INFO] [1468923818.447627038]: Unable to open camera calibration file [/home/firefly/.ros/camera_info/head_camera.yaml]\n[ WARN] [1468923818.447767038]: Camera calibration file /home/firefly/.ros/camera_info/head_camera.yaml not found.\n[ INFO] [1468923818.447925997]: Starting 'head_camera' (/dev/video0) at 640x480 via mmap (yuyv) at 30 FPS\n[ WARN] [1468923818.537832538]: unknown control 'white_balance_temperature_auto'\n\n[ WARN] [1468923818.552972955]: unknown control 'focus_auto'\n\nSegmentation fault\nfirefly@firefly:/opt/ros/indigo/lib/usb_cam$\n"], "url": "https://answers.ros.org/question/239813/usb_cam-exit-code-11-on-arm-firefly-rk3288/"},
{"title": "UbuntuARM on Raspberry B+ [closed]", "time": "2016-07-24 16:09:09 -0600", "post_content": [" ", " ", "I have the raspberry pi B+ (1.0, the one before the 2) and there are different instructions for installation on the B+. Why cannot I use the UbuntuARM installation on the B+? From my (limited) knowledge, the underlying framework is the same, but the 2 just has a better processor. What gives? "], "answer": [" ", " ", "Never Mind, I figured out that the ROSBerryPi method is REQUIRED when installing on Raspbian, the UbuntuARM only works if you can run pure Ubuntu, which the B+ cannot because of processing power. "], "url": "https://answers.ros.org/question/240257/ubuntuarm-on-raspberry-b/"},
{"title": "ROS on raspberry pi Zero", "time": "2016-08-06 11:18:49 -0600", "post_content": [" ", " ", "Hi, I have an Ubuntu 14.04 laptop which I use at work with ROS Jade installed in it, so I'm not allowed to do any changes to it. I recently bought a Raspberry Pi Zero (with Raspbian Lite) and wanted to install ROS in it, but I don't know if I will be able to use the same packages I use in my laptop inside the raspberry.", "The idea is to have a package in my laptop to do stuff, another one in the RPI and another one, which will be shared between both computers, that will handle all the WiFi communications between them.", "I couldn't find any info saying if it's ok to install Jade in the PiZero, so I wanted to make sure before starting doing it. \nMaybe I should also note that my idea is to add an Arduino into the setup."], "answer": [" ", " ", "It should be possible, only concern is if zero will be powerfull enough for what you want to do and packages...", "if you can find repository with packages, you should have no problems.\nIf you can't find, it will be tricky, because you need to compile ros from source and some packages take over 1GB RAM easily, which means you have to use swap on zero.... and that will make it slow as hell (not to mention that CPU itself will compile everything for a long time)... also, you propably want to put swap file on flashdrive over usb, rather than killing microsd ...", "What if I install Kinetic instead of Jade? Will have much problems to make it work?", "it should be easier, kinetic is at least targeted for debian jessie, just different architecture (it means you won't have to figure out all dependencies for ros to solve), on the other hand there is still compilation time...", "And I am actually blocked in making reliable raspbian repository ..."], "url": "https://answers.ros.org/question/241214/ros-on-raspberry-pi-zero/"},
{"title": "Gyro and Odometry Calibration Not Effective?", "time": "2011-10-25 16:59:54 -0600", "post_content": [" ", " ", " ", " ", "I have a turtlebot, which was obtained assembled from Clearpath Robotics.  I have been trying to run the Gyro and Odometry Calibration, and have been getting fairly large corrections for the gyro scale correction multiplier.  To test these results, I have been setting the corrections with rosparams, and then re-running the calibration.  I expect that on the next run, the corrections multiplier will be closer to 1.0, but it always remains in the range of 1.25-1.5 . The raw gyro value from the dashboard is about 509, and seems to vary appropriately when the robot is manually twisted.  Any ideas?", "Here is what typical run looks like for me:", "hi, can you please share your launch file of calibration? thanks!"], "answer": [" ", " ", "The turtlebot_node will not read from the parameter server when the parameters change unless you restart the node or use dynamic_reconfigure. If you would like to change the parameters while the turtlebot_node is running please use dynamic_reconfigure, which forces turtlebot_node to read from the parameter sever again. ", " ", " ", "I can think of two things which could be a problem.  ", "First make sure that the roscore is still running between executing the rosparam calls and launching the calibration so that the parameters persist.  ", "Secondly, I can't tell what are your parameters before you run the calibration?  Make sure you're multiplying the result by the previous value not just overriding it.  ", " ", " ", "This is what I have done with turtlebot's calibration, and finally got a good map building: ", "This is my result after 8 times running calibration: \nMy Gyro part number is ADXRS652 ( setting is 250 )", "Remember edit turtlebot.launch file with 2 new parameters.    ", " ", " ", "I tried to follow the tutorial and the dynamic reconfigure method.\nWhat both of them did was they changed the values of both ros parameters...", "Seeing this had no result, I tried to echo rostopic \"/turtlebot_node/parameter_descriptions\", the values were not changed. But it was modified under \"/turtlebot_node/parameter_updates\", but there was no effect of the calibration.\nThe method seems to be ineffective.", "So I tried to find out, which file is setting the value when the node starts. The file was : /opt/ros/diamondback/stacks/turtlebot/turtlebot_node/src/turtlebot_node/cfg/TurtleBotConfig.py \nI modified the correction values there. This method helps in changing the correction values. ", "But my gyro calibration is still wrong. It got better from last time. But still, I cant get a good result. ", "Anybody has any clue on this ?", " ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "Hi, have the same problem and now is solved .\nI have the turtlebot that I bougth from Irobotics.", "I tried with different ros versions (electric, fuerte), and several different installations.\nThe maps were a disaster, and each time I try a calibration, always told me to \ngyro_scale_correction of more than 1.2 multiplication, also \nwhile the bot was doing the calibration sometimes I see a message:\nImu error: -30 percent, or similar.", "I have a Willow Garage power board . this one http://store.iheartengineering.com/ihe-0200-0000-fa00.html", "but neves i was able to identify any chip number there.", "What it worked: \n1- using the dinamically configuration for the gyro calibration :\n   a) Set the Gyro measurement range on 250\n   b) Set Gyro_scale_correction  on 2.4\n   c) run the calibration, and now all is good, with small percent changes", "but every time I restart the turtlebot service or restart the laptop, I have to Dinamcally set the gyro rate from 150 to 250,   I changet this paramater also in /opt/ros/fuerte/stacks/turtlebot/turtlebot_node/src/turtblebot_node/\ngyro.py ", "\nthe parameter from 150 to 250. ", "\nbut doesnt read it.   any suggestion to leave this parameter working on my bot?", "cheers\nezex", "See the final instructions on the Tutorial for how to make the parameters persist: ", "\u4f60\u5728minimal.launch\u91cc\u9762\u4f7f\u7528", "\u6807\u7b7e\u8bbe\u5b9aGyro_scale_correction\u7684\u503c\u4e3a2.4\uff0cGyro measurement range\u7684\u503c\u4e3a250\u5c31\u53ef\u4ee5\u4e86"], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Firstly run ", ", check your part number of your Gyro then change. Keep other parameters same. ", "put your turtlebot in front wall, ", ", then you will get multiply values for Gyro_scale_correction and Odom_angular_scale_correction. Do multiplying.", "rerun ", ", set a new Gyro_scale_correction and Odom_angular_scale_correction. Back to step 2 until you get both multiply values are close to 1. ", "Gyro_scale_correction : 1.4472", "Odom_angular_scale_correction: 1.04", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["turtlebot@turtlebot-0284:~$ rosparam set /turtlebot_node/gyro_scale_correction 1.43\nturtlebot@turtlebot-0284:~$ rosparam set /turtlebot_node/odom_angular_scale_correction .943\nturtlebot@turtlebot-0284:~$ roslaunch turtlebot_calibration calibrate.launch\n... logging to /home/turtlebot/.ros/log/ce9563ee-ff80-11e0-947c-485d60755842/roslaunch-turtlebot-0284-2218.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://10.0.12.151:41995/\n\nSUMMARY\n========\n\nPARAMETERS\n * /openni_camera/depth_time_offset\n * /openni_camera/image_mode\n * /kinect_laser/output_frame_id\n * /openni_camera/depth_rgb_translation\n * /pointcloud_throttle/max_rate\n * /openni_camera/depth_mode\n * /openni_camera/shift_offset\n * /scan_to_angle/min_angle\n * /kinect_laser/min_height\n * /openni_camera/depth_rgb_rotation\n * /kinect_laser_narrow/max_height\n * /scan_to_angle/max_angle\n * /kinect_laser/max_height\n * /rosdistro\n * /openni_camera/projector_depth_baseline\n * /kinect_laser_narrow/output_frame_id\n * /rosversion\n * /openni_camera/debayering\n * /openni_camera/depth_frame_id\n * /openni_camera/image_time_offset\n * /openni_camera/depth_registration\n * /kinect_laser_narrow/min_height\n * /openni_camera/rgb_frame_id\n\nNODES\n  /\n    kinect_breaker_enabler (turtlebot_node/kinect_breaker_enabler.py)\n    openni_manager (nodelet/nodelet)\n    openni_camera (nodelet/nodelet)\n    pointcloud_throttle (nodelet/nodelet)\n    kinect_laser (nodelet/nodelet)\n    kinect_laser_narrow (nodelet/nodelet)\n    scan_to_angle (turtlebot_calibration/scan_to_angle.py)\n    turtlebot_calibration (turtlebot_calibration/calibrate.py)\n\nROS_MASTER_URI=http://10.0.12.151:11311\n\ncore service [/rosout] found\nprocess[kinect_breaker_enabler-1]: started with pid [2240]\nprocess[openni_manager-2]: started with pid [2241]\nprocess[openni_camera-3]: started with pid [2242]\nprocess[pointcloud_throttle-4]: started with pid [2243]\nprocess[kinect_laser-5]: started with pid [2245]\nprocess[kinect_laser_narrow-6]: started with pid [2252]\nprocess[scan_to_angle-7]: started with pid [2254]\nprocess[turtlebot_calibration-8]: started with pid [2269]\n[INFO] [WallTime: 1319602903.535752] has_gyro True\n[INFO] [WallTime: 1319602903.646246] Estimating imu drift\n[INFO] [WallTime: 1319602903.947774] Still waiting for imu\n[INFO] [WallTime: 1319602904.249399] Still waiting for scan\n[INFO] [WallTime: 1319602904.551117] Still waiting for scan\n[kinect_breaker_enabler-1] process has finished cleanly.\nlog file: /home/turtlebot/.ros/log/ce9563ee-ff80-11e0-947c-485d60755842/kinect_breaker_enabler-1*.log\n[INFO] [WallTime: 1319602904.853371] Still waiting for scan\n[INFO] [WallTime: 1319602905.154970] Still waiting for scan\n[ INFO] [1319602905.218424205]: [/openni_camera] Number devices connected: 1\n[ INFO] [1319602905.219076453]: [/openni_camera] 1. device on bus 001:14 is a Xbox NUI Camera (2ae) from Microsoft (45e) with serial id 'A00362A03027048A'\n[ WARN] [1319602905.222852677]: [/openni_camera] device_id is not set! Using first device.\n[ INFO] [1319602905.275323325]: [/openni_camera] Opened 'Xbox NUI Camera' on bus 1:14 with serial number 'A00362A03027048A'\n[ INFO] [1319602905.321096537]: rgb_frame_id = 'kinect_rgb_optical_frame' \n[ INFO] [1319602905.327575678]: depth_frame_id = 'kinect_depth_optical_frame' \n[INFO] [WallTime: 1319602905.456490] Still waiting for scan\n[INFO] [WallTime: 1319602905.758331] Still waiting for scan\n[INFO] [WallTime: 1319602906.061055] Still waiting for scan\n[INFO] [WallTime: 1319602906.362886] Still waiting for scan\n[INFO] [WallTime: 1319602906.664503] Still waiting for scan\n[INFO] [WallTime: 1319602917.274803] Still waiting for imu\n[INFO] [WallTime: 1319602917.577144] Still waiting for scan\n[INFO] [WallTime: 1319602917 ..."], "answer_code": ["rosrun dyanmic_reconfigure reconfigure_gui\n"], "url": "https://answers.ros.org/question/11687/gyro-and-odometry-calibration-not-effective/"},
{"title": "The parameters of the sensor itself", "time": "2016-10-17 21:03:40 -0600", "post_content": [" ", " ", " ", " ", "Hello.", "Is there any way to get  the parameters of the sensor itself, such as vendor,resolution,power,etc.", "Thank you."], "answer": [" ", " ", "In general, unless the driver writer has chosen to expose this information, vendor and power won't be available.  If it is available, it tends to be in a diagnostics message.  For instance, in the Hokuyo driver ", " here is where the information is populated: ", "Depending on the type sensor, resolution is available either in the ", " or "], "answer_code": ["urg_node", "sensor_msgs/CameraInfo", "sensor_msgs/LaserScan"], "url": "https://answers.ros.org/question/245891/the-parameters-of-the-sensor-itself/"},
{"title": "Suggest a wheel encoder", "time": "2016-09-12 08:06:15 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I want to buy wheel encoders to fix with the wheels of a car.", "Can anyone suggest simple encoders that they have already bought and interfaced with ROS?", "It would be of great help if you could share your code too?", "Thanks & Regards\nSriram"], "answer": [" ", " ", "The simplest setup is to use motors with wheel encoders built-in, or motors for which stock encoder modules exist. Barring that, the best setup is if you have access to the back shaft of the motor (before the gearing). You should be able to set up an optical or Hall-effect encoder on that side.", "If you have an actual car and want encoders for the wheels, you could either go an optical route, with some sensor mounted on the axle looking at stripes mounted on the wheels somehow, or perhaps Hall-effect magnetic sensors. But if you have an actual car, you might find it easier to read the existing odometer cable and combine that with the amount of steering to infer the pose changes.", "Hi Mark,", "Thanks for the answer. Yes as you said reading the odometry cable is the bes choice but unfortunately that is not an option I have. I'm working with an actual electric car, so its powered my a motor with an built-in ecoder. Will reading that give me an accurate distance measurement?", "I'll write a program to calucate the distance using the encoder ticks. Or should I use a seperate encoder as you suggested and interface it with the backshaft?", "If you have access to the motor encoders, that would be best. I have mostly worked on differential-drive systems, so there is a simple strategy for using both encoders. I presume your car has Ackerman drive. The math is different, but if you have steering, then slippage should be small.", "As far as using the encoders, you'll get two bits of input, call them A and B. They will cycle through 00-->01-->11-->10-->00, or the opposite order. If you concatenate the old and new inputs into a single 4-bit value, you can do if-else or switch to determine whether to increment or decrement.", "Alternatively, if you get interrupts only on changes to the A input, then code like this will suffice (or might have the wrong sense, depending on your wiring):", "There are also packages to do this, depending on your hardware.", "Hi,\nThanks a lot again. I'll first look into to the encoders and try to code them as you suggested and get back to you when I have some results", "One more thing: your motor controller may already be reading the encoder inputs, and there may be a way of reading a cumulative value from the controller. (Don't know anything about your hardware, of course.)", "Hi Mark,", "really sry for the late reponse. I finally got to know more abt the controller. It is a Curtis 1236 motor controller and it is communicated with curtis 1314 programming software which uses VCL. \nComing back to your question, yes the encoders are already connected to the controller."], "answer_code": ["if (A == B) {\n    ++ticks;\n} else {\n    --ticks;\n}\n"], "url": "https://answers.ros.org/question/243566/suggest-a-wheel-encoder/"},
{"title": "recall multiple ROS nodes in one main cpp ROS node", "time": "2016-09-24 06:25:57 -0600", "post_content": [" ", " ", " ", " ", "Hello, \nI'm using ROS indigo to fly my Erle Copter, \nI want to ask you about ROS nodes,", "I have several ROS nodes: \none of these nodes will take reading from one of the sensor and do some calculations, and based on the sensor readings the Copter (UAV) will know its direction.", "My question is, if I want to run the ROS node that is responsible of sensor readings after reaching one of the waypoints, how can I call that ROS node so it works only once the UAV reaches the waypoint and move?\nand then move to the second waypoint and run the same node again", "so recalling ROS node at specific time, can it be achieved  and how?", "I hope that you can understand my question since I didn't find what I want in the tutorial, all what I found is how to publish and subscribe in another one, but its not explained how to run a ROS nodes inside another ROS node (main one) "], "answer": [" ", " ", " ", " ", "If I understood your question, it seems to me that you are looking to this problem from the wrong perspective, you can solve it even with the use of simple topics and services without having to use Actionlib (which is a bit more powerful and complicated).", "Have you asked yourself how does a mobile robot navigates or plans a path? (if not, please search for it). It is not difficult, just create a structured architecture where your nodes communicate with each-other (publish/subscribe pattern), where there must be a node that plans your path and a node that takes care of the odometry (position), so, the node that senses the location (odometry) must be publishing the position all the time. In the path planner node, use an if statement inside the listener subscribed to the topic that is publishing the odometry (odom_pose) to make sure that you will generate the new goal only when odom_pose is close enough to the previous goal_pose.", "A better question could be more helpful.", " ", " ", " ", " ", "I am not really sure what you want, but I think you should have a look at actions. ", "  This allows a node to request another node to solve a task or run an algorithm. In your case one node could tell another one which waypoint was reached and that the node should now start to compute and publish the estimated pose.", " ", " ", " ", " ", "What ", " said is true, but if I understand your question correctly, you might also go this way:", " ", " ", "It is possible but messy using terminal command:", "if you put those line of code in a terminal file, you can run it and it will perform your node every 45 seconds."], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "make UAV reach a waypoint. Once it reached it, post info about it on a topic", "in the other node (the one responsible for sensor readings), subscribe to that topic and register a callback function. As soon as the first node published to that topic, that callback function would be executed.", " ", " ", " ", " ", "mkdir ~/bin", "put your line of code inside a simple file, let's say ", "chmod +x ", "export PATH=$PATH:~/bin in your ~/.bashrc", "then in another terminal just type waypoint", " ", " ", " ", " "], "answer_code": ["rosrun navigation waypoints\nsleep 45\nrosnode kill waypoints\nrosrun navigation waypoints\nsleep 45\nrosnode kill waypoints\n"], "url": "https://answers.ros.org/question/244374/recall-multiple-ros-nodes-in-one-main-cpp-ros-node/"},
{"title": "Setting up Schunk LWA3 on indigo, cannot initialize the robot.", "time": "2015-03-23 12:15:26 -0600", "post_content": [" ", " ", "Hi,", "I was wondering what is the proper way to set up Schunk LWA3 arm (older, gold colored modules)?", "I'm currently using the code in the schunk_robots library (with the necessary dependencies installed), and I am unable to initialize my arm. So I guess I have a several questions:", "Would the same code as what is in schunk_modular_robotics and schunk_robots for the LWA4d work for an LWA3?", " I'm using a Peak usb-can adapter, and right now I'm using it with the default drivers as recommended ( ", " ), but because this is an older version of the arm, is this the right thing to do? Or should I compile the pcan driver (with chardev?)? ", "I'm also wondering if I should be using the schunk_powercube_chain instead, but there is no documentation or code samples for it for indigo, and the last version which calls it is schunk_bringup in groovy . . .", "Thank you for your help, I'm a little new at working with real hardware.", "-------  details -------", "(please let me know if there is any other information I should provide)", "I have an older LWA, with only 6 joints (I've modified the relevant files, correctly, I think)", "I'm on Ubuntu 14.04, using ROS Indigo", "I'm using the pcan-usb adapter, with the following initialization:", "the following drivers are loaded:", "when I try to run robot.launch I get the following:", "when I attempt to initialize the robot (I tried different node ids for the first module (3/4/5/6))", "other terminal window:", "Hello, where you ever able to get this working?", "Yes, I was. It was quite a while ago, so I would need to look over the code and my notes, but if you have questions about getting the older Schunks running I'll do my best to answer.", "Thank you so much for the response. The main issue I have is initializing the arms and what the order of operations (commands) needs to be to launch the proper nodes. The powercube node crashes saying that ros params are not set ( Im pretty sure im setting them properly). Any help is awesome!", "Sure, I'm back at that lab next week, so I can take a look. But for now, if you cab post how you're launching it (launch files, any yaml param files etc) I'll be able to compare.", "Also, are you using a specific CAN adapter, or have dedicated can hardware?", "Hello, im back in the lab now. The current set up is Schunk LWA3, PCAN PCIe card (ive tried with and without the drivers, also as net and char devices), ROS Kinetic.\nThe process that I have been trying is powering on the arm, roscore, rosrun canopen driver, rosrun powercube", "Hello, so I have made a lot of changes. Essentially I have shcunk_bringup launching powercube and the joint stuff, then I use rosservice call /arm_controller/init. the Pcan device initializes fine but then I get a -207 error when it tries to write to module. I think its  an error with the module IDs", "Sorry for the large delay between replies. So, I haven't ran the robot since indigo, so all the code is as it was then.", "I ran it with the char device (using pcan drivers)", " launching with   ", " I also remember having to make changes to some parts of libm5api. I made a repo for that code (look through the commits, there are only a few), keep in mind this was a long time ago, so . . . the code is a bit of a mess:  ", "let me know if any of this helps."], "answer": [" ", " ", "Hi dignakov,", "the good package to start would be ", " but you should consider that this package will user ", " (CANOpen protocol) to communicate with the arm. If your arm/modules uses M5-Protocol than you should use shunk_powercube_chain (one updated version is ", "). If your arm is using SM-Protocol than probably you should implement communication to it by yourself (I don't know any package providing this).", "Hope it helps a bit (even though I am probably to late),", "Denis"], "question_code": ["    sudo ip link set can0 up type can bitrate 500000\n", "    lsmod | grep can\n\n      can_raw                17120  0 \n\n      can                    36575  1 can_raw\n\n      can_dev                20764  1 peak_usb\n", "    core service [/rosout] found\n    process[arm/robot_state_publisher-1]: started with pid [3921]\n    process[arm/driver-2]: started with pid [3922]\n    process[arm/joint_state_controller_spawner-3]: started with pid [3923]\n    process[arm/cob_control_mode_adapter_node-4]: started with pid [3924]\n    [ INFO] [1427125862.856646100]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n    process[arm/joint_states_relay-5]: started with pid [3992]\n    [ INFO] [1427125867.860408810]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n", "    rosservice call /arm/driver/init\n\n    success: \n      data: False\n    error_message: \n      data: could not reset node '3'\n", "    [ INFO] [1427128285.876938150]: Initializing XXX\n    [ INFO] [1427128285.877114246]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n    [ INFO] [1427128285.877209232]: Current state: 2 device error: system:0 internal_error: 0 (OK)\n    [ INFO] [1427128286.816488654]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n    [ INFO] [1427128291.820936133]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n    SHUTDOWN\n    [ INFO] [1427128295.978245127]: Current state: 2 device error: system:125 internal_error: 0 (OK)\n    [ INFO] [1427128295.978322473]: Current state: 2 device error: system:0 internal_error: 0 (OK)\n    [ INFO] [1427128295.978352019]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n    [ INFO] [1427128295.978370652]: Current state: 0 device error: system:0 internal_error: 0 (OK)\n    SHUTDOWN\n    [ INFO] [1427128296.823728257]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n    [ INFO] [1427128301.827175893]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting ..."], "url": "https://answers.ros.org/question/205693/setting-up-schunk-lwa3-on-indigo-cannot-initialize-the-robot/"},
{"title": "DIN-USB cable", "time": "2016-09-17 10:16:43 -0600", "post_content": [" ", " ", "Dear ROS users,", "i'm new on ROS platform, and i'm buying roomba 500 series in order to try ROS.\nnow i'm checking where can i buy the DIN-USB cable.", "can anyone help me?", "thank you\nThomas"], "answer": [" ", " ", "If you want to avoid assembling your own cable, you can buy one ", ". I am not aware of other options, but I'd be interested to know. This will allow you to communicate to the Roomba from a computer. ", "\nHowever, if you are interested in powering devices from the Roomba's battery you can see my ", " about making your own."], "url": "https://answers.ros.org/question/243928/din-usb-cable/"},
{"title": "Subscribing to a Bool msg via rosserial_arduino.", "time": "2016-09-07 14:51:53 -0600", "post_content": [" ", " ", "Hello All!", "I'm in the process of building a tool for a robot that I'm working with and we are trying to use an Arduino to communicate with ROS to control the end effector. We are right now broadcasting a Boolean topic via a separate node, and we would like the Arduino node to subscribe to the topic and then do an operation based off of the true/false state of the topic.", "In my callback I have: ", "But I keep getting many errors:", "Any idea what I'm doing wrong or could do differently?", "Thanks!!!"], "answer": [" ", " ", " ", " ", "You should use the message types directly. For example:", "See ", ". There, they use the ", " message type. The callback parameter type is just ", ". All ", " are worth a look.", " It uses quite a bit of memory, not a problem if you are using a Mega or a Teensy, for example, but can be limiting on an Uno or Leonardo. You can limit memory by declaring the node handle in a way slightly different than the tutorials:", "Using the ", " template type directly allows you to control the buffer sizes and the number of subscribers and publishers in the declaration, rather than having to modify ", ". The parameters 5, 14, 125, and 125 above are, respectively, the number of subscribers you will declare, the number of publishers, the size of the input buffer in bytes, and the size of the output buffer in bytes. Those buffer sizes are very skimpy, because I was running out of memory (leading to my abandonment of ", "). You need to have enough buffer size to hold the largest message you will send or receive, and you have to call ", " often enough that the output buffer doesn't fill up.", "ROS messages can be large. Also, because the Atmel chip is a non-Von Neumann architecture, static strings also take up RAM, so long topic names will start to eat up precious memory.", "As I said, you don't have to worry much about this on a Mega or Teensy. I had a lot of stability issues on a Leonardo-class board which housed my power supply and motor controller. I eventually gave up and used ", " instead."], "question_code": ["void messageCB(const std_msgs::Bool::ConstPtr& state)\n{\n  if (state.data = true)\n  {\n     //do stuff\n  }\n", "rotational_table_node.ino:17:22: error: \u2018ConstPtr\u2019 in \u2018class std_msgs::Bool\u2019 does not name a type\nrotational_table_node.ino:17:48: error: ISO C++ forbids declaration of \u2018state\u2019 with no type [-fpermissive]\nrotational_table_node.ino: In function \u2018void messageCB(const int&)\u2019:\nrotational_table_node.ino:21:13: error: request for member \u2018data\u2019 in \u2018state\u2019, which is of non-class type \u2018const int\u2019\nrotational_table_node.ino: At global scope:\nrotational_table_node.ino:54:76: error: invalid conversion from \u2018void (*)(const int&)\u2019 to \u2018ros::Subscriber<std_msgs::Bool>::CallbackT {aka void (*)(const std_msgs::Bool&)}\u2019 [-fpermissive]\nIn file included from /home/motherbrain/sketchbook/libraries/ros_lib/ros/node_handle.h:84:0,\n                 from /home/motherbrain/sketchbook/libraries/ros_lib/ros.h:38,\n                 from rotational_table_node.ino:5:\n/home/motherbrain/sketchbook/libraries/ros_lib/ros/subscriber.h:97:7: error:   initializing argument 2 of \u2018ros::Subscriber<MsgT, void>::Subscriber(const char*, ros::Subscriber<MsgT, void>::CallbackT, int) [with MsgT = std_msgs::Bool; ros::Subscriber<MsgT, void>::CallbackT = void (*)(const std_msgs::Bool&)]\u2019 [-fpermissive]\n       Subscriber(const char * topic_name, CallbackT cb, int endpoint=rosserial_msgs::TopicInfo::ID_SUBSCRIBER) :\n       ^\n"], "answer_code": ["void myCallback(const std_msgs::Bool& msg);\nros::Subscriber<std_msgs::Bool> mySubscriber(\"~some_topic\", &myCallback);\n...\nvoid myCallback(const std_msgs::Bool& msg) {\n    if (msg.data) {\n        ... do something ...\n    } else {\n        ... do something else ...\n    }\n}\n", "Empty", "const std_msgs::Empty&", "rosserial_arduino", "ros::NodeHandle_<ArduinoHardware, 5, 14, 125, 125> nh;\n", "NodeHandle_", "<ros.h>", "rosserial_arduino", "nh.spinOnce()", "ros_arduino_bridge"], "url": "https://answers.ros.org/question/243290/subscribing-to-a-bool-msg-via-rosserial_arduino/"},
{"title": "DC MOTOR + ENCODER + PYTHON", "time": "2016-10-12 01:53:30 -0600", "post_content": [" ", " ", " ", " ", " Hello, everyone.\nI have the next equipment:\n ", "  ,\n ", " \nand raspberry pi. ", "I want to control the dc motors via ROS and Python. Is it possible? I can't find some good example or tutorials. I will be so appreciated if you will help me.\nThank you.", "did you solve your problem?", "did you solve the problem "], "answer": [" ", " ", "I have a small robot that uses similar Pololu motors with ROS. A few considerations:", "For my robot I use the ROS package ", " for both the Arduino code and the Python ROS controller node. It's unclear whether you intent to have a microcontroller for the motors. I'm using a Pololu A-Star connected to a Raspberry Pi running ROS. If you were to use an A-Star, ", " has support for the A-Star motor controller.", "If you provide some more details about the rest of your hardware, I could provide more suggestions for a controller or ROS nodes you might want.", " Dear Mark.Thank you for your feedback.\nThe hardware is easy:\nRaspberry PI3 + 2 DC  motor and encoders + Line follower.\nLine follower link is :  ", " \nI will be so appreciated if you will help me.\nThank you very much.\nBest Regards ", "You may find it useful to have an Arduino-class microcontroller for doing low-level PID control, etc., rather than doing it on the Pi. Also, the line follower array you've chosen is a 5V device, so you'll have to either talk to it from an Arduino to use level shifters.", " The Pololu A-Star ( ", " ) plugs in to the Pi GPIO bus, and has a voltage regulator that can take a variety of battery voltages, and will power the Pi, too. It has on-board H-bridges to run the motors off the battery power. ", "You can, instead, control everything from the RPi, but you will need a voltage regulator and level shifters. Since you don't have any distance sensors, you won't be able to use the ROS navigation stack. Instead, write your own node to do line following.", "Thanks for your answer. So, in this case, I can't create few nodes and just tell to dc motors the distance(use Python...rospy)?", "Dear Mark, You was right. I use the motor driver for system, but I can't understand how to work with encoder. Can you help me with these?"], "answer_details": ["You might want motors with a lower gear ratio. Depending on the size of your robot, and how fast you want to move, you might want to consider the 210:1 or 150:1 motors. Also depends on your wheel size, of course.", "You'll want wheel encoders, such as ", ".", " ", " ", " ", " "], "answer_code": ["ros_arduino_bridge"], "url": "https://answers.ros.org/question/245514/dc-motor-encoder-python/"},
{"title": "error loading gazebo_plugins models (Pioneer)", "time": "2016-09-07 13:23:00 -0600", "post_content": [" ", " ", " ", " ", "I get one weird error trying to load some of the samples/models provided in gazebo_plugins on Ubuntu 16.04 with ROS kinetic installed via official repositories.", "Particularly, for the multi_robot_scenario fails. On the other hand the tricycle demo or the pendulum demos work properly.", "This is the error I get trying to load the multi_robot_scenario demo.", "In order to reproduce this issue you have to install the package ros-kinetic-gazebo-plugins and execute the launch file:", "roslaunch $(rospack find gazebo_plugins)/test/multi_robot_scenario/launch/multi_robot_scenario.launch"], "answer": [" ", " ", "I finally found the solution.", "It looks like there is a bug in gazebo 7.0.0 contained in the kinetic debian package repository for ubuntu 16.04. The bug appears when the urdf descriptions contains collision geometry objects with zero value parameters. ", "For instance in \"gazebo_plugins/test/multi_robot_scenario/xacro/p3dx/pioneer3dx_wheel.xacro\":\nwe find this chunk of urdf:", "The box size must not be zero. The error is solved if the above code chunk is replaced by the code chunk below:", "It is needed to apply this correction in several place in order to make work this demo:", "roslaunch $(rospack find gazebo_plugins)/test/multi_robot_scenario/launch/multi_robot_scenario.launch", "Hm. Is this a regression in Gazebo 7? In any case, it would be really nice if you could report this to the Gazebo bug tracker or the ", " tracker. Even if ", " is not a legal value, Gazebo should not just crash, imo.", "@Pablo-I\u00f1igo-Blasco, what files did you need to change (if you can still remember)? I changed the battery_block, sonar, swivel, sonar, and wheel xacro files and it still didn't fix the issue. I'm wondering if I missed one."], "question_code": ["Error [Param.cc:451] Unable to set value [-nan -nan -nan] for key[size]\ngzserver: /build/ogre-1.9-mqY1wq/ogre-1.9-1.9.0+dfsg1/OgreMain/src/OgreNode.cpp:630: virtual void Ogre::Node::setScale(const Ogre::Vector3&): Assertion `!inScale.isNaN() && \"Invalid vector supplied as parameter\"' failed.\n[r1/urdf_spawner-4] process has finished cleanly\nlog file: /home/geus/.ros/log/37a5e414-7527-11e6-8e8a-d8cb8abfd844/r1-urdf_spawner-4*.log\nService call failed: transport error completing service call: unable to receive data from sender, check sender's logs for details\nAborted (core dumped)\n[gazebo-2] process has died [pid 19670, exit code 134, cmd /media/geus/Storage/simulation_gazebo/src/gazebo_ros_pkgs/gazebo_ros/scripts/gzserver -e ode worlds/empty.world __name:=gazebo __log:=/home/geus/.ros/log/37a5e414-7527-11e6-8e8a-d8cb8abfd844/gazebo-2.log].\nlog file: /home/geus/.ros/log/37a5e414-7527-11e6-8e8a-d8cb8abfd844/gazebo-2*.log\n"], "answer_code": ["<collision>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n  <geometry>\n    <box size=\"0 0 0\"/>\n  </geometry>\n</collision>\n", "<collision>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n  <geometry>\n    <box size=\"0.1 0.1 0.1\"/>\n  </geometry>\n</collision>\n", "gazebo_plugins", "0 0 0"], "url": "https://answers.ros.org/question/243283/error-loading-gazebo_plugins-models-pioneer/"},
{"title": "Control Tutorial RC circuit  with ROS", "time": "2016-11-05 18:26:11 -0600", "post_content": [" ", " ", "Hey FOlks,", "I wonder if I can do this experiment without using MATLAB and instead using ROS, Now I understand how to make connection between ROS and Arduino. But what about the other features as shown here ", "Marwa"], "answer": [" ", " ", "Thanks, but from your answer seems that MATLAB is powerful tool for control theory rather than ROS. To be honest, I didn't find MATLAB useful at All, it is just black box!.", "If you just want to focus on the math, and avoid the implementation, Matlab is great, but you are also correct that it is mostly black boxes. Matlab provides a lot of features that integrate well together, whereas to do all of those things with ROS you need to combine many different tools.", "To be clear; I like ROS and Python, would probably do something like this with ROS, but it would take longer than doing the same exercise in Matlab.", "I have tried this example, but I don't know how to do that for taking analog signal from arduino  to it, any example please", " ", " ", "While ROS allows you to write code that runs an arduino and communicates with a PC, that's about where the similarities with Matlab/Simulink/Arduino end.", "You could probably implement some arduino code which excited an RC circuit and measured the response in the same way, and you could record the results, but ROS doesn't provide many software libraries for doing system identification or control system design.", "ROS does have very nice integration with python so, once you had recorded results, you'd probably want to load the bag file in python and use the tools in scipy to analyze your data, and then use matplotlib to plot it."], "url": "https://answers.ros.org/question/247334/control-tutorial-rc-circuit-with-ros/"},
{"title": "Which IDE(s) do ROS developers use? [closed]", "time": "2011-02-15 08:25:22 -0600", "post_content": [" ", " ", " ", " ", "While emacs and vi are great, there are also advantages in using IDEs, especially for code exploration and debugging. \nWhich IDE(s) do developers use for C++, Python and other languages?"], "answer": [" ", " ", "The ", " on ROS.org describes most of the non-Emacs/vi options.  To the best of my knowledge, Eclipse is the most popular.  There are scripts to help you integrate with the ROS build system, though it would definitely be wonderful if someone was willing to write an Eclipse plugin for ROS.", "I started to work on that. Any help will be welcomed, rigth now I wrote a Eclipse Plugin with a RosMenu Launch RosMaster and Launch RosBridge as the \"hello world\" tools ;)  and an sketch of a ROS View for the ROS nodes and services.  If there are others interested contact me: lospinos AT gmail", "Hi, I'm very interested in this initiative. Eclipse is a full featured IDE for the languages used in ROS development. I'm contacting you for more info about.", " ", " ", "I've found QtCreator to be easier to use than Eclipse, as it natively supports CMakeLists which are part of every ROS package. You just need to launch QtCreator from a terminal (NOT from the GUI menu) and it will have all the necessary environment variables to operate correctly.", "yea i love QtCreator is the most god-like IDE i have ever used. The support of opening cmake files natively is awesome. It also has a great UI.... Wish I could use it to code python as well", " Eclipse expects the build files (CmakeLists) to be in a separate tree from the source files, which is not the case with initial ROS install.  There is a simple script to implement this - ( ", " ).  Both Eclipse and QtCreator are great. ", "QtCreator is really wonderful for ROS development. You can also use the QtDesigner, which produces UI files, that can be used with both Qt C++ and PyQt when creating a rqt plugin. I also find the debugging especially for multithreaded applications much easier compared to Eclipse.", " ", " ", " ", " ", "I know that you specifically leave out vim and emacs in your consideration here, but I have to say, a well-tuned VIM install will do just about anything that you want.", "I have a few plugins of choice for VIM:", "NerdTree: Brings up a split pane with your current working directory contents.  Navigating using the same VIM keys, and allows you to change directory, files, move, copy, and most other filesystem operations.", "Ctags: Most people who use UNIX are probably versed with Ctags, but it bears repeating.  Generates a list of \"tags\" such as functions, classes, and variables.  This can then be accessed in a split-pane on VIM.  Python and C(++) compatible.", "Yankring: Provides a buffer of previously copied text.  Wonderful for keeping track of multiple copy-pastes as well as delete history.", "The benefit of using VIM, in my opinion, is that I am frequently connecting to my robot over an ssh connection, and often don't want a full-blown IDE.  When I am working in a terminal, I can open Vim right there, without switching back to an IDE.  Vim is also common (in one form or another) on many, many systems (except for Ubuntu by default, for some reason).  Vim provides me the power of an IDE without the weight of something like Eclipse.", "Also, if the whole texty-console thing scares you (or learning vim, for that matter), solutions like MacVIM and GVIM provide a nice GUI interface, and use many of the system commands for copy, paste, etc.", " Adding ROS tags to your OmniCppComplete\n", " There is also a ROS plugin for vim:  ", " . It helps a lot navigating around ROS files thanks to its :Rosed and :TabRosed commands. ", "It's been a while since I've seen this question, but I would also say that Vim does a great job with YouCompleteMe.  You can use rospackage to grab the include flags from multiple packages so that the completions work.  I can write something more thorough if there is interest.", " I agree with ", " that YouCompleteMe works very well with ROS after a little configuration. I wrote a Gist for this:  ", " . ", " ", " ", "I think ", " does a pretty good job, especially if you hack a lot and have to ssh into stuff remotely", " ", " ", " ", " ", "JetBrains (of IDEA fame) has recently released a beta for ", ", a C/C++ IDE. It pulls in project data via CMakeLists.txt, so it works with ROS workspaces out of the box. It really blows away QtCreator and Eclipse in terms of usability (although a little rough around the edges being a beta), and the ", " looks very impressive.", "Make sure to start the IDE from bash, or update the .desktop definition similar to ", ".", "I have some problem configuring the CLion IDE. It reports \"catkin was neither found in workspace nor in the CMAKE_PREFIX_PATH\" when building. I chaged the value of CMAKE_PREFIX_PATH in the cache, and that makes CLion working. However, afterwards, command \"catkin_make\" in console doesn't work.", "add\n'set(CMAKE_INSTALL_PREFIX \"${CMAKE_SOURCE_DIR}/../install\")\nset(CATKIN_DEVEL_PREFIX \"${CMAKE_SOURCE_DIR}/../devel\")'\nto the top level cmake and it works for me.", " ", " ", "Somebody took the IDE out of emacs without telling me.  ;)", " ", " ", "If I'm sitting in front of a big monitor then screen real estate is cheap and I use Eclipse. If I am coding on a robot, I bring up vim.", " ", " ", " ", " ", "bumping on an old question, but I needed to ramble about it...", "I've been using Eclipse to code in ROS for about 2 years now. I like code completion, tooltips, and occasionally debugging in a GUI. Overall it works OK.", "BUT:", "Reading about previous answers, I guess I'll give QtCreator a try. ", "Feeling better already, had to let some steam out... ;)", "EDIT: I did switch to QtCreator a couple of months ago, and I've never looked back!", "You can't complain here about how Eclipse is made. That's for the Eclipse developers.", "Besides the problem of understanding that packages are linked is common every IDE as this relation is declared in the manifest.xml which is not parsed by any IDE. Only way would be for someone to write a plugin.", "I use Netbeans and most of the same problems are there too, including the lenghtier compile time.", "One thing I could suggest to at least get rid of the dependency problem (if you find yourself needing to recompile multiple packs so often) is to customize the build command of your IDE to use rosmake.", "May I ask a question??\nI'new to ROS. I have been developed some programs in gedit for a long time and bebug them by using printf.I just installed eclipse, but have no idea how to debug with it. Breakpoints don't work.\nSorry for my pool english. Can I get ur help? my Email ", "You may want to use Eclipse, that provides you a good integration with GDB as a debugger.", " ", " ", "Just for reference, we use an ", "  that does text highlighting for cmake.", " ", " ", " I use RoboWare Studio. It can be found on  ", " . The introduction is as follows: ", "RoboWare Studio is an IDE based on VSCode and is specially designed for ROS (indigo/jade/kinetic). With a double-click installation, RoboWare Studio can automatically detect and load ROS environment without additional configuration. The \u201cout-of-the-box\u201d feature helps developers pick it up and figure it out quickly. It provides an intuitive graphical interface for developers to create ROS workspace/package, add source files, create messages/services/actions, list generated packages/nodes, etc. Meanwhile, The CMakeLists.txt file can be updated automatically. It supports release-build and debug-build. Developers can debug C++ and Python codes right from the editor, with break points, call stacks, and an interactive console. It also displays ROS packages and nodes in the interface.", "Thank you Gaojiao,info is useful.", "I am having dependency errors while installing roboware.\nCan you please help?", "You may encounter dependency error for the following reasons:\n1. your os is not ununtu 14.04 or 16.04.\n2. your os is ARM architecture (not x86).\n3. your os is 64-bit but Roboware Studio is 32-bit, or vice versa. (*amd64.deb is for 64-bit, and *i386.deb is for 32-bit)", "a 64-bit os cannot install 32-bit roboware studio? why?", "does it have auto-complete feature, helping code, no need to memorize so many function name?", "Yes, if your os is 64-bit, you should install 64-bit roboware, if your os is 32-bit, you should install 32-bit roboware. Their dependencies are different for 32-bit/64-bit architecture. Unfortunately, for now, 32-bit roboware does not have auto-complete and go-to-definition feature."], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "it's way too slow.\n", "Building is about 3 times slower than in the terminal", "the indexer frequently takes minutes to complete, meanwhile slowing down the GUI", "the GUI sometimes becomes unresponsive for no apparent reasons", "it does not understand relation between packages, so if I have packages A and B, B depending on A, both open in the workspace as separate projects, if I make a change in A/a.h, and Ctrl+B (build all projects in the workspace), then it won't recompile files in B that depend on A/a.h ... I have to be aware of it and manually force recompilation of those files, either by clean and build which is slow, or touch the files and build which is tedious. I guess I could manually add inter projects dependencies but then that's going to be tedious to manage manually...", "I am using ccache to speed up compilation and I am very happy with it. I have /usr/local/bin/gcc (and g++ and others), as symlinks to ccache. But for some reason eclipse does not use it. I don't know what it's using and it's scary. I suppose it's using /usr/bin/gcc but who knows", "the indexer occasionally cannot find some objects (variables, methods, etc.). Sometimes even if it's within one single project.", "I had to allow eclipse to use 8GB of memory in eclipse.ini so that the indexer wouldn't freeze the whole GUI. I can afford it but that seems ridiculous. I also tried using oracle's JRE but I didn't notice any improvement in term of memory management or speed.", "some build errors (in particular linker errors) are not always reported properly", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/9068/which-ides-do-ros-developers-use/"},
{"title": "create_autonomy does not seen by ros", "time": "2016-12-05 05:46:14 -0600", "post_content": [" ", " ", " I followed the instruction from  ", "  but roslaunch seems to be does not aware of ca_driver even if I source the package via source ~/create_ws/devel/setup.bash. Roscd can not locate the folder and roslaunch says  ", "[create_2.launch] is neither a launch file in package [ca_driver] nor is [ca_driver] a launch file name", "Is there anything that I missed? python-rosdep and python-catkin-tools packages installed, catkin build completed succesfully. I am using ros indigo and ubuntu 14.04 on macbook pro.", "Now I can see the launch file by editing bashrc file. I added the line ", "export ROS_PACKAGE_PATH=~/create_ws:$ROS_PACKAGE_PATH", "But now I can not connect to create 2. terminal says that \"failed to receive data from Create. Check if robot is powered!\" but it is powered and connected to computer and I have checked that serial usb port is connected as ttyUSB0. ", "Is there a setting I need to make on the robot?"], "answer": [" ", " ", "I believe the message you see indicates either (as it states) the robot is not powered on meaning you should try pushing the large \"Clean\" button, or the baud rate is not correct.  ", "See ", " for information regarding baud. It could be 115200 (I believe the driver's default) or 19200. You can try a different rate by adding a parameter to the launch file ", ":", "Good luck.", "Adding the baund rate parameter solved the problem. Thanks!!", "I followed this step and succeed once, then the same issue came back again and  I can't fix it by using this.", "\nFixed by adding "], "answer_code": ["ca_driver/launch/create_2.launch", "<param name=\"baud\" value=\"19200\" />"], "url": "https://answers.ros.org/question/249387/create_autonomy-does-not-seen-by-ros/"},
{"title": "How does ROS work with real/actual robot in general? How's the process when we send a data to the actual robot?", "time": "2016-10-13 02:46:51 -0600", "post_content": [" ", " ", " ", " ", "These are basic questions. I've been wondering how does ROS work with real robot, in GENERAL.\nPlease help me to clarify these things", "Please elaborate on those things. If possible, please give step by step example on those things e.g. when sending velocity command to an actual robot, how's the process? The data that we send from our PC is of cmd_vel type. does the robot knows this type, or it is being converted first in our PC before sending? and after the data is received on the robot, does the robot has its own program to react on that data?"], "answer": [" ", " ", " ", " ", "There cannot be a step for step explanation for how to do this, because this depends on the HW. If you can control the robot to your liking from a self-written program on the microcontroller, you can write the rest yourself. But ROS is not just: \"Plug in any roboter and they magically understand ROS\".", "The answer to all your questions below is actually the same:", "Any sensor or actuator provides an interface specification how to retrieve the data from it. What you need to do is to write a ROS driver for this HW, that adheres to the interface specification of the sensor/actuator and then \"translates\" this in the respective ROS message type.", "The same basically holds for a micro controller, however, there you might have the chance to decide on the interface specification yourself.", "When we create nodes, subscribers,and publishers etc, we create it in our PC. SO when we publish, only our PC knows. SO we do not send the info to the robot when we publish. How does the robot knows the info?", "How is it in real robot? How does the real robot know how to publish things to topics, if it does not have ROS installed in it?", "see edit above.", "Thanks for your reply. But I'm still confused. When writing code , we publish message to a topic, and we do not send it to the robot. How does the robot know the information/msg since the robot does not have subscriber to the topic.", "I meant when we publish message , it's only to the ros master which is our own pc. And i don't remeber  sending it e.g. cmd_vel to the robot. Where(in the code) does this(sending data from pc to robot , which is different from publishing message to a topic) happen?", "you have a ROS node which subscribes to this topic and this then is the HW driver which translates it to whatever the robot understands.", " ", " ", " ", " ", "As mig says, the answers depend on your hardware.", "Example 1 (my robot):", "Example 2 (hypothetical robot):", "In both cases the ROS nodes on the robot and ROS on your PC communicate over WiFi, presumably. You could run the ROS master on the robot (that's what I do) or your laptop.", "What is your hardware?", "Hi ,thanks for your helpful answer. ", "My hardware is a drone(crazyflie). I didn't remember installing ROS in the crazyflie itself, and afaik it's not preinstalled in the drone. So I assume that the robot does not have ROS installed in it.", "Since the robot does not have ROS installed , so all the nodes are my PC's nodes. What I am confused is : I don't remember sending info to the robot. I only publish message to topic. And all topics are in my PC.", "Can you please give one more example on: robot that does not have ROS installed in it? ", "So, all the nodes running in PC, and there's no node in the robot. How does the sending information to the robot happen? It does not happen during publishing message right?", "Thanks", "Well, another possibility would be a smaller robot I have that uses an Arduino-type board that has built-in Bluetooth. A ROS node on the PC could handle the sending and receiving data to the robot. Probably a roll-your-own protocol to send motor commands and get sensor data back.", "Once you get outside of ROS message/service communication, either you have to do the communication yourself (from a ROS node), or you have to use a ROS node that knows how to control the hardware (usually only for commercial robots).", "I think I finally got what ", " said now. There's a ROS driver in our PC that subscribe to e.g. cmd_vel, translate this cmd_vel to data type that the robot understands, and send it to the robot.", "Just want to reconfirm. pls correct me ", "ROS running on robot and PC. All the info exchange happen through subscribe & publish", "ROS ONLY running on PC. There's a ROS driver in our PC which subscribes to needed message e.g. cmd_vel, TRANSLATE it, and SEND it to robot.", "Those are all possible right?", " ", " ", "The answers you are getting are good.   But long.   A short answer is", "Your question:  \"How does the information in ROS get to the physical hardware inside your robot?\"", "There are two answers:", "1) You robot might have a computer inside of it that is running ROS.  Either an X86 \"PC\" type computer of a smaller ARM based computer.  This is common with larger robots or,", "2) Your robot might have a small micro controller that is way to small to run Linux and ROS.   So you make a radio link and run a ROS Bridge on both the PC and the uP inside the robot.  ROS Bridge allows a ROS Node to run on the uP.  (You can Google ROS Bridge for more info.", "That is the end of my short answers", "Now a comment:  ROS allows all kinds of variations.   You might even have multiple computers inside your robot some running Linux and ROS and others being just simple micro controllers perhaps running some RTOS (real time OS) or not.  Or to save space and power in the robot you can move one or more of the ROS computers physically outside but functionally inside the robot using WiFi.  You can mix and match. \n.\nFor your purposes. with a uP inside the robot and ROS only running on the desktop you'd be looking at ROS Bridge  ", "Just a note: if you intend to 'close the loop' over your radio link, I would actually go for a wired connection. As stable as you might get your radio, a wired connection will always be more performant, robust and have more predictable performance characteristics."], "answer_details": ["Yes, but not every robot has a microprocessor. Bigger/more complex robots typically only have a PC communicating directly with the motor drivers, sensors, ...", "Yes, ROS messages are for ROS, you need to translate this to whatever your actuators/sensors understand. For ", " you typically have a base driver which translates the velocity of the robot base link to motor commands and sends those to the motor drives. This could be a message to a microprocessor which then sets the motor velocities or a CAN message going to a motor drive over CAN bus.", "Well, yes", "yes. but it needs information from the robot (sensor data, odometry, ...)", "Not necessarily. You have to interface the PC with the microcontroller. If the microcontroller has a defined interface, you could do anything on the PC, but probably you have to write the microcontroller interface to the PC as well.", "This is up to your hardware. Check how the microcontroller commands the robot, this is what you need to do in the end. How you send the data from ROS to your system is up to you.", " ", " ", " ", " ", "Where does all these translating process happen? in the actual robot or in our PC?", "How does this reading info from the robot's sensors happen?\nI know that in simulation , the robot will publish message containing the sensor readings to certain topics.", "Raspberry Pi, on the robot, runs ROS.", "Arduino-style microcontroller talks to sensors and motors, does not run ROS.", "ROS node (", ") listens for ", " messages, decides motor speeds, and sends over a serial protocol to Arduino.", "Arduino sketch (", ", source in ", " package) listens for motor speed commands, controls motors, and sends sensor values over serial protocol to RPi.", " receives sensor values and publishes as ROS messages.", "Robot runs some Linux microcontroller, perhaps Raspberry Pi or Odroid, running ROS.", "Fancy motor controller connected to microcontroller using USB connection.", "Some sensors connected via I2C to microcontroller.", "ROS node on robot listens for ", ", decides motor speeds, and sends to motor controller.", "ROS node on robot talks I2C to sensors and publishes ROS sensor messages.", " ", " ", " ", " ", "ROS ONLY running on robot. So all the package involved e.g. navgation ,etc are in the robot itself .", " ", " ", " ", " "], "question_details": [" ", " ", "When we install ROS packages, we install it in our PC, and not in the actual robot. So the ROS library, driver,etc isonly in our PC, and not in the microprocessor of the robot. Is that right?", "Which means that only our PC knows/recogizes ROS message types (e.g. cmd_vel) ,but the actual robot does not recognize the ROS message types. The actual robot only recognizes primitive data types (e.g. int) or whatever data types are present in its microcontroller. ", "So when we publish message to a topic e.g. cmd_vel to velocity topic. This topic is only known in our PC, and not in the robot's computer?", "For a ROS package (e.g. navigation) working , the ROS package actually do all the works in our PC and just send result or information to the actual robot right.", "So when we use/code all these ROS packages, we actually code just on our PC, meaning that we do not change anything on the robot's microcontroller itself. The robot has its own microcontroller and program, and what we do is just sending info to the microcontroller from our PC. Is that correct?", "For a specific robot, Where/how to find out what data(message, command) an actual robot expects to receive?"], "answer_code": ["cmd_vel", "ros_arduino_python", "/cmd_vel", "ROSArduinoBridge.ino", "ros_arduino_bridge", "ros_arduino_python", "/cmd_vel"], "url": "https://answers.ros.org/question/245597/how-does-ros-work-with-realactual-robot-in-general-hows-the-process-when-we-send-a-data-to-the-actual-robot/"},
{"title": "Connect 2 ros board", "time": "2016-12-16 07:29:39 -0600", "post_content": [" ", " ", "Hi, I have 2 arm boards running ros. I need to share the topics between the boards.\nI want to use a wired connection, is the uart a good choice? (I've got also lan or usb on both boards)\nIs it possibile to do something like I do in wireless mode with the definition of master uri?\nMaybe do I need to use rosserial?\nThanks"], "answer": [" ", " ", "Depending on your hardware, serial is typically 115.2kbps to 1Mbps, and Ethernet is typically 100Mbps to 1Gbps (100x faster), so Ethernet is generally the faster and more flexible solution.", "Since you'll be setting up a point-to-point ethernet link, you'll probably need to assign static IP addresses to both boards. This is well-documented for most linux distributions and is outside the scope of this forum.", " Setting up ROS to communicate over a network is usually as simple as following  ", "Thanks, I'll try it. \nBut in the case I can't use ethernet, how can I use uart?", "I'm not certain it's possible to use the existing rosserial nodes over a point-to-point serial link, but you could probably use rosserial_python or rosserial_server on one end and write a custom node on the other end that uses the rosserial_embeddedlinux client.", "Really, I think using the serial link is such a poor idea that I don't want to give too many details. Using ROS over ethernet works, and that's where you should focus your energy."], "url": "https://answers.ros.org/question/250185/connect-2-ros-board/"},
{"title": "RK01 humanoid and ROS", "time": "2016-10-31 06:22:06 -0600", "post_content": [" ", " ", "Hey ROSflolk,\nI'm interested in using ROS with the new RK01 humanoid robot. RK01 is a new cheap chinese humanoid robot, it's very cute and seems to have a lot of sensors. Is someone interested in my collaboration for this purpose ?", "RK01 is similar to nao except it's lower performances, an ARM (Mediatek MT6472 SoC architecture) processor and for the fact it has no hands. I had mine for only 410\u20ac including port from shenzen. My RK01 is provided with a dirty android distro and poor software functionalities and I wanted to install ubuntu ARM and ROS to get either my c++ tool chain and ROS to do funky things with it. As I said RK01 have a lot of sensors and communication devices (GPS, GSM, Acelerometter, magnetometer, temperature sensor, proximity sensor, PIR sensor, IR I/O, 2MP camera) All it's servos have odometry caps ... RK01 is definitely the low cost nao/qrio ... Waooo, Keep calm ! It remains a real bottleneck with RK01, it have only 512mo of SRAM !!!", "The best way to get ROS working on RK01 is probably to install linux on it, like for RPI boards, we will have acces to all the packages we need. Installing ubuntu along Android can be a bad idea due to the low amount of RAM and for the fact we could not have easy access to native hardware. The best way could be to install directly linux on the ARM computer (an Cortex-A7, Mediatek MT6572).\nFrom now, I investigate on the way to do so. If someone want to join the effort ...", "Cheers,", "Marco"], "answer": [" ", " ", "I played a little bit with my RK01 and it was somwhat frustrating ...", "\nCan you confirm the sensors of your RK are working ? (GPS, Accelerometer, magnetometer, proximity sensors, IR)\nI could not get any data from these sensors. If only mine are not working, I'll not be able to teach him something more. Even simple remote control of the robot could be difficult to implement without any sensor data (except for the camera).", "\nIn order to facilitate access to the ROS plateform it would be great to get onboard a real linux OS, an Ubuntu one would be perfect. Ubuntu ARM exists for RaspberryPI with access to complete ROS packages and linux libaries. But for now I was not able to simply change the rom image with another mtk6572 Android ROM (in order to check sensors). ", "\nI opened it sometimes so I'm interested in your knowledge about the RK01 ins and outs. I compared camera and display port and it seem very different from those of the RaspberryPI. The underlying idea was to replace the board with somewhat more powerful and open source. But in this way we simply loose alla the mtk6572 onboard sensors, and probably to camera and display ...", "\nRK01 lacks in dedicated android apps. Some are in Chinese. Some are not working.", "Maybe I'll try to cook an alternative Android ROM or get access to sensors data in some way. It would be great to have an RK working on its factory board. Here we need an Android expert :/", "i really dont know what software is used to program its micro controller and i dont know what the name of the micro controller is called all i see are labels on the board where the sensors go. its programming needs tweaking. right now i get a message that reads \"unfortunatley childloc has stopped\".", " ", " ", "I have an R K01 I have customized it with a 4K security cam I am interested in collaborating ideas in the specially working together to mature this dumb robot I am too trying to reprogram it's software. I have disassembled and reassembled the robot successfully so I know the ins and outs of it."], "url": "https://answers.ros.org/question/246850/rk01-humanoid-and-ros/"},
{"title": "rplidar-turtlebot2 [closed]", "time": "2017-01-15 07:02:24 -0600", "post_content": [" ", " ", " ", " ", "Hi. Have any body out here used the ", " with ROS INDIGO from github.  ", "  I can't get the laser to spin. When I list al my USB's I can see the laser. If I conect the laser to my workstation it's working. The problem is only on my turtlebot notebook. I have also tested with the instalation file from vendor.I have the RPLIDAR A2. ", "Tanks", "GT"], "answer": [" ", " ", "I can't get the laser to spin [..] If I conect the laser to my workstation it's working. The problem is only on my turtlebot notebook. ", "If the lidar draws current from the USB port to power the electromechanics, then it could very well be that the netbook doesn't have the capacity to provide enough current on its USB ports.", "You could check the specifications of your netbook to figure out what the maximum rated power output of those USB ports is.", "If this is indeed the issue, I've seen people use a powered usb hub between the netbook and your peripheral. The USB hub would then provide the power to the peripheral. The TB has various connections for power output, so you could probably use one of those to power the hub.", "Thanks for the information. Have contacted the vendor and they had not heard off this issue before.\nThe motor is pulling 1.5 amp and I also think that the output for notebook can't handle so mutch amp.\nI will check around for external powered USB hub.", "A 'normal' USB (1 & 2) port can only provide 500mA, so that would seem to be well below what your device needs. An in-spec USB hub will also not provide that much current on a single port. Perhaps you can power the device directly from one of the DC connections on the TB base.", "FYI. I made it spin without USB hub, my fault not setting correct USB rules.", "If you elaborate a bit and post it as an answer, you could accept your own answer."], "url": "https://answers.ros.org/question/252027/rplidar-turtlebot2/"},
{"title": "Raspberry Pi3 Hector Slam [closed]", "time": "2017-01-15 12:51:37 -0600", "post_content": [" ", " ", "Hi,", "Just tried to see if hector slam algorithm works on raspberry pi3 with ubuntu MATE 16.04 w/ ROS kinetic. Unfortunately seems that the computational power of the board is not enough to handle the data stream while map is created so our discover is that raspberry is not enough powerful (we tried the commands on another pc and the map is created successfully). ", "So now our question is: Is it possible to stream  the data coming from laser scanner (connected to Pi3) to another powerful pc. Do proper elaborations and then send final commands to the raspy again (cmd_vel..). If yes which connection should we use? And how to do that? Any hints?", "Thanks "], "answer": [" ", " ", "HI", "Short answer is yes. The pi has plenty enough power - I would ensure you have a high bandwidth WiFi channel - I use a 300Mps adapter on a Raspberry Pi 2 for the set up you mention - works fine. 150Mps is a bit slow.", "Mark", "Additionally: make sure to compile things with optimisations enabled. With CMake / catkin: ", " (or ", ").", "How can stream data from raspy and then analyze it in the other PC?", "Simply install ROS on the other PC and run a Hector SLAM node there listening to the topics it  required.", "This tutorial should help re: running ROS on multiple machines - ", "Thanks guys! :)", "If you feel your question was answered, please don't close it, but accept the answer by ", " by ticking the checkmark to the left his answer. Thanks."], "answer_code": ["-DCMAKE_BUILT_TYPE=Release", "RelWithDebInfo"], "url": "https://answers.ros.org/question/252046/raspberry-pi3-hector-slam/"},
{"title": "SLAM with a camera without odometry", "time": "2017-01-13 07:04:46 -0600", "post_content": [" ", " ", "I am new to the computing world. I recently bought a Kinect v2 for windows for making a self-driving car to navigate in the indoor areas. I have a small RC car with simple functionalities of start, stop, right, left, backward but without any sensors mounted on the car. The car doesn't provide any data about the number of rotations or other odometry related data. ", " ", "Since, I intend to use the car in the indoor areas therefore the map of the environment is quite fixed (constant). I plan to create a map of the environment using kinect v2 and then later on compare the real time movement against that map in order to navigate. I found ", " as an alternate to gmapping but it uses laser scanner. ", "All suggestions are welcome :D", "Note - ", ",\n", ",\n", ",\n"], "answer": [" ", " ", "There is a library called RGBDslam which can use the Kinect, and doesn't require odometry. This has been tested on Fuerte and Indigo, and the github repository also has a Kinetic branch.", " For the ros wiki page:  ", " For the github repo:  ", "I assume if the above works you don't need to fake the laser scanner?", "According to a ", ", the power supply is rated for 12V, 2.67A. 12V batteries are not uncommon, but you'll have to be a bit more careful about the 2.67A rating. You may also need to solder your own connectors.", "Hi ", ", ", "thank you so much for your answer. I am trying to use RGBDSlam as well. Will update the results if it works. Do you have any idea about how can I create a 2D map using RGBDSlam and can we use that map with the ROS navigation stack?", " I've had a quick look around, and found  ", " , which should be able to be used with something like gmapping. There's also this  ", ", although it may be outdated.", "If you're just doing this, then you won't need to use RGBDslam. It's a good option, as 2D path-planning is much easier.", "I think I will be going with gmapping. I had another question about gmapping with Kinect v2 though. I was confused about the drivers. Do I need iai_kinect2 as well? I have already installed libfreenect2 though.", "I haven't used the Kinect myself, so I don't know for sure. If you can't get the depth stream from the camera you could try installing both, otherwise I think openni2 will do the trick.", " ", " ", "Hi There,", "I'm not sure what the best solution to your localisation problem is but it's non trivial. The laser scanner input used by Hector SLAM is a 2D line scanning LIDAR, whereas the kinect V2 is a full 3D RGBD sensor. You could simply take the central horizontal line of samples from the Kinect frame and feed that into Hector SLAM which would work in a mostly 2D envrionment. But you'd be using less than 1% of the data from the kinect for this. I may be wrong but I don't think there's an out of the box solution for full 3D SLAM in ROS at the moment.", "Secondly about the power supply for the Kinect, I assume you mean there's an AC power pack that feeds DC power into the Kinect sensor itself. I've not seen one in the flesh but, this is how 99% of these things work. You'll have to find out what voltage and how many amps the Kinect needs and find a DC-DC power converter to produce this voltage from the battery that your RC car has. Then you'll have a fully mobile robot."], "url": "https://answers.ros.org/question/251877/slam-with-a-camera-without-odometry/"},
{"title": "Turtlebot Purchasing Options", "time": "2011-05-20 01:05:36 -0600", "post_content": [" ", " ", "For those of us who already have a few Roombas and Kinects lying around, is there going to be an option to buy some of the hardware from Turtlebot independently from the whole kit?", "I am mainly interested in purchasing the IMU and the shelving \"stack\" that comes with the Turtlebot kit."], "answer": [" ", " ", " ", " ", "We ", " to make the TurtleBot hardware open source.  We're working on posting the docs and linking to suppliers for all the parts, including the shelving stack and gyro/power board.  So you will be able to buy them separately.  Check back soon at ", ".", " ", " ", "Hello,", "From its ", ", I understand there are two purchasing options:", "Maybe there is a third option where we can skip the kinect?", "Raph", " ", " ", "Thanks to the TurtleBot folks showing things off at Robogames 2011, there are a half dozen turtlebots in various stages of construction in the SoCAL robotics group.  Here is an image of the bottom turtlebot deck that we are working with:\n", " Each of the subsequent decks are the same but only uses the four larger holes. Construction drawings, autocad files for your laser cutter and tips are at:  ", "\nHere is the Gyro setup:\n", " ", " ", "I've been anxiously waiting for TurtleBot since it's announcement... I'm disappointed, though, in the pricing.  Parts seem a little expensive:  $99.95 for the standoff kit!?!?!  This is not a PR2!  ", "There is sooo much good coming out of Willowgarage.  ROS is amazing.  The interest in sharing so much information is astounding to me.  All of that has been truly a turning point.  But Sheez...  if Turtlebot is supposed to be an affordable way to gain entry into the field, how does one justify $99.95 for a handful of parts I should be able to duplicate by visiting my local hardware store?", "-Mike", " ", " ", " ", " ", "Hello, ", "We are working on gettig a turtlebot shop specialized for Europe online: ", "We will propose different kinds of kit (Complete, Core...), and \"a la carte\" configurations to match any specific needs.", "Raph", " ", " ", "Do not purchase from Dabit Industries. Had an extremely bad experience. 2 months and still waiting on parts."], "answer_details": [" ", " ", " ", " ", "USB Communications Cable", "TurtleBot Power and Sensor Board", "TurtleBot Power and Sensor Board", "TurtleBot Hardware", "Microsoft Kinect", "TurtleBot to Kinect Power Cable", "USB Stick TurtleBot Installer", "TurtleBot Core Kit (see above)", "iRobot Create Robot", "3000 mAh Ni-MH Battery", "Fast Charger", "Asus EeePC 1215N", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/10009/turtlebot-purchasing-options/"},
{"title": "Turtlebot NUC battery state", "time": "2017-01-04 15:37:47 -0600", "post_content": [" ", " ", "Hi community,", "I'd like to use the turtlebot with a more powerful NUCi5 instead of a poor netbook.\nThe NUC has no /proc/acpi/battery/BAT1", "Is it possible to replace it with simply setting:\nexport TURTLEBOT_BATTERY=/var/tmp/battery/BAT1", "and write my own file for battery simulation/fake ?\nLater I'd like to fill-up the content with real data from an attached external USB-ADC-Modul.", "Which parameters/values are neccessary to make default turtlebot happy with the fake-setup ?", "Any suggestions ?", "Thanks for some feedback", "Cheers\nChristian"], "answer": [" ", " ", " If you'd like to setup a way to generate a battery status that would be interesting. Note that we've recently added support to disable the battery monitoring too:  "], "url": "https://answers.ros.org/question/251275/turtlebot-nuc-battery-state/"},
{"title": "how ros abstract away the underlying hardware and work with generic messages instead", "time": "2017-01-14 22:21:42 -0600", "post_content": [" ", " ", " ", " ", "I read the below passage on book, ", "Once you get used to this style of\n  programming, there are some\n  significant advantages. As we have\n  already mentioned, many nodes can be\n  reused without modification in other\n  applications. Indeed, some ROS\n  applications are little more than\n  launch files combining existing nodes\n  in new ways or using different values\n  for the parameters. Furthermore, many\n  of the nodes in a ROS application can\n  run on different robots without\n  modification. For example, the\n  TurtleBot follower application can run\n  on any robot that uses a depth camera\n  and a mobile base. ", "I think this situation might only be suitable for something like ", ", which is well supported by ROS thanks to ", ", other than ", ", am I right?", "If so, then if I want to build a real robot, do I have to purchase artibox driver or sth like that ", "?", "I think I might be wrong because missing some basic concept about ", " not a simulated one. Any recommended link? I really want to know more about how the last sentence in the above passage works."], "answer": [" ", " ", " One example of this happening in the real world is the Turtlebot, where they switched from the Microsoft Kinect (no longer produced) to the Orbbec Astra:  ", " \n. They had to develop a new driver for the new sensor, but once they did, they were able to run all of the existing turtlebot apps like follower, SLAM and navigation without rewriting them. ", "The general idea here is that ROS reduces the work required to run the same software on many robots.", "In the example of the follower app, it can run on any robot with a depth sensor and a mobile base.", "If you buy a robot that already has ROS support for its depth sensor and mobile base, the only work you have to do in order to run the follower is to configure the follower app for your robot.", "If you buy a mobile base that doesn't have ROS support, and add a depth sensor that has ROS support, you only need to write the ROS node that converts mobile base commands into the specific commands for your base, and configure the follower app.", "The point here, and the power of ROS, is that you don't need to rewrite the follower app or the depth sensor driver in either case.", "So yes, if you buy a mobile base that doesn't have ROS support, you'll have to add it, but once you do, you'll be able to run existing ROS software like the follower app, SLAM, and navigation.", "Very clear explanation! I strongly recommend that this explanation should be added to ROS overview!"], "url": "https://answers.ros.org/question/251985/how-ros-abstract-away-the-underlying-hardware-and-work-with-generic-messages-instead/"},
{"title": "Persistent map frame in RTAB-Map localization mode", "time": "2017-01-18 23:46:20 -0600", "post_content": [" ", " ", "I have a map generated using a Kinect and RTAB-Map. I want to run RTAB-Map again in localization mode using the same database, and have repeatable coordinates within the map, regardless of my starting position.", "From what I understand, the ", " frame origin is set to the starting point when odometry starts, so I don't know my position within the saved map. I'd like to have a canonical set of coordinates that is the same for the same physical location between sessions. If there was a tf frame that is fixed within the saved map, that would work.", "Does RTAB-Map have this feature? What should I do to achieve this?", "Thanks", "I saw this in the rtabmap multi-session mapping video. There must be a way to get the absolute coordinates relative to the map."], "answer": [" ", " ", " ", " ", "Hi,", "Make sure parameter \"", "\" is set to ", " (default is ", ") so that the robot is localized accordingly to the first node in the map (your fixed map's frame). ", " will publish ", " frame to convert the robot pose into ", " frame. For example between sessions, each time rtabmap is localized against the same location, ", " will be always the same (the same pose in the map independently of the odometry value).", "Note that ", " is published only when the robot is first localized. On start, make sure the robot can recognize where it is (i.e., it is actually at a real location in the map).", "Start the camera:", "Create a map (clear the database and use RVIZ):", "When loop closures are detected, the ", " transform should change:\n", "Now kill ", " to save the database, then relaunch in localization mode:", "Move the camera in the same area than in the map, until a localization happens, then ", " will be published again (with some values depending on where you started the camera the second time):\n", "Note that here I clicked ", " on the ", " plugin to get back the map shown in RVIZ.", "cheers", " is always 0.000 for me. We confirmed ", " is false, and no matter where we start (we even checked the database to make sure the camera frame matches up almost perfectly with an image saved in the database), ", " and ", " are the same frame.", "Just to confirm, where is the parameter ", " set?", "I updated the answer. It is set internally to its default value (false). To change it, you can use ", ". You can also set ", " under ", " tag", "All RTAB-Map core parameters can be set that way. To see a complete list off all parameters: ", "Thanks for the update. Unfortunately, ", " is still always published as zero, even when there is a loop closure. That said, there is almost never a loop closure. It seems that downloading the cloud first may be necessary? We get more loop closures when we do this, but it takes 30 minutes.", "I still can't get it working. Is rviz mode required for localization? I'm using the rtabmapviz mode. It seems that loop closures are occurring (green flashes) but ", " still outputs zero values.", "Downloading the cloud in RVIZ is not necessary, it is just for visualization. RVIZ is not necessary either, it should work with or without ", " too. Do you have a video of the problem? Which version are you using? How do you launch the nodes? You can update your question for clarity.", "The project's not public but we can send you an email with the video. We're using version 0.11.11 that we built with eigen, gtsam, cvsba, g2o, and mkl. We're launching with a launch file that ", "s a copy of rtabmap.launch from rtabmap_ros, as well as starting kinect2_bridge."], "question_code": ["map"], "answer_code": ["RGBD/OptimizeFromGraphEnd", "false", "false", "rtabmap", "/map -> /odom", "/map", "/map -> /base_link", "/map -> /odom", "$ roslaunch freenect_launch freenect.launch depth_registration:=true\n", "$ roslaunch rtabmap_ros rtabmap.launch rtabmap_args:=\"--delete_db_on_start\" rtabmapviz:=false rviz:=true\n", "/map -> /odom", "rtabmap.launch", "$ roslaunch rtabmap_ros rtabmap.launch localization:=true rtabmapviz:=false rviz:=true\n", "/map -> /odom", "/map -> /odom", "RGBD/OptimizeFromGraphEnd", "/map", "/odom", "RGBD/OptimizeFromGraphEnd", "$roslaunch rtabmap_ros rtabmap.launch rtabmap_args:=\"--RGBD/OptimizeFromGraphEnd true\"", "<param name=\"RGBD/OptimizeFromGraphEnd\" type=\"string\" value=\"true\"/>", "rtabmap", "$rosrun rtabmap_ros rtabmap --params", "/map -> /odom", "rosrun tf tf_echo /map /odom", "rtabmapviz", "<include>"], "url": "https://answers.ros.org/question/252351/persistent-map-frame-in-rtab-map-localization-mode/"},
{"title": "How to do SLAM with just RaspberryPi3 PiCam and IMU Sensor with ORBSLAM2 ?", "time": "2017-03-15 04:15:31 -0600", "post_content": [" ", " ", "I am senior student at Aerospace Engineering. I am creating indoor ground robot and i want to do SLAM with RaspberryPi3 Pi camera and IMU sensor in indoor areas for improving my robotic level(i am a new). When i searched to web i founded ORBSLAM2 and i installed to my raspberry pi3 successfully but i don't ensure to is it possible it can do with just pi camera and imu sensor? If is it possible do you have any suggestion to read anything or keywords for search  ?\nRegards "], "answer": [" ", " ", "I think you will find it challenging to implement without any perception of depth. You would have to \"fabricate\" the environment based on 2D-computervision, providing pointcloud / laserscan to the stack. ", "The RPi probably lacks the computing power also; but that is an assumption based on experiences working with depth perception - which implies the amount of data published by these devices. ", "Don't take my word for anything, I'm just a couple of months into ROS myself. ", "thanks for your replay, do you can i get depth perception with opencv ?"], "url": "https://answers.ros.org/question/257037/how-to-do-slam-with-just-raspberrypi3-picam-and-imu-sensor-with-orbslam2/"},
{"title": "ROS Battery Status", "time": "2017-02-16 17:08:11 -0600", "post_content": [" ", " ", "Hi,", "Last December, my team and I participated in the first Maritime\u00a0RobotX Challenge is an international team competition uniquely designed to evolve into a multi-platform competition to include maritime, aerial and submersible tasks to broaden students\u2019 exposure to robotics applications and technologies.\u00a0Our team utilized the ROS for some tasks, such as Simultaneous Localization and Mapping (SLAM), Path Planning, and Color Recognition. I am a power system lead of the team, and I have been trying to program in ROS to check the status of our lead acid (standard car) batteries as well as some Lithium Ion battery packs. Unfortunately, none of the team members knows how to approach to this problem. Please help!", "Duplicate of ", "How about using the ", "? You publish a diagnostic message with the current battery value.", "Check out "], "answer": [" ", " ", "There's a pretty substantial gap between writing software and checking the state of your batteries, and you'll need support from your hardware to cross that gap.", "At the most basic level, battery monitoring is generally done by measuring the battery voltage, measuring and integrating the current through the battery, or both. This is an entire field of study with chemistry and is a bit too broad to cover here, but it's important to remember that the relationship between battery capacity and voltage can be highly non-linear depending on your battery chemistry.", "I'd suggest that you buy (or possibly build) hardware that can measure the SOC (state of charge) of your batteries, and then connect that hardware to your computer and write a ROS node that translates that data into a ", " message. I believe there are dashboard widgets that can consume and display this message in a reasonable way.", "P.S. - your question makes reference to a demo, but doesn't link to it. ROS is a very broad community, and most of us aren't familiar with all of the demos, so a link would be helpful."], "url": "https://answers.ros.org/question/254899/ros-battery-status/"},
{"title": "Turtlebot Create2 base_link location", "time": "2017-03-15 17:06:29 -0600", "post_content": [" ", " ", "According to REP 105, ", " should be in the center of rotation of the base, in this case a iRobot Create 2. What I can't figure out is where it is vertically. Is ", " at floor level or at the bottom of the robot or at the top or some other place?\nThanks,\nluketheduke"], "answer": [" ", " ", "A good place for the center of rotation would be the center of the wheels axle. In the case of the Create platform this should be about where the main power button is horizontally and center of the wheel vertically. The bottom (or floor) of the robot should be where you'd find a ", " frame."], "question_code": ["base_link", "base_link"], "answer_code": ["base_footprint"], "url": "https://answers.ros.org/question/257142/turtlebot-create2-base_link-location/"},
{"title": "ROS on Raspberry 3", "time": "2016-03-24 13:24:59 -0600", "post_content": [" ", " ", " ", " ", "Hello,\nIs there anyone already install ROS on raspberry pi 3?", "I have not heard any success stories yet. I'd suggest you start with an installation of Ubuntu 14.04 if possible and then follow the UbuntuARM instructions.", "Ubuntu mate?", "Most of the Ubuntu MATE images that I see are based on Ubuntu 15.04, which isn't supported by the ARM builds of ROS.", "so which ubuntu should be install it in my raspberry? i don't found ubuntu for raspberry", "I can't find a version of Ubuntu 14.04 that is compatible with the Raspberry Pi 3 either.", "I used an Ubuntu 15,04 image and compiled ROS (core) from sources. I didn't have any problems."], "answer": [" ", " ", " I have just finished successfully installing ROS Indigo on my raspberry pi 3 with debian jessie. I installed from source using this guide:\n ", "The guide is greatly detailed, but you have to remember to be patient for it will take a lot of time and energy to successfully install ROS from source. And remember to apply the patches for \"collada-dom-dev\" and \"rviz\" if you want to install the full-desktop version.", "Hi. Could you please tell me where the patch for collada-dom-dev is? There is a link on the tutorials page but I couldn't find the patch on the link given. And how is it to be installed? \nThanks.", " \nFrom there, there is an attached file that you can download.\nthen go to folder: cd ~/ros_catkin_ws/src/robot_model/collada_urdf/src\nplace the file there and rename it to fix.patch\nthen apply the patch: patch < fix.patch ", "It is also mentioned in the guide to change the name from \"collada-dom\" to \"collada-dom-dev\". That also did not work for me as rosdep still could not find collada-dom-dev. I changed the name from \"collada-dom\" to \"libcollada-dom2.4-dp-dev\" and that fixed the issue.", "Thanks. But for me it seems the folder that you suggested is not/has not been installed. There's no robot_model folder. Do you know why that would be?", "are you installing ros command line or ros desktop version?", "ros command line version", "For the ros comm you don't need to build collada, you only need libconsole-bridge-dev and liblz4-dev. You can skip to the next step of the installation: resolving dependencies.", "Thank you !", " ", " ", "you can do a Chroot 14.04,  on the 15.04 system.\nI did this for the dragonboard 410c and it works.", "Most likely I will do this for the RPI3 or just compile natively  ", " ", " ", " One easy way: Ubuntu Mate Xenial ( ", " ) with ROS Kinetic  ( ", " )\nWorks fine on my RPI3 ", " This is the best way, worked also fine for me. I had to use the  shadow-fixed repos though ( ", " ) ", " ", " ", "You can use Debian jessy? ", " ", " ", " ", " ", "Try this.", "with raspberry Pi3 ubuntuMATE  ROS Kinetic", "you can just download the image .iso  about 8GB .zip", "hey \u0131 downloaded the \u0131mage file and copy the sd cart.I connect Raspberry Pi 3 with putty but \u0131 couldn't find user name and password. Can you tell me what they are  ?", " I use ssh  ", "    to access pi\nYou should had a router   connect your PI3 and laptop to same local network\nAnd setup IP for pi  ex: 192.168.1.101    so first time u need to use screen and HDMI wire to doing the setup ", "BTW, pi user is ubuntu, password also ubuntu", "thank you \u0131 succesfully login with putty but \u0131 dont understant when you said \"doing setup\" ? what do you mean?", " Oh if your are using linux you under terminal you can ssh ubuntu@ <ip> to your raspberry pi\n", "\nOnce  they are in the same local network</ip>", "I have just passed the problem (access denied) with remote connection to pi 3 (Ubuntu Mate 16.04) using PuTTY and MobaXTerm. So, there are many options to fix it when u google it. But, the solution was easy for me. The login is root (not pi)", "thank you guys \u0131  \u0131 succesfully login with ssh", " ", " ", "I installed ", " and then followed the ", " and everything works fine.", "did you installed Rviz also?", "No, I did not install Rviz.", "For me it is giving below error,\nThe following packages have unmet dependencies:\n ros-indigo-ros-base : Depends: ros-indigo-actionlib but it is not going to be installed\n                       Depends: ros-indigo-bond-core but it is not going to be installed ....", " ", " ", "I have used it, in particular, I have used ubuntu mate 16.06 with ROS Kinetic. I have successfully driven pioneer robot with it using the tutorial given here (", "). It works fine. Best of luck to your project. ", " ", " ", "We have a Python ", " if that helps.  ", "You can also ", " does here. ", "Hope this helps.  Let me know if you have any questions.  ", "Cheers\nJack"], "url": "https://answers.ros.org/question/230076/ros-on-raspberry-3/"},
{"title": "How is the Hokuyo UST 10lx LIDAR connected?", "time": "2017-03-21 10:35:34 -0600", "post_content": [" ", " ", "There are two cables -  the ethernet and the a power cable. The power cable as 6 wires out of which two go to the power source of 12 V. Where do the other four wires go? What are they connected to? The ethernet cable was connected to a Jetson TK1 through a ethernet to USB connector. "], "answer": [" ", " ", "I don't have enough points to post a picture, but it's on page 5 of this specification manual.", "Hello,thank you for your answer. I saw that but I'm still confused as to what those wires are connected to. What is the I/O power source? Where is the IP reset?", " ", " ", "The two wires are as below:\n1. The ethernet wire is used for data reading and it can be connected to ethernet port of the pc.\n2. The other colorful wire is required for powering the lidar as well as for reading the data serially. (I will recommend to use it for powering the lidar only). For power, just connect the constant DC power supply to the corresponding pins and don't connect the rest of the pins. \n(Note: make sure you connect the right pins to power supply as wrong connections may damage the lidar ) \nYou can use \"urg_node\" package of ros for getting started with UST-10lx. Check this ", ", it might help you to some extent."], "url": "https://answers.ros.org/question/257525/how-is-the-hokuyo-ust-10lx-lidar-connected/"},
{"title": "dynamixel ax12 tutorial", "time": "2017-04-19 11:24:42 -0600", "post_content": [" ", " ", " I have acquired a Dynamixel AX12 + Dynamixel2USB, and I'm trying to follow up the next tutorial:\n ", "Once I reach point 2, I'm not able to discover the motor. The error is just:", "[INFO] [WallTime: 1492617892.352673] pan_tilt_port: Pinging motor IDs 1 through 25...\n  [FATAL] [WallTime: 1492617894.452074] pan_tilt_port: No motors found.", "I have checked if the FTDI kernel module is working and it's loaded, checked with ", " & ", "Also I have attached an oscilloscope to the motor DATA and see the RX signal is reaching the servomotor, but is not responding.", "I'm feeding the motor with ~12v. And I'm using this pinout:", "Anyone has experienced this behaviour before?\nThanks in advance!", "I'm working under Kinetic  + I have slow down the launch file baud rates to 9600"], "answer": [" ", " ", "Does the Dynamixel RoboPlus software work with the servo? It's Windows-only, unfortunately, but testing with that first will tell you if the problem is hardware or software.", "Looks like the problem was that the Dynamixel were not responding due to power grounding. I have shared the USB and power supply grounds together and now works. Not sure why is happening, becuase this should be already done using the Dynamixel connectors that are daisy chauined...", "I think you can test with the dynamixel_workbench ", "   // However, I am not sure. Still trying to get dynamixel_sdk working with ROS. "], "question_code": ["lsmod", "modinfo"], "url": "https://answers.ros.org/question/259640/dynamixel-ax12-tutorial/"},
{"title": "roslaunch openni_launch openni.launch no devices connected", "time": "2013-08-01 16:19:24 -0600", "post_content": [" ", " ", " ", " ", "I'm just starting out trying to use ROS with my Primesense device (Asus Xtion Pro Live) on Ubuntu 12.10 and ROS groovy. I'm trying to perform ", "but I'm faced with ", "This is the output of lsusb:", "It seems like my Primesense device isn't recognised (first row above). The question is, where can I download the appropriate drivers? I'm a bit confused because I can run ./SimpleViewer in the OpenNI samples (doesn't this mean the drivers are installed?). I've read a few other posts regarding this, including", "/", "and this", "/", "but nothing works. I've also tried looking up the Asus drivers download page here", "and noticed that it's quite outdated (OpenNI v1.5 and NiTE v.1.5, when v2.2 is out). I already have OpenNI v2.2 installed. Should I be installing NiTE 2.2? I ask because it's categorized as a Middleware Library. ", "Thanks!", "I see a problem like you, have you solved it successfully?", "Yes, the problem was that I installed the primesense drivers for v1.5, but my OpenNI version was v2.2. I downloaded OpenNI v1.5 instead and it worked.", "So how about 's the version of sensor driver?", "Do you use the Live USB installation or Debs ? I use USB Installation, I tested successfully ASUS XTION with NIViewer it can receive data from Xtion , but when I tested with openni.launch, I see it notice :\n[camera/depth/points-10] process has died [pid 12126, exit code 255, cmd /opt/ros/groovy/lib/nodelet/nodelet load depth_image_proc/point_cloud_xyz /camera_nodelet_manager --no-bond image_rect:=image_rect_raw __name:=points __log:=/home/turtlebot/.ros/log/134c8d7c-5714-11e3-8e6d-70f1a10ec189/camera-depth-points-10.log].\nlog file: /home/turtlebot/.ros/log/134c8d7c-5714-11e3-8e6d-70f1a10ec189/camera-depth-points-10*.log\nSo what problem with me?", "I updated my answer below, please check it. To fix your error, get the older version of openni_camera from source.", "I run the sample code follower successfull bu using openni2_launch and openni2_camera"], "answer": [" ", " ", " ", " ", "The problem was that I installed the Primesense drivers for v1.5, but my OpenNI version was v2.2. I downloaded OpenNI v1.5 from ", " instead and it worked.", "EDIT: Also, there is an issue with the openni_camera package. You can leave the deb package installed, as long as your compiled sources have priority. Use ", " and see if your source directory is first. This is what I have:", "Go to your sources directory, get the source code using git and use the ", " branch.", "Compile using catkin_make in the root of your catkin directory", "Finally, test it out with ", ".", "You can read more about this issue ", ".", "Primesense drivers can be downloaded from the OpenNI website. I'm using v5.1.2.1 from ", " (32-bit). 64-bit version is ", ". I think there are more recent versions over ", " if you like, but I haven't tried them. ", "Alternatively, if you choose to use v2.2, then you should install the packages openni2_camera and openni2_launch instead (see ", ").", "hi Andrew. A, I also have the same problem with your post. And I'm trying to solve it shown here ", ". Could you give me some advices? Thank you", " ", " ", "This problem which appears while running ", "Can have following two reasons as I know ....", "Kinect power supply is not connected properly (LED at kinect may be blinking but kinect not getting power supply of 12V DC for operation which comes either from 12V AC to DC adapter or from robot base).\nThis is the problem which normally happens. Please check the power supply.", "The USB of Kinect is not detected by the notebook. This is something that rarely happens.", "I'm not using a Kinect, but an Asus Xtion Pro Live. Also, I'm not sure why power supply is not connected properly, because as I mentioned, I can run the OpenNI sample Simpleviewer. \n\nIf the device isn't detected (which I think is the case), what can I do to solve this problem?", "I think power connection to device and device connection to the notebook are two main problem when openni replies. ", "\n[ INFO] [1375408552.874803204]: No devices connected.... waiting for devices to be connected. ", " \nI have not worked on  Asus Xtion Pro Live. so can not tell exactly.", "Make sure that you haven't plugged your camera in a USB 3.0 port. I advise not plugging it into a USB hub either - doing so caused me problems at some point. \n\nI suppose that you checked this already, but better be safe.", "Zayin, I've plugged my device in a USB 2.0 port, and it still doesn't detect. Also, apparently in OpenNI v2, there is no need for Primesense sensor drivers (", " (see comments below)).", "Do you think your problem could be related to this? ", " Your error message is totally different, but I'm the type to try anything. Btw, unlike the Kinect, the Xtion is powered by USB directly, so I doubt power is the issue.", "I don't have the Xtion and its laptop with me currently, but tomorrow I'll check what version of Openni I'm using (I'm not the one who installed it so I don't know). [Edit] I do have libopenni2-0. Then, there must be a way of making it work for you too. I'm using Fuerte and Ubuntu 12.04, though."], "question_code": ["roslaunch openni_launch openni.launch\n", "[ INFO] [1375408552.874803204]: No devices connected.... waiting for devices to be connected\n", "Bus 001 Device 008: ID 1d27:0601  \nBus 005 Device 002: ID 413c:2003 Dell Computer Corp. Keyboard\nBus 005 Device 003: ID 0461:4d22 Primax Electronics, Ltd \nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\nBus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub\n"], "answer_code": ["echo $ROS_PACKAGE_PATH", "$ echo $ROS_PACKAGE_PATH\n/home/andrew/catkin_ws/src:/opt/ros/groovy/share:/opt/ros/groovy/stacks\n", "rollback_usb", "cd ~/catkin_ws/src\ngit clone https://github.com/ros-drivers/openni_camera.git\ngit checkout rollback_usb\n", "cd ~/catkin_ws\ncatkin_make\n", "roslaunch openni_launch openni.launch", "roslaunch openni_launch openni.launch\n"], "url": "https://answers.ros.org/question/70585/roslaunch-openni_launch-opennilaunch-no-devices-connected/"},
{"title": "strange pitch direction", "time": "2017-05-04 01:57:37 -0600", "post_content": [" ", " ", " ", " ", "Hello!\nI have some troubles understanding rpy. Here is my simple URDF file:", "As you see, there are two links with a revolute joint between them. There is a pitch of -0.5 radians in the joint.\nAnd here is the rviz image :", "The question is why ", " is rotated upwards instead of downwards?"], "answer": [" ", " ", "You specify a ", " of -0.5 rads, this is a rotation around the y axis. The y axis is denoted in green. As (by convention) a right handed frame is used, you can use the ", " to determine the direction or rotation. From what I can tell, the rotation on the picture is correct. ", "thank you!"], "question_code": ["<robot name=\"test\">\n<link name=\"Body\">\n  <inertial>\n    <mass value=\"4.0\" />\n    <origin xyz=\"1 0 0\"/>\n    <inertia ixx=\"1\" ixy=\"1\" ixz=\"1\" iyy=\"1\" iyz=\"1\" izz=\"1\" />\n  </inertial>\n  <visual>\n    <geometry>\n      <box size=\"0.1 0.02 0.02\"/>\n    </geometry>\n\n  </visual>\n</link>\n\n\n<link name=\"link1\">\n  <inertial>\n    <mass value=\"4.0\" />\n    <origin xyz=\"1 0 0\"/>\n    <inertia ixx=\"1\" ixy=\"1\" ixz=\"1\" iyy=\"1\" iyz=\"1\" izz=\"1\" />\n  </inertial>\n  <visual>\n    <geometry>\n      <box size=\"0.1 0.02 0.02\"/>\n    </geometry>\n\n  </visual>\n\n</link>\n\n\n<joint name=\"j1\" type=\"revolute\" >\n  <parent link=\"Body\"/>\n  <child link=\"link1\"/>\n  <origin xyz=\"0.1 0 0\" rpy=\"0 -0.5 0\" />\n  <axis xyz=\"1 0 0\"/>\n  <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"10\"/>\n</joint>\n\n\n</robot>\n", "link1"], "url": "https://answers.ros.org/question/261022/strange-pitch-direction/"},
{"title": "schunk_canopen_driver crashes at startup", "time": "2016-12-09 11:08:47 -0600", "post_content": [" ", " ", "Hi,", "i am using the schunk_canopen_driver with the schunk_lwa4p robot. I run the standalone_interpolated_position.launch file.\nEverything works (after some time), BUT I have observed two things, that seem odd to me:", "When starting the robot, the node throws:", "<2016-12-09 17:54:19.291> CanOpen(Info) EMCY::update: Error reset EMCY received. Node 4 is now in state error free.\nterminate called after throwing an instance of 'icl_hardware::canopen_schunk::DeviceException'\n  what():  \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdt perform fault reset for node 4 after multiple tries. Is the device hard-disabled?\nOtherwise: Have you tried turning it off and on again? Check your configuration and make sure the device is properly connected.\n[schunk_canopen_node-1] process has died [pid 14265, exit code -6, cmd /home/marcel/ros/test_ws/devel_isolated/schunk_canopen_driver/lib/schunk_canopen_driver/schunk_canopen_driver_node __name:=schunk_canopen_node __log:=/home/marcel/.ros/log/07d1c7d2-be13-11e6-ad72-001e8c26fa47/schunk_canopen_node-1.log].\nlog file: /home/marcel/.ros/log/07d1c7d2-be13-11e6-ad72-001e8c26fa47/schunk_canopen_node-1*.log", "This is thrown once per node, i.e. in the first run, it is thrown for node 3, second node 4, ...., which means, i have to rerun the script once per node. This occurs on every startup, i.e. when start the robot (not the node).", "After the first thing occured and the node now may start flawlessly, an error occurs irregularly, but only at startup, i.e. when no error occurs, the node runs perfectly:", "[INFO] [1481301065.242754]: Loading controller: pos_based_pos_traj_controller_arm\nTraceback (most recent call last):\n  File \"/opt/ros/kinetic/lib/controller_manager/spawner\", line 212, in <module>\n    if __name__ == '__main__': main()\n  File \"/opt/ros/kinetic/lib/controller_manager/spawner\", line 190, in main\n    resp = load_controller(name)\n  File \"/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/impl/tcpros_service.py\", line 435, in __call__\n    return self.call(", "kwds)\n  File \"/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/impl/tcpros_service.py\", line 505, in call\n    raise ServiceException(\"unable to connect to service: %s\"%e)\nrospy.service.ServiceException: unable to connect to service: [Errno 104] Connection reset by peer\n[INFO] [1481301065.448173]: Shutting down spawner. Stopping and unloading controllers...\n[schunk_canopen_node-1] process has died [pid 24481, exit code -11, ...]", "This seems like it has trouble loading the controller. After killing and rerunning the launch file several times (this occurs irregularly), sometimes it just works.", "After I have gone through the first part and when the second part finished (This can be fast or take some time..), I can finally use the node normally.", "Does anyone work with the schunk_canopen_driver and has the same issues (or not). It bugs me, this behaviour seems not to be normal.", "Thanks,\nMarcel", "Full log of 2:", "Sorry, ROS seems to not accept the first code snippets although they are formatted correctly.", "Hi @marcelusai, did you find the solution to this problem?"], "answer": [" ", " ", "Hey Marcel,", "thanks for the detailed writeup. ", "The message indicating that the state Error Free was reached is perfectly normal. As you have terminated the software with the nodes still running before (as you have not disconnected the power) the NMT registers that the nodes are in a different state than expected. This warrants a state change and subsequent error clearing after which the EMCY reports that all errors have been cleared.", " However, after that the nodes are re-initialized and a commutation search is done. As stated in another question (see  ", "  ) we have identified a sleep which most likely triggers a PDO-timeout during this routine. Your log states that an RPDO timeout has occurred so it might be the same issue. You could also try the snippet stated there and see if that brings any change.  ", "Please tell me if that worked of the problem still remains. If successful the patch will of course be applied to the master.", "Best Regards\nGeorg", "Thanks for your answer!\nDoes the first part (about the error free state) mean, that at shutdown, I should first disable all joints, power off the robot and then terminate the program? - I always terminated the program before powering off the robot.\nI will now look at the link and test the snippet.", "Unfortunately, this does not solve the issue. I get this error for the standalone_interpolated_position.launch, the standalone_profile_position.launch works fine. Additionally we have one computer where the code works, and tried to get the code running on 3 different machines, with no success.", "Some of the machines had the same configuration (Xubuntu 14.04, ros indigo) and some had other configurations (xubuntu 16.04, ros kinetic). Are some additional packages required? I am wondering, why it is working on this specific machine and not on the others. Drivers are installed correctly."], "question_code": ["process[schunk_canopen_node-1]: started with pid [24481]\nprocess[ros_control_controller_spawner-2]: started with pid [24509]\n<2016-12-09 17:31:03.093> CanOpen(Info)::init: CAN Device was set to auto. \n<2016-12-09 17:31:03.093> CanOpen(Info)::init: Trying CAN device: /dev/pcanusb0... flags:2050\nprocess[robot_state_publisher-3]: started with pid [24510]\nthe rosdep view is empty: call 'sudo rosdep init' and 'rosdep update'\n[INFO] [1481301063.727872]: Controller Spawner: Waiting for service controller_manager/load_controller\nCan Init successful!\n<2016-12-09 17:31:04.280> CAN ..."], "url": "https://answers.ros.org/question/249759/schunk_canopen_driver-crashes-at-startup/"},
{"title": "\"Node has emergency error\" : Issue on Lwa4D", "time": "2017-03-31 11:46:29 -0600", "post_content": [" ", " ", "Hello,", "I try to set up a Schunk Lwa4d with PG70 gripper, using:", "H/W:", "S/W:", "I manage to initialize the arm driver successfully. After launching the dashboard, I try to execute one of the available actions (home, wave etc), but I get a jitter-like noise from my motors, followed by a \"Node has emergency error\" error.", "The actual errrors should be listed in ", ". If you launch with ", " you should get the errors printed to stdout as well. (requires v0.6.6 or newer).\nInstead of using the dashboard, you should try the contoller manager plugin for ", "."], "answer": [" ", " ", "I was finally able to solve my problem. After taking a look at the  ", " topic and executing ", ", I realized that the robot was outputting CANopen EMCY codes that indicated an issue with the motor current. Even though the wiring I initially used to connect the robot to the power supply complied with the LWA4D voltage and current specifications, I changed it and used thicker wires, and now the robot moves normally. ", "Thanks for finding this!  Do you remember what the specific error code was, and if there was any other info in the emcy message?"], "question_details": [" ", " ", " ", " ", " ", " ", "Schunk Lwa4d", "Schunk Pg70 gripper", "Peak CAN-USB interface device", "Ubuntu 14.04 (Kernel 4.4.0-31-generic)", "ROS Indigo", "schunk_modular_robotics , schunk_robots packages", "ros_canopen"], "question_code": ["schunk@schunk-CS-B:~$ roslaunch schunk_lwa4d robot.launch \n... logging to /home/schunk/.ros/log/987657f0-162c-11e7-bac4-40167ea4b96c/roslaunch-schunk-CS-B-30697.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://schunk-CS-B:43046/\n\nSUMMARY\n========\n\nCLEAR PARAMETERS\n * /arm/driver/\n\n\nPARAMETERS\n\n * /arm/arm_1_joint_position_controller/joint: arm_1_joint\n * /arm/arm_1_joint_position_controller/required_drive_mode: 1\n * /arm/arm_1_joint_position_controller/type: position_control...\n * /arm/arm_1_joint_velocity_controller/joint: arm_1_joint\n  * /arm/arm_1_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_1_joint_velocity_controller/type: velocity_control...\n  * /arm/arm_2_joint_position_controller/joint: arm_2_joint\n  * /arm/arm_2_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_2_joint_position_controller/type: position_controll...\n  * /arm/arm_2_joint_velocity_controller/joint: arm_2_joint\n  * /arm/arm_2_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_2_joint_velocity_controller/type: velocity_controll...\n  * /arm/arm_3_joint_position_controller/joint: arm_3_joint\n  * /arm/arm_3_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_3_joint_position_controller/type: position_controll...\n  * /arm/arm_3_joint_velocity_controller/joint: arm_3_joint\n  * /arm/arm_3_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_3_joint_velocity_controller/type: velocity_controll...\n  * /arm/arm_4_joint_position_controller/joint: arm_4_joint\n  * /arm/arm_4_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_4_joint_position_controller/type: position_controll...\n  * /arm/arm_4_joint_velocity_controller/joint: arm_4_joint\n  * /arm/arm_4_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_4_joint_velocity_controller/type: velocity_controll...\n  * /arm/arm_5_joint_position_controller/joint: arm_5_joint\n  * /arm/arm_5_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_5_joint_position_controller/type: position_controll...\n  * /arm/arm_5_joint_velocity_controller/joint: arm_5_joint\n  * /arm/arm_5_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_5_joint_velocity_controller/type: velocity_controll...\n  * /arm/arm_6_joint_position_controller/joint: arm_6_joint\n  * /arm/arm_6_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_6_joint_position_controller/type: position_controll...\n  * /arm/arm_6_joint_velocity_controller/joint: arm_6_joint\n  * /arm/arm_6_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_6_joint_velocity_controller/type: velocity_controll...\n  * /arm/arm_7_joint_position_controller/joint: arm_7_joint\n  * /arm/arm_7_joint_position_controller/required_drive_mode: 1\n  * /arm/arm_7_joint_position_controller/type: position_controll...\n  * /arm/arm_7_joint_velocity_controller/joint: arm_7_joint\n  * /arm/arm_7_joint_velocity_controller/required_drive_mode: 2\n  * /arm/arm_7_joint_velocity_controller/type: velocity_controll...\n  * /arm/driver/bus/device: can0\n  * /arm/driver/defaults/eds_file: config/Schunk_0_6...\n  * /arm/driver/defaults/eds_pkg: schunk_lwa4d\n  * /arm/driver/defaults/vel_to_device: rad2deg(vel)*250\n  * /arm/driver/name: arm\n  * /arm/driver/nodes/arm_1_joint/id: 3\n  * /arm/driver/nodes/arm_2_joint/id: 4\n  * /arm/driver/nodes/arm_3_joint/id: 5\n  * /arm/driver/nodes/arm_4_joint/id: 6\n  * /arm/driver/nodes/arm_5_joint/id: 7\n  * /arm/driver/nodes/arm_6_joint/id: 8\n  * /arm/driver/nodes/arm_7_joint/id: 9\n  * /arm/driver/sync/interval_ms: 10\n  * /arm/driver/sync/overflow: 0\n  * /arm/joint_group_interpol_position_controller/joints: ['arm_1_joint', '...\n  * /arm/joint_group_interpol_position_controller/required_drive_mode: 7\n  * /arm/joint_group_interpol_position_controller/type: position_controll...\n  * /arm/joint_group_position_controller/joints: ['arm_1_joint', '...\n  * /arm/joint_group_position_controller/required_drive_mode: 1\n  * /arm/joint_group_position_controller/type: position_controll...\n  * /arm/joint_group_velocity_controller/joints: ['arm_1_joint', '...\n  * /arm/joint_group_velocity_controller/required_drive_mode: 2\n  * /arm/joint_group_velocity_controller/type: velocity_controll...\n  * /arm/joint_limits/arm_1_joint/has_acceleration_limits: False\n  * /arm/joint_limits/arm_1_joint/has_effort_limits: False\n  * /arm/joint_limits/arm_1_joint/has_jerk_limits: False\n  * /arm/joint_limits/arm_1_joint/has_velocity_limits: False\n  * /arm/joint_limits/arm_1_joint/max_acceleration: 0.4\n  * /arm/joint_limits/arm_1_joint/max_effort: 5.0\n  * /arm/joint_limits/arm_1_joint/max_jerk: 100.0\n  * /arm/joint_limits/arm_1_joint/max_velocity: 0.4\n  * /arm/joint_limits/arm_2_joint/has_acceleration_limits: False\n  * /arm/joint_limits/arm_2_joint/has_effort_limits: False\n  * /arm/joint_limits/arm_2_joint/has_jerk_limits: False\n  * /arm/joint_limits/arm_2_joint/has_velocity_limits: False\n  * /arm/joint_limits/arm_2_joint/max_acceleration: 0.4\n  * /arm/joint_limits/arm_2_joint/max_effort: 5.0\n  * /arm/joint_limits/arm_2_joint/max_jerk: 100.0\n  * /arm/joint_limits/arm_2_joint/max_velocity: 0 ...", "/diagnostics", "--screen", "rqt"], "answer_code": ["/diagnostics", "candump can0"], "url": "https://answers.ros.org/question/258357/node-has-emergency-error-issue-on-lwa4d/"},
{"title": "Managing large video streams (4K ...)", "time": "2017-05-17 10:52:57 -0600", "post_content": [" ", " ", "Hi,\nWe want to equip our car mounted system with recording capability for high-quality video streams, possibly several Full HD and maybe 4K cameras. We need frames from those cameras to be recorded with a timestamps synchronized with other sensors (GPS,...) and to be stored at reasonable framerate (15-30 FPS). What is generally a good approach for implementing this?", "Currently we have 4 Full HD webcams in the system connected to a laptop through a single USB 2.0 port and we already face serious problems. The USB capacity prevents us to set the full resolution (ROS complains on startup of the nodes) so we have to run at 720p and 15 fps. Even with this, we only record around 7 fps on our Core i7 laptop. I suspect that the on-the-fly compression to jpeg (frames are stored compressed) can be the problem. I do not see this scaling very well. Do we need multiple USB controllers, multiple PCs, LAN based cameras (e.g. PointGrey)? Or is recording frames as JPEG images instead of a video codec (e.g. H264) even a good practice? It seems very wasteful to me. Is a more reasonable approach to record each stream as a separate video using an external SW (not ROS) and just note the time of the first frame? Would timing for the following frames be reliable? We can tolerate error of 500 ms over recording duration of 1 hour.", "Thanks!", "I have no real experience with this, but if you're not processing your video in real-time / while capturing it, I would try not to use ROS for the capture. As you already wrote, there are other ways to capture video streams, which are probably more efficient than storing individual raw / jpeg ..", ".. shots in a directory / bag file. The 'only' problem would indeed seem to be the sync between your video streams and other sensor data. If your capture system encodes a (wall) clock into your container then you should be able to sync everything using that. A small node that then decodes the ..", ".. video stream and uses the time code as the ROS timestamp could be an option. Or -- and probably more efficient -- use the embedded time codes to convert your video containers into bag files. That would all be off-line, so should reduce problems with resource usage that you now have...", ".. After conversion to bag files, you can then process things as you would normally do.", "gvdhoorn: Yes, that is the most likely path we are going to take. For our later application we need random access to frames anyway so it would then make sense to extract images as separate files and skip the bags completely."], "answer": [" ", " ", "I've run into the same issue but my application was tolerant of having a reduced frame rate so I haven't really solved it, but have thought about some approaches:", "If the camera can output mjpg directly then it should be possible for a ros node to grab that stream and reformat into the ros jpg format without cpu consuming decompression and re-compression.  I don't know if any nodes can do that now.  The quality of the time stamping is an unknown in that case.", "What is the minimally expensive/sized/power-consuming embedded board that can connect to a single usb camera and capture and compress at full resolution and frame rate?  Every camera could be paired with an embedded board.  It would be nice to have gigabit ethernet, so uncompressed output would also be an option, that rules out the Pi and similar boards.  The original Pi is not powerful enough, I don't know about the 3- and if the board costs more than $100 then the aggregate price starts to approach that of a small low cost gige camera (which won't produce compressed output, but can be very good quality, have roi functions, and can have different lenses, though not autofocus or powered zoom without additional cost and complexity...).", "The nice thing about the consumer usb cameras is that they can be good quality for the price, the economy of scale of skype/video-chat helps that.", "Or go with the industrial gigabit ethernet cameras, but also pair them each with embedded boards (which need to have gigabit ethernet) for compression- but the cost here starts to get high.  The gigE cameras are going to provide the bests solution to synchronization (you can have digital pulse signals sent to each camera to trigger frames, or possibly record in the camera the time of arrival of a lower rate sync pulse), but your 500ms seems so large that it doesn't require that.", "Another route is the network camera which is usually aimed at the security market, which are sort of the reverse of the gige camera in that they might have only compressed output but no full quality raw images.  Interchangeable lenses are possible too, and they are lower cost.  I think there are some standard data stream formats meant for dvrs.  I've seen some ros driver support but I haven't looked into it much.", "It would be great to hear from anyone else already doing any of this rather than going into a lengthy integration r&d effort to select systems and pair them and test them out.", "What I have actually done is throw additional computers at the problem, allocating different tasks among them- separate out compression and recording and cpu intensive real-time critical stuff onto different systems as much as possible (though issues with network bandwidth can arise)."], "url": "https://answers.ros.org/question/261964/managing-large-video-streams-4k/"},
{"title": "possibility of using PCL (Euclidean Cluster Extraction) for object detection", "time": "2015-01-30 10:30:53 -0600", "post_content": [" ", " ", " ", " ", "Dear All,", "I need to detect objects (1-3) using Kinect at the same time. I did some research but I could not find any package for objects detection in ROS. I want to use depth information as input data and finally calculate distance of objects to camera. For objection detection, do I employ clustering methods? for example, Euclidean Cluster Extraction is provided by PCL so could I use ", " in a ROS node? "], "answer": [" ", " ", " ", " ", "An object recognition package (which is now slightly outdated, but still working) is ", " ", "You can also use Point Cloud Library (PCL). Our research lab uses PCL, as well as specific feature descriptors, such as CVFH. PCL can sometimes be a pain to work with, but it's quite powerful and has some decent descriptors for object recognition.", "Look at these pages for more information:", "Is there anyway to find moving objects.... whatever the object is ... could be human or any other thing ? I have a kinect 3d camera and usinf ROS indigo?"], "url": "https://answers.ros.org/question/202122/possibility-of-using-pcl-euclidean-cluster-extraction-for-object-detection/"},
{"title": "Unable to show messages", "time": "2017-07-06 04:03:05 -0600", "post_content": [" ", " ", " ", " ", " I've seen similar questions, but they do not solve my problem. \n ", "I am using Ubuntu 12.04 and ROS Indigo.", "If I like my messages to be shown by the command: rosmsg show only the standard ROS-messages are showing up. (Using tab) If I use the name of the msgs like rosmsg show ATestMsg.msg the message can not be found. (error: Could not find msg 'ATestMsg')", "My messages can't be found. I think I've done everything the same as in the tutorial explained. Neither source devel/setup.bash nor catkin clean and rebuild seem to solve the problem.", "My CMakeList", "And my package.xml", "Did somebody ever occur similar problem? For any solution I would be very thankful."], "answer": [" ", " ", "I could solve the problem - it seems to be a very trivial one.", "The resourcing \"source devel/setup.bash\" needs to be done in every console window.", "you can add it to your .bashrc file", ": thanks for reporting back.", "Could I ask you to just ", " your own answer instead of closing the question? We try to avoid closing questions here on ROS Answers, as it's not visible from the question list why that was done. An answered question gets a green checkmark.", "Ok, sorry about that. I can not do that as it requires more than 10 points.", "No need to apologise. I've accepted it for you."], "question_code": ["cmake_minimum_required(VERSION 2.8.3)\nproject(a_vector_field)\n## Use C++11\n## add_definitions(--std=c++11)\n## Find catkin macros and libraries\nfind_package(catkin REQUIRED COMPONENTS\n  visualization_msgs\n  magnetic_controller\n  mag_msgs\n  tf\n  roscpp\n   rospy\n   std_msgs\n   message_generation\n)\nfind_package(Qt4 COMPONENTS QtCore QtGui)\n###################################\n## catkin specific configuration ##\n###################################\n## The catkin_package macro generates cmake config files for your package\n## Declare things to be passed to dependent projects\n## INCLUDE_DIRS: uncomment this if you package contains header files\ninclude_directories(include\n ${catkin_INCLUDE_DIRS}\n${mag_msgs_INCLUDE_DIRS}\n)\nadd_message_files(\n   FILES\n   CurrentsStamped.msg\n   FieldStamped.msg\n   ATestMsg.msg\n   SecondTestMsg.msg\n )\nadd_service_files(\n  FILES\n  get_magnetic_field.srv\n  load_calibration_file.srv\n  set_magnetic_field.srv\n  set_magnetic_field_gradient.srv\n  #set_position.srv\n)\ngenerate_messages(\n   DEPENDENCIES\n   std_msgs\n   mag_msgs\n)\nINCLUDE(${QT_USE_FILE})\nADD_DEFINITIONS(${QT_DEFINITIONS})\ncatkin_package(\n  INCLUDE_DIRS\n    include\n## CATKIN_DEPENDS: catkin_packages dependent projects also need\n  CATKIN_DEPENDS \n    mag_msgs\n    message_runtime\n    roscpp\n## DEPENDS: system dependencies of this project that dependent projects also need\n#  DEPENDS \n)\ninclude_directories(\n  include\n ${mag_msgs}\n ${catkin_INCLUDE_DIRS}\n ${armadillo_INCLUDE_DIRS}\n ${blas_INCLUDE_DIRS}\n ${lapack_INCLUDE_DIRS}\n)\n#add_dependencies(mag_msgs ${catkin_EXPORTED_TARGETS})\n###########\n## Build ##\n###########\nadd_custom_target(${PROJECT_NAME}_OTHER_FILES ALL WORKING_DIRECTORY ${PROJECT_SOURCE_DIR} SOURCES ${EXTRA_FILES})\n## Specify additional locations of header files\n## Your package locations should be listed before other locations\n## Declare a C++ executable\n##install(DIRECTORY include/${PROJECT_NAME}/\n##        DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION})\nadd_executable(${PROJECT_NAME}\n  src/${PROJECT_NAME}_node.cpp\n  src/magneticfield.cpp\n  src/avectorfield.cpp\n)\n## Specify libraries to link a library or executable target against\ntarget_link_libraries(${PROJECT_NAME}\n  ${catkin_LIBRARIES}\n${ARMADILLO_LIBRARIES}\n${BLAS_LIBRARIES}\n ${LAPACK_LIBRARIES}\n)\ntarget_link_libraries(${PROJECT_NAME} ${QT_LIBRARIES})\nadd_library(armadillo src/magneticfield.cpp src/avectorfield.cpp )\ninstall(TARGETS armadillo #blas lapack\n        ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}\n        LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}\n        RUNTIME DESTINATION ${CATKIN_GLOBAL_BIN_DESTINATION})\ninstall(DIRECTORY include/${PROJECT_NAME}/\n        DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}\n        PATTERN \".svn\" EXCLUDE)\n", "<?xml version=\"1.0\"?>\n<package>\n  <name>a_vector_field</name>\n  <version>0.0.0</version>\n  <description>The a_vector_field package</description>\n  <license>TODO</license>\n  <buildtool_depend>catkin</buildtool_depend>\n  <build_depend>roscpp</build_depend>\n  <build_depend>visualization_msgs</build_depend>\n  <build_depend>mag_msgs</build_depend> \n<buildtool_depend>mag_msgs</buildtool_depend> \n<build_depend>magnetic_controller</build_depend>\n <build_depend>libarmadillo-dev</build_depend>\n<build_depend>message_generation</build_depend>\n  <run_depend>message_runtime</run_depend>\n <run_depend>magnetic_controller</run_depend>\n  <run_depend>mag_msgs</run_depend>\n  <run_depend>roscpp</run_depend>\n  <run_depend>visualization_msgs</run_depend>\n  <export>\n  </export>\n</package>\n"], "url": "https://answers.ros.org/question/265569/unable-to-show-messages/"},
{"title": "What exactly is the OpenCR part?", "time": "2017-06-08 16:55:57 -0600", "post_content": [" ", " ", " ", " ", "I am not sure what it is. It looks like it might be an Arduino-compatible control board that is somehow especially well suited for ROS. But OTOH I see configurations that combine a Pi with an OpenCR so that confuses me. I've seen this: OpenCR3 and this: OpenCR5 but I am still not sure..."], "answer": [" ", " ", "Hi :)", "Thank you for your interest in OpenCR.\nThis is why OpenCR especially well suited for ROS.", "First, it has quite powerful main chip(ARM Cortex-M7 with floating point unit) and many digital and analog input/out pins(UART, SPI, I2C). It helps control and operating robot with various sensor data in precise period.", "Second, it support power outputs(3.3v, 5v, 12v). Users can use this power for their robots or sensors and it helps operating single board computer.", "Third, it can publish or subscribe topic using ", " . This main function helps ROS users want to use ROS library to their robots handily.", "Thanks.", "Thanks! I admit being a newbie here and just feeling my way. So typically is OpenCR combined with another processor (Pi, Arduino, NUC)? In other words if my robot is meant to carry all the processing onboard, would a good config be to combine an OpenCR with another more powerful processor, or if I h", "What is the expected price for the OpenCR?", "Are there already alternatives available? I'm looking for something that drives dc motors, stepper motors and has inputs for 7.2V Lipo (which it should convert to 5V to drive the raspberry pi).", " OpenCR can be connected to Pi, NUC that has USB port. For example, OpenCR is used control your robot and sense adjacent environment, while other computer(Pi, NUC, Laptop...) has more powerful processor calculates complex algorithm using given data from OpenCR.", " About price information reference please contact to  ", " . Thanks. ", "Can you point me to a code sample that shows how the OpenCR and the Pi divide up the work? I guess I don't understand, if I have a pi on board, why would I also need the openCr?", " SBC(Pi) and MCU board(OpenCR) has some differences. The MCU board is small and has real-time computing with low power(3.3v, 5v). SBC has operating system. These two electronic devices can cover each other's disadvantages(real-time and computing power). So we use both for robot.", " Hello,\nWhere exactly can we buy one? Is it the same as OpenCM:  ", " ", " ", "Thank you! Can you share a mechanical and circuit diagram of the interconnections maybe?", " Between OpenCR and RPi are communicated with USB port(microUSB to USB). This part is already included in TurtleBot3. It really simple :)"], "url": "https://answers.ros.org/question/263511/what-exactly-is-the-opencr-part/"},
{"title": "cob_people_detection No connection to server 'load_model_server'", "time": "2017-04-27 09:14:29 -0600", "post_content": [" ", " ", "Hello,", "I am following the Quick Start for cob_people_detection in readme.md.\nWhen launching ", "roslaunch cob_people_detection people_detection.launch", "everything launches fine but I have error saying: \"[ERROR] : Recognition model not trained.\" Which should be normal because I haven't done any training so far. So, when running :", "rosrun cob_people_detection people_detection_client", "I have error: ", ". It appears that server 'load_model_server' is launched in face_recognizer_node.cpp , BUT ONLY if there is trained data! So there is a loop, I can not train data without that server, and there is no server without trained data??", "What am I missing? Should I use some other launch file or some other client to train the data first?", "Thanks in advance!"], "answer": [" ", " ", "Hi, we solved the problem. Just download the current version of cob_people_perception from github. Unfortunately, an error was introduced in one of the commits some months ago.", " This is the power of positive thinking :) Thank you", " ", " ", "Try the combination of the following packages:", "and", "and build them (with ", ") in a new catkin workspace (for sure).\nDo not forget to install all dependencies for both packages and than execute ", " in devel folder.", "I assume you're working with ROS Indigo and (x)ubuntu 14.04 (like me). I had the same problem.", "Good luck", " ", " ", "Thanks for your interest. One of my colleagues just fixed the bug. ", " Please check  "], "answer_code": ["catkin_make", "source setup.bash"], "url": "https://answers.ros.org/question/260450/cob_people_detection-no-connection-to-server-load_model_server/"},
{"title": "start ur5 remotely", "time": "2017-06-27 02:39:47 -0600", "post_content": [" ", " ", "Dear Ros Users and UR Users,\nis there a way to start UR5 remotely? My main purpose is to save energy on a mobile platform where UR5 is mounted on.", "Thank you!", "Can you be a bit more specific? Are you talking about shutting down / turning on the UR controller, or about the (various) ROS drivers?", "I was searching a method to power on the arm when controller is already on. Actually, I found it possible through the UR Dashboard", "Ok, good to hear.", "Could I ask you to post a slightly more extensive answer (like: which URScript/Dashboard commands did you use) and then accept your own answer? That will mark the question as answered much more than if you close it."], "answer": [" ", " ", " Universal Robot provides a Dashboard to manage these kind of commands (power on, power off, ...)\nMore details are explained here:  "], "url": "https://answers.ros.org/question/264837/start-ur5-remotely/"},
{"title": "how can I use sensor_msgs/BatteryState", "time": "2017-07-10 08:04:45 -0600", "post_content": [" ", " ", "I have to Display values of : voltage       current         charge   and       capacity of my turtlebot (kobuki) in real time.\nCan you help me please"], "answer": [" ", " ", "You can write a simple suscriber ( you can follow ", " ) that will suscribe to your battery state topic.", "In the callback function you can access and display the voltage (for example) using ", " .\nAll the parameters you can access to are available ", " .", "thank you for your reply..Can you send me the Corresponding code with python, please...And how can I modify the code to subscribe to the topic using kobuki_msgs..what is the name of the topic?", "You will find how to write a simple suscriber in python ", ".\nI don't know the name of your topic but I am pretty sure you can find it using the ", " command when your robot is awake.", "Does it solve your problem ? If yes please accept the answer so others can also benefit"], "answer_code": ["msg->voltage", "rostopic list"], "url": "https://answers.ros.org/question/265899/how-can-i-use-sensor_msgsbatterystate/"},
{"title": "Using roslaunch to launch ros nodes", "time": "2017-07-19 16:21:41 -0600", "post_content": [" ", " ", " ", " ", "I'm familiar with creating nodes from scratch by coding publishes and subscribers, but i'm not very familiar with roslaunch.  Let's say i've downloaded a ros package off github, and this package contains several nodes.  If i navigate the package and find the launch folder, i know that i can use ", "roslaunch parentfolder launchfile.launch", "to run an individual launch file (and this usually launches some corresponding node).  But if this package i've installed has 5 nodes, i have to run roslaunch 5 times, in 5 different terminals to simultaneously run all of these nodes.  So i have two questions:", "Can i use roslaunch to launch more than one .launch file at a time?", "Will roslaunch only take .launch files, or can it be used to launch other types of files?  (and as a follow up, i'm not really too familiar with ros .launch files, i know basic xml but i'd like to become more familiar so any links or resources to that effect would help a lot)", "Thanks!", "You can also use launch files to ", " and to ", " as well."], "answer": [" ", " ", "Yes, you can use ", " to launch multiple nodes at once and you can even include other launch files (from other packages!) within a separate launch file. Let's use the ", "  as an example.", "Here's the code for the launch file that brings up a ", " simulation of the turtlebot (", ") :", "This launch file includes both other launch files and several nodes. Here is how it includes other launch files:", "and here is how it includes other nodes:", "You could simply include a launch file without ", " or ", " like this:", "or similarly for a node:", "Please refer to the ", " for more details, or ask another question.", "Thanks so much for this!  All of those arguments that you have entered, are those some sort of argv commands passed into a .cpp or python file somehow?", "They're the way that you pass data from one launch file to another which can then set parameters or do some boolean checks. I suggest that you read about ", ".", "Also read about the ", " and ", ". Knowing about these and how to use them will give you access to some of the power of using launch files."], "answer_code": ["roslaunch", "turtlebot_simulator", "gazebo", "turtlebot_world.launch", "<launch>\n  <arg name=\"world_file\"  default=\"$(env TURTLEBOT_GAZEBO_WORLD_FILE)\"/>\n\n  <arg name=\"base\"      value=\"$(optenv TURTLEBOT_BASE kobuki)\"/> <!-- create, roomba -->\n  <arg name=\"battery\"   value=\"$(optenv TURTLEBOT_BATTERY /proc/acpi/battery/BAT0)\"/>  <!-- /proc/acpi/battery/BAT0 --> \n  <arg name=\"gui\" default=\"true\"/>\n  <arg name=\"stacks\"    value=\"$(optenv TURTLEBOT_STACKS hexagons)\"/>  <!-- circles, hexagons --> \n  <arg name=\"3d_sensor\" value=\"$(optenv TURTLEBOT_3D_SENSOR kinect)\"/>  <!-- kinect, asus_xtion_pro --> \n\n  <include file=\"$(find gazebo_ros)/launch/empty_world.launch\">\n    <arg name=\"use_sim_time\" value=\"true\"/>\n    <arg name=\"debug\" value=\"false\"/>\n    <arg name=\"gui\" value=\"$(arg gui)\" />\n    <arg name=\"world_name\" value=\"$(arg world_file)\"/>\n  </include>\n\n  <include file=\"$(find turtlebot_gazebo)/launch/includes/$(arg base).launch.xml\">\n    <arg name=\"base\" value=\"$(arg base)\"/>\n    <arg name=\"stacks\" value=\"$(arg stacks)\"/>\n    <arg name=\"3d_sensor\" value=\"$(arg 3d_sensor)\"/>\n  </include>\n\n  <node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" name=\"robot_state_publisher\">\n    <param name=\"publish_frequency\" type=\"double\" value=\"30.0\" />\n  </node>\n\n  <!-- Fake laser -->\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"laserscan_nodelet_manager\" args=\"manager\"/>\n  <node pkg=\"nodelet\" type=\"nodelet\" name=\"depthimage_to_laserscan\"\n        args=\"load depthimage_to_laserscan/DepthImageToLaserScanNodelet laserscan_nodelet_manager\">\n    <param name=\"scan_height\" value=\"10\"/>\n    <param name=\"output_frame_id\" value=\"/camera_depth_frame\"/>\n    <param name=\"range_min\" value=\"0.45\"/>\n    <remap from=\"image\" to=\"/camera/depth/image_raw\"/>\n    <remap from=\"scan\" to=\"/scan\"/>\n  </node>\n</launch>\n", "<include file=\"$(find gazebo_ros)/launch/empty_world.launch\">\n    <arg name=\"use_sim_time\" value=\"true\"/>\n    <arg name=\"debug\" value=\"false\"/>\n    <arg name=\"gui\" value=\"$(arg gui)\" />\n    <arg name=\"world_name\" value=\"$(arg world_file)\"/>\n</include>\n", "<node pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" name=\"robot_state_publisher\">\n    <param name=\"publish_frequency\" type=\"double\" value=\"30.0\" />\n</node>\n", "args", "params", "<include file=\"$(find package_name)/launchfile.launch\"/>\n", "<node pkg=\"package\" type=\"node_type\" name=\"node_name\"/>\n", "roslaunch", "args", "parameter server", "param"], "url": "https://answers.ros.org/question/266857/using-roslaunch-to-launch-ros-nodes/"},
{"title": "Problem installing turtlebot", "time": "2017-07-17 19:52:32 -0600", "post_content": [" ", " ", " Hello everyone, I'm new here and I'm trying to install turtlebot on a Raspberry Pi 2's Ubuntu OS.\nI've already installed ROS without errors and now I'm trying to install the Turtlebot following these instructions:  ", " \nI went directly to 1.2.2 Ubuntu Package Install and ran that code but I get the following error every single time:  ", " Searching on Google I found the next question:  ", " \nThen the op says he/she found a workaround doing the following:  ", "Then: ", "Although it worked for him/her, it didn't work for me so I kept searching and found the next question:", "The answer chosen as the correct one gives some directions even after following them I'm still unable to correct the error I showed you. One of the comments on that questions says: ", "Have you modified your /etc/apt/sources.list or any of the files in /etc/apt/sources.list.d/ since you installed ROS?", "And I did modify because I had some problems when I was installing ROS and now that file looks like this:", "Maybe this is the problem, does anybody know how to correct this?\nI really want to start working with the turtlebot.", "Thanks in advance!"], "answer": [" ", " ", "The turtlebot simulator depends on gazebo, and gazebo isn't available for Ubuntu running on the Raspberry Pi 2.", " Assuming that you're followed the installation instructions on  ", "  , if you follow the link to the package status page and search for  ", " (  ", "  ), you can see that the  ", " column for turtlebot_simulator is red, indicating that the binary package isn't available for Trusty.", "This usually happens because there are missing dependencies or compilation errors when trying to build packages on ARM, and they don't usually get fixed because there is low demand for these packages, and many of the complex simulation packages like Gazebo run too slowly on ARM to be useful anyway.", "I suggest that you install the turtlebot_simulator package on an x86 desktop, laptop, or virtual machine instead.", "Thank you very much for your answer!\nI would really like to install it on a desktop computer or a laptop but we need to install it in the RPi2 because we are making a small car-robot base on this embedded system. Is there a workaround? Maybe to install it on a RPi3 using Ubuntu?", "We tried installing ROS using Raspbian but we got too many errors and it was impossible to find a workaround.\nAny suggestion will be really appreciated.", "To be able to run the base turtlebot packages you don't need to install the simulator on your RPi. In fact, I would rather not do that, as the Pi is really not powerful enough to comfortably run something like Gazebo.", "Thank you very much, I've installed Ubuntu on my laptop which will dual-boot along with my Windows OS and then installed ROS without problems. Thanks you for your help and I apologize for taking too long to answer back."], "question_code": ["Unable to locate package ros-indigo-turtlebot-simulator.\n", "sudo apt-get install libsdformat1\n", "sudo apt-get install ros-indigo-turtlebot ros-indigo-turtlebot-apps ros-indigo-turtlebot-interactions ros-indigo-turtlebot-simulator ros-indigo-kobuki-ftdi ros-indigo-rocon-remocon ros-indigo-rocon-qt-library ros-indigo-ar-track-alvar-msgs\n", "deb http://ports.ubuntu.com/ubuntu-ports/ trusty main universe multiverse restr$\ndeb-src http://ports.ubuntu.com/ubuntu-ports/ trusty main universe multiverse r$\ndeb http://ports.ubuntu.com/ubuntu-ports/ trusty-updates main universe multiver$\ndeb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-updates main universe mult$\ndeb http://ports.ubuntu.com/ubuntu-ports/ trusty-security main universe multive$\ndeb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-security main universe mul$\n"], "answer_code": ["turtlebot_simulator", "Thf"], "url": "https://answers.ros.org/question/266614/problem-installing-turtlebot/"},
{"title": "About Battery Management Systems", "time": "2017-07-26 05:24:02 -0600", "post_content": [" ", " ", " ", " ", "Hi all,", "for my robots I'm trying to find the best solution to manage power and batteries.\nFor my robots I use 4S and 6S LiPo.", "What I'm searching for is a good Smart BMS solution to charge/control batteries without removing them from the robot.\nIf the Smart BMS is not supported by ROS I'm ready to write the driver.", "Do you have any suggestion for me?", "Thank you", "Walter", "Why do you need a BMS on a 4S, 6S LiPo battery?  These battery packs can operate without the need of a Battery Management System. ", "If you are talking about cell balancing, any good charger, such as the SKYRC iMAX B6AC has a balancer connection which monitors the voltage of each cell while charging", "I use the iMAX B6AC to charge/balance the battery, but I would like to charge/balance it without removing it from the robot. Furthermore LiPo must be monitored to be sure to not let one of the cells to go below 3V of charge.", "The idea is to connect a \"recharge cable\" to the robot when ros_diagnostic says that the battery has reached a warning level and to continue to use it without turning it off tp replace the battery with a charged one."], "answer": [" ", " ", "If all you're looking for is a way to charge your batteries without having to take them off your vehicle, all you need to do is to branch off the power and ground wires from your pack, along with the balancer cable to a connector. Then you can connect that your charger. You can monitor battery voltage using a analog to digital converter on an arduino which can be running rosserial-arduino that posts the pack voltage. With this information you can have your robot handle an undervoltage condition. ", "To keep your batteries operating normally, all you have to do is to make sure you do not discharge them past the cutoff and also use a balancer each time you charge them.", " However you can buy lipo packs with a protection circuit monitor that handles balancing on their own and also watch out for other stuff such as shorts, undercharging and undercurrent. I HIGHLY recommend you buy a pack that has such a protection circuit already built in, such as this one:  ", " You can buy a PCM on its own here:  ", "  but this will take lots of effort and troubleshooting to debug.  ", "Battery management systems that interface with your robotic system, where you are monitoring every pack voltage and temperature along with real time discharge rates and required for chemistries other than LiPo, such as LiFePo4. With LiPo, you are fine with a PCM and just using a balancer on the charging cycle. ", "Your solution is something that I just took in consideration, but I'm searching for something more robust and professional. What I want to do is the same as a laptop: to connect the recharge cable and to continue to use the robot. \nI'd also like to try LiFePo4. What about them?", "A LiPo pack with a PCM is as professional and robust as you need. I have worked on many commercial robotics systems with LiPo and most didn't even have a pcm, and just required balancing every x amount of charge cycles. A LiFePo4 is diff chemistry that is more suceptible to thermal and cell voltage~", " differences. Here is a vid of a LiFePo4 pack I built complete with a BMS. It was a really big effort!  "], "url": "https://answers.ros.org/question/267407/about-battery-management-systems/"},
{"title": "TF inverse of a pose", "time": "2017-07-22 10:42:02 -0600", "post_content": [" ", " ", " ", " ", "I am trying to understand the ", " package. \nIt simply tries to follow the waypoints sent from the global planner (through move_base). In a function called diff2D, there is this line:", "where pose2 is the robot current pose and pose1 is the waypoint pose. It seems like this line returns the difference between the two poses. My questions are: ", "1) how this line (internally) works? or what is the mathematical concept behind this?", "2) does this line also calculate the difference between the two quaternions of the poses?", "3) Another question is what is the difference between tf::Pose and tf::Transform"], "answer": [" ", " ", " ", " ", "1) how this line (internally) works? or what is the mathematical concept behind this?", "You can see the source code for ", " ", ". Unfortunately, it's not the most readable code. As for the math, I've illustrated the concept here:", "Say you have two poses, ", " (red) and ", " (blue). ", " and ", " are drawn as vectors from the origin. You want to find the pose ", " (green), which is the difference from ", " to ", ". To do this, you have to \"subtract\" ", " from ", ". The mathematical concept here is called homogeneous transformation, part of linear algebra. Each pose can be represented as a 4x4 transformation matrix that contains both translation and rotation (these can also be represented as a vector + a quaternion, but the matrix lets you put both in one object). To compose (or \"add\") the poses, you multiply the matrices, which is what the ", " operator does when you \"multiply\" one pose by another. In this case, because we want to \"subtract\" ", " from ", ", we use the inverse, which is a matrix inverse. Also, note that the order of the multiplication matters with matrices--matrix multiplication is not commutative like scalar multiplication is.", "2) does this line also calculate the difference between the two quaternions of the poses?", "Yes. I wasn't able to draw the orientations into the sketch above, but composition of poses includes rotation as well.", "3) Another question is what is the difference between ", " and ", "Just semantics. On ", ", you'll see they're actually the same type! ", " is just a typedef of ", ". A ", " represents a position+orientation of an object, and it's meant to be a direct analog to the ", " message, whereas a ", " is an object that can perform a translation+rotation transform. Mathematically (and in code) they're identical."], "question_code": ["tf::Pose diff = pose2.inverse() * pose1;\n"], "answer_code": ["tf::Pose", "pose1", "pose2", "pose1", "pose2", "diff", "pose2", "pose1", "pose2", "pose1", "*", "pose2", "pose1", "tf::Pose", "tf::Transform", "tf::Pose", "tf::Transform", "tf::Pose", "geometry_msgs::Pose", "tf::Transform"], "url": "https://answers.ros.org/question/267134/tf-inverse-of-a-pose/"},
{"title": "How to write a shell script to start everything?", "time": "2017-08-08 13:56:09 -0600", "post_content": [" ", " ", "I am very new to shell script. Is this possible to write a script basically starts for example gazebo, rviz and slam? I tried to execute command by python by you can only execute one at a time, executing next requires exiting the current one. ", "Thanks!"], "answer": [" ", " ", " ", " ", "You can launch several nodes using a unique ", ".", "For example, ", " a launch file that can launch rtabmap_ros, hector_mapping, rviz and other things.", "Notice the parts that start with ", " and end with ", " : those mark the limits between the start of the different nodes.", "Check ", " for an answer to a similar question :-).", "Also, check out ", ". This is a far better way to go than using shell scripts. They have their place, but ", " files are far more powerful in this context."], "answer_code": ["<node (...) >", "</node>", "launch"], "url": "https://answers.ros.org/question/268337/how-to-write-a-shell-script-to-start-everything/"},
{"title": "setup.bash every time?", "time": "2017-08-28 14:19:01 -0600", "post_content": [" ", " ", " ", " ", "the last few times I have done anything w ROS while doing beginning tutorials, I just say 'roscore' and it seems to all run fine, but now im watching a video and it seems like he's saying I need to do the setup.bash steps every time I want to use ros. \n1. is it necessary to do the setup.bash steps every time?\n2.if so, why have I been able to use turtlesim and other things just running roscore?", "edit: I went into .bashrc and added source ~/catkin_ws/devel/setup.bash thinking that was what I was missing but then realized the one I was thinking of was already there, called 'source /opt/ros/kinetic/setup.bash'", "I can provide my experience. I also used to think that it wasn't needed and it worked for a awhile. Then I made some change and it stopped working. But I lost two days before answers.ros helped me find out what I was doing wrong.  I still break rule by putting it in ~/.bashrc instead of typing it.", "Hmm, interesting. I added it in my ~/.bashrc now but still interested in the logic. like is it because the setup.bash is connected w catkin and and roscore is a seperate entity? that's my guess rn", "What ", " are you referring to? The one that's in your ", " folder, the one in your catkin workspace ", " folder, or both?", ", I just edited my question to include that.", "I deleted my answer so that I don't add to the noise and confusion with overlaying and setting of the env vars with setup.bash", " This may be helpful:  "], "answer": [" ", " ", "You need to source the setup.bash for the workspace you're using every time you open a new shell. It sets up the environment variables that ROS needs to find packages, nodes, and launch files.", "This can be manual or automatic, and what you choose to do depends on your workflow and your personal preferences.", "If you only have a single workspace ( ie catkin_ws folder ), adding ", " to your bashrc will automatically source the setup for that workspace every time you open a new terminal.", "If the environment variables that ROS uses conflict with some other software that you use, if you delete your workspace, or if you switch between multiple workspaces frequently, you probably don't want to source the setup.bash automatically. Some users in this situation just source the setup.bash manually, but I've seen a lot of users create some kind of shell function or alias to make sourcing their setup.bash quicker.", "For example, in my bashrc I have defined several functions:", "Now when I open a new terminal, there's no ROS environment loaded, but I can quickly load one by typing ", ", ", " or ", " instead of typing the full path to the setup file.", "shell functions are quite powerful and you can have them run basically any shell command you want. There's a lot more that you can do with shell functions in general, but that's a bit beyond the scope of this forum.", "Thank you, great answer ", ". Just still unsure about the difference between the workspace setup and the ROS setup.", " is a workspace too! It just happens that it only has the packages installed through apt-get."], "question_details": [" ", " ", " ", " ", "is that the one I need to run every time or should I also add the catkin setup to run every time as well?", "also, do I still now need to roscd everytime to enter the workspace?"], "question_code": ["setup.bash", "/opt/ros/<distro>", "devel"], "answer_code": ["source ~/catkin_ws/devel/setup.bash", "# personal robot project\nfunction dagny {\n    source ~/dagny/devel/setup.bash\n    cd ~/dagny\n}\n\n# job-related ROS workspace\nfunction junior {\n    source ~/junior/devel/setup.bash\n}\n\n# dedicated workspace for diagnostics development\nfunction diagnostics {\n    source ~/diagnostics/devel/setup.bash\n}\n", "dagny", "junior", "diagnostics", "/opt/ros/<distro>"], "url": "https://answers.ros.org/question/269784/setupbash-every-time/"},
{"title": "Intel Euclid battery state", "time": "2017-08-27 15:12:53 -0600", "post_content": [" ", " ", "Hello community,", "I'm currently evaluating the Intel Euclid platform and I'm trying to find a solution to get battery state.", "Any ideas or suggestions", "Thanks in advance", "Cheers", "Chrimo"], "answer": [" ", " ", "$ rostopic echo /hardware_status\ncs_name: EUCLID_7187\nbattery: 99\nbatteryStatus: Full", " It looks like this might be published by  "], "url": "https://answers.ros.org/question/269683/intel-euclid-battery-state/"},
{"title": "turtlebot charging without battery", "time": "2017-09-12 07:04:39 -0600", "post_content": [" ", " ", "Unfortunately my turtlebot was delivered without the additional battery: 4S2P (4400 mAh) that has to be connected in the bottom of the kobuki base.\nIs there any way to charge the turtlebot 2 with Kobuki base without this battery.\nI tried to charge the turtlebot with the adapter which plugs in next to the on/off switch, but this doesn't work.\nI also tried to charge it via the docking station, but also without success."], "answer": [" ", " ", "There is no battery in the turtlebot base other than the one you plug into it at the bottom.", "If you don't have that, you have no battery, and nothing will charge."], "url": "https://answers.ros.org/question/270751/turtlebot-charging-without-battery/"},
{"title": "Need a camera with constant focus", "time": "2013-07-16 23:29:48 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I use a usb camera to localize NAO robot by tracking a marker placed on its head (using ar_pose marker tracker). My problem is with the camera autofocus which results in the poor performance when there are other moving objects in the environment.", "Is there a camera with constant focus you could suggest me to buy?\n(I need a camera with a wide field of view and a long data  cable as the camera is placed in the ceiling in my scenario)\nor whether it is possible to set the autofocus of usb camera off?", "Thanks,\nPouyan", "I think all cheap webcam's are constant focus, only the expensive ones have autofocus. Finding a wide angle is harder. ", "you need to specify a price range - there's a whole host of cameras that do not auto focus"], "answer": [" ", " ", "Your problem is physics related.  What you are asking for is a camera and lens that have a depth a focus from some minimum out to infinity.   such that the target will never be closer than the minimum focus.    Of course this can be done but there is a trade off.", "A camera that has a wide depth of focus will have three features you may not want:  A wide angle lens, physically small sensor size and numerically high f-stop number.   I camera like this typically has poor low light performance and has a bit of \"noise\" or grain in the image.   Most of the things engineers can do to improve the image alto reduce the depth of focus.  They make up for this by using a focus motor on the lens.  So it is a trade off, one thing for another.", "One way you can solve the grain and noise problem is by seriously increasing the amount of light.  Double the lighting power for every loss of 1 f-stop.   A fixed focus lens operating at (say) f/5.6 needs 4 times as much light as the same lens at f/2.8 and 8 times as much as one at f/2.0    Image quality improves quite a lot as you add light, does wonders for the signal to noise ratio (because you are adding signal)", "You also have a limit if you are using a USB 2 interface to the camera.  You can't get uncompressed 1080P at 60 fps trough a USB cable.  USB lacks the required bandwidth.  ", "All that said.  A good solution is the USB web cam that was made for the Sony Playstation II   It was VERY inexpensive, under $10.  They are used by many people for computer vision, especially video because that can to 480P at 60 fps.   They are fixed focus and there is a two step manual zoom but even the wide setting is not very wide.  This will be a problem if used for survilence.  You would need to attach an auxiliary lens    There is also a good quality microphone array on the camera.", " ", " ", "I'm not exactly sure which camera the Nao uses, but most webcams with a autofocus feature are UVC-compliant, so the autofocus is often configurable via software. See for instance ", " or ", ". I'm writing \"often\" because I've seen various cases where the UVC standard is not followed correctly, in which case the controls might not work, even if they are available.", " ", " ", "FLIR (formerly pointgrey) cameras are really good in my experience and come in many configurations. There's a ROS driver ", " that's plug-and-play which will get you up and running in a couple hours. ", "Obviously they're not in the same price range as the best-selling webcams on Amazon. ", " ", " ", "If you need to do this on a budget then buy a cheap webcam with no auto-focus and glue one of the wide angle lenses you can get for mobile phones over the top of it. This will give you a good field of view but the quality will not be great.", "The best camera I've used it a GoPro 4 but it's not easy to feed it into a computer live. I believe some of the knock off GoPro like cameras you can buy can be used as USB web-cams directly so that's probably worth looking into.", " ", " ", "The Axis F-Series IP security cameras are very nice:"], "url": "https://answers.ros.org/question/67521/need-a-camera-with-constant-focus/"},
{"title": "ROSLaunch and SRC", "time": "2017-09-13 16:31:28 -0600", "post_content": [" ", " ", " ", " ", "Hi there, I'm new to ROS.\nI'm trying to understand how ROS works. I already did the tutorials but I really can't understand how does launch files works. I mean, sometimes I don't know how the launch files are connected with the logical part included in the src/ folder.", "Thank you all!", "Welcome! I think that in order to get a better response, you should ask at least two separate questions. One about the launch files/source code connection and the other about RPLIDAR and SLAM as these are two different subjects. You can edit this question and then ask a new one as well."], "answer": [" ", " ", " ", " ", "First, a ", " brief explanation of launch files and ", ". According to the ", ":", " is a tool for easily launching multiple ROS nodes locally and remotely via SSH...", "Launch files provide a way, among other things, to launch a node (or nodes) with a single command:", "Let's say that your package is called ", " with a node called ", " saved in the ", " folder. You can run (i.e., ", ") your node by creating a launch file called ", " like so:", "and run it using", "The code inside of the launch file is just XML. The ", " tag tells ROS that this is a launch file (they don't have to have a ", " file extension) and the ", " tag tells ", " to run a node. Here are what the ", " tags attributes mean:", "You can also configure nodes via parameters and include launch files within launch files (within launch files...) to create a very complex system. In fact, some ROS packages can be composed of nothing but launch files from other packages! ", "Launch files are one of the many powerful features of ROS because they make it very easy to configure your system and they promote extensibility and resuse. There are many more features of launch files and I recommend that you read through the wiki to get a better understanding of them.", " does a good job of explaining the ", " XML format and the book ", " has a ", " as well.", " ", " is used to compile the code into executables (or, for Python you ", " it) and then you tell ", " what executable to execute with the ", " tag and its attributes.", "For the \"logical\" part: ", " will run exectuables from a package. It knows what executable you want to run through the ", " tag's ", " attribute. You put the executable's name as the value for ", " and ", " knows that you want to run an executable with the same name (from the package that you specified with the ", " attribute). This can be from C++, Python, or whatever supported language was used to write the node.", "The code in the ", " folder (or wherever it is located) is not going to be executed by ", ", only executables can be run.", "Well, thanks for the whole explanation. But the thing I don't understand it's what I said \"logical\" part. \nMost of the packages included in ROS have .cpp/.h/.py files, there you have a lot of code and I thought the launch files are executing those .cpp/.h/.py files.", "Now I get it. I didn't know about the \"type\". i really appreciate your help!!", "No problem, glad to help."], "answer_details": [": the package your node is in", " attribute says what the executable name is (for C++, for Python this is the filename so you would put the file extension)", ": this is what you want ROS Master to call your node. This is useful for giving nodes meaningful names and using multiple instances of the same node in the same namespace.", " ", " ", " ", " "], "answer_code": ["roslaunch", "roslaunch", "roslaunch", "roslaunch <package-name> <launch-file.launch>\n", "my_package", "my_node", "src", "my_node.launch", "<launch>\n  <node pkg=\"my_package\" type=\"my_node\" name=\"my_node\"/>\n</launch>\n", "roslaunch my_package my_node.launch\n", "launch", ".launch", "node", "roslaunch", "node", "pkg", "type", "name", "roslaunch", "roslaunch", "catkin", "chmod +x", "roslaunch", "node", "roslaunch", "node", "type", "type", "roslaunch", "pkg", "src", "roslaunch"], "url": "https://answers.ros.org/question/270857/roslaunch-and-src/"},
{"title": "Unable to locate package ros-lunar-desktop-full", "time": "2017-09-24 11:10:41 -0600", "post_content": [" ", " ", "Using the instructions at ", " titled \"Debian install of ROS Lunar\", I am able to successfully complete the steps up through 1.3.  On step 1.3 I had to install dirmngr, but was able to complete the step.", "In step 1.4, I successfully executed ", ".", "In step 1.4, I tried to execute ", ", but it failed with the error \"Unable to locate package ros-lunar-desktop-full\".", "I am running Debian 9.1 Stretch on a 64 bit AMD based PC.", "The command ", " does not return anything.", "I am pretty new to Linux, could somebody please help me understand what I have done wrong?", "Thank you,", "Randall", "According to ", ", the ", " pkg should be available on Debian Stretch, but only for AMD64. Can you install any other ROS pkgs? Try ", "."], "answer": [" ", " ", "I did not find a solution for this problem, but I resolved it another way.", "I originally installed Ubuntu on some old hardware to give ROS a try.  I experienced lots of trouble installing Ubuntu and keeping it running, so I replaced the install with Debian.  Debian installed cleanly and worked without issue.  I then tried to install ROS and ran into this problem.", "Since my hardware was old and low powered, I decided to build a new computer.  I tried installing Ubuntu on the new computer, and it went smooth as silk.  I then installed ROS with no problems.", "I'm now going through the tutorials.", "Thanks to all who read my problem and offered suggestions!", "I wouldn't say this is a ", ", but as long as you're happy we're ok."], "question_code": ["sudo apt-get update", "sudo apt-get install ros-lunar-desktop-full", "apt-cache search ros-lunar", "desktop_full", "ros-lunar-ros-base"], "url": "https://answers.ros.org/question/271419/unable-to-locate-package-ros-lunar-desktop-full/"},
{"title": "IMU Calibration on Ground Robot - how do people do it?", "time": "2017-09-28 20:15:17 -0600", "post_content": [" ", " ", "Hello,", "I am working on a ground based robot, and now we are working to install an IMU.", " At the moment we are using this IMU:  ", " . We are trying to figure out calibration with it however - how do people typically install and allow for calibration of an IMU on a ground based robot? Do they typically install it on a lead such that it can be moved in patterns to calibrate it fully? Or are there calibration free IMUs that can be installed and simply work when turned on seamlessly? ", "If there are calibration free IMUs, what ones do people recommend?", "Thanks!", "Your link does not appear to work.  404 not found error.", "What do you mean by ground based?  Do you mean it cannot be moved?  Can it be tilted at all?  Rotated?", " This is the link to the IMU:  ", "By ground based I mean a 150-500+ pound robot that cannot be tilted/rotated the way you might rotate/move around a lightweight drone to calibrate the IMU."], "answer": [" ", " ", "If you install the IMU on a lead and then pull it away from the mounting location to perform the calibration, then you're defeating much of the purpose of the calibration.  The calibration should be done with the IMU mounted in its operational location and with the robot powered on.  Maybe even with the robot moving.  That way, any magnetic fields and metallic parts that can affect the IMU can attempt to be calibrated out.  ", "Kurt, got it.  How then do you calibrate an IMU on a heavy robot that you can't move around in different patterns (i.e. figure eight pattern across the Z axis)?  Is there any standardized approach to doing this?", "This is really out of my domain.  But I suspect that if you move/drive the robot around in it's normal axis of motion during your IMU calibration, that the IMU will work well in that axis of operation.  And I suspect that it won't give you much data in the unused axes.  Not sure though."], "url": "https://answers.ros.org/question/271818/imu-calibration-on-ground-robot-how-do-people-do-it/"},
{"title": "neato laser stop", "time": "2017-09-28 12:00:02 -0600", "post_content": [" ", " ", "I have the neato laser running on my turtlebot2 using the Kobuki base. As soon as turtlebot boots up and the laser receives power from the on board computer via the usb port, it starts spinning. I would like to control the laser startup where I can turn it on only when needed like during gmapping or slam or when running minimal launch. I would like to turn off the laser when I'm not using slam or mapping or when I'm just charging the base. \nWhat is/are the ROS recommended ways of doing this ?"], "answer": [" ", " ", "I don't think the neato hardware supports this. In my experience, the neato lidar starts spinning as soon as you apply power.", "If you have a third-party controller board for your Neato, you may be able to use it to turn power on and off for your Neato.", "I have the controller board from Get surreal. I'm looking to controller is via software. I may look into turning the power off on the usb port (ACM0 in my case). The board has a button that turns off the power but it doesnt turn it back on like a toggle switch.", "According the ", " the controller has motor on and off commands, but I don't see anything in the ", " that would send those commands.", "You will probably need to modify the neato driver to send the on and off commands from a ROS service, or when the node starts and stops.", "Thank you ahendrix!", "I havent written or modified any drivers before. I may look into it."], "url": "https://answers.ros.org/question/271780/neato-laser-stop/"},
{"title": "Turtlebot 3 Questions", "time": "2017-10-22 20:03:05 -0600", "post_content": [" ", " ", " ", " ", "Can order a Turtlebot3 Waffle without an SBC or with an alternate SBC, and whether if I substitute the SBC am I now in an unsupported configuration?"], "answer": [" ", " ", "Turtlebot3 Burger and Waffle are sold as package so customizing your order will be difficult. ", "\nTurtlebot3 is running on the ROS based on Linux, I believe you can use the SBC as long as it supports Linux(Ubuntu 16.04), USB port, Wi-Fi connection. ", "\nPlease check the operation voltage/power consumption of the SBC as OpenCR might not support power recommendation for a custom SBC. ", "\nPlease understand that customizing the original package may void the product warranty."], "url": "https://answers.ros.org/question/273784/turtlebot-3-questions/"},
{"title": "Generic path to read a file in a ROS directory", "time": "2017-10-26 14:58:36 -0600", "post_content": [" ", " ", " ", " ", "I have created a ", " and ", " directories in my ", " directory. The ", " directory contains a csv file and ", " directory contains a Python script to parse the csv file. The first few lines of the script is as follows -", "How can I add a generic location path for the csv file in my Python script? And what modifications do I need to make in my ", " in order to install the ", " and ", " directories?"], "answer": [" ", " ", " ", " ", "To get path of your package you can use ", " package. Take a look at examples ", ".", "This is basically it:", "In order to install your scripts you need to use cmake's ", ".", "For example like this:", "Then it depends on if you're using catkin_make or catkin build command. To install the script you run ", ". Or if you're using catkin build you need to config your package with catkin ", " and then run ", ".", "This should install your script to install directory of your root's workspace.", "How should I modify ", " in my code?", "I'm not familiar with python. But I would probably save output of ", " to some variable. Then somehow add the last part of your path (", ") and insert it to ", ". But this is already python related.", "You should just be able to concatenate strings by adding them:", "If I may suggest: ", " use ", " instead of concatenating path segments with hard-coded path delimiters (ie: ", "). You never know where your code ends up.", "It works! Thank you ", "If ", "'s suggestion of using ", " for this worked, then I believe it would be good if you could mark his answer as ", " answer by ticking the checkmark to the left of it. It should turn green."], "question_code": ["/csv", "/scripts", "/src", "/csv", "/scripts", "import csv\nwith open('/home//default_ws/src/<package_name>/csv/alarms.csv', 'rb') as csvfile:\n    reader = csv.reader(csvfile)\n    for row in reader:\n", "CMakelists.txt", "/csv", "/scripts", "import csv\nimport os, rospkg\nrospack = rospkg.RosPack()\nwith open(os.path.join(rospack.get_path(\"package_name\"), \"csv\", \"alarms.csv\"), 'rb') as csvfile:\n    reader = csv.reader(csvfile)\n    for row in reader:\n"], "answer_code": ["rospack", "import rospkg \nrospack = rospkg.RosPack()\n# get the file path for rospy_tutorials\nrospack.get_path('rospy_tutorials')\n", "install(PROGRAMS\n  \"scripts/my_script\"\n  DESTINATION bin\n)\n", "catkin_make install", "config config --install", "catkin build", "with open('/home//default_ws/src/<package_name>/csv/alarms.csv', 'rb') as csvfile:", "rospack.get_path('your_package')", "csv/alarms.csv", "open()", "with open(rospack.get_path('<package_name>')+'/csv/alarms.csv', 'rb') as csvfile:\n", "os.path.join(..)", "/", "rospack"], "url": "https://answers.ros.org/question/274242/generic-path-to-read-a-file-in-a-ros-directory/"},
{"title": "ROS compatibility with PowerBot", "time": "2014-12-10 08:41:39 -0600", "post_content": [" ", " ", "Hi,", " I'm a beginner to ROS and robot control in general. Are the libraries provided by ROS compatible with the PowerBot from  ", "  ( ", ")?", "Thanks.\nR."], "answer": [" ", " ", " Yes there are drivers for the powerbot:  "], "url": "https://answers.ros.org/question/199239/ros-compatibility-with-powerbot/"},
{"title": "roslaunch if condition", "time": "2012-01-25 13:27:21 -0600", "post_content": [" ", " ", " ", " ", "Hi\nhow can say: run this node if argX==y", "The following does not work, and the documentation on this is lacking.", "<node pkg=\"xxx\" type=\"yyy\" name=\"yyy\" if=\"$(arg argX)==y\"/>"], "answer": [" ", " ", " ", " ", "You can do this now in ROS Kinetic using ", ":", " The documentation has been updated:  ", "<arg name=\"debug\" default=\"0\"/>", "<group if=\"$(eval arg('image_width') ==640)\">\n    </group>", "worked for me. The above did not work for me.", " ", " ", "The documentation on the roslaunch page for this syntax is complete, i.e. you can't do comparisons or any operators for that matter.  You can only test the value itself.", "link to the documentation --- ", " ", " ", " ", " ", " Is this answer still current? I see that xacro got an upgrade: from  ", " \n\"The more powerful evaluation capabilities in ROS Jade allow for much more complex expression. Virtually any python expression that evaluates to a Boolean is feasible.\" ", "EDIT 2015.6.30:", "I found a ", " suggesting that Ken's answer is probably still current, though I don't yet have jade to check for sure.", " ", " ", "This is supported as of ROS Kinetic.", " See also  ", "This does not answer the question which is about comparing ", " to some arbitrary (non-bool) value."], "answer_code": ["eval", "<arg name=\"arg_name\" default=\"desired_value\"/>\n\n<node pkg=\"node_pkg\" type=\"node_type\" name=\"node_name\" \n      if=\"$(eval arg_name == 'desired_value')\"/>\n", "  <group if=\"$(eval arg('debug') ==0)\">\n    <node pkg=\"xxx\" type=\"yyy\" name=\"yyy\">\n  </group>\n", "<group if=\"$(arg foo)\">\n  <!-- stuff that will only be evaluated if foo is true -->\n</group>\n", "foo"], "url": "https://answers.ros.org/question/12756/roslaunch-if-condition/"},
{"title": "plotting real time data", "time": "2017-06-26 09:52:49 -0600", "post_content": [" ", " ", " ", " ", "I want process the data from some ros nodes and make a real time plot, preferably using matplotlib. What is the easiest way to do this? ", "For example if I would like plot the average of two values in the ros network, or add a legend to the plot. I know about rqt_plot, but its usefulness is very limited since it can only plot already defined messages.. and you can't even change the axis limits afaik!", "Any tips? The best thing would be if there was a way to continually update a matplotlib figure and have it displayed as the roscore is running. ", " Take a look at  ", "  - I haven't tried it yet but I'd like to hear if it addresses some of your needs. ", "There is an infinity of answers on stackoverflow treating the continuous update of a matplotlib figure."], "answer": [" ", " ", " ", " ", "I ended up doing the following. It works in real time as long as pyplot can keep up with the messages. ", " ", " ", "I did liveplotting in the past in two ways:", "Your prefered way is the matplotlib so thats pretty easy and I explain a little bit more: ", "There you need a ", ". It makes an animation by repeatedly calling a function/method. You can read more ", ". ", "Furter in the repeatedly called function you should do something like:", "so there you append new data and than set it. What you now need to do different is that you setup a ROS Subscriber to your data you want to publish and in the callback function/method you need to append the new data and just set in the \nrepeatedly called function the new data. Thats only a close look about how you can do it to give you an idea.", "The other way is using c++ and writing for example a rviz plugin with QT. I prefere here the ", " for live plotting. But thats a little bit tricky and needs more time when you have no idea about QT. But for me it generates better looking plots and when you are used to it its very easy too. ", "When you need further help please ask! It isnt too detailed. Just to give you an idea.", "this works! but I've always found FuncAnimation to be cumbersome to work with. If anyone has other ideas I'm still interested", " ", " ", "There are many tools to plot.", " tool is quite nice to use (compared to rqt_plot)", " this is my favourite tool its so fast plotting data and so easy to use! Try it yourself! ", "sudo apt-get install ros-kinetic-plotjuggler", "Start in launch file:", "If anyone knows how to start from launch file a saved layout.xml  Would be great for sharing :) ", "Hi, thanks for recommending PlotJuggler (I am the author). I super appreciate it.", "PlotJuggler uses old fashion command line arguments instead os ros params. Display them with -h. You can do:", "<node pkg=\"plotjuggler\" type=\"PlotJuggler\" name=\"my_plot_Juggler\" args=\"--layout your_filename\"/>", "Yeah I saw that --layout our just -l option but it does not work for me If I start gazebo paused.I get cannot find curve with ... error. And if I just run the sim. for a small amount of time and then try to load my layout I get the streamer named ROS Topic Streamer cannot be loaded.", " If you can describe in more detail the problem and/or the desired behavior here,  ", " \nI will be happy to improve it. ", " ", " ", "I would recommend rqt_plot. Don't forget you can plot the ", ". Almost anything can be represented by them. It sounds like what you want to plot can be a Float32 message. ", "Also, click on the green checkmark on the rqt_plot gui above the plot area to access the axis max/min settings. ", "It's a bit time consuming to have to readjust axis settings manually for different experiments. I am really looking for a way to customise plots programmatically. Also, afaik there is no way to change the x-axis in an rqt plot, like if I want to plot x/y coordinates or motor input vs speed."], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", "rosrun plotjuggler PlotJuggler ", "Go to streaming and start ros_topic_streaming", "simply select your topics you wanna stream", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["#!/usr/bin/env python \nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport rospy\nfrom qualisys.msg import Subject\n\ndef plot_x(msg):\n    global counter\n    if counter % 10 == 0:\n        stamp = msg.header.stamp\n        time = stamp.secs + stamp.nsecs * 1e-9\n        plt.plot(msg.position.y, msg.position.x, '*')\n        plt.axis(\"equal\")\n        plt.draw()\n        plt.pause(0.00000000001)\n\n    counter += 1\n\nif __name__ == '__main__':\n    counter = 0\n\n    rospy.init_node(\"plotter\")\n    rospy.Subscriber(\"position_measurements\", Subject, plot_x)\n    plt.ion()\n    plt.show()\n    rospy.spin()\n", "animation.FuncAnimation", "xdata.append(frame)\nydata.append(np.sin(frame))\nln.set_data(xdata, ydata)\n", "<!-- start plotjuggler -->\n<node pkg=\"plotjuggler\" type=\"PlotJuggler\" name=\"my_plot_Juggler\" args=\"\" />\n"], "url": "https://answers.ros.org/question/264767/plotting-real-time-data/"},
{"title": "Get link absolute orientation - IMU position", "time": "2018-01-12 00:11:01 -0600", "post_content": [" ", " ", "Can anyone advise how to calculate the quaternion orientation of a link? I'm following a somewhat unconventional approach and using an IMU that outputs it's absolute quaternion to measure the position of my end effector link. I can see the absolute orientation of each link if I look at the TF tree in RVIZ.", "I have encoders on my other joints which gives me an accurate pose up to the end effector. To remove the effects of these other joints on the IMU reading I have tried to multiply the IMU output by the inverse of the orientation quaternions to that point. I have pulled these quaternions from the rotation component of the TF broadcast. I then set my end effector position using the roll component of the transformed quaternion e.g.", "The problem is that the variable ", " doesn't seem to match the TF absolute position of the link before the end effector. If I look at the TF output in RVIZ and publish a pose in the same position, there's a discrepancy between the two. I had a look at the source for the robot state publisher and it looks like it's multiplying the pose matrices of the joints from TF to get the absolute quaternion for each. Is that right?", "My end effector can rotate 180 degrees and getting it's orientation right is fairly critical to my application. Unfortunately I can't change to an encoder at this point so I'm stuck with my IMU. Because the other joints can twist and roll I need to remove their effects on the IMU and measure only it's relative change in orientation. Any help appreciated!"], "answer": [" ", " ", "If you've calibrated your IMU correctly then it will be producing orientation estimates in a global (truly) frame, up/down east/west north/south. If you create a static transform from your robots ", " to the orientation of this global frame. You can then produce a TF of the IMU's orientation in this new global frame. This will give you a frame in your TF tree which is the orientation of the IMU connected to the TF tree for your robot.", "Once this is setup and working you can query the TF system to find the transform from the link before your end effector to the IMU frame you've just created, if you take just the orientation part of the resulting transform it will describe the relative orientation of your IMU (and therefore your end effector) in the frame of the preceding joint.", "Hope this makes sense.", "I think I follow. With the static transform, would that take into account the links along the arm? e.g.\nbase_link -> slew_link -> extension_link -> roll_link -> tilt_link -> imu_position\nthen something like:", "Not quite, the static transform is converting the orientation of the IMU into the base_link. base_link to tilt_link is already provided by the robot so then you have to do", " ", " ", "I've marked Pete's answers as correct because it put me on the right path. My actual solution was a little different.\nFirst I created a transform listener in my main and passed this a tf2_buffer.\nWithin my class as part of my callback method I used the buffer to look up the transform from second to final link to the base link. I then multiplied the inverse of this by my IMU output, giving me the IMU output in terms of roll only.\ne.g.", "As long as the IMU isn't powered on in the vertical position (X axis aligned to gravity), I can correctly and consistently measure the position of my end effector now.", "Glad you got this working, it can be tricky to get your head around all these frames and transforms at times!"], "question_code": ["tf2::convert(global_transforms_.transforms[0].transform.rotation, extension_pitch_component);\ntf2::convert(global_transforms_.transforms[1].transform.rotation, slew_yaw_component);\ntf2::convert(global_transforms_.transforms[7].transform.rotation, orbital_roll_component);\ntf2::convert(global_transforms_.transforms[8].transform.rotation, tilt_pitch_component);    \ntf2::Quaternion output;\noutput = tilt_pitch_component * (orbital_roll_component * (extension_pitch_component * slew_yaw_component));\ntf2::Quaternion publication;\npublication = output.inverse() * imu_reading;\ndouble roll, pitch, yaw;\ntf2::Matrix3x3(publication).getEulerYPR(yaw, pitch, roll, 1);\nsetJointPosition(roll);\n", "output"], "answer_code": ["listener.lookupTransform(\"/base_link\", \"/imu_position\",ros::Time::now(), transform);\n", "listener.lookupTransform(\"/base_link\", \"/imu_position\",ros::Time::now(), transform);\n", "tf2::Quaternion all_joints_to_nozzle;      \ngeometry_msgs::TransformStamped transform_stamped;\ntransform_stamped = tf_buffer_.lookupTransform(\"base_link\", \"tilt_assembly\", ros::Time(0));\ntf2::convert(transform_stamped.transform.rotation, all_joints_to_nozzle);\ntf2::Quaternion nozzle_offset;\nnozzle_offset.setEuler(0.0, 0.07, 0.0);\n//This results in an accurate position of the roll of the nozzle exclusively\ntransformed_quaternion = (all_joints_to_nozzle * nozzle_offset).inverse() * imu_quaternion; \ndouble roll, pitch, yaw;\ntf2::Matrix3x3(transformed_quaternion).getEulerYPR(yaw, pitch, roll, 1);\nset_position(index, roll);\n"], "url": "https://answers.ros.org/question/279630/get-link-absolute-orientation-imu-position/"},
{"title": "Problems with findpackage in cmake", "time": "2018-01-09 05:31:10 -0600", "post_content": [" ", " ", " ", " ", "Hi there,", "I am using CLion for Ros-MoveIt development. Now I want to compile a simple testprogram, but that does not work. My OS is Xubuntu 16.04. When I am adding ", "find_package(catkin REQUIRED COMPONENTS roscpp moveit_ros_planning_interface)", "to my CMakeLists.txt this error occures:", "Here is my simple source code:", "catkin is installed on my system, I can compile another testprogram from the terminal.", "Thx for helping :-)", "YS,\nbuzzzz", "\nOk, meanwhile i get these errors:", "Any ideas?", "Did you source your ROS Environment? Do you build with ", " or ", "?", "What do you mean with source your ROS Environment? I have builded the testproject with catkin build.\nYS", "Oh, do you mean the enviroment variables? I have on set them for the testproject...", "Please provide additional Information by editing your question, not answering it.", "echo $ROS_PACKAGE_PATH gives me that back... \n/opt/ros/kinetic/share\nI think this should be the right one...", "It looks like you're trying to build w/ ", ", but the environment isn't set up correctly. For example,  the ", " directory should be in ", ", not ", ". Build with either ", " or ", " -- this will generate the correct structure."], "answer": [" ", " ", " ", " ", "As has been mentioned in several of the comments, using CLion requires building with cmake directly rather than through Catkin.  This is off the \"garden path\" for both CLion and Catkin, so user beware.", "That being said.... we found a way that works for us.  Usually.  When the stars are aligned.  From a clean checkout of your intended workspace:", "It's definitely dodgy though, and does not work well with the ", " tool at all.  When in doubt, wipe your build and devel directories and start over using regular ", " outside of CLion.  It helps to think of it like \"making CLion load catkin projects\" rather than \"use CLion to build catkin projects.\"", "Still, this technique does mostly work (as of Jan 2018) and CLion is a very powerful tool when hacking on ROS stuff.", " For those interested, I created an issue with the CLion dev team and it was closed as a duplicate of:\n ", " \n... with a comment that \"we have no plans to support ...etc\" ", "There are many versions of this approach scattered around the Internet; this is just the one that seems to work for us, on Ubuntu 16.04 / ROS-Kinetic, this week."], "answer_details": ["Run ", " like you normally would", "Run ", " to load the workspace's environment variables correctly.  Also like would when regularly using catkin.", "Start CLion ", " in order to start CLion with the right environment variables", "Import the workspace's src/CMakeLists.txt as an existing CMake project", "Under project settings, change", " to ", " to ", " ", " ", " ", " "], "question_code": ["CMake Error at CMakeLists.txt:5 (find_package):\n  By not providing \"Findcatkin.cmake\" in CMAKE_MODULE_PATH this project has\n  asked CMake to find a package configuration file provided by \"catkin\", but\n  CMake did not find one.\n\n  Could not find a package configuration file provided by \"catkin\" with any\n  of the following names:\n\n    catkinConfig.cmake\n    catkin-config.cmake\n\n  Add the installation prefix of \"catkin\" to CMAKE_PREFIX_PATH or set\n  \"catkin_DIR\" to a directory containing one of the above files.  If \"catkin\"\n  provides a separate development package or SDK, be sure it has been\n  installed.\n", "#include <iostream>\n#include <ros/ros.h>\n#include <moveit/move_group_interface/move_group_interface.h>\n#include <moveit/planning_scene_interface/planning_scene_interface.h>\n\n\nusing namespace std;\nusing namespace moveit;\n\n\nint main()\n{\n\n//planning_interface::MoveGroup group(\"right_arm\");\nmoveit::planning_interface::PlanningSceneInterface planning_scene_interface;\n\ncout << \"Test\" << endl;\n\n\nreturn 0;\n}\n", "    /home/myName/MyPrograms/apps/CLion/ch-0/172.4343.16/bin/cmake/bin/cmake --build /home/myName\n\n    /ClionProjects/IKMoveIt --target IKMoveIt -- -j 2\n    -- Boost version: 1.58.0\n    -- Found the following Boost libraries:\n    --   system\n    --   filesystem\n    --   thread\n    --   chrono\n    --   date_time\n    --   atomic\n    -- Using CATKIN_DEVEL_PREFIX: /home/myName/ClionProjects/IKMoveIt/devel\n    -- Using CMAKE_PREFIX_PATH: \n    -- Using PYTHON_EXECUTABLE: /usr/bin/python\n    -- Using Debian Python package layout\n    -- Using empy: /usr/bin/empy\n    -- Using CATKIN_ENABLE_TESTING: ON\n    -- Call enable_testing()\n    -- Using CATKIN_TEST_RESULTS_DIR: /home/myName/ClionProjects/IKMoveIt/test_results\n    -- Found gtest: gtests will be built\n    -- Using Python nosetests: /usr/bin/nosetests-2.7\n    -- catkin 0.7.8\n    Traceback (most recent call last):\n      File \"/home/myName/ClionProjects/IKMoveIt/catkin_generated/generate_cached_setup.py\", line 20, in <module>\n        from catkin.environment_cache import generate_environment_script\n    ImportError: No module named catkin.environment_cache\n    CMake Error at /opt/ros/kinetic/share/catkin/cmake/safe_execute_process.cmake:11 (message):\n      execute_process(/usr/bin/python\n      \"/home/myName/ClionProjects/IKMoveIt/catkin_generated/generate_cached_setup.py\")\n   returned error code 1\n", "catkin_make", "catkin build", "cmake", "catkin_generated", "IKMoveIt/build", "IKMoveIt", "catkin_make", "catkin"], "answer_code": ["catkin_make", "source devel/setup.bash", "Generation Path", "../build", "CMake options", "-DCATKIN_DEVEL_PREFIX:PATH=/path/to/your/workspace_ws/devel", "catkin", "catkin_make"], "url": "https://answers.ros.org/question/279226/problems-with-findpackage-in-cmake/"},
{"title": "What compact PC to buy to run ROS in a mobile robot", "time": "2015-09-22 11:26:52 -0600", "post_content": [" ", " ", " ", " ", "Hi guys!", "After a few months using my i5 laptop in a Pioneer 3 AT mobile robot with kinect and sick lms100 laserscanner, it's time to look for a suitable compact PC/board to put in my robot.", "I am looking for a compact PC with enough power to run navigation nodes, kinect, laser, and so on and would like to ask for advice to the community. I've found ", " to be a possible choice (looking for something to work just out-of-the-box) but I'm wondering if that would be compatible with all ros and ubuntu stuff. Maybe it is a bit overkilled?\nChecked some info about that ", ".", "\u00bfWhat compact PC do you think would fit my needs?", "Any advice would be helpful! thx"], "answer": [" ", " ", "The intel NUCs are a very good out-of-the-box solution. An i7 is plenty of power, but the i5 versions usually are good enough for most people. Just be sure that you are not limited by the accessible ports. Think of how many USB, LAN, etc, or other ports do you need? \nIf you use many cameras, the USB ports could be a limiting factor if they are controlled via the same host controller.", "Otherwise any custom build using mini-ITX offers a bit more flexibility. Then you have a choice of dual LAN ports, multiple USB3.0/USB2.0 host controllers, etc. ", "In our youBot, we upgraded the internal PC using the intel DQ77 mini itx, which is sadly not for sale anymore. But similar options with the newer generation processors should be available.", "To power it from batteries, we use the mini-box openUPS which can switch seamlessly between Battery and external Power, and charge LiPos and other batteries, when connected to power.", "I've seen some Gigabyte PCs too but have lower features than NUCs. I'll have a look at that mini-box openUPS you point to ensure correct power. Thanks!", "Im currently using a i7 NUC on one of my robots, and its awesome", " ", " ", "Try to look to some advantech Fanless PC  such as ", " or series 2000.", "The housing is great for a mobile robot aplication but I'm afraid it is a little bit expensive and with not so high specs... Thanks anyway!", "I take this for the 2 serials ports and the quad core processor.\nIt depends on what you need :)", " ", " ", " One suggestion for the P3AT is that its easy to power from the robot's batteries (battery board) if the computer only requires unregulated 12v.  You can always check with us at  ", "  for info on powering/connecting the computer.   Unfortunately most currently available Mini-ITX boards with integrated ports are slightly too big to fit inside the robot (though we may have a solution for that at some point.. its a few mm difference) but smaller form factors may fit.   But if its in an enclosure on top that's not a problem. ", "Reed (Adept MobileRobots)", " ", " ", " ", " ", "Depending on your setup you could always run some of the services on a remote machine and keep your laptop on the robot. Then the form factor would not matter...", "My idea is to run all services on that PC and just use my laptop to visualize data through Rviz or to teleoperate the robot", " ", " ", " ", " ", " If ruggedness is a concern at all, would something like this:  ", "  which is fanless and low cost relative to a Series 1000/Series 2000 as mentioned above be advisable? "], "url": "https://answers.ros.org/question/218043/what-compact-pc-to-buy-to-run-ros-in-a-mobile-robot/"},
{"title": "Camera Stream Muliplexer", "time": "2018-01-31 18:18:09 -0600", "post_content": [" ", " ", "HI Everyone, ", "The robot platform I am working on has 6 cameras onboard with different views. \nThe Coms system only has the bandwidth for one of these views, what is the best (quickest/easiest) way to select one of the streams and publish it as a single topic. ", "I assume using the image transport system would be a good place to start?", "Regards,\nNick ", "That seems like a good answer to me, ", "."], "answer": [" ", " ", "Does your robot have onboard processing running? Is the 'Coms system' a bridge between the system on on the robot and your 'base station'? If so, ", " could help here.", "There is an Intel NUC running as the main processor, so plenty of processing power.\nYou are right, I didn't explain the situation very well. \nThe robot is teleoperated over a non-ROS comumincation link to the base station. \nIt seems like topic_mux is perfect for my needs, thank you very much", "Do realise that the NUC will then subscribe to all 6 cameras, but only republish one topic, that your base station node(s) will subscribe to. Depending on what sort of cameras these are, that could be very wasteful of cpu resources. Be sure to look at the ", " parameter."], "answer_code": ["lazy"], "url": "https://answers.ros.org/question/281457/camera-stream-muliplexer/"},
{"title": "suck2blow reference?", "time": "2018-03-12 04:39:57 -0600", "post_content": [" ", " ", "i usually use rosawesome aliases to shortcut commands in the terminal. suck2blow is an alias for 'rosbag record', but why? Is it a reference to something? "], "answer": [" ", " ", " ", " ", "See ", " and ", " for a related questions, and as a shortcut, see ", " for probably a rather authoritative source (spoiler: they're ", " references).", "Edit:", "I asked if suck2blow a reference to something.", "Yes. Again, to Spaceballs: ", ".", "What's the meaning of suck2blow and why is it linked with rosbag play?", "Mega Maid (from ", "):", "Mega Maid is the transformed version of Spaceball I. It is shaped like a woman with a vacuum cleaner. The vacuum can both suck air out of a planet's atmosphere, and blow it back into an atmosphere. It appears very powerful, since it can clean a mountain of snow, and pull a forest's trees straight out of the ground (and put everything back neatly if set in reverse). ", "So ", " first 'sucks' (records) and then 'blows' (plays back).", "yes i've seen those and you haven't answered my question. I asked if suck2blow a reference to something. Like an easter egg. What's the meaning of suck2blow and why is it linked with rosbag play?"], "answer_code": ["rosbag"], "url": "https://answers.ros.org/question/285116/suck2blow-reference/"},
{"title": "Controllers fail during execution when using octomap with Moveit", "time": "2018-04-21 11:56:41 -0600", "post_content": [" ", " ", " ", " ", "I'm using MoveIt with the default ", " motion planning library. I have a 6 DoF arm to which I pass target poses using roscpp's MoveGroupInterface. I'm using ros_control and have created my own Fake Controllers of the type ", ". The target poses are acquired from the readings of a depth camera in Gazebo.", "By default, I do not use the octomap using the depth cam. In this cases, MoveIt is able to generate the plans and also execute them successfully. I can see the arm moving in Rviz. I do not have an arm in Gazebo, it's only loaded in Rviz.", "When I use the octomap, MoveIt can generate the plans, but fails during execution. All the joint states are being published on topic ", ". The same code works when I remove the sensors.yaml file and don't use the octomap. The controllers are up. I don't see any other errors in the terminal. Please help me identify the cause.", "EDIT:\nWhen I increase the resolution of the octomap from 0.01m to 0.05m, then suddenly things start to work? I changed the value in moveit_config/sensor_manager.launch file as so:", "I'd like to make things work at 0.01m resolution, it looks granular enough. ", "Here are the error messages:\nError from MoveGroupInterface terminal where I pass in commands:", "Error from the MoveIt terminal:"], "answer": [" ", " ", "The octomap pointcloud updater locks the planning scene during updates. I'm going to take a guess and say that you have a depth sensors publishing pointclouds at 20 to 30 Hz. If that is the case, the 'rest of MoveIt' doesn't get a chance to do some useful work before the updater locks the scene again.", "Try decreasing the framerate and see if that works any better.", "I'm publishing the point clouds at 6Hz and am running on a fairly powerful PC. Here's a snippet from my camera.gazebo.xacro file where I configure the depth cam plugin:", "Same value also in the ", " plugin config, 6Hz.", "Also, if I were to use a real depth camera, like the realsense, how would I configure it? Would I need to limit the framerate on the real camera as well?", "Just to test, set the framerate to ", " Hz. See what happens. The fact that lowering the resolution of the octomap helps suggests this has something to do with processing capacity."], "question_code": ["RRTConnectkConfigDefault", "FollowJointTrajectory", "/joint_states", "<param name=\"octomap_resolution\" type=\"double\" value=\"0.05\" />\n", "[ INFO] [1524323290.317619120, 261.964000000]: Ready to take commands for planning group arm.\n[ INFO] [1524323293.819272021, 265.299000000]: 3D Co-ords of next target: X: 0.487010, Y: -0.132180, Z:-0.215447\n[ INFO] [1524323303.789838914, 275.190000000]: ABORTED: Solution found but controller failed during execution\n", "  [ INFO] [1524323029.558940862, 3.456000000]: Starting scene monitor\n  [ INFO] [1524323029.562571287, 3.460000000]: Listening to '/move_group/monitored_planning_scene'\n  [ INFO] [1524323033.952942898, 7.842000000]: Constructing new MoveGroup connection for group 'arm' in namespace ''\n  [ INFO] [1524323035.143276258, 9.027000000]: Ready to take commands for planning group arm.\n  [ INFO] [1524323035.143337694, 9.027000000]: Looking around: no\n  [ INFO] [1524323035.143357118, 9.027000000]: Replanning: no\n  [New Thread 0x7fffbef7e700 (LWP 20461)]\n  [Wrn] [Publisher.cc:141] Queue limit reached for topic /gazebo/empty/pose/local/info, deleting message. This warning is printed only once.\n  [ WARN] [1524323072.113526614, 45.706000000]: Failed to fetch current robot state.\n  [ INFO] [1524323072.113676595, 45.706000000]: Planning request received for MoveGroup action. Forwarding to planning pipeline.\n  Debug:   Starting goal sampling thread\n  [New Thread 0x7fffb3fff700 (LWP 21008)]\n  Debug:   Waiting for space information to be set up before the sampling thread can begin computation...\n  [ INFO] [1524323072.116232595, 45.709000000]: Planner configuration 'arm[RRTConnectkConfigDefault]' will use planner 'geometric::RRTConnect'. Additional configuration parameters will be set when the planner is constructed.\n  Debug:   The value of parameter 'longest_valid_segment_fraction' is now: '0.0050000000000000001'\n  Debug:   The value of parameter 'range' is now: '0'\n  Debug:   arm[RRTConnectkConfigDefault]: Planner range detected to be 4.017020\n  Info:    arm[RRTConnectkConfigDefault]: Starting planning with 1 states already in datastructure\n  Debug:   arm[RRTConnectkConfigDefault]: Waiting for goal region samples ...\n  Debug:   Beginning sampling thread ..."], "answer_code": ["<update_rate>6.0</update_rate>", "libgazebo_ros_openni_kinect.so", "1.0"], "url": "https://answers.ros.org/question/289286/controllers-fail-during-execution-when-using-octomap-with-moveit/"},
{"title": "Images from fisheye camera are displayed correctly with BGR8 encoding but not with MONO8", "time": "2018-04-22 04:45:13 -0600", "post_content": [" ", " ", "Hi,\nI want to output grayscale images from a fisheye camera (FOV < 180 degree). I understand that ROS currently doesn't support fisheye ", ". However, I see that if I use a BGR8 image encoding (by setting sensor_msgs::image_encodings::BGR8), I can still get a \"normal\" distorted image, like this:\n", ".", "But if I change to MONO8 encodings, the image is unusable:\n", " ", "These 2 images were captured with my camera firmly attached to the table. \nSo I don't understand why I can get a \"normal\" color image but not a grayscale one? "], "answer": [" ", " ", "Hi There,", "The problem here is that the encoding you are setting doesn't actually control the image content it only describes what's there. In the case of your first image it is being stored in 8 bit BGR encoding so with that specified it looks as it should.", "In the case of the second image you've set the encoding to Mono 8 but the data of the image hasn't changed, as a result we are only seeing the leftmost third of the image. In this image each horizontal group of 3 grey pixels intensities are representing the blue, green and red components of the original image.", "It's possible that the camera node you're using cannot produce grey scale images directly. In that case you can make a simple node that receives color images converts them to grey scale using OpenCV and publishes them again.", "I have worked with wide and fish-eye lenses quite a bit in ROS, I use the ", " functions directly.", "Hope this helps.", "@PeterBlackerThe3rd: you are correct. I figured out that my camera needs to be set in gray_scale_mode to output grayscale images, otherwise, it will output color. Thanks for your clarification!", "No problem, glad you got it working."], "url": "https://answers.ros.org/question/289324/images-from-fisheye-camera-are-displayed-correctly-with-bgr8-encoding-but-not-with-mono8/"},
{"title": "How to setup a buildfarm for my git repo ?", "time": "2018-04-23 14:47:28 -0600", "post_content": [" ", " ", " ", " ", "NOTE: I'm asking this question after going through series of web pages, ros discourse discussions, ros answers discussions and issues in github. Please note that I made a considerable effort to learn but the documentation to me seems more sparse (high level). Please understand that I'm a total noob and if something you see here looks rubbish or garbage please point it out. I'm ready to learn. Having said that here it goes.", "I have a git repo with me that takes in the camera images and estimates user pose, saves data and trains a classifier. I want to setup a build farm for this repo, perform continuous integration and post build steps. Oh ! and also, please let me know if this is possible with the ros buildfarm. Provide documentation that clearly explains what to do if someone like me wants to setup a buildfarm.", "I started off here at ", " to learn about buildfarms and then I stumbled upon ", " which I tried to follow in the following ways. ", "I found that the next step is to change the common.yaml file which is found ", " which I directly edited on my forked repo. I have several questions on this one. Any good piece of information will be greatly appreciated.", " 2.1. In ", " file, with the ", " for ", ", I understood that I have to replace the values for ", " and ", " with my jenkins username & password and I changed it. If this wrong please advise me what exactly I should be doing here. ", "2.2. Next up I left ", " as it is. ", " were simple enough that I changed the IP address to my master machine and raspi (for now, I can change this to another VM if cross compilation won't work). ", " is time zone and I changed it accordingly.", "2.3. ", ", I would like to know how to actually do this. First up, I generated a ssh keygen based on this ", ", which should generate two keys, a public and a private one. In ", ", should I copy paste the public key or the private key, I generated locally ?", "Once I get the answers for all the questions, I will proceed with the next set of questions. Thank you for taking time in reading this."], "answer": [" ", " ", " ", " ", "What exactly I'm trying to achieve here ?", "I have a git repo with me that takes in the camera images and estimates user pose, saves data and trains a classifier. I want to setup a build farm for this repo, perform continuous integration and post build steps. Oh ! and also, please let me know if this is possible with the ros buildfarm. Provide documentation that clearly explains what to do if someone like me wants to setup a buildfarm.", "Depending on your end-goal, it may be easier to begin either without ros_buildfarm or using ros_buildfarm without a full buildfarm deployment:", "A. Running your repositories tests. ", "\nIf all you want is basic continuous testing, setting up an entire buildfarm might be overkill. There are examples of setting up ROS repositories in Travis. ", " provides a sample .travis.yml and information for doing exactly that. It might be a bit stale and it's not something I've worked with, but once set up, it would run your tests without the need for running your own buildfarm. MoveIt! has a ", " with an up-to-date Travis config as well. (Hat tip to ", " in the comments).", "B. If you also want to build binary packages for your repository, it's still possible to do without setting up a buildfarm deployment, but you will need to create suitable rosdistro and ros_buildfarm_config configuration files so that the ros_buildfarm scripts can be run on your local machines.", "C. Of course, you can also set up a complete buildfarm cluster to perform run ros_buildfarm operations. This is probably the best option if you're going to be adding more custom packages down the road or building for multiple rosdistros. It's also the course of action you're already on.", "I thought of having a Jetson Tx2 as my agent and a Raspberry Pi 3 B as my repo but I'm willing to change this to two more VMs having same specs as my master.", "I mentioned in your previous question that the buildfarm_deployment repository was originally written for x86 hosts only and that I know it doesn't support arm64 hosts out of the box and it probably won't work on a Raspberry Pi running Raspbian Stretch or even Ubuntu Xenial. I have deployed arm64 agents but I had to do the last 40% of the setup by hand which requires some experience and understanding of what the Puppet config management is doing for you so it won't be straightforward if you're new to Puppet and ros_buildfarm. You also won't be able to build for AMD64 (x86_64) targets with an arm64 agent, though you can build for arm64 with an AMD64 host using qemu, which the buildfarm_deployment sets up by default.", "I would recommend using AMD64 virtual machines although they don't all need to be as powerful as your Jenkins master. ", " has some recommended instances sizes using AWS instances sizes as a ...", "For Travis integration the ", " has been updated recently so is likely to still work.", "Thank you for your detailed explanation. I'm following the steps you mentioned and yes, I'm building a buildfarm (C). I would like to know whether I have to install jenkins in my VM instance ?", "I would like to know whether I have to install jenkins in my VM instance ?", " The buildfarm_deployment uses Puppet to install jenkins for you, it should work as long as your VM is relatively clean. Deployment instructions:  ", "I took the instructions and I have configured the common.yaml file accordingly. I have added a new edit to my question. Hope you could help ", ". Thanks !", "Puppet (err): Evaluation Error: Error while evaluating a Function Call, Error from DataBinding 'hiera' while looking up 'ntp::autoupdate': (<unknown>): could not find expected ':' while scanning a simple key at line 33 column 1 at /root/buildfarm_deployment/modules/profile/manifests/ros/base.pp:8:3", "I'm getting an error like the above one when I tried to run reconfigure bash script for master. Could you please help me out here ?", " The question is quite old - but I stumbled over the same issue, so it might help others.\nThat is most likely an error in some yaml file.\nLook here  ", "  Download yamllint. check common.yaml, ... ", " ", " ", "If ", " running your own infrastructure is an option (similar to option A suggested by @nuclearsandwich) you could setup Travis CI (or any other CI provider) to build and test your code. The ", " docs contain some example and snippets how to do so: e.g.  "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "What exactly I'm trying to achieve here ?", "What have I done so far ?", "Fork the ", ", create ssh keys in my master machine which is ubuntu 16.04 xenial and has ros kinetic which is running on a VM. I thought of having a Jetson Tx2 as my agent and a Raspberry Pi 3 B as my repo but I'm willing to change this to two more VMs having same specs as my master. Please let me know whether there are any other steps apart from setting up 3 instances which is not captured in the documentation that I have to follow. "], "question_code": ["jenkins::slave::ui_user", "jenkins::slave::ui_pass"], "answer_code": ["ros_buildfarm"], "url": "https://answers.ros.org/question/289455/how-to-setup-a-buildfarm-for-my-git-repo/"},
{"title": "Schunk_lwa4d problem initialising with ros_canopen", "time": "2018-04-23 09:52:13 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "Edit: I also opened issue ", "  in ros_canopen github.", "I am trying to use a schunk_lwa4d arm with an esd CAN-USB/2 in ROS but I have issues initializing it. I am using ", " launch file in the shunck_lwa4p package from schunk_robots.", "First I do: ", "and then I run the above launch file and try to initialise with ", ".\nIn the terminal I executed the rosservice I get:", "In the terminal I executed the ros launch I get:", "Any ideas?", "Edit: thanks for you reply Mathias,", "The problem was solved. The modules were not in CANopen. Using the widows tools from Schunk I had to reset the arm back to Canopen. Now it works in ROS.", "If you can move/manage the modules with the Schunk tool, then they are not in CANopen 402 mode.\nYou have to solve this python issue and set the modules to the proper mode.", "Hello, Mr. Chiou. I am using the same configurations.  I am getting exactly the same problem. Can you give me some advice how to solve that please ?", "Hi,", "As I say above the modules were not in CANopen. Using the widows tools from Schunk I had to reset the arm back to Canopen."], "answer": [" ", " ", " ", " ", "The ", " package reflects the (special?) configuration that was used with Care-O-bot 4.\nAs the init service error implies node 3 could not be reached.", "This might have several reasons:", "You can check/set these things with the software tools provided by Schunk.", "The node IDs can also read with candump:\nJust start ", " before powering on. While boot-up the devices should send messages with IDs ", " + node ID (hex!)", "Indeed the problem was that the modules were not in CANopen."], "answer_details": ["There is no node 3. Module IDs can be set freely. The Care-O-bot convention for LWA4d is 3-9.", "Your bitrate is not correct. (Not sure if 500k or 1M is the default)", "Your modules are not in CANopen 402 mode. ", " ", " ", " ", " "], "question_code": ["sudo modprobe esd_usb2\nsudo ip link set can0 up type can bitrate 500000\nsudo ip link set can0 txqueuelen 20\n", "rosservice call /arm/driver/init", "success: False\nmessage: \"could not reset node '3'\"\n", "[ INFO] [1524494867.079043073]: Using fixed control period: 0.010000000\n[ INFO] [1524494871.222930279]: Initializing XXX\n[ INFO] [1524494871.223441822]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1524494871.223800403]: Current state: 2 device error: system:0 internal_error: 0 (OK) error: 136\n[ INFO] [1524494871.224968675]: Current state: 2 device error: system:0 internal_error: 136 (protocol violations;)\n[ INFO] [1524494871.225023587]: Current state: 1 device error: system:0 internal_error: 136 (protocol violations;) ID: 136\n[ERROR] [1524494871.234310747]: CAN not ready\n[ INFO] [1524494871.899096913]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n[ INFO] [1524494876.910831528]: waitForService: Service [/arm/controller_manager/load_controller] has not been advertised, waiting...\n[ INFO] [1524494881.225471058]: Current state: 1 device error: system:125 internal_error: 136 (protocol violations;)\n[ INFO] [1524494881.225579044]: Current state: 0 device error: system:125 internal_error: 136 (protocol violations;)\n[ INFO] [1524494881.225706891]: Current state: 0 device error: system:0 internal_error: 136 (protocol violations;)\n[ INFO] [1524494881.225850755]: Current state: 0 device error: system:0 internal_error: 136 (protocol violations;)\n[ INFO] [1524494881.917377871]: waitForService: Service [/arm/controller_manager/load_controller] has not been \nadvertised, waiting...\n"], "answer_code": ["schunk_lwa4d", "candump can0", "0x700"], "url": "https://answers.ros.org/question/289428/schunk_lwa4d-problem-initialising-with-ros_canopen/"},
{"title": "publisher can only in While? no, maybe other reason.", "time": "2018-06-11 05:10:34 -0600", "post_content": [" ", " ", " ", " ", "After execute the following code, the subscriber CANNOT receive msg.\nI start this publisher by :\n ", ". \nAnd start the Subscriber in terminal by :\n", "When I show the rqt_graph, it shows that they have been get linked.", "But when I add while loop like this, it works well.", "Also, I need to restart the subscriber when I restart the demo_pub file. And how can I publish a message only once?", "The code has nothing wrong.", "I figure out the reason but cannot get the answer.", "After I change the master_urI value within .bashrc to other IP address, and then run \" roscore\" in the terminal of device with that IP, then the strange problem reappeared again.", "So, each time I change the master of roscore, I restart my ubuntu system will help.", "I just run \"source .bashrc\", and restart the terminal before.", "Even if I add the following code after publish,the subscriber still cannot get msg.", "It's look fine, can you add the code of your Subscriber ?", "Thanks, I just check it on terminal by ", "And  Even for the while code, when I restart the publish file , the Subscriber need to be restart for receiving msg.", "Do you start the ", " before or after the node ? I can't reprodruce any problem with your code", "Thanks, I have exec it on terminal before I run the  demo_pub.py."], "answer": [" ", " ", " ", " ", "The code has nothing wrong.", "my master on raspberry and nodes on ubuntu system of PC.", "Last night I recharge the raspberry's li battery and this morning the code works fine.", "The problem cannot reappear again, I guess there is 2 possible reason.", "After I change the master uri, the problem happened. And I restart my ubuntu system, the problem disappeared.", "Please don't use an ", " to provide more information about your ", ". This isn't a forum. Please update your question with this information instead.", "ok, thanks.", " ", " ", "I think I got it : ", "There is 2 possible problem here : ", "You didn't close your subscriber : If you registered your subscriber (in your case : rosptopic echo /chatter) with a roscore, then you closed this roscore and open a new one, the subscriber will not receive any message from the new roscore.\nYou can check it by : ", " Your publisher didn't have the time to send the message. Ros need some time to setup the publisher/Subscriber :  ", "  or  ", "Try to properly restart your roscore and subscriber and then try to add a little wait after your rospy.Publisher() to see if this works now.", "Thank you so much for the detail explainning. "], "answer_details": ["Battery is under voltage, so it works well after recharged.", "I restart the raspberry device and Ubuntu system, so\nthe ros system is reset also.", " ", " ", " ", " ", " ", "Open a roscore", "Send a message to /chatter", "Open your subscriber to /chatter", "Close your roscore", "Open your roscore again", "Send a message to /chatter, nothing will happen", " ", " ", " ", " ", "I didnot close or restart roscore.", "I add the loop sleep, but cannot\nwork as well."], "question_code": ["#!/usr/bin/env python\n# license removed for brevity\nimport rospy\nfrom std_msgs.msg import String\n\ndef talker():\n    pub = rospy.Publisher('chatter', String, queue_size=10)\n    rospy.init_node('talker')\n    hello_str = \"hello world %s\" % rospy.get_time()\n    rospy.loginfo(hello_str)\n    pub.publish(hello_str)\n\nif __name__ == '__main__':\n    try:\n        talker()\n    except rospy.ROSInterruptException:\n        pass\n", "#!/usr/bin/env python\n# license removed for brevity\nimport rospy\nfrom std_msgs.msg import String\n\ndef talker():\n    pub = rospy.Publisher('chatter', String, queue_size=10)\n    rospy.init_node('talker')\n    rate = rospy.Rate(10) # 10hz\n    while not rospy.is_shutdown():\n        hello_str = \"hello world %s\" % rospy.get_time()\n        rospy.loginfo(hello_str)\n        pub.publish(hello_str)\n        rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        talker()\n    except rospy.ROSInterruptException:\n        pass\n", "rate = rospy.Rate(10)\nwhile not rospy.is_shutdown():\n     rate.sleep()\n", "rostopic echo /chatter\n", "rostopic echo /chatter"], "url": "https://answers.ros.org/question/293771/publisher-can-only-in-while-no-maybe-other-reason/"},
{"title": "How can I speed up simulation in Gazebo?", "time": "2011-04-15 21:37:12 -0600", "post_content": [" ", " ", "What are the possible options for speeding up simulation in Gazebo? From talking to John I know there's work in progress on GPU-based acceleration of ODE. What's the status of that? What other settings for tuning performance are already available?\nThe scenario I have in mind involves teleoperation of a robot, so real-time simulation would be highly desirable."], "answer": [" ", " ", "Stefan-", "I spent some time early this year working configurating Gazebo to run faster with the PR2 model. Basically, my approach was to downsample sensors as much as possible, downsample physics, and then patch Gazebo as needed where the profile showed huge hits. Like you, my application is teleoperation. With a powerful machine, Gazebo can run at real time or faster (I run at 1.4x real-time using an Intel i7 2600K with a GT 560 Ti GPU). I would guesstimate it to run about 5x faster with the PR2 model.", "If you use the \"kt2_simulator\" branch of Gazebo, (", "), you'll find the changes to the URDF of the PR2, controller gains, and the world file. A rosinstall file will allow you pull down the branch of Gazebo we're using. These changes should go out in Electric.", "The biggest gains I found found were from lowering the physics update rate from 1KHz to 250Hz. Even if you have to increase the number of quick step iterations per physics loop, decreasing the update rate is still a win because you don't have to do collision checks as often. Obviously, this can cause problems in physics quality, and objects/robots can \"explode\". You'll have to tune controller gains for a lower update rate. The modified gains for the PR2 are in the \"kt2_pr2_defs\" package in the above stack.", "One major improvement to PR2 physics to a \"fixed joint reduction\" which John put in place. The PR2 URDF calls out over 40 fixed joints (usually to connect a sensor to a body). In Diamondback, Gazebo models these joints as revolute joints. In the branch, the URDF is reduced to only bodies that are connected by revolute or prismatic joints. This increases physics-only simulation by about 50%. If you run the \"urdf2model\" or \"spawn_model\" utilities, fixed joints in your URDF will be reduced.", "A large performance hit to the simulator is laser scan generation. The laser data is generated with ODE collision checking, and you can see a very noticeable difference in speed when you are subscribing to the laser data. For the PR2, I cut the update rate for the lasers in half, and reduced the number of rays from 640 to 180 for each laser. These are changes to the URDF, but ideally they will be dynamically reconfigurable at some point. ", "Camera rendering is also expensive. In the branch, I implemented a reconfigurable update rate for each camera to allow you to throttle down the render rate (this feature exists in Diamondback, but does not actually change the render rate of the camera, so does not affect the simulation speed). Slow down the camera update rates as much as you can. If you don't subscribe to a camera topic, the data won't be generated, so make sure you have lazy subscription in your pipeline.", "In the new branch, Gazebo cameras can generate depth data, like a Kinect. In ...", "Hello Kevin.\nI tried your version but it does not seem to compile because of dependencies to an old version of ffmpeg. Do you have a newer patched version that runs with the latest libraries and latest version of gazebo?", "Best, Reinhard", " ", " ", "In trunk of physics_ode and simulator_gazebo, in addition to Kevin's above mentioned improvements, another notable change improving PR2 simulation performance is the introduction of an experimental screw joint constraint for gripper and torso simulation.  The screw constraint alleviates previously existing numerical stiffness at gripper and torso joints due to high controller gains and high joint mechanism reduction ratio.  Allows PR2 to run at time step size as large as 4 milli-seconds (250Hz) without URDF controller gain changes from current released 1kHz control loop values.", " ", " ", "With gazebo 9 I would try to do more parallelism", " See here for more:  ", "parallelization of the\n  physics engine is the direction to go\n  in order to help improve performance,\n  with the goal of running complex\n  robots and environments in real-time.\n  Parallel Strategies", "Two strategies to parallelize physics\n  have been implemented: island thread\n  and position error correction thread.\n  For more details about these two\n  strategies results and analysis,\n  please refer to the parallel physics\n  reports on the Gazebo webpage. Island\n  Thread", "The first strategy attempts to\n  parallelize simulation of\n  non-interacting entities. Simulated\n  entities are interacting if they are\n  connected by an articulated joint\n  (such as a revolute or universal\n  joint) or are connected via contact.\n  Groups of interacting entities are\n  clustered into \"islands\" that are\n  mathematically decoupled from each\n  other. Thus each island can be\n  simulated in parallel. After each\n  step, the clustering of islands is\n  recalculated.", "Anyone tried that option yet?"], "url": "https://answers.ros.org/question/9747/how-can-i-speed-up-simulation-in-gazebo/"},
{"title": "How can I stop Rtabmap from publishing transform data on /tf topic?", "time": "2018-05-31 08:09:45 -0600", "post_content": [" ", " ", " ", " ", "Hello, \nI am trying to implement slam on a mobile robot. I have created my own frames and linked them and I am publishing them on /tf\nI do not want rtabmap to publish its own transforms on this topic (I am using rtabmap only for visual odometry)\nI have a launch file as follows:"], "answer": [" ", " ", "Hi,", "if you are using only visual odometry, I suggest to use the node directly than using rtabmap.launch:", "With ", " set to false, TF will not be published.", "cheers,", "\nMathieu", "Hello Mathieu\nThanks a lot, This is just what I needed!\nCan i ask some questions out of curiosity? Like 1]what does rtabmap use to generate visual odometry? 2]How can i increase the sampling rate of the code. should I create a new question for it?", "1) By default, it uses OpenCV's feature matching and PnP estimation, with local bundle adjustment using g2o to refine motion (when built with g2o support). 2) visual odometry works as fast as it can. If odom frame rate is lower than camera frame rate, you have computation power limitation."], "question_code": ["<launch>\n  <!-- Convenience launch file to launch odometry, rtabmap and rtabmapviz nodes at once -->\n\n  <!-- For stereo:=false\n        Your RGB-D sensor should be already started with \"depth_registration:=true\".\n        Examples:\n           $ roslaunch freenect_launch freenect.launch depth_registration:=true \n           $ roslaunch openni2_launch openni2.launch depth_registration:=true -->\n\n  <!-- For stereo:=true\n        Your camera should be calibrated and publishing rectified left and right \n        images + corresponding camera_info msgs. You can use stereo_image_proc for image rectification.\n        Example:\n           $ roslaunch rtabmap_ros bumblebee.launch -->\n\n  <!-- Choose between RGB-D and stereo -->      \n  <arg name=\"stereo\"          default=\"false\"/>\n\n  <!-- Choose visualization -->\n  <arg name=\"rtabmapviz\"              default=\"false\" /> \n  <arg name=\"rviz\"                    default=\"false\" />\n\n  <!-- Localization-only mode -->\n  <arg name=\"localization\"            default=\"false\"/>\n\n  <!-- Corresponding config files -->\n  <arg name=\"cfg\"                     default=\"\" /> <!-- To change RTAB-Map's parameters, set the path of config file (*.ini) generated by the standalone app -->\n  <arg name=\"gui_cfg\"                 default=\"~/.ros/rtabmap_gui.ini\" />\n  <arg name=\"rviz_cfg\"                default=\"$(find rtabmap_ros)/launch/config/rgbd.rviz\" />\n\n  <arg name=\"frame_id\"                default=\"camera_link\"/>     <!-- Fixed frame id, you may set \"base_link\" or \"base_footprint\" if they are published -->\n  <arg name=\"namespace\"               default=\"rtabmap\"/>\n  <arg name=\"database_path\"           default=\"~/.ros/rtabmap.db\"/>\n  <arg name=\"queue_size\"              default=\"10\"/>\n  <arg name=\"wait_for_transform\"      default=\"0.2\"/>\n  <arg name=\"rtabmap_args\"            default=\"\"/>              <!-- delete_db_on_start, udebug -->\n  <arg name=\"launch_prefix\"           default=\"\"/>              <!-- for debugging purpose, it fills launch-prefix tag of the nodes -->\n\n  <!-- if timestamps of the input topics are synchronized using approximate or exact time policy-->\n  <arg     if=\"$(arg stereo)\" name=\"approx_sync\"  default=\"false\"/>\n  <arg unless=\"$(arg stereo)\" name=\"approx_sync\"  default=\"true\"/>         \n\n  <!-- RGB-D related topics -->\n  <arg name=\"rgb_topic\"               default=\"/camera/rgb/image_rect_color\" />\n  <arg name=\"depth_topic\"             default=\"/camera/depth_registered/image_raw\" />\n  <arg name=\"camera_info_topic\"       default=\"/camera/rgb/camera_info\" />\n\n  <!-- stereo related topics -->\n  <arg name=\"stereo_namespace\"        default=\"/stereo_camera\"/>\n  <arg name=\"left_image_topic\"        default=\"$(arg stereo_namespace)/left/image_rect_color\" />\n  <arg name=\"right_image_topic\"       default=\"$(arg stereo_namespace)/right/image_rect\" />      <!-- using grayscale image for efficiency -->\n  <arg name=\"left_camera_info_topic\"  default=\"$(arg stereo_namespace)/left/camera_info\" />\n  <arg name=\"right_camera_info_topic\" default=\"$(arg stereo_namespace)/right/camera_info\" />\n\n  <arg name=\"compressed\"              default=\"false\"/>         <!-- If you want to subscribe to compressed image topics -->\n  <arg name=\"rgb_image_transport\"     default=\"compressed\"/>    <!-- Common types: compressed, theora (see \"rosrun image_transport list_transports\") -->\n  <arg name=\"depth_image_transport\"   default=\"compressedDepth\"/>  <!-- Common types: compressed, theora (see \"rosrun image_transport list_transports\") -->\n\n  <arg name=\"subscribe_scan\"          default=\"false\"/>\n  <arg name=\"scan_topic\"              default=\"/scan\"/>\n\n  <arg name=\"subscribe_scan_cloud\"    default=\"false\"/>\n  <arg name=\"scan_cloud_topic\"        default=\"/scan_cloud\"/>\n\n  <arg name=\"visual_odometry\"         default=\"true\"/>          <!-- Launch rtabmap visual odometry node -->\n  <arg name=\"odom_topic\"              default=\"/odom\"/>         <!-- Odometry topic used if visual_odometry is false -->\n  <arg name=\"odom_frame_id\"           default=\"\"/>              <!-- If set, TF is used to get odometry instead of the topic -->\n  <arg name=\"odom_args\"               default=\"$(arg rtabmap_args)\"/>\n\n  <!-- These arguments should not be modified directly, see referred topics without \"_relay\" suffix above -->\n  <arg if=\"$(arg compressed)\"     name=\"rgb_topic_relay\"           default=\"$(arg rgb_topic)_relay\"/>\n  <arg unless=\"$(arg compressed)\" name=\"rgb_topic_relay\"           default=\"$(arg rgb_topic)\"/>\n  <arg if=\"$(arg compressed)\"     name=\"depth_topic_relay\"         default=\"$(arg depth_topic ..."], "answer_code": ["<node pkg=\"rtabmap_ros\" type=\"rgbd_odometry\" name=\"rgbd_odometry\" output=\"screen\">\n  <remap from=\"rgb/image\"       to=\"/camera/rgb/image_rect_color\"/>\n  <remap from=\"depth/image\"     to=\"/camera/depth_registered/image_raw\"/>\n  <remap from=\"rgb/camera_info\" to=\"/camera/rgb/camera_info\"/>\n  <param name=\"frame_id\"    type=\"string\" value=\"camera_link\"/>\n  <param name=\"publish_tf\"  type=\"bool\"   value=\"false\"/>\n</node>\n", "publish_tf"], "url": "https://answers.ros.org/question/292835/how-can-i-stop-rtabmap-from-publishing-transform-data-on-tf-topic/"},
{"title": "Creating a ROS msg", "time": "2018-05-31 12:49:50 -0600", "post_content": [" ", " ", " ", " ", " I was following the steps of the tutorial \"Creating a ROS msg and srv\"\n ", " \n and the step 3 I got this message: ", "At the step 2.1  how can I know if these two lines are uncommented ? Do I have to open the package.xml file with the editor as I did with the CMakeLists.txt?\nAfter I finish editing the CMakeLists.txt file, do I have to save it with the same name? ", "I followed the similar steps for Creating a srv and the result in step 4.2 was correct. ", "I am using ROS kinetic.\nI am running Ubuntu 16.04 LTS.", "step 3 didn't work normally", "What does this mean? What happened, what errors did you get? Please update your question with this information.", "I updated my question with the message I got. I tried again with the same results."], "answer": [" ", " ", " ", " ", "At the step 2.1 how can I know if these two lines are uncommented ? Do I have to open the package.xml file with the editor as I did with the CMakeLists.txt?", "Yes, you have to open the ", " file using any text editor and uncomment those lines. Usually for ", " files, comments are between ", "  and ", ". So, just make sure that is not the case.", "After I finish editing the CMakeLists.txt file, do I have to save it with the same name? ", "Yes, that is necessary as it enables you to run ", " correctly (without errors).", "Unable to load msg [beginner_tutorials/Num]: Cannot locate message [unit8] in package [beginner_tutorials] with paths [['/home/spyros/catkin_ws/src/beginner_tutorials/msg', '/home/spyros/catkin_ws/devel/share/beginner_tutorials/msg']]", "This error can be solved by correcting ", " to ", ".", "I tried the tutorial again and I have the same problem. At the step 3 I got the message: ", "Unable to load msg [beginner_tutorials/Num]: Cannot locate message [unit8] in package [beginner_tutorials] with paths [['/home/spyros/catkin_ws/src/beginner_tutorials/msg', '/home/spyros/catkin_ws/devel/share/", " is not a message type, so that error is understandable. Should it be ", "? And that is not a message, but a primitive type.", "Yes, it should be ", ".", "Yes. That was my mistake. I changed it to uint8 and I got the correct result in the command \nrosmsg show beginner_tutorials/Num", "Thank you!", "I have updated the answer. If this answer is correct, then click the green tick so that this question is answered and can be closed."], "question_code": ["Unable to load msg [beginner_tutorials/Num]: Cannot locate message [unit8] in package [beginner_tutorials] with paths [['/home/spyros/catkin_ws/src/beginner_tutorials/msg', '/home/spyros/catkin_ws/devel/share/beginner_tutorials/msg']]\n"], "answer_code": ["xml", "xml", "<!--", "-->", "catkin_make", "unit8", "uint8", "unit8", "uint8", "uint8"], "url": "https://answers.ros.org/question/292867/creating-a-ros-msg/"},
{"title": "Create PointCloud2 in Matlab", "time": "2018-05-15 12:53:26 -0600", "post_content": [" ", " ", "I am looking to create a PointCloud2 message in MATLAB and publish back to ROS via the Robotics Systems Toolbox. I am receiving a PointCloud2 message from my Intel Realsense camera and processing the RGB data for a lane detection algorithm. I am then using the distance data to know how far away the line is and to also integrate into my local Costmap for use in the Navigation Stack. ", "Is there a quick and dirty way to create this message in MATLAB. There is a function called readRGB and readXYZ within MATLAB to extract a depth image and a color image from the rgbd PointCloud2 but I cannot get that information back into the PointCloud2 message. I am not very familiar with using Point Clouds and would appreciate any help!! "], "answer": [" ", " ", "Looking at the Robotics Systems Toolbox, there definitely isn't a quick way although there may be a dirty way!", "If the Height, Width, ... , Fields, and Data properties allow writing you could add this data in manually yourself. But this is a very clunky task to achieve in MatLab.", "Creating point cloud messages doesn't seem to be a task they wanted you do with this toolbox!", "Do you recommend another method for processing a 3D cloud? I have been looking at the Point Cloud Library but have no experience with it.", "I'm an only school c++ coder so I would naturally recommend PCL, it's very powerful as well as easy to extend with your own algorithms. If you're comfortable working in c++ then you should definitely give it at go.", "They have a fairly good range of examples and theoretical tutorials to get you started, and it's fully integrated with ROS.", "Thank you for your assistance!", " ", " ", "Hi,", "I've written a utility that can do this for both MATLAB and Simulink. You can find the files here:", "Sebastian"], "url": "https://answers.ros.org/question/291212/create-pointcloud2-in-matlab/"},
{"title": "Nodelet basics", "time": "2018-05-22 04:56:46 -0600", "post_content": [" ", " ", " ", " ", "Since I want to implement Nodelets into my ROS projects I have the following questions which I cannot find an answer for;\nOne of my projects' nodes has one function which is finding matches in patterns which requires a lot of processing power and it contains many functions with loops and a lot of math stuff. I assume replacing this node with nodelets will improve the performance since it'll take less time to run through the whole code. ", "Was your edit meant to change your question? Because I believe my answer already covers the current version of your question as well as the previous one."], "answer": [" ", " ", " ", " ", "One of my projects' nodes has one function which is finding matches in patterns which requires a lot of processing power and it contains many functions with loops and a lot of math stuff. I assume replacing this node with nodelets will improve the performance since it'll take less time to run through the whole code. ", "The big advantage of nodelets is that they allow you to communicate using ROS msgs essentially \"for free\" (ie: msg exchanges become pointer-passing between nodelets ", "). This is especially advantageous when you're exchanging large messages frequently (ie: camera images, large point clouds, etc), as the size of the message does not contribute significantly any longer to total time taken to send out and receive a message (pointers are fixed size).", "If performance of your \"function\" is not bound by ", ", but by ", ", then converting things to nodelets will not improve performance (how could it: your CPU does not get faster all of a sudden, it still has to execute the exact same amount of code).", "Can nodelets be combined with regular nodes?", "This depends on what you mean by \"combined with\".", "If you're asking: can nodelets exchange messages with nodes, then: yes.", "If you're asking: can nodelets be combined (into a single executable fi) with nodes, then no."], "url": "https://answers.ros.org/question/291873/nodelet-basics/"},
{"title": "Is there a way to get ROS_INFO, ROS_ERROR, etc to output line numbers?", "time": "2012-10-25 11:28:58 -0600", "post_content": [" ", " ", " ", " ", "The title says it all. Is there a flag somewhere I can set to get the ROS_INFO/ROS_ERROR/... macros to output line numbers, or is that something I'm going to have to feed to ROS_INFO myself by using __LINE__?"], "answer": [" ", " ", " ", " ", " page's section about ", " is what you are looking for.", "Essentially", " ", " ", "If you double-click a given roslog message in rxconsole, it will bring up a window with this information.", "Hmm; this isn't working for me. I'm sure I could get it working, but before I invest energy in that, I'd like to revisit the original question: is there any way to get ROS_INFO to print the line number (without need of external tools like rxconsole)?", "What's not working? When you double-click a  line in rxconsole, it shows \"Location\" which is a file and line number. This information is contained in all messages on the ", " topic. If you ", " you will see ", " and ", " message fields for each rosout message."], "answer_details": ["${line} for __LINE__", "${function} for __func__", "${file} for __FILE__", "${node} for the node name, no pre-processor equivalent.", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["export ROSCONSOLE_FORMAT='[${severity}] [${time}]@${line}: ${message}'\n", "rosout", "rostopic echo rosout", "file:", "line:"], "url": "https://answers.ros.org/question/46806/is-there-a-way-to-get-ros_info-ros_error-etc-to-output-line-numbers/"},
{"title": "Error of a map made by gmapping", "time": "2018-07-24 21:53:59 -0600", "post_content": [" ", " ", "I'm trying to run a waffle pi along a path of 60cm width.\nDuring the test, I noticed that there is difference between a map made by gmapping and local map made during navigation.", "Please see the attached image or link.\nBlack line is made by gmapping and green dotted line is made during navigation.\nThese lines are misaligned and the difference is approximately 20cm.", "This phenomenon still happens after update of a Turtlebot3 package (v1.0.0) and a OpenCR firmware (v1.2.0).", "Please give me your comment and/or advise.", "BRs.", "I don't see any error in mapping. I see an offset in localization. Are you running any nodes to locate the robot in the map like AMCL?", "Thanks. \nThere is an offset between a map made by gmapping and local map made during navigation. I think this offset has a bad influence for navigation.\nI'm not sure why the offset occurs and how to minimize the offset. I use AMCL particles for localization.\nThank you in advance."], "answer": [" ", " ", "This seams like error in odometry data or that you are running a robot without localization. Try to run AMCL node for localization. See ", ".", "Thanks. As shown in ", ", I run the AMCL node during navigation. As you say, there may be error in odometory data, but I don't know how I can fix it.", "I think there is a problem during map making. I noticed two things.", "(1) When location of robot changes, results of laser scan (green dot in rviz) points to different  position. Please compare red circle in ", " and ", ".", "(2) While the robot slowly goes forward during map making, it looks like that the robot goes back and forth in rviz. Please see ", ". I think this is related to (1). ", "Thanks in advance.", "this is all OK. The robot \"jumps\" when slam calculates data. It seams that your odometry data says that robot moves slower than it really moves. Please check your odometry data, this should be in some yaml file of your diff drive controller. It could be that your wheels are too small...", "Also you can change the covarinance of your odometry. I am just not sure where, It should be published by your differential controller or it is defined in configuations of your slam/localization."], "url": "https://answers.ros.org/question/298507/error-of-a-map-made-by-gmapping/"},
{"title": "camera_info_manager error: type argument given to delete", "time": "2018-07-20 02:35:00 -0600", "post_content": [" ", " ", " I downloaded the image_common package from  ", "  and tried to build. The problem is, it fails with ", "Unfortunately I can't interpret the error, since I don't have experience with google tests. Can anyone help?", "I am using Ubuntu 16.04 and ROS kinetic"], "answer": [" ", " ", "Can you please first clarify why you can't install the Kinetic binaries (ie: ", ")?", " Good question, obviously I didn't knew that it was available, I did only check here:  ", "  and started downloading it. Therefore the question is of course obsolete.\nIs there a list which of the packages are available in the packages and which not? ", "Checking the wiki page like you did, there is a green ", " badge at the top of the page. That would be one indication something is installable using ", ".", "Another would be to check the ", " for your release.", "In general: installing pkgs using the released binaries is ", " prefered, so try that first.", "Only in a few special cases would a build from sources be attempted:"], "answer_details": [" ", " ", " ", " ", "no release available", "you want to develop the pkg itself", "you need a build of a specific version"], "question_code": ["Errors     << camera_info_manager:make /home/.../catkin/Catkin_test/logs/camera_info_manager/build.make.002.log                            \nIn file included from /usr/src/gtest/src/gtest-all.cc:42:0:\n/usr/src/gtest/src/gtest.cc: In destructor \u2018virtual testing::Test::~Test()\u2019:\n/usr/src/gtest/src/gtest.cc:1897:10: error: type \u2018const class testing::internal::scoped_ptr<testing::internal::GTestFlagSaver>\u2019 argument given to \u2018delete\u2019, expected pointer\n   delete gtest_flag_saver_;\n          ^\n/usr/src/gtest/src/gtest.cc: At global scope:\n/usr/src/gtest/src/gtest.cc:2177:1: error: prototype for \u2018testing::TestInfo::TestInfo(const string&, const string&, const char*, const char*, testing::internal::TypeId, testing::internal::TestFactoryBase*)\u2019 does not match any in class \u2018testing::TestInfo\u2019\n TestInfo::TestInfo(const std::string& a_test_case_name,\n ^\n"], "answer_code": ["sudo apt install ros-kinetic-image-common", "released", "apt"], "url": "https://answers.ros.org/question/298017/camera_info_manager-error-type-argument-given-to-delete/"},
{"title": "error in adding extra module in opencv", "time": "2017-05-05 03:42:11 -0600", "post_content": [" ", " ", " ", " ", "hi guys ", "i am trying to add opencv_contrib module to my existing opencv and i am getting following error, please help ", "these are last few lines of error ..", "Hi! ...Could you show what it says on the file: \"CMakeOutput.log\"?  It seems like you have some missing dependencies. Try installing libavresample first : \"apt-get install libavresample-dev\" and try it again."], "answer": [" ", " ", " ", " ", "Hi!", "I don\u2019t have a clear answer, but I\u2019ll share my experience so far with Opencv module.", "I recommend you to work with the at least sources in order to keep opencv and opencv contrib synced, and because releases of opencv contrib are not that frequent. If you are planning to keep it automatically updated, you might want to use scripts. In a bash script (update.sh):", "then with powershell:"], "question_code": ["-- Looking for sys/videoio.h - not found\n-- Checking for module 'libavresample'\n--   No package 'libavresample' found\n-- Found apache ant 1.9.6: /usr/bin/ant\n-- Caffe:   NO\n-- Protobuf:   YES\n-- Glog:   NO\nCMake Error at /home/zubair/opencv_contrib/modules/dnn/cmake/OpenCVFindLibProtobuf.cmake:32 (ocv_download):\n  Unknown CMake command \"ocv_download\".\nCall Stack (most recent call first):\n  /home/zubair/opencv_contrib/modules/dnn/CMakeLists.txt:5 (include)\n\n\n-- Configuring incomplete, errors occurred!\nSee also \"/home/zubair/opencv/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/zubair/opencv/CMakeFiles/CMakeError.log\".\n"], "answer_code": ["#!/bin/bash \ncd opencv && git pull \ncd ../opencv_contrib && git pull\n", "$src = \"I:/opencv-master/opencv\"\n$srcextra =\n\"I:/opencv-master/opencv/opencv_contrib\"\n$build = \"I:/opencv-master/build\"\n$target = \"Visual Studio 15 2017\nWin64\" bash update.sh cd $build\n\nrm -Recurse -Force CMake* cmake -G\n$target -T v140,host=x64 `\n    -DOPENCV_ENABLE_NONFREE=1 `\n    -DOPENCV_EXTRA_MODULES_PATH=\"../opencv_contrib/modules\"\n`\n    ../opencv | Tee-Object -Variable RESULT if(\"$RESULT\" -eq 1) {\n    \"cmake didn't succeed, exiting.\"\n    exit } msbuild.exe OpenCV.sln /verbosity:m /m cd ..\n"], "url": "https://answers.ros.org/question/261143/error-in-adding-extra-module-in-opencv/"},
{"title": "TF Error: Lookup would requrie extrapolation into the future", "time": "2018-08-01 21:09:51 -0600", "post_content": [" ", " ", " ", " ", "I'm running rtabmap on a jackal robot with bumblebee2 stereo camera on ros-kinetic. I get the following error when trying to view pointcloud2 topic in rviz: ", "When I view the tf tree ", " I see that i'm getting -0.086 sec old from the map broadcast by rtabmap.", "Can you please update your question with a copy and paste of the error instead of linking to an image? Text from images isn't searchable and people cannot copy and paste the text from it.", "Thanks for the feedback! Any ideas on what the issue could be?", "It's most likely a ", ". Take a look at that page and look for other solutions on this site, it's a pretty common problem that's been asked many times.", "If you go through that page and try those solutions and are still having go ahead please update your question."], "answer": [" ", " ", "This is often caused by either of the following issues:", "(There are probably more ways to have this happen, but those two have been most common for me.)", "ROS relies on each host keeping track of the current time. If you don't have a battery-backed-up clock you'll have to either have network connectivity for NTP or synchronize the clocks in some other way."], "answer_details": ["Running nodes on more than one host when the host's clocks are not synchronized closely enough. You need to run NTP on each host, to synchronize their clocks with a network server, or set the times manually upon boot, or have one machine synch from another.", "Mixing messages played back a bag file and \"live\" messages from running nodes.", " ", " ", " ", " "], "question_code": ["Transform [sender=unknown_publisher] For frame [front_camera_optical]: No transform to fixed frame [map]. TF error: [Lookup would require extrapolation into the future. Requested time 1533228746.513341601 but the latest data is at time 1533228664.878277882, when looking up transform from frame [front_camera_optical] to frame [map]]\n"], "url": "https://answers.ros.org/question/299390/tf-error-lookup-would-requrie-extrapolation-into-the-future/"},
{"title": "Gtest errors when compiling workspace", "time": "2018-08-06 13:14:51 -0600", "post_content": [" ", " ", "For some time now (due to some update, probably), I have problems with GTest when compiling a workspace.", "When installing GTest on the system, libgtest.so and libgtest_main.so are created, and cmake finds them. They are static or dynamic depending on how it is installed. Everithing is OK (I suppose)", "When compiling a workspace, I always have the same error:", "This is caused because in /opt/ros/kinetic/share/catkin/cmake/test/gtest.cmake:363,  libgtest.so is required to be built, even if it exists in the system, producing that error.", "Is there something I'm doing wrong, or is it misconfigured in my system?", "What version of the package ", " do you have installed?", "0.7.11-0xenial-20180222-175501-0800"], "answer": [" ", " ", "You will need to update ", " to at least version 0.7.12 (see  ", " ). ", "Yes, it fixed my problem :) Thanks!!!", "The point is that, as far as I know, 0.7.11 is the current version for kinetic xenial, and it seems to be broken...", "Since other Xenial systems don't face the problem I would assume there is a difference on your system. Do you have a custom version of gtest / googletest installed? Anyway the latest version of ", " should become available in Kinetic in a future sync.", "It is an updated xenial distro, and gtest is installed using the libgtest-dev package in the distro (1.7.0-4). Obviously, there is a particularity, but I do not figure out which.", "What is the process of updating ros-kinetic-catkin to 0.7.12 since there's only one candidate (0.7.11) to install with apt install? ", "I suppose to build from source and install. What's the preferred install location?", " Version 0.7.14 has recently been released into Kinetic. The status page ( ", " ) shows that it has been built already (first green square), once all other Kinetic packages have been rebuild it becomes available in the testing repo... ", "(second square, currently blue since it has still the old version) - that should happen within 24h. After that it becomes available in the main repo when the \"ROS Boss\" decides to sync which is announced on Discourse (usually within a couple of weeks).", "The Debian package is in the ", " repo ( ", " ) if you want to install it ahead of time. In this case that should be fine - in other cases I wouldn't recommend it. "], "question_code": ["-- Using empy: /usr/bin/empy\n\n-- Using CATKIN_ENABLE_TESTING: ON\n-- Call enable_testing()\n-- Using CATKIN_TEST_RESULTS_DIR: /home/paco/ros_ws/tests_ws/build/test_results\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Found gtest: gtests will be built \nCMake Error at /opt/ros/kinetic/share/catkin/cmake/test/gtest.cmake:372 (add_library):   add_library cannot create imported target \"gtest\" because another target   with the same name already exists. Call Stack (most recent call first):   /opt/ros/kinetic/share/catkin/cmake/all.cmake:147 (include)   /opt/ros/kinetic/share/catkin/cmake/catkinConfig.cmake:20 (include)   CMakeLists.txt:52 (find_package)\n\nCMake Error at /opt/ros/kinetic/share/catkin/cmake/test/gtest.cmake:374 (add_library):   add_library cannot create imported target \"gtest_main\" because another   target with the same name already exists. Call Stack (most recent call first):   /opt/ros/kinetic/share/catkin/cmake/all.cmake:147 (include)   /opt/ros/kinetic/share/catkin/cmake/catkinConfig.cmake:20 (include)   CMakeLists.txt:52 (find_package)\n\n-- Using Python nosetests: /usr/bin/nosetests-2.7\n-- catkin 0.7.11\n", "ros-kinetic-catkin"], "answer_code": ["ros-kinetic-catkin", "catkin", "building"], "url": "https://answers.ros.org/question/299791/gtest-errors-when-compiling-workspace/"},
{"title": "Add extra information to a map created by Gmapping", "time": "2018-08-07 07:40:15 -0600", "post_content": [" ", " ", "Dear all,", "I would like to add an extra information to a map created by gmapping.\nIn fact, in addition to 2d coordinates and the Occoupancy Grid (-1: Unknow 0: Free 100: Occupied) of each cell, I wish to add the height of this cell from the ground. for example: the height of a table from the ground is 0.75 meters, for a ball; 0.30 meters ...", "Is there someone who did this before ?", "Best Regards."], "answer": [" ", " ", "You may want to have a look at the ", ", it's a very powerful map storage and visualisation library. You can store any number of layers of information, including elevation data that you describe. It can support many different layers which can be configured at runtime.", "I think you'll have to adapt gmapping to be able to work with this, but it's not impossible that someones done this too.", "Thank you for the answer Peter."], "url": "https://answers.ros.org/question/299893/add-extra-information-to-a-map-created-by-gmapping/"},
{"title": "roscd can't find newly created packages (Win10/WSL)", "time": "2018-08-16 11:47:21 -0600", "post_content": [" ", " ", " ", " ", "I've already read half the internet, here's what's wrong:\nPackages created with catkin_create_pkg in a catkin workspace can't be found by roscd, therefore roslaunch etc. don't work either.\nI already found out that the path to the workspace is missing in the ROS_PACKAGE_PATH, that might be the problem.\nHere's what I already tried:", "Also, when trying to launch something (that worked in the exact same workspace before!), I get the error", "My setup is kind of complicated: My workspace is in a OneDrive-synced folder (Win10 file system), which is then mounted using WSL to have a Linux environment for ROS development."], "answer": [" ", " ", " ", " ", "Result: Spaces in the path to the workspace are deleted [..]", "This is a problem (the spaces). See ", " fi. Afaik, this has not been resolved (but I could be wrong, ", " seems to address at least part of the ", " problems).", "My setup is kind of complicated: My workspace is in a OneDrive-synced folder (Win10 file system), which is then mounted using WSL to have a Linux environment for ROS development.", "This is also not a typical setup and especially the \"onedrive synced\" part is worrying me. What does that do to your file permissions? File permissions on all files need to be correct, or binaries won't run and all sorts of other problems can come up.", "I would strongly recommend you try to get a baseline by replicating your setup on a regular Ubuntu installation (perhaps a VM).", "If that works while you're following the same workflow, it's more than likely that either WSL and/or your OneDrive setup is interfering.", "Also note: WSL is not a supported platform for ROS, and you are bound to run into issues.", "I'm not sure it is a good environment to start learning ROS in.", "See ", " for a related Q&A.", "In the end, I set up a VM with Linux, which isn't ideal on a not-so-powerful portable machine for sure, but at least ROS works adequately. Thanks."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Setting up a new workspace on the same level as the old one. Result: Nothing better.", "manipulating the .bashrc and ROS_PACKAGE_PATH. Result: Spaces in the path to the workspace are deleted, although I used them with escape characters. So the path is incorrect and I get the adequate error.", "Creating a new plain package in the workspace/src, problem as described", "I've sourced the devel/setup.bash a million times, that doesn't fix it.", "Restarting the console permanently (always sourcing setup.bash afterwards)"], "question_code": ["[] is neither a launch file in package [] nor is [] a launch file name.\n"], "answer_code": ["rosbash"], "url": "https://answers.ros.org/question/300751/roscd-cant-find-newly-created-packages-win10wsl/"},
{"title": "Robot \"bucking\" while driving in reverse", "time": "2018-08-09 20:02:26 -0600", "post_content": [" ", " ", " ", " ", "Doing the Slam experiments, my robot does this (see attachment) buckling or stuttering motion. What causes that?", ": I took the liberty of converting your video to a gif so we keep this question self-contained.", "Thanks. I couldn\u2019t find a way to attach a video. Animated gif. Clever \ufffd\ufffd"], "answer": [" ", " ", "Are you sending two 'cmd_vel' topics? \nIf so, one might be sent correctly and one seems to be at zero speed.\nCheck the 'cmd_vel' topic through rqt_graph.", "A typical case for this to happen is when there is still some teleop node running somewhere when starting autonomous navigation.", " I suspected this might be the case but didn\u2019t immediately think of how to check. Thanks for tip. @felixwidmayer I suspect the same. I think I checked that I killed it but there might have been a zombie teleport process that was not fully killed.", " @felix-widmaier - I see \"bucking\" behavior when teleoping in reverse, with no other nodes or packages running. So the theory was good, it doesn't; really seem to apply. Rosnode list, shows this: rosout, rqt_gui_py_node_4440, turtlebot3_core, turtlebot3_lds, turtlebot3_diagnostics.Thoughts??", "Perhaps a screenshot of ", " would be more informative. Be sure to create it when you're actually observing the behaviour in the video.", "It's very likely that there is a node publishing zero-Twists.", "I will try but I am almost 100% sure because I looked at the graph and saw no-one publishing on cmd_vel.", "I am suspecting that it is a weight distribution problem because it seems to change if I slide the battery around a little. It doesn't solve it but it seems more or less severe", "Could also be there is no proper ramp-up of teleop velocity commands. I already commented on your ", " about how the TB2 addresses that.", " by all means you've been super helpful and I am grateful! The teleop code supplied (and I looked at the source) definitely does do a rampup. I assume it's correct but I can investigate to make sure it does what is stated.", "re: investigate: that may be a bit much at this stage.", "I would take a look at some of the msgs on the ", " topic (or whatever is being used on the TB3). Just use ", " when the robot is standing still (replace with appropriate topic). Then compare when teleopping."], "answer_code": ["rqt_graph", "/cmd_vel", "rostopic echo /cmd_vel"], "url": "https://answers.ros.org/question/300209/robot-bucking-while-driving-in-reverse/"},
{"title": "Is there a way to list automatically the dependencies of my ROS package", "time": "2018-09-18 07:24:26 -0600", "post_content": [" ", " ", " ", " ", "Hello everyone,", "I noticed that I'm able to run my Python nodes even if I didn't report the dependencies properly in ", " and ", ". Many things work simply because all the required ROS packages are installed on my computer. Now assume I want to distribute my package, how to make sure that I didn't miss any dependency?\nIs there an automatic way to do that?", "I noticed that I'm able to run my Python nodes even if I didn't report the dependencies properly in ", " [..]", "For Python nodes, which are not compiled, there are (normally) no build dependencies, so ", " is not involved in those cases.", "Python pkgs only have run depends."], "answer": [" ", " ", " ", " ", "There are two tools that can actually help you with this.", " by ", " figures out many issue, also missing dependencies. To quote the ReadMe:", "Also ", " helps in setting the ", " and ", " up correctly. The combination of both tools is pretty powerful. But you should still but some thought into what is going on...", "\"Approved\" :-D", "Wow, approved by the author... Thanks, that looks handy."], "answer_details": ["Checks for dependencies by looking in the source code, message, service, action and launch files.", "Inserts build/run/test dependencies into your package.xml", "Inserts dependencies into your CMakeLists.txt (in both the find_package and catkin_package commands)", "Sorts lists of dependencies (in both package.xml/CMakeLists.txt)", " ", " ", " ", " "], "question_code": ["CMakeLists.txt", "package.xml", "CMakeLists.txt", "CMakeLists.txt"], "answer_code": ["roscompile", "catkin_lint", "CMakeLists.txt", "package.xml"], "url": "https://answers.ros.org/question/303655/is-there-a-way-to-list-automatically-the-dependencies-of-my-ros-package/"},
{"title": "Can we change the order of package compilation in catkin_make  command?", "time": "2018-09-11 07:30:07 -0600", "post_content": [" ", " ", " ", " ", " compiles all the ros packages available inside the ", ".\nso, do we have any provision to change the order of the packages to be compiled.", "with example:", "if I want to compile all packages with ", ", I don't have control over which package should compile 1st. \nI want it to be compiled as above order.", "So , do we have any provision for reordering the compilation of packages with ", ". or should I go with ", " file having individual  compilation command?", "Well, this should be done automatically when you specify the dependencies in your ", " correctly...", "I second ", " the correct method is to specify the package dependencies in your CMakeLists file then catkin_make will automatically build things in the right order.", " , would you  please explaion what changes I need to make in the CMakeLists.txt of /package_3 .\nI have done:\npackage_3/CMakeLists.txt/ find_package( package_1,package_2)", "what else things need to be added ?", "see the answer by ", " and the link to the docs therein.\nBasically, for every target (which is found in the calls ", " and ", ") you need the respective ", " calls.", "I have done all necessary changes but still I could not arrange the projects. after ", " , the packages are arranged as ", ". ", "the error I am getting is:\nCould not find configuration file provided by \"/package_1\" with any of the following name \npackage_1Config.cmake \npackage_1-Config.cmake", "You might want to share the exact content of your packages otherwise there is too much guessing involved."], "answer": [" ", " ", " builds all packages in a workspace within the same CMake context. So there is no \"order\" between packages.", "To make sure your packages still compile correctly with ", " you have to make sure to also specify cross-package target dependencies (e.g. using the \"EXPORTED_TARGETS\" mentioned in the ", ").", "If you actually want to build the packages in separate CMake invocations (which will avoid the need for cross-package target dependencies) you can use either of the following build tools: ", " (recommended for ROS 1), ", " (not recommended but very powerful and used by many people already), ", " (developed for ROS 2 and should work for ROS 1 too, very young so not as robust as the other ones yet)."], "question_code": ["$ catkin_make", "catkin_ws", "catkin_ws:\n\n       /package_1\n       /package_2\n       /package_3: dependent on package_1 and package_2\n", "$ catkin_make\n", "catkin_make", ".sh", "CMakeLists.txt", "add_library(TARGET ..)", "add_executable(TARGET ..)", "add_dependencies(TARGET ${catkin_EXPORTED_TARGETS})"], "answer_code": ["catkin_make", "catkin_make", "catkin_make_isolated", "catkin_tools", "colcon"], "url": "https://answers.ros.org/question/303060/can-we-change-the-order-of-package-compilation-in-catkin_make-command/"},
{"title": "Don't understand remapping", "time": "2018-09-18 02:13:34 -0600", "post_content": [" ", " ", "Could someone explain me in details what the remapping is doing? And how it is working? "], "answer": [" ", " ", " ", " ", "From a really high-level: remapping makes the ROS middleware return a ", " when a ", " asks for ", " with a certain name.", "In other words: if there is a remap from ", " to ", " and ", " asks for ", ", it will actually be returned whatever is registered with ", ".", "To make it more concrete (with two examples):", "These remaps are completely transparent (ie: invisible) to ", ", they are processed by the ROS middleware.", "The \"only requirement\" is of course that ", " and ", " are of the same message type. Remapping is not magic, it changes names, not types (that would be message ", ").", "The power here is that I can write a ", " and you can write a ", " and we can both ", " the topic names we want to use. ", "If we used different topic names, but one of our users wants to \"connect\" our two nodes together, they only have to provide a remapping. Either:", "Edit: you've probably already seen it, but just for completeness: see the ", " on the wiki for some more information.", "And although it's an article about ROS 2, the ", " article on the ROS2 design website might provide some additional background. It also has a section on ROS 1 remapping."], "answer_details": ["for a publisher: if ", " wants to publish to topic ", ", but there is a remap ", ", then it may advertise a ", " publisher, but it will actually be publishing to topic ", "for a subscriber: if ", " subscribes to ", ", but there is a remap ", ", it will request a subscription to ", ", but in reality it will subscribe to ", "make ", " use the topic name that ", " expects", "make ", " use the topic name that ", " expects", "remap both to a third name, different from what ", " and ", " use internally", " ", " ", " ", " "], "answer_code": ["name_a", "name_b", "node_x", "name_a", "name_b", "node_x", "chatter", "chatter->other_topic", "chatter", "other_topic", "node_x", "chatter", "chatter->other_topic", "chatter", "other_topic", "node_x", "chatter", "other_topic", "node_x", "node_y", "node_x", "node_y", "node_y", "node_x", "node_x", "node_y"], "url": "https://answers.ros.org/question/303611/dont-understand-remapping/"},
{"title": "NodeHandle::subscribe fail to subscribe topic", "time": "2018-10-25 09:52:23 -0600", "post_content": [" ", " ", " ", " ", " Hi, I'm new to ROS and C++ and doing Programming for Robotics ( ", " ). My problem is \"Create a subscriber to the /scan topic\" in exercise2. Here is my code: ", "HuskyHighlevelController.Cpp File:", "husky_highlevel_controller_node.Cpp File:", "No problem with build but I launched husky_world.launch first then I ran this node (husky_highlevel_controller_node) and I checked the info of husky_highlevel_controller node and scan topic. Found No relationship between them (also rqt_console to check info message but receive nothing). I'm confused with ", "and", "From the ROS/Tutorials/WritingPublisherSubscriber(c++), I think my_topic should be the topic found in rostopic list but didn't work for subscribing scan topic. Thanks."], "answer": [" ", " ", "As it's your homework: What is the scope of your subscriber and how long will it live? ", "Thanks for the hint.", "was the reason. I replace it by", "Worked as expectedly.", ": please mark the answer by ", " as correct by ticking the checkmark to the left of the answer. It will turn green.", ": Done. I'll get familiar with the forum and thanks for the reminding."], "question_code": ["#include \"husky_highlevel_controller/HuskyHighlevelController.hpp\"\n#include <std_msgs/String.h>\n#include <ros/ros.h>\n#include <sensor_msgs/LaserScan.h>\n\nnamespace husky_highlevel_controller {\n\nHuskyHighlevelController::HuskyHighlevelController(ros::NodeHandle& nodeHandle) :\n  nodeHandle_(nodeHandle)\n{\n  ros::Subscriber subscriber =\n  nodeHandle_.subscribe(\"scan\",1000,&HuskyHighlevelController::ScanCallback,this);\n  ROS_INFO(\"Successfully launched node.\");\n}\n\nHuskyHighlevelController::~HuskyHighlevelController()\n{\n}\n\nvoid HuskyHighlevelController::ScanCallback(const sensor_msgs::LaserScan& msg)\n{\nROS_INFO(\"Received\");\n}\n\n}\n", "#include <ros/ros.h>\n#include \"husky_highlevel_controller/HuskyHighlevelController.hpp\"\n\nint main(int argc, char** argv)\n{\n  ros::init(argc, argv, \"husky_highlevel_controller\");\n  ros::NodeHandle nodeHandle(\"~\");\n\n  husky_highlevel_controller::HuskyHighlevelController huskyHighlevelController(nodeHandle);\n\n  ros::spin();\n  return 0;\n}\n", "ros::Subscriber sub = handle.subscribe(\"my_topic\", 1, &Foo::callback, &foo_object);\n", "ros::Subscriber sub = handle.subscribe(\"my_topic\", 1, callback);\n"], "answer_code": ["  ros::Subscriber subscriber =  nodeHandle_.subscribe(\"scan\",1000,&HuskyHighlevelController::ScanCallback,this);\n", "subscriber_ =\nnodeHandle_.subscribe(\"/scan\",1000,&HuskyHighlevelController::ScanCallback,this);\n"], "url": "https://answers.ros.org/question/306743/nodehandlesubscribe-fail-to-subscribe-topic/"},
{"title": "Windows 10 ROS1 problem (not using WSL)", "time": "2018-11-01 12:47:30 -0600", "post_content": [" ", " ", " ", " ", " Hi ,\nI tried to install ROS1 melodic on windows 10 ( ", "  not the using the WSL).\nI have this error when i use the command: ", "$ catkin_make", "CMake Error at CMakeLists.txt:20 (message):\n  Search for 'catkin' in workspace failed (catkin_find_pkg catkin\n  C:/ws/turtlebot3/src):", "-- Configuring incomplete, errors occurred!\nSee also \"C:/ws/turtlebot3/build/CMakeFiles/CMakeOutput.log\".\nInvoking \"cmake\" failed", "I think this is because I don't know how to do an equivalent of this\necho \"source /opt/ros/kinetic/setup.bash\" >> ~/.bashrc\nsource ~/.bashrc", "But i didn't find catkin in the c:\\opt\\ directory. So maybe i miss something or maybe catkin is not in these directories.", "If someone from Microsoft reads this, maybe it exists a complete tutorial only for ros for windows to explain how to create a simple subscriber and a publisher from within visual studio. At this time, I don't find something about this. The tutorial to install is great, but it seems not enough for me.", "Thanks", "Edit: Thank you Sean and Lou,", "Good to have an answer from you ! I met you at ROSCon one month ago.", "I completely followed the guide from Microsoft, included Windows troubleshooting guide. So I used these 2 commands  \"choco upgrade ros-melodic-desktop -y\" and \"choco upgrade ros-catkin-tools\" before asking here with no success.", "I think I found the problem. When using \"c:\\opt\\ros\\melodic\\x64\\setup.bat\", env variables are set. But I have this for the PATH (before my regular path):\nC:/opt/ros/melodic/x64\\lib;C:/opt/rosdeps/x64\\lib;C:/opt/ros/melodic/x64\\bin;C:/opt/ros/melodic/x64\\lib;C:/opt/rosdeps/x64\\lib;C:/opt/rosdeps/x64\\bin", "Why / and \\ ??? But it's not the problem here.", "I add the python path (because catkin_find_package is in it):\nset PATH=c:\\opt\\python27amd64\\Scripts;c:\\opt\\python27amd64;%PATH%", "And it works ! So maybe a problem of the setup.bat script ?", "So now I will continue my progress and will test with a publisher and a subsriber.", "One more question: in your GettingStarted Guide, you tell to install Visual Studio Community edition 2017. I know this great IDE and used it for projets with my students. But in the case of ROS, what are the advantages using it compared to VScode with ROS VSCode Extension ?", "Thanks again and good to see that Microsoft is back in the robotics area !", "In your command prompt, have you run \"c:\\opt\\ros\\melodic\\x64\\setup.bat\" before running \"catkin_make\"? Does it exist \"C:/opt/ros/melodic/x64\\share\\catkin\" in your installation?", "Hi Sean,\nThanks for your reply. Yes I used the setup.bat (if not used, we have not the good env variables). The directory catkin exists but contains only cmake directory. The problem is elsewhere (see my answer to Lou below).", ": please do not post answers unless you are answering your own question. Use comments to interact with other posters ", " update your original question by editing it. Use the ", " button/link for that."], "answer": [" ", " ", "(Both Sean & I are from Microsoft)\nYour feedback about a walkthrough is appreciated. I'll work on updating our getting started with the talker nodes walkthrough.\nI am also working with our video team to produce a video walkthrough as well. I'll post when that is published.", "To expand on what Sean said:\nDuring the initial setup, you ran ", ". (Upgrade will do an install if it wasn't there before). ", "This downloads a package (a zip file) with a powershell script which expands the archive into the opt directory. It then runs rosdep on the package - which imports dependencies from our chocolately server.", "If there is a network hiccup during that install, you may have a partially installed image. We have seen issues with catkin install, which we are trying to isolate. ", " will correct it.", "There's also a ", ", which we are updating as we encounter situtations.", " We will continue to monitor ROS Answers, and you can create a tracking bug here:  ", " . ", "Thanks!\nLou"], "question_code": ["edit"], "answer_code": ["choco upgrade ros-melodic-desktop -y", "choco upgrade ros-catkin-tools"], "url": "https://answers.ros.org/question/307433/windows-10-ros1-problem-not-using-wsl/"},
{"title": "toolbox for segment 3D pointclouds from lidar", "time": "2018-12-08 03:38:32 -0600", "post_content": [" ", " ", "Hi, is there anyone happen to know some toolbox or github source code to deal with the 3D pointcloud segmenation from lidar sensor? I am working on the project and need some segmentation toolbox."], "answer": [" ", " ", "I'd  recommend the ", ". It's very powerful although not the easiest to learn in the first place. It's also well integrated with ROS already, so you can convert to and from point cloud messages very easily. "], "url": "https://answers.ros.org/question/310249/toolbox-for-segment-3d-pointclouds-from-lidar/"},
{"title": "How amcl works with only lidar?", "time": "2018-12-24 05:58:56 -0600", "post_content": [" ", " ", " ", " ", "Hello ,", "I successfully create a map with hector_slam \u0131 used only Scance Sweep Lidar.", "Now I am trying to locate my robot in that room with using only lidar . ", "rosrun map_server map_server roslaunch\n  sweep_ros sweep2scan.launch ", "\n  ./room.yaml rosrun amcl amcl", "[ INFO] [1545651873.310212817]:\n  Requesting the map... [ INFO]\n  [1545651873.328642250]: Received a\n  2048 X 2048 map @ 0.050 m/pix", "[ INFO] [1545651873.525857023]:\n  Initializing likelihood field model;\n  this can take some time on large\n  maps... [ INFO]\n  [1545651873.723486742]: Done\n  initializing likelihood field model. [\n  WARN] [1545651888.878408658]: No laser\n  scan received (and thus no pose\n  updates have been published) for\n  1545651888.878305 seconds.  Verify that data is being published on the\n  /scan topic. [ WARN]\n  [1545651903.878392812]: No laser scan\n  received (and thus no pose updates\n  have been published) for\n  1545651903.878312 seconds.  Verify that data is being published on the\n  /scan topic. [ WARN]\n  [1545651918.878526048]: No laser scan\n  received (and thus no pose updates\n  have been published) for\n  1545651918.878456 seconds.  Verify that data is being published on the\n  /scan topic. [ WARN]\n  [1545651933.878627662]: No laser scan\n  received (and thus no pose updates\n  have been published) for\n  1545651933.878550 seconds.  Verify that data is being published on the\n  /scan topic. [ WARN]\n  [1545651935.387829538]: MessageFilter\n  [target=odom ]: Dropped 100.00% of\n  messages so far. Please turn the\n  [ros.amcl.message_notifier] rosconsole\n  logger to DEBUG for more information.", "when i check /scan with rostopic echo /scan i got these results", "header: \n  seq: 4245\n  stamp: \n    secs: 1545652436\n    nsecs: 876566097\n  frame_id: \"laser_frame\"\nangle_min: -3.1400001049\nangle_max: 3.1400001049\nangle_increment: 0.0010000000475\ntime_increment: 0.0\nscan_time: 0.10000000149\nrange_min: 0.20000000298\nrange_max: 40.0\nranges: [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, , inf, inf, inf, inf, inf, inf, inf,  inf, inf, inf, inf, inf, inf, 5.730000019073486, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, , inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.8699999451637268, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,  inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.3400000035762787, inf, inf, inf, inf, inf, inf, inf ..."], "answer": [" ", " ", "I think that it's about the frame_id in your /scan topic and your TF setup in amcl , please verify the laser frame setup in amcl or set the laser frame which is coming out from your scanse lidar node ", "Thank you, \u0131 have changed lidar frame id to base_laser and it stops complain about it. And also rather than running rosrun amcl amcl , when i launch \"amcl_diff/omni.launch\" it gave better results then amcl but still not good localization enough.", ":) glad to here it worked. \n Would you mind choosing this post as an answer (the green checkbox on the left) ?"], "url": "https://answers.ros.org/question/311452/how-amcl-works-with-only-lidar/"},
{"title": "Why is the laser scan used my most robotic even when the point cloud is present", "time": "2018-11-23 06:56:46 -0600", "post_content": [" ", " ", "Hi,", "I have been seeing that all most all the robots use the laser scan for the navigation instead of the point cloud data, is there any specific reason for the same.  ", "The navigation package does support the sensor_msgs/LaserScan and sensor_msgs/PointCloud as the sensor source but always laser scan is used.  "], "answer": [" ", " ", "The short answer is that in the case of robots moving in a 2D plane which the navigation package is designed for a 2D laser scan is sufficient in most cases. ", "SLAM in 3D is far more complex than SLAM in 2D, so if it's a safe assumption that the robot remains in a 2D plane then this is the best choice. There is no need spending orders of magnitude more processing power to localise in 3D if that's not a task you really need to do.", "However if you're moving in 3D space with 6 degrees of freedom, then you'll have to use the 3D information with a package such as RGB-D-SLAM or R-TAB-MAP, but this is not what the navigation stack is designed for."], "url": "https://answers.ros.org/question/309208/why-is-the-laser-scan-used-my-most-robotic-even-when-the-point-cloud-is-present/"},
{"title": "What is the status of rdmanifests?", "time": "2017-02-21 10:09:05 -0600", "post_content": [" ", " ", " ", " ", "The ", " package ", " instructs users to add a custom source to their local ", " (", ") containing two ", " elements that point to a single ", " file: ", ". That ", " then is used to make ", " satisfy the ", " dependency (when used with ", ").", "I can still remember the time when the main ", " database included snippets of ", " (or even things coming close to full scripts), package-local yaml files with similar lines and ", " files being distributed with packages. I also remember a major cleanup of ", " to remove all that (because of running arbitrary snippets of shell script with super-user permissions, ", " silently changing a system's configuration (by adding ", "s or other ", " repositories fi), and because it was just generally not maintainable nor scalable).", "Seeing a recently created package like ", " use something like ", "s makes me wonder what the official status is of those particular pieces of the ", " infrastructure, and whether current users / packagers / maintainers should be discouraged from relying on them?", "Afaik the buildfarm will also not build packages that rely on this (deprecated?) functionality, so perhaps that is deterrent enough already, but an authoritative answer would be nice to have.", "Note: I did not choose ", " for any reason other than that it is a good example of a current use of ", " files.", " is an example of a question about ", " that was posted as recent as 2017-01-04."], "answer": [" ", " ", " ", " ", " rdmanifests are defined here:  ", "They are a tool that works for developers who are installing from source on more exotic platforms. But it's an important tool for users not on our main supported platforms. ", "They are not recommended for significant distribution and sharing. And cannot be used in a released package. ", "We don't expect to put a lot of energy into adding features. But because there are use cases where it is valuable and there's no viable alternative, we have not chosen to deprecate it.", "My recommendation is that rdmanifests should only be used as a last resort if there are no other options.", "Thanks. Personally I don't like shell scripts run with super user access installing all sorts of things circumventing the platform's pkg manager, but I can understand why in some cases there might be no other way to do deployment."], "question_code": ["rosdep", "15-pylon_camera.list", "source::uri", "rdmanifest", "rdmanifest", "rosdep", "libpylon(-dev)", "rosdep install ..", "rosdep", "bash", "rdmanifest", "rosdep", "rosdep", "ppa", "apt", "pylon_camera", "rdmanifest", "rosdep", "pylon_camera", "rdmanifest", "rdmanifests"], "url": "https://answers.ros.org/question/255238/what-is-the-status-of-rdmanifests/"},
{"title": "How to use ROS to control a roboclaw motor.", "time": "2018-11-23 09:48:12 -0600", "post_content": [" ", " ", " Hi guys, I've been googling for ages and would really appreciate help on this, I am an absolute beginner trying to get ROS to control roboclaw which is in turn controlling maxon motors for a robot. I am running ROS Kinetic and I have a Arduino mega and a Roboclaw motor. Looking online it seems like the majority of people are using this library  ", " . I followed the instructions and installed it, unfortunately couldn't find any tutorials, but my question is how do I use it? Do I hook ROS up to an Arduino and then connect that to the motors etc? "], "answer": [" ", " ", "The github page has the instructions for how to use it, but if you have no knowledge of ROS you would not even be able to recognize what parts are the instruction. You need to get familiar with ROS.", "I suggest you follow the ROS getting-started tutorials and but build up the simulated turtle bot and get it running. It will only take a few hours and make your life much easier. ", "After using ROS and getting familiar, go back and look at the github page again. The instructions will then jump out at you.", "Now I will answer your direct question.", "1 - connect the roboclaw to your PC with a USB cable and power the roboclaw.", "2 - Install the driver exactly as shown on github", "3 - Start roscore", "4 - Start the driver", "5 - feed test command to cmd_vel topic - if you've gone through the ROS tutorials, you already know how to do this."], "url": "https://answers.ros.org/question/309213/how-to-use-ros-to-control-a-roboclaw-motor/"},
{"title": "Checking for GPS waypoint being passed?", "time": "2019-01-14 01:21:39 -0600", "post_content": [" ", " ", "I am working on an autonomous vehicle system that navigates and avoids obstacles.", "Navigation alone, it goes from waypoint to waypoint by hitting the circular threshold points, basically saying \"when within this distance, increment the target waypoint.", "Now, when there is an obstacle and the threshold point is \"inside\" the obstacle, 99.9% of the times the threshold point won't be hit as the is threshold is set to very low due to the system using RTK. I am trying to use two intersecting lines (lat and long) to check whether or not the current waypoint has passed the next one, however that doesn't seem to work very well and I can't wrap my head around how to do it. Here's my code ", ":", "}", "Here's two scenarios, A and B, assuming we are operating in Northern and Western hemispheres (sorry for lousy drawing):", "In A, if the system decides to take the left path, it will not be able to to determine whether or not the next waypoint has been passed due to it thinking it hasn't intersected along the longitude (x) axis.", "In A, if the system decides to take the upper path, it will not be able to to determine whether or not the next waypoint has been passed due to it thinking it hasn't intersected along the latitude (y) axis.", "Is there a different way of doing it? If not, how could I account for both paths passing the next waypoint?"], "answer": [" ", " ", "Instead of trying to solve this problem in terms of X and Y (Lat and Long) You want to use some more relevant geometry. Referring to your diagram above, if you define an infinite line through the next two green waypoints. Then you can calculate the position of the closest point on that line to your current position. If this position is before the first green waypoint then it hasn't been passed, however if that closest point is beyond the first green waypoint then the point has been passed and you can move to the next one. The maths for this is surprisingly simple using vectors:", "Define three vectors:", "Calculate the unit direction (D) between W2 and W1", "then distance in front or behind W1 (Dist)", "This will avoid the orientation problems you described. However this may mean that you could potentially drift a long way from the waypoints if there are a lot of obstacles. \nHope this helps though.", "Thank you so much, really appreciated. That does make sense, I am gonna try to see if I can implement that!", "No problem."], "answer_details": ["P the current location", "W1 waypoint 1", "W2 waypoint 2", "D = (W2 - W1) / |W2 - W1|", "Dist = (P - W1).dot(D)", " ", " ", " ", " "], "question_code": ["void trackWaypoint(double heading, double current_coords[NUMBER_OF_WAYPOINTS][2], double next_coords[NUMBER_OF_WAYPOINTS][2], uint32_t &wp) { // This is only for N and W hemispheres\n\nif (wp != (NUMBER_OF_WAYPOINTS - 1)) {\n\n    if ((heading >= 0) and (heading < 90)) {\n\n        if (((next_coords[wp + 1][0] - current_coords[wp][0]) < 0) and ((next_coords[wp + 1][1] - current_coords[wp][1]) > 0)) {\n            wp++;\n        }\n    }\n    else if ((heading >= 90) and (heading < 180)) {\n\n        if (((next_coords[wp + 1][0] - current_coords[wp][0]) > 0) and ((next_coords[wp + 1][1] - current_coords[wp][1]) > 0)) {\n            wp++;\n        }\n    }\n    else if ((heading >= 180) and (heading < 270)) {\n\n        if (((next_coords[wp + 1][0] - current_coords[wp][0]) > 0) and ((next_coords[wp + 1][1] - current_coords[wp][1]) < 0)) {\n            wp++;\n        }\n    }\n    else { // [270, 360)\n\n        if (((next_coords[wp + 1][0] - current_coords[wp][0]) < 0) and ((next_coords[wp + 1][1] - current_coords[wp][1]) < 0)) {\n            wp++;\n        }\n    }\n}\n"], "url": "https://answers.ros.org/question/312652/checking-for-gps-waypoint-being-passed/"},
{"title": "Hello community need insight about indoor navigation.", "time": "2019-02-02 08:43:01 -0600", "post_content": [" ", " ", "I am working on my final year project of which autonomous indoor navigation of a robot is apart of  as of I have no clue how to accomplish this task.\n[Project specifics : Based on a trigger  the robot must autonomously navigate to one room of total 3 rooms ] ", "\nI heard that ROS can be used to navigate autonomously and I am thinking to purchace a course on udemy (ROS for Beginners: Basics, Motion, and OpenCV) to learn about ROS .\nWhat I want to ask is that will the knowledge of ROS  help me solve my problem of autonomous indoor navigation or should I be looking some where else ?\nI am sorry for asking such a vague question but my knowledge about robotics in general is limited and I am trying to improve it . ", "Thanks for looking  into my question.   "], "answer": [" ", " ", "Yes. ROS can assist with autonomous indoor navigation. A class is a good way to get started. ROS is super powerful and flexible, so of course is complicated and requires lots of time and patience."], "url": "https://answers.ros.org/question/314556/hello-community-need-insight-about-indoor-navigation/"},
{"title": "What is the best way to monitor and Remote control the Robot from Tablet?", "time": "2019-02-07 13:02:40 -0600", "post_content": [" ", " ", "Hi,", "I'm trying to make some Tablet GUI which can monitor and also remote control my robot.", "\nMy robot is already connected to wifi and ROS messages are well communicated between the Ubuntu computer which is connected to the same wifi. Well communicated means, the remote computer can subscribe the message which the robot published, and the robot can subscribe the message which the remote computer published. The robot is publishing its position and camera-image. The remote computer is publishing velocity command.", "\nNow, I want to make a Tablet GUI which displays the robot position, camera-image, and some button which can control the robot's velocity.", "\nThe GUI doesn't need to be a dedicated application. Using a web browser should be fine.", "\nI have two tablets. One is Android OS and the other is Windows OS. Android could be better for me.", "What is the best way (Easy way) to make a Tablet GUI? Are there any tutorials for this situation?", "Thank you in advance."], "answer": [" ", " ", " ", " ", " ROSBRIDGE and  ", "  will have everything you need. Both have tutorials to make it easy.\nI have remote interface that runs fine in safari on iPhone, chrome on android and windows, and firefox on ubuntu. In my case tested from one state to another(1500 Km) using controls and streaming video with minimal lag. ", "UPDATE after additional question:", "My setup is personal hobby, so I just use an old laptop for the server running apache2 on ubuntu. It works fine for my limited case. ", "Nodes running on the laptop:", "Full navigation stack and robot interface. AMCL, movebase, map server etc all standard. Robot interface is custom.", "A few custom house keeping nodes that use voice and keyboard inputs to control robot state(sleeping, idle, navigating, etc) and issue goals to navigation stack, decode cmd_vel, etc.", "Raspicam is running on a raspberry pi with camera to get video stream. I strongly suggest you find a different source than raspberry pi for video. Major pain to get working.", "Nodes used to support the web interface:", "web_video_server", "robot_pose_publisher", "image_transport republish compressed in:=/raspicam_node/image/ out:=/convertedimage", "rosbridge_server rosbridge_websocket.launch", "The HTML code below is one of the pages that displays the video stream, shows the location of the robot in the map, and has buttons for sending robot to predefined locations. The output from these buttons get published to a topic that is used for voice control local to the robot so the buttons get handled as though they are voice commands being issued to the robot from someone standing right there. The robot will not obey any voice command not preceeded by \"robot\" so the function for the buttons puts a \"robot\" on the topic prior to sending the intended location request. These buttons will not help you with your application but serve as one example on how to put info to topic from web interface.", " One note: the file \"Ros2D.js\" from the  ", "  site needs to be in the folder with your index.html file. I'm sure I am doing this is a non-standard way but have no need to fix it. ", "I am doing this using ROS Jade on Ubuntu 14.04.", "Thank you so much for your answer. Your job sounds wonderful! If you don't mind, could you tell me more detail? For example, what library and tools did you use for server?\nThanks.", "Thank you for your wonderful additional advice!", "Would you have any advice on how to achieve this if my remote station is on a different network than my robot.", "Hi @billi\nplease describe more about your /recognizer/output topic. which package is running this node. please mention git repo link for the same.", "That topic is published by pocketsphinx : ", "\nIt is the results of speech recognition.  I use voice control as an input to a state-machine node I use to control the robot system. It is custom (no github) that controls robot hardware, monitors battery, issues goals to move-base, etc.  Since the system uses that topic as the primary input for the state-machine, it made sense the webpage would issue to that same topic. And then the webpage was written so that it plays well with the expected state-machine input."], "answer_code": ["<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\" />\n\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<script src=\"https://static.robotwebtools.org/EaselJS/current/easeljs.js\"></script>\n<script type=\"text/javascript\" src=\"http://static.robotwebtools.org/EventEmitter2/current/eventemitter2.min.js\"></script>\n<script type=\"text/javascript\" src=\"http://static.robotwebtools.org/roslibjs/current/roslib.min.js\"></script>\n<script src=\"https://static.robotwebtools.org/roslibjs/current/roslib.js\"></script>\n<script src=\"Ros2D.js\"></script>\n\n<script type=\"text/javascript\">\n  var pi_img = new Image(); \n  var ip = location.host;\n</script>\n\n<script type=\"text/javascript\" type=\"text/javascript\">\n  var ip = String(location.host);\n  var x = 0;\n  var z = 0;\n  var scale = 0;\n  var scale_max = 0.2;\n  var out_message = \"\";\n\n  // Connecting to ROS\n  // -----------------\n  var newURL = \"ws://\" + ip + \":9090\";\n  var ros = new ROSLIB.Ros({\n    url : newURL\n  });\n\n  ros.on('connection', function() {\n    console.log('Connected ..."], "url": "https://answers.ros.org/question/315015/what-is-the-best-way-to-monitor-and-remote-control-the-robot-from-tablet/"},
{"title": "Dynamixel X with OpenCM 9.04C and ROS possible or OpenCR/U2D2 need?", "time": "2019-01-27 09:53:07 -0600", "post_content": [" ", " ", " ", " ", "Dear all,", "I plan to build a robot arm with Dynamixel X servos. I also want to use ROS tools and drivers for the servos. I found there is a cheap OpenCM 9.04C Controller board (~20USD) available, also the U2D2 USB Interface (~50USD) and then the OpenCR Board (~200USD). In the ROS packages I only found the OpenCR mentioned. My plan is that ROS runs on the PC and I just need the interface from there to Dynamixel X. My question is: Can I control the Dynamixel X (T version with 3 cables) on ROS through the OpenCM 9.04 board? And can you name me the ROS packages I need for this? Or do I need to buy one of the 2 other boards (U2D2 or OpenCR)? ", "Thank you for any help, I'm new to Dynamixel ;-)"], "answer": [" ", " ", "Thank you for your inquiry about Dynamixels.", " To control Dynamixels on your PC, you are supposed to need U2D2 ( ", " ) and U2D2 power hub ( ", " ) ", "The releated packages are showing ", "DynamixelSDK and Dynamixel-Workbench", " ", "Hi, good to see this post. I have the similar situation. I have RPi3+openCM904 and i am willing to use ROS with this configuration. Can I go with configuring ROS on \"RPi3+openCM9.04+Dynamixel\"? If YES, please help me to setup the supported library and development. Please guide me accordingly~ Thanks! Best Regards, /Ash", " ", " ", " ", " ", " It works with U2D2 as mentioned above.\nHowever it could probably work with any UART instead of U2D2. E.g. any USB to Serial/UART adapter which start at round 10EUR. Also most \u00b5Cs already have this included. If you use Raspberry Pi, you could probably directly use the pins on the GPIO without buying any additional HW. Just connect GPIO14 and GPIO15 with a 2k resistor and connect GPIO15 as Dynamixel data line. Note that you need to activate the UART in SW/Operating System of raspberry Pi... I plan to try it but did not find time yet... There is a picture here:\n ", " The Dynamixel SDK / Workbench directly control the Dynamixel via UART. This can be U2D2 or any other UART. Details can be found at Robotis documentation:  "], "url": "https://answers.ros.org/question/313901/dynamixel-x-with-opencm-904c-and-ros-possible-or-opencru2d2-need/"},
{"title": "Fail: ABORTED: No motion plan found. No execution attempted", "time": "2016-07-30 02:02:48 -0600", "post_content": [" ", " ", "I have used moveit api but with function \"group.setPoseTarget(pose1)\" it give \" Fail: ABORTED: No motion plan found. No execution attempted\" and at terminal of rviz it give", "can any one tell me how to solve this problem?", "Have you solved it?", "Do you solve this problem? I have the same error, but I don't know how to solve it"], "answer": [" ", " ", " ", " ", "This can have several causes.", "Hi, I have a question that before I set the pose goal, how can I know whether the pgose goal is reachable? Is there any funtion in moveit to solve this problem?", "Is there a Python functionality for that?", " ", " ", "I was having the same problem, so thanks for the help. What tick it on to work was to wirte: ", "When i run the robot planning program theres only one thing bodering me which is this:", "I dont understand why i dont have a planner if the GUI tells me that it is OMPL and shows green. Is this  problem too?", "\"No planner specified\" means you did not explicitly specify ", " you wanted to use.\nThe default one is selected in this case."], "answer_details": [" might simply be unreachable by the arm -> use a pose of which you know that it is reachable", " is reachable but the inverse kinematics plugin failed to find a joint configuration that results in the end-effector being near that pose -> use a different IK or different parameters (trac_ik or a specialized ikfast perform good)", "The planner failed to find a path to the goal -> is there a path by which you can move from the current configuration to the goal configuration (within the specified software limits)", "if there are paths the planner should be able to find, you might want to try a different planner -> e.g. ", "the planner might find the path with more time available -> ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["[ INFO] [1469775045.492605776]: LBKPIECE1: Created 1 (1 start + 0 goal) states in 1 cells (1 start (1 on boundary) + 0 goal (0 on boundary))\n"], "answer_code": ["pose1", "pose1", "group.setPlannerId(\"RRTConnectkConfigDefault\")", "group.setPlanningTime(<insert-time-limit-here>)", "group.setPlanningTime(10);", "[ INFO] [1486575608.470163274]: No optimization objective specified, defaulting to PathLengthOptimizationObjective\n[ INFO] [1486575608.470393526]: No planner specified. Using default.\n"], "url": "https://answers.ros.org/question/240723/fail-aborted-no-motion-plan-found-no-execution-attempted/"},
{"title": "Can ROS kinetic and Melodic be mixed on the same network?", "time": "2019-02-13 15:34:41 -0600", "post_content": [" ", " ", "I have several computers, one is running Ubuntu 16.04 and Kinetic and another one 18.04 and Melodic.   I'd like to run ROS core on the Kinetic system and have some nodes running on the Melodic system.  Does this work?", "Yes, I know and and just try it and see but that is a poor test as I'd only learn about the exact setups I tried, I'm looking for a more general answer", "What sort of general answer are you looking for?\nAt the highest level, the same packages may not even be released for both ROS versions, so in that sense they're incompatible.", ": this exact topic has been discussed on this site many times, and seeing as you write:", "Yes, I know and and just try it [..]", "I have a feeling that you already saw those discussions.", "If something still wasn't clear, can you please update your question with what specifically ..", ".. you're still unsure about? \"some nodes\" is rather vague.", "To summarise all previous discussions: yes, it ", " work, provided that the message, service and action definitions that are used on the topics, services and actions haven't changed between two ROS versions.", "I have a mobile robot with limited size/power so I place some of the ROS nodes on workstation class machines and connect with WiFi.  Question is if ROS version need to be kept in sync.   Testing shows it works, but is this a fluke or is backwards compatibility a ROS design goal?"], "answer": [" ", " ", " ", " ", "I have a mobile robot with limited size/power so I place some of the ROS nodes on workstation class machines and connect with WiFi. Question is if ROS version need to be kept in sync. Testing shows it works, [..]", "tl;dr: I believe this should be addressed by the previous Q&As I referred to earlier, but to make it specific to your situation: no, you don't necessarily need to use the same ROS version on all involved hosts. As long as the messages you exchange haven't changed, things should work.", "However: there are some things to watch out for that go beyond syntax compatibility (ie: data format).", "Longer: ROS nodes make use of messages to communicate (services and actions are also msgs) and the most basic guarantee all participants must have for that to work is that they all use the same message ", " (that is: the syntax should be what they expect, otherwise they won't be able to decode incoming data).", "As long as the structure of messages hasn't changed (and that includes both names and types of fields) communicating with nodes \"from\" different ROS versions should work, or at least not result in (de)serialisation errors. ROS (1) will actually stop you from even getting that far, as a hash sum (over all fields and types) is used as a simple check to see whether there are any syntax/structure discrepancies between subscriber and publisher, and if there are the well known error \"Client wants topic X to have Y, but our version has Z. Dropping connection\" will be printed and nothing will be communicated.", "The corollary here is that if there is no such incompatibility, a connection is established and msgs are published and subscribed.", "is this a fluke or is backwards compatibility a ROS design goal?", "I hope this is now clear from the previous part of my answer: as long as the messages used have the same structure, communication can take place. Many messages in the standard sets of messages (", ", ", ", ", ", etc, etc) haven't seen any changes for a couple of ROS versions already. Chances are high that a network with ROS Indigo, Kinetic and Melodic would be able to exchange messages just fine.", "It all depends on whether message (or service, or action) definitions have changed.", "I'm not sure we can conclude that the bw-compatibility we achieve with this was a deliberate design goal or is a fortunate side effect of the basic publisher-subscriber compatibility check that is performed, but I would certainly not call it a \"fluke\" (perhaps ", " or ", " can say something about that, I wasn't there when all of this was designed/implemented).", "Finally: I explicitly emphasised message ", " here, as there is one characteristic of message based communication that is not captured by hash sums over message fields and that is semantics (or meaning).", "It could be that the way a certain field should be interpreted (ie: the ..."], "answer_code": ["std_msgs", "geometry_msgs", "trajectory_msgs"], "url": "https://answers.ros.org/question/315600/can-ros-kinetic-and-melodic-be-mixed-on-the-same-network/"},
{"title": "SCB Recommendation?", "time": "2019-02-18 14:54:39 -0600", "post_content": [" ", " ", "I am looking to start a project where I'll be using SLAM with a Lidar. I want my robot to be able to run SLAM on its own (on board) without the help of another computer. I was told that the RPI3B+ is not going to do the job.", "Which board ($150 is my limit) is powerful enough as well as well supported by the community would you guys recommend? Or whats are the recommend specs to run SLAM with no problem on a SCB?", "Similiarities with the raspberry pi would help (the display and camera ports)"], "answer": [" ", " ", "This recommendation doesn't come from experience using it for ROS, but the beaglebone black should do the trick as long as: A) you're not processing depth cameras B) your application level code on top of the navigation stack is relatively light C) your SLAM problem is small", "I'm actually in the middle of a project doing essentially what you're doing for an exercise in embedded robotics computing, that's where I'm starting. There's other single board computers you can buy >$100 which can run full-out Ubuntu on them and those will probably be OK", " ", " ", "I have run a rpi3 powered robot using a kinect or a ydlidar with navigation and they works OK add a usb camera and the RPI pegs out at %100 and becomes very slow navigation also fails. My new robot runs 3 RPI's and navigation runs on a desktop computer. I can run navigation on 1 of the RPI's  but its not as smooth and on the desktop. I  run a mix of Ydlidar, Kinect  RPI camera and 5 Arduino's (hence the 3 RPI's)", " ", " ", "I'm running navigation and camera on Rhoeby Cerebrum ($99, based on Raspberry Pi)."], "url": "https://answers.ros.org/question/315987/scb-recommendation/"},
{"title": "TurtleBot Bringup Not Connecting to Port", "time": "2018-03-30 10:52:09 -0600", "post_content": [" ", " ", "I've been using the TurtleBot3 Waffle with Intel Joule processor for about a month now, and everything has been working very smoothly up until yesterday. Yesterday I followed the same process I usually use to set the TurtleBot up to communicate with a Remote PC:", "[Remote PC]", "Run ", "Run ", "[TurtleBot]", "Power up the robot", "Run ", "After I run these steps, I usually am able to publish/subscribe to the various ROS topics the robot publishes/subscribes to. Yesterday I ran this setup and received this console output after launching the turtlebot3_robot.launch on the TurtleBot:", "\nAnd that's as far as it gets. It's getting hung up on connecting the OpenCR port. The only similar scenario to mine that I've seen asked about on here is at the following link: ", ". I've followed all the instructions in the answers provided with no successful outcome.", "Strangely enough, I let the robot charge overnight, and when I powered it on again this morning, it worked fine for the first 3 or 4 times I ran the boot sequence above. Then I tried to start again after 3-4 successes, and I ran into the same issue. It gets stuck on the connecting step.", "I am able to connect to the OpenCR board and flash the turtlebot3_core firmware from the Arduino program, so I know I have connectivity to the board. It's just the turtlebot3_robot.launch sequence that seems to be having trouble with connecting. I'm planning to re-flash the BIOS for the Intel Joule processor being used alongside the OpenCR board. I have tried powering on and off, unplugging and plugging back in every port possible to try and diagnose the issue, but I haven't had any success.", "If more information is needed, I would be happy to provide it. Thanks for the help.", "Hello, \nI have the same problem, do you find a solution?\nThanks.", "Turns out it was a power issue. The battery was too low to power the OpenCR board, so make sure your battery is charged."], "answer": [" ", " ", "Just to reiterate the solution: ", "Turns out it was a power issue. The battery was too low to power the OpenCR board, so make sure your battery is charged."], "question_code": ["roscore", "roslaunch turtlebot3_bringup turtlebot3_remote.launch", "roslaunch turtlebot3_bringup turtlebot3_robot.launch", "Checking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://[hostname]:45747/\n\nSUMMARY\n========\n\nPARAMETERS\n * /rosdistro: kinetic\n * /rosversion: 1.12.13\n * /turtlebot3_core/baud: 115200\n * /turtlebot3_core/port: /dev/ttyACM0\n * /turtlebot3_lds/frame_id: base_scan\n * /turtlebot3_lds/port: /dev/ttyUSB0\n\nNODES\n  /\n    turtlebot3_core (rosserial_python/serial_node.py)\n    turtlebot3_diagnostics (turtlebot3_bringup/turtlebot3_diagnostics)\n    turtlebot3_lds (hls_lfcd_lds_driver/hlds_laser_publisher)\n\nROS_MASTER_URI=http://[master_uri]:11311\n\nprocess[turtlebot3_core-1]: started with pid [2603]\nprocess[turtlebot3_lds-2]: started with pid [2604]\nprocess[turtlebot3_diagnostics-3]: started with pid [2605]\n[INFO] [1522418950.044044, 0.000000]: ROS Serial Python Node\n[INFO] [1522418950.263477, 0.000000]: Connecting to /dev/ttyACM0 at 115200 baud\n"], "url": "https://answers.ros.org/question/287099/turtlebot-bringup-not-connecting-to-port/"},
{"title": "How can I communicate between my ROS-pc and a plc from SEW? And how do I install it on my pc? The plc that I use is written in CODESYS", "time": "2019-03-11 10:56:34 -0600", "post_content": [" ", " ", "I'm using ROS melodic on my pc and want to sent commands to 2 wheels. The wheels are connected to a PLC from SEW. "], "answer": [" ", " ", " ", " ", "I don't know what the status of it is right now, but at the ", " in Stuttgart, INESCTEC presented about \"IEC 61131-3 and ROS compatibility\".", "Presentation: ", " (", ").", "You could perhaps send a message to the ROS-Industrial ", " to inquire about whether this is something that can be / is distributed, or at least accessible.", "Edit: it was also the topic of a MSc thesis: ", "Final edit: you may be interested in the ", " FTP in the ROSIN project:", "ROBIN aims to address the demand for flexible robotics in industrial environments and the necessity to integrate robots and automation equipment in an efficient manner. For this, the ROBIN project will focus on developing and releasing a bidirectional, reliable and structured communication bridge between ROS and CODESYS, a softPLC that can run on embedded devices and that supports a variety of fieldbuses, and even OPC-UA. The developed software will allow the parametrization of ROS modules through IEC61131-3 programming languages and also streamline the interoperability between ROS and robotic hardware or automation equipment, fully empowering the Industry4.0 paradigm of Plug\u2019n\u2019Produce.", "Results have not yet been made available, but this may offer a way to integrate with CODESYS systems in the near future.", "I have seen these slides but didn't find it very useful. It doesn't show me what I have to write in ubuntu to achieve this shared memory", "Hence my suggestion to send a message to the Discourse category to see whether the organiser of the conference could put you in touch with the presenter ..", "Also:", "I have seen these slides but didn't find it very useful.", "For future questions: could you please always mention what you've already found yourself or tried yourself? It'll save people trying to answer your question time & effort suggesting things you've already seen."], "url": "https://answers.ros.org/question/318266/how-can-i-communicate-between-my-ros-pc-and-a-plc-from-sew-and-how-do-i-install-it-on-my-pc-the-plc-that-i-use-is-written-in-codesys/"},
{"title": "what are benefits of using ROS-I and motoman_driver?", "time": "2019-04-17 08:08:47 -0600", "post_content": [" ", " ", "Hello,", "I want to develop vision based pick and place. let say I have camera on robot end effector, I am taking image of object and esimating it pose to give it to robotic arm using camera.  ", "As, I have motoman robot I can use motoman_driver to communicate with real robot.", "But this can also be done without using motoman_driver. for application like this if I use ROS-i,  can it seen as here \u00b4not required\u00b4? as this can be done also without using it.", "how can I use motoman_driver to give only POSE of object to robot controller(as we can do in traditional vision system with INFORM programming job) without Moveit motion planning? as motoman robot has its own controller to do all the thing.", "what exactly use of motoROS application?", "may be I am confused here.\nThanks."], "answer": [" ", " ", " ", " ", "I want to develop vision based pick and place. let say I have camera on robot end effector, I am taking image of object and esimating it pose to give it to robotic arm using camera. [..]\n  But this can also be done without using motoman_driver. for application like this if I use ROS-i, can it seen as here \u00b4not required\u00b4? as this can be done also without using it.", "using something like ROS (or more generically: a off-controller motion controller) can be beneficial if you wish to use something like sensor based motion planning.", "Yes, you can send a Cartesian position to the controller and parameterise an INFORM job with it as you describe. And in that case the controller will execute the motion hard-coded in the job, but with some changes based on the position you received from the camera. You don't need MotoROS for such a setup.", "If that is all you require, then using MotoROS is probably not going to be more efficient or interesting.", "When it ", " get interesting is when (some examples): ", "and there are many more situations in which external control of a robot can be beneficial.", "As good as (industrial) robot controllers have become and are at ", " (specifically: the robots of the OEM), robotic applications are typically much more than just a (single) robot. Such applications therefor typically also require more than just the controller of the OEM.", "Robot controllers are getting more and more complex, but I haven't found one that can process point clouds or do dynamic motion planning.", "A good example of a complex application that benefited from the external control that MotoROS made possible is the ", " (from ", "):", "That's a pretty complex application that is using on-the-fly motion planning (no CAD models, so no off-line CAD-to-path) with collision avoidance, in-process Q&A and integrating many different sensors and actuators from many different OEMs. The plane (ie: \"workpiece\") can be anywhere, the robot can be anywhere, surface conditions ...", "Note: you ", " actually send a trajectory that you have computed yourself (so without using MoveIt) to ", ". Just use the ", " action server for that.", "MoveIt is not needed at all.", "This would even work with single-point trajectories (actually: two-point trajectories (start, destination)). It would be joint interpolation though. But it is supported.", "Note also: I'm not saying a pick-and-place application cannot be complex. What I'm saying is that depending on where the complexity comes from, the resources that an OEM controller has may not be sufficient to control more than just the robot itself. YRC1000 controllers are pretty OK machines, but you cannot run arbitrary programs on them, and even if you could, they would be seriously limited in CPU processing power and memory available to them.", "To escape those limitations, external control makes sense.", " Thank you for the great answer. As I want to use motoman_driver only for communication between PC and controller. because I found that using Motocom32 I can give related object pick,place position but it is not \"free\". so using motoman_driver can I give pick, place position using vision directly? but I do not understand \"send a trajectory that you have computed yourself \"? is it not done by robot controller?", "No, you cannot just send positions. It's always a trajectory.", "MotoCOM allows you to set registers, that is not what MotoROS is for.", "If you have the option, you may want to take a look at the highspeed ethernet server.", "Edit: if you have MotoPlus ", " are comfortable programming in C, you can definitely ", " getting and setting registers to MotoROS. See ", " for some inspiration.", "oh, I understand wrong. I thought i will use motoman_driver, then sending POSE information to controller, Ik service and /follow_joint_trajectory I can do pick and place. so, I think then it is good to have Motocom32 as my programming experience is intermediate.", " how can I calculate trajectory for given POSE if I want to use motoman_driver? I think then I have to use Moveit?", "oh, I understand wrong. I thought i will use motoman_driver, then sending POSE information to controller, Ik service and /follow_joint_trajectory I can do pick and place", "Things are not as black-and-white as you make them to be.", ", you cannot use MotoROS to send single poses (to registers) and use INFORM. ", " you can actually create a pick-and-place application with MotoROS.", "But it all depends on where your focus is, and how much time you are willing to invest (and of course whether that time investment makes sense depends on what else you'd like to do).", " how can I calculate trajectory for given POSE if I want to use motoman_driver? I think then I have to use Moveit?", "a trajectory is nothing more than a sequence of joint space poses and associated ", " values (and velocity, if you don't want to stop at each point).", "\"calculating\" is a strong word: you can just \"make up\" joint poses and append them to a ", ". Set the ", " field correctly, make sure to specify proper ", " values, wrap it in a ", " and submit it to the ", " action server."], "answer_details": ["you want your motion plans to take obstacles into account (the robot controller will not do that, in fact: it will try to move through obstacles)", "when those obstacles are dynamic (ie: they move while the robot is moving)", "when your sensor data is voluminous and heterogenous (there is a lot of sensor data and it's also not all from the same type of sensor)", "when you have a complex motion planning problem that is also changing while the robot is executing it (fi: optimisation of process parameters in an on-line fashion)", "when the robot is only part of a larger system (ie: a 34 dof system in which the 6 (or 7) of the robot are a subset) and planning of motions must take all dofs into account", "when no part of your application can be preprogrammed in an INFORM job (essentially ", " joint poses are unknown at design time, fi because all future motions will depend on in-situ gathered data)", " ", " ", " ", " "], "answer_code": ["motoman_driver", "/follow_joint_trajectory", "time_from_start", "joint_names", "time_from_start", "FollowJointTrajectoryGoal", "/follow_joint_trajectory"], "url": "https://answers.ros.org/question/321293/what-are-benefits-of-using-ros-i-and-motoman_driver/"},
{"title": "Robot development", "time": "2019-04-17 04:32:13 -0600", "post_content": [" ", " ", "Hi guys,", "I am currently developing my own robot project, the hardware aspect is settled so i am handling the software. I have already installed ros_control and ros_controller packages and i plan to move my robot with a joystick sending a msg to ros_control. I understand that i might also need a hardware interface module for this to work. So is there any documented similar project that i can refer to, it will be of great help thank you!"], "answer": [" ", " ", "I would recommend reading up on ", " first. It can be kind of tricky to understand:\n ", "For connecting ", " with your hardware, this guide is a decent start.", "I hope this will help, but you'll have to specify your problem more to get a less generic answer.", "Hi Isha,", "Thanks for the reply", "Because i am really new to ROS, i am not too sure myself what i am to look out for. My end goal for this project is to move a physical robot using a joystick. I guess the problem here is the hardware interfacing part where i need to write the code to interface with ROS control for the robot to get a certain message for it to be moving?", "If your robot doesn't ", " already, you'd have to write the hardware interface yourself. The guide I've linked walks you through it pretty well.", "For the rest it depends on what 'moving your robot' really means. Is it a single arm? A biped? A car?", "If you haven't done them already, I'd highly recommend doing all the ", ". All of them, it will save you have to ask a lot of basic questions in the future.", " When you have done that, look for packages that already exist and are related to your problem. Such as  ", "Hi Isha,", "Yes i have already the basic tutorials from the ROS link, my moving robot is a cart/trolley powered up by roboteq motor drivers. So right now i am trying to write the hardware interfacing part for this motor to be able to move the cart."], "answer_code": ["ros_control", "ros_control"], "url": "https://answers.ros.org/question/321276/robot-development/"},
{"title": "Kinect 360 not working in Ubuntu 16.04", "time": "2018-11-29 03:38:13 -0600", "post_content": [" ", " ", "I am using Kinetic in Ubuntu 16.04.\nI have a kinect xbox 360 and the appropriate power supply.\nThe kinect works fine in windows 10.\nIn Ubuntu, it will not show up in lsusb. I can see the motor, but the rest of the components do not show. The best I can see is that it is being detected as a usb hub. But at each lsusb it changes IDs. Here is a sample from lsusb:", "Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 002: ID 8087:07dc Intel Corp. \nBus 001 Device 007: ID 04f9:0331 Brother Industries, Ltd \nBus 001 Device 006: ID 0922:0020 Dymo-CoStar Corp. LabelWriter 450\nBus 001 Device 009: ID 15d9:0a33 Trust International B.V. Optical Mouse\nBus 001 Device 008: ID 04d9:1702 Holtek Semiconductor, Inc. Keyboard LKS02\nBus 001 Device 005: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 004: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 003: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 099: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nBus 001 Device 098: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "running lsusb again gives:\nBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 002: ID 8087:07dc Intel Corp. \nBus 001 Device 056: ID 04f9:0331 Brother Industries, Ltd \nBus 001 Device 055: ID 0922:0020 Dymo-CoStar Corp. LabelWriter 450\nBus 001 Device 059: ID 15d9:0a33 Trust International B.V. Optical Mouse\nBus 001 Device 057: ID 04d9:1702 Holtek Semiconductor, Inc. Keyboard LKS02\nBus 001 Device 054: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 052: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 050: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 001 Device 101: ID 045e:02b0 Microsoft Corp. Xbox NUI Motor\nBus 001 Device 100: ID 0409:005a NEC Corp. HighSpeed Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub", "It looks like it is being detected as an NEC hub, but after many days of this I am not sure of anything.", "Has this been encountered before? What can I do next?", "Thanks,\nDave "], "answer": [" ", " ", "Thanks Orhan. I discovered the real problem. Non-microsoft 12v interfaces for taking the power to the kinect and splitting off the USB.", "I ran some tests and it will only work if using a Microsoft USB interface adapter.\nThe Chinese one I had bought off Ebay will not work.", "Dave", " ", " ", "Here's setup instructions for 16.04. It includes udev rules for problems like yours and other troubleshooting options.", " "], "url": "https://answers.ros.org/question/309609/kinect-360-not-working-in-ubuntu-1604/"},
{"title": "2d navigation with Arduino + iRobot?", "time": "2019-06-04 16:28:03 -0600", "post_content": [" ", " ", "So I have used iRobot + a laser scanner for 2d navigation using amcl, gmapping and move_base node. However, when I want it to run I have to place my laptop on top of it so now I am wondering if it is possible to use arduino so it is more convenient. I have been using ros but I feel like I am still at a beginner level so anything is appreciated!", "An arduino is not powerful enough do what you need it to do, its a microcontroller not a computer. I think you may be able to get away with a new raspberry pi but I'm not totally certain about that. A modern i3 processor / nvidia board should be enough though.", "which one do you think will be the easiest/quickest to do? Since I am still a beginner.", "Definitely not the nvidia board", "\nThis showed navigation with raspberry pi 3. Is it possible to just use laser scanner + irobot + raspberry pi? or do I need more?", "I cannot tell you, you would have to try for yourself or ask someone that has tried"], "answer": [" ", " ", "Yes, it is possible to place the PC remotely. ", "Method 1 - Do all ROS processing on a Raspberry Pi on the robot - I do not know if Raspberry Pi has processing power to run full navigation stack. I doubt it. ", "Method 2 - Use Raspberry Pi with ROS to interface your sensors and motor driver and and put data into ROS topics - use WiFi to network Raspberry Pi with PC and run AMCL and move-base on PC - this seems likely to succeed if you can assure uninterrupted WiFi access.", "Method 3 - Do it the hard way - using a wireless serial port and Arduino - I have a small two wheel robot with remote PC. It uses a Bluetooth serial port for all communication. On board there is a uC(not arduino but close enough) that puts all sensor data into a single data stream and sends to PC which has a node to decode the data stream and send data to appropriate nodes for processing.  All robot commands from ROS go through same serial link and uC sends commands to appropriate device (laser scanner, motor driver, wav files).", "There are ROS nodes that can help support this. \n", "I haven't used ROSserial so not sure how difficult it is to use.", "In my case, for data transfer to PC, I started with Neato Laser scanner driver hacked (", "), ported it to a thread in LPC1769, and interleaved all other data into the data stream using the same packet format.  Data interleaved includes IMU data, wheel odom, bumper states, battery voltage, and laser scan data. It's absolutely workable, but it required a bunch of effort to do it.", "If I had to start over, I would use Method 2.  You'd learn a lot more doing method 3. Let me know if Method 1 works.", "Method 2 seems like the easiest, I will do that just to make sure that I can do it then I will try with other methods mentioned above. How would I get start with method 2? Btw, I am using the create_autonomy iRobot package + a hokuyo laser (urg_node)", "If you already have it running on a PC, you should have a pretty good idea how to do it already.", "Follow tutorials for ROS networking\nTutorials for ROS on Raspberry PI\nInstall ROS drivers for sensors on PI", " Which tutorial? Could you link it? Thank you!"], "url": "https://answers.ros.org/question/324688/2d-navigation-with-arduino-irobot/"},
{"title": "Schunk LWA4P initialization problem with ros_canopen", "time": "2019-06-13 05:00:58 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I try to launch a LW4P with ros_canopen. I use ROS kinetic and a PCAN-USB Pro adapter.\nI already tried out the suggestions on similar problems on ", " and on ", " but unfortunately my issue remains.", "First I setup my CAN interface", "Then I launch the robot", "Hello , did you try to start just the driver \"canopen_motor_node\"  and manualy spawn your controller ? with rqt if it's easyer.", "Is your candump complete ? I don't see node many node state in you candump but, as log indicates a controller interface not found probleme.", "I think it migh help if you send your configuration file *controller.yaml and canopen_chaine_node.yaml etc."], "answer": [" ", " ", "If I am  not mistaken, this is an under-voltage emergency alert (", ").\nPlease check your power supply.", "thank you! Yes apparently power supply was one issue. The EMCY message have disappeared now. However, the other error messages are remaining and I am still not able to launch the LWA4P. \nI encountered some strange things:\nIt seems that some of my CAN modules are not recognized at all. At the startup ", " only gives me the following \n", "\nIf I only launch the recognized nodes without the non-recognized nodes, I can move the recognized nodes as expected.", "But what could be the reason for not recognizing some of my modules?", "Thank you in advance!", " ", " ", "I just solved my problem. ", "I had some broken spring contacts inside the LWA4P, f.e. the ", " and ", ". That's why my CAN modules have not been recognized. I fixed it and finally it works like a charm.", "Did you notice some bus off on your can interface while you had your spring contatcs throuble ? \nGlade your solved your problem by the way."], "question_code": ["$ ~/catkin-pem-robot-v2# sudo ip link set can0 down\n$ ~/catkin-pem-robot-v2# sudo ip link set can0 up type can bitrate 500000\n$ ~/catkin-pem-robot-v2# sudo ip link set can0 up txqueuelen 20\n", "$ ~/catkin-pem-robot-v2$ roslaunch pem_robot pem_robot_v3.launch \nWARNING: Could not change permissions for folder [/home/verena/.ros/log/c8077c0a-8dbd-11e9-b028-ac7ba18b123a], make sure that the parent folder has correct permissions.\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nxacro: Traditional processing is deprecated. Switch to --inorder processing!\nTo check for compatibility of your document, use option --check-order.\nFor more infos, see http://wiki.ros.org/xacro#Processing_Order\nxacro.py is deprecated; please use xacro instead\nstarted roslaunch server http://verena-CELSIUS-H730:33227/\n\nSUMMARY\n========\n\nCLEAR PARAMETERS\n * /lh_arm/driver/\n\nPARAMETERS\n * /joint_state_mixer/robot_description: <?xml version=\"1....\n * /lh_arm/driver/bus/device: can0\n * /lh_arm/driver/defaults/dcf_overlay/604Csub1: 1\n * /lh_arm/driver/defaults/dcf_overlay/604Csub2: 24000\n * /lh_arm/driver/defaults/dcf_overlay/6060: 1\n * /lh_arm/driver/defaults/eds_file: config/Schunk_0_6...\n * /lh_arm/driver/defaults/eds_pkg: pem_arm_lh\n * /lh_arm/driver/defaults/vel_to_device: rint(rad2deg(vel)...\n * /lh_arm/driver/name: arm\n * /lh_arm/driver/nodes/lh_arm_0_joint/id: 10\n * /lh_arm/driver/nodes/lh_arm_1_joint/id: 5\n * /lh_arm/driver/nodes/lh_arm_2_joint/id: 6\n * /lh_arm/driver/nodes/lh_arm_3_joint/id: 3\n * /lh_arm/driver/nodes/lh_arm_4_joint/id: 4\n * /lh_arm/driver/nodes/lh_arm_5_joint/id: 7\n * /lh_arm/driver/nodes/lh_arm_6_joint/id: 8\n * /lh_arm/driver/sync/interval_ms: 50\n * /lh_arm/driver/sync/overflow: 0\n * /lh_arm/hardware_control_loop/loop_hz: 125\n * /lh_arm/hardware_interface/joints: ['lh_arm_0_joint'...\n * /lh_arm/joint_state_controller/publish_rate: 50\n * /lh_arm/joint_state_controller/type: joint_state_contr...\n * /lh_arm/pos_based_pos_traj_controller_arm_lh/joints: ['lh_arm_0_joint'...\n * /lh_arm/pos_based_pos_traj_controller_arm_lh/publish_rate: 50\n * /lh_arm/pos_based_pos_traj_controller_arm_lh/required_drive_mode: 7\n * /lh_arm/pos_based_pos_traj_controller_arm_lh/type: position_controll...\n * /lh_arm/robot_description: <?xml version=\"1....\n * /robot_description: <?xml version=\"1....\n * /rosdistro: kinetic\n * /rosversion: 1.12.14\n\nNODES\n  /lh_arm/\n    driver (canopen_motor_node/canopen_motor_node)\n    joint_state_publisher (joint_state_publisher/joint_state_publisher)\n    robot_initialization (rosservice/rosservice)\n    ros_control_controller_spawner (controller_manager/spawner)\n  /\n    joint_state_mixer (pem_robot/joint_state_mixer.py)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n\nauto-starting new master\nprocess[master]: started with pid [6027]\nROS_MASTER_URI=http://localhost:11311\n\nsetting /run_id to c8077c0a-8dbd-11e9-b028-ac7ba18b123a\nprocess[rosout-1]: started with pid [6052]\nstarted core service [/rosout]\nprocess[lh_arm/driver-2]: started with pid [6071]\nprocess[lh_arm/ros_control_controller_spawner-3]: started with pid [6072]\nprocess[lh_arm/joint_state_publisher-4]: started with pid [6073]\nprocess[lh_arm/robot_initialization-5]: started with pid [6074]\nprocess[joint_state_mixer-6]: started with pid [6083]\nprocess[robot_state_publisher-7]: started with pid [6093]\n[ INFO] [1560418188.416560376]: Using fixed control period: 0.050000000\n[INFO] [1560418188.461155]: Controller Spawner: Waiting for service controller_manager/load_controller\n[ INFO] [1560418188.509712642]: Initializing XXX\n[ INFO] [1560418188.510056097]: Current state: 1 device error: system:0 internal_error: 0 (OK)\n[ INFO] [1560418188.510289199]: Current state: 2 device error: system:0 internal_error: 0 (OK)\nEMCY: 8A#2232257400000000\nEMCY: 85#223225740000803F\nEMCY: 86#2232257400001C42\n[ INFO] [1560418199.018367789]: Current state: 2 device error: system:125 internal_error: 0 (OK)\n[ INFO] [1560418199.018453747]: Current state: 0 device error: system:125 internal_error: 0 (OK)\n[ INFO ..."], "answer_code": ["EMCY: 8A#2232257400000000\nEMCY: 85#223225740000803F\nEMCY: 86#2232257400001C42\n", "0x3222", "candump can0", "can0  705   [1]  00\n  can0  706   [1]  00\n  can0  70A   [1]  00\n", "CAN_ERB_H", "CAN_ERB_L"], "url": "https://answers.ros.org/question/325633/schunk-lwa4p-initialization-problem-with-ros_canopen/"},
{"title": "Is it possible to remote execute a python ROS file", "time": "2019-06-13 16:29:29 -0600", "post_content": [" ", " ", "The question is hard to summarize. My goal is: to have a student in a brief Robotics workshop, with their own computer, to write or modify a simple python ROS script and run it and see the results. The student will have an arbitrary computer and we don't want the to have to try and install ROS.", "Laptop \"A\" running Win, Lin or Mac, does not have a working ROS installation. \nRobot \"R\" is running Linux, and has a working ROS installation.", "User on \"A\"writes a python ROS script and \"submits it for execution\" on the robot. What does it mean to \"submit it for execution\"? I am not sure. For example, one might scp the python file from \"A\" to \"R\" and into the \"right\" directory. And then ssh to the robot and issue a catkin_make, and roslaunch. That would work pretty well but requires careful use of shell commands.", "Next level: we write a sophisticated shell script (in python even) which issues those commands, with good error checking and recovery. Still kludgy but at least easier on the \"A\" user.", "Next level: there is a way to cause a node python file to run and establish contact with roscore and publish and subscribe to topics with a partial ros installation. What would be required? I am not sure.", "Another option: \"A\" computer ssh's to \"R\" and does their editing and running there. Drawback is that the \"R\" SBC is slow and it won't allow user of \"A\" to use their favorite text editor.", "Thoughts? I am sure others have had this particular scenario!", "How would you feel about using a build server like teamcity? Students could have their own repository for their code. When they're ready to run it, they commit and push. The build server then builds the code, and deploys it to the robot.", "Very interesting idea.... How would that work? I don't know TeamCity But for example github actions seem to be useful to activate when code is pushed to a repo. But github is a service and my robots are in the lab. They do have static IP addresses but they are not accessible from outside (I hope :). Even if they were how would the github action deliver the code to the robot and then trigger the roslaunch etc?", "DO you have a more specific scenario?", "So regarding teamcity in particular, You could install it on a server within your lab. Within teamcity, you can configure each build project to be triggered when somebody pushes code (doesn't matter if the code repo is cloud hosted, or hosted on your server). As part of the build, you can have it run arbitrary shell/bash scripts which could compile the code, copy it to the robot, and start the required launch file. You could even have it send certain commands to the robot to test the code/ execute certain maneuvers."], "answer": [" ", " ", " ", " ", "Another option: \"A\" computer ssh's to \"R\" and does their editing and running there. Drawback is that the \"R\" SBC is slow and it won't allow user of \"A\" to use their favorite text editor.", "you have quite a nr of constraints, but two approaches that come to mind:", "The first would be a low-fi solution, almost trivial to setup: SSH clients are available on all OS, and SCP clients as well. Find an SCP client which allows to continuously synchronise a directory on the students laptop/pc/whatever with one on the robot (", " for Windows supports this). Open a shell session over SSH, navigate to synchronised directory, execute script.", "The second would be fancier, and would probably appeal to non-command-line users. I haven't done this myself, but there are web based IDEs, even ones that allow you to execute code remotely as well. There are two main variants:", "An example of option 1 would be ", " (note: it's an example of the concept, I'm not suggesting you use mbed os or their online environment). Perhaps the team city approach mentioned by ", " is an example of this as well. I believe the main work here will be setting up your robot to make it a suitable target device for deployment.", "The second option is essentially similar to the first one, but you'd run one of the cloud-based web IDEs in their self-hosted configuration. As 'everything' is already running on the robot, it would already be the target device. I imagine there will be less setup work, but I'm not sure.", "A variant of the second option could be where a more powerful machine (but still local) hosts the web-based IDE and your robot is still setup as a target device. That could work around limited hw/processing capabilities of your robot.", "Note that none of the web-based approaches will allow students to use \"their favourite text editor\". But tbh I doubt that is really something you should constrain yourself by. Unless that text editor happens to be an actual IDE, things like code-completion and linting are not going to be automatically available. Those are valuable features, so I wouldn't want to develop without them, but these are students, so perhaps your goals are different.", "Finally: the SSH+SCP approach should take 30 mins to setup, max. The web-based stuff is much nicer but will take some time to figure out and setup.", "Edit: if you don't have a particular need to use an actual ", " installation, then perhaps something like ", " in combination with ", " might also work. Requires no ROS installation and can be ...", "For my own students I actually feel learning to use the command line is a very important skill, almost indispensable. At least with the current state of things you cannot ignore it, and after some initial hickups, most students do not have a problem with it.", "And just found ", ", which seems to support Python and can be self-hosted. The ", " is a bit spartan, but seems functional and is relatively easy to understand.", "And a random other alternative: ", ".", " Thanks... Two clarifications: when I said \"their favorite editor\" really all I meant was an editor other than nano or gedit. Graphical and running on \"A\", so e.g. atom, vscode, sublime, etc. ", "Second: I am organizing a 2-4 hour \"workshop\" for students curious about robotics so I want a rapid path to success, and not presuming any particular level of knowledge. For the real course certainly they need to know the command ine."], "answer_details": ["use SSH + an SCP client", "use some incarnation of a self-hosted web based IDE + deployment solution", "IDE is cloud based, target device is local. Development happens in browser, deployment is to local target device", "IDE is self-hosted, running on the robot. Development happens in browser, deployment is to the same robot", " ", " ", " ", " "], "answer_code": ["winscp", "rospy", "rosbridge_suite"], "url": "https://answers.ros.org/question/325733/is-it-possible-to-remote-execute-a-python-ros-file/"},
{"title": "How to integrate libcreate in an own ros package - (turtlebot1)", "time": "2016-11-30 17:57:08 -0600", "post_content": [" ", " ", "Hey there,", "what I recently did was installing ", " (similar to a turtlebot1). I cloned this files in my catkin_ws/src/create_autonomy and then I just catkin_make them. All works fine. Now what I found out is that this ROS driver relies on a  ", ". This library is however build in catkin_ws/devel/src/libcreate.", "What I want to do next is create an own package (e.g: catkin_ws/src/roomba_521) and in this package I would like to have some c++ files which somehow include this library, so that I can use functions etc. ", "(1) That is why I would like to ask ", " do I have to install  the ", "(2) How do I have to reference to this library e.g. in a c++ file / just include these files?", "Wait, catkin_ws/devel/ only has include, lib, and share. Where is the src? Also how did you link libcreate and create_autonomy?"], "answer": [" ", " ", " ", " ", "You can use ", " (", ") as a reference. This assumes that libcreate is installed as a catkin package (currently by cloning libcreate into your workspace and building with catkin). Then, in your package.xml you can add build/run dependencies to libcreate, include/link in your CMakeLists.txt (", "), and include the header ", " and make API calls in your code (", "). Hope it helps.", "Thanks Jacob ;) that was easier as I thought! I got it.\nI also would like to use your code and libcreate library for my own project which is specefic for the roomba 521. Do I have your permission to create an own github repository witch contains code that is very similar to yours?", "Yes, as long as you adhere to the ", " license. Is there a core feature that ", " is missing that you cannot just use that?", "No your code is working perfect. Anyway I just would like to experiment a little bit with it and simply git clone this modified code that is why I am thinking about making my own repository. I hope this is ok for you? Anyway I found out that I am not able to execute dual processes on the roomba.", "Dual Processes on the roomba: I was for example not able to simultanously play a song and change the colour of the power clean led light.", "OK. If you end up improving the functionality of the driver, pull requests are welcome :)", "of course I will let you know if I have something.", " your links doesn't lead to any page. Could you re upload it?", " updated."], "answer_code": ["create/create.h"], "url": "https://answers.ros.org/question/249142/how-to-integrate-libcreate-in-an-own-ros-package-turtlebot1/"},
{"title": "Mavros topics not publishing?", "time": "2014-09-24 13:35:40 -0600", "post_content": [" ", " ", " ", " ", "Hi,", " I've been trying to connect to a Pixhawk quadcopter using the mavros library ( ", " ) and I had some issues. We are using a pixhawk and a 3DR Iris quadcopter, and we launch the mavros code with ", "The code launches with no errors are we see a long list of topics being displayed:", "The good news is that the /mavros/state, /diagnostics, and /mavros/radio_status seem to be publishing correctly", "The bad news is that every other topic is empty. No GPS data, no battery data, no RC data, etc. Has anyone else run this code and knows how to get this data to publish?", " The closest thing online I found is this online book:  ", "The problem is, they experienced the same problem but gave no reason/fix for it.", "Thanks!", "\nOn a fresh install of Ubuntu 14.04 and a newly installed mavros", "I tried to get sensor data from a ", ". As per ", " 's suggestion, I used rqt_logger_level to view the mavros.ros.mavconn debug messages.", "Launched the file using px4.launch, enabled mavconn Debug messages, and watched the message ID's that showed up.", "Then message ID 253 was published three times in a row, in conjunction with FCU lines for ArduCopter V3.1.2, PX4 and PX4v2:", "[DEBUG] [1411862177.961715937]: serial0:recv: Message-Id: 253 [51 bytes] Sys-Id: 1 Comp-Id: 1\n[ INFO] [1411862177.961821314]: FCU: ArduCopter V3.1.2 (e2ed3dd1)", "After that, during bootup, there is a fast stream of 22 IDs split up by 0, 166, and 109", "At the end of a long stream of 22's it gets 43, 44, 40, 39, and 47 in quick succession:", "[DEBUG] [1411862299.848786274]: serial0:send: Message-Id: 43 [2 bytes]\n[DEBUG] [1411862299.929775171]: serial0:recv: Message-Id: 44 [4 bytes] Sys-Id: 1 Comp-Id: 1\n[DEBUG] [1411862299.929917816]: serial0:send: Message-Id: 40 [4 bytes]\n[DEBUG] [1411862300.143771523]: serial0:recv: Message-Id: 39 [37 bytes] Sys-Id: 1 Comp-Id: 1\n[ INFO] [1411862300.143899206]: WP: item #0 GAA WAYPOINT CUR params: 0, 0, 0, 0 x: 42.2936 y: -71.2638 z: 0\n[DEBUG] [1411862300.143942143]: serial0:send: Message-Id: 47 [3 bytes]\n[ INFO] [1411862300.143995581]: WP: mission received\n[ WARN] [1411862300.673739713]: PR: request param #133 timeout, retries left 2, and ...", "Are you using APM firmware on the PX4?  If not, try using the PX4 launch file.  There is a quirk for PX4.  Do you get a heartbeat?", "I'll post the code I get upon launch in the answer below, it maxes out the comment length", "Try enable ", " debug output using rqt_logger_level. Also use rqt to view diagnostics and topic updates.\nI tested with apm2_radio.launch with original APM 2.6, works. On pixhawk i only use PX4, but should no matter.", " - I tried using the PX4 launch file and have the same problem, unfortunately. I do get the heartbeat message: CON: Got HEARTBEAT, connected.", " - I enabled the ros.mavros.mavconn Debug level and got a lot of messages like this: \"serial0:recv: Message-Id: 22 [25 bytes] Sys-Id: 1 Comp-Id: 1\" with a couple different message IDs. There weren't any obvious flags showing up in the debug stream.", "I could easily be doing something dumb where I have to arm the quadcopter or flip some switch before it starts publishing to the sensor data topics. Currently I just turn the copter on, hit the safety button, and check whether /mavors/imu/data, /mavros/battery, etc. are getting anything", "Check that following message ids are send: 30 (ATTITUDE), 27 (RAW_IMU), 24 (GPS_RAW_INT).", "20-23 is parameter protocol messages.", "Also what version you use?"], "answer": [" ", " ", "Long story short, the mavros topics weren't publishing because there were firmware updates for the various radios in the system that we hadn't implemented.\n    1) The radio modem that connected to the computer was correct\n    2) The radio modem on the Pixhawk was out of date\n    3) The firmware on the Pixhawk itself was out of date", "It's our first time working with the Pixhawk so we didn't think to check the firmware of various components, when we upgraded using the APM Mission Planner GUI then we started getting all topics being published correctly. Thanks to ", " for his help.", "I can't mark this as the answer because my karma isn't high enough, but for us the question is closed", "Sometimes this error will come up again and running APM Mission Planner, connecting, and disconnecting will help solve the issue.", " ", " ", " ", " ", "i know this is an old question but google still get you here first.", "i had the same problem with apm 2.5 and 2.6. i had ( two apm 2.6 and one apm 2.5) one of the apm 2.6 worked with no problem. but the other two apm not.", "i found a command on this \n", "\nand it had this command", "and everything for all apm worked after then.\nit also contains how t do this from C++", "That solved the problem!", "This worked for me!", " ", " ", "I am having similar issues. \nI am running:", "$ roslaunch mavros px4.launch", "Everything seems fine, when I try to arm the PX4  I type:", "$ rosrun mavros mavsafety arm", "One terminal said: [INFO][141398710.136667466]:FCU: command received;", "whereas another terminal said:\nRequest failed, Check mavros logs", "(Arming with RC is OK)", " ", " ", "Hi-", "I am having similar issues.  My setup: Ubuntu 14.04, ROS Indigo, 3DR X8 with PX4.", "I am running:", "$ roslaunch mavros px4.launch", "$ roslaunch mavros_extras teleop.launch", "I am trying to arm the quad.  When I press [2] to arm, I do not get any debug messages or response.  When I pressed [0] or [1] to takeoff/land, I get a command received, request failed.  My joystick is working properly and I can arm/disarm using the RC controller.  Is there another script that needs to be running for teleop mode using a joystick?  Or perhaps a dead-man switch?  Maybe a setting in a yaml or launch file?  ", "Thank you for any assistance.  ", "Hi Christopher, we are having a similar arming issue to this but our original topics problem has been solved, so I'm going to close the question. I'll make a new question for the arming issue", " Teleop stript is not comleted and may not fully work. Issue:  ", "Also i receive several reports that APM rejects arming on APM:Copter. Last time i see APM code it require some checks and may return \"UNSUPPORTED\" if that checks fail.", "Please ask at diy-drones what steps required to ARM. Because cmd/arming is simply sends command 400. E.g. my bixler with APM:Plane use autoarm, so explicit arming not reqired, but it responds to 400 normally. PX4 on the desk don't arm (but conditions are not met).", " Hi Christopher, I opened a new question here:  ", "It might be cleanest if you move your query there, but probably not a big deal", " ", " ", " ", " ", "Hello ", "  and @eric-schneider ,\nI'm stuck somewhere similar but my source of the problem is elsewhere I think.", "1) I am using Ubuntu 14.04 and mavros for ROS Indigo.", "2) FCU I am using 3DR Robotics Iris equipped with Pixhawk. ", "3) I installed mavros using ", " . This part is fine. Next, I connect the USB receiver to my computer and I change the \"ttyACM0\" in the ", " file to \"ttyUSB0\".", "4) Then I call the launch file, and I see", "Now I wonder if ", " is the problem because when I launch a new terminal and do a ", "\nI can see - ", " Did you read  ", "  ? You shall use  ", " because by default 3DR Pixhawk comes with APM firmware.\nSecond don't change launch file defaults, add ", ".\nNext check ", ".", "Hi ", " , I managed to get the data (like battery and etc) by updating the firmware for both radio and the drone using Mission planner and roslaunch px4.launch (didn't see your answer until now).", "You're saying regardless of that, we should use apm.launch because ETH pixhawk was px4 & 3dr is APM?", " ", " ", "I had a similar issue, px4.launch was executing properly but no heartbeat inspite of the UART serial communication working properly (I did a loopback test). I am using a raspberry pi 3 B with a pixracer.", "What worked out for me was changing the baudrate in the launch file to 921600. Hope this helps someone!"], "question_details": [" ", " ", " ", " ", " ", " ", "First, during the section where you get the heartbeat, there were a mixture of 0, 166, and 109 messages.", " ", " "], "question_code": ["roslaunch mavros apm2_radio.launch\n", "/diagnostics\n/mavlink/from\n/mavlink/to\n/mavros/battery\n/mavros/fix\n/mavros/gps_vel\n/mavros/imu/atm_pressure\n/mavros/imu/data\n/mavros/imu/data_raw\n/mavros/imu/mag\n/mavros/imu/temperature\n/mavros/mission/waypoints\n/mavros/radio_status\n/mavros/rc/in\n/mavros/rc/out\n/mavros/rc/override\n/mavros/state\n/mavros/time_reference\n/rosout\n/rosout_agg\n", "$:rostopic echo /mavros/state\n    header: \n      seq: 100\n      stamp: \n        secs: 1411583514\n        nsecs: 793784329\n      frame_id: ''\n    armed: False\n    guided: False\n    mode: ALT_HOLD\n", "sudo apt-get install mavros\n", "# Changed the default fcu_url to ttyUSB0 instead of ttyACM0, because that's the port my 3DR modem was on.\n<arg name=\"fcu_url\" default=\"/dev/ttyUSB0:57600\" />\n", ".mavconn"], "answer_code": ["sudo apt-get install ...", "px4.launch", "[ INFO] [1429900676.296419593]: FCU URL: /dev/ttyUSB0:115200\n[ INFO] [1429900676.296568384]: serial0: device: /dev/ttyUSB0 @ 115200 bps\n[ INFO] [1429900676.297267179]: GCS bridge disabled\n[ INFO] [1429900676.338080124]: Plugin 3dr_radio loaded and initialized\n[ INFO] [1429900676.340020654]: Plugin actuator_control loaded and initialized\n[ INFO] [1429900676.343256392]: Plugin command loaded and initialized\n[ INFO] [1429900676.349410801]: Plugin ftp loaded and initialized\n[ INFO] [1429900676.355797673]: Plugin global_position loaded and initialized\n[ INFO] [1429900676.358029042]: Plugin gps loaded and initialized\n[ INFO] [1429900676.358063670]: Plugin image_pub blacklisted\n[ INFO] [1429900676.368613134]: Plugin imu_pub loaded and initialized\n[ INFO] [1429900676.371929447]: Plugin local_position loaded and initialized\n[ INFO] [1429900676.423800247]: Plugin mocap_pose_estimate loaded and initialized\n[ INFO] [1429900676.425444083]: Plugin param loaded and initialized\n[ INFO] [1429900676.429073723]: Plugin px4flow loaded and initialized\n[ INFO] [1429900676.431786149]: Plugin rc_io loaded and initialized\n[ INFO] [1429900676.433578150]: SA: Set safty area: P1(1.000000 1.000000 1.000000) P2(-1.000000 -1.000000 -1.000000)\n[ INFO] [1429900676.435489856]: Plugin safety_area loaded and initialized\n[ INFO] [1429900676.438453801]: Plugin setpoint_accel loaded and initialized\n[ INFO] [1429900676.444126377]: Plugin setpoint_attitude loaded and initialized\n[ INFO] [1429900676.448153997]: Plugin setpoint_position loaded and initialized\n[ INFO] [1429900676.449946335]: Plugin setpoint_velocity loaded and initialized\n[ INFO] [1429900676.455628269]: Plugin sys_status loaded and initialized\n[ INFO] [1429900676.458712512]: Plugin sys_time loaded and initialized\n[ INFO] [1429900676.459608376]: Plugin vfr_hud loaded and initialized\n[ INFO] [1429900676.464448577]: Plugin vision_pose_estimate loaded and initialized\n[ INFO] [1429900676.467479429]: Plugin vision_speed_estimate loaded and initialized\n[ INFO] [1429900676.470861478]: Plugin visualization loaded and initialized\n[ INFO] [1429900676.473968732]: Plugin waypoint loaded and initialized\n[ INFO] [1429900676.474017908]: Autostarting mavlink via USB on PX4\n[ INFO] [1429900676.474039611]: Built-in mavlink dialect: ardupilotmega\n[ INFO] [1429900676.474059391]: MAVROS started. MY ID [1, 240], TARGET ID [1, 50]\n", "[ INFO] [1429900676.474039611]: Built-in mavlink dialect: ardupilotmega", "rostopic list", "/diagnostics\n/mavlink/from\n/mavlink/to\n/mavros/actuator_control\n/mavros/battery\n/mavros/global_position/compass_hdg\n/mavros/global_position/global\n/mavros/global_position/gp_vel\n/mavros/global_position/local\n/mavros/global_position/rel_alt\n/mavros/gps/fix\n/mavros/gps/gps_vel\n/mavros/imu/atm_pressure\n/mavros/imu/data\n/mavros/imu/data_raw\n/mavros/imu/mag\n/mavros/imu/temperature\n/mavros/local_position/local\n/mavros/mission/waypoints\n/mavros/mocap/pose\n/mavros/px4flow/ground_distance\n/mavros/px4flow/raw/optical_flow_rad\n/mavros/px4flow/temperature\n/mavros/radio_status\n/mavros/rc/in\n/mavros/rc/out\n/mavros/rc/override\n/mavros/safety_area/set\n/mavros/setpoint_accel/accel\n/mavros/setpoint_attitude/att_throttle\n/mavros/setpoint_attitude/cmd_vel\n/mavros/setpoint_position/local\n/mavros/setpoint_velocity/cmd_vel\n/mavros/state\n/mavros/time_reference\n/mavros/vfr_hud\n/mavros/vision_pose/pose\n/mavros/vision_speed/speed_vector\n/mavros/visualization/track_markers\n/mavros/visualization/vehicle_marker\n/mavros/wind_estimation\n/rosout\n/rosout_agg ...", "apm.launch", "fcu_url:=/your/port/url", "/diagnostics"], "url": "https://answers.ros.org/question/193411/mavros-topics-not-publishing/"},
{"title": "RTABmap deletes half of my map", "time": "2019-07-17 09:32:03 -0600", "post_content": [" ", " ", "Hi,", "When I am doing SLAM using RTABmap, at half point, I got half of my map and rtabmap deleted some of my map for some reason, why is that?\nI have uplodade the database here:\n", "Also, how can I set an initial point during the localization? As rtabmap does not localize sometimes and if I click on 2dposeestimate button, nothing happens.", "The last question is, how can I reduce the resolution of my map?", "Ok after reading more papers on Rtabmap, I have figured out that some of by bag-of-words went to the Long term memory as I have set the parameter for the TimeThr = 700. Still, I cannot understand why I cannot localise the robot myself with a button and how to use the long term saved memory for the nav"], "answer": [" ", " ", "Hi,", "The problem is that the working memory (WM) is very small. For this map size, nodes should be easily be all in WM (even on a RPI3). The real problem is that just for processing one frame, you already reached the time threshold of 700 ms, thus locations just seen are already transferred to long-term memory. You can disable memory management to see the difference (", "). Looking at the timing statistics saved in the database (using rtabmap-databaseViewer -> Statistics view -> TimingMem):\n", "A lot of time is used to create the local occupancy grids (green line). ", " curve includes all others. What is your computer? Here is on my computer (also in ms):", "You are feeding 720p stereo images to create the grid. When input is stereo, disparity image should computed (at 720p) to create the grids. If you are using a ZED, I would recommend to use the depth image already created by the ZED wrapper to save time on disparity computation. Another way is to use smaller images as input to rtabmap node. Another approach is to decimate the stereo images prior to compute the disparity image, using ", " for example (you would need to set ", " if you do so).", "To save time on feature extraction (red line of first figure), you can set ", ".", "For the 2d pose estimate button of RVIZ, you should remap its publishing topic to ", " (well, assuming rtabmap node is started in rtabmap namespace), and this only works in localization mode (", ").", "cheers,", "\nMathieu", " I am using Jetson Nano.", " I have added a new question where I used Lidar and depth images instead of stereo images. Also, I set the TimeThr to 0. Can you have a look and possibly help me with my questions?\nLink: "], "answer_code": ["Rtabmap/TimeThr=0", "Signature creation", "Mem/ImagePostDecimation=2", "Grid/DepthDecimation=2", "Mem/ImagePreDecimation=2", "/rtabmap/initialpose", "Mem/IncrementalMemory=false"], "url": "https://answers.ros.org/question/328753/rtabmap-deletes-half-of-my-map/"},
{"title": "[SVO] Get pose for a long time", "time": "2019-07-21 08:54:37 -0600", "post_content": [" ", " ", " ", " ", "I'm using the ", " package. I already calibrated my bottom camera (using the Pinhole model, with the specs of ", " modified to be in the format expected) and modified my .launch file so it gets the images from the output of the ", " (i.e, grayscale and rectified), but it's very difficult to obtain the pose of the camera over a long period of time.", "This is my .launch file:", "and this is how I use the image_proc package:", " gives this output:", "When I run ", " this is the output:"], "answer": [" ", " ", " ", " ", "[ WARN] [1563654234.306474166]: First\n  image has less than 100 features.\n  Retry in more textured environment.", "tells that environment doesn't have enough features.", "it's very important that when svo is in Init mode , camera moves slowly and linearly.", "[ WARN] [1563654235.348895448]: Init\n  WARNING: 40 inliers minimum required.\n  [ INFO] [1563654235.348931120]: RESET", "it's tells you move camera badly in init mod or  environment doesn't have enough features. i tested svo for drone in long time flight over 20 minutes and it's work very well", "Ok, but then it says ", ", so I think that initializatiom is correct", "in svo, first an init image select and track features till camera moves enough and needs at least 40 features.it can see by green line on the image that comes from svo and after init the green line disappeared you can change the time of tracking by changing init_min_tracked and by changing init_min_inliers change minimum of features that needed", "Ok, and which parameters did you use? I\u2019m using an AR.Drone 2.0, which bottom\u2019s camera is not so good", "in my opinion you better use orb slam and font camera it's better because svo works well with wide lens cameras", "I need to use the bottom camera. I will use svo because it was recommended to me and I already spent a lot of time trying to make it works. Can you tell me about the parameters you used, or any good tutorial? The official tutorial is too short"], "question_code": ["<launch>\n\n    <node pkg=\"svo_ros\" type=\"vo\" name=\"svo\" clear_params=\"true\" output=\"screen\">\n\n        <!-- Camera topic to subscribe to -->\n        <param name=\"cam_topic\" value=\"/ardrone/bottom/image_rect\" type=\"str\" />\n\n        <!-- Camera calibration file -->\n        <rosparam file=\"$(find svo_ros)/param/ardrone_bottom.yaml\" />\n\n        <!-- Default parameter settings: choose between vo_fast and vo_accurate -->\n        <rosparam file=\"$(find svo_ros)/param/vo_fast.yaml\" />\n\n    </node>\n\n</launch>\n", "ROS_NAMESPACE=/ardrone/bottom rosrun image_proc image_proc\n", "rosnode info svo", "Node [/svo]\nPublications: \n * /rosout [rosgraph_msgs/Log]\n * /svo/dense_input [svo_msgs/DenseInput]\n * /svo/image [sensor_msgs/Image]\n * /svo/image/compressed [sensor_msgs/CompressedImage]\n * /svo/image/compressed/parameter_descriptions [dynamic_reconfigure/ConfigDescription]\n * /svo/image/compressed/parameter_updates [dynamic_reconfigure/Config]\n * /svo/image/compressedDepth [sensor_msgs/CompressedImage]\n * /svo/image/compressedDepth/parameter_descriptions [dynamic_reconfigure/ConfigDescription]\n * /svo/image/compressedDepth/parameter_updates [dynamic_reconfigure/Config]\n * /svo/image/theora [theora_image_transport/Packet]\n * /svo/image/theora/parameter_descriptions [dynamic_reconfigure/ConfigDescription]\n * /svo/image/theora/parameter_updates [dynamic_reconfigure/Config]\n * /svo/info [svo_msgs/Info]\n * /svo/keyframes [visualization_msgs/Marker]\n * /svo/points [visualization_msgs/Marker]\n * /svo/pose [geometry_msgs/PoseWithCovarianceStamped]\n * /tf [tf2_msgs/TFMessage]\n\nSubscriptions: \n * /ardrone/bottom/image_rect [sensor_msgs/Image]\n * /svo/remote_key [unknown type]\n\nServices: \n * /svo/get_loggers\n * /svo/image/compressed/set_parameters\n * /svo/image/compressedDepth/set_parameters\n * /svo/image/theora/set_parameters\n * /svo/set_logger_level\n\n\ncontacting node http://juan:34907/ ...\nPid: 19891\nConnections:\n * topic: /rosout\n    * to: /rosout\n    * direction: outbound\n    * transport: TCPROS\n * topic: /tf\n    * to: /rqt_gui_py_node_20048\n    * direction: outbound\n    * transport: TCPROS\n * topic: /ardrone/bottom/image_rect\n    * to: /ardrone/bottom/image_proc (http://juan:40291/)\n    * direction: inbound\n    * transport: TCPROS\n", "roslaunch svo_ros live.launch", "... logging to /home/juan/.ros/log/1b6a94be-ab2c-11e9-84b5-68ecc55d6600/roslaunch-juan-19873.log\nChecking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://juan:45331/\n\nSUMMARY\n========\n\nCLEAR PARAMETERS\n * /svo/\n\nPARAMETERS\n * /rosdistro: kinetic\n * /rosversion: 1.12.14\n * /svo/cam_cx: 254.861406985\n * /svo/cam_cy: 132.774654931\n * /svo/cam_d0: 0.0254329004542\n * /svo/cam_d1: 0.00484972274251\n * /svo/cam_d2: -0.00316468715909\n * /svo/cam_d3: -0.0168351839378\n * /svo/cam_fx: 724.628466389\n * /svo/cam_fy: 722.57992602\n * /svo/cam_height: 360\n * /svo/cam_model: Pinhole\n * /svo/cam_topic: /ardrone/bottom/i...\n * /svo/cam_width: 640\n * /svo/grid_size: 30\n * /svo/loba_num_iter: 0\n * /svo/max_n_kfs: 10\n\nNODES\n  /\n    svo (svo_ros/vo)\n\nROS_MASTER_URI=http://localhost:11311\n\nprocess[svo-1]: started with pid [19891]\ncreate vo_node\n[ WARN] [1563654227.665696970]: Cannot find value for parameter: svo/publish_img_pyr_level, assigning default: 0\n[ WARN] [1563654227.666169164]: Cannot find value for parameter: svo/publish_every_nth_img, assigning default: 1\n[ WARN] [1563654227.666518905]: Cannot find value for parameter: svo/publish_every_nth_dense_input, assigning default: 1\n[ WARN] [1563654227.667630230]: Cannot find value for parameter: svo/publish_world_in_cam_frame, assigning default: 1\n[ WARN] [1563654227.668068619]: Cannot find value for parameter: svo ..."], "answer_code": ["[ INFO] [1563654235.023326256]: Init: Selected first frame."], "url": "https://answers.ros.org/question/329015/svo-get-pose-for-a-long-time/"},
{"title": "How to talk to two ROS devices over serial?", "time": "2019-07-18 04:00:47 -0600", "post_content": [" ", " ", " ", " ", "Heya, I am quite new to ROS. ", "I have two independent raspberry pi running on ROS. Is it possible to communicate with each other over serial?", "for eg: \npi_1 will have a node to control a servo, LED and a counter. I would like the pi_2 to able to publish and subscribe to the nodes on pi_1 over serial. ", "Could someone give me leads to how I can implement this? ", "\nIs it rosserial that I am supposed to use? If so how can I communicate through two different ROS devices?", "Do you have any other suggestion on implementing this?", "Thanks in advance,", " ", "In the current situation, I am not able to use ethernet as one pi is a drone and I would need more distance than wifi. I basically want to establish a connection with each other over wireless serial. like a base station and a drone.", "So any other option than rosserial?"], "answer": [" ", " ", "Heya, Thanks everyone for the support.", "I did some research on ", " advice. And I have successfully got the two Pi s speaking through serial ethernet. Now I think I can go forward the 'the typical ROS setup for multiple computers' route.", "Follow the instructions if anyone would like to try:\n", "Enjoy!", " ", " ", " ", " ", "ROS serial is designed to communicate between a ROS node and a simple peripheral device which isn't running ROS such as a micro-controller.", "If you're trying to communicate between two ROS nodes running on two different raspberry Pis all you need to do is make sure that they are both on a shared network (Wi-Fi or ethernet) and then the distributed nature of ROS will handle everything for you automatically.", "Is there any particular reason why you thought about using a serial connection for this?", "Okay I understand your task now. To be honest, you could do this with a much lower power device such as an arduino or teensy on the drone. A raspberry pi is overkill unless it's doing other work as well. I'll emphasise again ROSSerial doesn't connect ROS nodes together it connects peripheral devices to ROS.", "Using an arduino you could follow the tutorials, the ", " controls an LED which is one of your tasks. You should be able to extend this to control a servo as well.", "In case the author isn't tied to serial and that's just an assumption of the right way -- you can put them both on the same network (ethernet) and then use the typical ROS setup for multiple computers.", "Thanks, guys for the feedback. ", "In the current situation, I am not able to use ethernet as one pi is a drone and I would need more distance than wifi. \nI basically want to establish a connection with each other over wireless serial. like a base station and a drone.", "So any other option than rosserial?", "Well, I wanted an option to connect ROS nodes together over serial. \nIf that is not possible, I think this is the next best thing!", "Probably keep an Arduino in between the two ROS systems!", "I'm not sure it'd be a good thing to use here, but it's actually possible to run regular TCP/IP over a serial line. Lookup \"SLIP\".", " Thanks for the lead, Would need to find out how to implement that then.", "Cheers"], "url": "https://answers.ros.org/question/328832/how-to-talk-to-two-ros-devices-over-serial/"},
{"title": "Octomap / GPU Voxel / Franka / MoveIt /  Collision Avoidance - Help", "time": "2019-07-23 07:29:23 -0600", "post_content": [" ", " ", "Hi folks,", "I want to implement a collision Avoidance for a Franka Robot using two Intel D435i 3D Cameras, Octomap and Moveit.", "First Problem: The Octomaps refresh very slowly, at least in rviz. I would say about 2 Seconds when i put an obstacle into the view of the 3D cameras. I tried to limit the octomap distances, bigger voxels, etc. It is still very slow. When i display the two original point cloud topic streams from both the camera nodes, they display flawlessly with 60 fps. Is it still to much data for octomap?", "Second Problem: I thought about using GPU Voxels instead of Octomap. This uses CUDA, but libfranka needs an PREEMPT_RT linux kernel. But there are no nvidia drivers for RT Kernels. The nouveau drivers have still their performance problem and liqourix kernel which has realtime capabilities and support for nvidia drivers does not work with libfranka. ", "What can i do about it? Later i want to implement some ML algorithms for object detection for the panda robot. \nDo i have to run the visual part of ROS on a second machine and the libfranka part on another one?\nOr any other suggestions? ", "Thanks in advance\nArmin", "I got some news from franka-community. At the franka_ros/franka_control node some can pass a realtime ignore Parameter to the Franka::Robot instance inside of the cpp program. The robot should then run even on a non realtime kernel. I tried it with a generic linux kernel and the liqourix kernel and started the nodes with launch-prefix\"sudo -S -E nice -n -20\" but only the gripper moves. The robot itself throws a controller. So still playing with the idea of two separate systems."], "answer": [" ", " ", "I think the two machine solution is the best. for controlling the franka you do need much computation power. just some Ethernet adapters  "], "url": "https://answers.ros.org/question/329125/octomap-gpu-voxel-franka-moveit-collision-avoidance-help/"},
{"title": "can't locate node in package", "time": "2014-03-28 05:36:02 -0600", "post_content": [" ", " ", " ", " ", "HI everybody, I am having problems in building a large project. I'am trying to add some modularity to my .launch file. I built my launch file as follows", "with the file included being", "the node in the heterogeneous package is (successfully) built with the following CMakeLists.txt file                                                                                                                                                                       ", "The file si successfully compiled and the executable added to the project workspace. In fact, if I prompt from the catkin workspace ", "I obtain ", "However when I launch the .launch file I get", "I really cannot figure out what the problem is. I am using ros hydro\nAny ideas?\nThanks \nAndrea", "EDIT: actually I cannot run the node with ", "This doesn't work any more. I use the ros version kinetic, farid's method is useful for me.", "I just had the same problem, but for me the problem was in the launch file. I created my node with catkin command, and then personalized the cmake and the launch file, but I did a mistake by switching name_node to name.\nI know it is a very specific mistake but maybe it will help the next guy!", "True:", "False:"], "answer": [" ", " ", "I have found the problem, I had to add the line ", "to the CMakeLists.txt file", "This solved my problem too. Thanks for the find.", "But your CMakeLists.txt file above already contains a catkin_package statement. \nDid simply adding that line solve your problem?", "It's a two years old post, so I cannot be 100% sure. But yes, that solved the issue. Now that I know more about that I can tell you that the error is I put the catkin_package statement at the end. After declaring the executable. It has to be done before the find package statements (imho)", " ", " ", "For python nodes, check the permissions on the script. If it's not marked as executable, then you will get this same cryptic error message.", "This answer solves it for me. This one gets me every time, and somehow I forget later on down the road.", "Thanks for this info. This is why i cannot locate node in package.", "yeah, I got the same problem. Thanks for your answer!", "2 years later and also ran into this issue... They really could setup an additional check for permissions before throwing an error about not finding the file.", "ROS is an open-source project. It depends on people like you to find issues and solve them. If all we do is complain nothing will change. You have the power to fix this for all future users, including yourself.", " ", " ", "For those fellows who are still getting the same error:", "Go to your catkin workspace:", "and bash file needs to be added to devel folder:", "This works for me, my ros version is kinetic.", " ", " ", "I had the same issue in my case it was no execution permision for python file.\nTo make file executable please go your package/src directory and type ", "You may want to check first if files have execution permision by cmd", "If file is executable it should have x permision in list like:", "If file is not executable ls -la will display it without x:", "thanks, this worked  for me", "I've the same error and I checked according to your advise for executing permission. In there I have no permission sign. How can I do to get permission?", "thanks very much, the same problem with python file, it solved now.", " ", " ", "Executable was not built or has another name - is the usual reason for this error.", "It is not actually an answer to the question, because executable was checked by the question creator. But if someone else find it, please check the obvious reason first. : )", " ", " ", "I have created two ROS nodes and have compiled them using add_executable command in the \"CMakeLists.txt\". I have also added them to the launch file that I am running using roslaunch. But when running roslaunch, I get the same ERROR: cannot launch node of type [r/r2_detect]: can't locate node [r2_detect] in package [r]", " I tried adding catkin_package() to the \"CMakeLists.txt\" as suggested here:  ", " , but it did not work.\nWhere should I add catkin_package() ? ", "what did you do guys to solve your issue ?", " ", " ", "For python nodes, writing it as ", " instead of just ", " works for me."], "question_code": ["<launch>\n <include file=\"$(find gazebo_ros)/launch/empty_world.launch\">\n   <arg name=\"world_name\" value=\"$(find vtol_gazebo)/worlds/vtol.world\"/>\n  </include>\n\n\n    <include file=\"$(find heterogeneous)/config/heterogeneous.xml\"/>\n\n</launch>\n", "<launch>\n\n  <node pkg=\"heterogeneous\" name=\"het\" type=\"heterogeneous\">  \n  </node>\n\n</launch>\n", "cmake_minimum_required(VERSION 2.8.3)\nproject(heterogeneous)\n\nfind_package(catkin REQUIRED COMPONENTS\n  roscpp\n)\n\nset(ARMADILLO_INCLUDE /usr/local/include/)\nset(ARMADILLO_LIBRARIES /usr/lib/libarmadillo.so)\n\ninclude_directories(include/${PROJECT_NAME} ${Boost_INCLUDE_DIR} ${catkin_INCLUDE_DIRS} )\n\nadd_executable(heterogeneous src/Heterogeneous.cpp)\ntarget_link_libraries(${PROJECT_NAME} ${catkin_LIBRARIES} )\n\ncatkin_package(\n  DEPENDS\n    roscpp\n)\n", "find -executable -name heterogeneous -type f\n", "./build/heterogeneous/heterogeneous\n", "ERROR: cannot launch node of type [heterogeneous/heterogeneous]: can't locate node [heterogeneous] in package [heterogeneous]\n", "rosrun heterogeneous heterogeneous\n", "<node name=\"$(arg node_name)\" pkg=\"hil\" type=\"hil_node\" clear_params=\"true\" output=\"screen\">\n", "<node name=\"$(arg node_name)\" pkg=\"hil\" type=\"hil\" clear_params=\"true\" output=\"screen\">\n"], "answer_code": ["catkin_package()\n", "$ cd ~/catkin_ws/\n", "$ source devel/setup.bash\n", "chmod +x your_file.py\n", "ls -la\n", "rwxrwxrwx\n", "rx-rw-r--\n", "type=\"file.py\"", "type=\"file\""], "url": "https://answers.ros.org/question/145801/cant-locate-node-in-package/"},
{"title": "using move_base recovery for initial pose estimation", "time": "2014-04-10 01:02:46 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I want my robot to find its initial pose on its own rather than providing it with rviz \"2d pose estimate\". For that I already figured out that you can call AMCLs /global_localization service, whicht distributes the particles over the whole map. Second I'd need to rotate the robot. Therefore move_base already has the recovery behaviour, but how to trigger that? I know there is the C++ API, but isn't there a service call for that? Something that smoothly integrates with move_base resp. nav stack? Or can I use the actionlib for that? Anything any trigger despite the cpp api?", "I'm wondering for a long time why there isn't something out there for initial pose estimation with the widely used combination of amcl and move_base. Are you guys all using the rviz-button? all the time you start demos?", "As I could not find an answer to this, I wrote a node triggering a recovery-behaviour similar to the ", " implementation. However those behaviours (", ", ", ") need the global and local costmap as parameters. Just creating your own as suggested in the code snipples from the wiki doesnt seem to work as intended since those maps are not the ones used by move_base: Running ", " then does not clear the costmap used by move_base. ", ", and this is strange, ", " sometimes is not carried out due to potential collision. How can there be collisions if the costmap is empty? Or IS there a connection between a costmap you create and name ('local_costmap', 'global_costmap') and the ones used by move_base? This is confusing ...", "An alternative solution would be to use ", " and send ", ", but how can I tell move_base to just use the local planner and ignore the global one?", "so .. nobody has the problem of initial pose estimation with amcl?"], "answer": [" ", " ", " ", " ", "Hi", "What robot are you using?", " In my case, I'm working with Turtlebot II robots. I avoid using the rviz-button on my demos by using first the autocking sequence. Turtlebot have a docking (battery recharger) with infra-red emitters and the robot base have IR-receptors ( ", " ). You can make the robot to search and find this base automatically. I take advantage of this by using the dock position as reference (putting it in an known place on the map) When the robot reach it, it publish a message to the topic 'initialpose' (PoseWithCovarianceStamped) with the reference position and AMCL now can work normally. ", "Maybe this is not precisely what you are searching to do, but what I recommed is that you search for a way in which therobot reaches a known position looking for some kind of landmark first, and use that reference to give the AMCL algorithm the initial position. I think that without some help (initial position), the AMCL probably will not work.", "Thanks, landmarks are one way to do it. But I want to keep it generic as the demo scenario changes often.", "good advice !", " ", " ", " ", " ", "Hi ,", "Initialize your robot for the very first time only(from Rviz)", "Write a node which take care of two things.", "1.Subscribes /", " and updates it in a  local .txt file.\nSo at any instant of time, you will have robot's current position saved in the local file.", "2.Publish the value to  /", "  one time when you execute the node (probably at the time of initialization of robot position)", "So at any point even if the robot's battery turn off or any issue happens, on relaunching the nodes again, the robot will be spawned in the last known position", "Note: It will work fine, in case if the robot is not moved manually, ie for robot kidnap problems this won't help"], "url": "https://answers.ros.org/question/151526/using-move_base-recovery-for-initial-pose-estimation/"},
{"title": "Autoware NDT_Matching not matching", "time": "2019-07-11 18:27:27 -0600", "post_content": [" ", " ", "Hello, ", "I am currently attempting to run Autoware\u2019s ndt_matching process with little success. I have gotten the ndt_matching to work in the ROSBAG Demo provided on the gitlab with the suggested/preset settings and with changing it from starting with GNSS to starting at an initial position of [0,0,0,0,0,0] and changing the type of matching (pcl_generic, pcl_anh, and pcl_openmp). ", "So far I have built a few .pcd map files with the ndt_mapping process and have attempted to use that to localize off of using a similar process to what is described in the example launch files:", "my_localization.launch:", "I then go in manually to add the ndt_matching from the Runtime Manager (and have started also using ndt_matching_monitor from there as well). ", "Since the ndt_mapping process does not seem to include gnss data from the bag file, I initialize the position with [0,0,0,0,0,0]. Predict Pose is ON. I have increased and decreased the following values individually and in combination: Error Threshold, Resolution, Step Size, Transformation Epsilon, and Maximum Iterations for three of the four Method Types (pcl_generic, pcl_anh, and pcl_openmp) {did not use the pcl_anh_gpu due to memory errors on my computer related to the gpu}\nI am not using Odometry nor IMU but I do have Get Height checked because that was what the original ndt_matching had as its option on the original my_localization.launch. I have started with the preset settings (the ones that are used and work on the ROSBAG demo) of Error Threshold=1, Resolution=1, Step Size=0.1, Transformation Epsilon=0.01, and Maximum Iterations=30, but again have increased/decreased those values and tested the results. ", "I have created 3 .pcd maps: two of the area surrounding my building, with a different [0,0,0] for both, and one inside my lab.\nInitially, when using the outside maps, I was having the problem (mostly with pcl_openmp) of the ndt_matching initially checking around the start point but then eventually zooming off the map unable to recover. However, now even when I run the map inside my lab and the vehicle has not moved from the position where the map was recorded, the ..."], "answer": [" ", " ", "Please try as follows.", "Play rosbag(slam_topics_2019-07-01-10-02-59.bag) and pause immediately.", "Execute the following command to fix header.stamp to current ros time.", "Set BaseLink to Localizer(params is [0.465, 0, 1.945, 3.142, 0, 0])", "Publish Point Cloud Map(autoware-190703-2.pcd)", "Publish TF(autoware/ros/src/.config/tf/tf_local.launch)", "Launch voxel_grid_filter(config: \"Points Topic\" is \"/points_raw\")", "Launch ndt_matching(config: \"Initial Pos\" is [-5.68, 59.18, -5.775, -0.141, 0.004, 0.703])", "Restart rosbag.", "Yamato, ", "Thank you for your reply! ", "The above works, with a slight modification: Step 2 should not have the indent before ", " and I also put the entire part in green into single quotes ( ' ) instead of double as to not interfere with the naming of the fields. ", "My follow-up question is how did you get the values for the Initial Pos for Step 7? This seems to have been the source of my issue and I am wondering what determined them. Thank you again!", "Thank you for your comment on the correction.\nThe method of determining the value of Initial Pos used the 2D Pose Estimate of Rviz and searched the position which can be matched manually.\nAnd the output value of the following command was made Initial Pose.", "@Yamato", "Hello, ", "I have been tested Autoware localization (lidar localizer using ndt_matching) with Velodyne (16, 32 and 64) model sensors and Ouster 64 channel sensor. With all the sensor localizer working good. But with the Hesai Pandar64 channel lidar getting an error like this bellow. Above mentioned sensors tested with the same map data and with the frame id modified to \"velodyne\". ", "Could you please comment on this, looking forward to hearing from you.", "\nHello,\nPlease paste the error message.", " Since this is an unrelated and new issue, please post a new question.", "@Yamato\nThanks for your kind reply. ", "Error Message :\nFor frame [velodyne]: No transform to fixed frame [world]. TF error: [Lookup would require extrapolation into the past. Requested time 946655660.000381000 but the earliest data is at time 1568785504.549147331, when looking up transform from frame [velodyne] to frame [world]]", "Also commented in the Autoware forum https", " It seems that TF has not been converted well, because the sensor time stamp is strange.\nPlease modify the sensor driver.", "@Yamato Thanks for your reply, I will check the sensor driver and get back to you."], "question_details": [" ", " ", " ", " ", " ", " ", "my_mapping.launch with tf_launch (with the world to map transform set to 0,0,0 instead of the default values in the usual tf.launch) and the points_map_loader node with the .pcd file I created previously.", "my_sensing.launch with a driver for a 64-plane Ouster LiDAR that outputs a topic /os1_cloud_node/points in PointCloud2 format and a driver for a Microstrain IMU that outputs /gps/fix and translates that into the /fix topic required by ndt_matching (of msg type NavSatFix)", " ", "setup_tf.launch of base_link to os1_lidar", "vehicle_model.launch that loads the urdf file for my vehicle and starts the joint_state_publisher and robot_state_publisher nodes", "points_downsample.launch where it looks at the /os1_cloud_node/points topic (remaped from the /points_raw topic) for point values and I have tried multiple values for the voxel_grid_filter", "fix2tfpose.launch", " "], "answer_code": ["$ rosrun topic_tools relay_field /os1_cloud_node/points /points_raw sensor_msgs/PointCloud2 \"header:  \n seq: m.header.seq  \n stamp: now  \n frame_id: m.header.frame_id  \n height: m.height  \nwidth: m.width  \nfields:  \n- {name: \"x\", offset: 0, datatype: 7, count: 1}  \n- {name: \"y\", offset: 4, datatype: 7, count: 1}  \n- {name: \"z\", offset: 8, datatype: 7, count: 1}  \n- {name: \"intensity\", offset: 16, datatype: 7, count: 1}  \n- {name: \"t\", offset: 20, datatype: 6, count: 1}  \n- {name: \"reflectivity\", offset: 24, datatype: 4, count: 1}  \n- {name: \"ring\", offset: 26, datatype: 2, count: 1}  \n- {name: \"noise\", offset: 28, datatype: 4, count: 1}  \n- {name: \"range\", offset: 32, datatype: 6, count: 1}  \nis_bigendian: m.is_bigendian  \npoint_step: m.point_step  \nrow_step: m.row_step  \ndata: m.data  \nis_dense: m.is_dense\"\n", "height: m.height", "$ rosrun tf tf_echo world base_link"], "url": "https://answers.ros.org/question/328320/autoware-ndt_matching-not-matching/"},
{"title": "Robot Localization output incorrect for movement in Y axis with Vectornav IMU", "time": "2019-09-19 13:39:30 -0600", "post_content": [" ", " ", "Hello,", "I am using Robot Localization to localize rover with an Marvelmind Indoor GPS, Vectornav IMU and Wheel Encoder Odometry.", "For the vectornav imu I am using code set from: ", "I have ensured that the vectornav IMU follow ENU standard. However the output orientation is flipped only on the Y axis (With respect to actual movement in real life) but correct when moving along x axis in the global frame.", "In the test above I moved the rover in a square as representation with the blue arrows. However the ekf output is in yellow and does not match the actual robots orientation only in the Y axis. The robot turns in the correct direction however its just not matching up with the Blue arrows (The real movement of the robot) as its only flipped in one axis.", "When I switch the vectornav imu code to the NED format I get this:", "In which the red arrows is the EKF output and the orange is the real robot movement. How all the arrows are moving in the square pattern that I moved the robot however the actual robot had movement that was vertically flipped. ", "Thoughts? Please let me know if there is more information I can provide for troubleshooting!"], "answer": [" ", " ", "Some thoughts, may be right, may be not.", "Some (nicer) IMUs will have a different coordinate system and you should make sure to check the docs, and I think this is what you may be seeing here. IMUs report things in the inertia frame of reference, so +Z is anti-gravity direction which is pretty much the opposite as most would think. If you're saying that the Y axis seems to be flipped, that's makes me suspect that the frame is actually rotated 180 about the X axis so that the Y is opposing and the +Z is in the \"intuitive\" direction meaning _proper_ acceleration vs. acceleration. ", "Sidenote: That's a very very nice IMU which comes with an extremely powerful SDK, I hope you're making good use of it and not just fusing the raw output to R_L :-)", "So I am matching the outputs to what the robot localization site is recommending reference below. Though I am somewhat confused by terminology that if the \"right side up\" position means that it is just sitting flat.  Or that if the Right side of the IMU is actually facing up. ", "The weird issue is that I am getting correct orientation in the odometry frame of ekf output when fusing just wheel odometry and imu. However when I fuse the ekf  in the map frame to include the indoor GPS map I get the above behavior.", "I'm just confused as to why even though I conform it to the REP standard it is giving me behavior like this in the map frame but not odometry frame.", "IMU Setup Recommendation: \n", "I'd recommend at least trying my suggestion and seeing if that helps. Also keep in mind that R_L recommends to run 2 filters: a local filter of IMU/odom for odom->base_link and then another with GPS+(stuff?) for the map->odom. Make sure you're not trying to dump everything into 1 filter"], "url": "https://answers.ros.org/question/333438/robot-localization-output-incorrect-for-movement-in-y-axis-with-vectornav-imu/"},
{"title": "Using rosserial to read data from a device", "time": "2019-10-02 04:08:32 -0600", "post_content": [" ", " ", "Hello, I am working on ", " on a Virtual Machine (", ") and I need to collect data from a ", " (BMS).", "The connection is made by a USB cable and a ", " so I use a USB port as a serial port.", "I managed to connect the BMS to my virtual machine and I can see it in the list when I type ", " in the terminal of Ubuntu :", "Now my problem is that I need to ", " the data from this BMS and I understood that ", " could help me, but I just started to use ROS one week ago and I don't know how to use this.", "Could you please give me some information about how to use rosserial to read this data please ?", "Thank you !"], "answer": [" ", " ", "Rosserial may not be what you need to use in this case, rosserial is designed to be used with micro-controllers where you are writing the firmware yourself. It gives you access to a limited set of ROS message handling functions directly from the micro-controller code.", "If you're writing the firmware of the battery management system yourself then you could use rosserial for this. If the hardware already has it own firmware and you need to read and write data to the serial port to communicate with it then you'll need to use a conventional serial library for this.", "I've used ", " in C++ for this before, or you could use ", " if you're working in python."], "url": "https://answers.ros.org/question/334408/using-rosserial-to-read-data-from-a-device/"},
{"title": "ROS newby - suitability of old Thinkpad X240?", "time": "2019-11-06 03:02:08 -0600", "post_content": [" ", " ", " ", " ", "Hello", "I am new to Ubuntu and ROS. I am considering to buy a second-hand Thinkpad X240 (year 2013) to get started with Ubuntu and ROS. I will be using it at first to provide a GUI to view a GPS/RTK signal.", "My question is about the 2013-vintage Thinkpad - is that likely to be a problem with latest Ubuntu and ROS or ROS 2.0?", "(Again I am just trying to get some initial experience with low-cost older hardware, before I make an informed decision on newer hardware).", "Thanks much for any advice.", "--- Thank you all below for help. Good to hear that I can proceed, and a great heads-up about rviz."], "answer": [" ", " ", "I've been using an old X220 (2012?) all year as my robot brain (ie it literally sits on the robot base and runs a few ROS nodes for the onboard sensors and the base controller) without issue. For this purpose I would absolutely recommend an old Thinkpad. The only drawback in my case is the battery life, which for mine lasts only 2 hours or so if I'm running a lot of tests. \nI haven't tried to run RVIZ or any other graphics-related programs on it though, and my gut feeling is the older Thinkpads wouldn't be good for that.", " ", " ", "I think you'll have some problems running gazebo and rViz. Also battery upgrade would be good, as ", " mentioned.", " ", " ", " ", " ", "I am writing this on an X230 with Ubuntu 18.04, and it's great! It handles everything really well, ROS has been written quite efficiently. As a starter piece of hardware, it will be great. I got mine for around $200 AUD.", "I would HIGHLY recommend getting a larger battery pack though; as ", " mentions, the battery life isn't great. Something like the following ", "."], "url": "https://answers.ros.org/question/336929/ros-newby-suitability-of-old-thinkpad-x240/"},
{"title": "Connecting a 3d Camera Over a network", "time": "2019-11-08 05:29:45 -0600", "post_content": [" ", " ", "I am trying to connect to Astra Stereo S \u2013 Orbbec remotely. \nSetup:\nThe camera is connected to a PC on the robot and the ", " is running  on a remote PC.  ", "Desired Solution:\nSetting up camera parameters on the internet and retrieving on the Ros parameters and changing the files appropriate to look for the camera parameters.", "I have successfully changed the code (here ", " in ", " and ", " in ", ") to look for the parameters on the network if available. ", "But now the software is trying open device (it is connected on a robot PC and the software is looking in the local machine). I think the changes should be made ", " in ", " which actually takes ", ".", "Currently I am not having the solution which sort of script should I run on the robot PC which allows to open a usb camera and how should I open the device from remote PC. ", "Sorry if the questions is silly (I am not an expert in vision).", "Any suggestions and leads will highly be appreciated."], "answer": [" ", " ", "Hi,\nI hope, I understand your question right.", "I use this powerful framework [ ", " ] for this task.", "My robot has installed a local webserver and I use ros_bridge and web_video_server nodes in combination with a small html/javascript page.", "Easy job to control the robot and use robots cam, sensors and actuators... just sending json to ros_bridge.", "Cheers\nChrimo", "Hi Thank you for your reply. \nI just came back to this after a week so sorry for being late in the reply. \n", " library seems great, but I have a question just after going through the description of the project. Can I send the data over the network without any rosic way to the library because I don't want to run the camera driver on the robot as it will slow down the process."], "question_code": ["OndeviceConnected()", "initDevice()", "OpenCamera()", "UVCCameraConfig"], "url": "https://answers.ros.org/question/337165/connecting-a-3d-camera-over-a-network/"},
{"title": "Ros on two machines, different time stamps", "time": "2019-11-01 15:54:39 -0600", "post_content": [" ", " ", " ", " ", "I have two ", " recorded on two different machines (ROS kinetic Ubuntu 16.04) at the same time. I need to synchronize messages of two bag files in order to be able to manipulate the data, I am using ", ". However, I get no output from approximate synchronizer which I guess is due to different ROS stamps on these two machines. Here is a sample frame stamp from two machines: ", "Machine 1:", "sec  4583 |\nnsec  855689180", "Machine 2:", "sec  1570727234 |\nnsec  4864000", "While I know the format for machine 2, I am not sure what format is the time being written on Machine 1, but I need to synchronize these two. My first question is that if ", " can solve this issue? I tried checking the time difference by the command ", ", yet it looks good to me:", ".", "If ", " wouldn't be helpful, how can I get these two machines clocks under the same format."], "answer": [" ", " ", "Machine 1:", "sec 4583 | nsec 855689180", "Machine 2:", "sec 1570727234 | nsec 4864000", "It would appear likely that \"machine 1\" was running with ", " set to ", " while \"machine 2\" was using the regular system (wall) clock.", "Were you running a simulation on \"machine 1\"?", " This is the scenario: I got two LiDARs on two different machines, each machine runs its own ROS Kinetic on Ubuntu 16.04. So on each machine I recorded ", " including ", " topic. The time stamp you see above is for when I play those two ", " files on a third machine and ", ". Since those two bag files have been recorded for the same duration of time, I expected to get close enough time stamps and under the same format, so that I can sync two topics.", "I checked ", " param, it was not configured at all. So I issued ", " command, then set ", " to false and then fired up my sensor again. I checked the time stamp of the generated topic, still the short one which is ", ". How can I eliminate this time format ?", " gets its clock from the system clock, unless ", " is set to ", ". The first thing I would check is whether Machine 1 has a properly functioning system clock. Is it an embedded platform? Some other special clock configuration (ie: external, GPS synced, battery backed RTC, etc)?", "What is the output of ", " and ", " on Machine 1 and Machine 2?"], "question_code": ["ROSBAG", "message_filters/sync_policies/approximate_time.h", "chrony", "chronyc tracking", "System time     : 0.000291987 seconds fast of NTP time", "chrony"], "answer_code": ["use_sim_time", "true", "rosbag", "/points_raw", "rosbag", "echo /points_raw", "/use_sim_time", "roscore", "/use_sim_time", "sim_time", "ros::Time()", "use_sim_time", "true", "lsb_release -a", "uname -rmvp"], "url": "https://answers.ros.org/question/336638/ros-on-two-machines-different-time-stamps/"},
{"title": "Issues with Arduino:  Wrong checksum, Mismatched protocol version", "time": "2019-12-25 23:26:40 -0600", "post_content": [" ", " ", "I am trying to run base controller on an arduino uno, which will run the motors by subscribing to topics for left and right motor speeds which come through the Twist messages, and also publishes encoder values to topics ", " and ", "\nWhen I run the arduino rosserial by connecting it to my PC, there is no problem and everything works fine.\nHowever, when I connect the uno to my raspberry pi, which is connected to my PC, problems occur. I notice the following messages and errors on my terminal running the command ", " :", "These messages are repeated over and over. I also notice weird behavior in the robot itself. There is random latency between my key press and actual movement, and the longer I press the key, the longer it continues that movement \"after\" I release the key.", "I thought it might be an issue with the uno's dynamic memory size. So I shifted to arduino mega. It uses 28% of its dynamic memory, ", " I don't think its an issue with buffer as well, because ", ".\n", " If yes, then why isn't that causing problems when I have no publishers? (The uno works perfectly well when the code is just subscribing to speed topics and running motors, even when connected to pi).", "Arduino Code:", "I'm not an arduino expert, but you appear to be using ", " at the same time, with the same serial port.", "That is not supported.", "Remove the ", " and ", " calls and try again.", "Hi ", " that wasn't a problem when just running motors. However, I'll try that", " I tried your suggestion, but still the same problem.", "Regardless of whether it solves your problem, you cannot use ", " together with ", ". Unless you've changed the serial hw ", " should use (on the Arduino side, not the ROS side)."], "answer": [" ", " ", "I would try with a larger delay in the main loop - try 100 milliseconds rather than 10 Microseconds and see if that helps", "Thanks ", "! I changed delay in loop to 10 ms and it works perfectly!", "Great - please click on the tick to show that the answer has solved the problem", "Sure, can you just help me know why did the delay cause problem in distributed system and not when running arduino directly through pc?", "Sure.", "Sending the message with a delay of 10 microseconds between messages means you were sending very high number of messages/second, roughly a thousand times more than with a 10 millisecond delay (the time spent running other code will have a more significant effect on the very short delay rate, so it would not be exactly a thousand times). Your pc will most likely be a much faster computer than the pi, which means it would be able to process the data that has arrived faster than the raspberry pi could. So it was able to process each message before the next one arrived. The less powerful pi was not. If it is not processing them faster than they arrive then messages accumulate in the buffer at the receiving end, and this very quickly fills up, causing loss of data and errors. In addition to the slower speed the ...", "As the microcontroller doesn't have an operating system it can perform simple tasks at a much higher and more reliable rate  than a computer with an operating system can as it is not constantly switching tasks in the way the operating system is, so it is very easy to accidentally send data at rates that will overwhelm the computer that is receiving them - I have often seen computers brought to a standstill because  serial data is being sent to them by a microcontroller with a sensor reading, serial print and no delay in the main loop", "That's really helpful. Thanks a lot!"], "question_code": ["/lwheel", "/rwheel.", "rosrun rosserial_python serial_node.py /dev/ttyACM0", "[INFO] [1577335551.127150]: Connecting to /dev/ttyACM0 at 57600 baud\n[INFO] [1577335553.249219]: Requesting topics...\n[INFO] [1577335553.290434]: Note: publish buffer size is 512 bytes\n[INFO] [1577335553.298687]: Setup publisher on lwheel [std_msgs/Float32]\n[INFO] [1577335553.321495]: Setup publisher on rwheel [std_msgs/Float32]\n[INFO] [1577335553.348951]: Note: subscribe buffer size is 512 bytes\n[INFO] [1577335553.357481]: Setup subscriber on left_wheel_speed [std_msgs/Float32]\n[INFO] [1577335553.386113]: Setup subscriber on right_wheel_speed [std_msgs/Float32]\n[INFO] [1577335556.177063]: wrong checksum for topic id and msg\n[INFO] [1577335559.040959]: wrong checksum for topic id and msg\n[ERROR] [1577335561.888282]: Mismatched protocol version in packet ('\\x00'): lost sync or rosserial_python is from different ros release than the rosserial client\n[INFO] [1577335561.896526]: Protocol version of client is unrecognized, expected Rev 1 (rosserial 0.5+)\n[INFO] [1577335564.716741]: wrong checksum for topic id and msg\n[ERROR] [1577335579.781605]: Lost sync with device, restarting...\n[INFO] [1577335579.790889]: Requesting topics...\n[INFO] [1577335579.826395]: Setup publisher on lwheel [std_msgs/Float32]\n[INFO] [1577335579.839117]: Setup publisher on rwheel [std_msgs/Float32]\n[INFO] [1577335582.842464]: wrong checksum for msg length, length 4\n[INFO] [1577335582.850964]: chk is 0\n", "#include <ros.h>\n#include <std_msgs/Float32.h>\n#include \"Arduino.h\"\n\nros::NodeHandle nh;\n// Left encoder\n\nint Left_Encoder_PinA = 2;\nint Left_Encoder_PinB = 9;\n\nvolatile long Left_Encoder_Ticks = 0;\n\n//Variable to read current state of left encoder pin\nvolatile bool LeftEncoderBSet;\n\n//Right Encoder\n\nint Right_Encoder_PinA = 3;\nint Right_Encoder_PinB = 10;\nvolatile ...", "Serial", "rosserial", "Serial.begin()", "Serial.println(..)", "Serial", "rosserial", "rosserial"], "url": "https://answers.ros.org/question/340675/issues-with-arduino-wrong-checksum-mismatched-protocol-version/"},
{"title": "move_base strange behaviour when using earth referenced heading", "time": "2019-04-16 01:47:43 -0600", "post_content": [" ", " ", " ", " ", "Hey guys,", "I am performing gps waypoint navigation using ", " for the navigation side of things. My localization setup uses ", " i run two instances of this in order to fuse the gps, so first instance has wheel odom + IMU and second instance has wheel odom + IMU + GPS. ", "My IMU does not provide an earth referenced heading so i have a gnss heading receiver setup and i pipe the heading into a Pose message of the following format:", "I have two pose messages setup, one for the odom frame and one for the map frame ", " are the exact same but with different ", ". I fuse the earth reference heading into both ", " nodes in the following way:", " Whenever i fuse in the earth referenced heading, record some GPS waypoints and attempt to navigate i get a very strange behaviour! see below:", " When i ", " fuse the earth reference heading. the navigation works fine????? see below:", "Does anyone know why this would be happening? I remap the ", " odom topic to my ekf filtered output:", "And i have checked this topic's output with both fused gnss heading and without and they seem to be fine?", "I have run the outputs of some bag files i recorded of a figure 8 movement of both ", " nodes and the gps odometry output of ", " through matlab to ..."], "answer": [" ", " ", "I have solved this issue, My GNSS Heading is provided in terms of an angle between 0 and 360 degrees True North, i then cast this to a domain between pi and -pi for the Pose message. The way i was casting this angle to the new domain was incorrect and hence the gps odometry was showing an incorrect orientation which when fused with the ekf was applying that orientation to the ouput meaning the robot was getting lost.", "hi,friends! what's the data of /gps/odometry_odom? i have the same problem.i use 9 aixs imu + GPS + odom,but i can not get the origin pose when the power on,Could you give me some suggestions?Thank you"], "question_code": ["move_base", "robot_localization", "frame_id", "robot_localization", "ekf_se_odom:\n  frequency: 20\n  two_d_mode: true\n  sensor_timeout: 0.15\n  transform_time_offset: 0.0\n  transform_timeout: 0.0\n  print_diagnostics: true\n  debug: false\n\n  map_frame: map\n  odom_frame: odom\n  base_link_frame: base_link\n  world_frame: odom\n\n  odom0: /warthog_velocity_controller/odom\n  odom0_config: [false, false, false,\n                 false, false, false,\n                 true, true, false,\n                 false, false, true,\n                 false, false, false]\n  odom0_queue_size: 10\n  odom0_nodelay: true\n  odom0_differential: false\n  odom0_relative: false\n\n  # GNSS HEADING POSE MESSAGE (ODOM)\n\n  pose0: /gps/odometry_odom\n  pose0_config: [false, false, false,\n                 false, false, true,\n                 false, false, false,\n                 false, false, false,\n                 false, false, false]\n  pose0_queue_size: 10\n  pose0_nodelay: true\n  pose0_differential: false\n  pose0_relative: false\n\n  imu0: /mcu_imu/data\n  imu0_config: [false, false, false,\n                false, false, false,\n                false, false, false,\n                false, false, true,\n                true, true, false]\n  imu0_differential: false\n  imu0_nodelay: false\n  imu0_relative: false\n  imu0_queue_size: 10\n\n  use_control: false\n\nekf_se_map:\n  frequency: 20\n  sensor_timeout: 0.15\n  two_d_mode: true\n  transform_time_offset: 0.0\n  transform_timeout: 0.0\n  print_diagnostics: true\n  debug: false\n  debug_out_file: \"/home/alec/debug_ekf.txt\"\n\n  map_frame: map\n  odom_frame: odom\n  base_link_frame: base_link\n  world_frame: map\n\n  odom0: /warthog_velocity_controller/odom\n  odom0_config: [false, false, false,\n                 false, false, false,\n                 true, true, false,\n                 false, false, true,\n                 false, false, false]\n  odom0_queue_size: 10\n  odom0_nodelay: true\n  odom0_differential: false\n  odom0_relative: false\n\n  # GNSS HEADING POSE MESSAGE (MAP)\n\n  pose0: /gps/odometry_map\n  pose0_config: [false, false, false,\n                 false, false, true,\n                 false, false, false,\n                 false, false, false,\n                 false, false, false]\n  pose0_queue_size: 10\n  pose0_nodelay: true\n  pose0_differential: false\n  pose0_relative: false\n\n  odom2: /bunkbot_localization/odometry/gps\n  odom2_config: [true,  true,  false,\n                 false, false, false,\n                 false, false, false,\n                 false, false, false,\n                 false, false, false]\n  odom2_queue_size: 10\n  odom2_nodelay: true\n  odom2_differential: false\n  odom2_relative: false\n\n  imu0: /mcu_imu/data\n  imu0_config: [false, false, false,\n                false, false, false,\n                false, false, false,\n                false, false, true,\n                true, true, false]\n  imu0_differential: false\n  imu0_nodelay: true\n  imu0_relative: false\n  imu0_queue_size: 10\n\n  use_control: false\n", "move_base", "<remap from=\"odom\" to=\"/bunkbot_localization/odometry/filtered_map\" />\n", "robot_localization", "navsat_transform_node"], "url": "https://answers.ros.org/question/321194/move_base-strange-behaviour-when-using-earth-referenced-heading/"},
{"title": "Local planner behaves unordinary", "time": "2019-01-22 06:22:47 -0600", "post_content": [" ", " ", "Hello,", "I am using base_local_planner (TrajectoryPlannerROS with DWA enabled) and it works in general, it sends the commands to cmd_vel and my robot interprets them quite good using this simple inverse kinematics formula (I am cancelling negative velocities as my robot cannot handle them well):", "But when I set a goal, the local planner publishes short and strange plans. Even though it does like that, sometimes my robot is able to get to the goal and stop but sometimes it collides to the objects.", "Here is the video: ", "The red line is the local planner and green line is the global planner.", "Here is my TrajectroyPlannerROS params:", "recovery_behavior_enabled: true\nclearing_rotation_allowed: true", "The behaviour is a result of preventing negative velocities. For a differential drive to turn clockwise in place, you will need positive left wheel velocity and negative right wheel velocity. The local planner is drawing an arc towards the goal but because it isn't being executed, it has to replan.", " But how can I enable negative velocities and also state that small values (both negative and positive) should be greater than 1.2? Because if I put min_vel_theta -1.2, it produces angular velocities like 0.08 and my robot does not move (too low velocity)"], "answer": [" ", " ", "I ran across you question because regardless of what I put in for min_vel_theta and min_vel_trans it always gets reset to 0. This is despite what the the website says the default value is ie -1. You are correct that in order to turn clockwise your cmd_vel needs to have z  a negative value. if you can not produce a negative the planner will not turn that way.  I am going to try to hard code some values. ", "Also I see your   min_vel_theta: 3.0 this should be negative or at least less than max_vel_theta: and your min_vel_x should also be negative or at least  less than max_vel_x: Also note that if you have  dwa: true then sim_time parameter is not used but it used the controller_frequency instead.", "Now for the question of needing 1.2 or -1.2 versus .08 or -.08. I think this may not be a planner issue but a base controller issue. What I mean by that is your robot is not correctly translating a requested velocity to the correct PWM value to make the robot move.  Most examples for motor control that I have seen assume that pwm values are linear. Its not. there is a minimum value needed get the robot to overcome inertia. then a usually lower minimum value to keep it moving. The problem is these values are not constant. They depend on the type of surface your robot is moving on and the state of your battery.  I was experimenting with my large outdoor bot on both a indoor surfaces (smooth concrete and thin carpet) and outdoor (driveways and pathways)  and on the indoor surface a value of 235 out of 124 would get the bot rolling but outdoors it would require 350 to 375 on flat and over 400 for any kind of incline."], "question_code": ["v_right = (cmd_angular_z*self.ROBOT_BASE_LENGTH)/2.0 + cmd_linear_x\nv_left = cmd_linear_x*2.0 - v_right\n\nif v_right < 0:\n    v_right = 0\n    v_left *= 2\nelif v_left < 0:\n    v_left = 0\n    v_right *= 2\n", "TrajectoryPlannerROS:\n  acc_lim_x: 1\n  acc_lim_y: 0 # as we have a differential robot\n  acc_lim_theta: 1.6\n  max_vel_x: 0.6 # configure, was 0.2\n  min_vel_x: 0.2 # was 0.4\n  max_vel_y: 0.0  # zero for a differential drive robot\n  min_vel_y: 0.0\n  max_vel_theta: 1.6 # 0.35\n  min_vel_theta: 3.0 #-0.35\n  min_in_place_vel_theta: 3.0 #0.25\n  min_rot_vel: 3.0\n  escape_vel: 0.0\n\n  holonomic_robot: false\n\n  meter_scoring: true\n\n  xy_goal_tolerance: 0.3 # 0.15, 0.3->about 20 cm\n  yaw_goal_tolerance: 0.20 # 0.25, about 11 degrees\n  latch_xy_goal_tolerance: true  #false\n\n  sim_time: 3 # <2 values not enough time for path planning and can't go through narrow\n  sim_granularity: 0.05 # How frquent should the points on trajectory be examined\n  angular_sim_granularity: 0.025\n  vx_samples: 20\n  vy_samples: 0 # 0 for differential robots\n  vtheta_samples: 5 # 20\n  controller_frequency: 15\n\n  pdist_scale: 0.8\n  gdist_scale: 0.4\n  occdist_scale: 0.01\n  oscillation_reset_dist: 0.2\n  heading_scoring: true\n  heading_lookahead: 1 # how far to look ahead in meters\n  heading_scoring_timestep: 0.8\n  dwa: true\n  publish_cost_grid_pc: false\n  global_frame_id: odom\n  simple_attractor: false\n\n  reset_distance: 4\n\n  prune_plan: true\n"], "url": "https://answers.ros.org/question/313363/local-planner-behaves-unordinary/"},
{"title": "Is redistributing ROS2 allowed?", "time": "2020-01-14 01:12:40 -0600", "post_content": [" ", " ", "Hi, I have a question about ROS2 and licensing.\nI would like to create a Python (pip) package for a built version of ROS2 (version Dashing) to allow for simple installation.\nThis package would then be publicly available through a URL (for free, of course).\nIs this allowed under the current license, and are there any caveats? ", "It is my understanding that the components that build up ROS2 are all under the Apache 2.0 license, and I have not found any \"NOTICE\" files in any of them.", "Thank you in advance!", "I'll explicitly post this as a comment, and not as an answer, as this is ", " a legally binding advice about the question you ask. Also, you'll not receive any legal advice here that you'll be able to point to and say \"they told me that it is okay...\"", "Now that the disclaimer is out of the way...", "Is this allowed under the current license", "Yes, given that you follow all conditions that the license puts on you (see ", " for some hints)", "are there any caveats", "Sure, there might be clashes with packages installed via the regular package manager, depending on how you set that up. And any you'll need to update your package for every new release of a ROS2 package. To just name a few (you'll probably guess that I don't think what you try to do is a very good ...", "license. And those licenses don't need to be the same. It is actually fairly common that some packages, even distributed by the OSRF, don't have the recommended license (take ", " in ROS1 as an example). So you'll need to check this on a package-per-package basis and comply with the license terms for each package!", "I have not found any \"NOTICE\" files in any of them", "Having a \"NOTICE\" file is not a requirement by the Apache 2.0, nor is distributing a copy of this the only obligation from the Apache 2.0 during redistribution. Check paragraph 4 of ", ".", "So, long story short:\nWhy do you plan to do this in the first place? I guess there are better ways to do this. But to help with that, you'll need to explain in detail your line of thought about proceeding in ...", "Ignoring the legal aspect to this, just some links to ongoing discussions and efforts around \"easy distribution\" of ROS (2) packages in \"a single package\" (although not necessarily limited to that):", "And it would indeed be good to have a little more information about your intentions ", ". Do you have any particular reason for wanting to do this? Or just because it's nice?", "+100 for asking this question btw: licensing is not the first thing people think of when looking into this, but it's certainly a very important aspect of the problem."], "answer": [" ", " ", "Like any open source project you can redistribute ROS. ROS is not monolithic so you will have to pay attention to the licenses of all components that you include. ", "Overall packaging takes a lot of ongoing work and as an individual I would suggest that it will not something that's sustainable without a community effort. As ", " mentions there are several community discussions ongoing and it likely makes more sense to get involved with those than to try to go at it on your own.", "Along similar lines please consider the impact that you might have if you name grab package names that then would prevent or interfere with the community projects or upstream maintainers from releasing packages as well if they choose to in the future.", "And as ", " mentions I would strongly suggest that you ask a little bit more specifically about what you're looking to do, before jumping to a conclusion about the best way to solve it and asking for specific help on that problem."], "question_code": ["gmapping"], "url": "https://answers.ros.org/question/341661/is-redistributing-ros2-allowed/"},
{"title": "Is it correct to set topic names from parameters?", "time": "2020-01-28 21:17:07 -0600", "post_content": [" ", " ", "This is a mix of a practical question and an academic question.", "When, if ever, is it correct to set topic names from parameters? If it is, how do you handle aspects such as the default topic name and clashes with remappings in launch files?", "Just a quick note: if remapping worked slightly better (", ", ", " and ", " fi) I would actually answer \"never\" to your question. As it is, using parameters ", " sometimes be the faster option to \"get something to work\".", "Remapping is part of configuration of the application, not the node. It's an activity that happens outside of the node itself, as it is part of the composition step of your application.", "If you start using parameters for this, separation of concerns seems violated to me, as suddenly nodes become responsible for 'knowing' how to integrate into a composite. That's not something I'd like.", "Edit: just noticed the ", " tag. As there is also a ", " tag, I linked to issues in ", ".", "I tried to start a conversion and I was down voted for it :(", "The voting here is whether you agree or disagree with the stated answer to try to build a consensus on what is the best approach. It's not a measure of the value of your statements or the validity of your concerns. But if an individual doesn't think that the answer is something that someone in the future should follow that's why they would downvote it."], "answer": [" ", " ", "My quick answer is \"only when the number of topics being used is unknown\". For example, in the navigation stack where you use parameters to control which sensor topics are subscribed to in order to detect obstacles.", "At all other times I think remapping should be used.", "However, if there are cases where remapping is not powerful enough, then parameters might be appropriate, but I would say that's a work around and instead we should try to improve remapping to allow for more complex cases. For example, in ROS 2 we described how you could use regex-like behavior in remapping, e.g. remap ", " to ", "(see ", "), which more cases than were allowed in ROS 1.", "I would say that's a work around and instead we should try to improve remapping to allow for more complex cases", "This is my feeling, too.", " ", " ", " ", " ", "I'll go with a couple examples", "Situation 1: You have a configurable number of sensors you'd like to subscribe to. You don't know at compile time what they are, or how many there are going to be. Examples: Navigation stack's voxel layer allowing many sensors. Remaps don't make sense because you'd then have to create unbounded ", " subscribers for every sensor topic because you don't know how many of ", " sensors a user might want. Another example in SLAM Toolbox allowing for multiple laser scanners for a similar purpose. Now, once you make a parameter for one, its inconsistent to not do them for all.", "Situation 2: You have created something similar to a ROS1 nodelet that is intended to be part of a nodelet chain. You have a multi-stage processing pipeline for a depth camera or something for filtering or data processing. In that case, you have a fixed input and a fixed output for each stage, probably just manipulating the same data. In this situation, you could remap everything, but that's very, very ugly and complex in a launch file. I never want my complexity in an XML file I have to sit there and read through a debug. I will always create those topics as parameters so that there's a configuration file that has the namespaced node with the ", " and ", " so that when I clearly read a configuration file, I can clearly see what's input goes to what output. And this is especially compact if there's a directional graph like structure to your processing pipeline. A simple yaml file is easier to read and maintain than an XML file with 10 stages. This situation is getting a little ranty but one more point: would you rather have 5 yaml files for different configurations or 5 launch files with different configurations? ", "Situation 3: You're writing something for outside consumption. That can be \"outside\" as in outside your organization (ei open source) \"outside\" as in outside this project (ei an external facing topic for the Navigation project like a sensor or output result). If its ", " \"outside\" in one of these ways,  I will probably not create a parameter because the likelihood of me needing to change it are slim to none. And for the case I do, remapping is still available. ", "If a package contains any of these situations, I automatically parameterize all other topic names for ", ". Once you do one, it would be inconsistent to not do it with all. Since output topics are still consistent (a map is a map is a map, and only one of them, etc) I would not parameterize the output topic. ", "tl;dr, by my use, I pretty much always parameterize inputs, but nearly never do outputs, unless in Situation 2. Typically, I will parameterize topics related to sensor data almost instinctively because its \"outside\". I deal with clashes of remapping by not using ...", "My comments on your comments:", "re: situation 1: that seems like a slightly different case, as the topic names are not \"set from parameters\" I feel. You have ", " items as part of a parameter dict, which then causes, eventually, ", " topics to be created.", "re: situation 2: remapping with nodelets in a processing pipeline: that's actually exactly what the pattern is that is very often used :) See the ", " fi. They all have fixed ", " and ", " topics which get remapped to connect everything together.", "I never want my complexity in an XML file I have to sit there and read through a debug. ", "so now you split the complexity between two places: the logic in the nodelet to create the topics based on parameters and the ", " + ", " file that loads the parameters. I've seen many things go wrong with ", " and ", " files ...", "Re-reading your answer ", ", this:", "I deal with clashes of remapping by not using remapping. I see that mostly as a debug tool and shouldn't be used in production settings when it can be helped.", "is a really strong opinion. Could you describe a bit more the \"clashes of remapping\"? I'm not sure I understand that.", ": could you perhaps give an example of how parameters would solve \"clashes with remappings in launch files\" (seeing as you brought it up)?", "If remapping is considered part of an application level (or subsystem level) configuration activity, then I'm not entirely sure how clashes could occur. The \"hand of god\" putting the system together should be aware of those (and if perhaps not a priori, at least a posteriori).", "I deal with clashes of remapping by not using remapping. I see that mostly as a debug tool and shouldn't be used in production settings when it can be helped.", "is a really strong opinion.", "I agree that this an atypical opinion in my experience, and I actually think that if you're not using remapping then your nodes are not designed to be reused. That might acceptable in some cases, but generally I wouldn't recommend it.", "For example, a driver node for a laser should publish to ", " not ", ", and remapping should be used to setup the topic name as you want. The author of the node should keep it generic, and users should not edit the source of the driver just to avoid using remapping.", "Also if your node is using parameters to determine the topic name, e.g. two ...", "I believe that configurations have no place in the launch system. Remapping implies that you have robot specific configurations inside of launch files. I think a certain level of configuration within the launch files are fine, things that are either direct arguments or derivatives of arguments into the launch file, or pulling from environmental variables, etc. ", "What I don't want, is hardcoded values in my launch files that makes them hard to be reused. If there's a configuration element, I want that in a parameter file that is easier to control and version control. The most frustrating type of debugging I do is when I have to read through XML and find that there was a remap and either its wrong, inconsistent with documentation, or misspelled. This was a major point actually from the ROSCon 2019 talk on common bugs found in ROS code. ", "Atypical?, that's my ...", ", my assertion is that the code itself should have supported a topic parameter to not require the need for a remap. Not that you've remapped a topic in a launch file, it is now a hard-coded parameterization outside of the configuration files you need to keep an eye on if you are trying to share a codebase with potentially dozens of variations of robots. I find that additional thing to be unscalable.", "The 2 situations you mention are both solved by parameters, that doesn't create launch dependencies on configurations.", ": if launch files are not what orchestrates your ROS application, what do you use? Because I believe that is the level ", " was mostly considering when he asked the question.", "You seem to be thinking of launch files that wrap one or two nodes.", "I still don't really understand how reading a ", " is any easier than an ", " file, but that may be personal.", "Well I think that asks another question that's not directly related. I'm talking about launch files that wrap one or two nodes, sure, but when you wrap then those in a launch file to bring up a robot / full application, you're largely missing reuse opportunities if there's remaps in the launch files. You then require to have different launch files for different configurations of robots rather than being able to have the nodes configure themselves based on parameters or globals. ", "XML contains the configurations of remaps, sure, but also contains a bunch of other stuff. If you're trying to change a configuration or debug a problem for why X data isn't getting to Y node, then all the other stuff (that I'd argue is what launch files are meant for) is \"in the way\" for configuration debug and management.", ": could you perhaps give an example of how parameters would solve \"clashes with remappings in launch files\" (seeing as you brought it up)?", "I don't think they would. If you have a parameter to set a node topic, and then someone tried to use a remapping, I'm not sure which would take priority and either way it would lead to confusion. Not using the tool designed for the task you are doing to avoid clashing with your use of another tool to do that task seems backwards to me."], "question_code": ["eloquent", "melodic", "ros_comm"], "answer_code": ["/front/*/image_raw", "/front_new/\\1/image_raw", "N", "N", "input_topic: XYZ", "output_topic: ABC", "N", "N", "pcl_ros", "input", "output", ".launch", ".yaml", ".launch", ".yaml", "scan", "/my/namespace/for/my/project/scan_unflitered", ".yaml", ".xml"], "url": "https://answers.ros.org/question/342777/is-it-correct-to-set-topic-names-from-parameters/"},
{"title": "How to retrieve data from xv-11 lidar using rviz?", "time": "2018-08-02 08:13:54 -0600", "post_content": [" ", " ", "I want to work with my xv-11 lidar but I am a newbie in ros, rviz. I connected the lidar succesfully and can see surroundings, data from it. Now I want to do a simple system that if it detects objects closer than 1 meter it will light red led connected with arduino, if there is no object closer than 1 meter, green led will be on. How can I define my function for getting that data from rviz? Can you at least give me a roadmap.  ", "I am using odroid xu4, lidar xv-11, ubuntu 18.04 with ros melodic."], "answer": [" ", " ", " ", " ", "Hi, ", "I suppose that lidar you use provides LaserScan.msg type, which has fields as given in ", "for your purpose ,to find points closer than 1m you can use a callback as following; ", "so after each call your green and red flags will be updated. You can use those flags to do the task you described."], "answer_code": ["       YourClass::YourClass(){\n\n          bool red(false),green(false);\n          float boundary = 1.0;\n          lidar_sub_ = node_handle_ptr_->subscribe(\"lidar_topic\", 1,&YourClass::callback, this);\n\n       }\n\n\n       void YourClass::callback(const sensor_msgs::LaserScanConstPtr &scan){\n          red = false;\n          green = true;\n          for (int i = 0 ; i < scan->ranges.size(); i++){\n            if(scan->ranges[i] < boundary){\n              ROS_INFO(\"A POINT CLOSER THAN BOUNDARY FOUND\") ;\n              red = true;\n              green = false;\n              break;\n            }\n          } \n        }\n"], "url": "https://answers.ros.org/question/299440/how-to-retrieve-data-from-xv-11-lidar-using-rviz/"},
{"title": "What is the difference between Vector3 and point message? Do they contain the same data values?", "time": "2016-06-03 14:56:48 -0600", "post_content": [" ", " ", "I made a tf listener which gets a transform of type tf::StampedTransform between two given frames.\nThen this transform is converted to ros message type from ros tf type via the function tf::transformStampedTFToMsg().\nIt gives a message of type geometry_msgs::TransformStamped which contains Vector3 translation and not point translation.", "What I need ultimately is the position of child frame origin w.r.t. the parent frame origin. And I am intending to use the information provided in the Vector3 translation as it is, assuming that it contains exact values for the translation required to move from the parent frame origin to the child frame origin. Please correct me if I am wrong.", " I got confused after reading out from this link:\n ", " \nthat Vector3 only provides direction??? So does it mean that the magnitude values are just raw values? If not, then they should be the same as in point translation, no? "], "answer": [" ", " ", " ", " ", "This question was answered ", ". Quoting to preserve the answer in case the link ever goes down:", "Even though Point and Vector3 have the same content, the tf2 library checks the type of message and acts on it differently, depending on whether it is a Point or a Vector3.", "Ultimately, this was a decision made by the developer to highlight the conceptual distinction between a Point (a point in space which cannot be rotated), and a Vector3 (a direction which has no definite location in space but which can be rotated).", "The developer did not have to make this distinction, and in fact many physics libraries use a single Vector3 datatype to handle directions, points, velocities, etc.", "As with Colors (red, green, blue) and Dates (year, month, day) and many other things which can be represented as a Vector3, ultimately it comes down to the need to draw artificial distinctions to make the code easier to understand for users and other programmers.", "From the documentation in the ", "This represents a vector in free\n  space. It is only meant to represent a\n  direction. Therefore, it does not make\n  sense to apply a translation to it\n  (e.g., when applying a generic rigid\n  transformation to a Vector3, tf2 will\n  only apply the rotation). If you want\n  your data to be translatable too, use\n  the geometry_msgs/Point message\n  instead.", "I feel it's important to draw attention to this part of the answer on SO:", "The developer did not have to make this distinction, and in fact many physics libraries use a single Vector3 datatype to handle directions, points, velocities, etc.", "while technically there is certainly no difference in the ", " between a point and a vector3 (although only in a 3D space), the ", " are vastly different. ", " is what is important here, and that's the reason why we have different message types in ROS.", "It's not about form, it's about meaning."], "url": "https://answers.ros.org/question/236046/what-is-the-difference-between-vector3-and-point-message-do-they-contain-the-same-data-values/"},
{"title": "Gumstix, uvc_camera and cv_capture", "time": "2011-02-14 16:36:20 -0600", "post_content": [" ", " ", "I've observed something surprising with a small UVC camera on the\ngumstix overo. If I use the camera_node from uvc_camera, I get barely\n2 fps in 640x480.\nOn the other hand, if I make a small application using the opencv\ncvCapture function, I get close to 10fps at the same resolution.", "Here is the cv code in all its complexity:", "On the same camera on my laptop, both cv_capture and uvc_camera\npublish at 10 fps.", "I guess that means there is something that would need optimisation in\nthe uvc_camera/camera_node, but for now I'll be fine with just\nreporting :-)"], "answer": [" ", " ", "I've seen similar discrepancies in performance from the driver based on computer as well. See ", " (though in that situation, all of the computers in question had enough CPU horsepower to not drop the framerate).", "Could you report which camera you are using? Also, no idea if you can get a monitor on the overo, but if you can, you could use guvcview to see if that also experiences a slow down in frame rate when you try to use similar settings to the uvc_camera driver."], "question_code": ["       CvCapture* capture = 0;\n       capture = cvCaptureFromCAM( -1 );\n       IplImage *cv_image = NULL;\n       ros::Rate loop_rate(10);\n       while (ros::ok())\n       {\n           cv_image = cvQueryFrame(capture);\n           try {\n               image_pub_.publish(bridge_.cvToImgMsg(cv_image, \"bgr8\"));\n           } catch (sensor_msgs::CvBridgeException error) {\n               ROS_ERROR(\"error\");\n           }\n           ros::spinOnce();\n           loop_rate.sleep();\n       }\n\n       cvReleaseCapture(&capture);\n"], "url": "https://answers.ros.org/question/9055/gumstix-uvc_camera-and-cv_capture/"},
{"title": "How can I communicate with mbed at high speed(1khz)? [closed]", "time": "2018-06-24 05:01:39 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I want to commicate mbed LPC1768 mbed at high speed(1khz) by rosserial.", "So,I test the simple program to measure communication speed.", "This is the mbed program", "I use rosserial_python.", "I use rostopic hz to measure the communication speed.", "I can communicate at 500hz.But, I change", "flipper.attach_us(&flip,1000);//1khz", "I get the message \"wrong checksum for topic id and msg\" and ", "I can't communicate at 1khz.", "So,can I communicate with mbed at high speed(1khz)?", "(I use Ubuntu16.04LTS,kinetic,ros_lib?kinetic(https://os.mbed.com/users/garyservin/code/ros_lib_kinetic/))", "rosserial was not made to handle very high frequency topics but you can try to investigate what is the bottleneck. I think it can be:", "[...]", "The result will be interesting to see what should be improved.", "Thank you for your advises.", "I can communicate at 750hz by using indigo version rosserial_python(more simple code than kinetic).\nSo, I think bottleneck is python efficiency.", "I try rosserial_server ,but I get the message ", "and I cannot communicate.", "Do you know the way to solve this error?", " Argh, this error is known but unresolved:  ", " \nUsually, I reboot the computer and do not use rosserial_python before to avoid any conflict in the serial port communication. ", "I reboot the PC,I can use rosserial_server.And I can communicate at 1000hz(1khz)!(I need to change \"ros_spin_interval\" from milliseconds(10) to milliseconds(1) in \"session.h\")\nThank you for your help."], "answer": [], "question_details": [" ", " ", " ", " ", "serial usage (use a logic analyzer to see if the bus is already too busy)", "rosserial_python efficiency (you can try rosserial_server, written in C++)", "rosserial_mbed implementation\n", "your MCU compute power"], "question_code": ["/*\n * rosserial Publisher Example\n * Prints \"hello world!\"\n */\n\n#include\"mbed.h\"\n#include <ros.h>\n#include <std_msgs/String.h>\n\nros::NodeHandle  nh;\nTicker flipper;\nstd_msgs::String str_msg;\nros::Publisher chatter(\"chatter\", &str_msg);\n\nchar hello[13] = \"hello world!\";\n\nvoid flip(){\n        str_msg.data = hello;\n        chatter.publish( &str_msg );\n        nh.spinOnce();\n        }\n\nint main() {\n    nh.getHardware()->setBaud(921600);\n    nh.initNode();\n    nh.advertise(chatter);\n    flipper.attach_us(&flip,2000);//500hz\n    while (1) {\n    }\n}\n", "$ rosrun rosserial_python serial_node.py /dev/ttyACM0 _baud:=921600\n", "$ rostopic hz chatter\nsubscribed to [/chatter]\naverage rate: 500.280\n    min: 0.001s max: 0.004s std dev: 0.00055s window: 499\naverage rate: 500.137\n    min: 0.001s max: 0.004s std dev: 0.00054s window: 999\naverage rate: 499.996\n    min: 0.001s max: 0.006s std dev: 0.00056s window: 1500\n", "$ rosrun rosserial_python serial_node.py /dev/ttyACM0 _baud:=921600\n[INFO] [1529843960.239399]: ROS Serial Python Node\n[INFO] [1529843960.246924]: Connecting to /dev/ttyACM0 at 921600 baud\n[INFO] [1529843962.352072]: Requesting topics...\n[INFO] [1529843962.363132]: Note: publish buffer size is 512 bytes\n[INFO] [1529843962.364404]: Setup publisher on chatter [std_msgs/String]\n[INFO] [1529843963.201945]: wrong checksum for topic id and msg\n[INFO] [1529843963.836671]: wrong checksum for topic id and msg\n[INFO] [1529843964.422006]: wrong checksum for topic id and msg\n", "$ rostopic hz chatter\nsubscribed to [/chatter]\naverage rate: 685.848\n    min: 0.001s max: 0.004s std dev: 0.00022s window: 682\naverage rate: 653.469\n    min: 0.001s max: 0.004s std dev: 0.00022s window: 1304\naverage rate: 643.762\n    min: 0.001s max: 0.004s std dev: 0.00022s window: 1930\n", "\"[ WARN] : Socket asio error, closing socket: asio.misc:2\"\n"], "url": "https://answers.ros.org/question/295152/how-can-i-communicate-with-mbed-at-high-speed1khz/"},
{"title": "How does Mavros RC override work?", "time": "2014-11-09 22:54:18 -0600", "post_content": [" ", " ", "Hi,", "I've been trying to work with the mavros, but the ", " doesn't have a lot of detail about it. I'm using an Iris and connecting with roslaunch mavros apm2_radio.launch.", "First I'm setting SYSID_MYGCS to 1, as specified at the bottom of ", ". Then I'm publishing messages on the command line ", "I'm testing RC by leaving the quadcopter in an unarmed state, connecting a gimbal to RC Channel 6 using APM Mission Planner, then publishing various values on Channel 6 (1560 in the example code) and trying to get the gimbal to move. What happens now is that", "I feel like I just don't understand what mavros/rc/override is sending to the quadcopter. The gimbal is perfectly controllable using the RC controller before we publish anything to mavros/rc/override, so we know the gimbal is commandable with Channel 6.", "I apologize if this is a drone-discuss (or other quadcopter forum) question, not a mavros question, but I don't have a good enough understanding of what mavros is calling under the surface in order to usefully search the drone-discuss forums. If someone has a link to useful non-ROS information I'd welcome it...", "Did you figure this out?", "Unfortunately not, I'm not sure what's going on with the RC topics. I've been working on other stuff in the meantime."], "answer": [" ", " ", "The ", " is exactly what you think, it directly overrides the RC that would be sent via the transmitter.  The values typically range from 1000 to 2000.  If you want give control back to your RC transmitter or ignore a certain channel, publish 65535 on the channel.  Can you check what values are coming in on ", " when you control it using the RC transmitter? ", "I'm away from the quadcopter for a couple weeks, but I'll look into it when I get back, thanks!", "I have similar problems I can seem to make the override work. The /mavros/rc/in topic displays changes when I use RC but when I run the command ", "it is not working.", "Hi Vinh, Have you managed to solve this. I have the same problem. cheers", "my suggestion, is RC override will work with PX4 firmware. But do not use it. It gives unexpected result. Sometimes it will go up smoothly while other time it will jump very fast and dangerous.", "so how do you control yaw using mavros? say imagine you want to rotate 180 degrees around z"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "When the quadcopter is first powered I have control of the gimbal using our RC controller, channel 6", "After setting the SYSID_MYGCS value I still have control with the RC controller", "Publishing a mavros/rc/override message with a small deviation in the Channel 6 value does not change the gimbal position, but it does make it so that the RC controller can no longer affect the gimbal position", "Publishing messages with a variety of Channel 6 values has no effect, except when there is a large change of >150 in the Channel 6 measurement. This makes the gimbal travel fully in one direction and stay stuck there until you send a largely different Channel 6 value that makes the gimbal rail in the other direction."], "question_code": ["$ rosrun mavros mavparam set SYSIS_MYGCS 1\n$ rostopic pub -1 /mavros/rc/override mavros/OverrideRCIn '[1500, 1500, 1500, 1500, 1325, 1560, 2000, 1500]'\n"], "answer_code": ["mavros/rc/override", "mavros/rc/in", "rostopic pub -1 /mavros/rc/override mavros_msgs/OverrideRCIn '[1500, 1500, 1500, 1500, 1325, 1560, 2000, 1500]\n"], "url": "https://answers.ros.org/question/197109/how-does-mavros-rc-override-work/"},
{"title": "board: minimum hardware requirements? single board? full motherboard? nuc? mini-itx?", "time": "2014-07-09 12:57:38 -0600", "post_content": [" ", " ", "hi. we're trying to decide what board we should use for our robot prototype. any advice/recommendation is much appreciated.  we're looking for an affordable board that can handle kinect and kobuki.", "our robot is fairly similar to turtlebot:", "from our initial research:", "singleboard computer (i.e. raspberry pi, beaglebone black, etc):  the cost is really good but it seems like they can't handle kinect well.  we haven't done extensive work with ros for ubuntu arm yet, but there seems to be some limitations vs. ros full desktop.", "full motherboards (i.e. intel i5 or i7):  they definitely can do the job, but the costs are really high.  and the processing power seems to be overkilled.", "midrange options like NUC, mini-ITX, etc:  i have no idea how they perform and if they work well with kinect and kobuki", "any recommendation/feedback/advice is much appreciated."], "answer": [" ", " ", "Personally, I think most of the single board computers don't have enough computing power to handle the kinect well, and they'll also be a bit slow for doing motion planning for an arm. Some of the newer quad-core, 1.5GHz+ boards might be sufficient (Odroid U3 and up, Radxa Rock, new Qualcomm boards, and probably others).", "The performance of the NUC and other mini-ITX boards will be very dependent on which CPU and how much memory they have. You can get most of these boards with a wide range of CPUs, so the decisions about which CPU and which x86 form-factor to choose are not strongly correlated.", "I would make the decision about which CPU and how much memory you need before deciding on a form factor. The original turltlebots managed to squeeze by with a low-powered, dual-core Intel Atom processor and 1 or 2GB of RAM. For a more complex robot, you should probably move up to an Intel i5 or an i7.", "Once you have your CPU picked out, look for low-powered motherboards that support it. Personally, I think the NUC would make a great robot because some of the newer NUC support a wide-range voltage input that shouldn't need much or any extra regulation when running from a battery. This part of the NUC spec is NOT OBVIOUS, and NOT SUPPORTED BY ALL NUCs, and you have to read the full technical specification VERY CAREFULLY to find it, but it's there. Some of the NUCs also have a alternate 2-pin molex power connector on the board that will be more secure than the external barrel connector. There are also a fair number of other low-power motherboards that have single power supply inputs (mostly 12V), and the PicoPSU power supplies that are very compact and will run from a DC input.", "RAM is pretty cheap these days, so buy as much as you can fit in your motherboard. Probably 4GB at the bare minimum.", "If you're building a real, production robot, be prepared to buy and try several different computer configurations before you settle on the final board.", "thanks ", ". most of our computation is done on the cloud, so locally we only need a cpu+board to handle some light-weight tasks and act as a communication device with the cloud.  the heaviest local task is kinect+kobuki for navigation. do you think a mini-itx with atom e3800 + 4g ram works?", "I would err towards the more expensive computer for an initial prototype, so that you can get something running, and then work on optimizing the software to decrease the computing power required. If you have a very limited budget you could try to borrow a laptop with a similar CPU to test with.", "This of this this way: if you buy the cheaper computer and it isn't powerful enough, you've wasted time, money, and you still have to go buy something more expensive."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "we'll use kinect for depth camera", "we'll use kobuki base or build our our differential drive 4 wheel drive base.", "we'll use have one or two arms with dynamixels.", "we would love to have low power consumption so that the robot can operate as long as possible", "and of course, it runs ROS"], "url": "https://answers.ros.org/question/185761/board-minimum-hardware-requirements-single-board-full-motherboard-nuc-mini-itx/"},
{"title": "Small Single Board Computers suitable for ROS", "time": "2011-06-20 00:44:09 -0600", "post_content": [" ", " ", "Hi,", "I've been wondering which SBC's do ROS users recommend. I'm particularly interested in", "I've used BeagleBoard with ROS, but unfortunately I found it's processing power to little for my needs. I'm also aware of PandaBoard, but, after playing around with BB-xM, I think x386 architectures will be better suited for fast prototyping. I'm also aware of AscTec's Atom Board, but I find it too expensive (AFAIK it costs 1200Eur + VAT in Germany). The option I'm considering now is IEI's ", ", but the manufacturer is only assuring it's compatibility until Ubuntu 8.10 (but there is good chance newer versions will work out-of-the-box, I suppose) - costs around 400$.", "Am I missing something worthy of knowing?"], "answer": [" ", " ", "I know of:", " ", " ", "I can recommend the FitPC2 (", "/). We use it since years on our KidSize humanoid robots (though non-ROS for this application) and started using it on UGVs now. There's some trouble when you want to run an hardware accelerated X server with newer versions of Ubuntu (> 9.10) due to the Poulsbo chipset, but it otherwise works fine. The case can be removed if you want to have absolutely lowest weight, but we found it to be pretty robust and worth keeping (it also doubles as a heatsink for CPU and chipset)."], "answer_details": ["And a little bit larger ", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": [" ", " ", "small designs (less or around 10x10cm)", "lightweight (less than 200g, preferably around 100g)", "high processing power, approx. 2GHz (single or multiple cores), 2GB RAM, or more", "2 or more USB's", "5V or 12V DC power supply", "preferably x386", "ROS Diamondback friendly, ie. supports Ubuntu 10.04 or newer."], "url": "https://answers.ros.org/question/10324/small-single-board-computers-suitable-for-ros/"},
{"title": "relationship between ROS and robot hardware", "time": "2012-02-13 17:55:43 -0600", "post_content": [" ", " ", "Anonymous", "Hello, I just got started with ROS and I have a quick clarification question before I get myself into the wrong mindset about thinking of ROS. Please let me know if any of the following are correct/incorrect/half-correct - thanks in advance!", "I want to build a teleoperated robot. I have a 'laptop' and a 'workstation' from which I wish to control the robot."], "answer": [" ", " ", "Both of those options are \"correct\". There is not a correct way to handle this kind of system. It's entirely based on your application's needs. ", "For example, many quadrotors don't have ROS loaded on the bot itself. They run a dedicated firmware that was specifically designed for efficiency and speed. Since they provide a communications protocol, one must simply write a ROS node that translates ROS commands into simple drive commands. All complex decision-making is performed on the \"workstation\". This is similar to the NXT scenario. You do all of the major processing off the robot and then let the dedicated hardware (the \"brick\") do all of the low-level stuff. There is no \"right\" way. ", "On my robot, we have two PCs that communicate as a single system. We then connect to those PCs via ssh to add additional computation power to the system. ", "It also depends how much control you want over hardware. You can either access hardware through the built-in controls (on the \"brick\") via a programming API, or you can control the drivers directly with the \"laptop\" configuration. ", "Another thought to consider is latency. If the laptop (local) does most of the base work, then latency matters less. If you're using scenario 2, EVERY command must be sent across the wireless signal. If latency is bad, robot performance will be directly affected. ", " ", " ", "cenario 4: install ROS on 'laptop' and 'workstation' :)"], "question_details": [" ", " ", " ", " ", " ", " ", "scenario 1: install ROS on 'laptop'. SSH into laptop from workstation to control robot. This is like Turtlebot.", "scenario 2: install ROS on 'workstation', use a Bluetooth/Wireless stack on 'workstation' to receive input from robot, make complex decisions, and send commands to differential drives & whatnot. Is the 'laptop' still necessary on the robot (translating bluetooth messages to the actuators)? Is this more like NXT, where the NXT brick doesn't actually have ROS loaded on it but rather is being controlled by a ROS-powered workstation via bluetooth stack?", "scenario 2 (continued): if ROS is not installed on the physical robot (teleop), what components of ROS would I actually use on the workstation? The NXT brick would handle all the basic moving & sensing, and the workstation only has to use ROS bluetooth to handle data?"], "url": "https://answers.ros.org/question/27563/relationship-between-ros-and-robot-hardware/"},
{"title": "p2os groovy C++ Velocity and C++ Pose Listener [closed]", "time": "2013-06-18 05:29:03 -0600", "post_content": [" ", " ", " ", " ", "We need help in compiling the test sketches for listening and sending information to the robot from the tutorial ", ". \nHow to set the dependencies and set the right paths to  header files? Will it be enough to compile them with g++ and then start it using the line from the Tutorial ", ". But what is the p2osTutorial? Is it some package, from which the compiled file will be started?", "We have created p2osTutorials package and C++ listener works correctly.\nUnfortunately C++ velocity doesn't make the robot move. The first problem was solved adding the line ", " to the file, but the robot still didn't move. We build the driving file the same way as the pose file (just added a new velo.cc file to the p2osTutorial/src) and edited the CMakeList.txt. I think we are doing something wrong?", "Thanks for your answer. We compiled it the normal way but it seems that the ", " line does not move the robot", "When we start the node we printed out the cmd_msg it looks fine to us (robot should move).\nlinear: \n  x: 0.5\n  y: 0\n  z: 0\nangular: \n  x: 0\n  y: 0\n  z: 0", "After looking at the tutorials (again ;) with the new informations that we have now) i can see that the code from your tutorial C++ Velocity is almost the same. I guess we still have any problems in our dependencies or something like that.", "Thanks and regards ", "Thank you for the fast anwser and sorry that we did not tell this the motors are enabled and the robot drives over the simple commandline like: ", "\nin the scenario we try to test the C++ Velocity program we can not \"move\" the robot because the weels are \"blocked\" ", "or do we need to enable the motors in the way you actually described", "Thanks again ", "Thank you for the tutorial, it worked fine everything could be compiled and also the program starts. The motors are enabled by the enableMotor command.\nBut the robot won't drive neither with the GUI_Command nor with the C++ Velocity program. We see that some kind of data is send in the output of the terminals. The question is if this is a ROS problem or is something missing that is essentially is needed to drive with the robot. ", "Thanks and regards", "We will geht with the ropstopic list:", "C++ Velocity and echo_pose is active (p2os driver and so on also). I hope there is something in this list that is missing...", "Thanks", "http://oi43.tinypic.com/2dmgt3l.jpg (image description)", "I hope this will clear things up thank you gain", "sorry found out that there is another mode and i can see where the problem ...", "hm... That's odd. Why don't you try using the gui_command program? Just rosmake it and see if you can control it. It's in the vanderbilt-ros-pkg. ", "do a rostopic list and post the output for me", "Hm... Ok. Now do an rxgraph and post the screenshot. (sudo apt-get install rox-groovy-rxgraph)", "After you install it, just run the command \"rxgraph\" with no rosrun in front of it. ", "By the way, feel free to just email me if you have p2os specific questions: ", "This is entirely my fault. You were publishing to the wrong topic... You need to publish to \"/cmd_vel.\" Check the code I put in my answer. ", "Also, do a git pull in the root of the vanderbilt-ros-pkg. I forgot to change that code back, so I have been doing that on the wrong topic as well. It should work now. So sorry for that. ", "Thanks no problem in the meantime i figured it even out by myself ;) (the /cmd_vel problem)  and finally got to drive it :) the rxgraph is a really good thing thanks for that. Now i hope we can finally start with our project :). Thank you very much for the help."], "answer": [" ", " ", " ", " ", "No... Sorry for the misunderstanding... You need to create a package. Navigate to your ROS workspace (for you it should be done by ", ") then run the following command. ", "Then add the following line to the CMakeLists.txt:", "Note: you need to move your source to the subdirectory ", ".", "Update: I have now added a new tutorial called ", " I also updated the tutorial you are using with some more detail. ", "Ok, so I see the confusion now... You will not need to run any g++ commands at all. If you have not done so, I recommend that you go through the ", " to get a better understanding of how the build process works. To answer your question, yes, it is a package. You will have ROS run the program with a ", " command. ", "I think I know the issue... You might not be enabling the motors. Run the ", " and enable the motors. Alternatively, you can use the vanderbilt-ros-pkg. A lab member wrote a node to do this. It's called p2os_enableMotor. ", "Here's how you can use it: ", "That should make the pioneer movable. The motors are not enabled by default. ", "Huzzah! I see it now. The controller node is not interfacing with the p2os node. Could you email me/post the code for your controller? I want to make sure there isn't something weird going on. ", "My goodness... This is my fault. Try this code: ", "I updated the setup tutorial to give you some more information. ", "Thanks we test it tomorrow it is already late here ;) regards"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "/aio /base_controller/command\n/battery_state /cmd_motor_state\n/cmd_vel /diagnostics /dio\n/gripper_control /gripper_state\n/motor_state /pose /ptz_control\n/ptz_state /rosout /rosout_agg /sonar\n/tf"], "question_code": ["http: //www. ros. org/wiki/p2os-vanderbilt/Tutorials", "rosrun p2osTutorial eo_pose", "ros::init(argc, argv, \"velo\");", "cmd_pub.publish(cmd_msg);", "rostopic pub /cmd_vel geometry_msgs/Twist -r 10000 '[1, 0, 0]' '[0, 0, 0]'"], "answer_code": ["cd ~/ros_workspace", "roscreate-pkg p2osTutorial roscpp geometry_msgs nav_msgs\n", "rosbuild_add_executable(eo_pose src/eo_pose.cc)\n", "src", "rosrun", "p2os_dashboard", "roscd && cd ../src\ngit clone https://github.com/allenh1/vanderbilt-ros-pkg.git\nsource ../devel/setup.bash\nrosmake p2os_enableMotor\nrosrun p2os_enableMotor enableMotor\n", "const double TWIST_LINEAR = 0.5; //.5 m/s forward\nconst double TWIST_ANGULAR = 0; //0 rad/s \nint main(int argc, char **argv)\n{\n    ros::NodeHandle n;\n\n    ros::Publisher cmd_pub = n.advertise<geometry_msgs::Twist>(\"/cmd_vel\", 100);\n\n    ros::Rate loop_rate(10);\n\n    while (ros::ok())\n    {\n        geometry_msgs::Twist cmd_msg;\n        cmd_msg.linear.x = TWIST_LINEAR;\n        cmd_msg.angular.z = TWIST_ANGULAR;\n\n\n        cmd_pub.publish(cmd_msg);\n\n        ros::spinOnce();\n\n        loop_rate.sleep();\n    }\n\n\n    return 0;\n}\n"], "url": "https://answers.ros.org/question/65406/p2os-groovy-c-velocity-and-c-pose-listener/"},
{"title": "Extrapolation error using hector_mapping + move_base", "time": "2013-05-23 05:54:56 -0600", "post_content": [" ", " ", " ", " ", "Hello all,", "I have a problem with my robot setup which I cannot resolve, even after reading all relevant posts on here. I really would appreciate your help.", "The system starts up not generating any errors (see https://dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/start_up_log.txt), producing an accurate map, and localization working well, too. As soon as I select a goal in rviz, move_base reproducibly throws the following errors:", "However, if I make the following change in the local_costmap_params.yaml file:", "to", "I don't get any errors and move_base generates a plan to the goal, which is also displayed in rviz. Unfortunately, the generated cmd_vel commands are very shaky, oscillate and weird navigation behaviour is the result. ", "[hector_config.launch] https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/hector_config.launch", "[move_base_config.launch] https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/move_base_config.launch", "[base_local_planner_params.yaml] https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/base_local_planner_params.yaml", "[costmap_common_params.yaml] https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/costmap_common_params.yaml", "[global_costmap_params.yaml] https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/global_costmap_params.yaml", "[local_costmap_params.yaml] https://dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/local_costmap_params.yaml", "it would be great if you could provide solution to your problem, because I believe most ppl like me is struggling to get the navigation stack to work. Thank you verymuch!", "I have the same problem. I looked at the code, the lookup fails in transformGlobalPlan, ", " Either this is too strict about the timestamps or it has a bug which only surfaces in special cases.", "I tried move_base with gmapping last week and got a similar issue... the robot moved for a few centimeters sometimes, but stopped with the same messages. Comparing the TFs, I get 40Hz from hector_slam and 50Hz from gmapping. So it seams the local planner is too strict with timestamps.", "Hello Achim, I just saw your two recent posts. What are you suggesting to fix the problem? Increase the frequency of hector_slam? I still haven\u00b4t found a solution to make it work..."], "answer": [" ", " ", "That's a very exhaustive report - certainly very helpful for finding the cause for your troubles. That being said, the cause of the problem isn't immediately obvious to me, as things look fine from looking over them (at least from a published tf/hector_mapping POV). I assume the /odom frame moves around in rviz as expected? First guess would've been some time sync problem between computers, but looking at your exhaustive tf debugging output, that doesn't seem to be the case. If there's no other troubles regarding the state estimation (robot pose, map etc all look fine) my guess is that the culprit is the setup of move_base (which hopefully someone else can look at).", "Thank you for taking a look at this Stefan. Please see my updated post above regarding your odom frame question.", "That sounds like correct odom frame behavior. What you could try is setting the fixed frame to odom and see if LIDAR data with decay time drifts slowly when the robot is moving as described here in comments: ", "/", " ", " ", "I am unfamiliar with hector_mapping, but can you temporarily replace it with a static transform publisher? ", "Yes, unless the origin is set to something different, in which case the bottom left corner will be at the point specified in the origin.", "also, I have one more question about the costmap2d. Currently, I am using Gmapping which produces a static map. The costmap2d takes in laserScan and static map to produce a costmap. If the static map already contains information about the unknown/free/occupied, why does costmap 2d still", "needs the laserScan to reproduce the same information?", "Answer 1: In case you're not GMapping. Answer 2: To make the local costmap which doesn't use the static map.", ": Good idea, I will try to replace the map->odom transform generated by hector_mapping with a startic transform publisher and report back.", ", so basically in the nutshell, Map published from Gmapping has almost the same structure as with the 2D Map from Costmap2D? Where could I find the detail structure of the Costmap2d?", "For the most part. Addendum Answer 3: If you use the voxel structure, it is not the same information.", "I have the same problem as the one stated in this question. Since Huibuh doesn't seem to be reporting back as he said, I have tried replacing map->odom with a static transform publisher myself. Nothing changed! Same error!(I even tried different frequencies: last param of the static publisher)", " ", " ", "\nI have a quick quesiton about the 2d Map of the nav::occupancy_grid message type\nIs the (0,0) point (ie, the origin of the map) represents the lower left corner of the map?"], "question_details": [" ", " ", " ", " ", " Custom robot with chain drive, i.e. non-holonomic. I have written a base_controller node for it which sends drive commands to the motors and uses the encoder information to calculate odometry and publish as tf+nav_msg (as described http:// www.ros.org/wiki/navigation/Tutorials/RobotSetup/Odom). I have also created a *.urdf file and use robot_state_publisher to publish the tf's for the model.", " Hokuyo UTM-30LX laser scanner", " max_driver (custom base_controller), hector_mapping, move_base, hokuyo_node", " The hokuyo_node and the base_controller node run on an embedded PC, which is connected via ethernet to a powerful laptop running the remaining nodes. The time of both machines is synced via crony.", " Ubuntu 12.04 + ROS Groovy, everything up to date ", " [link] (https:// dl.dropboxusercontent.com/u/10986309/2013-05-23_ros.answers/frames.pdf)"], "url": "https://answers.ros.org/question/63360/extrapolation-error-using-hector_mapping-move_base/"},
{"title": "error executing minimal.launch: Create : robot not connected yet, sci not available", "time": "2013-11-11 04:19:27 -0600", "post_content": [" ", " ", "Hello,", "when I execute roslaunch turtlebot_bringup minimal.launch I get the following warning:", "turtlebot@turtlebot:~$ sudo service turtlebot status ", "turtlebot stop/waiting", "turtlebot@turtlebot:~$ roslaunch turtlebot_bringup minimal.launch ", "... logging to /home/turtlebot/.ros/log/fc655c5c-4709-11e3-b5a3-94dbc938ebd4/roslaunch-turtlebot-18937.log", "Checking log directory for disk usage. This may take awhile.\nPress Ctrl-C to interrupt", "Done checking log file disk usage. Usage is <1GB.", "started roslaunch server  //192.168.187.116:54204/", "PARAMETERS", "NODES", "/zeroconf/\n    zeroconf_avahi (zeroconf_avahi/zeroconf)\n  /", "auto-starting new master\nprocess[master]: started with pid [18955]\nROS_MASTER_URI=//192.168.187.116:11311", "setting /run_id to fc655c5c-4709-11e3-b5a3-94dbc938ebd4", "process[rosout-1]: started with pid [18968]", "started core service [/rosout]", "process[zeroconf/zeroconf_avahi-2]: started with pid [18981]", "process[robot_state_publisher-3]: started with pid [19003]", "process[diagnostic_aggregator-4]: started with pid [19022]", "process[turtlebot_node-5]: started with pid [19042]", "process[kinect_breaker_enabler-6]: started with pid [19106]", "process[robot_pose_ekf-7]: started with pid [19107]", "process[mobile_base_nodelet_manager-8]: started with pid [19147]", "process[cmd_vel_mux-9]: started with pid [19167]", "process[turtlebot_laptop_battery-10]: started with pid [19207]", "process[appmaster-11]: started with pid [19225]", "process[app_manager-12]: started with pid [19237]", "[WARN] [WallTime: 1383759408.626836] Create : robot not connected yet, sci not available", "[WARN] [WallTime: 1383759411.647325] Create : robot not connected yet, sci not available", "[kinect_breaker_enabler-6] process has finished cleanly", "log file: /home/turtlebot/.ros/log/fc655c5c-4709-11e3-b5a3-94dbc938ebd4/kinect_breaker_enabler-6*.log", "I execute turtlebot_dashboard.launch and all is ok,but when I try to probe the executable \"gira.cpp\", the turtlebot doesn't move. This is the executable gira.cpp:", "#include \"ros/ros.h\" ", "\n#include \"geometry_msgs/Twist.h\" ", "\nint main(int argc, char **argv)\n{\n    ros::init(argc, argv, \"gira\");   ", "}", "I also check that /cmd_vel changes its value:", "rostopic echo /cmd_vel", "and the output is:", "linear", "x:0.1", "y:", "z:", "angular", "x", "y", "z:0.3", "does anyone know that I'm doing wrong or that I can review?", "(I'm working with groovy on turtlebot 1 ..."], "answer": [], "question_details": ["/app_manager/interface_master", "/cmd_vel_mux/yaml_cfg_file", "/diagnostic_aggregator/analyzers/digital_io/path", "/diagnostic_aggregator/analyzers/digital_io/startswith", "/diagnostic_aggregator/analyzers/digital_io/timeout", "/diagnostic_aggregator/analyzers/digital_io/type", "/diagnostic_aggregator/analyzers/mode/path", "/diagnostic_aggregator/analyzers/mode/startswith", "/diagnostic_aggregator/analyzers/mode/timeout", "/diagnostic_aggregator/analyzers/mode/type", "/diagnostic_aggregator/analyzers/nodes/contains", "/diagnostic_aggregator/analyzers/nodes/path", "/diagnostic_aggregator/analyzers/nodes/timeout", "/diagnostic_aggregator/analyzers/nodes/type", "/diagnostic_aggregator/analyzers/power/path", "/diagnostic_aggregator/analyzers/power/startswith", "/diagnostic_aggregator/analyzers/power/timeout", "/diagnostic_aggregator/analyzers/power/type", "/diagnostic_aggregator/analyzers/sensors/path", "/diagnostic_aggregator/analyzers/sensors/startswith", "/diagnostic_aggregator/analyzers/sensors/timeout", "/diagnostic_aggregator/analyzers/sensors/type", "/diagnostic_aggregator/base_path", "/diagnostic_aggregator/pub_rate", "/robot/name", "/robot/type", "/robot_description", "/robot_pose_ekf/freq", "/robot_pose_ekf/imu_used", "/robot_pose_ekf/odom_used", "/robot_pose_ekf/output_frame", "/robot_pose_ekf/publish_tf", "/robot_pose_ekf/sensor_timeout", "/robot_pose_ekf/vo_used", "/robot_state_publisher/publish_frequency", "/rosdistro", "/rosversion", "/turtlebot_laptop_battery/acpi_path", "/turtlebot_node/bonus", "/turtlebot_node/port", "/turtlebot_node/update_rate", "/use_sim_time", "/zeroconf/zeroconf_avahi/services"], "question_code": ["app_manager (turtlebot_app_manager/app_manager)\nappmaster (turtlebot_app_manager/appmaster)\ncmd_vel_mux (nodelet/nodelet)\ndiagnostic_aggregator (diagnostic_aggregator/aggregator_node)\nkinect_breaker_enabler (create_node/kinect_breaker_enabler.py)\nmobile_base_nodelet_manager (nodelet/nodelet)\nrobot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\nrobot_state_publisher (robot_state_publisher/robot_state_publisher)\nturtlebot_laptop_battery (linux_hardware/laptop_battery.py)\nturtlebot_node (create_node/turtlebot_node.py)\n", "    ros::NodeHandle n;\n\n    ros::Publisher vel_pub_=n.advertise<geometry_msgs::Twist>(\"cmd_vel\", 1);   \n\n    geometry_msgs::Twist vel;\n\n    ros::Rate loop_rate(10);   \n\n    while (ros::ok())   \n    {\n      vel.linear.x = 0.1;   \n          vel.angular.z = 0.3;   \n\n      vel_pub_.publish(vel);\n\n          loop_rate.sleep();\n    }\n\n    return 0;\n"], "url": "https://answers.ros.org/question/99821/error-executing-minimallaunch-create-robot-not-connected-yet-sci-not-available/"},
{"title": "Integration of energy framework with ROS", "time": "2014-04-11 12:33:43 -0600", "post_content": [" ", " ", "I am currently working on a software framework to monitor the energy consumption of robots or any embedded device. I wanted to build my framework in a similar manner as ROS.", "At the moment, I have created a user API that does the following (front end):    ", "This framework allows users to profile their code for energy consumption.  (I have also created the back-end but is not relevant to the question I want to ask, so I am not including its details here.)", "Is it possible to integrate my framework with ROS? For example, I would like to profile devices attached to a central machine running ROS. Is it possible to set checkpoints and sample them within a node process, along with the rest of the ROS API? If yes, may I know how I would go about doing this?", "Thank you very much."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "a user can create a probe (a data structure used to collect data from an energy source) and associate it with a device attached to the embedded board.  ", "set a checkpoint with respect to any probe that is already created; once set, energy monitoring of the corresponding device begins.", "A user can 'sampleCheckpoint' at which point the latest energy reading collected from the corresponding device is returned.  ", "Finally, a user can also delete checkpoints and probes.  "], "url": "https://answers.ros.org/question/152146/integration-of-energy-framework-with-ros/"},
{"title": "ROS internals - topics storage and network comms?", "time": "2014-04-11 02:52:34 -0600", "post_content": [" ", " ", " ", " ", "I am currently working on a software framework to monitor the energy consumption of robots or any embedded device. I wanted to build my framework in a similar manner as ROS.", "At the moment, I have created a user API that does the following (front end):    ", "This framework allows users to profile their code for energy consumption.  ", "The backend: ", "\nThe architecture of the system is built in a producer-consumer fashion (similar to ROS), where energy sources are the producers and any software using the API are consumers. There is a central authority in between that stores checkpoints and probe related data in an sqlite database. And all communications between producers and consumers happen via this central authority over unix sockets.", "I would also like to integrate this framework with ROS at some point. In order to do that, I believe I need to create a ROS package for the framework?", "Thank you.", "Some nitpicking: ROS (the middleware) is built upon publish-subscribe and a flavour of RPC (or implicit and explicit invocation). While similar, a plain producer-consumer system is not the same.", "Thanks for pointing that out."], "answer": [" ", " ", " ", " ", "To answer your questions:", "1 . Does ROS also use UNIX sockets to do all its network communications?", "No, it uses TCP/IP for nodes (to cross process boundaries), and pointer-passing in nodelets (essentially nodes mapped on threads).", "2 . Does ROS store all the topics & subscribers associated with each topic in a database of some sort or just in memory?", "Plain ROS (ie: no extensions providing additional transports) uses in-memory buffering for messages, and a special node (the ", ") as a central index to provide topic->node mapping services. AFAIK that mapping is stored in memory, no databases (if you mean (R)DBMS) involved. Node->node communication is also direct, so the master is only involved in the initial setup of the connection.", "Edit: see also the ", " wiki page.", "Thanks for your reply. I've just read that nodes send messages to each other via topics. So is a topic a 'name (char *) + memory buffer (user definable datatype)'  data-structure? And, does the master keep a track of all the topics in a linked-list data-structure? Thanks."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "a user can create a probe (a data structure used to collect data from an energy source) and associate it with a device attached to the embedded board.  ", "set a checkpoint with respect to any probe that is already created; once set, energy monitoring of the corresponding device begins.", "A user can 'sampleCheckpoint' at which point the latest energy reading collected from the corresponding device is returned.  ", "Finally, a user can also delete checkpoints and probes.  ", "Does ROS also use UNIX sockets to do all its network communications?", "Does ROS store all the topics & subscribers associated with each topic in a database of some sort or just in memory?"], "answer_code": ["master"], "url": "https://answers.ros.org/question/151946/ros-internals-topics-storage-and-network-comms/"},
{"title": "Mapping with kinect instead of Hokuyo laser", "time": "2014-05-06 06:18:57 -0600", "post_content": [" ", " ", " ", " ", "So,my question is:-", "How to add kinect to my model to appear in rviz and gazebo?? What should I modify in my urdf model and .xacro file??\nHow can I reuse the code of a famous robot for adding kinect to my model and make mapping with kinect ?? \nAny suggestions ,please??"], "answer": [" ", " ", " ", " ", "Hey, \nI dont have your robot's urdf model file so i could't test the solution, I can guide you with basic hack (hope it ll help), any ways\nHere's the trick, ", "In your urdf model file, remove <sensor> //hokuyu stuff </sensor> (now here we should put kinnect stuff,and that can be taken from turtlebot urdf) ( b4 that note down the transformations of hokuyu, i.e., xyr rpy of sensor position)", "Locate turtlebot_description folder in your system (easiest way is  : $ roscd turtelbot_description), in folder 'urdf/sensor' you will find kinect.urdf.xacro file. copy it to package folder. \nalso locate these files 1) turtlebot_gazebo.urdf.xacro 2) turtlebot_properties.urdf.xacro(mostly in parent directory) and copy it to same package folder (as we dont want to mess up original files).", "Changes:\n   in \"kinnect\" file first change first two <include> files. : ", "<xacro:include filename=\"$(find your_package)/turtlebot_gazebo.urdf.xacro\"/>\n  <xacro:include filename=\"$(find your_package)/turtlebot_properties.urdf.xacro\"/>", "in turtlebot_properties.urdf.xacro file : <property name=\"cam_px\" value=\"-0.087\"/>\n                                                     <property name=\"cam_py\" value=\"-0.0125\"/>\n                                                     <property name=\"cam_pz\" value=\"0.2870\"/>\n                                                      <property name=\"cam_or\" value=\"0\"/>\n                                                     <property name=\"cam_op\" value=\"0\"/>\n                                                      <property name=\"cam_oy\" value=\"0\"/>  (sensor position) ", "\n                                       Here write corresponding x,y,z value from hokuyu part. ", "now in your URDF file,", "write these lines (instead of older <sensor> part)\ninclude our \"kinnect\" file: ", "<xacro:include filename=\"$(find your_package)/kinect.urdf.xacro\"/>", "<sensor_kinect parent=\"your_base_link\"/>", "\"Most probably\" this should work, as I havent tested it, I cant be sure that this will be compiled without any error. But if you receive any error then, (you know the basic trick so). just track down missing file or parameter in these file and compare it with turtlebot urdf model.  ", "For your second question, your code will work without making any changes(hopefully) if not then list down all the topics published, see where the scan or base_scan has been published and make appropriate changes in your launch file. (you can use remap function to remap one topic to another one).", "Hope this will help.\nSudeep  ", "Thanks a lot for your help. I will attach my_robot file.xacro to the original question to make it more obvious. I will try to follow these steps and I hope that it will work successfully.", " Excuse me,When I tried that,it doesn't work.\nSo,I tried to follow the same concept of adding laser and look at the files of my package which indicate any thing that related to the hokuyo laser to replace it with things that related to kinect. In the source file of my package there is laser.cpp which represented a f laser node in the bin file of the package after building it. So,How can I make a kinect.cpp file or how can I modify the laser.cpp file to make it work with kinect. I searched the turtlebot package for this kinect.cpp in its src file,but there is nothing.", " , Can you please provide more information about the error. If you are getting topic /image/depth/raw or something like that then, open a new terminal and write  rosrun depthimage_to_laserscan depthimage_to_laserscan image:=/camera/depth/image_raw ..(it will convert kinnect cloud data to laser) and also remove hokuyu laser node from launch file !", "After modifying the package ,When I typed the command that used to run mapping at rviz\nI had the following error \nInvalid ", " tag: Cannot load command parameter [robot_description]: command [/opt/ros/fuerte/stacks/xacro/xacro.py /home/eman/mashro3/wild_thumper60/urdf/robot1_base_04.xacro] returned with code [1]. \n\nParam xml is ", "I believe, this error is due to some missing files or mismatched parameter. can you please check few lines above this error. some description about error like missing file or invalid parameter must be there!", "Thanks for your help.There was written above the error that ( invalid parameter \"parent\" while expanding macro \"turtlebot_kinect\") . When I changed the name of macro \" turtlebot_kinect \", rviz and gazebo have been opened.But in Gazebo, the 6 wheeled robot appeared  without the kinect. In rviz,there is no mapping .", "Ohh Okay! Have you tried other solutions (I just mailed you some!)? Please check published topics : /image/depth or something like that. are you getting these topics ?", "Kinect gives point cloud data, so mapping wont work till you convert it into laser scan"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "I had a 6 wheeled robot.", "I made mapping using Hokuyo laser and it worked successfully on rviz as shown", "Unfortunately I don't have the Hokuyo laser. I have a kinect. so,I want to make mapping using kinect sensor instead of the Hokuyo laser. ", "The most powerful advantage of ros is that being able to reuse code.", "There are famous robots that uses kinect like turtlebot,corobot,pioneer and pi robot.", "This is my_robot.xacro file"], "question_code": ["<?xml version=\"1.0\"?>\n\n<robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" \n    xmlns:sensor=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor\"\n        xmlns:controller=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#controller\"\n        xmlns:interface=\"http://playerstage.sourceforge.net/gazebo/xmlschema/#interface\"\n    name=\"robot1_xacro\">\n\n\n    <include filename=\"$(find erratic_description)/urdf/erratic_hokuyo_laser.xacro\" />\n\n        <!-- BASE LASER ATTACHMENT -->\n        <erratic_hokuyo_laser parent=\"base_link\">\n                <origin xyz=\"0.18 0 0.11\" rpy=\"0 0 0\" />\n        </erratic_hokuyo_laser>\n\n    <xacro:property name=\"length_wheel\" value=\"0.05\" />\n    <xacro:property name=\"radius_wheel\" value=\"0.05\" />\n    <xacro:macro name=\"default_inertial\" params=\"mass\">\n               <inertial>\n                       <mass value=\"${mass}\" />\n                       <inertia ixx=\"1.0\" ixy=\"0.0\" ixz=\"0.0\"\n                                iyy=\"1.0\" iyz=\"0.0\"\n                                izz=\"1.0\" />\n               </inertial>\n    </xacro:macro>\n\n    <link name=\"base_footprint\">\n        <visual>\n            <geometry>\n                    <box size=\"0.001 0.001 0.001\"/>\n                </geometry>\n            <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n        </visual>\n        <xacro:default_inertial mass=\"0.0001\"/>\n    </link>\n\n    <gazebo reference=\"base_footprint\">\n        <material>Gazebo/Green</material>\n        <turnGravityOff>false</turnGravityOff>\n    </gazebo>\n\n    <joint name=\"base_footprint_joint\" type=\"fixed\">\n        <origin xyz=\"0 0 0\" />\n        <parent link=\"base_footprint\" />\n        <child link=\"base_link\" />\n    </joint>\n\n\n    <link name=\"base_link\">\n        <visual>\n            <geometry>\n                    <box size=\"0.22 0.380 0.085\"/>\n                </geometry>\n            <origin rpy=\"0 0 1.5707\" xyz=\"0 0 0.05\"/>\n            <material name=\"blue\">\n                <color rgba=\"0 0 .8 1\"/>\n            </material>\n        </visual>\n        <collision>\n            <geometry>\n                    <box size=\"0.22 0.380 0.085\"/>\n            </geometry>\n        </collision>\n        <xacro:default_inertial mass=\"10\"/>\n    </link>\n\n    <link name=\"right_wheel_1\">\n        <visual>\n                <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n                </geometry>\n            <!-- <origin rpy=\"0 1.57075 0\" xyz=\"-0.085 0.15 -0.25\"/> -->\n            <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n            <material name=\"black\">\n                <color rgba=\"0 0 0 1\"/>\n            </material>\n        </visual>\n        <collision>\n            <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n            </geometry>\n        </collision>\n        <xacro:default_inertial mass=\"1\"/>\n    </link>\n\n    <link name=\"right_wheel_2\">\n        <visual>\n                <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n                </geometry>\n            <!-- <origin rpy=\"0 1.57075 0\" xyz=\"-0.085 0 -0.25\"/> -->\n            <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n            <material name=\"black\"/>\n        </visual>\n        <collision>\n            <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n            </geometry>\n        </collision>\n        <xacro:default_inertial mass=\"1\"/>\n\n    </link>\n\n    <link name=\"right_wheel_3\">\n        <visual>\n                <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n                </geometry>\n            <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n            <material name=\"black\"/>\n        </visual>\n        <collision>\n            <geometry>\n                    <cylinder length=\"${length_wheel}\" radius=\"${radius_wheel}\"/>\n            </geometry>\n        </collision>\n        <xacro:default_inertial mass=\"1\"/>\n    </link ..."], "url": "https://answers.ros.org/question/161816/mapping-with-kinect-instead-of-hokuyo-laser/"},
{"title": "Problem with navigation through rviz", "time": "2014-04-27 22:21:47 -0600", "post_content": [" ", " ", "Hey everyone, I am trying to do the tutorial about navigation. I am able to create a map with gmapping. After that I would like the turtlebot to navigate with rviz and the 2D Nav Goal button. The problem is, it not working. When I start the amcl.demo and use roswtf I get following message:", "Beginning tests of your ROS graph. These may take awhile...", "analyzing graph...", "... done analyzing graph", "running graph rules...", "Traceback (most recent call last):", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/roswtf/__init__.py\", line 204, in _roswtf_main", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/roswtf/graph.py\", line 412, in wtf_check_graph", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/roswtf/graph.py\", line 104, in ping_check", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/rosnode/__init__.py\", line 392, in rosnode_ping_all", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/rosnode/__init__.py\", line 324, in rosnode_ping", "File \"/opt/ros/hydro/lib/python2.7/dist-packages/rosnode/__init__.py\", line 78, in _succeed", "ROSNodeException: remote call failed: not authorized", "remote call failed: not authorized", "Aborting checks, partial results summary:", "Found 1 warning(s).\nWarnings are things that may be just fine, but are sometimes at fault", "WARNING The following node subscriptions are unconnected:", "Found 1 error(s).", "ERROR Communication with [/master_sync_turtlebot_1025C_2800_1248496815] raised an error: list index out of range", "I have no clue what to do... I am searching since few days. Can someone help me?\nI am using Hydro on Ubuntu 12.04", "Greetings", "You built a map with gmapping, didn't you. So, your turtlebot works normally with teleop. Your ssh works normally. Your problem is to be not able to navigate your turtlebot with rviz. Did you check my solution( ", " )", "Thanks for replying. I will try your information and tell you how it went. I am just once in the week working with the turtlebots, it might take some time, but I will let you know :)"], "answer": [], "question_details": [" ", " ", "/camera/camera_nodelet_manager:\n", "/tf_static", "/amcl:\n", "/tf_static", "/initialpose", "/mobile_base_nodelet_manager:\n", "/mobile_base/commands/reset_odometry", "/mobile_base/commands/external_power", "/mobile_base/commands/sound", "/mobile_base/commands/digital_output", "/mobile_base/commands/led1", "/mobile_base/commands/led2", "/kobuki_safety_controller/enable", "/mobile_base/commands/motor_power", "/kobuki_safety_controller/disable", "/cmd_vel_mux/input/teleop", "/kobuki_safety_controller/reset", "/move_base:\n", "/move_base/local_costmap/footprint", "/move_base_simple/goal", "/mobile_base/sensors/bumper_pointcloud", "/tf_static", "/move_base/global_costmap/footprint", "/move_base/cancel", "/rviz:\n", "/tf_static"], "question_code": ["wtf_check_graph(ctx, names=names)\n", "error_rule(r, r[0](ctx), ctx)\n", "_, unpinged = rosnode.rosnode_ping_all()\n", "if rosnode_ping(node, max_count=1, verbose=verbose):\n", "pid = _succeed(node.getPid(ID))\n", "raise ROSNodeException(\"remote call failed: %s\"%msg)\n"], "url": "https://answers.ros.org/question/158116/problem-with-navigation-through-rviz/"},
{"title": "Turtlebot and turtlebot arm and hokuyo laser rangefinder", "time": "2012-03-25 16:33:09 -0600", "post_content": [" ", " ", " ", " ", "Hi, I'm interested buying ", " + ", " + ", ". So I have few questions: ", "P.S Sorry for my poor English."], "answer": [" ", " ", "1) You should be able to get everything to work.", "There are a couple of different ways to mount the laser, we have some pictures of how we do it here. ", "The TurtleBot Power Splitter can be used to provide power to separate devices. However the Create is only capable of supplying 1.5 Amps of current. If you attempt to pull more current, a protection circuit will reset the robot. Also the charging system is limited so if you try charging the robot without shutting down the Kinect, Laser or Arm the battery empty faster than it can refill and may damage the battery.", "2) Maybe. I think it can be done, but as far as I know there is currently no way to charge the battery without the robot. Also, the laptop battery often seems to be the limiting factor.", "3) The strength of the arm somewhat depends on how you are lifting.", "We have not tested this fully, but the best measure is the servo motors maximum specifications. The Robotis AX-12A servos have a Stall Torque : 1.52N.m (at 12.0V, 1.5A)", "The arm itself weighs about 0.4kg and is about 30cm long. The first servo can hold 5N in position and assuming all of the weight of the arm is at the end, the mass of the arm applies a force of about 3.8N which means the arm should be able to lift about one apple. Keep in mind this is a rough estimate and we have not tested it.", "4) Someone from turtlebot.eu may be able to correct me, but they are similar in overall performance. I believe the the Roomba 521 may have slightly better wheel encoders but it lacks a gyro. Also, I think there is more room on the Create.", " ", " ", " This links explain how to add a Hokuyo Laser Sensor to Turtlebot\n ", " ", " ", " ", "Using ", " and ", " can I connect ", " directly to the PC (", " Turtlebot)? ", "Yes! That was one of our design goals."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Can I connect everything in one robot? What parts more I must buy?", "Can I buy extra battery and charge it without robot? I use robot and charge second battery.", "How heavy staff can rise this arm?", "I'm form Europe. I find ", ". They sell Roomba 521 and I Heart Engineering sell iRobot Create. What is the difference? "], "url": "https://answers.ros.org/question/30433/turtlebot-and-turtlebot-arm-and-hokuyo-laser-rangefinder/"},
{"title": "Best Practices in ROS. (Making FRC->ROS Tutorial)", "time": "2014-08-05 14:49:30 -0600", "post_content": [" ", " ", "Hello!", "I am learning ROS and want to teach it to my (and every) FRC team in few months.\nI plan on doing this make a \u201cros walkthrough\u201d to demo the \u201ccorrect\u201d way to use ros.", "For the example, we will make an autonomous (click and go) ROS-Powered robot from start to finish using best practices.", "I want the finished code to also be a sturdy starting/sanity point for exploring ROS for the 350,000+ FRC students using equipment available to FRC teams.", "Example Hardware:", "Software: How should I do it? I think:", "Config", "Thank you very much for using your expertise to advance the human condition!", "prosa", "prosa100,", "It's been a while since you posted this question but I was just wondering how your project to teach ROS to the FRC community turned out.  Did you develop any tutorials or packages?  Can you please contact me at altemir .at. verizon dot net and let me know?"], "answer": [" ", " ", "Here's my insight:", "Software:", "--Currently only Indigo is supported on 14.04. You can build the source from scratch for hydro, but its simpler to just use indigo.", "--The Nav Stack that already comes with ROS is what you should use.", "-- If your talking about the actual tutorials for the nav stack, ", " it is. If you are talking about a downloadable ros package, I personally really like clearpath robotics husky package. Although its not named tutorial, there are multiple launch files with various configurations for almost all purposes (odom only, odom and gmapping, amcl, etc...). They also give you a already set up model of their husky robot ready to be used in gazebo. I use it myself if i dont feel like plugging in my robot. If you have any questions about that let me know. Ill be glad to help. ", "Config", "-- For people who are new to ROS, I would recommended writing a tf_publisher node that broadcasts all of the transforms from your sensors to your base_link frame. That way they will clearly see where all their data will be going. ", "-- Make a node that grabs your encoder ticks (motor controller node) and then make a geometry_msgs/Twist message out of that. Then send that to your odometry node, where you can follow the ros tutorial how to get position x, y, z and velocity vx, vy, vz.", "-- I currently am using robot_pose_ekf for kalman filtering, but being a beginner and not understanding at all how and why robot_pose_ekf works the way it does with not publishing a odometry message but pose with covariance stamped (PWCS), Robot_Localization would be alot easier for students to understand. Unlike robot_pose_ekf, it does not limit you to the amount of sensors you can have as inputs to the filter. Also, it publishes a odometry message (versus PWCS), which will cause a lot less confusion after going through the navigation tutorials for the first time.", "-- Arduino imu should be okay. If you are going to use it, I can give you some advice on how to use it in robot_localization since that imu may be noisy, and how to configure it for your scenario.", "-- For encoders, its pretty much a repeat of what I said before. Personally, I like to keep wheel_encoder data in its own node, and odometry as its own node as well. ", "-- Not fimiliar with using ros on arduino, so can you explain what you want with the base_controller part a little more?", "Hope this helps."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "Drivebase: Old FRC Differential Drive Kit bot", "Motor Controllers: Serial -> Arduino ->\u201cRC-Style\u201d PWM -> \u201cTalon SR Speed Controller\u201d", "Encoders: Serial <- Arduino <- US-Digital Quadrature Encoder E4P", "IMU: Serial to Arduino to 3 axis accel + 3 axis gyro", "Sensor: Kinect", "Brain: A laptop running Ubuntu 14.04 LTS", "Input: another laptop.", "I should use ros indigo because it is the \u2018current\u2019 release.", "I should use navigation instead of nav2d", "navigation_tutorials are for hydro and are mostly hardware dependent.", "I do not want to use obsolete things.", "I want to make reusable code. I do not want to reinvent the wheel.", "sensor transforms: make a node that publishes  the transform base_laser \u2192 base_link", "sensor source: for kinect use freenect_stack", "I make a node that reads from the encoders and publishes an odometry source or the encoder position. (which?)", "Should I use robot_localization?", "imu: arduino publishes \u201cimu\u201d (sensor_msgs/Imu)", "encoders: arduino publishes \u201cwheel_encoders\u201d or \u201cwheel_odom\u201d (nav_msgs/Odometry)", "base controller: Should use diff_drive_controller? Should the arduino subscribe to a motor_speed, motor_power, or cmd_vel?"], "url": "https://answers.ros.org/question/189039/best-practices-in-ros-making-frc-ros-tutorial/"},
{"title": "Mavros not arming quadcopter?", "time": "2014-10-08 10:01:31 -0600", "post_content": [" ", " ", "Hi,", " I've been trying to use mavros ( ", " ) to fly a 3DR Iris quadcopter that has a Pixhawk on it, but we're having trouble arming the system. ", "Run through of the scenario:", "We run 'rosservice call /mavros/cmd/arming true' in a new terminal, it returns", "success: false", "result: 3", "Does anyone have any ideas? Do we need to be outside with GPS fix? Are there any flags that need to be satisfied in the code before it will allow arming?", "Thanks"], "answer": [" ", " ", " Result codes defined in this enum:  ", "But last time i was seen APM code related to cmd 400 it may return 3 in many ways (it's disappointing me).", "Also this topic more related to FCU Firmware rather than mavros, please ask at drone-discuss.", " Others can see the related drone-discuss question here:  ", " I Have the same problem (just instead of 3 i have result: 4) and I went through all the drone-discuss in the group and I don't get which answer worked for this case?"], "question_details": [" ", " ", " ", " ", "We're inside, plug the battery in for the quadcopter, and the quad goes to flashing blue. This stands for \"Autopilot ready but no GPS lock\". We can arm the quadcopter with the RC controller at this point.", "We plug in the radio modem and run 'roslaunch mavros apm2_planner.launch'. At this point we are connected to the quadcopter. When we rostopic echo the /mavros/battery and /mavros/imu/raw_data topics (among others) we get good data. We can pull and push missions.", "We put the RC controller in AUTO mode", " ", " "], "url": "https://answers.ros.org/question/194507/mavros-not-arming-quadcopter/"},
{"title": "rviz not able to visualize laser scan", "time": "2015-11-27 14:42:53 -0600", "post_content": [" ", " ", " ", " ", "I'm having trouble with RVIZ being unable to show laser scan point data. I'm not sure if this is an RVIZ problem, a problem in my RVIZ config, or a problem with my setup in general (ie: i'm doing something bad with coordinate frames or something along those lines).", ": I am new to using tf frames within ROS. I decided to learn how to use tf and frame headers to properly tag all my sensor data so I can use provided packages (like for example hector_mapping). I'm going to try and give background to how I arrived at where I am now in case the context helps because I'm really not sure what I did to break it.", "Here's how I used to be able to see my laser scan data (it is from a little neato lidar):", "Next I created a node which transforms my neato_laser frame to base_link, and then configured hector_mapping and launched it. It worked well and I was able to visualize both the map and laser scan data after configuring rivz.", "At some point (the problem is I'm not sure exactly what changes I made to trigger, if any) RVIZ stopped showing laser scan data.", "Here is what I know:", "I've tried with the full config that allowed me to see the map + laser scan data before, and I've also tried reverting to setting all frames (fixed frame, target frame, etc) to neato_laser and only adding the LaserScan to the visualization.", ":", "ITs quite likely I'll need to provide more info/do some debugging to trace this problem but I'm not sure how to proceed. Any help in how to sort this out is much appreciated."], "answer": [" ", " ", "so I went ahead and restarted the computer for the first time in a couple weeks and the problem resolved itself.", "Not only is the point cloud being rendered correctly now, but also things that were missing before that I didn't notice (such as labels on tf transforms) have popped up as well.", "What? You didn't change anything within those weeks?"], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "open rviz", "add LaserScan", "Set topic to neato_scan topic", "Hector_mapping is still able to see and transform the laser_scan data correctly. The map generates fine and the robot is able to localize from the laser data + map. To me this indicates the coordinate frame transforms are infact setup correctly.", "I am able to verify there is scan data being generated by doing rostopic echo /neato_scan", "In RVIZ, no matter what I try, I can't get it to visualize the laser scan data.", "LaserScan shows status \"OK\" and under topic it shows xxx messages received which counts up correctly.", "Occasionally a single point will show up for a split second, usually at the origin. If i place a large decay time, it will show several points, mostly at the origin, and some at impossible locations (for example in mid air directly above the origin)", "There is no evidence of any errors. All of RVIZ status fields show OK with green check mark."], "url": "https://answers.ros.org/question/221636/rviz-not-able-to-visualize-laser-scan/"},
{"title": "Rviz *_LIST visualization/flickering", "time": "2016-03-24 04:00:33 -0600", "post_content": [" ", " ", " ", " ", "Anyone has a clue about the strange flickering effect affecting LINE_LIST and TRIANGLE_LIST markers in Rviz?\nI'm publishing three different marker-arrays", " As you can see in the video  ", "  when I move the pose inside RVIZ only the cylinders are always visible, all the  ", " markes appear and disappear as I move the view.. ", "Suggestions? :-)", "===== UPDATE =====", "William, the code is online in our github channel, and the code that provides all the parsing from OSM is inside ", ".\nI've just created a ", " with only the TFs and the markers plus a ", " file you need to correctly visualize everything. As you'll see, all the markers are referred to a /map frame which is the standard UTM origin, while local_map is the reference frame I use to visualize all the markers i.e. the fixed_frame in Rviz. Even if the ", " is big I don't think the problem may be referred to this huge transform (I mean, some loss of precision with the variables) since the cylinders are pretty fixed in their position (and all the markers are referred to that position, also the blue GPS track you can see in the image attached to my first post). ", "I'm putting also a bigger ", " recorded with the ", " option, it's about 60M and here you have also the map and lot of stuff (btw I think the first one is enough). As I wrote before, my suspects are about only the ", " arrays. It seems that with those markers there is some kind of precision loss somewhere, but I can't figure out where (I've already checked and all the points seem to be Double-s).", "Again, thanks for your help.\nAugusto", "=== UPDATE 2 ===", "This issue seems to be related with large TF transforms and it only affects *_LIST markers.\nIssue on the repository created, have a look ", "try setting the lifetime of your markers to 0 (infinite)", "I've already tested that parameter, nothing changed. ", "But it's not that kind of problem, all the three visualization_msgs::MarkerArray share the same initialization code (except for the type, obviously) and only the visualization_msgs::Marker::CYLINDER are correctly rendered.", "Hmm, is it possible for you to share a minimal example of this? It's hard to guess what the issue might be without looking at how you're publishing the markers. It might even be sufficient if you could provide a bag file with just the marker's and tf topics.", "Hi William, ", "I'm pushing a bagfile and a rviz-configuration online, you should be able to download them in a few minutes. I'll update the original question with some links and description", "Hi William, do you think it's better to insert this question as an issue in the Rviz github page?", "I haven't had time to look into it unfortunately. You can migrate to an rviz issue if you like, but I think it's usually best to wait until you're sure it's a bug and not a question. Moving it to an issue won't have any affect on my ability to review it or impact when I'll have some time to look.", "Sure, I thought that in the github version maybe the issue might have more visibility in the \"rviz-community\"! I'll wait, no problem at all ;-)", " thanks for the bag file and the rviz config. I do think this is a rendering bug with rviz (or possibly Ogre3D). I'm going to keep looking into it, can you open an issue on rviz in the meantime? Thanks again for your patience."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "the first one, CYLINDER, yellow in the image (which are the waypoints in OpenStreetMaps)", "second, the roads, LINE_LIST, green", "third, buildings, TRIANGLE_LIST, light blue in the image."], "url": "https://answers.ros.org/question/230030/rviz-_list-visualizationflickering/"},
{"title": "terminate called after throwing an instance of 'boost: Permission denied'", "time": "2017-04-17 23:29:56 -0600", "post_content": [" ", " ", "While running roslaunch ca_driver create_2.launch I receive this error. I have used the command for $ sudo usermod -a -G dialout $administartor, logged out and logged in again but it still doesn't work. ", "terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::system::system_error> >'\n  what():  open: Permission denied", "[ca_driver-2] process has died [pid 7759, exit code -6, cmd /home/administrator/create_ws/devel/lib/ca_driver/ca_driver __name:=ca_driver __log:=/home/administrator/.ros/log/f3f9e360-23ee-11e7-a992-f46d04793b72/ca_driver-2.log].\nlog file: /home/administrator/.ros/log/f3f9e360-23ee-11e7-a992-f46d04793b72/ca_driver-2*.log", "Please help. Thanks", "Can you post the output of ", "? Which robot base are you using? Create 2?", "Thank you for your response. I am using Create 2 directly connected to computer via USB cable. Actually I am just beginning to work on ROS and Ubuntu and doesn't know how to check the output at \"ls /dev/tty*\". When I try with the code with USB cable removed it gives the same error: No such file or d", "\"No such file or directory\". I have also tried launching Create 1 it also returns the same error with: Permission denied. The rostopics being published are :\n/diagnostics\n/joint_states\n/rosout\n/rosout_agg\n/tf\n/tf_static", "Are you using a virtual machine by chance?", "It gives the following list /dev/tty    /dev/tty23  /dev/tty39  /dev/tty54      /dev/ttyS10  /dev/ttyS26... \n...../dev/ttyS9   /dev/tty2   /dev/tty35  /dev/tty50  /dev/tty9       /dev/ttyS22  /dev/ttyUSB0", "This /dev/tty/USB0 goes off when I remove the cable and shows again when I plug in th cable ag", "Thanks for your help. It is working for me now. I think there was authorization problem. I run the this command\n$ sudo -s\nand then this\n$source ~/create_ws/devel/setup.bash\nand launched create_2 which is now working. Thanks.", "Glad you hear it's working."], "answer": [" ", " ", "Every time I got this error, i will just to run", "then create_autonomy will work. \nBut I makes me wonder how to permanently add this USB permission to Ubuntu?", "Try ", " and then logout/login. (", ")"], "question_details": [" ", " ", "Connect the cable to your computer and robot", "Make sure the robot is powered on (main LED should be green)", "Open a terminal and run the command ", ". Copy and paste the output here."], "question_code": ["ls /dev/tty*", "ls /dev/tty*"], "answer_code": ["sudo chmod a+rw /dev/ttyUSB0\n", "sudo usermod -a -G dialout $USER"], "url": "https://answers.ros.org/question/259468/terminate-called-after-throwing-an-instance-of-boost-permission-denied/"},
{"title": "TF broadcaster on robot , where to start?", "time": "2017-11-10 00:09:45 -0600", "post_content": [" ", " ", "Hey , i am new to ROS . Currently i am working on this robot", "(http://)(http://)(http://).", "components ", "The current configuration doesn't have a encoders . This is a big issue for determining the odom . ", "The bot is currently partially ROS enabled (Skid steer ) . I want to configure the bot with TF such that the camera and wheels are in the right perspective . ", "I am not able to figure out where to start , should i write a URDF/XACRO description of the robot and add the camera to it ?", "If anyone knows some simple tutorials please point me towards that .", "I've given you enough karma to attach images. Please ", " your question to add the image in."], "answer": [" ", " ", " ", " ", "We have somewhat of an expression in my lab:", "If you're using ROS, but don't have a urdf, you're doing it wrong", "that is of course grossly generalising, but there is some truth there.", "Whether or not you have encoders, start creating a urdf modelling the structure of your robot.", "Worry about adding other things later (such as \"TF broadcasters\": you'll know when you need them, right now, you don't yet).", "Also note: encoders (ie: wheels) are not the only source of odometry. Visual odometry and IMUs are also good sources.", "Thanks , you are right . I should leave everything and first focus on creating the URDF model . I am already on it ."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "sabertooth 2x32 motor driver ", "18aH 12v battery ", "raspberry pi ", "Logitech USB camera", "Line laser", "2X 250watt ebike motors"], "url": "https://answers.ros.org/question/275539/tf-broadcaster-on-robot-where-to-start/"},
{"title": "cannot show attached object in rviz", "time": "2017-12-23 21:09:35 -0600", "post_content": [" ", " ", " ", " ", "Hi.", "I'm working with rviz and moveit built from source for the kinetic-devel branch. For the rest of the stack, I'm using the binary packages. OS is Ubuntu xenial.", "When I attach objects to the robot arm, it does not show in rviz.", " I'm having trouble with a robot model I created from scratch, but the same thing can be reproduced by the moveit \"ROS API Planning Scene Tutorial\" at\n ", "Here is an excerpt from stdout (with line numbers manually added) when I run", "roslaunch moveit_tutorials planning_scene_ros_api_tutorial.launch :", "Here is what happens on the rviz screen:", "I have read the following page that seems to be related, but I am not sure if it is:", "The thread says that an incorrect combination of rviz checkboxes make the attached object appear, but in my case none of the possible combination made the object appear.\nIn any case, the fix discussed in this thread is merged to kinetic-devel, and I am building kinetic-devel .", "Is there something I am missing?", "--- self follow-up ---", "It seems that the planning scene update message is not correctly propagated from the moveit node to the rviz node.", "Here is what I did:", "I used rqt_graph to study the topic names that are used to communicate the planning scene messages, and learned that", "I set up 2 terminals running rostopic echo for the two topics.\nWhen my application sends out a message to attach an object in the world to the robot, it is seen in the /planning_scene topic, but the corresponding information is missing in the /move_group/monitored_planning_scene topic.  It seems that the /planning_scene message is triggering a /move_group/monitored_planning_scene message, but the \"attached_collision_objects\" array is not propagated.", "I expect that the ..."], "answer": [" ", " ", "The mentioned bug had already been fixed and committed on Dec. 15.\nMy local git clone was behind that."], "question_details": [" ", " ", " ", " ", " ", " ", "At line 1, a green box gets shown at the wrist location of PR2", "At line 5, the box disappears. Expected behavior is a color change of the box. Green is used for collision objects, and purple(150,50,150) is used for attached objects. The box should turn purple.", "At line 7, the green box appears again, as expected.", "\"/planning_scene\" is used for communication from the move_group client (the application) to the move_group node.", "\"/move_group/monitored_planning_scene\" is used for communication from the move_group node to the rviz node."], "question_code": ["1 [ INFO] [1514083362.221299219]: Adding the object into the world at the location of the right wrist.\n2 [ INFO] [1514083372.229633700]: Attaching the object to the right wrist and removing it from the world.\n3 [ERROR] [1514083372.230641932]: Found empty JointState message\n4 [ INFO] [1514083372.230957243]: Removing world object with the same name as newly attached object: 'box'\n5 [ INFO] [1514083372.231715610]: Attached object 'box' to link 'r_wrist_roll_link'\n6 [ INFO] [1514083382.229897324]: Detaching the object from the robot and returning it to the world.\n7 [ INFO] [1514083382.230552447]: Detached object 'box' from link 'r_wrist_roll_link' and added it back in the collision world\n8 [ INFO] [1514083392.230129313]: Removing the object from the world.\n"], "url": "https://answers.ros.org/question/278125/cannot-show-attached-object-in-rviz/"},
{"title": "clearpath husky emergency stop issue [closed]", "time": "2018-05-20 15:45:04 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I have a clearpath husky a200. Recently, there has been a problem where the husky will keep activating the emergency stop during operation (see attached video). The problem used to happen maybe once in a while; however, it has gotten to a point now where we are are not able to even teleoperate the robot.", "Things we've tried that didn't work:", "Please let me know if you've seen this issue before?", "UPDATE (May 21st): I have tried installing a new ISO for the husky, still didn't fix the problem", "UPDATE 2 (May 23rd): I have found that the error seems to happen less when operating at extremely low speeds and acceleration via keyboard teleoperation (teleop_twist_keyboard: this package allows the user to increase/decrease speed). This might be why joystick operation causes the error to happen immediately (the joystick commands outputs a preset high velocity/acceleration upon user input). ", "Could there be a corrupted file somewhere in Husky that prohibits certain user vel/accel commands which subsequently stops the husky? If so, where should I look? ", "Also, how can I lower the speed output from the joystick operation to verify that this is indeed a speed/acceleration limit issue?", "Could you clarify whether you've contacted Clearpath about this already?", "yes, I have emailed tech support at clearpath but I was hoping someone whose experienced this before may help me before clearpath responds", " here's a related post:  ", "Clearpath has confirmed that this is a hardware issue."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "charging the battery overnight to ensure full charge", "checked all connections on board from sensors, estop receiver...etc", "charged the estop pendent to ensure full charge"], "url": "https://answers.ros.org/question/291729/clearpath-husky-emergency-stop-issue/"},
{"title": "canopen master hard- and software advice", "time": "2018-07-12 13:57:12 -0600", "post_content": [" ", " ", "At work I am trying to integrate a ", " with a KUKA industrial robot. I have some experience with integrating a sensor with this robot, so I'd like to use this project to increase my skills with ROS. For now I want to focus on controlling all functionality of the generator from ROS.", "The generator supports communications based on RS485 and CANOpen. I have a detailed manuals of both communications options (easily found online) and the .EDS files for the CANOpen communication. For the project I'd like to get more into working with industrial field busses, so using CANOpen has my preference.", "The questions I am currently trying to answer:", "Thanks for your help.", "Just a comment: rs485 is also often considered a fieldbus. If you want to work message oriented (instead of dictionary based), I believe rs485 would make sense.", "As to the canopen profile: does the manual not state compliance with some profile?", "Message oriented would be my preference. However I expect to need to control the power supply in real-time in the future. The manual states that when using RS485 for this device gives a 5 ms delay.", "I can send you the docs directly.", "On CANopen device profile the docs say:\nObject 0x1000 Device Type\nThis object indicates the device type. As there is no device profile for ultrasonic generators, this object is set to 0 (no standard device).", "Pedantic, but:", "However I expect to need to control the power supply in real-time in the future", "As long as the delay is constant or has an upper-bound, it would still be real-time. But I understand you want minimum latency?", "For the first iteration of the system (spot welding and multiple spot welding in sequence) I see no strict requirements on latency. ", "The second iteration would involve contiuously welding in which the welding parameters need to be controlled. However I do not know (yet) what latency is acceptable.", "Do you have any pointers to RS485 based equipement drivers in ROS that I could use as template/inspiration?", "re: example: no, not directly. RS485 is basically a serial port, so unless you're doing something special (ie: multidrop) can be used as one. It also depends on a bit on what hw you're using in that case (ie: there are inexpensive USB<->RS485 converters, but also dedicated expansion boards)."], "answer": [" ", " ", "I am not exactly clear which device profile my device falls under. I expect CiA301 but not sure. ", "Every CANopen device must support CiA 301, it is the base for all CANopen communication.", "How can I find out? Is this part of the .EDS file?", "Object 0x1000 contains the device type. The actual value might or might not be listed in the EDS file.", "Will I be able to use canopen_master for the communication with the power supply. ", " Should work. You can start with  ", " , it will give acces to all objects of the EDS/DCF. Mostly meant for testing. ", "It should be subclassed to work with spefific devices, like  ", "\n for CiA 402 drives.\nAs an alternative you can write your own interface, ", "Therefor I was looking at either a CANgineBerry or a PICAN board. For the last one I am not sure if it supports CANOpen or SocketCAN.", "SocketCAN is a kernel driver interface for CAN devices. (like ", ").\nCANopen is a protocol spefification. (like ", ")", "In addition ", " has a plugin-system for using non-SocketCAN devices as well.", "The website of the latter states \"SocketCAN driver, appears as can0 to application\"\nHowever, I was not able to find the support status of the CANgineBerry.", "Does anyone have experience with either of these boards (or another) in combination with the canopen_master?", "I think I have tested it with a PEAK USB dongle on Raspberry PI.\nIf SocketCAN works, ros_canopen should work as well."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Will I be able to use ", " for the communication with the power supply. I am not exactly clear which device profile my device falls under. I expect ", " but not sure. How can I find out? Is this part of the .EDS file?", "For hardware I'd like to use a Raspberry Pi in the controlbox (located near the toolplate) for the EEF. I already have ethernet and power running through the robot arm. Therefor I was looking at either a ", " or a ", " board. For the last one I am not sure if it supports CANOpen or SocketCAN. Are they actually different?  Does anyone have experience with either of these boards (or another) in combination with the canopen_master?"], "answer_code": ["canopen_motor_node", "eth0", "TCP/IP", "ros_canopen"], "url": "https://answers.ros.org/question/297194/canopen-master-hard-and-software-advice/"},
{"title": "ekf_localization_node : Wheel odometry and IMU filter output /odometry/filtered result worst than only wheel odometry", "time": "2019-05-28 06:15:25 -0600", "post_content": [" ", " ", "I'm trying to filter the IMU and Odometry for better odometry result with ROS Kinetic robot_localization ekf_localization_node.\nThe result from topic /odometry/filtered looks even worst than the only wheel odometry result.", "2.\n The ekf_localization node give me this warning", "These are my ekf_localization_node launch file, IMU topic, odometry topic and my ekf_localization configuration", "ODOM TOPIC : /odom", "imu topic : /imu_data", "Configuration", "A couple things I notice off the bat (not sure any given one will fix your issue)", "Your IMU message has no covariances, you need those.", "If its not able to transform a static TF, you probably need to increase your robot state publisher by quite a bit until that goes away", " Thank you for your reply, I'm not sure about how to calculate covariances for IMU, is that any hints for that?", "Your spec sheet should contain the variances you are looking for", "My IMU is JY901B, it does provided data sheet, but I didn't see any information related to covariances."], "answer": [" ", " ", "The first thing to check is that your IMU is actually being fused. Your IMU data is in the ", " frame. Are you providing a transform somewhere from ", "->", "?", "Also, try turning off ", " in your IMU config. You're fusing absolute yaw, which comes from a magnetometer, and magnetometers are notoriously erroneous.", "Finally, try adjusting the process noise vs. the measurement covariance for the state variables you're fusing. Lower covariance in the measurement and higher process noise will mean that the filter trusts your sensors more. Otherwise, the filter will prefer to stick with its predictions, and, as we don't model angular acceleration, you can lose accuracy when the robot turns."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "The result of the odometry and the odometry/filtered result is shows below, the green line is the odometry/filtered result filter by ekf_localization_node, the blue line is the robot wheel encoder odometry. I control the robot go one round, and the odometry/filtered result drift a lot. How to configure it for a better odometry/filtered"], "question_code": ["[ WARN] [1559037541.564209301]: Transform from imu to base_link was unavailable for the time requested. Using latest instead.\n", "header: \n  seq: 36880\n  stamp: \n    secs: 1559041171\n    nsecs: 825065674\n  frame_id: \"odom\"\nchild_frame_id: \"base_link\"\npose: \n  pose: \n    position: \n      x: -11.4829724863\n      y: 1.05053742504\n      z: 0.0\n    orientation: \n      x: -0.0\n      y: 0.0\n      z: 0.763262843766\n      w: -0.646088098735\n  covariance: [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]\ntwist: \n  twist: \n    linear: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n    angular: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n  covariance: [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]\n", "header: \n      seq: 79151\n      stamp: \n        secs: 1559041431\n        nsecs: 174880504\n      frame_id: \"imu\"\n    orientation: \n      x: -0.0113399562479\n      y: 0.00536445058446\n      z: 0.525673082796\n      w: -0.850594167677\n    orientation_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n    angular_velocity: \n      x: 1.11133271904e-83\n      y: 1.4868652577e-183\n      z: -1.13872042843e-82\n    angular_velocity_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n    linear_acceleration: \n      x: -0.0335302734375\n      y: 0.24908203125\n      z: 9.87706054688\n    linear_acceleration_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n", "frequency: 30\nsensor_timeout: 1\ntwo_d_mode: true\ntransform_time_offset: 0.3\ntransform_timeout: 0.0\nprint_diagnostics: true\ndebug: false\ndebug_out_file: /path/to/debug/file.txt\npublish_tf: true\npublish_acceleration: false\n\nmap_frame: map              \nodom_frame: odom            \nbase_link_frame: base_link  \nworld_frame: odom           \n\nodom0: /odom\n\nodom0_config: [false,  false,  false,\n               false, false, false,\n               true, true, false,\n               false, false, true,\n               false, false, false]\n\nodom0_queue_size: 2\nodom0_nodelay ..."], "answer_code": ["yaw"], "url": "https://answers.ros.org/question/324129/ekf_localization_node-wheel-odometry-and-imu-filter-output-odometryfiltered-result-worst-than-only-wheel-odometry/"},
{"title": "ekf_localization_node : Filterd odometry yaw depend on IMU too much and Laser scan drift when robot rotate with filtered odometry.", "time": "2019-06-12 03:58:17 -0600", "post_content": [" ", " ", "I have a ", " fuse by wheel odometry ", " and IMU ", " with ekf_localization. I let my robot facing a wall and do some test with the ", ". I have two problem right now:"], "answer": [" ", " ", "But when I lift the robot up to make the wheel loss contact with the ground and move the robot forward and backward, both the odometry/filtered and odom output in rviz will move forward and backward too. When I rotate robot on the spot without wheel and ground contact, only the wheel odom move, the odometry/filtered is not moving, this make sense to me because the robot is not rotating only the wheel itself is running.", "Right, this normal/expected, as are your other bullet points. The reason for this is that your IMU is also measuring yaw velocity, and it has an all-zero covariance matrix (which you should fix), so the filter trusts it much, much more than it trusts the wheel velocity for detecting turning. But the IMU doesn't report and kind of linear velocity, so the filter is only using the wheel encoder data for linear motion.", "So to answer your question, the filter isn't too dependent on the IMU; you are telling the filter to trust the IMU much more than it trusts the wheel encoders. In general, I'd say this is correct, but your IMU may not report velocities very accurately. In any case, if you want the wheel encoders to have more of an effect, then you need to make sure the IMU and wheel encoder covariance values for yaw velocity are on the same order of magnitude.", "When I rotate the robot with the odometry/filtered, my laser scan will drift when the robot rotate and the laser will back on right position when the robot stop.", "This says to me that you need to do more covariance tuning, or that your IMU data may be lagged. There will _always_ be some amount of lag in a filter, and you will experience drift if your IMU is under-reporting its error. If I were you, I'd see how it does with _just_ wheel encoder data as an input. If there's less laser \"smearing,\" then the issue is with the IMU data. I'd also try increasing the initial and process noise covariance for yaw velocity."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "My ", " when I move the robot forward and backward, rotate the robot on the spot, the ", " output seem to be like good in rviz.", "  The rviz output, green arrow > ", ", blue arrow > ", ", the axis in front the base link axis is my laser axis.", "\nBut when I lift the robot up to make the wheel loss contact with the ground and move the robot forward and backward, both the ", " and ", " output in rviz will move forward and backward too. When I rotate robot on the spot without wheel and ground contact, only the wheel ", " move, the ", " is not moving, this make sense to me because the robot is not rotating only the wheel itself is running.", "When I leave the robot not moving and rotate only the IMU, for sure the ", " won't move, but the ", " will be move exactly same with how I rotate my IMU manually.", "Rviz output: When the robot lift up, first: Rotate the robot , Second: Move robot forward and backward, Third: Rotate only the IMU. ", "Is my ", " depend too much on the IMU? And is it related to covariance? How to make a better configuration on this?", "When I rotate the robot with the ", ", my laser scan will drift when the robot rotate and the laser will back on right position when the robot stop.", "This is the rviz output when the robot is facing a wall and not rotating. The green arrow is ", " and the blue arrow is pure ", "from wheel encoder. ", "This is the rviz output when I set the laser scan decay time to 20s and rotate the robot. The laser scan will rotate when the robot rotate. ", "Rviz video "], "question_code": ["odometry/filtered", "odom", "imu_data", "odometry/filtered", "odometry/filtered", "odometry/filtered", "odometry/filtered", "odom", "odometry/filtered", "odom", "odom", "odometry/filtered", "odom", "odometry/filtered", "odometry/filtered", "odometry/filtered", "odometry/filtered", "odom", "header: \n  seq: 42461\n  stamp: \n    secs: 1560329405\n    nsecs: 936365909\n  frame_id: \"odom\"\nchild_frame_id: \"base_link\"\npose: \n  pose: \n    position: \n      x: -0.210383832846\n      y: -0.0374875475312\n      z: 0.0\n    orientation: \n      x: 0.0\n      y: 0.0\n      z: 0.145501269807\n      w: 0.98935806485\n  covariance: [0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01]\ntwist: \n  twist: \n    linear: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n    angular: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n  covariance ..."], "url": "https://answers.ros.org/question/325453/ekf_localization_node-filterd-odometry-yaw-depend-on-imu-too-much-and-laser-scan-drift-when-robot-rotate-with-filtered-odometry/"},
{"title": "Expected behaviour when cmd_vel == 0", "time": "2019-09-09 08:16:27 -0600", "post_content": [" ", " ", "I've just finished writing a hardware interface for my custom robot. It finally performs as hoped, but I'm not sure what ", " == 0 m/s should actually cause. Does it mean:", "Maybe it's just personal preference, but I am very interested in hearing what the ROS official convention is."], "answer": [" ", " ", " ", " ", "For all robots I've worked with, wrote my own hw interfaces for or have used hw interfaces of others with, ", " was always used to dictate the ", " in which the system should be (ie: messages encode the desired state or set point). For those robots a command of 0 m/s meant: motors powered, but not rotating.", "This has made sense to me, as by convention ", " carries setpoints in the form of ", ", which contains the following comment:", "This expresses velocity in free space broken into its linear and angular parts.", "So, ", "s sent to a mobile base platform encode a body relative set of linear and angular velocities. These are then typically mapped onto joint space velocities for wheels in case of a wheeled mobile base such that 'the robot' (or mobile base) attains the desired attitude.", "Following this, a ", " carrying only zeroes would encode for a 0 m/s linear and 0 rad/s angular state in the body local reference frame, or in other words: a non-moving robot.", "Your other option (unpowered or backdrivable actuators) would lead to a non-zero state in case of \"coast[ing] downhill\" (as the encoders, which will probably be present to support velocity control, will register a non-zero displacement). That would lead to a velocity error, which a controller would probably try to rectify (by breaking or applying a corrective velocity).", "Edit: I don't believe there is a REP that documents or standardises this particular aspect (as in: in a robot-agnostic manner), but there is ", ", which in the section called ", " writes:", "Subscribed Topics", "cmd_vel (", ")", "The desired velocity of the robot. The type of this message is determined by the drive_mode parameter. Default is ", ".", "which seems to confirm my experience and intuition.", "As almost all use of ", " seems to follow this convention (as authors of early nodes looked to existing implementations to match their own against, and TurtleBot and PR2 were the most prominent ones), I believe only your second alternative would be the correct one.", "Edit 2: there is also ", ", which doesn't directly deal with ", " or its semantics, but discusses something similar in the ", " section:", "The command is a body relative set of accelerations in linear and angular space.", "Note that this talks about 'accelerations', but the idea and use is similar to ", " and velocities for wheeled robots.", "Thank you for such a detailed response. This is exactly the information I was looking for, and it makes perfect sense.", "I believe that if you'd actually want to achieve something like backdrivability, ", " any active controller could be used achieve that (in combination with some support in your ", ").", "You can stop and start controllers with ", " by calling the appropriate service on your instance of the ", ".", "A combination of ", " and ", " in your ", " could probably be used to determine whether there is ", " active controller, and when there isn't, you could enable some special mode in your hardware that allows for coasting or backdrivability."], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "Stop powering the robot's wheels, allowing it to coast downhill (i.e. robot is not powered)", "Hold the robot still, as if it were being actively braked (i.e. robot velocity = 0 m/s)"], "question_code": ["cmd_vel"], "answer_code": ["cmd_vel", "cmd_vel", "Twist", "Twist", "geometry_msgs/Twist", "geometry_msgs/Twist", "cmd_vel", "cmd_vel", "cmd_vel", "hardware_interface", "ros_control", "ControllerManager", "prepareSwitch(..)", "doSwitch(..)", "hardware_interface"], "url": "https://answers.ros.org/question/332587/expected-behaviour-when-cmd_vel-0/"},
{"title": "Improving the smooth path after dual-EKF-navsat node with robot_localization", "time": "2019-10-24 06:28:10 -0600", "post_content": [" ", " ", "Hi,\nI'm currently using robot_localization package in order to combine data from an IMU, Visual Odometry and GPS.\nFollowing the tutorials robot_localization dual-EKF wiki page I'm able to get the correct odometry and tf, But my path is not smooth and i notice a discrete jump on the trajecory.\n", " ", "this is my tf_tree ", "I use a rosbag simulation of the kitti dataset.", "I'm on ROS melodic, Ubuntu 18.04. Just some general guidelines and things that I could try from improving the smooth path would be greatly appreciated.\nThank you in advance!", "VO ORBSLAM2 (/odom_cov)", "GPS(/gps_cov)", "IMU (/imu_cov)", "Launcher file ..."], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "The green path is the Visual Odometry (OrbSlam2)", "The blue path is the output of ekf_se_map (/odometry/filtered_map )", "The red is the output of ekf_se_odom", "The yellow path is the output of navsat_transform_node (/odometry/gps)", "The white path is the ground truth"], "question_code": ["header: \n  seq: 384\n  stamp: \n    secs: 1317381172\n    nsecs: 210807644\n  frame_id: \"odom_r\"\nchild_frame_id: \"filter_link\"\npose: \n  pose: \n    position: \n      x: 255.214271206\n      y: -4.10897729772\n      z: 30.1642496267\n    orientation: \n      x: -0.0498665846097\n      y: -0.0129415959665\n      z: -0.476836520412\n      w: 0.877480923671\n  covariance: [0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8]\ntwist: \n  twist: \n    linear: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n    angular: \n      x: 0.0\n      y: 0.0\n      z: 0.0\n  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n", "header: \n  seq: 1560\n  stamp: \n    secs: 1317381292\n    nsecs: 817400695\n  frame_id: \"gps_link\"\nstatus: \n  status: 0\n  service: 1\nlatitude: 48.9721387355\nlongitude: 8.47601192734\naltitude: 198.272659302\nposition_covariance: [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5]\nposition_covariance_type: 0\n---\n", "header: \n  seq: 92\n  stamp: \n    secs: 1317381140\n    nsecs: 292656139\n  frame_id: \"i_link\"\norientation: \n  x: 0.000643883176529\n  y: -0.0433980004141\n  z: 0.12066934251\n  w: 0.991743469226\norientation_covariance: [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5]\nangular_velocity: \n  x: 0.00472081300331\n  y: -0.0131300385873\n  z: 0.0896434253573\nangular_velocity_covariance: [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5]\nlinear_acceleration: \n  x: 0.423154205467\n  y: 0.855483885359\n  z: 9.97805472549\nlinear_acceleration_covariance: [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5]\n---\n"], "url": "https://answers.ros.org/question/336071/improving-the-smooth-path-after-dual-ekf-navsat-node-with-robot_localization/"},
{"title": "Autoware Calibration Toolkit: Green Circle Too Small", "time": "2019-11-27 20:30:24 -0600", "post_content": [" ", " "], "answer": [], "question_details": [" ", " ", " ", " ", " ", " ", "I want to calibrate my camera using Velodyne. \n", "As title mentioned, when I Grab, the green circle to extract Lidar points is too small then I cannot calibrate it accurately (like the botton right figure show below) because it can only extract couple of points. ", "I referred to the YouTube guidelines and the green circle it used is bigger enough to extract many points on the chessboard. ", "Is there any ways to modify the size of the green circle?", "I am using Linux Ubuntu 16.04, Autoware version: 1.10, Lidar: Velodyne-16, Camera: GMSL"], "url": "https://answers.ros.org/question/338755/autoware-calibration-toolkit-green-circle-too-small/"},
{"title": "How do you create a log of published messages?", "time": "2020-02-02 12:13:00 -0600", "post_content": [" ", " ", " ", " ", "I'm completely new to robotics and software as a hobby. I've got my robot up and running and I'm trying to create a log file for specific messages in \"create_autonomy\".", "I'm hoping to log this once a minute in the terminal window or a .txt file", "Any ELI5 help would be awesome, I'm not sure where to start", "This video ends at what I'm trying to do\n"], "answer": [" ", " ", "You could either simply modify the driver to log that information with a ", " set at 1 second when you publish that information or create another ROS node to subscribe to that information with the same log. "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", "hour:minute:second    battery/charge \n(95%) ", "hour:minute:second   joint_states        (forward, 1.5\nmph)"], "answer_code": ["ROS_LOG_THROTTLE"], "url": "https://answers.ros.org/question/343106/how-do-you-create-a-log-of-published-messages/"},
{"title": "MeBo 2.0 on ROS?", "time": "2018-01-06 19:06:41 -0600", "post_content": [" ", " ", " ", " ", "Hello All, I\u2019m a beginner with ROS. I have it up and running just fine, and have completed the basic Turtle sim. I also have a book and some other literature on ROS, and have a pretty good idea on how ROS basically works.", "I have a robot toy called a ", "The robot has both and Android and iOS application. They way the robot is controlled is to connect to its WiFi, and then you are presented with a view through its camera with a soft button control overlay on the screen. You can go forward backwards, etc, and control the manipulator.", "There is a project out on the web that shows the MeBo web-API, but I can\u2019t seem to make the MeBo respond with HTTP-only commands (I think it\u2019s my syntax???). See for details: ", "My question is, is it possible to control the MeBo 2.0 from a laptop running ROS (Ubuntu 16.04LTS Kinetic) to control the robot with HTTP commands through WiFi, and is it possible to import the camera preview page from the MeBo\u2019s web page and use OpenCV to do basic navigation?", "The MeBo itself uses a Sonix Access Point camera board similar to the SkyViper GPS drone (less GPS of course). Although firmware is available for most of Rocket Toys products, I have yet to find the actual firmware source for the MeBo 2.0 on their github site ", "I would think this is possible, and was thinking of having the MeBo carry a ROS-enabled Raspberry Pi with Astro-Pi Sense Hat for a mobile solution. I know from reading some of the ROS tutorials, the MeBo would have to be defined with a URDF file, but I don\u2019t have much information on how I would go about adding a consumer toy robot like the MeBo 2.0", "Any help, thoughts or suggestion will be appreciated. As I mentioned I have a ROS beginner, and I\u2019m learning as much as I can on my own.", "Thank you!", "Edit: Thank you for your prompt reply!", "One note of interest is, that I've actually opened the MeBo 2.0 to see what is on the inside. The camera head has a camera and Sonix AP board with a speaker. That board communicates through a ribbon cable of sorts to the motor board in the base. The motor board has both a serial interface and a SW debug port on it. The drive train of the robot consists of a central motor for each pair of wheels... both left side and right side are each driven by an independent motor through a series of gears to give the MeBo 2.0 four wheel drive. The front axle of each drive train is internally connected to a simple wheel encoder. Feedback for the arm position consists of analog voltages going to the motor board.", "Although all of the sensors exist, there is no \"diagnostics web page ...", ": please don't post answers unless you are ", " your own question. For everything else, update your original question or use comments to interact with other posters."], "answer": [" ", " ", " For people googling for this in the future, I had help with some users on the Letsrobot.tv Website, to hack this robot, and our API is open source on this github  ", " \nit includes the Video API, and Controlls, arms, Claw, Wrist, and turning speed\nFeel free to download it! Lots of love! :) ", " ", " ", "Hello,", "This is a great toy, thanks for sharing it :)", "You have no luck, there seem to be very low to none hacker for this toy (the link you provide is not updated since a year)", "But you can be the first !", "For a project like this, I will go this way : ", "The link you provided look like a begining of this work, a .pcap file (packet capture between the app and the robot) that can be analysed to see which command are sent (", ")", "Describe the API :", "Build an API in Python or C++ :", "Python will be easier here, since you can export it to C++ if necessary", "Build a simple ROS driver :", "At this point you have a ROS driver that accept command and give access to camera.", "If you want to go further (optional but nice to have for learning) : ", "Build a URDF model of the robot : ", "You need to do it from scratch, and be very precise when you do it. Maybe ask the manufacturer for some help since hacking their toy is very good for the brand image.", "Build a MoveIt! plugin for the robot :", "Have fun :)"], "answer_details": [" ", " ", " ", " ", "Define the position of the computation module : ", "You can have an on-board raspberry, but since the toy can only communicate in WiFi, your raspberry will be on the robot communicating in WiFi with the robot. It can be good for a outdoor robot or totally autonomous robot. But you will have to manage the raspberry battery and position on the robot (pro tips : use a lot duct tape :) ). The main code will be on the raspberry, the ROS driver on the raspberry too. But since you can have only 1 WiFi connection on the raspberry (without an external WiFi dongle), you will not be able access the raspberry will the robot is running.", "You can also control the robot with a raspberry connected to your local router in ethernet, and controlling the robot remotely in WiFi. The main code will be on your computer, the ROS driver on your raspberry. You will be able to access your raspberry in ssh through the ethernet connection.", "record all data transfered from the smartphone app to the robot : \n", "Launch Wireshark or tool like this.", "Record all http command passing from the app to the robot", "analyse them to extract the commands", " ", "With Swagger.io for example or just a plain text file", "Cameron Lane, the author of the github you provide began to do it ", ", you can probably ask him to collaborate on his work)", "Expose the Python/C++ API with standard ROS message, service and/or action.", "You will be able to command the robot with standard message from a ROS node.", "To plan movement of the arm and the base", " ", " ", " ", " "], "url": "https://answers.ros.org/question/279030/mebo-20-on-ros/"},
{"title": "I cant control my rplidar(can't stop)", "time": "2017-07-31 05:05:09 -0600", "post_content": [" ", " ", "Hello, everyone \nI have a question for you.\nmy environment is Raspberry pi 3, Ubuntu mate 16.04 LTS, rplidar A2.\nI just follow rplidar tutorial(http://wiki.ros.org/rplidar)", "everything is ok, but when i killed rplidar(press Ctrl+C), \nterminal show me : shutting down processing monitor complete\nBUT, lidar continues to work(spining), How can i stop rplidar ? ", "Do i have to do extra work? or something i forgot?\nPlease answer me how to solve the problem.", "Not very familiar with rplidar, but shouldn't it always spin while powered? By killing a node you are basically killing the driver only.", "the RPLidar A2 driver (should) starts and stops the motor on launch and exit respectively.", "can you share the source of that information?", "In official node ", " and nodelet's implementation ", "I dont have any idea about how kill a node. can you show me command?", "  i just know when i stoped RPLidar, just press Ctrl+C, i don't know how kill respective node.", " I appreciate it. Thanks."], "answer": [" ", " ", "I did 2 solutions about my problem.", "so i connect power-supplied USB 3.0 hub with RPlidar. ", "add 2 lines of command", "with 2 solutions, i can start , stop my rplidar.", " ", " ", "I have a quite similar situation with you. I use Raspberry pi, jessie, rplidar A2. ", "I met the situation when raspberry is powered by another robot. ", "However, when I use rplidar with my PC ubuntu, 14.04 LTS everything works well.", "It also works well if I use a stable power source. So I think the problem may be the power supply.", "Maybe you can check your power support for raspberry or rplidar.", "By the way, to stop the lidar, you can run ", ". ", " should also works.", "Thnks I solved it. It was lack of power. so i connect power-supplied USB 3.0 hub and Rplidar."], "answer_details": ["Power problem", "it looks power problem about my rplidar.", "i used 5V/1A power connect. but when i see CPU monitor, i think it was lack about using RPlidar", "when i use rplidar i always insert 2 lines of command \n$ source /home/{user}/catkin_ws/devel/setup.bash\n$ source /opt/ros/kinetic/setup.bash", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["rosservice call /stop_motor", "Ctrl+C"], "url": "https://answers.ros.org/question/267717/i-cant-control-my-rplidarcant-stop/"},
{"title": "use an awesome package? - Getting started", "time": "2017-03-24 08:21:12 -0600", "post_content": [" ", " ", "Can anyone direct me to a tutorial on how to use a package i find in a search.", "Say i find a package or library using the ros wiki - how do I use it or implement it?", "Thanks for your patience, I'm just getting started."], "answer": [" ", " ", " ", " ", "The best way to install a ROS package, if you just want to use it, is to install it with ", " under Ubuntu.", "e.g.", "To see, whether the package you are looking for is availabe through apt, just type:", "If there is no apt package available, you typically have to go to the ", " directory in your catkin workspace, clone the git repository, that contains your package and run ", " from the root of your workspace.\nYou can get the URL to clone the repo from GitHub.", "Step-by-step:", "Hopefully, the package will build without errors. Sometimes you will have to install some additional packages or libraries via apt first which your package depends on.", "Afterwards, you should probably source your setup.bash:", "Now you are ready to run the nodes in your package, by using ", " or ", ". But the usage of each package varies individually. Hopefully you will find the specific documentation on the ROS wiki.", "A good example is IMHO the ", " driver.", "This is a good answer (especially the \"install using ", " first\", +1 for that), but could be better if you'd add a step between ", " and ", " where you use ", " to install a pkg's dependencies.", "See ", " for how that would work.", "Thanks for the comment. To be honest, I haven't used rosdep so far. It seems to be a convenient tool. I added the instructions to my answer above.", " ", " ", "you need to download the folder with the files and make it:", "$ cd pepperl_fuchs_r2000", "$ mv CMakeLists.txt.NO_ROS_LIB_ONLY\n  CMakeLists.txt", "$ mkdir build", "$ cd build", "$ cmake ..", "$ make", "other way is to do:", "$sudo apt-get install\n  ros-kinetic-<your package=\"\">", "but the pakage must exist in the repo-source!"], "answer_details": ["Go to the ROS-wiki page of your package and click on the GitHub link under the \"Source:\" bullet point.", "On GitHub click on the green \"Clone or Download\" button and copy the URL from the text field.", "Open a terminal and got to the ", " directory of your  catkin workspace.", "Type: ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["apt-get", "sudo apt-get install ros-kinetic-PACKAGE\n", "sudo apt-get install ros-kinetic-slam-gmapping\n", "apt-cache search ros-kinetic\n", "src", "catkin_make", "src", "rosrun", "roslaunch", "apt-get", "cd ..", "catkin_make", "rosdep"], "url": "https://answers.ros.org/question/257780/use-an-awesome-package-getting-started/"},
{"title": "What hardware is required to work with a kinect and rviz?", "time": "2013-08-31 01:49:29 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "for my master thesis i will have to scan a lot of people with a kinect, record the data to bag-files and visualize the rgb-pointcloud with rviz. To do that i'd like to buy a new laptop. My Question is what hardware i will need to do that.\nI guess an intel i5 processor, 8 GB of RAM and an SSD will be enought. But what about the graphics card? Has it to be a dedicated graphics chip or is an integrated intel hd 4000 ok?", "If you have worked on a laptop with a kinect and rviz, please let me know what hardware you are using and how smooth it works.", "Thanks in advance,\nBlizzard"], "answer": [" ", " ", " ", " ", "That hardware is more than enough. You don't need a dedicated graphics chip. RViz does have some problems on Intel integrated graphics chips, but those are just rendering problems of meshes (such as the robot model) and can be circumvented by using LIBGL_ALWAYS_SOFTWARE=1. If you don't want to go through that hassle, get an NVidia dedicated graphics card, but as I said, you don't really need to. (If you wanted to run a complex Gazebo simulation, or run ", ", that's a different story; you should definitely get a good NVidia card for that).", "Other points to watch out for:", "Regarding your question what kind of laptops I've successfully used to record Kinect bag files (works smooth on all of them):", "This gives me more safety in my decision for a new laptop. Thank you for your detailed answer."], "answer_details": ["The Kinect doesn't work on USB 3.0 ports, so make sure your laptop has at least one USB 2.0 port.", "You might not be able to write the point clouds to disk at full frame rate if your hard disk is too slow. One Kinect point cloud is around 10 MB, at 30 Hz that's 300 MB/s. Rosbag will buffer in memory, but once that's filled up, you'll lose some data. The best way to circumvent this is to record the depth and rgb images instead (around 2 MB/frame, or 60 MB/s) and use a script such as ", " or ", " to compute the point clouds for them offline. Or just throttle the frame rate. Anyway, I thought I'd mention it because an SSD might help there.", "The Kinect needs an external power supply (wall socket or a big battery and some soldering). If that's a problem, you could get an Asus Xtion Pro Live instead. Basically the same hardware, but USB powered.", "3.5 year old Dell Latitude E4300: Core 2 Duo, 4 GB RAM, dedicated NVidia (256 MB)", "2.5 year old MacBook Pro (also running Ubuntu, of course): Core 2 Duo, integrated Intel chip set", "0.5 year old Dell Latitude E6430: i7, 8 GB RAM, dedicated NVidia (1GB)", " ", " ", " ", " "], "url": "https://answers.ros.org/question/75835/what-hardware-is-required-to-work-with-a-kinect-and-rviz/"},
{"title": "Joint configuration issues with Schunk Powerball-MoveIt integration", "time": "2013-05-22 01:06:47 -0600", "post_content": [" ", " ", " ", " ", "Hello everybody,", "I am currently attempting to integrate the Schunk Powerball manipulator with MoveIt through the ROS packages developed by Fraunhofer IPA (github.com/ipa320) and experience some issues with the joint configuration.", "After running the setup assistant and following the instructions in the Industrial Robot Simulator tutorial, I get a configuration which works fine with the simulator. But if I then try to use the IPA packages by launching the respective packages instead of the simulator, I continuously get the following messages:", "I have configured the joint names as \"arm_X_joint\" as required by the robot's URDF description. The robot's topics use the namespace \"/arm_controller/follow_joint_trajectory\" instead of \"/joint_trajectory_action\", but I've configured this in the controllers.yaml file like written in the tutorial.", "Here are some of my launch files and the controller/joint configuration:", " (this one works fine): ", " (this one does not work): ", ": ", ":", "Now I'm really a bit out of ideas. Does anybody have an idea where this obviously wrong joint configuration might come from? Has anybody worked on the integration of the Powerball into MoveIt yet?", "I know that this might be a little stumbling in the dark if someone hasn't worked with the IPA ROS packages before, but maybe you can give me a hint where I could continue debugging. If necessary, I can provide you with some more of my launch or configuration files.", "Thank you, best regards,", "Tobi "], "answer": [" ", " ", " ", " ", "Try this launch file:", "I just took\n", "\nand put it into your\n", "\nbut I commented out the \"upload\njoint configuration\" part.(It uploads\n", "\nto the parameter server. But this\nyaml has a different joint config.\nAnd MoveIt already does this for\nyou.)", "<launch>", "</launch>", "Check your ", "\nformatting.", "If this doesn't work recursively check all launch files called by powerball.launch for parameter server uploads (yaml files) you don't need.", "Can you post your\nmoveit_controller_manager.launch. ", "Make sure you have disabled the\ncontroller_manager.", "My ", "My ", "In your error message the joint\n    \"arm_controller\" is not found.\n    Somehow the controller name\n    definition is parsed as a joint\n    definition. You could try another\n    name in your controllers.yaml and\n    check how the error changes. Perhaps\n    just make a completely new\n    controllers.yaml.", "Thanks a lot! Unfortunately, it still doesn't work although I've tried all your suggestions. Everywhere joint names appear, they are in the format arm_X_joint. I've also checked /arm_controller/joint_names and /controller_joint_names at runtime, they seem to be set correctly. Any other ideas?", "One last guess: in schunk_hardware_config/powerball/config/powerball_trajectory_controller.yaml the joint index starts with 1. In your controllers.yaml in starts with 0. Same for schunk_default_config /config/ powerball_joint_configurations.yaml. Please check new 5. in my answer.", "So indeed I had to remove arm_0_joint. But the actual error was located somewhere different, that is, there was a bug in the ipa_canopen package which made these messages appear. I've fixed it and reported it back to the IPA people. Thanks a lot for your help!"], "answer_details": ["Make sure that the urdf for the\nMoveIt Assistant was created out of\nschunk_hardware_config/powerball/urdf/powerball.urdf.xacro", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["[ERROR] [1369212664.452440575]: Joint 'arm_controller' not found in model 'powerball'\n[ERROR] [1369212664.452510068]: Joint '' not found in model 'powerball'\n[ERROR] [1369212664.452566988]: Joint '' not found in model 'powerball'\n[ERROR] [1369212664.452616925]: Joint '' not found in model 'powerball'\n[ERROR] [1369212664.452665534]: Joint '' not found in model 'powerball'\n[ERROR] [1369212664.452718125]: Joint '' not found in model 'powerball'\n", "<launch>\n # The planning and execution components of MoveIt! configured to run\n # using a simulated ROS-Industrial node\n\n #-------------------------------------\n # These actions are normally handled in the \"real\" robot's bringup launch file\n #\n # load the URDF\n<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find schunk_hardware_config)/powerball/urdf/powerball.urdf.xacro'\" />\n # run the robot simulator and action interface nodes\n <include file=\"$(find industrial_robot_simulator)/launch/robot_interface_simulator.launch\" />\n\n # publish the robot state (tf transforms)\n <node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" />\n #-------------------------------------\n\n <include file=\"$(find powerball_moveit)/launch/move_group.launch\">\n  <arg name=\"publish_monitored_planning_scene\" value=\"true\" />\n </include>\n\n <include file=\"$(find powerball_moveit)/launch/moveit_rviz.launch\"/>\n\n <rosparam command=\"load\" file=\"$(find powerball_moveit)/config/joint_names.yaml\"/>\n\n</launch>\n", "<launch>\n<param name=\"robot_description\" command=\"$(find xacro)/xacro.py '$(find schunk_hardware_config)/powerball/urdf/powerball.urdf.xacro'\" />\n <include file=\"$(find schunk_bringup)/components/powerball_solo.launch\" />\n\n <include file=\"$(find powerball_moveit)/launch/move_group.launch\">\n  <arg name=\"publish_monitored_planning_scene\" value=\"true\" />\n </include>\n\n <include file=\"$(find powerball_moveit)/launch/moveit_rviz.launch\"/>\n\n <rosparam command=\"load\" file=\"$(find powerball_moveit)/config/joint_names.yaml\"/>\n\n</launch>\n", "controller_manager_ns: pr2_controller_manager\ncontroller_list:\n  - name: \"arm_controller\"\n    ns: follow_joint_trajectory\n    default: true\n    joints:\n     - arm_0_joint\n      - arm_1_joint\n      - arm_2_joint\n      - arm_3_joint\n      - arm_4_joint\n      - arm_5_joint\n      - arm_6_joint\n", "controller_joint_names: ['arm_1_joint', 'arm_2_joint', 'arm_3_joint', 'arm_4_joint', 'arm_5_joint', 'arm_6_joint']\n"], "answer_code": ["schunk_bringup/components/powerball_solo.launch", "moveit_planning_execution_real.launch", "schunk_default_config/config/powerball_joint_configurations.yaml", "<!-- upload robot_description -->\n        <include file=\"$(find schunk_hardware_config)/powerball/urdf/upload_powerball.launch\" />\n\n<!-- start robot_state_publisher -->\n<node pkg=\"robot_state_publisher\" type=\"state_publisher\" name=\"robot_state_publisher\"/>\n\n<!-- upload joint configuration \n<include file=\"$(find schunk_default_config)/launch/upload_param_powerball.launch\" /> -->\n\n<!-- start arm  -->\n<include file=\"$(find schunk_bringup)/components/powerball.launch\" />\n\n\n<include file=\"$(find powerball_moveit)/launch/move_group.launch\">\n<arg name=\"publish_monitored_planning_scene\" value=\"true\" />\n</include>\n\n<include file=\"$(find powerball_moveit)/launch/moveit_rviz.launch\"/>\n\n<rosparam command=\"load\" file=\"$(find powerball_moveit)/config/joint_names.yaml\"/>\n", "controllers.yaml", "<launch>\n  <arg name=\"moveit_controller_manager\"\n       default=\"pr2_moveit_controller_manager/Pr2MoveItControllerManager\"/>\n  <param name=\"moveit_controller_manager\"\n         value=\"$(arg moveit_controller_manager)\"/>\n\n  <arg name=\"controller_manager_name\"\n       default=\"pr2_controller_manager\" />\n  <param name=\"controller_manager_name\"\n         value=\"$(arg controller_manager_name)\"/>\n\n  <arg name=\"use_controller_manager\" default=\"false\" />\n  <param name=\"use_controller_manager\"\n         value=\"$(arg use_controller_manager)\" />\n\n  <rosparam file=\"$(find kurtana_moveit_config)/config/controllers.yaml\"/>\n</launch>\n", " controller_manager_ns:\n    pr2_controller_manager\n    controller_list:\n      - name: \"katana_arm_controller\"\n        ns: follow_joint_trajectory\n        default: true\n        joints:\n           - katana_motor1_pan_joint\n           - katana_motor2_lift_joint\n           - katana_motor3_lift_joint\n           - katana_motor4_lift_joint\n           - katana_motor5_wrist_roll_joint\n"], "url": "https://answers.ros.org/question/63201/joint-configuration-issues-with-schunk-powerball-moveit-integration/"},
{"title": "Rosjava + Android phone + Kinect, is it possible?", "time": "2011-05-22 08:41:00 -0600", "post_content": [" ", " ", " ", " ", "Hi All,", "The announcement of the Java wrapper to run ROS on Android phones is really exciting. I was wondering how someone might connect a Kinect to an android phone to use it as a sensor? I'd like to build a small robot and an Android phone seems like a great alternative to a fully blown laptop.", "Any help would be appreciated."], "answer": [" ", " ", "Two thoughts:", " ", " ", "If the support for ROSJava is limited at the moment, for the purposes of getting an Android phone up and running quickly with a Kinect, is there an immediate disadvantage in using a rooted Android phone with an install of Ubuntu (and then ROS), as opposed to RosJava? ", "Would the rooted phone with an arm version of ubuntu give me the support to run the Kinect?"], "answer_details": ["You'll have the usual powering-the-Kinect problem. Just a USB port isn't enough; see the ", " for details on how to build a power supply in a particular case.", "The Kinect (both the OpenNI and libfreenect versions) are in C[++], not Java. I don't know if user processes on Android have to be in Java, but just having ROSJava doesn't get you Kinect support out of the box.", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/10025/rosjava-android-phone-kinect-is-it-possible/"},
{"title": "turtlebot bad gyro calibration (also)", "time": "2011-09-06 12:04:39 -0600", "post_content": [" ", " ", "Hi there.\nI read the post from the other gentleman that was having bad gyro calibration issues, but the thread did not seem to be same issue as mine so starting new thread.\nI have ran the calibration steps, create stopped facing the wall where it started, I created my own launch file basically identical to the one on the tutorial, except using my calibration values.\nHowever even when I start up my turtlebot with the new custom gyro and odom compensation settings, I still get a diagnostic error in the dashboard that says. Gyro Sensor: Bad Gyro Calibration.\nGyro enabled= true, raw gyro rate = 8 calibration offset = 8", "Of course when I try to do room mapping of a basic hallway, the map is just a jumble and is all over the place. Any suggestions for how to troubleshoot would be appreciated.\nThanks,"], "answer": [" ", " ", " ", " ", "If your gyro is returning values of 8 for the raw gyro rate then you may have the same problem as the other post. Changing the calibration values will not cause the gyro to return values of 8 for the raw values. This most likely means that the gyro is not powered and the irobot create is reading the noise on the line. ", "One of three senarios can cause the gyro not to be powered: "], "answer_details": ["The irobot create did not power on correctly. Try power cycling the irobot create.", "The power board is not seated properly. Try pressing down on the power board to make sure it is properly seated on the connector. ", "There is an electrical problem with the power board or irobot create. Try following the steps listed in this question ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/11130/turtlebot-bad-gyro-calibration-also/"},
{"title": "TurtleBot -- Bad Gyro Calibration", "time": "2011-08-30 03:17:33 -0600", "post_content": [" ", " ", " ", " ", "I have been running the SLAM map building tutorial with the turtlebot and I have been having a problem where the turtlebot image in Rviz does not rotate nearly as much as the actual real life turtlebot.  This causes the map that is being created to be completely wrong, as the turtlebot is not putting the data it finds in the right place on the map.  I have run the gyro and odometer calibration and have set the rosparams to the proper values.  I am also getting an error in Rviz under the robot model:", "rear_wheel_link\nNo transform from [rear_wheel_link] to [/map]", "and a warning under the path (path) and pose array (pose array) that says \"no messages received\"", "as a further error, my turtlebot dashboard diagnostics is giving me an error:", "Full name: /Sensors/Gyro Sensor\nComponent: Gyro Sensor\nHardware ID: ", "Level: Error\nMessage: Bad Gyro Calibration", "Gyro Enabled: True\nRaw Gyro Rate: 5\nCalibration Offset: 5\nCalibration Buffer: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ..."], "answer": [" ", " ", " ", " ", "To debug this you're going to need to disassemble your TurtleBot. You'll need a DMM as well. ", "Remove the platters from your TurtleBot so that you can get at the TurtleBot power board. Put your DMM in DC voltage mode and touch pins 8 and 14 with the DMM probes and to read the voltage. The voltage should be 5V. ", "Top down view of the power board header pins:", " ", " ", "The rear_wheel_link warning is due to that link being unactuated and free floating it cannot be rendered due to lack of information.  ", "The warning about the path is because a planning path has not been sent yet.  And when running SLAM gmapping doesn't publish the pose_array like amcl does.  ", "The \"Bad Gyro Calibration\" is your problem.  Can you cut and paste the entire error from the dashboard for that error, and edit your question to include it?", " ", " ", "I took it apart and ran those tests and it was generating 5V just fine in both the serial port directly and the power board.  Upon putting it all back together, it worked!  Perhaps the power board had just become a little unseated.  Either way, problem solved!"], "answer_details": ["If the voltage is not 5V this is most likely a problem with your\nirobot create base.  ", "If it is 5V this is most likely a problem with your power board.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/11061/turtlebot-bad-gyro-calibration/"},
{"title": "rosbag recording pointclouds", "time": "2011-09-30 01:59:31 -0600", "post_content": [" ", " ", " ", " ", "Hi,\nI am using rosbag in rosdiamonback for recording pointcoulds sequences (video). So far, I feel it could not provide smooth running video, there is always lagging or even lost on some frames. While recording on images give nice smooth video from frame to frame, at least no obvious lost frames. I am wondering if it is the problem from the rosbag itself for pointclouds recording or I am missing something.", "If someone has alternative solution for pointcloud sequences recording, pls let me know.", "Best,\nKang"], "answer": [" ", " ", "Pointclouds are extremely demanding in terms of bandwidth and space usage. I am surprised to see that you are recording at full frame rate. Unless you really have a fast computer (at least an I7 processor) you will have problems running rosbag play on a pointcloud bag and rviz on the same machine.", "You should have a look at various pointcloud compress packages, for example:", "As a rule of thumb, always try to save the pointcloud from the same computer it is producing it, and load it from the same computer it is processing it.", "Neither seem to be part of ROS anymore (at least they are not indexed and the provided links don't work). Any advice what to do using a newer version of ROS (mine is Indigo)?", " ", " ", "Tools like ", " and ", " are useful for determining how quickly your device is publishing data. If the bandwidth is too high, your system may be having trouble storing it fast enough.", "You say the pointclouds are being recorded as video. With ", ", compressed JPEG video can be recorded by appending ", " to the topic name."], "answer_details": [" This is very simple as pubishes a zipped message, and it has the unzipper as well. The power of Python!", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["rostopic hz", "rostopic bw", "image_transport", "/compressed"], "url": "https://answers.ros.org/question/11378/rosbag-recording-pointclouds/"},
{"title": "localizing multiple ARMarkers using ar_pose", "time": "2012-02-27 19:01:33 -0600", "post_content": [" ", " ", " ", " ", "I have some general questions about ARMarkers and also the ROS wrapper, ar_pose. I searched for general information on using them in practice but couldn't find any detailed information.", "My goal is to put markers on 25 dishes or so on a tabletop and some shelves. The camera won't ever be more than 6ft away. Is that doable? ", "Any tips or lessons learned from experience would be appreciated.", "Thank you very much in advance!"], "answer": [" ", " ", "Hi. I have been using ar_pose for some time, but I am afraid I cannot answer exactly your questions, but maybe give some insights.", "I am using markers as an easy way to track people. I created big hats (markers are 25x25 cm) and the markers can get very far, and still be detected. My ar_pose frame rate is pretty much limited by the camera frame rate. I think your goal is doable, you just need to find a proper configuration of markers size/camera resolution/camera distance.", "Hope this helps."], "answer_details": ["I ", " there is no limit (but I may be wrong).", "Do not know, never measured it.", "It depends on your camera resolution and its distance to the markers, you can make your markers very small as long as your camera can ", " their details.", "Same as 2.", "That depends on your computer power and the number of markers you are detecting.", " ", " ", " ", " "], "question_details": [" ", " ", "What is the maximum number of ARMarkers that can be used in a scene at once? I'm assuming it depends on the dimensions of the markers...", "What's the expected amount of error when localizing a marker from 5ft away? ", "What's the smallest size marker that a camera can localize 'accurately' from 5ft away? (Is there such a thing as 4cm x 4cm markers? Do they get smaller than that?)", "How much does the accuracy improve when going from a 640x480 camera to a 2MP camera?", "What is the average frame rate from the ar_pose node that is capable of detecting multiple armarkers? When using a 640x480 camera and a 2MP camera? "], "url": "https://answers.ros.org/question/28661/localizing-multiple-armarkers-using-ar_pose/"},
{"title": "the best detect and recognition method", "time": "2012-11-29 13:03:13 -0600", "post_content": [" ", " ", "Hi all, i am building my vision system to detect and recognition objects ..", "firstly, i did object 3D model cloud points Using ", " ", "then tried to detect it with ", " package but it isn't the best because:", "i will make some reasoning relations on these objects after that in Knowrob ", "So, i want a help to choose the best method to detection and recognition objects \ni am reading in bigg_detector now and searching at others ..", "any help to choose the best detect and recognition method ??"], "answer": [" ", " ", "Hi Salma,", "There are several object detectors/recognizers that you could test to have an idea of the complexity of the task and figure out which one better fits your needs:", "HTH", "  thanks alot ,, very useful links ... i am building a system can deal with roboearth server\nSo i nees OWL description of the model to be knowledge in Knowrob  thats why i used roboearth stack ,, i don't know if I must use it to detect and recognition the object or can use any method   ", " ", " ", "Unfortunately, there is no \"best\" object detector.  Each one works best on some objects, and less well on others.  If all of your objects are in that set that it works well on, then that's the \"best\" detector.  Without knowing more about your objects, and your application, it's hard to give more concrete advice.", " ok thanks bill ,, i am building system can deal with Roboearth Server to share knowledge between robots .. my application is to recognition objects and make reasoning operation to find what is missing component on the table ,, so i am restricted by OWL description of models as i think "], "answer_details": [": provides you LINE-MOD (which should detect non-textured objects), TOD and transparent objects detection", ": PCL is indeed a powerful option to detect non-textured objects", ": monocular model based object detector (see tutorials in ", ")", "and many more out there ...", " ", " ", " ", " ", " ", " ", " ", " "], "question_details": ["detect Only texture objects and i have One colored objects", "red marker not located all the time ", "can't detect the object when turning it reversal", "i tried to add 2 models to detect them in the same time but it didn't work"], "url": "https://answers.ros.org/question/49444/the-best-detect-and-recognition-method/"},
{"title": "Wireless data transfer to multiple subscribers", "time": "2013-05-31 05:59:49 -0600", "post_content": [" ", " ", "In our current setup, we have a laptop connected wirelessly to our robot, everything running ROS. The laptop does most of the processing and sends commands back to the robot, pretty standard stuff.", "However, we have noticed that if we are recording all (or even some) of the data being sent by the robot (with rosbag record) while also doing the usual processing, the laptop slows down intermittently, negatively affecting the rate of control commands being sent out.", "As a potential solution, we would like to use two laptops, one for data processing and control command generation, and the other one for data recording and maybe data visualization. This would also nicely split tasks between people, with the controller laptop operator focusing on running his algorithm an the data recording laptop operator making sure the experiments are running as designed.", "However, I am concerned about the data being sent wirelessly being needlessly duplicated if it has to go to both laptops. The setup I envision has the two laptops connected (wired) to a network switch, along with our wireless radio (a Ubiquity Networks NanoStation) so everything is on the same subnet. On the other side of the wireless connection we have a Ubiquity Networks Rocket connected to a single PC also on their own subnet.", "Is my concern warranted, will the data be sent twice if both laptops are subscribing to a topic? Some of the data are images from various cameras so double sending this would be very bad. Is there any way to resolve this problem other than having one of the laptops relay messages to the other (so the second laptop would only subscribe to relayed topics on the first laptop)? "], "answer": [" ", " ", "To make sure your topic goes over the air only once when having multiple subscribers, you can have a look at ", ". A relay on (one of) the laptop(s) would subscribe to the image from the robot and republish it to multiple subscribers on a different topic in your local wired network. ", "Thanks Markus, this looks like what I need. I'll try to write a node relays all topics from a given machine, essentially acting as a proxy for the aircraft.", "Thank you Markus. topic_tools/relay is working well for this purpose", " ", " ", "Yes, your concern is warranted: for each publisher-subscriber pair, a new TCP-connection is established. So the amount of bandwidth is proportional to the number of subscribers.", "I can't think of any elegant solutions right now, my only ideas:", "Thanks Philip. I'm working with aharmat on this, and we'll be implementing the 3 suggestions you made. I think we'll use topic_tools/relay instead of republish, because it is more general. It seems as though the problem is the Wifi, not the local recording, so we could use 1 machine."], "answer_details": [" ", " ", " ", " ", "Republish the data as you suggested. In fact, there already is a ", ". However, you'd probably have to rename the topics in your code.", "Try compressing your image topics by installing the required codecs on all machines and using the compressed streams (see ", "). Mayby this already saves enough bandwidth for doing the connection twice (which I doubt and doesn't scale further)?", "Use a powerful pc that can both control the robot and record data. Have one or both of the \"operators\" work remotely on this machine (", ").", " ", " ", " ", " "], "answer_code": ["ssh -X"], "url": "https://answers.ros.org/question/64068/wireless-data-transfer-to-multiple-subscribers/"},
{"title": "Turtlebot 2 on Virtual Machine with no Kinect picture", "time": "2013-05-24 05:17:38 -0600", "post_content": [" ", " ", " ", " ", "Hello,", "I am using a TurtleBot 2 with a Kobuki base. I can bring up the Kobuki dashboard and all the lights are green, but I get a constant \"no devices connected\" log message. It appears my Kinect is not properly working with the software (the light on the cable is not lit, though the light on the Kinect itself is)", "Apparently this is a known issue that can be fixed by changing to \"full mode\" in the TurtleBot Dashboard, but I've looked all over the place and I can't figure out how to do this for the Kobuki version of the dashboard. Can anyone please tell me how to do this?", "This is my first post here and I'm very new to everything about Linux, ROS, and the TurtleBot in general, so I sincerely apologize if this question is poorly worded or irrelevant.", "Thanks in advance.", "------------------EDIT------------\nAdding the steps I've taken, but needed to remove anything that looked like a link because I don't have enough karma to post those...", "First thing I do is connect the power to the Kinect (light on the cable goes green), and connect the Kinect USB and the Kobuki USB to the laptop.\nThen, I run ", " and get the following:", "After this, the Kobuki plays a little tune. \nThen, I ..."], "answer": [" ", " ", "Hi Pants,", "I'm not aware of any 'full mode' for Kobuki - maybe that belongs to the Create?\nIf you talk about the launch files, then there is a minimal.launch, which only brings up the minimal configuration needed for the TurtleBot to run (e.g. does not include the driver for the Kinect). On top you are supposed to start whatever you need to run your application, e.g. navigation. Please add to your question what programs/nodes/launchers you are starting/using to reproduce your problem.", "Looking at your problem that the Kinect is not working, I noticed you have mentioned that the LED on the Kinect cable is not lit. That's odd. It is supposed to be turned on. Please check, that the connection cable is plugged into the netbook (data) and the Kobuki base (power).", "Hi bit-pirate, thanks for the response! The reason I expected full mode to solve the problem is this line in the user manual for Turtlebot 2: \"Kinect is not responding and green light on Kinect adaptor is off. Either issue a motion command to the Create or use the SetTurtlebotMode service in ", " turtlebot_node to change the mode to \"full mode\" . \"", "Anyway, these are the steps I've taken:\n1. Put desktop and laptop on same network (via Hamachi VPN)\n2. Plug laptop to Kobuki via USB cable\n3. Plug Kinect power to 12V 1.5A port on Kobuki, and Kinect USB to laptop\n4 Turn Kobuki on", "The window that opens shows no Kinect output and I'm not surprised, since the cable light is still not lit even though the Kinect light is. Is there another roslaunch command that is needed to start up the Kinect? Trying to follow the tutorial (http://ros.org/wiki/turtlebot) exactly", "BTW, running ROS Groovy and Ubuntu 12.04 (64 bit) on a VirtualBox on both computers. I can run the keyboard teleop demo perfectly fine. \nSorry for the comment spam. A little confused by the character limit and not sure if I'm doing something wrong here...", "I have found the advice you mentioned in Clearpath's TurtleBot 2 manual. Is this the one you are referring to? Since they mention the Create, which is used in the TurtleBot 1, I'm assuming they just missed to update it. For Kobuki - used in TurtleBot 2 - there is no such thing as a \"full mode\".", "Now looking at your Kinect problem: 1. Try to avoid using virtual machines. I tried it once and failed with getting the Kinect running. Others noticed similar problems. Not sure, if there is a working solution other there to get the Kinect running inside a virtual machine.", " ", " ", "Hi,", "In case anyone is still interested in this question, I gave up using VirtualBox entirely. I ended up buying this laptop:\nwww.woot.com/offers/asus-11-6-touchscreen-core-i3-vivobook", "I set up a dual-boot of Ubuntu 12.04 alongside Windows and suddenly the rviz and follower tutorials work fine. In the future I'd love to use some port monitor to see why the Kinect wasn't showing properly.", "Thanks to bit-pirate for contributing.", "I'm glad you found a solution to your problem. Please close this question by selecting the answer which you like most (click the checker sign on the top left of the answer). Thanks!"], "answer_details": [" ", " ", " ", " ", "On laptop, run \"roslaunch turtlebot_bringup minimal.launch\" and wait for the Kobuki to play a happy sound", "On desktop, run \"rqt -s kobuki_dashboard\" -- all the lights on the top left of the window are green", "Open a new terminal on the laptop and run \"roslaunch turtlebot_bringup 3dsensor.launch. After this step, the dashboard on the workstation starts displaying \"no devices connected\" messages.", "Open new workstation terminal and run \"roslaunch turtlebot_rviz_launchers view_robot.launch\" ", " ", " ", " ", " "], "question_code": ["roslaunch turtlebot_bringup minimal.launch", "SUMMARY\n========\n\nPARAMETERS\n * /app_manager/interface_master\n * /bumper2pointcloud/pointcloud_radius\n * /cmd_vel_mux/yaml_cfg_file\n * /diagnostic_aggregator/analyzers/input_ports/contains\n * /diagnostic_aggregator/analyzers/input_ports/path\n * /diagnostic_aggregator/analyzers/input_ports/remove_prefix\n * /diagnostic_aggregator/analyzers/input_ports/timeout\n * /diagnostic_aggregator/analyzers/input_ports/type\n * /diagnostic_aggregator/analyzers/kobuki/contains\n * /diagnostic_aggregator/analyzers/kobuki/path\n * /diagnostic_aggregator/analyzers/kobuki/remove_prefix\n * /diagnostic_aggregator/analyzers/kobuki/timeout\n * /diagnostic_aggregator/analyzers/kobuki/type\n * /diagnostic_aggregator/analyzers/power/contains\n * /diagnostic_aggregator/analyzers/power/path\n * /diagnostic_aggregator/analyzers/power/remove_prefix\n * /diagnostic_aggregator/analyzers/power/timeout\n * /diagnostic_aggregator/analyzers/power/type\n * /diagnostic_aggregator/analyzers/sensors/contains\n * /diagnostic_aggregator/analyzers/sensors/path\n * /diagnostic_aggregator/analyzers/sensors/remove_prefix\n * /diagnostic_aggregator/analyzers/sensors/timeout\n * /diagnostic_aggregator/analyzers/sensors/type\n * /diagnostic_aggregator/base_path\n * /diagnostic_aggregator/pub_rate\n * /mobile_base/base_frame\n * /mobile_base/battery_capacity\n * /mobile_base/battery_dangerous\n * /mobile_base/battery_low\n * /mobile_base/cmd_vel_timeout\n * /mobile_base/device_port\n * /mobile_base/odom_frame\n * /mobile_base/publish_tf\n * /mobile_base/wheel_left_joint_name\n * /mobile_base/wheel_right_joint_name\n * /robot/name\n * /robot/type\n * /robot_description\n * /robot_pose_ekf/freq\n * /robot_pose_ekf/imu_used\n * /robot_pose_ekf/odom_used\n * /robot_pose_ekf/output_frame\n * /robot_pose_ekf/sensor_timeout\n * /robot_pose_ekf/vo_used\n * /robot_state_publisher/publish_frequency\n * /rosdistro\n * /rosversion\n * /turtlebot_laptop_battery/acpi_path\n * /use_sim_time\n * /zeroconf/zeroconf_avahi/services\n\nNODES\n  /zeroconf/\n    zeroconf_avahi (zeroconf_avahi/zeroconf)\n  /\n    app_manager (turtlebot_app_manager/app_manager)\n    appmaster (turtlebot_app_manager/appmaster)\n    bumper2pointcloud (nodelet/nodelet)\n    cmd_vel_mux (nodelet/nodelet)\n    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)\n    mobile_base (nodelet/nodelet)\n    mobile_base_nodelet_manager (nodelet/nodelet)\n    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)\n    robot_state_publisher (robot_state_publisher/robot_state_publisher)\n    turtlebot_laptop_battery (linux_hardware/laptop_battery.py)\n\nauto-starting new master\nprocess[master]: started with pid [2069]\nROS_MASTER_URI=*removed because I can't post links*\nsetting /run_id to 31b356b6-c889-11e2-a601-0800271ddb21\nprocess[rosout-1]: started with pid [2082]\nstarted core service [/rosout]\nprocess[zeroconf/zeroconf_avahi-2]: started with pid [2085]\nprocess[robot_state_publisher-3]: started with pid [2086]\nprocess[diagnostic_aggregator-4]: started with pid [2107]\nprocess[mobile_base_nodelet_manager-5]: started with pid [2119]\nprocess[mobile_base-6]: started with pid [2120]\nprocess[robot_pose_ekf-7]: started with pid [2121]\nprocess[cmd_vel_mux-8]: started with pid [2140]\nprocess[bumper2pointcloud-9]: started with pid [2141]\nprocess[turtlebot_laptop_battery-10]: started with pid [2260]\nprocess[appmaster-11]: started with pid [2409]\nprocess[app_manager-12]: started with pid [2496]\n"], "url": "https://answers.ros.org/question/63428/turtlebot-2-on-virtual-machine-with-no-kinect-picture/"},
{"title": "Parallax Gyro work on Turtlebot 1?", "time": "2014-01-07 14:12:07 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I've built my own Turtlebot using the iRobot create and xBox Kinect. I have the latest ROS installed and running. I picked up a ", " (", ") from my local RadioShack and wanted to know if it will work with the ROS software. I found this post: ", " (//answers.ros.org/question/9749/is-an-imu-required-to-run-turtlebot-stack/?answer=14244#post-id-14244) and was wondering if it would connect to the create the same way.", "I've been building this to learn programming/robotics and pretty good at following directions. But don't have the knowledge to start from scratch yet. I want to think everyone that shares their projects and code. It really helps us newcomers getting started.", " Would this be the correct connections for this gyro if ROS supports it?\n(", ")"], "answer": [" ", " ", "This post is a wiki.\n        Anyone with karma >75 is welcome to improve it.", "One the Turtlebot1, the ", " was attached to one of the analog input pins on the Create.", "To determine if your gyro is compatible, you'll need to check a few things:", "If you find a compatible gyro, make sure that the axis that you hook up to the Turtlebot's analog input pin is the vertical axis.", "EDIT:", "The gyro you've picked out is a digital gyro, and won't be compatible with the Turtlebot1.", "ahendrix, thanks for the reply. I misunderstood the description of this gyro. I thought that it could be wired as a digital or analog ( three or four ) wire setup. I'll look for an analog gyro. Thanks again for your time."], "answer_details": ["Does it use the same power supply voltage as the original gyro? (5V)", "Does it have an analog output pin?", "Does the analog output have the same magnitude as the original gryo? The original gyro specifies 12.5 mV/degree/sec", " ", " ", " ", " "], "url": "https://answers.ros.org/question/115741/parallax-gyro-work-on-turtlebot-1/"},
{"title": "Turtlebot unable to check laptop battery/BAT0/state", "time": "2012-03-27 19:00:19 -0600", "post_content": [" ", " ", " ", " ", "My laptop (Thinkpad X120e) reports its battery status in proc/acpi/battery/BAT1/ folder (which does have the status file), but NOT in BAT0 that turtlebot expects. How do I either change the Ubuntu environment to use BAT0 or change the minimal.launch script to look in the BAT1?"], "answer": [" ", " ", "I had the same problem.  Since the EeePC 1215N was discontinued, the Turtlebot now ships from I Heart Robotics with a Lenovo ThinkPad X130e.  This laptop's battery shows up in /proc/acpi/battery/BAT1, not BAT0 as is hardcoded into ROS electric's laptop_battery.py.  Also, the battery's capacity and charge state is reported in mW instead of mA, so just swapping BAT0 to BAT1 in the script doesn't fix it.", "I've uploaded an updated version of electric's laptop_battery.py that should fix both of these problems:\n", "I use a shell glob to find BAT*, and recognize either mW or mA for battery capacity. This should work with both EeePC and ThinkPad based turtlebots, but I only have the ThinkPad to test with. To use the new script, from your turtlebot:", "Thanks for the help! I have patchified things and made some adjustments. Please post feedback on the patch to the ticket.", " ", " ", "Now upgrading to Fuerte and reading ticket#134 I would appreciate guidance. The laptop_batterry.py file in fuerte /stack appeas to be the \"bat0\" version and the patch includes the \"bat*\" text which sucessfully enabled the \"bat1\" in my x120e. I presume the patch should be applied. How?", " ", " ", "Tnx. Edited \"laptop_battery.py\" in .../scripts folder changing 2 instances of BAT0 to BAT1. It eliminated the error message occurring during the Netbook minimal.launch but the Workstation \"turtlebot_dashboard\" laptop battery icon is still greyed out (Turtlebot battery status is reported). Is there another instance of BAT0/BAT1 that needs to be edited somewhere?", "There's possibly a rule in the battery diagnostic aggregation which might need to be changed.  ", " ", " ", "To change the node reporting you will need to edit laptop_battery.py in turtlebot_node package.  A patch to parameterize it would be greatly appreciated.  ", " ", " ", " ", " ", "This edit did it and Dashboard Laptop battery icon works now with Green icons. One note: In step 3 sudo can't be used, just the \"rosmake\". "], "answer_details": ["wget ", "sudo cp laptop_battery.py /opt/ros/electric/stacks/turtlebot/turtlebot_node/scripts/laptop_battery.py", "sudo rosmake turtlebot_node", "sudo service turtlebot restart", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/30631/turtlebot-unable-to-check-laptop-batterybat0state/"},
{"title": "Recommended quadrotor for using hector_slam package", "time": "2014-03-09 17:49:12 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "Which would be the recommended quadrotor for using hector_slam. I am currently trying the nicely documented tutorial : hector_quadrotor ( ", " ) . I was wondering on which all quadrotor everyone is using the hector_slam package. ", "While looking for this answer, I came across these links:", " ", " ", " ", "I could not figure out which quadrotor was used for development and testing of Hector SLAM. So with my finding, my best option (might be only) would be Ascending Technologies Pelican. Has anyone tried using Hector SLAM on Pelican ? ", "-Suraj", "According to their paper, hector_slam was developed on a ground vehicle."], "answer": [" ", " ", " ", " ", "There isn\u00b4t really a specific platform that I heard about to work best. Ideally, your platform should", "A few examples of (mostly Mikrokopter based) quads running hector_mapping can be found in ", ". ", " you can see an example of running a bag-file of the CCNY dataset through hector_mapping (originally recorded with a Asctec quad).", "Thanks Stefan."], "answer_details": ["have a Hokuyo UTM-30 (or similar)\nLIDAR", "provide enough computing power (old\nAtom Z530 was sufficient, most more\nmodern options should be sufficient)", "provide sensible IMU data (especially\nroll/pitch). That\u00b4s basically a given\nfor a quadrotor, as it wouldn\u00b4t be\nable to fly otherwise ;)", " ", " ", " ", " "], "url": "https://answers.ros.org/question/137701/recommended-quadrotor-for-using-hector_slam-package/"},
{"title": "Which robot can or should I get to run ROS?", "time": "2011-02-15 10:46:54 -0600", "post_content": [" ", " ", " ", " ", "I've heard a lot of good things about ROS and I want to try running it.", "PR2 is a little too expensive to me though. Is there anything more afordable to give ROS a try? \nWhat is the best use of my money?"], "answer": [" ", " ", "Note that if you want to try running ROS you don't really need robot hardware. The cheapest option costs no money at all - It's running a robot in simulation. There are quite a few robot models available for doing that. I can recommend the 'erratic_robot' stack, which provides a differential driven robot that you can use to drive around in gazebo simulation.\nIt of course always depends on your focus, but for playing around, understanding basic concepts of navigation, the coordinate frame conventions etc. this is a very good option.", " ", " ", "Depending on what you want to try with ROS, you can start with a relatively basic platform.  I think that the main requirements for this platform (in order to be able to fully appreciate ROS) would be:", "For basic navigation, mapping, and SLAM work, this should be enough to whet your appetite.  As you move to more advanced concepts (such as arm motion planning), you may need more advanced hardware, but this will cover a wide variety of topics.", "As an aside, I have essentially described the ", " that ", " linked to in his answer, just letting you know that other options are also available.", "FYI, Bilibot is officially dead.", " ", " ", "There are many robots using ROS.  See ", "For a learning platform something like the Create or Rovio might be a good place to start.  There are several projects working to build up the user experience on the create such as ", "/ coming soon.  ", " ", " ", "Some answers have mentioned the Bilibot, which is discontinued, or the Turtlebot, which is a bit rickety and expensive. The ", " costs about the same as a Kinect-enabled Turtlebot, but looks a lot nicer and has more features. I haven't used it though.", "Qbo hasn't had updates in more than a year, and runs ubuntu 11.10 + ROS electric (both very end-of-life), so definitely not recommended.", "The Turtlebot2 is an updated version of the original Turtlebot and is much more robust. I've been borrowing one from a friend and I'm enjoying it.", " ", " ", " ", " ", "If you want a relatively cheap but still powerful and flexible setup then also you might want to look at ", ". It is basically a mini-ITX board customized towards robotics needs.\nWe are holding a robotics course here at KTH with students building a robot and writing software for a contest from scratch, they start only with a toolbox, aluminium sheets and wheels + RoBoard + a webcam + ", " + couple of range finding sensors (IR + Sonar mostly).", "The end goal is to have a contest where students compete in a simulation of a rescue mission. The robot is in a maze, maps and \"rescues\" golf balls (victims) in the priority order indicated by a near by barcode from the maze all the while avoiding bombs.", "Roboard also has a ROS repository and an OK C++ library.\nHere's the ROS blog post about Roboard\n", "here's the course wiki with instructions:\n", "/\nHere's a student group video with a robot that they've built from scratch.\n", "Hope this helps.", "This year we want to switch to ROS, I guess at some point I should send an email to ROS people when it matures.."], "answer_details": [" ", " ", " ", " ", "Locomotion: Some way of moving around in an environment.  As Tully suggests, an iRobot Create is a good approach.  You may also roll-your-own.  Generally, if you design your own platform, you will have to write your own nodes to interface with the motors, rather than relying on previously-written stacks.  A suggestion, though, encoders (or some sort of odometry) are more or less a must for navigation applications.", "Computation: A (somewhat portable) computer that is capable of running ROS.  There are plenty of solutions for this from a single board computer up to a full motherboard (or servers like the PR2!).  Computation power is typically determined by how much you want to do.  You can play with some of the ROS nodes on your computer to get a feel for what size computer you will need (and then buy bigger!)", "Sensing:  In order to do navigation and SLAM, you will need some way to see your environment around you.  The Kinect has really revolutionized this part, mainly because of the price point.  Other sensors are available, such as laser range finders (not cheap), or webcam inputs.", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/9072/which-robot-can-or-should-i-get-to-run-ros/"},
{"title": "How to power off Kinect from software", "time": "2015-01-25 08:32:39 -0600", "post_content": [" ", " ", " ", " ", "I have a Kinect connected to the onboard Linux-based PC on my robot. \nHowever, in many scenarios I am not using the Kinect, so I would like to be able to turn it off from command line to save power.", "I tried the following:", "This works, in the sense that the power light of the Kinect goes off for a moment, and the USB device is removed. However, the Kinect gets redetected within a second - re-enabling it.", "Following results in the same behaviour:", "Note that I am using a 3.4 kernel, and following does ", " work:", "Is there a way to disable the Kinect from software?", "Thank you!"], "answer": [" ", " ", " ", " ", "I found that the command", "works, but ", "What works reliably for me is the following:", "Note that the 2.5 seconds sleep is crucial - 2 seconds will not work (too fast), 3 seconds neither (too slow). This was so on both machines on which I tested this.\nIt seems that the sweet spot is when the Kinect USB Hub is detected, but not yet any of the actual Kinect devices:", "When troubles finding the sweet spot, you can just \"bruteforce\" it, as follows:", "Finally, to re-enable:", " ", " ", "The kinect is powered by an external 12V power supply, so turning it completely off from software will probably require special hardware. There may be ways to reduce the Kienct's power consumption without turning it completely off.", "There are a few steps that I would take if I was investigating this:", "Now look at all of your power measurements, compute (or measure) their affect on your battery life, and figure out which steps actually cause the system to use more power.", "I suspect the big power consumers in your system are actually the kinect drivers and the algorithms, and that you can significantly reduce your power consumption just by stopping those nodes when you're not using them. The ", " package may be useful for implementing this.", "I did some measurements with a crude power meter (resolution: 0.01A), and found following:\n- Kinect connected to power supply only: 0.00 A\n- Kinect connected to power supply, and to USB: 0.06A going to 0.04A\n- Kinect driver running: 0.06A going to 0.04A\n- Kinect app running: >0.06A", "So yes, it seems you are right, it is not worth bothering about those 40mA extra being used while idling.", "Also thanks for the capabilities pointer, will definitely look into that!", "But the Kinect V2 will give us a reason to worry about in the future. With 12.2V: Kinect connected to power supply only: 0.00 A - Kinect connected to power supply, and to USB: ~0.38A - Kinect app running: ~1.18A(+-0,06A)", "I just detected a major flaw in my current measurement setup, and retested:\nKinect connected to power supply only: 0.00 A (baseline) - Kinect connected to power supply, and to USB: 0.50A stabilizing at 0.38A - Kinect driver running: 0.38A - Kinect app running: >0.45A", "... which is in line with what ", " reports.", "So it seems to make sense to try disabling the kinect completely - as that would save about 0.38A", "You'll probably need to build a custom circuit to disconnect power to the kinect when you're not using it. To determine if that's a worthwhile effort or not, try to figure out your current battery life, total power consumption, and how much run-time you'll gain by disabling the kinect."], "answer_details": [" ", " ", " ", " ", "Measure the power consumption of your system, without the kinect plugged in. This gives you a baseline \"zero\" to compare future results against.", "Plug in the kinect, and measure the power consumption without the ROS drivers running.", "Run the ROS drivers, and measure the power consumption. I suspect most of the power draw associated with the kinect is actually associated with streaming data from it and processing that data.", "Start up the rest of your ROS nodes, and measure the increased power consumption associated with running your algorithms. (I suspect this is significant).", " ", " ", " ", " "], "question_code": ["\necho '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "\necho '1' > /sys/bus/usb/devices/1-1/remove\n", "\necho '0' > /sys/bus/usb/devices/1-1/power/autosuspend_delay_ms\"\necho 'auto' > /sys/bus/usb/devices/1-1/power/control\"\n"], "answer_code": ["echo '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "echo '1-1' > /sys/bus/usb/drivers/usb/unbind; sleep 2.5; echo '1-1' > /sys/bus/usb/drivers/usb/unbind\n", "[ 3451.492033] usb 1-1: new high-speed USB device number 109 using ehci-pci\n[ 3451.624348] usb 1-1: New USB device found, idVendor=0409, idProduct=005a\n[ 3451.624351] usb 1-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\n[ 3451.624860] hub 1-1:1.0: USB hub found\n[ 3451.624970] hub 1-1:1.0: 3 ports detected\n", "for (( i=0 ; i < 10 ; i++)) ; do echo '1-1' > /sys/bus/usb/drivers/usb/unbind; sleep 0.5; done\n", "echo '1-1 > /sys/bus/usb/drivers/usb/bind\n"], "url": "https://answers.ros.org/question/201704/how-to-power-off-kinect-from-software/"},
{"title": "What topic should I subscribe to know when the drone is in \"hover state\"?", "time": "2015-01-24 09:54:52 -0600", "post_content": [" ", " ", "Basically, I'm trying to write a piece of code that waits for the drone to enter hover state and then take a picture from the drone's camera. I've manage to take the picture (by subscribing to ardrone/image_raw topic). I only need to know what topic should I subscribe so I can set up a subscriber and using a callback function I will be able to take pictures from the drone only when it is in hovering state.", "Thank you in advance.", "p.s:\nI use tum_ardrone and ardrone_autonomy to communicate with the drone. I've also created a package that uses some opencv code to take the picture from the drone's camera."], "answer": [" ", " ", "Hello\nwhat means TBA unit"], "answer_details": [": ROS message header", ": The remaining charge of the drone's battery (%)", ": The Drone's current state: * 0: Unknown * 1: Inited * 2: Landed * 3,7: Flying * 4: Hovering * 5: Test (?) * 6: Taking off * 8: Landing * 9: Looping (?)", ": Left/right tilt in degrees (rotation about the X axis)", ": Forward/backward tilt in degrees (rotation about the Y axis)", ": Orientation in degrees (rotation about the Z axis)", ", magY, magZ: Magnetometer readings (AR-Drone 2.0 Only) (TBA: Convention)", ": Pressure sensed by Drone's barometer (AR-Drone 2.0 Only) (Pa)", " : Temperature sensed by Drone's sensor (AR-Drone 2.0 Only) (TBA: Unit)", ": Estimated wind speed (AR-Drone 2.0 Only) (TBA: Unit)", ": Estimated wind angle (AR-Drone 2.0 Only) (TBA: Unit)", ": Estimated wind angle compensation (AR-Drone 2.0 Only) (TBA: Unit)", ": Estimated altitude (mm)", "..4: Motor PWM values", " Linear velocity (mm/s) [TBA: Convention]", " Linear acceleration (g) [TBA: Convention]", ": Timestamp of the data returned by the Drone returned as number of micro-seconds passed since Drone's boot-up.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/201666/what-topic-should-i-subscribe-to-know-when-the-drone-is-in-hover-state/"},
{"title": "point cloud transform", "time": "2015-04-27 18:57:48 -0600", "post_content": [" ", " ", "i transformed point cloud 2 from kinect in camera optical depth frame to base link frame. However, the point cloud published in the new topic appears to be a lot slower. Did i do sth wrong?"], "answer": [" ", " ", "It's impossible to know if you did something wrong without seeing some source code.", "But I would guess that one of two things are happening:"], "answer_details": ["Are you using a waitForTransform() call? If so, your transformations from base link to camera_depth_optical_frame could be taking too long to publish.", "Your machine might not have enough processing power to transform the point cloud quickly. I've noticed performance problems with any point clouds over around 50,000 points.", " ", " ", " ", " "], "url": "https://answers.ros.org/question/208064/point-cloud-transform/"},
{"title": "[SOLVED] Serial Port read returned short error with arduino uno via bluetootle with rosserial", "time": "2014-09-10 02:28:17 -0600", "post_content": [" ", " ", " ", " ", "Hi all!", "I'm working with arduino car under directly ROS topic command. I have a arduino uno board with Arduino Sensor Shield v5.0 installed. I'm running the basic publish and subscribe tutorial from rosserial: ", " ", "When using USB shown as dev/ttyACM0,  things are doing well.", "Then, I'm trying to connect with HC-05 bluetooth module. First I connect it with command:", "sudo rfcomm connect /dev/rfcomm0 00:06:71:00:3E:87 1", "And the ", "Then launching rosserial as before with additional argument :", "rosrun rosserial_python serial_node.py _port:=/dev/rfcomm0 _baud:=9600", "With the tutorial code on the car:", "The terminal become a waterfall of running warning:", "For most of the time its complaining about expected 72 bytes.", "And thetopic, ", "rostopic info chatter", "will return result (hello world!) quite randomly (it correctly shows with 1 Hz when using USB)", "I've done another experiment on subscribe function. Arduino Car subscribe to std_msgs/Empty and topic is published by", "rostopic pub toggle_led std_msgs/Empty --rate=1", "The result is similar: some of the command can arrived (by moving the sonar servo) but quite randomly, and sometimes move more then 1 time in 1 second (published in 1Hz).", "I've tried to read the source but still couldn't locate the problem. ", "Any help or suggestion are very welcome, thanks.", "edit:\nIt truns out it is the problem of baudrate of my bluetooth module! The chip (YFRobot) is a china made cheap one and not is a real HC-06 or any official supported chip. The common method of setting baudrate in a console just won't work. There is something like a single post in some unkown chinese forum that provides the datasheet (Luckily I can read simplified Chinese ^^). After a weird setup process, it's fine now, except that the module just won't work beyond  exceed certain rate (57600 I think).", "ahendrix hello.\nI've never set the baud rate other then nh.getHardware()->setBaud() in bluetooth module. But if the baud rate is wrong, shouldn't it be completely unable to communicate? Using Arduino IDE with it's serial monitor and function like Serial.begin/Serial.write have no problem at all.", "Most bluetooth modules have a UART buad rate that cannot be set through normal software. They usually have some sort of AT command set for modifying the baud rate.", "Since the arduino serial console and Serial.write work over bluetooth, that means that the baud rate setting you're using matches the bluetooth module's setting, and it means that the problem is elsewhere.", "Mmmmm ahendrix thank you. hope this can be fix. I'll keep trying.", " I suggest you re-post the solution as an answer and select it as the right-answer so that others can tell this question has an answer.", " OK, I see. I'm not sure whether if it is okay to do so here at ros answers. Some of the Q&A site forbid self-answering the question. Anyway, thanks.", "edit: I see the line \" you are encourage to answer you own....\" when I editing my answer ^_^"], "answer": [" ", " ", " ", " ", " It truns out it is the problem of baudrate of my bluetooth module! \nThe chip (named YFRobot HC-05) is a china made cheap one and not is a real HC-06 or any official supported chip. The common procedure of setting baudrate in a console just won't work. Finally There is something like a single post in some unkown chinese forum that provides the datasheet (Luckily I can read simplified Chinese ^^).  ", "You have to:", "But baud rate higher then 57600 is not usable on my module. Hope it helps someone.", " ", " ", "Are you sure the baud rate in the bluetooth module's firmware is set correctly? Does the serial over bluetooth work if you load a non-ROS sketch on your arduino?", " ", " ", "I changed the timeout from 5 to 10 in Serialclient.py. This seems to work for me, though not 100 percent. "], "answer_details": ["connect PIN SET to 5V", "power up", "set up some kind of terminal that maps input 'Enter' to '\\r\\n' (carriage return and new line?). I think this is different from some of the board.", "use AT command to change the settings. ", "\n AT+BAUD1---1200 ", "\n AT+BAUD2---2400 ", "\n AT+BAUD3---4800 ", "\n AT+BAUD4---9600(\u6a21\u584a\u51fa\u5ee0\u8a2d\u7f6e\u662f9600\u983b\u7387) ", "\n AT+BAUD5--19200 ", "\n AT+BAUD6--38400 ", "\n AT+BAUD7--57600 ", "\n AT+BAUD8--115200 ", "\n AT+BAUD9--230400 ", "\n AT+BAUDA--460800 ", "\n AT+BAUDB--921600 ", "\n AT+BAUDC-1382400  ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "question_code": ["#include <ros.h>\n#include <std_msgs/String.h>\n\nros::NodeHandle  nh;\n\nstd_msgs::String str_msg;\nros::Publisher chatter(\"chatter\", &str_msg);\n\nchar hello[13] = \"hello world!\";\n\nvoid setup()\n{\n  nh.getHardware()->setBaud(9600);\n  nh.initNode();\n  nh.advertise(chatter);\n}\n\nvoid loop()\n{\n  str_msg.data = hello;\n  chatter.publish( &str_msg );\n  nh.spinOnce();\n  delay(1000);\n}\n", "[INFO] [WallTime: 1410329846.797489] ROS Serial Python Node\n[INFO] [WallTime: 1410329846.814548] Connecting to /dev/rfcomm0 at 9600 baud\n[WARN] [WallTime: 1410329849.792440] Serial Port read returned short (expected 72 bytes, received 8 instead).   \n[WARN] [WallTime: 1410329849.793548] Serial Port read failure: \n[INFO] [WallTime: 1410329849.794408] Packet Failed :  Failed to read msg data\n[INFO] [WallTime: 1410329849.795036] msg len is 8\n[WARN] [WallTime: 1410329850.814268] Serial Port read returned short (expected 16 bytes, received 13 instead).\n[WARN] [WallTime: 1410329850.815325] Serial Port read failure: \n[INFO] [WallTime: 1410329850.816327] Packet Failed :  Failed to read msg data\n[INFO] [WallTime: 1410329850.816984] msg len is 8\n"], "url": "https://answers.ros.org/question/192356/solved-serial-port-read-returned-short-error-with-arduino-uno-via-bluetootle-with-rosserial/"},
{"title": "Turtlebot 2 kinect type(xbox360 version or PC version?)", "time": "2016-05-22 23:05:25 -0600", "post_content": [" ", " ", "Hi, I'm tring to build a turtlebot 2 by myself instead of buying one on Clearpath.\nWhen considering buying a kinect, there're 2 types available, one for xbox360, and another PC version.\nWhich one should I take? Or do they both work fine."], "answer": [" ", " ", " ", " ", "I bought once an old Kinect for XBOX 360 for less than 80$ and it worked fine.", "In the Microsoft website, they said to prefer Kinect for PC because it contains improvement on the SDK and the API... but, naturally, this the Windows SDK, not the Linux one, which means that it's the same for ROS users to use Kinect for XBOX or Kinect for PC (unless you are using ROS for Windows which is not a stable version)", "My advice, if you want to buy a new Kinect, is to prefer the Kinect v2 for XBOX ONE, which is better than Kinect v1 for XBOX 360 :", "And of course, this version is ", "The actual ", " including the USB adapter is 150$.", "If you prefer to although to buy Kinect for XBOX 360, make sure it includes the USB adapter. ", "EDIT:", "Thanks a lot for your answer and advice. It's really helpful. I've been always wondering wether Kinect v2 will work with ROS or not, cause there really wasn't much mention about this in the Turtlebot tutorial.", ": here are several information just wrong! Kinect v1 supports 1280x960@<15fps, too. Kinect v1 has a Title motor, Kinect v2 does not! The effective resolution of Kinect v1 is about 220x170. Using Kinect v2 on a robot is a bad idea: extreme noise, reflections and more power consumption.", "Ah thank you ", " for these informations. To be honest, I never tried v2 so I didn't know for the noise and reflections. Theorically it seems good, but in practice, as you said, we need to deal with extra power consumption, and extra computations.", ": I had high expectations, too. But I'm very disappointed by Kinect v2. Kinect v1 has a low spatial resolution but due to structured light you get at each position good data or no data, but never wrong data.", "Further disadvantage of Kinect v2: The distances depend on the surface color, too. Dark surfaces have a larger distance.", "Ah i see ", ", thanks a lot, I was planning to buy one! In their website, Microsoft said that it can track more articulations (I was planning to use it for detecting gestures). But if the distances are not so accurate, it will lead to false mapping I suppose?", ": It depends very strong on material and angles. For detecting gestures the kinect v2 should be fine. Alternatively you can take a look at the new Intel SR300, which was build for detecting gestures in short range (<120cm).", "Ah yes i've seen the Intel SR300, but still the short range is a problem in my case and I think Kinect is more suitable for a mobile robot since it can detect the full body joints from a range of 3 to 4m"], "answer_details": ["HD resolution: 1960x1028 30fps (compared to 640x480 for v1)", "Larger range depth sensor: 512x424 (compared to 320x240)", "Larger field of view: 70x60 degrees (compared to 57x43)", "USB 3.0 support", "It seems that v2 doesn't include a tilt motor anymore!", "Don't forget that v2 has more power consumption than v1", " ", " ", " ", " "], "url": "https://answers.ros.org/question/234963/turtlebot-2-kinect-typexbox360-version-or-pc-version/"},
{"title": "Robot distance/angle positioning with sensors", "time": "2016-07-15 23:49:35 -0600", "post_content": [" ", " ", "Hello guys,", "In my project - which I successfully simulated with Gazebo ROS my robots needs\nto position itself exactly in perpendicular position to a metallic surface - specifically 3 meters from the\n side-door of a car so that the front of the robot points the same direction of the car.", "I successfully solved the issue in Gazebo ROS using a lidar sensor.", "The algorithm works the following way.\nI calculated the distance/angle from the target surface using 3 different scan ranges from the lidar output.\nUsing a few trigonometric functions I was able to calculate the angle correction that had to be applied\nto the robot base_link to be perfectly perpendicular to the target.", "So everything is perfect in simulation!", "The pain now comes in the real world...", "I tested my align-base function in real world using a RPLidar (robot peak lidar).\nI noticed that the lidar scan output on a black metal surface is very unreliable.\nConsider that black and metal is very common on a car body (I didn't test on other colors yet)", "Can't wait to hear your opinions!", "Thanks!"], "answer": [" ", " ", "Hello! I want to create a specific car in to implement in ROS and i can't find a something like tutorial or examples of creating cars from scratch (these cars should look well, like your car) and simulate them in rviz. Do you have some ideas which programs or packages I should use? \nI know that is my answer is not common with topic but I can't send you a private message :) \nThanks a lot ", " ", " ", "The RoboPeak and other low-price lidar sensors tend to have trouble with dark surfaces that are at the far end of their sensing range (>50% of rated range); the Hokuyo URG-04LX and the Neato Lidar have similar issues in similar situations.", "These sensors tend to do better a close range.", "I can think of a few options:", "Hello,\nthank you very much for your suggestion. Anyway what I noticed is that the black metal surface is almost invisibile even if placed at 1.5mt of distance (...i know it seems really strange, but it doesn't happen with a white wall for example!). I will send you rviz screen capture!", "Most of the trouble I've seen with these types of sensors has been with matte black surfaces indoors, which mostly absorb the laser beam instead of scattering it and sending some back to the sensor. Reflective black surfaces may be worse because they reflect the laser away from the sensor."], "answer_details": [" ", " ", " ", " ", "I'm not sure what your algorithm is doing, but you may be able take more samples and average out the noise", "Take advantage of better sensor performance at close range by starting your robot closer to the vehicle, doing the alignment, and then drive directly away from the vehicle", "Get a more powerful lidar like the Hokuyo UTM-30LX (Maybe ", "?)", " ", " ", " ", " "], "url": "https://answers.ros.org/question/239640/robot-distanceangle-positioning-with-sensors/"},
{"title": "Compatibilities Ubuntu-ROS-ROSI", "time": "2016-08-26 09:28:55 -0600", "post_content": [" ", " ", "Is there any place where to find the compatibilities between the different UBUNTU-ROS-ROSI releases?\nbecause I cant find it in all the different ros-wiki's"], "answer": [" ", " ", "The only packages in ROS-Industrial released into Jade at this moment are those in ", ". Those packages can be installed using ", " (at least on Ubuntu).", "As to the rest of ROS-Industrial: that has not been released, but as the differences between Indigo and Jade are relatively minor, most -- if not all -- packages should be usable under Jade. The only drawback is that you'd need to build them from source.", "Is there any place where to find the compatibilities between the different UBUNTU-ROS-ROSI releases?", "If with \"compatibilities\" you mean: into which ROS releases ROS-Industrial packeges have been released, then I'm afraid there is (currently) not a single location that displays that information.", "You can however find out in multiple places if a package is released for a specific platform and ROS release:", "because I cant find it in all the different ros-wiki's", "(slightly pedantic) but there is only a single \"ROS wiki\", the one at ", "."], "answer_details": ["look at the green 'badges' at the top of the wiki pages for the package: switch to the ROS release that you are interested in (using the version rocker at the top of the page), and look for a ", " badge. If you can't find that, or the package doesn't have a button in the rocker for the ROS release you are interested in, then it's safe to assume it hasn't been released.", "use any of the available ", " pages, such as ", " for Jade. Those pages can be filtered: ", " is a query for ", " in Jade. See ", " for all status pages. If a package is not listed on that page, it's safe to assume it hasn't been released.", " ", " ", " ", " "], "answer_code": ["apt-get", "\u2714 Released", "industrial", "wiki.ros.org"], "url": "https://answers.ros.org/question/242504/compatibilities-ubuntu-ros-rosi/"},
{"title": "Problem when setting 2D Nav Goal [closed]", "time": "2017-04-26 12:40:09 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "\nI'm running ", " in Ubuntu 16.04.2 LTS on a Raspberry Pi 3.", "\nMy navigation stack is up and running and my robot is able to localize itself in a given map. I'm also able to drive around via a joystick (", "-topic).", "\nThe next step is to ", ". I am able to set 2D Pose Estimates there, but whenever I'm trying to set a 2D Nav Goal, nothing is happening, except from the arrow and the path being visually displayed in rviz.", "\nUnfortunately, when echoing the ", "-topic, which is being published by ", ", it always displays the following:", "As well I get the following error thrown until I kill ", ":", "Since I don't get that error, before setting a 2D Nav Goal, I assume that there is either something wrong in one of my costmaps or move_base settings...", "I appreciate all your help. Thanks in advance.", "\nNico", "Please add the rosgraph. According to the /cmd_vel output your robot must drive clockwise, but you said your robots listens to /input_joy/cmd_vel, so its either /cmd_vel or /input_joy/cmd_vel.", "I just added the rqt_graph... Thanks in advance", "In global_costmap_params.yaml the static_map should be true.", "Ok, I changed that, but still my problem stays the same...", "In addition to the static_map for the global_costmap_params.yaml, also make sure that your robot IS holonomic and if it's not, set that to false in base_local_planner_params.yaml. I would guess that your robot is NOT holonomic, since holonomic robots are not that widespread with RPi implementations.", "Also, try adding this ", " in base_local_planner_params.yaml.", "Thanks a lot, guys, the problem is solved... I got some bad values set in my ", "-file, which didn't fit that robot.", "\nThanks again."], "answer": [" ", " ", "To sum up what was said in the comments for future readers, nico_b had to:", "You can now close your question. Happy coding! :)"], "answer_details": ["Change holonomic_robot to false in base_local_planner_params.yaml.", "Lower the control frequency to 3Hz (due to RPi3's low computing power) in base_local_planner_params.yaml.", "Change static_map to true in global_costmap_params.yaml.", " ", " ", " ", " "], "question_code": ["ROS Kinetic", "/input_joy/cmd_vel", "/cmd_vel", "move_base", "linear: \n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: -1.0\n", "move_base", "Control loop missed its desired rate of 5.0000Hz... the loop actually took 0.4580 seconds\n", "TrajectoryPlannerROS:\n  max_vel_x: 0.19\n  min_vel_x: 0.095\n  max_vel_theta: 0.256\n  min_in_place_vel_theta: 0.192\n\n  acc_lim_theta: 3.2\n  acc_lim_x: 2.5\n  acc_lim_y: 2.5\n\n  holonomic_robot: true\n  meter_scoring: true\n", "plugins:\n  - {name: static_map,      type: \"costmap_2d::StaticLayer\"}\n  - {name: obstacles,       type: \"costmap_2d::VoxelLayer\"}\n  - {name: inflation_layer, type: \"costmap_2d::InflationLayer\"}\n\nobstacle_range: 2.5\nraytrace_range: 3.0\nfootprint: [[0.05, 0.05], [0.05, -0.05],[0, -0.10]]\n#robot_radius: 0.20\ninflation_radius: 0.55\n\nobstacles:\n  observation_sources: laser_scan_sensor\n  laser_scan_sensor: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true}\n", "global_costmap:\n  global_frame: /map\n  robot_base_frame: base\n  update_frequency: 0.8\n  static_map: false\n  width: 5\n  height: 2\n", "local_costmap:\n  global_frame: odom\n  robot_base_frame: base\n  update_frequency: 1.0\n  publish_frequency: 2.0\n  static_map: false\n  rolling_window: true\n  width: 3.0\n  height: 3.0\n  resolution: 0.05\ncontroller_frequency: 5.0\n", "<launch>\n  <master auto=\"start\"/>\n\n  <node pkg=\"move_base\" type=\"move_base\" name=\"move_base\" output=\"screen\">\n    <rosparam file=\"/home/ubuntu/catkin_ws/src/project/launch/config/costmap_common_params.yaml\" command=\"load\" ns=\"global_costmap\" />\n    <rosparam file=\"/home/ubuntu/catkin_ws/src/project/launch/config/costmap_common_params.yaml\" command=\"load\" ns=\"local_costmap\" />\n    <rosparam file=\"/home/ubuntu/catkin_ws/src/project/launch/config/local_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"/home/ubuntu/catkin_ws/src/project/launch/config/global_costmap_params.yaml\" command=\"load\" />\n    <rosparam file=\"/home/ubuntu/catkin_ws/src/project/launch/config/base_local_planner_params.yaml\" command=\"load\" />\n  </node>\n</launch>\n", "controller_frequency: 3.0", "base_local_planner_params.yaml"], "url": "https://answers.ros.org/question/260335/problem-when-setting-2d-nav-goal-closed/"},
{"title": "are there standard battery level messages in ROS", "time": "2014-09-18 05:01:16 -0600", "post_content": [" ", " ", "for developping low level interfaces for a new robot, are there existing standard messages for battery level (in %) or voltage, or thresholds (low) / flags (charging / dis-charging) ?  "], "answer": [" ", " ", "We ", " a ", " message in ", " ", " ", " ", " For the turtlebot dashboard the laptop battery status is send as a message and also via diagnostics.\nThe  ", "  package contains a LaptopCharge message. ", " ", " ", "The ", " and the ", " both have battery state messages.", "However non of them is some sort of standard.", " ", " ", "From indigo onwards, there is a standard battery message type in the ", " umbrella.", " ", " ", "You can also use ", "."], "answer_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " : message types", " : scripts for parsing and outputting battery state on diagnostics. ", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/192938/are-there-standard-battery-level-messages-in-ros/"},
{"title": "Differential Drive PID Output", "time": "2018-04-23 15:20:30 -0600", "post_content": [" ", " ", "I am using the ", " package for a PID controller. I am confused at the motor_cmd topic. It states that it is the output of the PID, the power going to the motors, in arbitrary units. I am attempting to use the output as a velocity command but this is not working for me. How do I utilize the PID output of the differntial_drive package? ", "What exactly is it that you're trying to do with the output? This package only works if it's connected to a motor with a rotary encoder. This is a closed loop controller, without the hardware to close the loop it will not work.", " Please don't use images to post text. Text in images is not copy and pasteable and cannot be searched. This is what the quote (", ") button is for.", "It would help if you explained what \"not working for me\" means. Do you get errors? What is the behavior of the motor? Does ", " show anything for the ", " topic?", "I was attempting to use motor_cmd as a velocity topic then converted to PWM for my motors - this was not working. I believe I fixed my issue using wheel_vtarget+motor_cmd as my velocity command that I convert to PWM."], "answer": [" ", " ", "You cannot use the output of the PID controller as a velocity command. As the documentation states, it publishes two topics.", "If you do not have encoders on your wheels, axles or motors, then you cannot use this package unless you approximate wheel encoder data some other way (which sounds iffy and not worth the effort, to me).", "Yes I have encoders and am calculating wheel velocity correctly (with this package). I was attempting to use the motor_cmd as a velocity and obviously that is not working. I think I have a solution now where I take my wheel_vtarget and add motor_cmd as a sort of offset!"], "answer_details": [", the power going to the motor in arbitrary units, which you need to use in your motor driver code to control the motor. You need to set the ", " and ", " parameters to define the allowable range for your motor driver code's input. Essentially, this topic outputs a value that is a proportion of the range between ", " and ", ", which can be considered the percentage of maximum power to apply to the motor. It does not output a velocity, so it cannot be used as a velocity command.", ", the current velocity of the wheel in meters/second, calculated from the input encoder data and the parameters that control the relationship between ticks and distance traveled. As well as being published, this is used internally to drive the PID controller. You need to set the global ", " parameter based on your encoder's data sheet. This parameter specifies how many ticks there are from the encoder when the wheel has travelled one meter. You may also want to set the node-local ", " parameter. This is the number of values to use when calculating the rolling average that becomes the ", " topic. You should also set the ", " and ", " parameters to ensure correct behavior when the encoder output wraps.", " ", " ", " ", " "], "question_code": ["\"", "rostopic echo", "motor_cmd"], "answer_code": ["motor_cmd", "out_min", "out_max", "out_min", "out_max", "wheel_vel", "ticks_meter", "rolling_pts", "wheel_vel", "encoder_min", "encoder_max"], "url": "https://answers.ros.org/question/289457/differential-drive-pid-output/"},
{"title": "If i currently have kinetic instead of indigo, and i have to install a package that was design for indigo, (ros-indigo-image-pipeline) how do i check that i can replace indigo with kinetic or if it is even available for kinetic?", "time": "2018-05-23 15:11:57 -0600", "post_content": [" ", " ", " I am currently working with Jackal and all their codes and packages imply to use indigo but i currently have kinetic. Here is a link to the website i am using as reference. \n "], "answer": [" ", " ", " ", " ", "First a quick answer: I've used Kinetic with a Jackal running Indigo many times and have run into very few issues. ", " is certainly released on Kinetic and can be installed using ", ". You likely already have this package installed.", "A good question is how to check what packages are released for a particular distro (allowing installation from ", ")? You could:", "If a particular package you need to use isn't released on your target ROS distro, you can always try building from source by obtaining a copy of the source (typically a ", " will do this), and putting it into catkin workspace and trying to build. The multitude of issues that can arise when building from source is a whole other can of worms.", "Thank you, this was very helpful. I am just getting started with ROS and Jackal seems like a good project to start and get a hang of things. Hopefully the few issues you ran into with Kinetic and Indigo are not that bad and I can figure it out.", "IIRC, the only issue I've run into is with the message definition of ", " changing between Indigo and Kinetic (as described ", " ). Note that most Jackal packages have been released on Kinetic, so it could...", "... be possible to update Jackal to 16.04+Kinetic. I haven't tried this, but if you do run into issues it might be worth reaching out to Clearpath."], "answer_details": ["Check using ", " on your machine (e.g. ", ")", "Check the wiki. Note every package page has status indicators at the top highlighting if a package has been released or not. Notice the green \"Released button\" on the ", " page when you select Kinetic? ", " There are also several tools online for checking build status. These are likely less useful if you are not a package maintainer, but if you are trying to track down an issue it's good to know these exist.  ", "  and  ", "  could both be useful. ", " ", " ", " ", " "], "answer_code": ["image_pipeline", "sudo apt-get install ros-kinetic-image-pipeline", "apt-get", "apt-cache", "apt-cache search image-pipeline", "git clone", "visualization_msgs/Marker"], "url": "https://answers.ros.org/question/292064/if-i-currently-have-kinetic-instead-of-indigo-and-i-have-to-install-a-package-that-was-design-for-indigo-ros-indigo-image-pipeline-how-do-i-check-that/"},
{"title": "OpenNI2 reconnection loop to the sensor", "time": "2018-07-02 07:11:49 -0600", "post_content": [" ", " ", " ", " ", "When I start OpenNI2 on ROS Kinetic with the command ", " it start a continuous loop of reconnection to the sensor without any error. What am I doing wrong?", "\nA part of message showed is:", " : 0.3.0", "OS: Ubuntu mate 16.04", "\nSensor: Asus Xtion Pro Live", "\nPlatform: Raspberry Pi 3", " 1) In general please go through support guideline  ", " . 2) Provide more info (also noted in the guideline). E.g. Provide system output instead of verbally describing \"start a continuous loop of reconnection to the sensor without any error\" / output  ", "3) You seemed to have posted ", ". Also as noted in the support guideline, please refrain from multi-posting about a single issue/question.", "I have correct all the issues and upgraded the question."], "answer": [" ", " ", " ", " ", "Thanks for answer.", "\nI realized that the Asus Xtion Pro Live I have is the old version.", "\nWith the command ", " I have seen ", ".", "\n600: old version", "\n610: new version", "\nSo I tried to install ", " instead of ", " and it seems that everything is working perfectly.", "Good catch! I marked this as an answer. Also filed a ticket ", ".", " ", " ", " ", " ", "Not an answer though, comment section is too small for this.", "Thanks for posting more info. Looks like ", " ROS driver keeps losing the device connection even though it's able to re-connect immediately[1]. I noticed you're on RasPi3. I've never tried using ", " with RasPi3 so can't speak for it much, but looks like there are people who's had success (e.g. ", "), so keep on troubleshooting might be worth it. It's just the fact you're on micro device makes me lean more toward on thinking that the root cause isn't in ROS driver but somewhere else.", "Some of the things I would do and fix if I see any issue:", "HTH.", "[1] Just as FYI, device re-connection in ", " ROS driver level was introduced from version 0.2.9 (in ", ") and personally this has been working stable on our system that runs 24/7."], "answer_details": [" ", " ", " ", " ", "Make sure the device's OS level connection is stable. Search into ", " for any suspicious output.", "Connect Xtion to other computer (preferably Ubuntu to make comparison simple) and check if that works.", "Use a USB cable that can provide power externally (e.g. ", " (I'm not affiliated to the seller etc.)). Less power provision may cause instant power off of the device.", " ", " ", " ", " "], "question_code": ["roslaunch openni2_launch openni2.launch", "...\n[ WARN] [1530533003.851796857]: Detected loss of connection. Stopping all streams and resetting device\n[ INFO] [1530533007.235240341]: Device \"1d27/0600@1/100\" found.\n[ INFO] [1530533007.851254498]: Detected re-connect...attempting reinit\n[ INFO] [1530533008.908591795]: Re-applying configuration to camera on re-init\n[ INFO] [1530533008.929105097]: Starting color stream to adjust camera\n[ INFO] [1530533008.929701752]: Starting color stream\n[ INFO] [1530533008.994149364]: Restarting publishers, if needed\n[ INFO] [1530533008.994949244]: Done re-initializing cameras\n[ WARN] [1530533009.924432796]: Device \"1d27/0600@1/100\" disconnected\n\n[ WARN] [1530533009.924909818]: Device \"1d27/0600@1/100\" disconnected\n\n[ WARN] [1530533010.851467225]: Detected loss of connection. Stopping all streams and resetting device\n[ INFO] [1530533013.346860905]: Device \"1d27/0600@1/104\" found.\n[ INFO] [1530533013.851728189]: Detected re-connect...attempting reinit\n[ INFO] [1530533015.018626401]: Re-applying configuration to camera on re-init\n[ INFO] [1530533015.041487448]: Starting color stream to adjust camera\n[ INFO] [1530533015.042497378]: Starting color stream.\n[ INFO] [1530533015.107487509]: Restarting publishers, if needed\n[ INFO] [1530533015.108841390]: Done re-initializing cameras\n[ WARN] [1530533016.306939896]: Device \"1d27/0600@1/104\" disconnected\n\n[ WARN] [1530533016.307475772]: Device \"1d27/0600@1/104\" disconnected\n\n[ WARN] [1530533016.851243668]: Detected loss of connection. Stopping all streams and resetting device\n[ INFO] [1530533019.735480141]: Device \"1d27/0600@1/108\" found.\n[ INFO] [1530533019.851516723]: Detected re-connect...attempting reinit\n[ INFO] [1530533021.408008103]: Re-applying configuration to camera on re-init\n[ INFO] [1530533021.429622839]: Starting color stream to adjust camera\n[ INFO] [1530533021.430399701]: Starting color stream.\n[ INFO] [1530533021.493632167]: Restarting publishers, if needed\n[ INFO] [1530533021.494297937]: Done re-initializing cameras\n[ WARN] [1530533022.508973575]: Device \"1d27/0600@1/108\" disconnected\n\n[ WARN] [1530533022.509527680]: Device \"1d27/0600@1/108\" disconnected\n...\n", "rosversion openni2_launch", "rosversion openni2_launch"], "answer_code": ["lsusb", "Bus 001 Device 006: ID 1d27:0600 ASUS", "openni_launch", "openni2_launch", "openni2_camera", "Xtion", "/var/log/syslog", "openni2_camera"], "url": "https://answers.ros.org/question/295938/openni2-reconnection-loop-to-the-sensor/"},
{"title": "ROS Debugging VSCode Python", "time": "2018-08-20 08:31:43 -0600", "post_content": [" ", " ", " ", " ", "Hi Everyone,", "I have read/tried various solutions in an attempt to debug ROS in VS Code without any luck. Can I please get some help in solving this mystery?", "My workspace runs fine from when I run from an Ubuntu terminal prompt and from an IDE terminal prompt.\nsource devel/setup.bash\nrosrun location tagsub.py", "My env:\nUbuntu 16.04\nVS Code IDE\nROS extension installed", "My ROS package:\nName: location\nSimple pub/sub with a custom message", "From an Ubuntu terminal prompt I go to my workspace root, and start code.\nIn VS Code terminal (which is on my workspace root), run catkin build then roscore. In the lower left corner of IDE, ROS Master now has a check mark.", "I then created a launch configuration by:", "Here is my launch entry:", "Click on my source file tagsub.py file tag in the editer window", "The debug process starts and get this error:\nException has occurred: str", "In IDE terminal pip list | grep ros returns:", "Also from an IDE terminal prompt, ", "you need to setup your environment for debugging and source that path, then run your package and link it from VSCode", ", can you please be more specific on how to do this? I have also tried running source devel/setup.bash from a launch pretask and it still is not working.  Thank you."], "answer": [" ", " ", " ", " ", "There might be other ways but this is one I know:", "It seems lengthy but most of it is just one time configuration thing, once that is done you can change node name and path in configuration to work with any other python node. Hope this helps.", "Thank you for your response/help.", "So step 4 runs the node and step 6 attaches to it? In the launch.json I have program set to ${workspaceFolder}/src/location/scripts/locationByName_service.py", "It starts ok, then errors out on import rospy with module not found error.", "Welcome, 4 and 6 work exactly as you mentioned. And I don't know where this error is coming from, don't remember encountering this. Maybe you can ask a new question about it in more details and close this one."], "answer_details": ["Go into your workspace and show hidden folders (ctrl+h)", "There is folder called .catkin_tools, and a sub folder called profiles. Go into profiles and make a copy of your default profile. Call it debug (or anything you like). Go into debug and change build.yaml and config.yaml to not to have build and devel but to have debug_devel and build_devel (again you can name them anything).", ".", "Then build your package in debug mode add a flag when building ", ".", "When it's built, source devel_debug/setup.bash and run your node.", "Then go into VSCode and debug mode (ctrl+shift+D), you will see green play button on top right press that, it will probably prompt to launch a json, open it and configure it for your python node ", ".", "Once configured you can chose that configuration and press play button it will pop-up a list of available process to link against, select your node from it, it will ask for password and then you can debug.", " ", " ", " ", " "], "question_details": [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", "clicking on the debug icon on the left most part of the IDE", "clicking on the gear icon on the upper left section of the IDE, this opens a new launch.json file", "click on the the button Add Configuration...(in the lower right of the window). This pulls up a window of choices", "choose {} ROS : rosrun", " ", "Press F5"], "question_code": ["{\n  \"request\": \"launch\",\n  \"type\": \"ros\",\n  \"command\": \"rosrun\",\n  \"name\": \"rosrun\",\n  \"package\": \"location\",\n  \"target\": \"tagsub.py\",\n  \"debugSettings\": \"debugSettings\"\n}\n", "'No module named rospy'\n  File \"/home/zzfl0b/location/src/location/scripts/tagsub.py\", line 3, in <module>\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n", "gazebo-ros                    2.5.17\nrosbag                        1.12.13\nrosboost-cfg                  1.14.3\nrosclean                      1.14.3\nroscreate                     1.14.3\nrosgraph                      1.12.13\nroslaunch                     1.12.13\nroslib                        1.14.3\nroslint                       0.11.0\nroslz4                        1.12.13\nrosmake                       1.14.3\nrosmaster                     1.12.13\nrosmsg                        1.12.13\nrosnode                       1.12.13\nrosparam                      1.12.13\nrospy                         1.12.13\nrosservice                    1.12.13\nrostest                       1.12.13\nrostopic                      1.12.13\nrosunit                       1.14.3\nroswtf                        1.12.13\nsmach-ros                     2.0.1\ntf2-ros                       0.5.17\n", "$ env | grep ros:\nROS_ROOT=/opt/ros/kinetic/share/ros\nROS_PACKAGE_PATH=/home/jeff/location/src/roscpp:/home/jeff/location/src/rospy:/home/jeff/location/src/std_msgs:/home/jeff/location/src/location:/opt/ros/kinetic/share\nLD_LIBRARY_PATH=/home/jeff/location/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/local/cuda/lib64\nPATH=/opt/ros/kinetic/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/home/jeff/bin:/home/jeff/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nPYTHONPATH=/home/jeff/location/devel/lib/python2.7/dist-packages:/opt/ros/kinetic/lib/python2.7/dist-packages\nPKG_CONFIG_PATH=/home/jeff/location/devel/lib/pkgconfig:/opt/ros ..."], "answer_code": ["build_space: build_debug & devel_space: devel_debug", "CMAKE_BUILD_TYPE=debug"], "url": "https://answers.ros.org/question/301015/ros-debugging-vscode-python/"},
{"title": "How to record robot odometry in multiple bagfiles after resetting the robot's computer without resetting its odometry and timestamps?", "time": "2018-08-29 13:16:09 -0600", "post_content": [" ", " ", "I'm running the nodes rosaria, rosaria_client , hokuyo node and avt_vimba camera to record some datasets of routes around campus roads with rosbag in a Pioneer 3AT robot. The robot runs Ubuntu 14.04 with kernel 4.4.0 and ROS Indigo.", "I have managed to record 3 out of 5 datasets without much trouble. However, the last 2 are giving me trouble as the robot's power runs out before finishing them due to the routes being too long. Initially, I thought the batteries were defective, and while that was the case, after changing the batteries, the robot's power still runs out before it can finish any of the routes left.", "The solution my teacher and I came with was to record the datasets in multiple files. However, that would require to save a bagfile until the robot's power is about to run out, shut down the robot, change its batteries for some spares (we have them) and restart the recording from the spot the robot stopped, and that will invariably reset the odometry and timestamps of the recorded messages.\nIs there a way to resume that recording while keeping the last recorded odometry and timestamps? If so, how? If not, then is there a workaround?"], "answer": [" ", " ", " ", " ", "Hello, been a while since I checked ROS Answers, but here are some additional ideas: ", "Hi ReedHedges, it may be a little late but the first point in your answer worked but it also was related with your 3rd point. I got around offsetting data by saving the very last data before a shutdown and using it to offset the new data for the secong bagfile. I didn't know about the second point."], "answer_details": ["It may be possible to \"hot-swap\" the batteries without shutting it down, if two (discharged) batteries are able to continue powering all the equipment on the robot. Just replace them one at a time. If using multiple sets of batteries, it's generally a good idea to keep them in \"matched\" sets.  If you're buying new batteries it is now possible to get slighly higher capacity SLA batteries in that form factor; we switched from 7Ah to 9Ah batteries starting a few years ago. There might be higher capacity ones also available now.   ", "The skid-steer P3AT with its very wide high friction tires is not very efficient when turning.  You can replace the wheels with other wheels (but you need to change firmware calibration and ARIA parameters), or just cover the wheels with some tape with less friction.  Or drive the robot on sand or dirt (which is it's original design intention.) I used to keep a bucket of sand and sometimes put it on the pavement in the area I was using a P3AT.", "I don't know about offsetting timestamps.  Maybe there is a tool somewhere that can process the contents of the later bagfiles and do this?  There is a function in the ARIA library for applying an offset to the odometric pose I don't recall exactly how rosaria will handle that, and there isn't a rosaria interface for it, but you could add it.  The ARIA method is ArRobot::moveTo()).  If there is a standard or typical ROS command for this kind of thing, it should be used. Connect with the rosaria project on github to discuss.   There is a robot firmware command (SETO) to reset odometry to 0,0,0 but there isn't one to initialize it to any arbitrary pose.  (The MobileSim simulator has one, but not the P3AT robot firmware.)", " ", " ", " ", " "], "url": "https://answers.ros.org/question/301994/how-to-record-robot-odometry-in-multiple-bagfiles-after-resetting-the-robots-computer-without-resetting-its-odometry-and-timestamps/"},
{"title": "Set RViz camera/view on fuerte", "time": "2013-02-25 20:48:22 -0600", "post_content": [" ", " ", " ", " ", "Hi,", "I'm writing a RViz plugin which allows the user to play rosbags and record them.\nDuring playing the rosbags I would like to make it possible to do a tracking shot in the scene. Therefore I have to set the current view in RViz.", "Is there a possibility to do this programmatically?", "There are some articles which say it is possible with a RViz plugin, but it's never explained anywhere.\nIf it is not possible in a single plugin, is it possible via librviz?", "Thanks in advance, best regards, Josch.", "EDIT:\nThe node should run on fuerte and Ubuntu 12.04 LTS.", "EDIT 2:\nNew question: How do I get an instance from the VisualizationManager? Creating my own one requires a RenderPanel and a WindowManagerInterface-implementing class.", "Hi, please mention your ROS version, and your ubuntu distribution. If you are in Groovy, I guess what you want already exist partly in the package rqt. Bests, Steph", "Hi, thanks for your answer. I added the versions to my post.\nWhat features from rqt do you think could help me?"], "answer": [" ", " ", " ", " ", "There are multiple options:", "You will find more information in the tutorials and API docs via the ROS Wiki: ", "Thank you very much!\nWhen using the custom ViewController, how can I tell RViz to use that ViewController?\nAnd when using the C++ API, how can I get the current VisualizationManager? Are these static methods?", "Using the target frame my orientation won't bind to the target frame's orientation. :-/", "If your plugin is a Display, you can access the Visualization Manager via it's member ", ". \nApart from that, the API docs on ", " are an excellent reference and will hopefully answer your questions.", "My plugin is a rviz::Panel plugin, but i just figured out it still has the vis_manager_ variable. Unfortunately it is not set during the hole runtime. Can I set the VisualizationManager and RViz will use it? Or can I get the original instance from somewhere?", "Does somebody know if getting the instance from the VisualizationManager used by RViz is possible from an rviz::Panel plugin?\nOtherwise I will have to create a second plugin and do some service communication.", "Panel::initialize populates vis_manager_ and then calls your onInitialize method: ", "Thank you again. So I have to call initialize, since it is obviously not called. But I need to pass a VisualizationManager to the initialize function. Therefore I have to create a vis_manager inststance, which requires a RenderPanel and a WindowManagerInterface-implementing class, am I correct?", "I'm saying that because vis_manager_ accessing vis_manager_ in any way ended up in crashing RViz. So it must either be empty or not accessable from my classes.", "You need to overwrite virtual void onInitialize() in you panel. Before that is called, vis_manager_ will not be initialized.", " ", " ", "I'm not aware of any functionality built into RViz directly to do this. Our lab has been working on-and-off on adding programmatic camera support and it (as far as we can tell) involves building a new view controller class and recompiling RViz. We hoped there would be a way to do it using plugins, but the only way to directly access the necessary Ogre camera object is using a special view controller.", "Well I was afraid that it's not possible to access the view controller from RViz out of the box.\nDid you already try setting the camera view by publishing a transform to the target_frame topic? Or change the /world orientation to have the desired view?", "That's the easy way to do it, but it doesn't work if the user manipulates the UI, since the user can still change the camera view by hand and you can't get the new view back to change the transform. If you access the underlying Ogre camera, you can get information on the current camera viewpoint.", "I really want to try that, but how can I access the ViewController and Ogre camera from the render panel?", " ", " ", "For those who need a quick solution or lack a little experience, I've build ", " you can use as a starting point. ", "\nIt allows to generate camera trajectories within rviz in a convenient way. ", "\nBe aware, that it was implemented for ros kinetic. "], "answer_details": ["Add a specialized view controller to your plugin (the most powerful option)", "Use the animated view controller plugin to control the camera via ROS messages (not released yet): ", "Set the \"target frame\" to a special tf frame you create for that purpose", "Use the C++ API (see VisualizationManager::getCurrentViewController, ViewController::setTargetFrame and ViewController:lookAt)", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "], "answer_code": ["vis_mananger_"], "url": "https://answers.ros.org/question/56287/set-rviz-cameraview-on-fuerte/"},
{"title": "Drive rc car using the commercial ESC", "time": "2019-02-14 00:37:34 -0600", "post_content": [" ", " ", "ROS : Melodic\nOS : Ubuntu 18.04\nRC Car : ZD Racing 10427s", "I am trying to build my first automated rc car.\nI am currently designing the actuation module that will take the \"geometry/twist\" msgs and drive the vehicle.", "I am trying to reuse the ESC that came with the RC Car, so I am searching for ROS package to do that.\nI read somewhere that an arduino is needed.", "Any suggestion would be helpful..."], "answer": [" ", " ", "Hobby ESC take the same kind of signal as a hobby servo: a 1-2 millisecond pulse, where the pulse length sets the speed and direction. Pulses shorter than about 1.5mS are one direction; pulses longer than 1.5mS are the other.", " You can generate the servo drive signal with an arduino running rosserial:  ", "  (and it's relatively easy to extend that demo to control two servo outputs; one for your ESC and one for steering). ", "There are probably libraries for the Raspberry Pi that will generate a similar signal.", "I've done this kind of conversion before, and there are a few things to watch out for:", "After using the stock ESC in my robot for about a year, I eventually converted it to a brushed 55T rock crawler motor and a Pololu motor controller with direct PWM and direction inputs.", "So if I understand correctly... I can use either an arduino or a motor controller (like Pololu)...\nBut is there a ROS package already available for this....??", "How to convert the twist msg to pwm ??", " ", " ", "Hi,", "in my projects, I use an Arduino and a simple ros python driver for external ESC driven motors.", "Additionally, check donkeycar ", " , \nthey use I2C connected interfaces to control ESC and motors. ", "Happy karting", "Cheers", "Chrimo", " How do you convert twist msg to pwm ??\n "], "answer_details": ["Depending on your ESC, there may be a small deadband around the 1.5mS, or that 1.5mS may drift a little bit. This can make it difficult to drive slowly, and difficult to properly send a stop command.", "Some ESCs have forward/reverse/brake modes. If possible, try to put your ESC into forward/reverse mode instead.", "Most ESCs will have a 5-6V OUTPUT on the servo connector instead of needing a 5V supply voltage. If you measure this voltage and hook it up to the right voltage supply on your arduino, you can use it to supply power for your arduino or other circuitry.", "The stock ESC in most hobby RC cars is tuned to go FAST, and getting it to go slowly can be a challenge.", " ", " ", " ", " ", " ", " ", " ", " "], "url": "https://answers.ros.org/question/315629/drive-rc-car-using-the-commercial-esc/"},
{"title": "Getting Transform Exception after implementing voxel layer", "time": "2019-10-23 22:05:58 -0600", "post_content": [" ", " ", " ", " ", "Hello friends,", "I recently made the switch from a 2d costmap to a 2.5d voxel layer costmap based on suggestions from one of my ", ". I decided to go with the ", " plugin because it promised to have improvements over the standard voxel_grid plugin and it seemed to have a lot of documentation. After implementing and some troubleshooting i was able to get everything working and i am able to see the voxel grid in rviz. The issue i am having is that i am contunually getting the following errors:", "i thing this is effecting the function of the rest of the robot because then i place a navigation waypoint it takes a minute or two to do anything if at all.", " ", " "], "answer": [" ", " ", "So, looking at your first error message:", "The timestamp on the message is 4523.04 - and it says the earliest data is 4513.12 -- this seems reasonable, you've got a 10 second buffer of TF data (which is the default).", "So your real issue is that you are trying to transform a camera image from more than 10 seconds ago (4512.57). There are several possible causes:", "Im afraid you might be right about the CPU usage (see the image added to the post) I had a sneaking suspicion considering im using an old HP Mini 110 from like 7 years ago. Are there any ways that you know of, short of buying a new PC, that i may speed things up? like using a smaller map, bigger voxels, a headless version of ubuntu, etc?"], "answer_details": ["Are you running the camera driver and navigation on the same computer? If not, make sure the clocks between the computers are synced (I'd suggest using chrony)", "How bogged down is the CPU? A quick check of \"top\" or similar tool while things are running would give you an idea if your CPU can handle everything. If your load averages are really high, you not have enough processing power to actually process the messages before the TF buffer goes out.", "It' is  possible there is something funky going on with the camera driver or timing (although I'd rule out the first two things first) - can you see camera data in RVIZ or does it have TF issues too?", " ", " ", " ", " "], "question_code": ["[ERROR] [1571884523.045139138]: TF Exception for sensor frame: , cloud frame: camera_depth_optical_frame, Lookup would require extrapolation into the past.  Requested time 1571884512.575208000 but the earliest data is at time 1571884513.123332977, when looking up transform from frame [camera_depth_optical_frame] to frame [map]\n\n[ERROR] [1571884524.605706427]: TF Exception for sensor frame: , cloud frame: camera_depth_optical_frame, Lookup would require extrapolation into the past.  Requested time 1571884513.871130000 but the earliest data is at time 1571884514.679229172, when looking up transform from frame [camera_depth_optical_frame] to frame [odom]\n", "footprint: [[0.5,0.35],[0.5,-0.35],[-0.5,-0.35],[-0.5,0.35]]\nraytrace_range: 3.0\nobstacle_range: 2.5\nobservation_sources: laser_scan_sensor\nlaser_scan_sensor: {sensor_frame: camera_link, data_type: LaserScan, topic: scan, marking: true, clearing: true}\n\nrgbd_obstacle_layer:\n  enabled:               true\n  voxel_decay:           30     #seconds if linear, e^n if exponential\n  decay_model:           0      #0=linear, 1=exponential, -1=persistent\n  voxel_size:            0.25   #meters\n  track_unknown_space:   true   #default space is unknown\n  observation_persistence: 0.0  #seconds\n  max_obstacle_height:   2.0    #meters\n  unknown_threshold:     15     #voxel height\n  mark_threshold:        0      #voxel height\n  update_footprint_enabled: true\n  combination_method:    1      #1=max, 0=override\n  obstacle_range:        6.0    #meters\n  origin_z:              0.0    #meters\n  publish_voxel_map:     true   # default off\n  transform_tolerance:   0.2    # seconds\n  mapping_mode:          false  # default off, saves map not for navigation\n  map_save_duration:     60     #default 60s, how often to autosave\n  observation_sources:   rgbd1_clear rgbd1_mark\n  rgbd1_mark:\n    data_type: PointCloud2\n    topic: camera/depth/points\n    marking: true\n    clearing: false\n    min_obstacle_height: 0.0     #default 0, meters\n    max_obstacle_height: 1.0     #default 3, meters\n    expected_update_rate: 0.0    #default 0, if not updating at this rate at least, remove from buffer\n    observation_persistence: 0.0 #default 0, use all measurements taken during now-value, 0=latest \n    inf_is_valid: false          #default false, for laser scans\n    clear_after_reading: true    #default false, clear the buffer after the layer gets readings from it\n    voxel_filter: true           #default off, apply voxel filter to sensor, recommend on \n  rgbd1_clear:\n    enabled: true                #default true, can be toggled on/off with associated service call\n    data_type: PointCloud2\n    topic: camera/depth/points\n    marking: false\n    clearing: true\n    min_z: 0.1                   #default 0, meters\n    max_z: 7.0                   #default 10, meters\n    vertical_fov_angle: 0.7      #default 0.7, radians\n    horizontal_fov_angle: 1.04   #default 1.04, radians\n    decay_acceleration: 1.       #default 0, 1/s^2. If laser scanner MUST be 0\n    model_type: 0                #default 0 (depth camera). Use 1 for 3D Lidar\n", "global_costmap:\n  global_frame: map\n  robot_base_frame: base_link ..."], "answer_code": ["[ERROR] [1571884523.045139138]: TF Exception for sensor frame: , cloud frame: camera_depth_optical_frame, Lookup would require extrapolation into the past.  Requested time 1571884512.575208000 but the earliest data is at time 1571884513.123332977, when looking up transform from frame [camera_depth_optical_frame] to frame [map]\n"], "url": "https://answers.ros.org/question/336032/getting-transform-exception-after-implementing-voxel-layer/"}
]