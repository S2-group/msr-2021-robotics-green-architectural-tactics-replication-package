[
{"title": "Setting up buildfarm", "thread_contents": ["Im trying to setup a build and trying to follow the tutorial on this page.", "I cloned this repo and entered the directory.", "git clone ", "but im unable to locate the ", " file to build the image.", "Hi ", ", that documentation is out of date from our old docker in docker experiments. I\u2019ve removed it. Please follow the main instructions in the ", " to set things up.", "I\u2019ve opened a PR to update the docs ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/setting-up-buildfarm/2330"},
{"title": "Initial setup", "thread_contents": ["Im trying to setup a buildfarm and need some clarification.", "Question:", "Thanks in advance,", "I cloned this repo - ", "The doc in the repo says I need to model as per the config repo # ", "Question:", "The file inside the buildfarm_deployment repo does not look like the config files in the buildfarm_deployment_config. Each directory(master,repo and slave) has only manifests and repo_files directory. Where should I drop these config files?", "You don\u2019t need to fork the buildfarm_deployment repo, but the buildfarm_deployment_config repo. See the instructions ", " about how to \u201cfork\u201d privately.", "The system is separated such that you can consider the buildfarm_deployment repo to be used as a tool or library without modifications. You would only need to fork it if you want to change the behavior, and then you would modify your config to refer to your own forked version.", "Is there a dockerfile that I can use to spin off 3 servers in my local env.", "No there is not. We originally started developing and testing doing that. However because we use Docker for isolation of the build steps we had to drop running the 3 host types in Docker too. This is because Docker-In-Docker is possible, but still has many hiccups that caused many more problems than we wanted to deal with. There\u2019s a hope in the future to support running all three server configs on the same host. But that will take a bit of refactoring and is not currently a development priority. We recommend 3 VMs for testing/deployment.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["I cloned this repo - ", "\n", "The doc in the repo says I need to model as per the config repo # ", "\n", "The file inside the buildfarm_deployment repo does not look like the config files in the buildfarm_deployment_config. Each directory(master,repo and slave) has only manifests and repo_files directory. Where should I drop these config files?", "Is there a dockerfile that I can use to spin off 3 servers in my local env."], "url": "https://discourse.ros.org/t/initial-setup/2331"},
{"title": "Scheduled maintenance for build.ros.org Thursday 24 August 13:00-15:30 Pacific (UTC-7)", "thread_contents": ["We\u2019re getting ready to migrate the ROS buildfarm from Trusty to Xenial.", "\nIn order to completely test the migration of the Jenkins host we\u2019re scheduling some downtime to take a preliminary snapshot of the production Jenkins data.", "At 13:00 Pacific time tomorrow Jenkins will be put into shutdown mode, preventing new jobs from starting.", "\nDepending on the number and estimated completion time of running jobs, they may be allowed to complete or they may be terminated.", "Once there are no running jobs we\u2019ll begin archiving the directory and transfer it to a test host.", "\nAfter the transfer is complete we will bring Jenkins out of shudown preparation and close the maintenance window.", "Updates will be posted to ", " and I\u2019ll post back here once the whole process is complete.", "Maintenance has been completed and the buildfarm is back online.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/scheduled-maintenance-for-build-ros-org-thursday-24-august-13-00-15-30-pacific-utc-7/2498"},
{"title": "New release of the ros_buildfarm package (version 1.4.1)", "thread_contents": ["I have just released version 1.4.1 of the ", " package. There are only a few changes this time. The one addressing the Debian mirrors from the ", " is likely the most helpful one if you target Debian:", "Happy Building!", "PS: This release will likely be the last one targeting Trusty-based machines. If you are using the ", " branch in your custom buildfarm you should carefully read the upcoming announcement since the next release will require Xenial-based machines running Java 8.", "One note regarding targetting the Debian mirror fix:", "\nIt proved to work well for us when deploying the buildfarm on AWS machines. Some people experienced issues with this CDN when using ros_buildfarm on travis (example ", ") so you may want to target a different mirror (by replacing this ", ") if you are facing the same issue", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["use cloudfront mirror for all Debian sources"], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-1-4-1/2537"},
{"title": "Breaking changes coming: buildfarm_deployment moving to Ubuntu 16.04", "thread_contents": ["This post announces some breaking changes for build farms based on ", " and ", ".", "The canonical ROS buildfarm at ", " has recently been migrated to Ubuntu Xenial ", "Over the next couple of weeks I\u2019ll be preparing a guide with specific changes and suggestions for implementing the migration based our experience migrating the canonical farm. There will also be instructions for keeping your instances targeting the current Trusty-based farm once the master branch receives the Xenial updates.", "If you\u2019re interested in reading ahead you can check out the following pull requests on GitHub", "I don\u2019t have an exact hard deadline for when the master branch will move to xenial only as it depends somewhat on the stability of the new build farm and the amount of time I can allocate to writing the migration guide but expect at least two weeks\u2019 notice and I don\u2019t expect to have it done before ROSCon.", "On that note, I\u2019ll be at ROSCon and it\u2019ll be my first time so if you want to talk about the buildfarm or other ROS infrastructure there hit me up!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/breaking-changes-coming-buildfarm-deployment-moving-to-ubuntu-16-04/2634"},
{"title": "Build failing: mongdb_store", "thread_contents": ["Hi, apologies if this is the wrong topic to put this in (if so, please move/advise), but I have a package that is failing to build on the buildfarm and I can\u2019t immediately see why.", "The package is ", " for 15.04 (jessie).", "An instance of the job is here: ", "It appears to be failing due to a missing rosdep key for ", " but this is listed in the rosdep: ", "We have this building successfully on 14.04 (", ") and 16.04 (", "), so I\u2019m not sure what the issue us with 15.04. Anyone got any ideas?", "thanks,", "Nick", "The key error is in the debian pipeline after rosdep has already been resolved by bloom.", "Your problem is that mongodb does not have a armv8 version for Jessie. It has i386, amd64, and armhf: ", "So you\u2019ll want to add this package to the jessie armv8 blacklist: ", " And then the buildfarm will stop trying to build the package.", "Thanks! PR created: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/build-failing-mongdb-store/2685"},
{"title": "Status of Xenial migration", "thread_contents": [": can you comment on the current status of the recently merged Xenial upgrades to ", " and related repositories?", "I\u2019m starting to deploy a new buildfarm instance and would like to do a Xenial based one, but seeing the recent PRs against ", " and some of the tickets make me hesitant.", "There\u2019s also still a ", " branch. Is that a remnant, or is that also going to be merged at some point?", " I\u2019m very glad you asked! Making a status post has been on my list of things to do and has been consistently pre-empted so I appreciate that you prompted one.", "In one sentence: It works, there are some issues inherited from the Trusty buildfarm as well as some new ones and more changes are coming.", "\nThis is causing a significant bottleneck for low level packages in the very large rosdistros, kinetic and indigo. Unless a farm is rebuilding the entire rosdistro this may not be too much of a problem. We\u2019re managing it on the canonical buildfarm until we have time to pick up the investigation by throttling the number of concurrent jobs when low-level packages are rebuilding.", "\nThe early days of the new buildfarm gave the Jenkins java process too much of the total system memory and during high load periods spikes would trigger the oom-killer. Worse still, the buildfarm would not come back on its own. I\u2019m pretty sure the issue here is that the init script created by the jenkins puppet module is not properly configured to be managed via xenial\u2019s systemd/init compatibility and I plan to resolve this by writing an explicit systemd service for Jenkins rather than using the one built into the puppet module.", "\nThis is a seeming regression that has yet to be investigated and primarily effects buildfarm instances with elastically provisioned workers.", "I would like to work with the community to settle on a branching/versioning model that will satisfy our need to keep ", " operating smoothly and with live changes conducted properly through configuration management and the community need to have configuration management that doesn\u2019t change drastically week over week. I\u2019m very open to suggestions from the community here. I\u2019d be fine adopting ", " outright, adopting basic versioning, or maintaining, with the help of a community team, ", " and ", " branches of the buildfarm deployment repositories.", "The buildfarm_deployment repository has seen a lot activity recently, primarily because improving and maintaining it is one of my core responsibilities. In order to facilitate the move to xenial I paid down quite a bit of technical debt in the form of duplication. I also made some refinements which had implications beyond my understanding at the time and which required further changes down the line. With outstanding issues on the Trusty buildfarm becoming increasingly pronounced, I also dropped some features from the initial \u201crelease\u201d of the xenial branch in order to perform the migration.", "The largest of the postponed features is currently in progress as ", " and will enable deploying a ROS buildfarm on a single host, rather than the three needed to run the complete buildfarm today.", "The deployment scripts had overlapping configuration values for the different roles and in order to realize a single-host buildfarm the configuration \u201cAPI\u201d/structure will need to change as well. So the current configuration values will require later changes to keep up with master when that pull request merges. I\u2019m happy to open a discussion on discourse, or in a GitHub issue, to go into further detail on the branching model discussion. Where do folks prefer?", "There\u2019s also still a xenial branch. Is that a remnant, or is that also going to be merged at some point?", "The xenial branches are remnants and will be removed at a future date. The buildfarm_deployment xenial branch was merged by ", " and has not been deleted yet to accommodate hosts that autoreconfigure based on the xenial branch. I think my ROS 2 farm is the primary culprit here and per ", " I was waiting to delete branches to give folks testing them time to move off and onto ", " which is the branch that currently sees all new development.", "The ", " branch in the ", " repo was just a left over. I just removed the branch to avoid confusion.", "Hi all,", "I\u2019ve been trying to get ros_buildfarm instance up and running using the latest updates and, I\u2019ve faced two major issues; I still haven\u2019t been able to solve one of them.", "The first one might sound really silly. I used to get jenkins installed from ", ", that at the time it had a newer version of it (2.73.3, though 2.89.1 is now the most recent version). However, yhe default configuration of ", " uses ", ".", "I filled the configuration files with the data from the servers I was going to use and I proceeded to install and reconfigure the ros_buildfarm. Once it was over, apparently with no errors, I wasn\u2019t able to get into jenkins by using its web interface. I then checked apache and the opened ports: Apache was installed, and port 80 was open, though only to IPv6.", "Finally, after finding the pupper log file by chance (I\u2019ve never used before puppet and I didn\u2019t know it had a log file), it had an error on a non-existant jenkins version. Once the configuration file was changed to ", ", the whole process worked properly. It would had been nice to get an error message regarding an unavailable jenkins version or maybe some hints about the existing log file or the possible errors.", "The second problem is related to job generation. After cloning the ", " and ", ", and filling in the data, I\u2019ve not been able to generate jobs due to a python ", " (although this might not be the place, just in case, i\u2019ve uploaded ", ". This has me currently blocked, and I don\u2019t know how to solve it, so some hints or documentation would also be nice.", "Thank you for your hard work,", "Best regards,", "The second problem is related to job generation. After cloning the ", " and ", ", and filling in the data, I\u2019ve not been able to generate jobs due to a python ", " (although this might not be the place, just in case, i\u2019ve uploaded ", ". This has me currently blocked, and I don\u2019t know how to solve it, so some hints or documentation would also be nice.", "Looking through the log you linked:", "Don\u2019t the ", " scripts require Python 3.x (see also ", ")?", "I had opted to use python2 because the scripts need the ", " package. Only the python2 version of the package is present in the ", ".", "I have proceeded to remove those and install their python3 versions by using ", ", and now the error is gone, thanks!.", "I had opted to use python2 because the scripts need the ", " package. Only the python2 version of the package is present in the ", ".", "I think that is why the instructions install that package using ", ".", " glad you\u2019re using the ROS buildfarm scripts and thanks for sharing your experience and issues doing so. If you could please open an issue on ", " for the jenkins LTS version that would be awesome. Feel free to open issues on that repository if anything else doesn\u2019t work as documented during the deployment process.", "Additionally, per our ", " please ask questions on ", ". Adding the ", " tag to your question is usually enough to notify folks who can help.", "Thanks,", "\nSteven!", ": trying to deploy an instance of the Xenial based farm. Running into some issues and have some questions.", "Actual issues I\u2019m posting on the appropriate tracker, but for questions I should probably start a new thread in the ", " category here on Discourse?", " Thanks for opening issues. I\u2019ll be aure to look at them when I get to the office.", "Questions about running the buildfarm_deployment projects are best asked on ", ". I\u2019m watching several labels related to the buildfarm including, I think, \u201cbuildfarm\u201d.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/status-of-xenial-migration/3330"},
{"title": "Jenkins pw utility", "thread_contents": ["Perhaps useful to others: ", ". This is a simple script that generates a Jenkins compatible password hash using ", ".", "Starting up a Jenkins instance to get hold of such a hash for your new Jenkins deployment seemed a bit convoluted ", "Thanks ", " That looks super useful. This would be great to leverage in the deployment so we could avoid storing the intermediate values in the config and having to keep a parallel record of them.", "You mean the admin password ", " the hash? Yes, I was thinking about that as well.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/jenkins-pw-utility/3779"},
{"title": "New to buildfarm!", "thread_contents": ["I\u2019m new to build farm. Can someone explain or send me the link of any tutorial that explains how to setup a buildfarm. I\u2019m having a hard time understanding the documentation here. ", "The README of the repository you linked to point to the following wiki page: ", "That should be you with the overview of two parts \u201cmachine provisioning\u201d as well as \u201cjob generation\u201d. For each part the corresponding repository contains instructions how to proceed.", "I tried to follow the instructions in ", ". I mirrored the repo to my git account. But the next step includes making some changes in the common.yaml file which I couldn\u2019t find. I\u2019m missing something. I just don\u2019t know what it is. It will be great if you could provide me with some step by step guide on how to use these links.", "Thanks !", "The file ", " is part of the configuration repository: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-to-buildfarm/4364"},
{"title": "New release of the ros_buildfarm package (version 2.0.0)", "thread_contents": ["I have just released version 2.0.0 of the ", " package.", "The most important reason for the major version bump is that it requires the provisioned machines to use Xenial as well as Java 8. So ", " for your current buildfarm. In order to switch from a previous release (1.4.x) you must update your provisioned machines according to the current ", " instructions. The advantage of the newer host system and versions of various dependencies are plenty of fixes and improvements.", "Also this version targets Jenkins (up to) version 2.89.x. The latest LTS release of Jenkins 2.107.x brings additional changes which ", " hasn\u2019t been updated for. There will be a follow up release addressing this in the near future.", "You will find a complete list of all the changes in the ", ".", "Happy Building!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-2-0-0/4381"},
{"title": "ROS Buildfarm October 2017 Guide to new changes", "thread_contents": ["Open Robotics hosts the primary ROS buildfarm at ", " which has recently migrated to new hosts running Ubuntu Xenial. In the process of doing so we substantially overhauled the configuration management and made some desirable terminology changes which require intervention when updating the ros_buildfarm python module and scripts.", "If you or your organization run a Jenkins instance or cluster that uses the python libraries and scripts found in the python package ros-buildfarm (also at ", ") the upcoming changes affect you.", "Additionally if your buildfarm machines were originally provisioned with the configuration management tools in ", " and ", " there are significant changes that are of particular importance if you kept the default autoreconfigure settings. Action is required to ensure successful operation even if you are not ready to update at this time.", "Xenial support will not be merged into master branches earlier than 19 October. Between now and then, I hope you\u2019ll determine which path you\u2019ll take, review the migration guide, and raise any questions or concerns you have before that date.", "The ros_buildfarm release needed to perform either upgrade method is still pending. The ", " of ", " contains the bulk of the likely changes. As soon as is feasible we will create the release and update this guide.", "Do not attempt to re-run ", " from buildfarm_deployment on the master host. It will clobber many Jenkins configuration details.", "Running the xenial branches for buildfarm_deployment and buildfarm_deployment_config on a trusty-based host has never been tried and is not supported. Some modules assume systemd is the service supervision provider and definitely will not work.", "It\u2019s possible to update your running hosts with limited configuration changes that will allow them to benefit from changes to subsequent releases.  ", "Change the path to the jenkins slave jar in /etc/defaults/jenkins-slave to use the new home directory.", "Check the crontab for entries with hard-coded paths to the old home directory.", "Apply the label ", " to all executor nodes with the previous label ", ". To ensure this change will persist between restarts usually requires changes to /etc/default/jenkins-slave.", "\nRename the ", " job to ", " via the Jenkins web UI.", "Update the ros_buildfarm tools on your buildfarm", "With some changes to your buildfarm\u2019s configuration you can continue to use the current (Trusty) configuration management infrastructure and buildfarm scripts until you are ready to perform the upgrade. Potentially you could continue to use the Trusty configuration indefinitely but you will be unable to use newer versions of the ros_buildfarm tools.", "We\u2019ve done no testing to support upgrading buildfarm hosts to Ubuntu Xenial in place. The migration to Xenial for Open Robotics was performed by provisioning new hosts running Ubuntu Xenial, running the updated configuration management and migrating the Jenkins and repository data to the new hosts. While all buildfarm deployments would benefit from the improvements in the updated buildfarm stack, particularly large instances, it is ", " in order to use newer versions of the ros_buildfarm scripts. You can instead follow the section marked ", ". Our migration followed the basic procedure below:", "Provision new Xenial hosts: master, repo, and agent.", "Run the Xenial configuration management scripts from ", " on the new hosts.", "Put Jenkins into Shutdown mode and stop any remaining builds (or let them finish)", "Use rsync to copy packages from the existing repo host to the new xenial repo host.", "Stop trusty jenkins master, and jenkins agents on all trusty machines.", "\narchive /var/lib/jenkins  Expect 10-40MB/s depending on compressibility and IO availability on an AWS machine.", "Stop ", " and ", " on Xenial hosts if they were running.", "Transfer archive to new master ~10 minutes AWS internal", "Move existing /var/lib/jenkins into /tmp (it should not contain anything worth preserving)", "Extract Jenkins archive into /var/lib on the Xenial host", "Bring new jenkins master online with migrated config", "Run ", " using version TBD of ros_buildfarm", "\nStart jenkins agents on xenial hosts", "The \u201cTBD\u201d version of ros_buildfarm needed for the xenial-based hosts mentioned above is 2.0.0. The release announcement is here: ", "I either am having a moment with discourse or cannot edit the post directly to update the guide.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "Ubuntu 16.04 LTS", "Jenkins LTS 2.60.3", "Docker CE 17.05", "Java 8", "Puppet 3.8", "Update Jenkins terminology in job names, scripts, system directories, and docs.", "Add config generation for upload jobs (only for ", ")", "Refactor of puppet modules", "Modules attempt to follow the \u201cRoles and Profiles\u201d pattern and are factored into reusable components.", "Although this is untested, it is more possible than before to incorporate profile modules into a separate puppet infrastructure.", "Updated and pinned to current puppetforge releases for upstream puppet modules.", "Switched to the puppet future parser (puppet 4.x compatible parser)", "Retire vendored upstart module in favor of systemd service provider on Xenial.", "Add script to build reprepro 5.1.1 from backported sourcedeb.", "Add script to fetch Jenkins plugin versions from ", " and generate a puppet module installing those plugin versions.", "Unified installed puppet modules across roles.", "Refactored hiera config to share common data and provide role-specific configuration separately.", "Install_prequisites.bash now uses system packages for puppet and librarian", "Reconfigure.bash stores the configured role to prevent accidents when reconfiguring.", "Update user account and hiera key names for current Jenkins terminology where possible.", "Rename the local user account from ", " to ", ".", "This is a somewhat system dependent operation. On a Trusty system. The procedure below should cover most installs.", "Gracefully stop services running as the jenkins-slave user.", "Check for running processes ", "\n", "If there are remaining non-critical processes stop them with ", " otherwise wait for them to shut down gracefully.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Using the ros_buildfarm scripts version TDB or greater run ", "\n", "Review the diff output for potential issues.", "Commit the changes with ", "\n", "Update the auto-reconfiguring host configuration.", "If your configuration is set up to use the ros-infrastructure/buildfarm_deployment repository directly, you will need to make sure that any hosts with the ", " setting have their autoreconfigure_command updated to use the ", " branch rather than ", ".", "Set ros_buildfarm to use the last release before the xenial-related changes.", "In order to preserve the current behavior until you\u2019re ready to upgrade make sure you\u2019re using version 1.4.1 or earlier.", "\n", " is designed to track the master branch of the ros_buildfarm scripts. Using the master branch or subsequent ros_buildfarm releases will use updated terminology that may cause errors or unexpected behavior if not handled by following the section: \u201cUpdating ros_buildfarm on existing (Trusty) hosts\u201d.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/ros-buildfarm-october-2017-guide-to-new-changes/2840"},
{"title": "Building ROS on Mac in CI", "thread_contents": ["I am considering building some ROS packages on Mac regularly for catching up broken build/tests on this platform.", "The first step would be to do a prototype (", " looks promising). If it goes OK, this needs to be integrated with ROS Buildfarm.", "What do you think - useful, doable?", "The main issue is the need for a reproducible build, which is hard to do because macOS doesn\u2019t have docker and no one (outside of Travis-CI that I\u2019m aware of) has automated use of VM\u2019s with checkpoints to emulate a \u201cclean starting state\u201d for each build. This is a crucial step for building packages in my opinion, but AFAIK (it could have changed) ", " only provides hosting on mac minis, not the automation for repeatable jobs. Travis-CI does provide this as continuous integration, and I believe some people are using (abusing?) this to build Homebrew bottles ", " might know more about that then me at this point.", "If you\u2019re interested in trying anyways, you\u2019ll want to catch up on the existing efforts:", "Those two links are the best summaries I can find, but efforts so far have stalled, as just keeping it building from source on macOS with Homebrew has become of a burden than the community appears to be willing to bear.", "In order to be integrated into the \u201cROS buildfarm\u201d (by this I assume you mean ", "), we\u2019ll need a way to provision machines, generate artifacts on a per package basis (bloom does this for debian and fedora packaging), and extend the build farm itself to create the necessary jenkins jobs to actually execute the packaging on the build machines and upload the result somewhere. Once you get to this point, ", " and/or ", " might have more to add.", "Just glancing at ", " again, it looks like they now support provisioning VM\u2019s on demand and use VMware under the hood:", "Also, I\u2019ll mention that Homebrew does it\u2019s packaging (last time I checked, again could have changed) not by having VM\u2019s, but by deleting the homebrew directory each time before a new build starts. So this is more like ensuring you completely cleanup the system before finishing a job, rather than discarding the changes and going back to a previous docker image or VM snapshot. This might also be possible, but in my experience it\u2019s harder for ROS since we not only install things from Homebrew, but also things from pip for Python dependencies (also possibly Ruby\u2019s gem, etc\u2026).", "I maintain the osrf/simulation tap where I build bottles for gazebo and its dependencies. We use a few mac minis with jenkins for the bottle builds.", "I have seen folks attempt to build bottles with travis-ci, though I\u2019m not sure what the current status is:", "\n", "\n", "The biggest challenge with maintaining homebrew bottles on a non-core tap is that your bottles can break at any time if a new bottle is built for one of your dependencies (such as boost or protobuf). I have a daily job that tests one of our gazebo bottles, and then we manually rebuild them if we notice that job failing due to an outdated dependency. We try to stay on top of it and don\u2019t see too much downtime, but this is not scalable. I wouldn\u2019t want to be in charge of keep homebrew bottles for all of ROS working.", "\n", "\n", "My initial focus is just building and testing.  Packaging will come later if building and testing is successful.  I believe both things is going to be useful - knowing that a particular change causes a build break or a test failure on Mac is a useful information for package maintainers.", "Yes, doing a clean (or more generally determistic) build is a concern for any CI process, and this work should get some data on the topic.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "If you want to avoid building \u201ceverything\u201d at once (including all recursive dependencies) you will need packaging to provide the binary packages for the dependencies.", "Dirk is exactly right. Even if you only want to use travis for CI (which is easy enough to set up), once you have enough dependencies, your build will timeout if you don\u2019t install them from pre-compiled binaries (bottles in the homebrew case).", "According to ", "  the", "\njob\u2019s timeout is 50 minutes. When I was following ", "  on my computer", "\nfor ", " I think the whole process was done in less than 50 minutes and it includes quite a few packages being built.", "I plan to start with something small, see how it works, and then extend if successful.", " and/or ", " might have more to add.", "I think most of the key points have been covered. Any automated build and testing will benefit the project but in order to be widely useful, it needs to be reproducible and make efficient use of resources. You might find the ", " repository interesting. It shows how we build and test ROS 2 from source on ", " which has several windows and mac machines attached to it. As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "If it goes OK, this needs to be integrated with ROS Buildfarm.", "We would definitely encourage contributions for MacOS support into the ros_buildfarm repository; giving community members the opportunity to deploy it. However since MacOS is not an officially supported platform, it is a small fraction of the user base, and costs for Apple hardware or compute time are significantly higher than other platforms, deploying MacOS to the official infrastructure is not a decision that can be made based solely on the technical capability to do so. Once we have the capability, if an organization would like to sponsor efforts to deploy and support the ongoing maintenance of MacOS infrastructure it could be reconsidered.", "I started with Travis, picked up ", ", and added the building and testing on Mac.  ", ". Then I opened ", " to discover that the last build in Travis happened 5 years ago.  That\u2019s many changes ago.", "Is Travis an option? Having VMs for free for OS projects sounds like a good deal to me.", "For smaller packages like rosdep Travis is definitely an option that\u2019s already in use. You\u2019ve found the Travis CI history for the old repository location. It was moved to", "rosdep multi-package manager system dependency tool - ros-infrastructure/rosdep", "So you can see the current status is here: ", "It looks like you\u2019ve already found it, but for others reference there\u2019s now a [PR under review}", ")  for rosdep testing on macOS.", "As the project grows we\u2019re starting to feel the pain of this approach as CI times climb.", "Do you have any information on how you plan to address this pain?", "Do you have any information on how you plan to address this pain?", "I think what ", " said: provide binary packages for the dependencies. You can see my other comments in this thread about that current status for macOS.", "I am focusing right now on building python projects on macOs.  Example: ", ".", " folder contains few things, and it will be pretty much the same for every project.  I am thinking about moving the content of .travis into a separate repo and adding it to existing repos as a submodule.", "What do you think about using submodules for this case?", "Normally I dislike using submodules, but given that it\u2019s only required for travis, I think it would be ok. At least I can speak for the repositories I maintain, other maintainers might not feel that same way.", "I too dislike submodules, and although they would only be necessary for travis if they are there by default it can be confusing/disruptive for developers. If it\u2019s only necessary for travis it would be simple to package that functionality into a resource that travis would fetch/install during the setup phase, like it does like any other testing/tooling dependencies which are installed via the .travis ", " segment.", "Thanks for the ideas.", "MacOS CI for ROS packages would be a great addition, even for core repositories like ros_comm (see e.g. ", ").", "You might want to have a look at ", ", which can be easily added to other repositories without submodules, just loke ", " mentioned. Maybe it even makes sense to integrate mac support there, since this is already in use by many packages it seems.", "You might want to have a look at ", ", which can be easily added to other repositories without submodules", "The same goes for ", ". Afaik neither of the two offers support for macOS atm. For ", " it is certainly planned to also test on macOS and Windows in order to satisfy the need in ROS 2. Once that works on Jenkins it should be feasible to run the same on Travis (with macOS) and AppVeyor.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-ros-on-mac-in-ci/4071"},
{"title": "ROS Industrial CI for package maintainers and users", "thread_contents": ["I have registered 40 new packages and updated 48 existing packages in this Sync (for Kinetic 2018-04-04). Most packages are the robot-related software by ", " such as TurtleBot3(mobile), OpenManipulator(manipulator), ROBOTIS OP3(humanoid), THORMANG3(humanoid), MANIPULATOR-H(manipulator) and RH_P12_RN(gripper) that can be used with ROS.", "To be honest, I was worried  \u2018What kind of error occurred?\u2019, \u2018What email will Tully send me?\u2019 every time when I registered a new package. But from this sync, I used ", ". I think it is an essential tool for package maintainer.", "Easy continuous integration repository for ROS repositories - ros-industrial/industrial_ci", "Many people are already using ROS Industrial CI, but I still think there are users who don\u2019t know what this is and how to use it. I strongly recommend using this tool if you are not using it. It does not matter whether you are a package maintainer or not. If you use ROS Industrial CI, This makes it very easy to manage dependencies for your ROS package. In addition, various tests can be run in various development environments, so you can find what problems are involved.", "Thank you to all the maintainers and I am grateful to ", ", ", ", ", " for their hard work.", "\n", " Thank you for your kind advice. ", "Thanks for the feedback ", "!", "For fairness, there are at least a few CI options in ROS you can find in ", ". You\u2019re welcomed to update a ", ".", "May the CI be with your development.", "I have added my solution, ", " on the wiki page, I did not know this page existed.", "\nI should try again ROS Industrial CI ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-industrial-ci-for-package-maintainers-and-users/4390"},
{"title": "Help with Buildfarm setup", "thread_contents": ["This is with reference to my previous question (", "). I\u2019m trying to be as exhaustive as possible. I have no previous experience in setting up a ros buildfarm before and I have very little knowledge about it. I tried to follow the documentation here (", "). I got to the second step after mirroring this (", ") repository and tried to change the parameters inside the common.yaml file. I changed the master ip, repo ip, jenkins uname and password. But the other things listed in the documentation is not available in the common.yaml file.", "I\u2019m making my machine as the master and a raspberry pi as the repo and another raspberry pi as agent. The master IP is my machine\u2019s IP and the repo and the agent IPs are the two raspi\u2019s IP. In common.yaml file, I edited these stuffs and changed the jenkins::slave::ui_user and ui_pass to my github username and password which is authorized in my jenkins account. Then, autoreconfigure is setup to be false in the common.yaml file but it is listed as branch in the documentation. Should I go about changing this ?", "Coming to ssh keys, the key specified is the public key I generated using the ssh keygen command in my master machine ?", "Anybody who has setup a buildfarm before, it will be great if you could provide me with an example and if possible explain why we are performing each step along the way.", "P.S: This is not my homework. This is something I\u2019m really willing to learn. Thanks !", "This question was posted to ", " as ", " and I\u2019ve done my best to answer it there.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/help-with-buildfarm-setup/4512"},
{"title": "New patch release of the ros_buildfarm package (version 2.0.1)", "thread_contents": ["I have just released version 2.0.1 of the ", " package.", "Compared to the recent 2.0.0 it only contains a few minor changes (see the ", " for details).", "If you are coming from a 1.x release please make sure to read the ", " before trying to upgrade.", "Happy Building!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-patch-release-of-the-ros-buildfarm-package-version-2-0-1/4648"},
{"title": "How to configure buildfarm after setting up jenkins?", "thread_contents": ["I have already asked this question in ros answers. To be on the right group, I\u2019m creating the question here too. This is a continuation of the question that I asked ", " previously. Step 1 was to basically make jenkins run on master. I was able to achieve that. Now the next step I believe is to configure jobs for the packages that I have built. I understood that I have to edit the configuration files here in this ", " but for some reasons the documentation is not so clear on what are the files that should be edited.", "So here it goes. This is what I have done so far. I went into this ", " and forked the ros_buildfarm_config repo to my account according to the instructions ", ".", "Please don\u2019t double post questions in multiple forums. You have already asked this ", " in the right forum ", " and have people trying to help you there. It\u2019s poor etiquette to ask questions multiple times.", "Please review our support guidelines at ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["As I earlier mentioned, let us assume that I\u2019m using kinetic distribution and in the Disable notification of maintainers/committers section, I was asked to set false to all committers which by default was already set to false in all the files. When I say all files, I changed or verified whether they are false in index.yaml, doc-build.yaml, doc-released-build.yaml, release-armhf-build.yaml, release-build.yaml, source-build.yaml inside kinetic folder.", "Next up was to update administrator notification. I changed the email on all files as said in the previous point.", "\nI want to know whether whatever I did in these two points are correct. If yes, I request you to update the documentation, its totally out of sync and not intuitive.", "I figured out that the jenkins_url takes the ip address of the master machine and the repo_hostname.example.com simply gets replaced by the repository ip address on all the yaml files mentioned earlier. Is that right ? Also I want to know whether the PGP key used here is the same as the ones used in the buildfarm_deployment_config ?", "After that I understood that inorder to run jobs on my custom ros package(s) in the buildfarm, I was given two options. One to perform full fork and the another one being add-on fork. As I understood that clearly, I decided to go with option 2 which is Add-on fork so that I don\u2019t have to worry about other ROS packages from ", ". Interesting thing here is that I saw that we have to add a tag to custom distribution files. I understand that we have to add those but to what files is not so clear. Also mentioning custom distribution file is not very clear to me.", "Is there a better way (may be a structural way) to actually tell me, what I should be doing in ", " ? Like could you be more specific on where should I place the import_distro.yaml file ? May be an example would be great. And also I have no idea how to invoke import_upstream job. Could you please be clear here ?"], "url": "https://discourse.ros.org/t/how-to-configure-buildfarm-after-setting-up-jenkins/4830"},
{"title": "Ros-ourdistro-... style add-on rosdistro", "thread_contents": ["Hi there,", "We\u2019ve successfully deployed the buildfarm in three virtual machines, and we have created an add-on rosdistro for kinetic with a simple package that only depends on roscpp. Now we can build and generate debian packages using the infrastructure.", "Next step for us is to properly handle our forked ros packages. Let\u2019s assume a package called ros-kinetic-foo (from official repositories), but we don\u2019t want to use it because we want to use our forked version. Our idea is to make a clear separation between the official and forked version,  creating a package called ", ".", "Basically we\u2019d want to use ros-kinetic-* packages together with our ros-ourdistro-* packages.", "We\u2019ve tried creating a new entry in our forked rosdistro. The ourdistro/distribution.yaml is a copy of the kinetic/distribution.yaml file, and ourdistro/addon_distribution.yaml only contains the package that depends on roscpp", "When executing devel job, the package tries to find ", ", and not ros-kinetic-roscpp", "Not sure how this problem should be addressed. What\u2019s the best way to handle it?", "You\u2019re going to run into several issues:", "So at the very least, you should expect to have only one \u2018ROS distribution\u2019 active at a time.", "Hello,", "I haven\u2019t had the chance to set up an addon rosdistro myself yet but from reviewing the ", "I don\u2019t think your custom rosdistro should include the distribution.yaml from kinetic as ", " as this will tell rosdep will expect to  resolve dependencies using ros-ourdistro-roscpp rather than ros-kinetic-roscpp. When you remove that distribution file you\u2019ll need a way to resolve dependencies in your addon distro to their kinetic counterparts. Currently the tool for doing so is ", ". It will take an upstream rosdistro and generate a static rosdep definitions file for it. Adding that definitions file to your local rosdep sources will let you use rosdep and bloom to resolve dependencies for your addon packages to the upstream versions instead.", "I do have a prototype branch ", " that allows for the inclusion of additional rosdistros in rosdep sources directly but I can\u2019t fully recommend it as it may change while in review to be included in upstream rosdep.", "there\u2019s no way to get two base workspaces in ", " to overlay via catkin", "This limitation I don\u2019t have any suggestions to get around.", "On the robots from PAL Robotics (e.g., the Tiago), they do something similar to what you\u2019re trying to achieve. They use a stable version of e.g. ROS Indigo, which is just a snapshot of the official packages and which is installed to ", ". In addition, they have their own \u201cdistro\u201d which they call \u201cPAL Dubnium\u201d that contains their own addon packages and which is installed to ", ", and which overlays the Indigo workspace. I\u2019ve found ", " on Google, but sadly it doesn\u2019t contain many details. Perhaps you should contact the author (Luca Marchionni) directly.", " may have slightly more time to answer from PAL Robotics.", "Overlaying works pretty well.", "Hi like ", " and ", " have said we do use our on custom distribution on top of ROS.", "We started doing this before the new buildfarm was created and the documentation linked by ", " was created, so what we\u2019ve done may differ with the steps detailed there.", "We freeze make a snapshot of a ROS distribution\u2019s repositories (let\u2019s say Kinetic), and then we release our custom ROS packages or our own software. Our package naming scheme is pal-erbium-my-pkg-name.", "We install them on /opt/pal/ and ROS keeps installed in /opt/ros. Instead of sourcing /opt/ros/kinetic/setup.bash, you source /opt/pal/erbium/setup.bash, and this file already sources kinetic and makes the overlays behave as expected.", "As long as you source the original ROS workspace (say Kinetic) before building your custom distro, I believe the overlays work flawlessly.", "We usually have 3 levels /opt/ros/kinetic, /opt/pal/erbium and working workspace.", "As long as you source the original ROS workspace (say Kinetic) before building your custom distro, I believe the overlays work flawlessly.", "I believe what ", " means to say is that you cannot have two install spaces active if they were not built as overlays. Your setup works, as ", " was active while ", " was built. But if that was not the case, there is no link between the two and ", "ing one will deactive the other.", "The setup described by the OP was like that.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["there\u2019s no way to get two base workspaces in ", " to overlay via catkin", "rosdep doesn\u2019t know how to resolve inter-distro dependencies.", "there\u2019s no way to get two base workspaces in ", " to overlay via catkin"], "url": "https://discourse.ros.org/t/ros-ourdistro-style-add-on-rosdistro/5205"},
{"title": "Planned downtime for ros_bootstrap repository", "thread_contents": ["2018-09-11 at around 5AM pacific time the ros_bootstrap repository will be offline for 48 hours as we prepare to migrate the underlying host.", ": We are about to perform the DNS changeover to migrate to the new bootstrap repository. The new repository is keeping everything at the same paths so there should be no interruption when your local DNS starts resolving to the new location.", "User facing:", "InRelease files are not currently being generated. We are investigating the issue but as it does not prevent use of the repository we are proceeding with the migration as the existing host has a retirement deadline.", "Packages with ", " now appear in all binary architectures. The underlying repository software now includes packages with ", " in\u2026 all architectures regardless of which ones were specified manually. This has resulted in a few auxiliary packages being included in architectures they were not previously listed in but should not affect the operations otherwise.", "Empty repositories  for ", ", ", ", and ", " have been dropped completely.", "Behind the scenes", "This thread may also be used later in the week for additional migration related downtime. In which case both this top post will be edited and a follow up message will be sent.", "During this time the import_upstream job on ros_buildfarm instances will be unable to reach the default bootstrap repository host causing a temporary failure to import packages. import_upstream jobs targeting other repositories such as the primary ROS repositories will not be affected.", "I\u2019ll send an update when things are back up.", "The bootstrap repository is back online. Thanks everyone.", "We are hoping that the migration itself will be seamless and take place within the next 24-48 hours. I\u2019m doing my best to keep everything in the same place from an apt perspective so that no configuration changes are needed.", "We are about to perform the DNS changeover to migrate to the new bootstrap repository. The new repository is keeping everything at the same paths so there should be no interruption when your local DNS starts resolving to the new location.", "User facing:", "InRelease files are not currently being generated. We are investigating the issue but as it does not prevent use of the repository we are proceeding with the migration as the existing host has a retirement deadline.", "Empty repositories  for ", ", ", ", and ", " have been dropped completely.", "Behind the scenes", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "The new repository is generated by ", " rather than reprepro and hosted via AWS S3.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "The new repository is generated by ", " rather than reprepro and hosted via AWS S3.", "\n"], "url": "https://discourse.ros.org/t/planned-downtime-for-ros-bootstrap-repository/6018"},
{"title": "Changes to buildfarm_deployment for Jenkins JEP-200", "thread_contents": ["For the past several months, the example ", " has included a pinned Jenkins version to prevent using versions that comply with ", " from being used due to issues with the ros_buildfarm scripts and the plugin versions they use. (First reported by ", " in ", ")", "I\u2019ve been working on upgrading the buildfarm deployment and ros_buildfarm scripts to work on post-JEP-200 deployments and am happy to report that we\u2019re now using the latest Jenkins LTS on both ", " and ", ".", "That work is being prepared for merging across three pull requests:", "To allow time for community members using the master branch of buildfarm_deployment to make the transition we won\u2019t be merging these changes right away. The important dates are here:", "For those who aren\u2019t able to schedule the upgrade before 2018-12-17, the ", " branch is a snapshot of the current master which won\u2019t be receiving further updates and moving your existing deployments to this branch will keep you in the current state as long as you need.", "Additionally, I\u2019ve written the guide below based on my experiences updated ", " and ", " with review and testing from some intrepid community members. I\u2019ve done my best to communicate everything I encountered during the upgrade process but I can\u2019t make any guarantees that the guide below will cover every contingency. If you do run into trouble feel free to ", " or ask a question with the ", " tag on ", "The last LTS version prior to JEP-200 was 2.89.4 which has been the pinned version for new deployments since ", ".", "\nThe current Jenkins LTS as of this writing is 2.138.3.", "Due to the way the buildfarm_deployment is put together, it is not recommended to re-run puppet or the reconfigure.bash script on the Jenkins master after initial configuration (see ", ").", "\nBuildfarm deployments wishing to update existing deployments will need to take manual steps on their jenkins hosts to migrate successfully to the latest version of ros_buildfarm (forthcoming).", "Aside from some minor changes to the Jenkins sign in page and other visual updates, most users of ROS Buildfarm deployments will not notice any significant differences.", "\nThe minor issue ", " has been resolved with updates to the GitHub API and GitHub Pull Request Builder plugins.", "\nUsers with Jenkins API tokens may need to recreate them after coordinating operators.", " mandated that a large number of plugins used by the buildfarm be updated,", "\nso I took the opportunity to update all of them and test comprehensively rather than trying to work out only those updates that are necessary.", "Jenkins upstream recommendeds revoking existing \u201cLegacy\u201d API tokens and replacing them with the new token system.", "\nLegacy tokens are not revoked on upgrade and there\u2019s a dedicated UI for reviewing and revoking tokens as you have the opportunity to reprovision them. See ", " for details.", "\nROS Buildfarm aspects which can make use of API tokens:", " ", " ", "Unless you\u2019ve made a reliable backup before you start I cannot recommend proceeding with this guide.", "\nBackups aren\u2019t real until you\u2019ve validated the restore procedure.", "\nThis guide was written after upgrading ", " revised after upgrading ", " and should be used as a starting point.", "\nThere may be other necessary actions for your setup that this guide does not capture.", " These upgrades are only recommended if you\u2019re running a relatively recent Ubuntu 16.04 based buildfarm that was set up after the buildfarm_deployment migration to xenial or has undergone the procedure described in ", "Your buildfarm deployment config is likely a duplicate of the ", " branch from ", ". This repository, like ", ", has a master branch which roughly tracks the production deployments ", " and ", " and updates to the buildfarm_deployment puppet logic may not be compatible with your existing configuration.", "\nSome changes to deployment behavior are also implemented directly via configuration directive.", "\nTo review your buildfarm_deployment_config against the changes made since, I\u2019d suggest creating a local clone and using git-diff to compare the state of your config against the upstream one.", "Before merge of ", "Put jenkins into shutdown mode and let any outstanding jobs complete.", "Take jenkins offline with ", ". If there are many jobs queued, Jenkins may take a while to shut down. Waiting until the jenkins java process stops is recommended.", "Stop jenkins agent on master with ", "(Optional) Run ", ". Review the list of upgraded packages", "Remove any holds placed on jenkins and run ", ".", "If prompted to update the ", " file, you may either reject the changes (what I did) or review them to be integrated with your setup.", "Install updated plugins. I\u2019v prepared ", " which will use bash and curl to install plugins by dropping them in the correct location. You could also modify this script to use the jenkins-cli.jar, or just use it as a list of plugin updates to perform with an online jenkins instance although I experienced filesystem permissions issues when doing so and the resulting state left my Jenkins un-bootable. However, if you check filesystem permissions beforehand to make sure all plugins are owned by ", " rather than ", " it may work for you.", "Remove any .pinned files from your jenkins plugin directory ", ". You may save the list of pinned files if you wish to restore them later.", "Start jenkins, watch the logs for any issues failing to initialize plugins", "From the Jenkins UI, review any administrator warnings.", "(Optional, but recommended) Disable JNLPv2 and JNLPv3 on the Jenkins Global Security Configuration screen.", "Sign into Jenkins as the user your agent hosts authenticate as and generate an API token for use as the ", " copy this token into your buildfarm_deployment_config as the ", " value.", "SSH Credentials can no longer be read from a file on disk due to ", ". Credentials using the key from disk get migrated to \u201cDirectly entered\u201d keys without the key being read from disk. Very early or modified deployments of the buildfarm may have been using an ssh key read from a file. This can be fixed with the following steps", "Reconfigure Jenkins using a compatible branch of ros_buildfarm.", "\nAs of 2018-11-16 the Jenkins LTS work for ros_buildfarm is not yet merged into master or released. The pull request ", " has a branch with some workarounds for plugin issues and template updates for plugin versions in configuration structures.", "Update the Script Approval whitelist. If you have a highly customized ROS buildfarm deployment your scriptApproval.xml whitelist of allowed scripts and Groovy signatures might have diverged. To proceed you have three options:", "\nA. Let jobs fail due to sandbox violations and whitelist them as needed.", "\nB. Manually apply ", " to the scriptApproval.xml file in buildfarm_deployment to your deployment\u2019s ", " and restart Jenkins.", "\nC. Copy the ", " file to your instance replacing your existing ", " and restart Jenkins.", "Download the swarm client jar version 3.14 from ", " and place it in ", " on the master host. Make sure it\u2019s owned by the jenkins-agent user.", "Open ", " and make the following changes:", "Remove the old swarm client jar ", "Start the Jenkins agent ", " and verify that it is able to connect to your master host.", "If you follow this guide, please report back your successes or any problems you encounter.", "While I mentioned that these changes would merge 17 December. They remained unmerged until today as I was busy with the release of ROS 2 Crystal and the first Crystal patch release.", "The jenkins-lts-upgrades branches will remain online for the next week before being removed. Anyone currently using them should be able to switch over to the ", " branch.", "The jenkins-lts-upgrades branches will remain online for the next week before being removed. Anyone currently using them should be able to switch over to the ", " branch.", "These branches have now been removed from GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["2018-12-17 buildfarm_deployment ", " branch will require Jenkins >= 2.138.3 and updated plugins.", "2019-01-17 buildfarm_deployment transitional branch ", " will be removed from ", ".", "Swarm agent client authentication", "\nIf your organization\u2019s clone of the buildfarm_deployment_config repository uses GitHub for authentication, switching to use of Jenkins API tokens from GitHub API tokens will reduce the number of GitHub API calls necessary to manage authentication to your buildfarm deployment.", "\nThe token can be used for ", ".", "~/.buildfarm/jenkins.ini may contain a Jenkins token rather than a password", "\nEspecially if your buildfarm deployment uses GitHub for authentication it\u2019s recommended that operators create and use Jenkins API tokens rather than GitHub API tokens for authenticating to your buildfarm when running ros_buildfarm scripts.", "\nThe GitHub API tokens will require your buildfarm to authenticate the token with GitHub on each request, which can exhaust your Jenkins instance\u2019s API rate limit for large deployments.", "\nUsing a Jenkins API token to avoid a GitHub API request for each authentication is also anecdotally faster, resulting in faster reconfigurations.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Copy the text of your ssh key from the buildfarm_deployment_config master.yaml field ", ". Take care to trip the leading whitespace if you copy it from the YAML block text format.", "Open the Credentials UI and select the global credential matching your ssh username (jenkins-agent by default). Select \u201cUpdate\u201d", "Paste the ssh key text into the text area and select \u201cSave\u201d. No other field should need to be changed.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Update the line for ", " to ", "\n", "Update the line for ", " to match the API token used for step 12 when updating the Jenkins master.", "\n", "\n", "\n", "\n", "Pull the latest version of your buildfarm_deployment_config and run ", " from the repository root.", "Verify that both the ", " and ", " nodes have successfully reconnected."], "url": "https://discourse.ros.org/t/changes-to-buildfarm-deployment-for-jenkins-jep-200/7014"},
{"title": "Irel_blocked-releases-page job on local build farm keeps failing", "thread_contents": ["I\u2019ve got a local build farm that is otherwise working fine; we\u2019re releasing and building packages for Indigo, Kinetic, and Melodic without any problems.", "The one except is that several days ago, the Irel_blocked-releases-page job started failing, and I can\u2019t figure out why.  Here\u2019s the last section of the build log for it:", "The cause seems obvious \u2013 some kind of failure parsing \u201c\\n\u201d, ar_sys: !!python/str\" near the end \u2013 but I have grepped through every file on all of our build farm servers and every file in our build farm repositories and cannot find that string.  I\u2019m guessing it\u2019s being automatically generated by something, but I don\u2019t know what or where to start looking.  For reference, the Krel and Mrel versions of that job are just fine.", "Does anybody have an idea what might be causing that or where I should start looking?", "Thanks in advance!", "Does anybody have an idea what might be causing that or where I should start looking?", "This looks related to the safe_load change from ", ". Did you check the gzipped rosdistro caches? The stacktrace has the rosdistro function get_distribution_caches. It may be that the cache generation is not dumping safe_load-able YAML.", "As a nitpick request, questions like this are probably best asked on ", ".", "The same job is failing on the official farm since Jan 24th: ", "It appears that the hydro rosdistro cache has non safe_load-able YAML entries.", "My inclination would just be to disable the job since we\u2019re not going to modify the hydro distribution cache and at least on the official farm Indigo is approaching its end of support.", "cc ", " for their thoughts.", "Yeah, disabling that job I think makes sense. Indigo is already far ahead of hydro in package count and tracking new releases into it at this point is not a priority. Most of the packages listed by the job have been specifically chosen not to be ported.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/irel-blocked-releases-page-job-on-local-build-farm-keeps-failing/7853"},
{"title": "DNS change for build.ros2.org", "thread_contents": ["The IP address of our Jenkins host for ", " has changed due to configuration changes within our hosting provider. The authoritative DNS records have already been updated but ", " may be unavailable for you while DNS propagates.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/dns-change-for-build-ros2-org/9202"},
{"title": "Jenkin login needed for buildfarm", "thread_contents": ["Did something change on the ROS buildfarm? It now redirects to a Jenkins login when I try accessing the jobs. I think we were previously able to see all jobs without any login, so I\u2019m guessing the maintainers added the login layer for added security. But any way for ROS devs to access the buildfarm now?", "Appreciate any guidance!", " is offline at the moment. Please watch ", " for further updates.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/jenkin-login-needed-for-buildfarm/9306"},
{"title": "Buildfarm Emails", "thread_contents": ["Just found a bunch of emails from the build farm in my spam folder, and am posting this so that other people are aware that they might end up there.", "I noticed that these emails were not signed by ", ", so its plausible that something in the email config needs to be updated to reflect the ", ".", "Thanks for reporting this ", ". We need to make some additional configuration changes so that emails from the new build farm deployment conform to current email origin standards. I\u2019ll try to get them deployed today.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/buildfarm-emails/9372"},
{"title": "New release of the ros_buildfarm package (version 3.0.0)", "thread_contents": ["I have just released version 3.0.0 of the ", " package.", "The most important reason for the major version bump is that it requires the provisioned machines to use a post-", " version of Jenkins. So ", " for your current buildfarm. In order to switch from a previous release (2.x) you must update your provisioned machines according to these ", " instructions.", "You will find a complete list of all the changes in the ", ". I want to highlight a couple items here:", "Happy Building!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Support for the build tool ", " using a configuration option. While the devel / PR jobs on ", " use ", " the same jobs on ", " use ", " instead. This feature has been in use since ROS 2 Crystal.", "Add CI jobs for building and testing workspaces defines by a ", " file. In contrast to devel / PR jobs which operate on a single repository this new job category allows us to provide larger integration builds historically hosted on ", " on ", ". At the moment we use nightly builds for a set of ROS 2 repositories with different configuration to cover different use cases: e.g. only a single RMW impl., all tier 1 RMW impl., debug and release builds, etc."], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-3-0-0/9456"},
{"title": "[build.ros.org] Planned downtime for Thursday", "thread_contents": ["Cross-posting from the ROS status site: ", "We\u2019re changing the volume type of the build farm repository host which requires the host to be offline for a disk snapshot and copy. We\u2019ll use the downtime to perform updates and reboot ", " as well since the build farm can\u2019t run while the rosdistro caches are unavailable.", "The primary ROS repositories at ", " will remain available.", "This will begin after the Kinetic sync.", "As a head\u2019s up to folks who saw it. I did the timezone calculation incorrectly so you may have seen a notification that the maintenance window was starting today. That was incorrect.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/build-ros-org-planned-downtime-for-thursday/9827"},
{"title": "Buildfarm down", "thread_contents": ["According to ", ", ", " has been down quite a while now (getting ", "s).", "Fans or tubes clogged?", "It looks like despite the fact that it went down over the weekend none of our probes triggered to actually put it in an outage state and so I didn\u2019t get any notifications for it.", "I brought Jenkins back up and it went down again a few minutes later\u2026 Normally I check the dmesg log for out-of-memory issues as the Jenkins JVM lasts 2-3months before it needs a restart. But today I was getting segfaults in aufs. The docker version on the build farm master hadn\u2019t been updated since it\u2019s initial deployment so I brought it up to the latest stable version of docker-ce. We\u2019ll see if that resolves the issues or if we need to bring docker down completely and move to the overlay2 storage driver.", "As a (hopefully) finally follow up: There were no clogged fans or tubes, although the notification endpoint that our uptime probe was sending to was not the one our status page was listening on. Which is why the top of the status page said the Build Farm component was Operational while our uptime had been flatlining for hours. That\u2019s been fixed. I also renamed \u201cJenkins Uptime\u201d below to \u201cBuild Farm Uptime\u201d so it is clearer that the uptime and the component are the same thing.", "Thanks for the update Steve.", "I\u2019ve seen some nice segfaults with docker being updated and running containers; those are nice to diagnose and triage.", "I\u2019m guessing this was all partly also caused by the recent security issues that affected ", ".", ": would you already have an ETA for when the redeployed buildfarm is going on-line / becomes available?", "It seems pull request testing via ros-pull-request-builder is still unavailable - or are there any steps that need to be taken by package maintainers to re-enable it?", "Thanks for reporting. There\u2019s some configuration missing to get the pull request builder\u2019s credentials updated. I\u2019ve put it in the queue of follow up work and will hopefully have it updated by the end of the week. I\u2019ll post back here when it should be working again.", "It took a couple of tries to get the updated credential to stick and I forgot to close the loop here. Pull Request jobs should be back online. Please let me know if a repository of yours is not getting PR jobs queued.", "Thanks to ", " for reminding me to post here.", " Awesome, thanks for fixing it!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/buildfarm-down/9125"},
{"title": "Agents unavailable on build.ros.org", "thread_contents": ["I am waiting for some release jobs to run to test a new driver version from ", "\n", "\n", "But the jobs are all pending:", "The build queue seems to be about 300 jobs long.", "I\u2019ve pinged ", " and ", " about this.", "\n", "Thanks for the head\u2019s up. I\u2019ve restarted the agent that was misbehaving and blocking the auto-scaling behavior and added a few more to work through the backlog.", "As a minor bikeshed I think reports like this are best kept in the Buildfarm category rather than release management.", "As a minor bikeshed I think reports like this are best kept in the Buildfarm category rather than release management.", "That\u2019s my fault, bad advice from me ", ".", "I moved it to that category. ", "As a minor bikeshed I think reports like this are best kept in the Buildfarm category rather than release management.", "Thanks! and thanks ", " for moving this to the right category", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/agents-unavailable-on-build-ros-org/10458"},
{"title": "Buildfarm PR testing with non-shallow clone", "thread_contents": ["Currently the PR testing makes a shallow clone of the repository (", "). We work with a set of packages that uses ", " to extract the most recent version tag to set some CMake parameters on configuration (rather than hard-coding the version). With a shallow clone, these tags are missing and thus the CMake configuration fails.", "\nIs there a way to indicate non-shallow clones or work around this?", "Unfortunately there is no way to configure non-shallow clones on the buildfarm. We don\u2019t have such an option since that would result in significant larger bandwidth needs.", "I would like to point out that you approach also doesn\u2019t work if you e.g. share the sources through an archive. I would suggest to come up with a fallback behavior if there is no git tag information available.", "Thank you very much - this is what I expected. We are upstreaming changes to fail gracefully but wondered if there is another option.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/buildfarm-pr-testing-with-non-shallow-clone/11294"},
{"title": "Generating 'dev' and runtime artefacts from ROS packages", "thread_contents": ["First: I\u2019m not entirely sure this belongs here, but I couldn\u2019t think of a better category. It\u2019s probably something that will touch the buildfarm if we can come up with something, hence why I posted it here.", "I\u2019ve been interested in reducing the size of deployments of ROS applications, both native and in Docker (and Singularity) images for some time now, and one thing that I have been curious about is whether it would be possible to generate separate runtime and ", " archives from ROS packages (to use the Debian/Ubuntu terminology).", "Some Googling led me to ", " on ROS Answers, which has an answer by ", " mentioning the support for automatic generation of ", " symbol packages (which has been active for Melodic and newer releases), but couldn\u2019t find additional discussions about the topic.", "Using multi-stage ", "s tremendous savings can be accomplished (going from 4GB+ to 200MB images), but this is non-trivial with ROS packages in the mix.", "Thinking about it, I can see two aspects which may make this either difficult or reduce the gains significantly:", "Pt 1 may not be too problematic. It\u2019s an assumption and would be something to figure out.", "Pt 2 could perhaps be approached very naively: assume everything exported by ", " in a ", " call (in ROS 1) ", " is local to the exporting package is what should go in a ", " package. Assume everything else would go into the runtime package. That information would somehow be transferred to Bloom when it populates the package structure and ", " templates.", "Another option could be to use CMake\u2019s support for ", " in ", " rules (as that could make it work for ROS 2 as well), but I have no idea whether that would help when generating Debian/RPM/something-else packages.", "I also realise that to do this right is probably going to be complex and will require quite some effort.", "This post here on Discourse is to find out whether others have thought about this as well or perhaps even have implemented something in this direction.", "This is definitely something that has been considered at various times. And it\u2019s been thought about recently by ", " who has been looking into generating rpms. As rpms have stricter linters.", "The automation of separating dbg symbols was eventually resolved with it becoming part of the standard debian toolchain:", "From that issue you can find links to various other discussions of adding ", " packages as far back as 2011. But on reflecting on it I think that the reason that we haven\u2019t found a solution for created them is that it doesn\u2019t make sense to \u201cautomatically\u201d split packages. There are many heuristics to say when to put something into the dev version or the runtime component. But in reality there\u2019s many corner cases that will never be met so the maintainer will basically need to list out resources to land in either regular or dev variants of the package. In the Debian world this is the role of the debian package maintainer. Each resource is placed into one or the other bucket. But more importantly the downstream package maintainer then has to add the right dependency on either the runtime or devel package. That downstream maintainer also splits their package into runtime and devel packages. But more importantly they also split the dependencies conditionally on the devel and runtime dependencies appropriately.", "Now you can see that we actually have 4 different cases for one dependency. There\u2019s both devel and runtime versions of both packages. And the dependencies declared on the downstream package have to be conditionally evaluated for the subset of the package that is being used to build or made available.", "We have a rich language of dependencies with at least 5 different versions of dependencies for a package including ", ", ", ", ", ", ", ", ", ". And that\u2019s excluding the extra test and doc dependencies as well as groups etc. We would then have to include a way to declare associations of each of those dependencies to the runtime or devel versions of a package.", "Looking at this from a little bit further away, the main reason for having ", " and ", " packages is because historically ", " was released as a tarball and then the debian maintainers would pick up the tarball, and select what went to each subpackage, and add the appropriate dependency declarations. Since then the process has not evolved much, though now often it\u2019s pulled directly from the source repository. But in general the upstream maintainers are not thinking about the impacts of releasing their software and packaging.", "By comparison in the ROS ecosystem core developers actively use packages as atomic units and leverage those packages even when developing, and they use and pay attention to the package dependencies. The efficiency gains from modular packaging that are usually gained by the debian maintainers work can be leveraged in the development environment too.", "As such I think that it might behoove us not to think of this as a problem to be fixed at the release stage, but as an area of efficiency that we can gain by structuring our upstream packages similiarly. The benefits of separating the build and runtime dependencies helps overall with compile times. Dependency trees do not grow as quickly if they are not the union of build and runtime dependencies. Parallel build tools can build faster when these potentials are paid attention to in the development environment.", "And I think that many of our common practices that the ROS community has developed such as recommending making message only packages gets a lot to this end. They are the higher level equivalent of headers and break apart the dependency trees by providing standard interfaces. And through encouraging small modular packages we get the ability to leverage the find grain dependency control.", "Specifically separating the build and runtime dependencies is something that has been shown to be quite effective that we\u2019re not actively doing. However, instead of trying to do that at release time. I\u2019d suggest that we consider making that a best practice for the ROS maintainers to create full packages that separate concerns into runtime and build time dependencies. This will give the developer the ability to fully specify the dependencies for each and where content goes is also clearly obvious. We could even pick up the naming convention of debian to name the content designed for build time to end with ", " however that would then conflict with the Fedora/RPM convention to name things with ", ".", "In ROS 2 we\u2019ve leveraged this separation of dependency types to enable swapping out the runtime libraries and hide the underlying implementations yet building them all against the same headers by defining a clear interface specification. There\u2019s always a matter of tradeoffs between making things more modular and keeping things as simple as possible.", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "At work I am in a similar situation. We extensively work with docker images and found that ROS certainly doesn\u2019t make it easy to have small docker images. On the larger end they could easily reach 12GiB (although a lot of that was our own negligence). Even a base install of ROS resulted in close to a 800MiB compressed image.", "You\u2019re absolutely correct that there is an inherent limitation at the moment with ROS packages ", " including the dev artifacts and that has given us real trouble in a few areas. One of the things we did actually find however is that nine times out of ten it isn\u2019t actually the ROS packages themselves that are the problem - virtually all of the built debians contain just shared libraries and headers which is only going to be barely larger than a non-dev package. It\u2019s the upstream dependencies that they hold which are consistently problematic.", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", ". Some of the dependencies that get pulled in are just plain useless for actually ", " ROS applications:", "To workaround this problem we came up with a novel solution. Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system. That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", ". The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "A hackier approach would be on the rosdep side. At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "The final approach is as ", "  has suggested, explicitly create \u2018-dev\u2019 ROS packages. The downside of this approach is legacy, there is a substantial amount of legwork to implement this approach and tons of breaking changes to go with it. It also seems overkill since ROS already has most of the mechanisms in place to resolve this on the release side.", "Before installing our ROS dependencies into our docker images we install custom, fake, metapackages. These metapackages are special in that they don\u2019t install anything themselves, they simply \u201cprovide\u201d (in the debian packaging sense) all of the upstream dev dependencies of which ever ROS packages we\u2019re intending to install (plus a subset of ROS packages that we definately don\u2019t need) without actually adding anything to the host system", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "As an example the ", " deb package, which is in any core installation of ROS, has a dependency on libboost-dev. This boost metapackage is ", " and has massive dependencies of its own, installing it on a fresh Ubuntu docker image installs 586MB worth of stuff even with ", " .", "Yes, that was something I quickly ran into as well.", "That way when we later install the ROS dependencies it doesn\u2019t actually install the dev dependencies. It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "The positive news is that solving this specific problem in ROS packaging more generally is probably much easier than implementing ", " packages in their entirety. In theory all that is needed is to produce ", " packages that just have extra ", " dependencies of their own, no extra files.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "To a degree this should be straight forward as package.xml already has a mechanism for declaring these dev only transitive dependencies: ", " . The downside of this approach is that it involves fixing dependencies on basically every package in the ROS ecosystem since that\u2019s mainly used to export headers at the moment.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Thanks for the insight, this is quite a creative way to deal with this (although technically still a work-around ", " ).", "Are these REP-127/140/149 metapackages, or the Debian variant? If the latter, are you using ", " for this?", "Debian variant. When I prototyped it originally I did use ", " although we create them with custom tooling nowadays.", "Could you go into a bit more detail about your approach (haven\u2019t had my coffee yet)? Are you referring to extra ROS packages here which package the dev dependencies?", "No. Ultimately we only care about the debian packages. Certainly one approach to achieve that ", " to have separate dev ros packages but that involves overhauling basically every C++ package in the ROS ecosystem since they all invariably depend on boost in some way.", "So if I understand you correctly you\u2019d like to also use it to export \u201cother\u201d build dependencies?", "Yes, so for example I could say my ", " package has ", " on boost-dev, but ", " on ", ". This would then allow bloom to automatically generate two debian packages:", ": depends on boost-non-dev, contains all of the foo package itself", "\n", " depends on boost-dev and ", "This would only require a fairly procedural update to fix up everyone\u2019s package.xml", "It turns out with this alone the size of a bare ROS docker image went from ~800MiB down to ~100MiB compressed. We\u2019ve similarly extended this strategy to tame OpenCV and CUDA which means even our largest base images max out at about 400MiB.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", "At the moment the ", " rosdep key is declared as being the dev packages for every given OS: ", ". It should be possible to create a \u201cparallel\u201d rosdep file that declares the non-dev versions of packages which can then be automatically used by bloom to generate runtime packages with the non-dev dependencies.", "This is something that came to mind multiple times when trying to reduce the amount of packages being pulled.", "\nWhile very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys. While some stacks like OpenCV and PCL used to have a single/couple deb with everything, they now provide multiple debs and most likely we should leverage them. For some stacks there is already already a narrower set of keys available, the ", " and the ", " one: ", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Nice! Do you have any Dockerfiles you could share publicly that demonstrate this approach?", " The would be great and could be inspiration to reduce the size of the official ROS images that are ginormous.", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president. ", "So in conclusion I think what I\u2019d like to suggest is that we already have support for all the bells and whistles needed to very clearly define the necessary dependencies. And that if we can identify places where separating the build and runtime dependencies would be valuable that we consider simply making separate packages and not try to find ways to automatically split a package and then find ways to refer to those partial packages in downstream packages. We\u2019re generating packages, we have packages. Keeping it one to one is basically the simplest solution.", "This would be a nice way to go about it, but as ", " also wrote, that\u2019s going to be a lot of work (and will result in many more packages as well, which has an adverse affect of build times). For ROS 2 (as ", " suggested) this might be possible still.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "What ", " describes seems like a low-hanging fruit that could allow us to significantly reduce the footprint of ROS 1 (and potentially ROS 2) deployments without requiring too much work.", ": could you provide a little more detail on what you are doing with those metapackages such that we could replicate it? I\u2019m sure ", " would be able to figure out whether we could apply a similar approach to the official ", " and ", " Docker images.", "In an ideal world, all packages would define separately their ", " with the ", " version of the libraries they use and the ", " with the runtime libs they really need to run the package.", "Given the number of packages using (or implicitly relying) on these blanket rules, it would be very challenging to actually apply this to an entire distro.", "Maybe I or someone else can come up with a graphviz/dot script to identify the worst offenders in individual subsets of the packages? I\u2019ll try my luck with ogre/rviz", "There are quite a few tools available for visualising dependency trees (", " and ", " can do it fi).", " indeed brings in approx 580 MB of dependencies (when installed on a bare ", "). ", " is essentially almost singularly responsible for this (207 pkgs for ", " vs 210 pkgs for ", ").", " already lists very specific parts of Boost as dependencies, but as it depends on ", " (as ", " wrote), installing just ", " on a bare ", " image wants to install 655 MB of packages.", "Of this only 8.1 MB is actually placed inside ", ".", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", " already lists very specific parts of Boost as dependencies", "I guess that\u2019s what I meant by \u201crelying on blanket rules\u201d. While roscpp ", "'s and uses specific parts of boost, it doesn\u2019t declare dependencies on any part of it in it\u2019s ", ", relying on lower level packages to provide all the parts of boost it needs.", "Not sure how the proper subset of boost packages appeared in the control file though\u2026", "Edit: ", ": I imagine you have some way of figuring out the runtime dependencies of particular ROS nodes / packages and then generate metapackages based on that information. Have you automated this, is this a manual process, or are you relying on package manifests?", "Definitely tools helping to extract this information will be very useful", "Maybe it\u2019s something that could be encouraged for ROS 2 ? as the number of packages is still pretty limited, so would be auditable. We would need to find a way to discourage / warn users to steer them away from using these blanket rules.", "Hear! Hear! A fresh a proper slate, no time like the present to set the president.", "Yeah getting these split for ROS 2 and allow to install only the nondev version of ROS packages would be great.", "\nLooks like ROS 2 is facing other challenges as the core is completely boost-free and the ", " image ", ", my guess is from the way message packages are currently packaged.", "While very convenient to have these \u201cblanket\u201d rosdep keys, it is very uncommon for any package to need \u201call of boost dev packages\u201d or \u201call opencv dev packages\u201d. A great deal of space would be saved if packages were to use more specific and targeted rosdep keys.", "There is nothing holding us back to do exactly that already. All it takes is a minute to make a PR to update any packages which do use these \u201cblanket\u201d rosdep keys (see below for an example).", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "While dev/non-dev ROS packages would certainly get us further we could already do something today. So please consider to create PRs to replace the usage of \u201cblanket\u201d rosdep keys with something more specific.", "Our Dockerfiles by themselves aren\u2019t all that useful since most of what they do is install fake packages.", "The generation of the fake packages is the interesting part, I\u2019m not sure how much I can share so I\u2019ll describe it in high level terms. The entire process operates entirely within the realms of Debian packaging, nothing ROS specific.", "When we want to build an image we declare its high level debian dependencies, typically this will be a set of ROS packages (e.g. ros-kinetic-ros-core) and some other libraries that we want to use with it. We then have a Python script which takes this list and traverses the dependency graph. As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section. As we prune off packages we add them to a list of packages to \u201cprovide\u201d in the metapackage we\u2019re going to generate.", "Along with this we also explicitly add certain non-dev dependencies to the images. For example we explicitly add ", " of the non-dev boost packages (which turn out to be not that big) to our images to make up for not having the boost-dev meta-package.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Maybe it\u2019s something that could be encouraged for ROS 2 ?", "I don\u2019t see why ROS 1 packages couldn\u2019t be updated too. I doubt the number of packages (which use these \u201cblanket\u201d rosdep keys) and enough people care about is actually that high.", "For ROS 1 however I doubt it is feasible any more (nr of packages, size of maintainer/developer community & sunsetting).", "ROS Noetic is not even released yet and will be supported for 5 years. I would think that is motivation enough to still update package you care about. Also it doesn\u2019t mean that the maintainer of a package has to do it. Anyone can create a pull request with this kind of change - the maintainer only has to accept it and roll a new release.", "And as mentioned above an example how easy it is to improve the dependency size: ", " uses specific rosdep keys (", ", ", ", ", ") instead of ", " (which maps to ", "). The effect on installing ", " (assuming you have ", " installed anyway):", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", ".", "This seems to have happened before (from: ", "):", "Before the ROS package ", " was bringing in the boost dependency transitively. Since ", " now uses the system package of ", " boost is not exposed as a transitive dependency anymore.", "Anyway the packages in ", " should state their direct dependencies explicitly anyway and not rely on transitive dependencies for this. Please see ", " for the proposed fix.", "Technically those downstream packages would need to be fixed of course. I just wasn\u2019t sure that was a feasible thing to do in ROS 1 still.", "That was all really. I\u2019m more than happy to start submitting PRs. But I don\u2019t like breaking things and potentially problematic changes like this have been met with hesitation from OR maintainers in the past, so it\u2019s not too strange for us to first look at approaches that would result in similar gains without causing too much trouble \u201cupstream\u201d.", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", " Same reason here.", "\nIn the above example of ", " not declaring its\u2019 boost dependency. It\u2019s likely that this ", " PR broke ", " in the process.", "\nIn the case where the changes are targeting noetic only, it wouldn\u2019t be too much of an issue as maintainers would update their packages before releasing. But for people not having a dedicated ", " branch it may propagate build failures every time one layer fixes the dep declaration.", "If that\u2019s a road we\u2019re comfortable going down for Noetic, I\u2019m happy to help submitting PRs.", "It\u2019s likely that this ", " PR broke ", " in the process.", "Seeing as ", " hasn\u2019t been released yet for Noetic that isn\u2019t much of a problem right now.", "Noetic would seem like the time to start pruning these package manifests.", "re: replacing blanket ", "s: moving dependencies on keys like ", " to ", " et al. would keep builds working. The runtime dependencies would be much lighter though.", "Perhaps a two-step process could be used: first tighten up the ", "s, then change build dependencies.", "And a related PR for ", ":", "As it traverses the graph it will start to prune off dependencies based off of certain criteria, some of this will be hard coded rules based on package names but mostly it\u2019s automated heuristics. The main one is the one which eliminates packages based on their ", ". While all the ROS packages sit in the ", " section (since bloom doesn\u2019t know any better) the packages in the upstream Ubuntu repos are nicely split up which makes it easy to eliminate all the packages in the ", " section or the ", " section.", "Bloom\u2019s patch support could perhaps be used to change the default ", " to more appropriate values. That would require some work, but would seem doable for at least stuff in ", " and similar metapackages.", "But should we perhaps consider adding support for categories to ROS package manifests? Packages could be placed in ", " by default, but if the author/maintainer provides the metadata, Bloom could use that to populate the ", " field.", "RPM did have \u201cgroups\u201d, but those seem to have been deprecated some time ago.", "Here\u2019s an example of the pruned graph for ros-kinetic-ros-core (nodes are only shown the first time they\u2019re found in the graph):", "Entries without a ", " are pruned then, I gather? And the second column shows the cumulative sub total of the packages installed by (and for) that branch of the tree (but the trees only include a dependency if it wasn\u2019t added somewhere else earlier (so ", " shows up with 23MB, but that\u2019s probably because it depends on all sorts of Python dependencies which aren\u2019t shown a second time))?", "With some primitive scripting and a very manual process I\u2019d arrived at a similar set of packages, but I hadn\u2019t gotten ", " down to ", " MB yet.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "I started part of the process and opened PRs to ", " and its\u2019 dependencies to get rid of the blanket ", " key (in both build and exec). This was pretty straight forward (though not automated) and did not include any audit/tightening of the runtime dependencies. This allowed to propagate the ~300MB space gain to the all of ", ".", "re: \u201cfirst tighten up the ", " s\u201d: Migrating all these packages to format 2 or 3 would also allow us to leverage the ", " versus ", " and tighten them accordingly. Bloom could then use that information to segregate ", " vs actual runtime dependencies.", "And a related PR for ", " :", "Great !", "Reason I was hesitant to start doing this is that we risk breaking downstream packages that may be (implicitly/unknowingly) depending on ", " to bring in ", " .", "Assuming that almost all packages depend on e.g. ", " the only chance to breaking downstream packages is if all packages including and below ", " are switching from ", " to something more specific for an already released distro. If only one package limits the change to Noetic (since it already needs a separate branch for other reasons) downstream packages will still have ", " available in Kinetic / Melodic. Therefore I think the chance for regressions is fairly low.", "Perhaps a two-step process could be used: first tighten up the ", " s, then change build dependencies.", "What would be the advantage of that? The first step already affects downstream packages. The second step is local to the package and is easy to test and doesn\u2019t affect other packages - so why should it not be done at the same time?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS has (very) few leaf packages: many ROS packages export libraries, headers and other resources that get consumed by others as part of a build. This means that when a package is installed, it\u2019s most likely installed because it\u2019s being used by another. Main contributor to package size are the libraries (this is an assumption), which would always be present and thus having a separate runtime and ", " package doesn\u2019t offer much net gain.", "it\u2019s not necessarily something that can be automated: creating proper Debian packages can be ", ". Part of what makes it complex is knowing what should go where, and this is not always obvious either. A single source package could spawn multiple binary packages, and the rules that govern this are typically largely hand written.", "gcc", "perl", "python3", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB", "Before it pulled in ~500MB", "Afterwards it only pulls in ~200MB"], "url": "https://discourse.ros.org/t/generating-dev-and-runtime-artefacts-from-ros-packages/12448"},
{"title": "About the Buildfarm category", "thread_contents": ["This is a category to discuss the ROS buildfarm.", "Topics may relate to both the active instance of the ROS buildfarm: ", " or custom deployments.", "Related repositories include:", "This is a continuation of the discussion on google groups mailing list: ", " Migrated here following our ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/about-the-buildfarm-category/399"}
{"title": "Migration of Buildfarm SIG to Discourse", "thread_contents": ["Hi everyone,", "We have migrated the buildfarm SIG onto a discourse category. ", "Messages to this category, such as this one, will be automatically sent to the old mailing list for backwards compatibility. If you have not already please create an account on ", " with an account you should be able to start a thread by emailing: ", " .", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/migration-of-buildfarm-sig-to-discourse/401"},
{"title": "Big workspace management", "thread_contents": ["Hey, I have a question about how to manage the workspace of a large collection of packages (~50/100). My main question is how to setup this workspace. We have a large number of unreleased packages and a few forked versions of official packages.", "Can I use ", " to recursively check out the source repositories of my custom packages? I tried a forked rosdistro but apparently ", " does not check out dependencies of unreleased packages. Is there an other way?", "Hey Rayman,", "In our ", " we put all source code dependencies for repository in the file ", ". During the build system will pull all those repositories from Git into local workspace and if those repositories have repository.rosinstall it will do it for them as well for several levels. It is using wstool feature ", ".", "So you may reuse this approach or even give a try to the whole \u201cShadow\u2019s Build Tools\u201d solution which is really easy to setup.", "Hi Rayman,", "the information about the dependencies of a package comes from its", "\nmanifest. If your packages refer to names which are not part of the", "\nrosdistro the tool can\u2019t resolve the source repository for them. And for", "\nrepositories which are not released no packages and therefore no manifests", "\nand recursive dependencies are known.", "I would suggest to put all your custom repos (including the forks) in a", "\n", " file (optionally with the distributed approach Andriy", "\nmentioned) and fill the dependencies using ", " from binary packages.", "Thanks for the quick response. I\u2019ll give the distributed solution of ", " a try.", "Just a quick question, the manifests ", " needs are cached in the in the ", " file generated by ", "? I\u2019m trying to figure out how the whole system works.", "Correct, a Jenkins job regenerates the cache every 5 minutes. It makes all", "\nmanifests of released packages available without having to fetch them from", "\neach release repo. This is the data the ", " as well as", "\nthe build farm mainly operate on.", "The cache only contains the manifests of released packages since only for", "\nthem it is clear when they have changed. Basically when the version number", "\nof the release changed. For source repositories it would require to", "\nconstantly check the repository content since it can change without any", "\nchange to the rosdistro files.", "Hi Rayman,", "Presently rosinstall_generator only works for released packages, but there are pending PRs on rosdistro which enable it to work with source repos as well, please check it out if interested: ", "Clearpath is primarily deploying our ROS software now using rosinstall_generator-based bundles; you can see a small prototype of this idea here if it\u2019s of interest: ", "Nice, that patch would really help my use case! I could probably do the following", "I hope I can find time next week to try out your patch.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["define a custom ros distro with our source repositories", "use ", " to resolve dependencies and use wstool to check out all these source repos", "finally use rosdep to resolve the standard ros depdendencies"], "url": "https://discourse.ros.org/t/big-workspace-management/437"},
{"title": "How to completely remove a package from a build farm's status page?", "thread_contents": ["I\u2019ve set up a private build farm that we\u2019re using to build our packages, and so far everything\u2019s working well.  It\u2019s configured to point at and load packages from an internal, private GitLab server that we use, and I\u2019ve been able to bloom >130 of our packages that it has successfully built and pushed to our apt repository.", "We\u2019ve got an internal clone of rosdistro, where we\u2019ve modified the index.yaml file to add a seconds distribution.yaml file to the indigo and kinetic distributions (the only ones we support), and those distribution files have a \u201cswri\u201d tag in them; then, in all of the build.yaml files in our ros_buildfarm_config repository, we have a tag_whitelist that contains \u201cswri\u201d.  So far, so good; our build farm only builds packages that we add to our custom distribution.yaml file and not ones that get merged in to the main distribution.yaml file when we update from the main rosdistro repo.", "There\u2019s one problem: When I first set up the build farm, there was a brief period where I had not added the tag_whitelist yet, and there was an update to the \u201ctornado\u201d package and our build farm decided to build it.  It\u2019s not building any more now that the whitelist is in place, and I\u2019ve manually gone through the filesystem on the master, slave, and repo computers and removed every file I can find referencing \u201ctornado\u201d, but somehow when the indigo_default.html page gets generated on the repo server, and it says that ros-indigo-tornado 4.2.1-1 exists in the \u201cbuilding\u201d repository for Tsource.", "Why is that entry still being added, and how can I remove it?", "Thanks in advance!", "\u2013", "\nP. J. Reed, Senior Research Analyst", "\n(210) 522-6948", "\nIntelligent Vehicle Systems (", ")", "\nApplied Sensing Department (", ")", "\nSouthwest Research Institute (", ")", "The status page shows the content of your apt repository. While obsolete binary packages are being removed in a sync that is not the case for source debs. You will have to remove the source deb from the apt repo manually.", "That\u2019s what I had thought, but as far as I can tell I do not have any reference to it in my repo at all:", "Is there some location where it\u2019s still referenced outside of the repository?", "Can you attach a screenshot of the status page (e.g. with a search string \u201ctornado\u201d to hide your package names). That should show where the package is being found.", "Sure:", "\n", "In addition, running ", " on the repo, slave, and master machines all return no relevant results.  (aside from a Jenkins jar file on the master with 4.2.1 in it)", "You should find a ", " entry in the index of source packages then (in your apt repo under ", "). The entry will then reference the directory of the ", ", ", ", and ", " files.", "If that is not the case I can only speculate that the job is using an outdated apt cache. Maybe the log of the status page job contains something useful.", "I\u2019m surprised that you\u2019re not finding the package with locate.", "To find the packages in the repo", "You can use a query the repo directly like this:", "When doing things on the repo make sure to su to jenkins-slave to not override file permissions.", "As ", " user on your repository the following command should clear the packages after listing all matches on the default repo locations.", "I tried manually editing the contents of the Sources.gz file, but it must be automatically generated by something else, because it just reappeared after the next build.", "Using the remove_packages.py script did the trick, though.  Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-completely-remove-a-package-from-a-build-farms-status-page/911"},
{"title": "Clone depth for PR devel jobs", "thread_contents": ["All,", "I\u2019m having some trouble with getting PR jobs to run correctly. I\u2019ve tried making a simple PR to illustrate my problem. First, here\u2019s the output of our build server:", "The reason it fails is because the merge creates an add/add conflict, even though no conflict exists. Distilling the steps above, you can test the results with these commands:", "To figure out what was going on, I went and looked at the output of some PR jobs on the ROS build server. Here\u2019s the relevant output for ", ":", "The only difference between the two is this line:", "While both have ", " for the initial fetch (", "), but only I have that flag set for fetching the PRs.", "So, armed with this information, I tested it:", "Sure enough, this works just fine.", "So the question is why the PR job is using the ", " switch at all. Looking at the job config, I see this:", "It\u2019s not clear if this is the source of the issue I\u2019m seeing. The relevant build templates are here:", "\n", "So my question is this: why am I seeing that flag? Is Jenkins parsing the configuration differently?", "I looked into the configure page of the referenced job (", "). It only shows the \u201cShallow clone\u201d option which is enabled. But it doesn\u2019t show the \u201cShallow clone depth\u201d option from your screenshot.", "The snippet where that configuration is being specified is ", " (a bit below the snippet you posted).", "That indicates that you are using different versions of the Jenkins plugins. ", " currently uses version 2.4.1 of the Git plugin (see the version nummber in the snippet). The shallow clone depth option was added in version ", ".", "Usually the plugins and their configs are forward compatible. Maybe something needs to be changed to work with the newer version?", "Perhaps. The code that manages the generation of the depth switch is ", ". Interestingly, if I manually set the depth to 2:", "\u2026everything works just fine. I\u2019m not familiar with how the templates map to the Java code, though there appears to be a direct correspondence between the XML tags and the class structure, so perhaps if we add", "to the ", "? Not sure if 2 will always be enough, but then if not, it seems like we\u2019d have to choose an arbitrary depth, in which case we start to lose the benefit of shallow clones/", ".", "After you have manually configured the job you can go back to the configure form and change the url ending in ", " into ", ". That will show the raw job configuration. Please post the relevant block of the xml here.", "I believe that the plugin is attempting to merge by default before running the build. If the pull-request branch is based off of an older commit than the shallow clone, presumably it cannot do the merge without the appropriate history available.  Is the branch that you\u2019re trying to test fast forwardable?", "So to create the PR, I just made a branch of indigo-devel on GH, added a ", ", committed/pushed, and submitted a PR. I can merge it using ", " if I try to merge the branches myself, and it also appears to work (using fast-forward) when I change the ", " switch for the line that fetches PRs in the Jenkins job. I agree that it\u2019s behaving as if the PR branch was generated from something predating the current state of indigo-devel, but it wasn\u2019t. ", "I guess it comes down to choose between \u201cmerge before build\u201d and \u201cshallow clone\u201d:", "Sounds like a good approach. ", "I created a pull request to change the behavior of the PR jobs as discussed:", "Hi!", "\nWhere are all these log statements from?", "I can\u2019t find anything similar if I look on the Jenkins log page.", "If you refer to the messages about the git commands those are part of the console output of each job (at the very beginning).", "Hi,", "Thank you for answering!", "Yes, I mean logs like this one:", "When I look into the log from my Jenkins job, I can\u2019t find this much detail(and I am using git for PR). The only thing in the logs was ", "I\u2019m using the Jenkins Git plugin and the Jenkins Bitbucket plugin.", "Did you look into some other logs or that you enabled some option to have more verbose logging?", "For example ", " has the following output:", "You can find the template for the job configuration here: ", " Maybe that helps you finding the difference.", "  It sounds like you\u2019re debugging a different system. Please ask a question on ", " with enough information to reproduce your problem. This thread is not the appropriate place to debug, it\u2019s a discussion which resolved itself through an enhancement 6 months ago. For guidelines for asking questions please see ", "The logs we\u2019re talking about are in the console logs for the job without any extra options. For example: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["If we use a shallow clone a merge will potentially fail - simply because there is no way to choose the \u201cright\u201d depth which always works.", "So if we want to keep merging before build (which I think is very valuable, that feature was added in March) we have to disable shallow clones. This would only be done for PR jobs since those are the only ones doing \u201cmerge before build\u201d."], "url": "https://discourse.ros.org/t/clone-depth-for-pr-devel-jobs/419"},
{"title": "Error during running the devel job locally: Unable to connect to 54.183.65.232", "thread_contents": ["Guys, I am trying to make a local build to verify some of my changes using the following instruction:", "However the generated script ", " can\u2019t connect to ", " and I see the following error:", "Is the server temporarily unavailable?", "Manually changing this generated IP to what ROS jenkins uses ", " helped to proceed with the task, but still not clear yet for me how this IP was generated when I followed the instruction.", "The examples were using the default branch of the buildfarm config repo. This branch does no longer work since it references a test farm which is not available anymore. I just updated the examples to point to the production branch instead which is used by the official buildfarm and will continue to be available: ", "The README in ", " mentions the different branches but the information is bit \u201chidden\u201d in the text and not easy to discover.", "Thank you. Now it works.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/error-during-running-the-devel-job-locally-unable-to-connect-to-54-183-65-232/1425"},
{"title": "New release of the ros_buildfarm package (version 1.3.0)", "thread_contents": ["I have just released version 1.3.0 of the ", " package. I want to highlight a few items from the ", ":", "The Debian packaging has changed (similar to other packages recently, e.g. ", ", ", ", ", "). The Debian packages ", " only contain the command line scripts (therefore only one of them can be installed). They depend on the ", " package which contains the actual Python modules (thos two are side-by-side installable). This makes it easier to reuse Python code from both Python 2 and Python 3 at the same time. From the user point of view this change is transparent - the ", " package is an implementation detail. For the package provide through ", " nothing has changed. For more information on this please see ", ".", "Happy Building!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["add support for new platforms targeted by ROS Lunar (Ubuntu Yakkety and Zesty, Debian Stretch)", "provide new status page showing packages ready to be released / blocking other releases (e.g. ", ")", "improve output of ", " / ", " jobs to fold sections when being used on Travis", "improve ", " jobs to support external repos without requiring them to be listed in rosdistro"], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-1-3-0/1508"},
{"title": "Debian source package for PySide2", "thread_contents": ["I\u2019m trying to build PySide2 for Raspbian (armhf/ARM6). Source package for PySide2 seems to be missing (", "). Is there any information how the package was built? I also can\u2019t find anything in buildfarm logs.", "The PySide2 related packages are imported from a ", " (see ", " and ", "). If the PPA doesn\u2019t contain them we don\u2019t have them in our repo either. If the PPA has what you are looking for but not the ROS repo please link to the exact file and we can make sure to import it into the ROS repo.", "(Thanks)", "I needed this file: ", " . It\u2019s used by e.g. \u2018apt-src\u2019 to build .deb package locally.", "\nFor some reason, shiboken2 source package has source package imported, but pyside2 not.", " Thank you for identifying the file.", " Can you please update the reprepro rules to import the mentioned file and then run an import with the updated config.", "I\u2019ve ticketed it at:", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/debian-source-package-for-pyside2/1572"},
{"title": "Stop blooming for EOL distros and comment them out from distribution files", "thread_contents": ["Hi everyone,", "As you may have noticed, the ros buildfarm doesn\u2019t [build packages for EOL ubuntu distributions anymore] (", ")  ", "Currently bloom generates control files for each distribution defined in the ", " files from rosdistro.", "In an effort to make releasing packages and maintenance easier, we decided to stop blooming packages for EOL distributions. The original discussion is available ", ".", "\nTo do so we will ", ".", "Before rolling out this change we wanted some feedback, from the members of the community building their own packages, regarding this change. Feel free to post feedback directly on this thread", "The ROS team", "Hi everyone,", "We moved forward and removed the ", ". From now on bloom will not generate files for ubuntu EOL distributions", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/stop-blooming-for-eol-distros-and-comment-them-out-from-distribution-files/1652"},
{"title": "New release of the ros_buildfarm package (version 1.3.2)", "thread_contents": ["I have just released version 1.3.2 of the ", " package. I want to highlight a few items from the ", ":", "Happy Building!", "Can you tell me if ", " still should be able to setup master/slave. Or do you have some insights on the current relationship /buildfarm_deployment <-> /ros_buildfarm.", "Greetings!", "While that repository is used to provision the buildfarm machine it hasn\u2019t been actively used for a long time. Initially we have provisioned the official buildfarm with those scripts but since then we have only updated it but never used the code to provision a new buildfarm from scratch.", "Currently there is some work in progress to update the deployment code to use Xenial (rather than Trusty): ", " The PR is in progress though and I don\u2019t think it is ready to be used as-is at the moment (but hopefully soon).", "There are some (minor) issues with the deployment scripts and puppet files that make them not work oob, but those can be addressed / worked around. I have a fork that at least works for me. You can take a look at it and see if there\u2019s something there you could use.", "The work by Steven to migrate everything to Xenial is shaping up nicely. He\u2019s done a good job of cleaning up the puppet files adding proper modules and refactoring.", "OK thanks I will have a look at it soon. I noticed the workarounds one has to do to get it working but was wondering if I somehow managed to cause them by myself or if it was in inherent problem. Thanks for the quick answers!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["update to latest versions of Jenkins plugins (e.g. Groovy 2.0)", "fix using latest Ubuntu Docker images which don\u2019t have ", " installed anymore", "improve reconfigure jobs performance"], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-1-3-2/1749"},
{"title": "Ros_buildfarm_config role of \"keys\" property", "thread_contents": ["Hi, I am currently deploying a custom buildfarm, while I seem to get more and more grip of the topic I still have a hard time to understand the role of the public key in ", " in ", ".", "\nWhat it is purpose? Is something signed with it? Can you point me to some more information?", "Update: So far I have the conclusion that the keys refers to the public key to verify the packages obtained internally via apt-get. Is this correct?", "The ", " are inside the ", " section. The repositories are the apt repositories that are needed to complete the build. The keys are the GPG public keys to add to apt\u2019s trusted keyring. Without the public key for the repository apt cannot securely download packages from the repositories listed. This is the same as if you add a new apt source to any computer you need to both add the source url and add the gpg key as trusted. ", " in the installation instructions. The only difference is that it\u2019s pulling the key from the key server by fingerprint instead of passing the whole gpg key directly.", "Ok thanks for the clarification!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-buildfarm-config-role-of-keys-property/1895"},
{"title": "Missconfiguration leading to missing package?", "thread_contents": ["Hi, I am still doing my first steps with a custom buildfarm. Now I got stuck during the \u201clocal\u201d build when runing the script generated by \u201cgenerate_devel_script.py\u201d. The code gets checkout out fine and building and testing distro packages like rviz etc works. However with my own package it thows:", "[\u2026]", "\nResolved the dependencies to the following binary packages:", "During handling of the above exception, another exception occurred:", "Traceback (most recent call last):", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 198, in ", "\nrawpkg = self._cache[key]", "\nKeyError: \u2018ros-kinetic-moveit-ros-planning\u2019", "During handling of the above exception, another exception occurred:", "Traceback (most recent call last):", "\nFile \u201c/tmp/ros_buildfarm/scripts/devel/create_devel_task_generator.py\u201d, line 251, in ", "\nmain()", "\nFile \u201c/tmp/ros_buildfarm/scripts/devel/create_devel_task_generator.py\u201d, line 116, in main", "\nget_binary_package_versions(apt_cache, debian_pkg_names))", "\nFile \u201c/tmp/ros_buildfarm/ros_buildfarm/common.py\u201d, line 144, in get_binary_package_versions", "\npkg = apt_cache[debian_pkg_name]", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 200, in ", "\nraise KeyError(\u2018The cache has no package named %r\u2019 % key)", "\nKeyError: \u201c", "\u2019\u201d", "Is the something wrong with my buildfarm configuration or did I make a mistake in my package.xml?", "\nOr can I somehow refresh the cache", "sudo apt-get install ros-kinetic-moveit-ros-planning works fine\u2026", "Some hint on possible causes would be helpful even if it is just a guess.", "I don\u2019t know how you have setup your own buildfarm and what packages you are actually building. But looking at the official build farm and the status of the ", " package: ", "The public repo contains an older release of the package (the blue boxes in the X64 and X32 columns). So if you are using that apt repo you can successfully install the Debian package. But the current version fails to build (red boxes in the X64 and X32 columns) and therefore there is no Debian package available in the building and testing apt repos.", "On the official farm that is caused be the latest OpenCV3 release which fails to build the sourcedeb (see ", "). And that package is a transitive dependency of ", ". So until a new OpenCV3 is being made that problem will stay the same.", "Ok, thanks for the quick answer!", "Seems that I still lack essential understanding of the buildfarm mechanisms I will try to dig depper into it. But so far I understood that:", "Did I get this right so far? The only possible thing is to wait for a fix?", "Could I get my package to green using travis as described in: ", " ?", "Did I get this right so far? The only possible thing is to wait for a fix?", "Could I get my package to green using travis as described in: ", " ?", "It doesn\u2019t matter where you run it. If a package that your package depends on is not successfully building, your devel job cannot install all the dependencies, and thus your package cannot be successfully built.", "You have a few other options:", "Note that we\u2019ve ", " release in the main rosdistro and a rebuild is in progress so if that\u2019s the only issue blocking your tests, the wait for a fix approach is likely to work in the short term.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["liblapack-dev", "ros-kinetic-catkin", "ros-kinetic-joint-state-publisher", "ros-kinetic-moveit-core", "ros-kinetic-moveit-ros-planning", "ros-kinetic-pluginlib", "ros-kinetic-robot-state-publisher", "ros-kinetic-roscpp", "ros-kinetic-rviz", "ros-kinetic-tf-conversions", "ros-kinetic-urdf", "ros-kinetic-xacro", "\nTraceback (most recent call last):", "\nFile \u201c/usr/lib/python3/dist-packages/apt/cache.py\u201d, line 194, in ", "\nreturn self._weakref[key]", "\nFile \u201c/usr/lib/python3.5/weakref.py\u201d, line 131, in ", "\no = self.data", "\nKeyError: \u2018ros-kinetic-moveit-ros-planning\u2019", "If I currently declare in my package a <build_depend> to moveit_ros_planning the devel job will fail since moveit_ros_planning does currently not build successfully.", "Because it does not build successfully its debian does not appear in ", " and ", " and therefor can not be apt inside the devel job.", "The result will be that my package won\u2019t get \u201cgreen light\u201d until moveit is fixed and thus I case I would be part of the official buildfarm it would not get included in the official ros release.", "You can wait for a fix.", "You can try to help fix the issue in the dependency. On your custom buildfarm you can choose to override versions of packages, such as rollback or patching repos too.", "You can find a way to eliminate that dependency and then you won\u2019t be blocked by it."], "url": "https://discourse.ros.org/t/missconfiguration-leading-to-missing-package/1898"},
{"title": "Help with Initial Setup (404 reprepro issue)", "thread_contents": ["I did a straight clone of the default configs (rewriting the server addresses with my instances) and managed to get jenkins functioning for the most part, but I\u2019m getting a 404 issue on reprepro that I also saw here (", "). I checked that it is not authentication related. I clearly see root and jenkins-slave logging into the slave in the auth.log. Also, syslog and puppet.log look clean. The creation of docker images is failing due to the 404 issue since the reprepro repo directory is empty. Do you have any advice on what I should check out next. Do I need to do an initial prerelease to populate the repository?", "Did you run the ", " job? It\u2019s usually the first thing required to bootstrap the repo: ", "Almost any operation on the repo will bootstrap it. But the ", " has no dependencies so we just recommend running that instead of creating a single use job that would setup the repositories.", "Not sure whether this is what caused it, but for my ", " host I had to remove the \u2018second part\u2019 of the ", " value in ", " (", " and in the other places where the key is referenced) or ", " would complain about not being able to find the referenced key.", "This stopped being a problem when I updated my config to use my own key, so I didn\u2019t investigate this any further.", "If you still have it around, you could check your ", " (in ", ") and search for any ", " lines. For me it complained that it couldn\u2019t find the key, even though it was successfully imported in an earlier step.", " ", " thanks for the suggestions,  I\u2019ve been trying to get as far as I can with building locally.  Once I get that working I will test these out. I\u2019ve added https private git repo support to ros_buildfarm, ros_distro, and bloom ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/help-with-initial-setup-404-reprepro-issue/1950"},
{"title": "Ros_buildfarm_config reference design for a repo with packages built on top of kinetic", "thread_contents": ["I am currently working on creating a private build farm that creates around 10 packages. I want to layer these on top of kinetic. Are there any reference designs for this to save me some time digging through the docs.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-buildfarm-config-reference-design-for-a-repo-with-packages-built-on-top-of-kinetic/1958"},
{"title": "Jenkins 2.60.1 and Script Security plugin", "thread_contents": ["I\u2019d like to avoid having to deploy this farm again, so just to make sure: my jenkins \u2018got upgraded\u2019 to 2.60.1, which runs fine after making sure a java 8 jre/jdk is present on the vm(s).", "But just about every job I have now fails with a ", " stating that such-and-so method is not approved yet, which then requires me to approve that specific method, re-run the job, repeat, ad infinitum (it seems).", "Is this expected with Jenkins 2.60.x, and is this karma for trying to upgrade things,or is this \u2018fixable\u2019 and am I missing a setting somewhere?", "Yeah, we\u2019ve done the upgrade and had to click a lot of accepts too.", "With the new security restrictions there\u2019s two levels of approvals. One for approving methods and the other for approving script content. In our testing we are most of the way to having all the methods explicitly scripted.", "Talking with ", " we have a theoretical understanding of how we could use the Jenkins API to automatically whitelist the scripts that are generated, when they are generated. Our useage pattern is not well supported by the tighter security model. So how to do this will take some experimentation.", "So in the short term unfortunately there\u2019s a lot of manual approvals. ", " The good news is that once you\u2019ve gotten through the approvals not too many new ones jump up unless there\u2019s a change in the generators. And in the long term we definitely want to automate it.", "Is this expected with Jenkins 2.60.x, and is this karma for trying to upgrade things,or is this \u2018fixable\u2019 and am I missing a setting somewhere?", "The security advisory and updated versions that introduced the whitelist changes were announced ", ". The ROS 1 buildfarm hasn\u2019t updated as it\u2019s still based on Trusty which lacks Java 8. The buildfarm we\u2019re currently using for the next beta of ROS 2 has not upgraded to 2.60 because we\u2019re focused on getting the release shipped. From scanning the changelog, I don\u2019t believe there\u2019s anything new that\u2019s security related in this LTS release from the changes in April.", "There\u2019s a ", " that the new Xenial based buildfarm masters will start with but as far as I recall manual intervention will still be required for a few jobs. I didn\u2019t document which jobs specifically as thoroughly as I wish I had.", "Since the list of scripts / API to approve only populates incrementally (and some code paths are not utilized often or when no problems are happening) it might be better to start with a full list (copied from the ROS 1 buildfarm): ", "thanks for the replies.", "So in the short term unfortunately there\u2019s a lot of manual approvals. ", " The good news is that once you\u2019ve gotten through the approvals not too many new ones jump up unless there\u2019s a change in the generators. And in the long term we definitely want to automate it.", "argh, that must\u2019ve been a lot of clicking. I was hoping to avoid that.", "The ROS 1 buildfarm [\u2026] on Trusty which lacks Java 8", "True. I deliberately installed an Oracle JDK as they seem to be more performant. That was already a version 8, so no problems with the new Jenkins. Or at least, until jobs started to fail \u2026", "There\u2019s a seed scriptApproval.xml that the new Xenial based buildfarm masters will start with but as far as I recall manual intervention will still be required for a few jobs. I didn\u2019t document which jobs specifically as thoroughly as I wish I had.", "O nice. I\u2019m going to see if that resolves some of the endless clicking.", "Since the list of scripts / API to approve only populates incrementally (and some code paths are not utilized often or when no problems are happening) it might be better to start with a full list (copied from the ROS 1 buildfarm): ", "Is that the same list as ", " linked to?", "Is that the same list as ", " linked to?", "I believe that Dirk\u2019s is slightly more comprehensive as it is based on a recent version of the current ROS 1 buildfarm.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/jenkins-2-60-1-and-script-security-plugin/2094"},
{"title": "New release of the ros_buildfarm package (version 1.4.0)", "thread_contents": ["I have just released version 1.4.0 of the ", " package. I want to highlight a few items from the ", ":", "Happy Building!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["all Docker containers which previously used Trusty are now using Xenial", "add a new nightly job which will automatically retrigger failed jobs (not endlessly but a few times)", "improve performance to generate maintenance jobs", "fix a problem with the wrapper script ", " which overlayed the system package ", "\n"], "url": "https://discourse.ros.org/t/new-release-of-the-ros-buildfarm-package-version-1-4-0/2199"},
{"title": "Looking for professional help to set up Buildfarm", "thread_contents": ["Hi all,", "We\u2019re looking for a professional to help us setting up our ROS build pipeline (setting up of buildfarm, dockerization, training of admin personnel, etc.)", "If you are interested or know someone who could help with this, please drop an email to ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-professional-help-to-set-up-buildfarm/2313"}
]