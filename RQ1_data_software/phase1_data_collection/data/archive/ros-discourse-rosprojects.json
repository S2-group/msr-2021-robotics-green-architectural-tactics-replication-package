[
{"title": "Working with large ROS bag files on Hadoop and Spark", "thread_contents": ["Large amount of sensor and robotic data is produced by the industry at an ever increasing peace. Be it from areas like mobility, perception, smart factory or from development tools through planing, modelling or simulation.", "New effervescent robotic topics of research like self driving cars put pressure to develop new tools and techniques to deal with larger and more complex data sets. Some projects and industry players publicly announced the adoption of ROS as part of their process.", "On the other hand, ", " and ", " Ecosystems are seeing a tremendous adoption for processing and analysing large data in parallel. (The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets)", "ROS command ", " subscribes to topics and writes a bag file with the contents of all messages published on those topics. For performance reasons the messages are written interlaced as they come over the wires, with different frequencies.", " operations can be applied in parallel.  Or more precisely the parallelism requires associativity. (Although concurrency technically is not parallelism it also requires associativity.) Spark provides an unified functional API for processing locally, concurrently or on multiple machines.", "The assumption was that the ROS bag files have to be converted into a more suitable format before they can be processed in parallel with tools like Hadoop or Spark. It turns out that the format is good enough for processing with a distributed file system like HDFS but it happened that nobody has written an Hadoop InputFormat for it.", "So we did it. We took the time and wrote a Hadoop RosbagInputFormat ", " published under Apache 2.0 License.", "RosbagInputFormat is an open source splittable Hadoop InputFormat for the rosbag file format.", "We also prepared a Dockerfile and step-by-step tutorial that you could use to try the concepts presented here:", "We hope that the RosbagInputFormat would be useful for you. It would be great if you give us some feedback.", "Thanks!", "\nAdrian, Jan", "Jan, this seems great. Is it meant to run in the data center or as an on-premise solution? In other words are there any special requirements on the computer HW?", "Did you do any profiling on how much faster can you process data using this spliter + spark vs if you\u2019d just do it sequentially   by playing a bag from an ext4 formatted disk?", "D.", "Both are possible, data center and on-premise. There are no hardware requirements but we recommend a 3 nodes setup to see the benefits of parallelism. Start with 64-128 GB memory, 4 disks and quad-core per node.", "Spark performance really scale out with multiple machines. If there are 3 splits is 3 times faster with 3 workers. etc.", "HI JAN", "\ncan you open rosbaginputformat_2.11-0.9.3.jar  source  on github ros_hadoop project ? i don\u2019t know how it works. 3Q", "The scala source code is in the ", "The jar is packaged by the build script ", "It works similar to many other Hadoop InputFormats.", "Hi Jan,", "Will there be any plans to write up a tutorial for the scala api?  I am able to pull the topic names but it seems a little cumbersome compaired to the python api.", "Thanks", "Hi Jan,", "I was trying to implement \u201cflux-project\u201d from github \u201c", "\u201d. But I am facing a error as \u201cImagePullBackOff\u201d.", "I wanted to know where the image \u201cpod/flux-ros-hadoop-5f8cd4bc67-h5thl\u201d is located.", "Hi Joe,", "Basically an Scala solution without ROS means effort of reimplementing parts of ROS in Scala. Spark might be Scala/Python but ROS is C++/Python so the glue is Python not Scala.", "i have run the programme  on  spark-shell  ,but i got a exception .", "\nscala> fin = sc.newAPIHadoopFile(", "\n|     path =             \u201chdfs://127.0.0.1:9000/user/root/HMB_4.bag\u201d,", "\n|     inputFormatClass = \u201cde.valtech.foss.RosbagMapInputFormat\u201d,", "\n|     keyClass =         \u201corg.apache.hadoop.io.LongWritable\u201d,", "\n|     valueClass =       \u201corg.apache.hadoop.io.MapWritable\u201d,", "\n|     conf =             {\u201cRosbagInputFormat.chunkIdx\u201d:\"/srv/data/HMB_4.bag.idx.bin\"})", "\n:6: error: identifier expected but string literal found.", "\nconf =             {\u201cRosbagInputFormat.chunkIdx\u201d:\"/srv/data/HMB_4.bag.idx.bin\"})", "do you know why ?   Eagerly awaiting your reply\u2026", "Hi Jie,", "In our tutorial we used Python code not Scala. Please use PySpark and Python.", "thank you very much .  i have a other problem,the site : ", "  does not work.  ", "Hi Jan \uff0c", "\ndid you know how to extract the pcd file from rosbag with  hadoop&saprk ? I don\u2019t see this example in the project.", "Hi Jan,", "I am using Azure HDInsight Spark cluster to extract data from rosbag files using RosbagInputFormat. I have followed the readme file. While running the code in pyspark I am getting the following error,", "It is not able to read the idx file from local system.", "Could you please help me with that?", "Thanks,", "\nSayandeep", " Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/working-with-large-ros-bag-files-on-hadoop-and-spark/2314"},
{"title": "ROS@Raspberry Pi + RasPiRobot + HC SR04 + SG90 + car", "thread_contents": ["Hi,", "\nJust a WiP announcement of a project, allowing to control a ROS robotic car from an Android app. The car is powered by a Raspberry Pi, running raspbian + ROS. A RasPiRobot board is used to control the motors, an HC SR04 ultrasonic sensor is used to measure distance and an SG90 step motor to rotate the sonar. I used an existing HC SR04 ROS node project as a basis but converted it to use interrupts from the sonar. A simple SG90 driver has been written from scratch. PWM handling to run the car wheels is new too. The communication with the Android app runs over SSL. It\u2019s still quite raw (especially the Android app, since it\u2019s my first experience of writing one from scratch), but all the functionality is there and working. Both github repositories are referenced from ", "\nRegards", "\nGuennadi", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-raspberry-pi-raspirobot-hc-sr04-sg90-car/8749"},
{"title": "Hybrid cross-compilation for ROS-based projects for Raspberry Pi", "thread_contents": ["Hello,", "I\u2019m new here so I apologize in advance in case I\u2019ve (once again) posted this to the wrong place ", "  I\u2019ve just finished my latest project and thought to share it with the community. It\u2019s a method for hybrid cross compilation of ROS and ROS-based projects for Raspberry pi (and probably other armhf boards running linux).", "I developed it since I wanted to use cross compilation on a ROS project I\u2019m working on, but there seemed to be no complete guide on how to do it, with all info in one place, updated for newer ROS versions. This hybrid approach relies on a docker container to do the muscle work (compilation & linking) while it uses a portion of the target filesystem to resolve the dependencies. The method should be fairly portable, modular and flexible towards different ROS version and flavors.", "So far, it is possible to cross-compile ros_comm and ros_robot (without collada_urdf) and then simply copy the install folder back to target system. What\u2019s more important, it allows one to compile catkin workspaces containing ROS packages, link them against target system libraries and ROS libraries - all on the host system. Afterwards, simply move the install folder back to target system and use it.", "Perhaps it can help someone else as well ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/hybrid-cross-compilation-for-ros-based-projects-for-raspberry-pi/8973"},
{"title": "ROSject of the week: train a DeepQNN to control a cartpole with Gazebo sim, ROS control and OpenAI", "thread_contents": ["In this rosject we show you how to build the whole training system for a cartpole simulated in Gazebo and controlled with ROS using OpenAI framework.", "You can get the full ROS training code, documentation in the form of Jupyter notebook, openai_ros package and cartpole Gazebo simulation here:", "Get your copy of the ROSject cartpole3d_openai_ros by @theconstruct", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosject-of-the-week-train-a-deepqnn-to-control-a-cartpole-with-gazebo-sim-ros-control-and-openai/9089"},
{"title": "Does Anyone Else Want a Search Engine Package?", "thread_contents": ["I am making a ROS package which would fetch information for my robot and I am wondering if anybody else would need something similar. If people do, I\u2019ll try make it useful for everyone and release it.", "So the package has a node which would call APIs to retrieve information on things like the weather, traffic, random trivia, random jokes and fun facts, recipes, news, sports results, etc. etc. - basically anything that a user of a social robot would want to know.", "It isn\u2019t Alexa though, it wouldn\u2019t have anything to do with speech recognition, NLP, intents, etc. -  it would just provide information to the rest of the ROS system (which could include that kind of stuff). The node would probably expose a service and clients would specify the what data it wants in the request and would get a json back in the response. Or maybe it could publish it on a topic?", " ", " If people wanted to give more information, I would love to know:", "Thanks all!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["does anybody else have a need for it?", "how would people like to interact with it (a service, topic, etc)?", "what kind of information do other people need for their robots?"], "url": "https://discourse.ros.org/t/does-anyone-else-want-a-search-engine-package/9094"},
{"title": "flexxros - Python Web GUIs for ROS", "thread_contents": ["I recently needed to write a small web GUI for one my ROS projects. Since I don\u2019t feel comfortable with javascript, html, etc., I instead turned to ", " for this and it turned out to be an excellent choice. To make it simpler to develop GUIs in future projects, I developed a library that makes all of this very easy, called ", " with (somewhat sparse) documentation available ", ".", "\nImportantly, you can still use all the widgets provided by flexx to build more complex GUIs.", "It uses python 3 to define a webserver and client in a single script (see e.g. ", ") and communicates between the two using websockets. I don\u2019t know how performant this setup is compared e.g. to rosbridge but it serves my purposes at least (minimal coding and no html/javascript required).", "I would be happy if people tried it out and provided some early feedback, either through filing an issue or posting here. If anyone would like to contribute, you are more than welcome to submit PRs!", "Best,", "\nNils", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/flexxros-python-web-guis-for-ros/9156"},
{"title": "Flask_ask_ros: a ROS node inside an Amazon Alexa web service", "thread_contents": ["Hi all,", "My team has released a public version of a ROS package that allows for voice control with the Amazon Alexa.", "In summary, it is a rospy node inside a Flask web server which receives requests from a custom Alexa skill. The web server script specifies functions that execute with each request, and those can, in turn, call rospy methods (publishers, services, etc).", "This eliminates the need for having a rosbridge connection between your Alexa web service and the ROS master by basically making the web server itself a ROS node.", "Github: ", "Please try it out and report any issues! Thank you.", "I am trying to get this working, but the Instructions for modifying the skill in the Amazon developers console make no sense at all. There is no configuration tab.", "Great job!! It\u2019s pretty convenience to use.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/flask-ask-ros-a-ros-node-inside-an-amazon-alexa-web-service/4618"},
{"title": "Out of the box app for Rosbot 2.0 Pro", "thread_contents": ["I have just bought a Rosbot 2.0 pro. I am a little surprised at how basic the WebGUI demo app that they offer is. I know this is a research and programming platform, but I imagined there would be a basic app that demonstrates the core capability of ROS on this vehicle.", "I imagined there would be a public (or private) project to demonstrate a basic capability - like:", "I am sure there is more you could do from here, but this base capability in a GUI app would allow the demonstration of any vehicles ability to navigate around.  It also seems like a fairly basic system to develop - for an experienced ROS programmer (Which I am certainly not - and probably never will ever be\u2026) It seems like the WebGUI system would be a good place to start \u2026 just add a few extra functions.", "Does anyone have - or can point me to - such a project?", "Thanks.", "Barry.", "Hi Barry,", "I understand your request for onboard software.", "\nEven if Husarion, the company after ROSbot, does not provide that software, they have a pretty good ", ".", "Additionally, they have started to translate that material into rosjects, so you can execute the code on the web with a click, testing in simulation, and then transfer to the real robot. ", ".", "For your information, the CEO of Husarion, ", ", he will be showing at the ", " next week, ", ". He will provide full code and documentation about how to do it.", "About projects using ROSbot:", "I hope this helps!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["GUI application you can run on a browser/tablet.", "Create a map, manually drive around an area to discover.", "Define a few positions/position/pose in the map. (Drive to then create)", "Save the Map and the positions", "Set a task or mission to go to a taught point.", "\n", ": create a simple navigation behavior: the patrol behavior. This behavior is about making the robot move along an area in a closed loop that repeats at infinitum. This behavior can be useful for detecting intruders, cleaning spaces or looking for out of stocks", "\n", ": Shows how to train a Turtlebot and a ROSbot on the same Reinforcement Learning task", "\n", ": Notebook, code and simulations for the Live Class about how to use an already trained DQNN neural network in your robot"], "url": "https://discourse.ros.org/t/out-of-the-box-app-for-rosbot-2-0-pro/9344"},
{"title": "ROSject of the week: Robotic Snake with Variable Segments, Control and Auto-Collision", "thread_contents": ["This rosject was born from ", " that engulfed our interest of what could be possible done procedurally.", "And here you have it, you can generate a snake in Gazebo, fully controlled with as many segments as you want, just with a single command. ", "ROSject file: ", "hi, is this snake usable as a wire, to simulate e.g. how a servo pulls on a wire-rope? Could it be as small as e.g. 0.45mm diameter ?", "Probably yes. You can try it by modifying the dimensions of the basic link model. Since the snake is built procedurally, once you modify the basic element, the full snake/wire will use that for all the links.", "is it only in TheConstruct usable , or is the code publicly available? Is the stiffness of the links changeble? to simulate different kinds of wire", " contain the source code of the simulation/ROS code/extra libraries/ Jupyter notebooks. So you can check the code, modify it, download it to your local computer or do whatever you want with it (like change the stiffness of the links).", "We create a lot of simulations, documentation and ROS code, and we provide it for free in the form of a rosject. A rosject is just a set of ROS packages with the mentioned content.", "But ", ".", "\nYou can download any rosject to your local machine as a series of ROS packages, that you can compile and launch locally.", "We just think that it is a lot more convenient to work on the cloud than on the local machines. But we let you completely free to decide. You can decide to download the code to your machine. That is great for us!", ".", "\nAnd we will keep insisting how more convenient it is to use it on the cloud instead of locally. Mainly because:", "So go ahead and download the rosject if the week! ", ": To download you just need to open the rosject in the ", " (just click on open), then go to Tools->IDE. On the IDE, do right click on the directory/directories you want to download, and that\u2019s it!", "Enjoy, and let me know if you have any doubt.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["You don\u2019t need to install", "You project will work forever under the same circumstances", "You can share your project with a single click and it will work in the same way to whoever receives the copies.", "You can have a class setup in a minute, all people with different computers running the same code", "You can reproduce the results of other", "You can work with different versions of ROS", "You can review the exam, the demo, the problem of a third person and get his problem reproduced in a minute with the same setup as his.", "\u2026"], "url": "https://discourse.ros.org/t/rosject-of-the-week-robotic-snake-with-variable-segments-control-and-auto-collision/9387"},
{"title": "Message Flow Analysis for ROS Through Tracing", "thread_contents": ["As part of an undergraduate research project, I used Trace Compass, a trace viewer and analysis framework, to leverage existing ROS instrumentation and create an analysis that can draw the path of a message through ROS nodes. It can show how much time a message spent inside queues and callbacks.", "I\u2019ve written a ", " to present my project and give more context on tracing & robotics, explain how everything works, and to show off the resulting analysis.", "Right now it\u2019s mostly a proof-of-concept, but it highlighted a few paths that could be explored in the future.", "Let me know if you have any questions or comments!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/message-flow-analysis-for-ros-through-tracing/9437"},
{"title": "ROSject of the week: Robot Fish Naro from ETH Zurich", "thread_contents": ["This week\u2019s rosject contains the simulation of a ", " developed by ETH Zurich.", "\nIt contains the ", ", so you can start straight testing your navigation control.", "This is a video with the result of just applying a simple sinusoidal signal to the joints:", "The rosject can be found here, containing the full code and some notes: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosject-of-the-week-robot-fish-naro-from-eth-zurich/9669"},
{"title": "EZ-RASSOR - A Space Mining Robot from UCF", "thread_contents": ["Hi everyone!", "I have been working on a team to create a software suite for a new robot called the EZ-RASSOR. Controls are handled through ROS Melodic and we created a mobile application to interface with the robot. Lastly, we created an extensive installation script to assist with getting ROS and our dependencies installed.", "We would love your feedback on our work and will continue improving on this project going forward.", "\n", "Ron", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ez-rassor-a-space-mining-robot-from-ucf/9753"},
{"title": "WebCam Lidar for workshops", "thread_contents": ["Hi,", "I\u2019ve put together a simple package for workshops on how to create a laser scanner from a webcam. Feel free to grab it: ", "nils", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/webcam-lidar-for-workshops/10005"},
{"title": "Android OS for robotics", "thread_contents": ["Hello,", "I am thinking about creating a new project, an Android os variant that includes ROS as an extension to the android framework. Something like Android TV, Android auto or Android things but dedicated to robotics.", "To be clear,  this has nothing to do with smartphones, the idea is a full operating system that will combine Android , google services and ros and that is targeted for boards that supports Android.", "What do you think ? Any thoughts, suggestions or start points?", "I like the the idea and interested in your progress.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/android-os-for-robotics/7686"},
{"title": "Qt-Javascript hybrid applications", "thread_contents": ["Hi all,", "Not a project in itself, but just wanted to share some highlights from a recent adventure with Qt-Javascript hybrid application development for visualisations.", "I\u2019ve always been caught in a tug-of-war between robotics developers wanting something that they can contribute to and slot into their build environment and product teams demanding web applications for cloud services / mobile devices. Never really found a way to win that battle - one need or the other usually gets met, and quite often clumsily, but rarely both.", "Until now. I recently got caught in the middle of such a tug-of-war again and given that it\u2019s been a couple of years, decided to do some research and move myself past the prejudiced \u2018javascript is evil\u2019 stance I\u2019ve always taken. What I found was a good reminder to keep looking at things with fresh eyes!", "So I did a spike test and built some javascript libraries and qt-js hybrid applications for visualising the runtime state of behaviour trees. The results? I now have a package that can comfortably sit inside the developer\u2019s environment (pythonic or ROS2) and one that they are quite likely to contribute to with the very appealing benefit - all of the core functionality is in the javascript libraries and web application which can be very easily migrated to a polished cloud service or mobile device application by the web teams with the skills to do so.", "Hope this is as enlightening for others planning to do some gui development as it was for me. Happy to field questions here.", "This is quite interesting. Thanks for contributing!", "\nHave you seen this: ", "\nThis bot was based of webOS + ROS2 and supports full-screen web applications which behave and work like Native and can integrate with ROS2. Hence there is no need of extra layer of Qt.", "\nCurrently ", " (Open Source Edition) is also supported as a Tier3 Development Platform for ROS2 development. See ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Javascript libraries aren\u2019t all that hard to write", "The javascript world is ludicrously well equipped with an abundance of data visualisation libraries", "Qt5 has QWebEngineView which is now based on Chromium\u2019s Webkit (great for obvious reasons)", "QWebEngineView hides all the details of app hosting, file serving", "Very easy to interact between the Qt application and the web app", "The hybrid application can comfortably be part of a python setuptools package", "Javascript libraries are easy to share and deploy via Qt resource (.qrc) bundles", "\n", " - javascript libraries for rendering behaviour trees with a simple qt-js demo (parse the README for more details on how the qt-js integration works)", "\n", " - a qt-js hybrid viewer that uses the py_trees_js javascript libraries for visualising behaviour trees running in a ROS2 ecosystem"], "url": "https://discourse.ros.org/t/qt-javascript-hybrid-applications/10281"},
{"title": "Distance_map - A package to convert occ_grid to 2D distance map", "thread_contents": ["Introducing ", ", a ROS package to convert ", " & ", " to 2D distance map.", "While there exists several implementations for 3D distance map (a.k.a. distance field) (e.g. in ", ", ", ", ", ") it seemed more complicated than necessary to get anything done in 2D. Especially when it comes to integrating it with ROS navigation stack. For this reason we developed the ", " package(s) that offer,", "This is all available on github (see link above) and we are looking forward to hear your feedback and how you may use this package !", "Cheers.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\u2018conversion\u2019 from ", " to ", "\n", "\u2018conversion\u2019 from ", " to ROS message ", "\n", "exact/near un/signed Euclidean distance map (OpenCV / Dead Reckoning algorithm)", "gradient computation", "Rviz visualization", "a small interactive tool to introspect a distance map."], "url": "https://discourse.ros.org/t/distance-map-a-package-to-convert-occ-grid-to-2d-distance-map/9823"},
{"title": "MyzharBot - Autonomous Tracked robot", "thread_contents": ["The robot in the photo is MyzharBot, a tracked autonomous robot powered by ROS.", "\nMyzharBot is a project that I started many years ago to continue to work on robotics after I left the University Research Community to start to work in industry. Now MyzharBot has reached its fourth version\u2026 and is still in evolving.", "\nWhat I want to do with MyzharBot is to continue to study algorithms of autonomous navigation mainly based on Computer Vision, the research field that I love the most together with Machine Learning and Artificial Intelligence.", "\nThe robot is powered by the powerful ", " that allows to analyze in real time the information coming from the ", " stereo vision sensor.", "\nThe motors are controlled by small motor control board ", " developed by ", ", an Italian non-profit association that aims to promote robotics, of which I am one of the founders.", "\nThe previous version, MyzharBot v3 was one of the first robots to use an Nvidia Jetson board (TK1) documenting this on the web, so Nvidia decided to add me to the \u201cJetson Champ\u201d group and invited me to GTC 2015 conference and to GTC 2016 conference, to demonstrate the use of their boards in their booth during the exposition time slot.", "What I think is that MyzharBot is nothing compared to the amazing robots developed by the major research centers in the world, but I\u2019m really proud of my work since I cannot dedicate my whole time on it, but only a few hours during the week ends (and a lot of nights!!!) and despite of this it works and it is a huge hit every time it is exposed publicly.", "For the fast evolution of the robot during the last few years I must thank Mauro Soligo, Raffaello Bonghi and all the members of Officine Robotiche that collaborated on Electronics, Mechanics and Software developing\u2026 a big thank you also to the Nvidia Embedded group that supported me with their amazing boards and their constant presence in the last 18 months.", "More information on the project are available on its website: ", "PS I\u2019m Walter Lucetti, an old computer engineer born in the far 1977 ", "Hi, nice robot ", "Have you made any comparisons between the the depth image from the ZED sensor and structured light scanners like the kinect or Asus Xtion in semi-well-lit indoor conditions?", "I\u2019m planning to make a 1/3 scale autonomous version of a warehouse truck for Toyota Material Handling during an upcoming summer internship. Right now I\u2019m trying to figure out what sensor to use for loading and unloading operations and the ZED camera seems like it could be a good option ", "Hi samlam,", "if you plan to use your robot exclusively indoor then the Asus Xtion is the", "\nbest choice. I use stereo vision because I want to go outdoor, but the", "\nprecision of a RGB-D sensor \u201ckinect-like\u201d is surely higher.", "The ZED camera is an amazing sensor, Stereolabs wrote a really good SDK", "\nthat allows you to take advantage of stereo vision without having to fight", "\nwith the \u201cstereo calibration process\u201d that is always the real problem of", "\nstereo vision.", "\nThe depth map generated by ZED is really similar to a RGB-D depth map and", "\nfurthermore it is HD (more than 640x480).", "The limitation is that it is however a stereo vision sensor and if your", "\n\"environment\" is not highly texturized the stability of the depth measure", "\nis not really high.", "Take in consideration that I\u2019m planning to add a Depthsense DS325 to the", "\nrobot (I own it), I\u2019d like to use it indoor to generate a better", "\nenvironment map and when the light is not enough to retrieve RGB", "\ninformation and then Depth information by ZED.", "Walt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/myzharbot-autonomous-tracked-robot/192"},
{"title": "Speech Commands", "thread_contents": ["I have a module that enables a certain amount of speech control of a ROS robot, using rosbridge. There is a video at ", "Speech Commands runs in a Chrome browser under Android, Windows,", "\nor Linux on whatever computer or phone you are using to control the robot.", "It is available on GitHub: ", ".", "At present it uses ROS parameters to keep waypoints, Unfortunately these are not persistent.  Surely someone has done a neat way to solve this problem, but I haven\u2019t found it yet.  Suggestions, etc. are welcome.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/speech-commands/218"},
{"title": "Python decorator library for rospy", "thread_contents": ["I have developed a helper library for rospy that uses Python decorators to abstract a bit from publishers and subscribers. You are able to instantiate publishers and subscribers by decorating functions that either return or receive ROS messages. I also wrap the running and instantiating of the ROS node by specifying an entry point for the node that will be called in a separate thread at a given frequency. Also, for more convenience, I provide decorators for classes that use the names of the parameters to automatically lookup parameters from the Parameter Server and instantiate the class with the collected parameters.", "Check it out: ", "\u2013ajw", "That\u2019s pretty cool! ", "Thanks! I was trying to make something that I would use all the time. I based it Flask.", "Are there any suggestions on how to improve roshelper? Or how about better names for the project?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/python-decorator-library-for-rospy/290"},
{"title": "Humanitarian Demining with ROS?", "thread_contents": ["Hi everyone,", "I was wondering if members of the ROS community were aware of any past/ongoing open-source robotics projects around humanitarian demining?", "\ni.e projects with the goal of designing and building a software/hardware architecture for robots (Rovers,UAVs) to perform autonomous or semi-autonomous landmine detection and/or removal.", "It seems like the kind of application that could make great use of the ROS framework and maybe also some open-hardware design, so I am surprised to see relatively few references in Google :", "I was wondering what were the main obstacles to creating such a project :", "As a ROS user with experience in navigation, this is the kind of project that I would be glad to dedicate some spare time to, so I just wanted to start by collecting some general feedback. What are your thoughts?", "Thanks!", "When ROS was first released, I was developing software for Cornell Minesweeper, which was a set of robots devoted to just this problem. Surprisingly, almost 5 years after we shut the team down, the website is still up: ", "We used an early form of the ROS navigation stack, a GPS, an old SICK lidar. We tested out at Ft. Belvior in VA. Our biggest problem, failure on test day. Everything ran great the night before the demo. Then we got out in front of the military, and the robot refused to move. Cornell cut funding after that kind of failure, and we just all went our separate ways. I definitely think it\u2019s a good problem to go after, and I\u2019d be more than happy to share my experience in designing and developing the different Minesweeper robots.", "As far as why you don\u2019t see any calls for remote developing, there were certainly a great number of hardware specific issues, developing drivers/firmware/interfaces etc. Hardware was expensive, ~20k, and that was for just a metal detector, some of the fancier landmine detection methods cost 100k+. Some of this could be alleviated by allowing network access to sensors, or possibly by bagging lots of data if you need signal processing type work.  Testing environment is dependent on the particular sensor type, in our case, we just buried nails in dirt. Lastly, I\u2019d say there is a desire to help with humanitarian causes, at our largest we were a team of just under 30 undergrads. Not the biggest team at Cornell, but a good medium sized team. Smart and enterprising people, some of whom went on to found Kiva Systems.", "Thanks Barrett, and sorry for the late response. That looks like a very interesting project, and sharing some of that experience with the community is a big help for sure.", "\nI am interested in collecting more feedback before actually starting any work because I will have only limited spare time to dedicate to it myself over the next few months.", "\nSince the hardware cost and development seems to be a major issue, I am wondering if such a project could be broken down in sub-projects, abstracting parts of the hardware wherever we can. Maybe something like :", " : defining a software architecture to interface and control some common rover platforms for outdoor navigation. I don\u2019t know how heavy mine sensors are or what power they consume but that would obviously impact the choice of the platform, in addition to the usual list of specs for rovers.", "\nIf available ROS-ready platforms can do the job, we would have to consider whether they are affordable enough for a community of developers starting with no external funding. To interface with much more affordable \u201ccrawler\u201d RC cars, we could try to collaborate here with DIY communities like ArduRover ( ", ").", " : developing ROS drivers to interface with one or several common sensors for mine detection. This area is a big unknown for me, so I would be glad to hear more from users who have experience with such sensors, and maybe start listing some commercially available sensors with their characteristics.", " : outdoor localization, mapping and planning. For development, this part should be able to use a simple model of the mine sensor(s), and a mix of simulations and field tests with more basic platforms, like cheaper RC cars.", "Some suggested ", " would be :", "Whether or not you have experience in the field, if you are interested in such a project, please share your ideas!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Minesweepers competition, 27-30 oct 2016, Egypt : ", "\n", "Humanitarian Robotics and Automation Technology Challenge (HRATC) and the IEEE RAS-SIGHT(Special Interest Group on Humanitarian Technology) : ", "\n", "Clearpath Robotics apparently supports these competitions : ", "\nAnd for the above competitions, I cannot find any organized code repository calling for remote contributors.", "Cost of hardware too high for individual contributors to build their platform?", "Difficulty to reproduce a proper testing environment ?", "General lack of interest for humanitarian robotics?", "\n", "\n", "\n", "\n", "\n", "\n", "Collecting more feedback from the ROS community, investigating collaborations with other communities", "Refining the breakdown of subprojects above", "Listing commercial mine sensors and their characteristics", "Defining typical test scenarios and reproducible test environments. Listing the typical specs of a \u201cdemining rover\u201d", "Listing related online resources"], "url": "https://discourse.ros.org/t/humanitarian-demining-with-ros/259"},
{"title": "RoDI educational robot ROS package", "thread_contents": ["Hello folks,", "I just wanted to share a simple ROS package (", ") I wrote to add ROS support for the ", ".", "RoDI is a wireless, low cost and easy to use educational robot designed by ", ".", "Since RoDI is open hardware and software, I hope this package can be useful for people wanting to learn ROS using a (not simulated) robot that they can even built.", "If you want to know more about RoDI and the rodi_robot ROS package, I wrote a ", " with more details.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rodi-educational-robot-ros-package/594"},
{"title": "Collaborator / Mentor for NASA Space Robotics Challenge", "thread_contents": ["I am exploring competing in the NASA Space Robotics Challenge (SRC) having competed previously in the Sample Return Robotics Centennial Challenge. The SRC is a software only project using a simulation of the R5 / Valkyrie on Mars to complete tasks. There is a qualifying run of 2 tasks that occurs on November 15th. The main competition is June 2017.  The rules are published but it is unclear how much support is provided for working with the R5, e.g. do I have to make it walk or is that basic software provided.", "I have been climbing the ROS learning curve this past year but know that there is a lot more to learn, especially with the R5. I\u2019m retired and can devote a lot of time to this.", "I\u2019m looking first for a mentor who could just help me over the rough spots. Maybe someone who\u2019s worked with the R5 but doesn\u2019t have time for this project. Alternatively, a collaborator or a team who\u2019d be interested in working on this.", "Hi there,", "\nIn my humble opinion the SRC is much more advanced than the Sample Return Robotics Centennial Challenge. Humanoid robots have a wide range of extra limitations you need to cope with as well as the software stack for them are generally more complex.", "I had a bit of experience with the Valkyrie unit in Edinburgh earlier this year and from that experience I have to say it won\u2019t be easy to get things running on the simulator for a beginner. The lowest level you have access to is the ", " interface where controllers can be managed. I am not sure how the SRC will work out regarding walking; the real Valkyrie platforms come with a walking controller supplied by one of NASA\u2019s contractors.", "I can help you get started if you wish but I doubt I\u2019ll have time to contribute too much (working on my PhD).", "Thanks for responding. I\u2019ll keep you in mind for questions. I have a contact from the SRR who is currently working with the Valkyrie and giving me some pointers.", "A lot is going to depend on the software stack provided for running the Valkyrie. Good luck with your PhD.", "Hello Sir, this might be out of topic\u2026but are you the Hackaday author? Great to see you here! ", "Yes, I write for Hackaday. Not right now with the competition taking all my time.", "Dear rmerriam,", "\nwe just saw your message at ", ".", "At The Construct, we love people with such good spirit and initiative as you, and we want to support them.", "We know that it is a little late, but we\u2019ll be happy to give you a hand with your simulation for the SRC. Our company is specialised in web robot simulations using gzweb and gazebo, so we may be able to help you on working in such environment, even provide you the simulation cloud for free.", "Let us know if we can help you.", "Best", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/collaborator-mentor-for-nasa-space-robotics-challenge/439"},
{"title": "Swedish competition projekt, (T.R.A.I.N)", "thread_contents": ["Are there any links for the project or descriptions as to how and what this was designed for in the competition?", "\nJust thinking you might want to add a little more in your first post, as I haven\u2019t heard of this swedish competition.", "We are working on a robot for a new competition in Sweden called SweArc. Here is a link to the rules: ", "We have less than one month left before everything has to be finished, and a lot of work left to do\u2026", "The mechanical construction is mostly done, and will start installing the electronics this week. We are planning to use a kinect and a lidar from a neato for perception and a fairly modern laptop with an nvidia 850M and i5 processor running indigo.", "My plan is to try to get the navigation and people-stack working as soon as possible since theese should be useful for solving some of the tasks.", "I am fairly new to ROS so I would greatly appreciate suggestions for packages and stacks that could be useful for solving the subtasks listed in the pdf linked above ", "There have not really been any good Swedish robot-competitions, so a friend of mine is trying to get one started ", " It is sort of a trial-run\u2026", "We made some videos of the construction work if anyone is interested:", "\n", "And we made a mini-plattform to try and figure out how to control the sevos and wheels for the semi-holonomic base.", "\n", "I tried looking at the base controller of the pr2 for controlling the wheel angles and motor speeds form the twist message, but could not quite figure it out at that time so I wrote my own\u2026", "Not really sure how to calculate the odometry form wheel encoders and servo-angles taking the rotation into account somehow\u2026 My current thinking is to use an mpu9250 for absolute angle, subtract the rotational component from each wheel and take an avrage of all the wheels remaining x and y components. Not sure if that would work or if I\u2019m explaining it well engouh though\u2026 ", "Suggestions are welcome ", "Actually, each velocity component (vx, vy, vtheta) contributes to the measured wheel velocity in a linear fashion (given a fixed wheel orientation). You can formulate this problem as A * v = b, where v is the robot velocity (vx, vy, vtheta) and b are the wheel velocity measurements. Since the problem is usually over-constrained, you have to use SVD or similar methods to obtain a least square solution.", "Hope that helps. Nice robot!", "Thanks ^^", "Not sure that I understand what you mean though. The problem I am trying to solve is mapping back the measured wheel rotational velocities and angles of the weels (which are not fixed right?) to the base_link frame in terms of vx, vy and vtheta and figuring out some sort of least square sulotion that wold account for slippage and similar things. Can you use tf for this somehow? What am I missing?", "You don\u2019t have the wheel position in relation to base_link as part of A * v = b right? Would you not need that information to calculate vtheta for base_link?", "You don\u2019t have the wheel position in relation to base_link as part of A * v = b right? Would you not need that information to calculate vtheta for base_link?", "This information would be part of the A matrix. For example, the 3rd column (corresponding to vtheta) contains the 2D distance of the wheel from base_link, since vtheta * distance / wheel_diameter is the wheel velocity generated by vtheta. The other columns contain sin/cos terms depending on the wheel orientation. Just think about the influence of the single vx, vy and vtheta values on wheel velocity.", "With \u201cfixed orientation\u201d I meant \u201cyou know the orientation\u201d, sorry.", "Hi, just a side note since this thread is regarding robot competition in Sweden. There are a couple of others as well. We\u2019ve been attending the RobotSM (", ") in G\u00f6teborg, and Robot Championship (", ") in Stockholm for the past few years. Both are very popular, and has a lot of visitors and participants.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/swedish-competition-projekt-t-r-a-i-n/181"},
{"title": "Using ROS for simulating autonomous RC cars?", "thread_contents": ["Hi,", "\nI\u2019m a student at University of Oslo, working on robotics and electronics, and also a professional IT consultant and C# programmer for the last 12 years.", "As a hobby I dabble with autonomous RC cars; ", "I have had very basic introduction to ROS in one of the classes in university, but would like to learn more about ROS. More specifically I\u2019d like to use ROS as a simulation environment for simulating and improving the driving algorithms seen in the YouTube video above.", "I\u2019d like to ask you for some advice on what part of ROS I should look into, and if this is a valid platform for doing this kind of stuff. I\u2019ve seen that there is something called Gazebo which can be used together with ROS to run simulations. Would it be possible to recreate something like the racing track from the YouTube video above in Gazebo?", "The cars typically use IR sensors to detect the walls, and perhaps use gyro/accelerometers for motion detection. Is is possible to simulate those kind of sensors in a virtual environment using ROS and Gazebo?", "Are there any other modules etc related to ROS that I should look into?", "Thanks!", "First, welcome to the ROS community ", "!", "\nSecondly, I\u2019d just like to suggest if you have specific questions about ROS, then the Q/A stack-exchange format on ", " may be better sutted. The discourse site here is geared towards open discussions, but as you post is sort of both, I\u2019ll go ahead and share some relevant sources here.", "Personally i\u2019m also interested in the same topic, including aerial RCs. For ground scale models you should check out the work being done at IRIM, Georgia Tech with the project called ", ":", "The AutoRally platform is a high-performance testbed for advanced perception and control research. The robot, developed at Georgia Tech, is integrated with ROS and designed as a self contained system that requires no external sensing or computing. The robot is a robust, cost-effective, and safe platform that opens the space of aggressive autonomous off-road driving to researchers and hobbyists.", "Some friends of mine are the students behind the project, such as Brian Goldfain, Paul Drews, Grady Williams and others. They all heavily use ROS and Gazebo for a lot of their research for this project, and if you check out the ", ", it looks like they\u2019ve published docs on how to build your own, and the code to run it!", "Thanks for that good tip! I\u2019ve briefly seen the AutoRally project before, but I\u2019ll dig some more into it this time. Seems like a very relevant project.", "I\u2019ll especially look into the Gazebo simulations they\u2019ve made, and see if I can do something similar for my project.", "At about 1:30 in this video they have a nice visualization of the path planning. Do you know what they used to make that?", "\n", "I also see that there is something called RViz, but haven\u2019t had a chance to look into what it is yet.", "I\u2019ve pinged some of the authors, so hopefully they\u2019ll chime in here soon.", "\nI\u2019m not sure what visualization tool they used ", ":30, but it looks custom.", " is a really helpful tool to visulize and interact graphically with ROS.", "\nThere are plenty of examples in using RVIZ to visualise candidate trajectories as well:", "\n", ": ", "The autorally project is definitely awesome. I recently read their pdf on how to construct the computer box they use on their car, and learned a ton of tricks on how to make a robust computer system for an outdoor robot. Kudos to those guys for taking the time to write an incredibly detailed guide!", "Hi and thanks for the enthusiasm for the project!", "The Gazebo environment for our AutoRally robot is available as part of the ", " in our GitHub organization that was previously linked. Also on there is some documentation to get you started working with the simulation in the readme and wiki. Please poke around in the code (the most useful for you will probably be autorally/autorally_description and autorally/autorally_gazebo) to see how we have everything setup and to at least hopefully get a little inspiration for your project.", "The visualization you mention in our video was generated offline with custom python scripts using the ROS bags recorded during tests of the MPPI control algorithm. Rviz does have some great visualization tools for some ROS message types we use from time to time.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-ros-for-simulating-autonomous-rc-cars/924"},
{"title": "Autonomous rail-guided robot with manipulator", "thread_contents": ["Hi ROS community!", "I\u2019d like to share a project I\u2019m currently working on for my master thesis, for two reasons: (1) I think it is a very nice project that can be a nice demonstration of how the ROS ecosystem can accelerate the development of autonomous systems and (2) I am using a large number of packages and tools that are out there, and would really like to have a spot to brainstorm with fellow ROS enthusiasts about how to integrate everything in an elegant way. Of course, if I have specific questions, I\u2019ll ask them on ROS Answers, but this seems a better spot for open discussion.", "On the project: I am working on increasing the autonomy of a (prototype) inspection platform. The video below shows an older version of the platform, but the idea stayed pretty much the same for the new version.", "My focus will be on increasing the autonomy; practically this means (amongst other things) integrating the different subsystems we have and implementing planning.", "I\u2019ll be posting some of my ideas/struggles here regularly (iff I have the feeling they are read, of course), and I\u2019d love to hear your input and ideas! If you like the comfort of a digital pub for this kind of dicussion, I frequent the ", " and ", " IRC channels on FreeNode and am always up for a good discussion about your project, or mine. ", "To start off with sharing one of my recent struggles in designing the system: I would like to be able to run simulations of my robotic platform, to be able to test planning capabilities in a later stage. This simulation does not have to give insight in system dynamics like the inertias of the links and the interaction between the drive units and the rail; the main requirement is that I can have a virtual environment to drive around in, and observe using a virtual camera (just as in the rail system). This camera will be used to detect Points of Interest that need inspection by the manipulator.", "Some important assumptions / requirements:", "What I came up with for a design:", "So, now things get interesting.", "So, plenty of food for thought for me. I would love it if I could use just one robot description file, and only one representation of my static environment, but I am not sure if that is possible.", "Any input on these matters is definitely appreciated!", "Some update on my progress! All input is highly appreciated. The video below shows the current state of affairs (turn on Closed Captioning for some explanation): I am able to move the robot over a piece of rail in simulation, while RViz tracks it position. Very happy with it, because for now it does exactly what I need. ", "Some remarks, though:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["In this phase: Assume the environment is static and known. So: we know the shape and location of the rail w.r.t. the different metal parts in the environment.", "In this phase: Assume the position of the robot in the environment (so: it\u2019s location on the rail is known)", "It would be nice if the robot manipulator in the simulation is able to collide with (so: cannot go through) certain other parts, like the steel parts in the environment, the rail and other parts of the robotic platform.", "Of course, the arm should actively prevent hitting stuff.", "I should be able to deduce from a single 2D camera image where a Point of Interest is in 3D (raycasting).", "Use RVIZ for visualizations of the knowledge of the robot. Use Gazebo for the simulation.", "The only difference between the info in RVIZ and in Gazebo is that initially, RVIZ/the robot does not know about the points of interest.", "Use MoveIt! for arm control.", "Since dynamics are not of interest: publish the pose of the robot model from ROS to Gazebo. Don\u2019t really model the platform going over the rails, just set its position on top of the rails. This prevents me from having to model wheels, interaction between wheels and the rails, etc.", "In order to allow some collisions (drive unit with rails) in the Gazebo simulation, I need to use the ", ". This is part of the SDF spec, and not present in URDF.", "To move my arm in simulation, I want to use ", ". This means I need to use a URDF besides the SDF. This also means my manipulator model cannot be static.", "In order to use MoveIt! for collision detection and planning, the static things in the environment need to be in my Planning Scene. Since I also want collision detection with the other parts (drive units, etc.) of my robot, they should all be part of one large URDF, with some kinds of joints between the different parts of the platform. (The alternative would be to add the other parts as collision objects and update their positions in the Planning Scene manually, but that is not an elegant solution at all.)", "If I want to set the pose of the simulated robot through ROS now, it actually means that I can only set the pose of the full robot model and will have to apply control the joints between the different robot parts. I do ", " like that, since this is not actually an actuated joint. I would like to just publish the pose/tf of the different parts/links.", "Back to the planning scene: in the end, I need to perform a raycast to get a 3D location from my camera picture. To be able to do raycasting in the planning scene, I think I need to use octomap. However: adding the fixed environment (rail and tank) is done through adding Collision Objects, since I can use the exact same STLs for my world object in Gazebo. (Of course, I can convert some of my STLs to an octomap and use that one in simulation. I actually tried that but ran into some bugs in the implementation: the octomap was always visualized with its origin equal to the robot base_link frame, not the world frame, whatever I tried. I need more time to actually verify/fix this.)", "The ball joints between the different carts are modelled using three continuous joints and two dummy bodies. This is far from elegant, but MoveIt! and Multi-DOF joints resulted in more problems than I could solve. ", " and I talked about alternative approaches over IRC, and if time allows, I\u2019ll definitely try to fix this.", "Allowing collisions between some parts instead of modeling the carts on the rails was a nice idea, but it didn\u2019t work. Apart from the collision mask troubles, it resulted in messy and unpredictable situations because the dynamics would still be active. So, when one body moves, the other bodies want to move as well and do not stay aligned with the rails nicely. I simplified the collision meshes and Gazebo actually simulates the carts sliding over the rails.", "Collision objects are apparently always remapped to the base link of the robot. So\u2026 if the robot moves, the collision objects move with it. While writing this, I remember that this might be something that I can fix with a virtual floating link. That is a better solution than just republishing the objects, which is a waste of resources and results in jittery motion. This might also make it easier to switch back to using an Octomap, which means I can use raycasting.", "Performance of the simulation is adequate, I think. It is far from real time, I expect the main reason for this is the number of contact points between the carts and the rail. That seems pretty hard to accurately simulate."], "url": "https://discourse.ros.org/t/autonomous-rail-guided-robot-with-manipulator/966"},
{"title": "Robotiq 140mm 2-Finger Adaptive Gripper", "thread_contents": ["I\u2019ve created a description package for the Robotiq 140mm gripper. It is available at ", " under GPLv3. Enjoy.", "Have you considered submitting this to the \u201cofficial\u201d robotiq repo: ", "If you are interested, we would ask that you change the license to BSD or Apache.", "I don\u2019t mind changing the license. What is the procedure for submitting? Use a PR? One commit?", "Yes, a PR against ", " would work.", "A single commit would probably be best: there isn\u2019t much history to retain here, I think.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/robotiq-140mm-2-finger-adaptive-gripper/1193"},
{"title": "New book for ROS Projects : ROS Robotics Projects", "thread_contents": ["Hi all", "My name is ", ", I would like to introduce my 3rd book, ", " which is discussing 14+ latest ROS projects. All projects are using ROS Kinetic.", "The book can be pre-order from now and it is expected to release on April - 2017. If you pre-order the book, you can access the chapters early once it editorially accepted.", "You can pre-order the book from following link", "You can also check the topics discussed in the book from the above website.", "Regards", "\nLentin Joseph", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-book-for-ros-projects-ros-robotics-projects/1229"},
{"title": "Unofficial Anki Cozmo ROS node", "thread_contents": ["Hi, robot lovers", "I created unofficial Anki Cozmo ROS driver using its official SDK for my holiday hobby program.", "\n(It is so sad that we can\u2019t buy Cozmo in Japan even now!! I imported it from US)", "If you have Cozmo already, I\u2019m happy if you try it and give me some feedbacks!", "\nYou need carefully read the ", ", it has some hack to use rospy on python3.5.", "Source and document:", "\n", "cozmo_driver - Anki cozmo ROS driver", "\n", "Video:", "\n", "I\u2019ve already posted to cozmo sdk forum.", "\n", "Official sites:", "\nCozmo: ", "\nCozmo SDK: ", "Wow, cool work ", "Playing with my cozmo right now! Very nice work. I was wondering if there was any way I could get depth info? It\u2019d be nice to have some range sensors! I don\u2019t mind contributing if there\u2019s some access to it somewhere!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/unofficial-anki-cozmo-ros-node/1073"},
{"title": "A \"download&play\" open source robot project(raspberry pi +ros, ready for play)", "thread_contents": ["Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-download-play-open-source-robot-project-raspberry-pi-ros-ready-for-play/1299"},
{"title": "Robotic Humanoid hand", "thread_contents": ["Hi,", "\nI try to build a robotic humanoid hand. I use ", " finger and servos.", "I am working on a little system to measure the force of every wire rope with flexi-force sensors to have a force feedback for the servos. I use ROS on raspberry pis to control the servos and read the flexi-force sensors.", "Also i am searching for a good method to detect the angle of every phalanx with a computer. I mean with a RBGD camera or a digital glove or anything else, perhaps someone knows a good idea.", "\nthanks", "Do you mean for input or servoing feedback?", "In case you want to be able to map the motions of your hand to the robotic hand I would suggest looking at ", "hi,", "\nthanks, i see they seem to have a good api also ", "Yes, I have one and was thinking of tying to use it in combination with an oculus rift for remote operation for a robot-hand I am helping my girlfriend build, if I ever have the time that is\u2026", "\n", "Introducing the Leap Motion Mobile VR Platform \u2013 a suite of hardware and software designed for untethered VR/AR headsets.", "\n", "if you got a leapmotion device, does it detect good all angles of every phalanx of one finger? I mean , could you bend your finger, just a little, and it detects it ? Or is it more rough, just detects if your finger is open or closed ?", "It has good accuracy for most positions and usually detects even small changes in angles. ", "The sensor is not very impressive, but they have a good hand-model that they map the readings to and good filtering software. It works pretty well for hand tracking but not much else\u2026", "cool, thanks for the info", "here is my first prototype of a force tester on wire rope. Its very big, but it works very well. The wire rope is pressing against a flexiforce sensor.", "\n", "and the backside", "\n", "perhaps someone got some good ideas ?", "now its smaller, but still very big", "\n", "l", "Have you considered measuring the current used by the servos or reading the load from a smart-servo like ", " to determine tension?", "yes, i have considered this, but i have not yet looked which boards can provide this reading. I cannot see, how you can read the load from the servo you mentioned, in the document pdf is nothing mentioned of reading the load from the servo ?", "P.S. I bought a leap motion, not much time to look at it right now", "The dynamixel servos use a serial protocol ", "If you want you can probably use whatever board you want and current sensing modules like ", ", or someting similar.", "There is a ros stack for interfacing with dynamixel servos, but I am not sure that the xl-320 is supported\u2026", "\n", "nice info again, thanks i need to test this. my thinking of having a stand-alone tension tester is, that someone could use other methods of actuating the wire-ropes, like perhaps with fluidic muscles or linear motors or, or , or.", "But a try with these sensors is it worth anyway, thanks", "i made it smaller, it could get smaller, but it still produces good responses", "\n", "i changed a little bit and printed 8 of them. Here the full setup.", "\n", "Now i need to connect the flexi-force sensors. After that, and with the information the leap motion device should deliver me about this finger, i think i got enough information to build a reinforcement neural network, so that the finger could learn by themself how it should be moved to close or open a hand.", "\nBut first, there is a little bit of work", "thats prototyping, i did a new one. Combined servo holder and force-tester into one small piece.", "\n", "With this setup i reduced the selfmade things, like alu-pipe for winding and alu-connector to servo. The force tester still needs some metal/alu stuff, but this is not complex, time consuming. Now i need 8", "Again, i made it smaller. Now its not getting much smaller with the flexi-force sensor. I made a picture which shows that only a minimum of selfmade stuff is needed for this combined servo holder with force/tension sensor, and it works pretty well.", "\n", "Wow, i like this setup. Its easy to handle (even if it perhaps doesnt look so) and works pretty good. And it is relativ small. I printed a ring, so i can easily plug every single combined-servo-holder into a hole of this ring.", "\n", "Which then looks from bottom side, a little bit like the ATLAS experiment of the LHC (Large Hadron Collider) ", "\n", "And fully assembled, ok without the wires to the flexi-force sensors, it looks so:", "\n", "Where you can see, that it has a very small base", "i want that the computer knows in which position all phalanx are. ", " : i tried the leap motion device, but it seemed not well supported at all and it looks like it not recognizes the fingers so well as described in their info.", "But i got a xtion pro and a kinect one (V2), the kinect one i got running with iai_kinect2. But now i dont find a way to extract one object and then get the coordinates of it, then mapping it to an URDF model ? Perhaps someone could help me", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/robotic-humanoid-hand/188"},
{"title": "STDR Simulator needs support!", "thread_contents": ["Dear all,", "As the team behind the development of the STDR Simulator (", "), we are really happy to have seen our work be used by both robotics researchers and in the education in several projects and University theses. This was made possible due to the full STDR-ROS compatibility, ease of utilization and simplicity, since anyone can setup an experiment in a few minutes.", "Unfortunately, the last months we are struggling to support incoming issues, feature and pull requests, as our free time is quite limited due to personal and professional-related responsibilities. Thus we cry for assistance in the ROS community, as we strongly believe that this project must not die, concerning its usefulness till now. We propose to progressively transfer first the development process and secondly the maintaining procedure to any team that desires to continue our work. Specifically, at first we will assist this team towards merging or not submitted code, as we will try to pass on the aspired workflow and concept of the tool.", "In case any team (or individual) wants to uptake the task of continuing one of few full ROS compatible 2D simulators out there, please let us know by replying to this thread: ", ".", "Best,", "\nThe STDR team", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/stdr-simulator-needs-support/1401"},
{"title": "Python pocketsphinx speech recognition (GStreamer free)", "thread_contents": ["Hi. I come from speech recognition community, and only start experimenting with ROS.", "I found several examples of ROS voice control using pocketsphinx. However, they seem a little too complicated, out-dated and also require GStreamer dependency.", "I prepared ", " using the latest pocketsphinx-python release. It works in keyword spotting mode, which means better filtering of out-of-vocabulary words.", "I debugged it only with turtlebot world simulator, which works fine. However, I did not have a chance to try it on anything else.", "The questions are:", "Thanks for the suggestions if any", "Hi Arseniy,", "I am workiing with Ubiquity Robotics, and last year I wrote a speech", "\ncontrol module for these robots, which use ROS.  Originally I tried to use", "\npocketsphinx, but I couldn\u2019t make it work well (surely my fault), and when", "\nI learned about the web speech api I decided to go that way.  Also, I", "\nrealized that putting the recognition in the robot was a bad idea because", "\nit could easily go out of earshot.", "So my stuff is written in Javascript, runs in a browser on notebook or", "\nphone.  It does recognition using web speech and transmits commands to the", "\nrobot using rosbridge ", "(see", "\n", "), over wifi.", "This works pretty well, as long as there is a good internet connection and", "\ngood wifi between the controlling station and the robot.  Pocketsphinx", "\ncould provide local recognition and so make the internet connection", "\nunnecessary, if the local wifi is reliable and the distances permit.", "The only problem I see with command spotting mode is in setting up", "\nwaypoints, which (at least in my implementation requires the use of", "\nunfamiliar words.", "Here is a demo. ", "and github ", "I\u2019ll be happy to talk shop anytime.", "Joe Landau", "\n", "That would definitely be useful as there are already a few efforts to deal with speech recognition (like ", ").", "\nI would suggest you try to find other packages on the wiki or ROS answers (e.g. ", ") and try to see if you could come up with a common message. That would greatly help speech recognition efforts.", "\nFor pocket sphinx specifically, check with the authors of the other packages you have found and try to see if they would be ok to merge with yours (they might not be on this mailing list so pinging them right up could be nice).", "\nAnd yes there is interest! Robots need to interact with humans!", "Thank you for the suggestions and for the links.", "As for pocketsphinx, I tried to search for the existing projects. ", " seems to be no more maintained according to commit and PR activity.", "Since that time pocketsphinx community came up with some nice features, super simplified interface and minimum dependencies, better accuracy and many more languages supported. So I was feeling it should be somehow spread out. One concern is that my expertise is enough to only provide a simple working example, while I have no idea how this should be integrated in real robots / wether it will have conflicts with other versions of ROS, interfaces, etc.", "You are right, I\u2019ll try to reach the authors of other similar projects, too.", "\nThanks again!", "This is pretty cool! I agree with Vincent that robot human interaction is important, but unfortunately mostly ignored.", "Since it seems that you want to use this project as an opportunity for learning ROS, there are a few things you could do to become for situated with ROS, also while integrating your code better. Ordered roughly in order of difficulty.", "This list is merely some suggestions for using this project to lean. Anyway, this is list is already getting quite a ways up on the ROS learning curve, and will put you well on the way to becoming comfortable with ROS. If you run into any issues, I, and I\u2019m sure others in the ROS community will be glad to help.", "Rohan", "Great input! I\u2019ll use your project as an example then.", "Agree that using ASR onboard is hard. Noise and far-field signal distortions are still challenging unless you have microphone array and some fancy processing.", "Adding new words in keyword spotting mode is actually quite easy and can be done on the fly. But yes, for good performance we are limited with a few dozens of phrases\u2026", "Again thanks a lot. This was helpful.", "Ah great, I definitely have to study this. This was something I actually removed from the old pocketsphinx gstreamer project.", "Thank you, will modify the code soon", "Just for reference, here is an implementation / wrapper for the Sphinx4 Java library: ", "Unfortunately it\u2019s a bit complex to install just this package without the entire RAPP Platform infrastructure, but I think it\u2019s worth a look.", "We have a live instantiation of RAPP Platform if you want to test the call with a Python API. ", " and ", "Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["could it be useful?", "if yes, what is the proper way to integrate it?", "Put your node in a catkin package, this allows it be be run/launched with rosrun and roslaunch, as well as depend on your node from their own packages.", "Write a demo roslaunch file, which could for example run both the simulator and your node", "Use ROS parameters instead of command line args, this way your node can be configured the same ways as other nodes", "You could also try separating out detection of words and the actions in separate nodes. The speech recognition node can be given a dictionary at start and publish ", " to a node that moves the robot based on the commands. This way people can swap out parts, such as using Web-based speech instead of pocketsphinx, or using a natural language processing node instead of a simple dictionary one"], "url": "https://discourse.ros.org/t/python-pocketsphinx-speech-recognition-gstreamer-free/1405"},
{"title": "Master thesis subject to help ROS", "thread_contents": ["Hey,", "I will be starting my Master thesis next month, and having a lot of possible topics. I really would love to know what is it that the ROS needs the most, as an OS for robots. That I could work on as master thesis, knowing that my masters will be in AI and Robotics.", "\nI would love any suggestions and all help is appreciated.", "I\u2019m in a similar position i\u2019m starting a Masters project to create an autonomous drone to map out buildings using slam. As I\u2019m just starting the technology review now i\u2019m finding that this project has been done at loads of institutions before and I can run the simulations in Gazebo very easiliy. What aspect of ROS did you decide to work on for your masters project?? Are you working on something that has already been done before??", "Cheers", "\nPilk", "I am also a masters student doing work on creating things for ROS. I am working on the UI side of things. Seems like that is an area that is really lacking, especially web based systems. Does anyone else agree with this observation?", "I am also a masters student doing work on creating things for ROS. I am working on the UI side of things. Seems like that is an area that is really lacking, especially web based systems. Does anyone else agree with this observation?", "Yes, I would love a web-based RViz, rqt and other such tools fi. Not sure how suitable something like that is for a MSc (or if I even understood you correctly), but it would be very welcome (possibly connect with web-based gazebo, rwt, roslibjs, etc, etc).", "I was thinking more about thing developed to be used for your master\u2019s work. That what most of my stuff is. I am creating an autonomous agriculture rover. Most of my work is configuration and testing of already built tools. This has lead me to create new tools to reduce the complexity of developing rovers.", "I just started this system a few weeks ago. It needs a lot more work but it is a start.", "jack-ui - :spades: web app for ROS field robots", "We have integrated rviz, rqt and the other graphical tools on the web. A video demo ", ".", "You can use them, together with simulations, to develop ROS code, test and else here: ", "We have integrated rviz, rqt and the other graphical tools on the web. A video demo ", ".", "You can use them, together with simulations, to develop ROS code, test and else here: ", "Is that a \u2018terminal service\u2019 / ICA like web client? I\u2019ve used those myself, what are your experiences with touch screen devices with that?", "It\u2019s a nice start, but an actual (HTML5, canvas, webgl, etc) implementation would be even nicer ", "Thanks for your responses.", "When I initially thought about choosing to create a robot for my masters project I was quite enthusiastic about creating my own visualization software until I saw what was already out there and it sort of blew the winds from my sails. Thanks for pointing this out for me, im quite eager to learn about web sockets and webgl and creating a web-based interface for ROS sounds appealing to me.", "However, I\u2019ve only got three months to work on a self contained project. What kind of features would you like to see in a web client? You mentioned touch screen compatibility\u2026", " I\u2019ve checked out your work, looks like a good start to something and its given me some ideas gl ", "Feel free to fork it. I will be update the readme with more details on what all I want to do. Been busy with some ekf problems on my robot.", "Hi,", "I have been working for quite some time on a REST interface for ROS systems (based on python) : ", ".", "\nVery useful for introspection into running multiprocess systems and debugging. Better client code for visualisation would sure be a big help, but I am currently focusing on a more critical part : getting the web server interfacing with ROS right\u2026", "Since a big part lacking in ROS is documentation/support/tutorials for developing reactive systems, I have been thinking of investigating using ", " in a web cient to offer a graphical view of a running ROS systems. I didnt get around doing it yet\u2026 If it s something that interests you, it can be quite self contained I think\u2026  And I think it would be an interesting experience for the people interested in web system development for ROS. There also, the tricky part will probably be interfacing with ROS.", "You could remake the usual ROS turtle simulation tutorial, but web based, just for fun ", " Or remake some QT tools, or of course just pick you own idea\u2026 Feel free to ping me if you want more info about any of these.", "The most significant problems (aka ", ") off the top of my mind are:", "The common GUI tools are decent but they could be better. For example, after loading files with the Moveit Setup Assistant, you can\u2019t reload a new set of files. You have to close the program and begin again.", "Aligning components in a URDF is incredibly tedious. The ROS-I CAD-TO-ROS project started to address this issue, but it has pretty much stalled. It would be great to have some Solidworks-type \u201cmating\u201d functionality. Huge opportunity here.", "How to easily and accurately locate a camera in a tf frame? (Maybe there\u2019s a package to help with that. I will do some more searching.)", "Thanks again for the responses, since there is a group of us working on an autonomous building mapping robot I have decided to work on a custom web-based front-end for supervising it (from inspiration from you guys).", " I will check out your work at some point i\u2019m heavy into report writing at the moment, it would be interesting to develop a graphical view into the ROS system as it is running in browser and l see how far i get with that.", "I think all of you who want to contribute some of your masters effort to ROS are great!", "On the web dev side, certainly look at: ", "And this too (", "):", "\n", "The problem I have with ros-control-center is a lack of UI UX purpose. How can this be used for robotics operation? How can the user see the status of important topics at one time? How can you manage users and multiple robots? What about data collections and logging? Is there security in the connection? How does this scale for deployment? etc. It does a great job demonstrating the possibilities of roslibjs. I used it to learn how the library works.", "I agree, its a tool for inspecting but it looks like a tool for configuring your router, not exactly user friendly. Have you tried the RMS (Robot Management System) yet??  what do you think about that?", "Yes. I am using their library (roslibjs) to communicate with ROS.", "Glad to say that one of the chapters in my upcoming book is about ROS Web applications.", "I think this book will be a guide for doing ROS based Robotics Projects", "Let me know if you want any contributing content to your book!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/master-thesis-subject-to-help-ros/443"},
{"title": "Tricycle Plugin Fixed!", "thread_contents": ["I\u2019ve rewritten the tricycle plugin so that it now produces good navigation using rqt commands in Gazebo, with good correspondence to maps produced in rviz using rtabmap. You can download my code to experiment with ", " ", "My inspiration came from the Instantaneous Centre of Rotation discussion ", ".", "Input is greatly appreciated! Let me know if there are other files you need in order to play around with it.", "ONGOING ISSUES:", "Motion looks good in simulations, but I worry that this is producing the troubles I\u2019ve experienced reaching 2D nav goals in rviz (aborts goals, tries to \u201cback in\u201d to them).", "It could be a deeper problem with the teb_local_planner not really being designed for Ackerman robots, but I definitely don\u2019t see it helping if the robot wants to \u201cback in\u201d to all sent goals.", "From what I can tell, the conversion from angular velocity commands to steering angle that is built into the teb_local_planner is incorrect when applied to Ackerman robots: it ought to be angle = asin(omega / v * wheelbase), as opposed to angle = atan(omega / v * wheelbase).", "This change would move the point of interest for angular velocity commands from the midpoint of the rear wheels (as in a diff drive robot), to the point of contact with the ground of the steering wheel.", "Could this be because the odom updates (for the steering wheel location/orientation) are being defined by reference to the \u201cspecial point\u201d between the rear wheels (see line 401)?", "Also possibly related is an apparent (systematic) mismatch between the twist message in odom and the cmd_vel values? These values match when using the differential drive plugin, but does the difference in kinematics for an Ackerman robot make a mismatch here totally normal?", "EDIT: BIG ISSUE (EDIT2: NEVERMIND, I\u2019M AN IDIOT):", "Example: the target angle perfectly matches the rqt steering angle as you increase it from 0 to, say, PI/3. But then if you try to lessen the rqt steering angle to PI/4, it will actually INCREASE to PI/2. If you then lessen the rqt steering angle to -PI/4, it will perfectly match that. But then, increasing the rqt steering angle to -PI/6 gives you -PI/3.", "What in the world could this be coming from?!?", "Hi, the box link is broken, could you re-uplaod the code?? I\u2019m highly interested in it. Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["I\u2019m concerned that my sign conventions for x and y seem to be reversed (compared to what I imagined them to be) in the code (see line 357 onward).", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "Possibly related is the fact the velocities in odom seem to be given in the global frame. With the differential drive plugin, you only ever see linear x velocity being non-zero (linear y velocity = 0 always).", "\n", "\n", "\n", "\n", "Another concern is drift. Errors accumulate when switching between hard clockwise and counterclockwise turns. There may be an issue with how I\u2019ve handled setting the steering angle to avoid jittering around angle = 0 (see line 288).", "Alternatively, it might be a problem with the friction in my model?", "I\u2019ve noticed that when changing steering directions, the target/applied angle suddenly doubles! The effect is zeroed out whenever an rqt command steering angle of 0 is sent.", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/tricycle-plugin-fixed/1044"},
{"title": "Looking for any ROS 'Ballbot' projects", "thread_contents": ["Hello ROS Users,", "\nI\u2019m looking for any ROS based \u2018Ballbot\u2019 projects that may be out there already. Our team is thinking of doing one but first I wanted to see if there were already any in existence we might leverage.", "Please reply if you know of any!", "Thanks,", "\nMatt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/looking-for-any-ros-ballbot-projects/1509"},
{"title": "A 3D printed 6 axis robotic arm powered by ROS - Niryo One", "thread_contents": ["Hi everyone !", "I want to introduce to you a project I\u2019ve worked on by co-funding the startup Niryo. We have built a 3D printed 6 axis robotic arm powered by ROS, in order to bring accessible robots for everyone.", "We use a Raspberry Pi 3 board inside the robot, with Xubuntu for RPI and ROS kinetic. Some packages we are using : Moveit!, rosbridge, joy, \u2026 The Raspberry Pi 3 mainly runs ROS, communicates with a microcontroller (Arduino) to control the motors, and communicates with the outside through rosbridge.", "For ROS community, we have thought that it would be nice to learn ROS, or test industrial use cases directly on a real robot, for less than 1,000$. That way many people can have an industrial-like robot at home, and schools/universities can get low-cost robots for education/research purposes.", "We\u2019ve just launched a ", ". I invite you to visit this page to get more info about how we use ROS (there is a complete part about it if you scroll a little bit) and what are our motivation.", "All the ROS stack + firmware + STL files will be released on github on September (which is also the Kickstarter shipment date). We will also provide a complete set of written/video tutorials to learn ROS with Niryo One.", "I am introducing this project to you to get some feedback from ROS users, so we can make the robot better. And if you wish to support us, that would be even greater !", "We also have a blog post on our website explaining how we use ROS on Niryo One, ", ".", "Feel free to ask any question, or make any suggestion ! Thank you.", "Wow, really nice job guys. Good luck! ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-3d-printed-6-axis-robotic-arm-powered-by-ros-niryo-one/1535"},
{"title": "Any interest in standard ml for ROS?", "thread_contents": ["In my programming languages class, I have been working in standard ml recently. While I\u2019m not a huge fan of the language, I think that there are some really nice bits of syntax that might make it interesting for ROS use.", "Is anybody using ML in their work/interested in the slightest in having an ML bridge for ROS?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/any-interest-in-standard-ml-for-ros/1634"},
{"title": "Using Reinforcement Learning to Perform Motion Planning for a YuMi Robot", "thread_contents": ["I worked on a personal project over the last two weeks to teach myself reinforcement learning. Here\u2019s a write-up on my work - ", "Very interesting work!", "I did not really go into the code, so these questions might be trivial:", "In the published iteration, I don\u2019t use MoveIt! at all. Details of why I phased it out are in the blog.", "\nIt\u2019s not my algorithm (i wish it was !); but yes it could be an alternative to RRT.", "\nObstacle avoidance will be somewhat involved because the state has to incorporate information about occupied voxels and the reward function has to be modified to include a cost for distance to collision objects.", "Nice!! I shall try and implement it myself,\u2026 somehow.", "Was a bit of a surprise to see an old ROSAnswers post when I read through the article ;). Never did get the Fanuc running, much to the benefit of health and safety.", "Nice work! Something similar your stuff is also implemented by OpenAI for connecting OpenAI Gym and ROS. You can find ye package here: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["What part of the MoveIt! framework do you still use? You write that you bypass it by interacting with joint states directly. Could your algorithm be an alternative for the RRT algorithm, or one of the other planning algorithms?", "Would it be easy to extend this to obstacle avoidance as well?"], "url": "https://discourse.ros.org/t/using-reinforcement-learning-to-perform-motion-planning-for-a-yumi-robot/1030"},
{"title": "Andruino R2 (DIY ROS Robot low cost)", "thread_contents": ["This work presents the design of an open educational low-cost (about 35 euros, celular phone not included) modular and extendable mobile ROS robot based on Android and Arduino, with Local Area Network (LAN) and Internet connection capabilities, to be used as an educational tool in labs and classrooms of information and communications technology (ICT) vocational training, or in engineering courses, as well as in e-learning or massive open online courses (MOOC) as an alternative or complementary to virtual labs. It is a first step introducing what we call \u201dBYOR: Bring Your Own Robot\u201d education policy equivalent to \u201dBYOD: Bring your own devices\u201d in computers\u2019 world. Use iot cloud and deep learning capabilities", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/andruino-r2-diy-ros-robot-low-cost/1675"},
{"title": "Our General Purpose Service Robot S.A.R.A", "thread_contents": ["So let me present to you our General Purpose Service Robot S.A.R.A. (Syst\u00e8me d\u2019assistance Robotique Autonome). She was build for the Robocup@HOME competition by Walking Machine, a student run robotic club in Montreal Canada. She also runs on ROS.", "The video explains some characteristics of our robot.", "Awesome! See you in Japan!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/our-general-purpose-service-robot-s-a-r-a/1689"},
{"title": "New book for ROS Projects: ROS Robotics Projects Updates!", "thread_contents": ["Hi all", "I would like to share the main projects that are discussing on my book ", ". Also, I would like to share a demo video of self-driving car project mention in the book. ", "Here is the demo video link", "Hi all", "The book ROS Robotics Projects is officially released.", "Buy your copies from following links:", "PACKT: ", "From ", ":", "\n", "From Amazon.in", "\n", "Book Website: ", "  ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-book-for-ros-projects-ros-robotics-projects-updates/1475"},
{"title": "ROS Automation", "thread_contents": ["Hi, I\u2019m a Computer Science Student and currently working on a project of ROS automation i.e. we are developing a platform (Software) to help ROS programmers get rid of the manual instructions/command typing on terminals and/or different files including manual declaration of dependencies, etc. We are working on basic features and would like to know what other features you (ROS Programmers) would want to be automated? Your feedback and support will be much appreciated. Thanks.", "Hi Yusra,", "\nI am glad someone is working on this, I was thinking on starting something like this myself. The main problem I encountered was opening a number of terminals for each launch file or command I had to run(In case of a lot of modules it went up to 9 or 10). I tried to write a simple bash script to run these in their respective sequence but was unsuccessful, I realised a feedback system was required where a launch file when completely launched gave some confirmation so that the next commands can be run in a new terminal, so I left this task to be done when I had some extra time on my hands. I would love to know how you are approaching the automation.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-automation/1742"},
{"title": "Goose Chasing Robot", "thread_contents": ["Hi,", "I have a terrible problem with geese in my yard. I\u2019ve also been a heavy ROS user for the past 2 years now so I thought I\u2019d make a robot to help chase the geese away. I have a prototype up and running, it can visually recognize geese up to 20ft away and will navigate to there location at 0.5m/s. I\u2019d like some help on a few things if anyone is interested.", "\nA better state machine", "\nAutonomous docking", "\nMobile app support (I think this looks like a good project to work off of ", ")", "The first thing I could use help on is the state machine. If anyone has experience with using ros_smach as is willing to help (or just advise), please email me (", ")", "Hey ", ", cool project ", "In terms of the state machine, here are some resources that might be of interest:", "For mobile support, I don\u2019t know how deep you want to get into mobile development but you could look into rosjav/android: ", "\nThat would have ROS running on an Android device right? I think I\u2019d rather have the master running on a Linux board and just have a tablet for veiwing information. I have a NVidia Jetston TX1 that I\u2019m running my master on now. That has enough power to run a neural net for recognizing geese.", "Great! This is exactly the type of info I was looking for. Thanks you!", "P.S.", "\nI also went to Cornell. I\u2019m the one who emailed you about NASA SRC advice ", " you can have your master running in the linux board, and an Android tablet using rosjava to connect to that master, subscribe to topics and view information from there. Creating a client application using ROS Android / ROS Java is not that hard, and it may serve your purpose very well!", "BTW, cool project! I would love to see a video of the robot chasing geese.", "You may find some inspiration in Richard Vaughan\u2019s PhD work: ", " ", " There is a link in the video description with more details.", "Wow, that  will be immensely helpful ", "  Thank you!", " ", " Here\u2019s the paper ", ". Enjoy!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["SMACH tutorials: ", "\n", "My fork of their hard-to-find source code: ", "\n", "\n", "FlexBE (Flexible Behavior Engine): ", "\n", "Source code of its core: ", "\n", "GitHub organization (states and GUI app): ", "\n", "FlexBE is an extension of SMACH, but may or may not fit your use case.", "I was somewhat involved in the development of FlexBE, so feel free to ask follow-up questions.", "\n", "Boost Statechart (C++): ", "\n", "Steep learning curve but much faster and more expressive.", "\n"], "url": "https://discourse.ros.org/t/goose-chasing-robot/1651"},
{"title": "State Machine Visualization", "thread_contents": ["Hi,", "I\u2019m working on a state machine visualization for robotic development. The goal is to improve the actual visualization giving more power and feedback to developers to see and even modify certain aspects of their programs.", "\nSo far I developed a live programming language for robotics called LRP (", "). In this language we developed a live visualization that interact with the source code in real time.", "\nNow I\u2019m working on a visualization for SMACH (", "). I know the existence of the visualization for smach, but I want to improve it with our research on LRP and more.", "\nDo you have any real example of a robotic program using state machines (with any API) that I could analyse?", "\nDo you have any experience using state machines to write programs?", "\nHow do you use the standard visualization of a state machine API when there is a visualization?", "\nAny experience of debugging a program using state machines?", "If you feel more confident talking in private, you can email me: ", "I\u2019m planning to make a form, but first I want some impressions to start working with it and then make a better form for more people.", "Thanks in advance for your help,", "\nBests,", "\nMiguel", "Hey ", ",", "We (", ") developed FlexBE (Flexible Behavior Engine) for our entry in the 2015 DARPA Robotics Challenge finals. It\u2019s an extension of SMACH and was the high-level executive running onboard our Atlas humanoid robot. One of the operators monitored and interacted with the executive via a GUI.", "References:", "This looks absolutely amazing ", "!", "Good luck ", " for your project!", "Do you have any experience using state machines to write programs?", "I used state machines at my first job doing industrial automation. We used ", " and ", " structure for our state mahine. It was easy to setup and understand.", "Now I\u2019m a mobile robotisist and I am finding SMACH hard to setup and hard to understard ", " so its good to hear someone is trying to improve it.", "How do you use the standard visualization of a state machine API when there is a visualization?", "After starting my code I run rosrun smach_viewer smach_viewer.py as quick as possible to view my first few transitions. Then I got to the state I\u2019m testing out, something doesn\u2019t work so I shut down my code and the SMACH viewer (it would be really nice if I could leave the SMACH viewer open and have it reconnect).", "Any experience of debugging a program using state machines?", "I control my state transitions with an xbox controller which I find convenient.", "Thank you guys,", " I\u2019m going to take a look at it, specially the examples ", "(it would be really nice if I could leave the SMACH viewer open and have it reconnect)", "SMACH does not do that? I think it does", "I control my state transitions with an xbox controller which I find convenient.", "how do you do that?", "SMACH does not do that? I think it does", "Hmm tried this again and it does work ", " I think I\u2019ve run into scenarios where it doesn\u2019t, either that or I\u2019m crazy. Who knows. But thank you, think will help with my debugging.", "how do you do that?", "I use wireless Xbox controller with a", ". The ROS package for this is called ", ". I then wrote a small python node to publish a string like \u201cstop\u201d or \u201cgo_to_kitchen\u201d to the /controller_command topic.", "Then I use a ", " to listen to the /controller_cmd topic and transition to a state according to what message it sees.", "I am finding SMACH hard to setup and hard to understar", "I was re-reading your answer and I notice this (I do not know how I miss this before). Why do you think SMACH is hard to setup and understand?", "Relative to PLC state machines I find SMACH very difficult to setup and understand. But compared to the spaghetti code that would result with a complex project and no state machine, SMACH is a vast improvement.", "In the PLC world there is a very common state machine structure called ", ". Almost all automated manufacturing lines can use this same state machine, modified only slightly. Where I worked there was a file that contained the state transitions and at the top was written DO NOT EDIT THIS CODE! In your own code you set flags which were then used to change the state. You get the hang of it in about a day and then it\u2019s very easy to program with. (BTW there is no parallel processing done in PLC\u2019s)", "Now with SMACH and robotics there is no such common state machine, and its very temping to mix your functional code with the state machine code, making it more difficult to understand. These are two of the major reasons I think I find it confusing, but I do appreciate the fact that SMACH is a much more powerful tool than something like PackML, so it makes sense that its harder to setup and understand.", "I think that one day there could be a very common state machine setup using SMACH that does serve as a jumping off place for any robot. For example there may be a \u2018teleop\u2019 state, a \u2018autonomous\u2019 state, a \u2018pause\u2019 state, and a \u2018recharge\u2019 concurrence state would make up a pretty basic state machine that would apply to a lot of robots. I may try to create something like that and post it to github if I have the time. But right now I\u2019m trying to learn FlexBe.", "For those of you not familiar there\u2019s already some interest in integrating with PackML in the ROS-I community.", "Submitted by: Lex Tinker-Sackett, Mfg Technology Specialist, 3M PackML State Model (from the PackML Implementation Guide) ROS-Industrial is a foundational technology abstracting robot applications for industry. ROS-I runs on PC hardware (currently...", "I found ROS packages here:", "ROS packml (https://en.wikipedia.org/wiki/PackML) support package", "As well as a SMACH - PackML integration here:", "bohr_devel - PackML state machine for ROS developmental repo", "Just a heads up.", "I am currently working on an implementation of Behavior Trees as an alternative of classical FSMs.", "This will also include an editor and a real time visualization of the current state.", "This is an early screenshot of the editor:", "It is not ready for prime time but it will be in a couple of months.", "Cheers", "Davide", "Hi all,", "For those who do not know it, and I don\u2019t know why, this package is very well written with a very nice and pro rqt plugin:", "Contribute to decision_making development by creating an account on GitHub.", "IMHO supersedes SMACH, already by the fact that is C++ and not python, but also because it implements FSM, HFSM, BT, and TAOs.", "It was available in ", ", but I think the maintainers lost interest in releasing into further distros, although I\u2019m on 16.04/Kinetic and it works like a charm.", "\nBest,", "\nCarlos", "IMHO supersedes SMACH, already by the fact that is C++ and not python, [\u2026]", "I don\u2019t really agree with / care for the \u201cit\u2019s not Python\u201d remark, but other than that, +1 for this package.", "Haha my apologies, I\u2019m a bit anti-python ", "Will be announcing a behaviour trees implementation in a few days - be interesting to see where our implementations meet.", "The ", " and ", " pages are where you can do a quick dive to see what it is up to.", "No editor (yours looks nice), but we found scripting a tree in python flexible and quick enough. More important for us were an rqt monitor, bagging and replaying capabilities.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Overview, screenshots, and videos: ", "\n", "Source code of engine core: ", "\n", "Various states and the GUI app: ", "\n", "\n", " (", ")", "\n", " (", ")"], "url": "https://discourse.ros.org/t/state-machine-visualization/1691"},
{"title": "Sumo ROS Gazebo Simulation", "thread_contents": ["I\u2019ve created a Gazebo simulation of the Parrot Jumping Sumo attempting to make it match a real sumo. The project is on github at:", "rossumo-gazebo - Gazebo simulation of Parrot Jumping Sumo with ROS wrapper for Ardrone SDK 3", "This is a side by side video of movements of a real sumo being recorded with ROSBAG and then being fed into the Gazebo simulation on the right:", "My goal of creating the simulation is to use reinforcement learning to teach a sumo to do things like climb stairs in Gazebo (similar to reinforcement learning in ", " with a turtlebot) and then apply it to a real Sumo and hopefully get a real sumo climbing stairs. Unfortunately my jumping mechanism on the Sumo stopped working so will have to teach the sumo to do something other than climb stairs.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/sumo-ros-gazebo-simulation/1802"},
{"title": "New ROS book and Open Source Tutorials Code", "thread_contents": ["Dear Ro(s)boticists,", "I\u2019m happy to announce a new ROS book: ", " All book tutorials support ROS kinetic and are available open source at: ", "The authors of the book also want to say thanks to all the ROS community for the great work done with the official documentation and tutorials. We humbly hope this book and tutorials help on that effort.", "Thanks!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-ros-book-and-open-source-tutorials-code/1805"},
{"title": "Gazebo on Gentoo", "thread_contents": ["All,", "I\u2019m working on pulling in the other version of Gazebo to the ros-overlay for Gentoo.", "I have an open question: would people prefer to emerge specific version of gazebo, or just have one ebuild?", "e.g. To emerge Gazebo 8, you\u2019d emerge \u201cgazebo8\u201d instead of just emerge \u201cgazebo\u201d.", "Thoughts?", "Hello ", ":", "Thanks for your efforts of maintaining Gazebo in Gentoo. According to the ", ", currently gazebo7 and gazebo8 are supported.", "I think that one of the valid points you can consider to support one or more versions of Gazebo ebuilds in the ros-overlay is how many ROS versions you want to support in that overlay: ", " is the version supported in ROS Kinetic and ", " is the one used in ROS Lunar. This can help Gentoo users to have the same simulator experience than the users on Ubuntu/Debian.", " those are some good points you make there. I suppose it doesn\u2019t make much sense to support gazebo pre 7.0.0.", "Maybe it would be best to have a ", " and ", " ebuild (along with the proper versioning, of course).", "Perhaps this should be extended to other packages in the repo?", "Maybe it would be best to have a gazebo-kinetic and gazebo-lunar ebuild (along with the proper versioning, of course).", "I would not recommend using the ROS distros as identifiers for the gazebo packages: they are both independent projects with their own names and versions, it is probably more consistent to keep the naming/versioning scheme of both. I, as a gazebo user/dev, would be surprised to see gazebo versions named with ROS release names. It should be easier to create ", "and ", " and make ebuilds to depend on the correct version or create a ", " named ", " (it should make maintenance more easy and I have some valid use cases where you can use gazebo7 or gazebo8 with the same version of ROS)", "Ok, that sounds good. Thank you for the input!", "Speaking as a very-very-long-time Gentoo user, I would prefer to see the package named following the Gentoo conventions. So it should be called \u201cgazebo\u201d, there should be versions available for 7 and 8 (and minor/patch releases as appropriate), and it should be slotted so that a user can have one version 7 and one version 8 installed simultaneously.", "So it should be called \u201cgazebo\u201d, there should be versions available for 7 and 8 (and minor/patch releases as appropriate)", "Agree. +1 for using the slots.", "it should be slotted so that a user can have one version 7 and one version 8 installed simultaneously.", "This would require patching since upstream does not implement the option of having two versions installed at the same time and some files are going to use the same names.", "Yes, patching would be required, but that is fairly common for slotted ebuilds. I think that only the installation target names would need patching. It might alternatively be possible to write the ebuild so that it instructs Gazebo to install to a known directory based on version and then make symbolic links from the ", " etc. locations using version-named files to the actual files.", "It might alternatively be possible to write the ebuild so that it instructs Gazebo to install to a known directory based on version and then make symbolic links from the /usr/lib etc. locations using version-named files to the actual files.", "I like that a lot, actually. I think this would be the best way.", "Ok, so there is an update here\u2026", "Apparently, Gazebo 8 will not be ready for Lunar. So I suppose we should just keep things where they are currently.", "Moreover, I am starting my internship at OSRF on Monday (May 22nd) and talking with ", " about working on a better solution to using ROS on Gentoo than the ROS overlay approach I am doing currently.", "The idea is to generate ebuilds from the ROS buildfarm for all bloom-released packages.", "That sounds like an awesome idea. You should be able to create an eclass to handle catkin packages and builds each one using catkin_make_isolated.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gazebo-on-gentoo/1786"},
{"title": "Beginner with ROS and robotic :", "thread_contents": ["I just want to make poppy walk ? Or be able to command nao or asti (anyrobot ) with python, I can use jupyter notebook work with poppy but apparently it doesn\u2019t walk in V-rep (i use a simulator, robots are too expensive).", "\nRegards", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/beginner-with-ros-and-robotic/1873"},
{"title": "New ROS Gentoo Install Guide", "thread_contents": ["Not sure who\u2019s been watching the activity for the ros/ros-overlay repo, but I would greatly appreciate some feedback! There is a new installation guide up for ROS Lunar. The new process should work for any rosdistro released package (as long as the dependencies could be satisfied).", "Let me know what you think!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-ros-gentoo-install-guide/1889"},
{"title": "Autonomous map generation", "thread_contents": ["Hi, there,", "I am going to request to generate a package, as a place storing implemented code about autonomous map generation. And I hope the package would be helpful to guys who want robot to fulfill SLAM process and automatically create mapping data.", "We didn\u2019t use frontier_exploration package. Rather than interoperating in RVIZ or gazebo interface during SLAM process, we tried to find a more convinient way.", "We tested the code on Turtlebot2 with gmapping and cartographer algorithms.", "Please let me know if there is already a similar package or if I violate some rules here.", "Thanks", "You might find this interesting: ", "Hi.", "\nMay I ask you why you don\u2019t want to use frontier_exploration and slam_gmapping package?? It work really good and is very easy to use.", "That\u2019s great! Thanks for sharing.", "Thanks for attention.", "I didn\u2019t touch frontier_exploration deeply, but I found it needs the user to operate on the remote map (e.g. creating some polygons) during SLAM generation. So I want to find some ways to make the map generation fully automonously, even needn\u2019t open any remote monitor/map screen.", "Is there any infomation I missed? or is there any other solution meeting the full automation? Thanks.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/autonomous-map-generation/1871"},
{"title": "Building your own DJI M100 drone", "thread_contents": ["Hi, forks", "\nIt is my pleasure to introduce some of our works in regarding M100 Matrice dynamic system identification and control.", "\n", "What we are doing is that identification of underlying dynamics of M100 by recording input virtual RC commands and its states using motion capture device. Given the system, model predictive controller (MPC) produces input commands; roll, pitch angles, yaw rate and vertical velocity.", "We have modified dji_onboard_sdk_ros and integrated into our attitude and position controller together with multi-sensor fusion framework. You can find source code and relevant documentations from the link below.", "\n", "dji_onboard_sdk_ros - Patched ROS packages for DJI onboard SDK (M100 tested), Deprecated, Please use our latest version from https://github.com/ethz-asl/mav_dji_ros_interface", "\n", "Documentation is still ongoing but I believe that it would be a nice starting point toward having your fully autonomous drones.", "\nIf you have questions or inquiries, please let us know and will follow up.", "\nIn addition, it would be fantastic to hear your feedbacks in regarding this project.", "\nBest,", "\nInkyu", "Hey. Im working on the matrice 100 and wish tyo know some details regarding its control for autonomous flight using vision. can you please help me?", "Thank you", "Regards", "Hi, Archit Kamath", "\nThanks for your interests.", "\nRegarding topics you mentioned, please have look our repo.", "\n", "dji_onboard_sdk_ros - Patched ROS packages for DJI onboard SDK (M100 tested), Deprecated, Please use our latest version from https://github.com/ethz-asl/mav_dji_ros_interface", "\n", "\nShould you have any questions, please let us know directly through repo. It\u2019s public and everyone can see and comment on this.", "\nCheers,", "\nInkyu", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-your-own-dji-m100-drone/1272"},
{"title": "ROS + Kinect for autonomous grasp", "thread_contents": ["I\u2019m implementing visual computing system in a anthropomorphic robot to automatically detect objects and grasp it. Like this (", ")", "I want to develop a simulation using ROS. Should I use gazebo to do this as well?", "\nDo you know any material that can helps me in the beginning of this journey?", "thank you in advance.", "I can divide your problem in some subproblems as follows.", "First for object recogition ( ps it will not give precise pose ) but it will provide you with the bounding box around different objects in a clustered scene : ", " .", "\nROS wrapper : ", " .", "From there you can start with the pose estimation.", "You can find some of the pose estimation libraries here : 1- ", "\n2- ", " : this one is having also a gazebo simulation ready made ( i didn\u2019t test it yet)", "\nThere are many more that i am sure they are available but either i dont remember or i am not aware of its existance.", "Currently i am trying to develop my own object pose estimation as i wasn\u2019t satisfied with most of the available libraries but that is a side project so it will take much time to come to life.", "Moveit can be used to path and motion planning of the arm ", " .", "Also i urge you to follow up with some work of the teams participating in Amazon picking challenge. In here you will find list of the repos for these teams : ", " .", "This paper is nice also : ", " .", "\nI hope this can help you with anything.", "Hi Caio,", "\nI don\u2019t know if ", " made by Shadow Robot can be useful for you. It uses a Kinect + UR5 + Shadow Hand", "\nIt works online and you have everything working already, including Python API for grasping. Of course you can modify it at will.", "Cheers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-kinect-for-autonomous-grasp/1793"},
{"title": "Valter The Robot", "thread_contents": ["Let me present to you my personal project Valter The Robot. General interest in modern trends in automation and thinking about the time, usually spent, on doing housekeeping routines had inspired me few years ago to start this project.", "For at least last 3 years (after major Valter assembly was completed) I\u2019m heavily using ROS for various of tasks. I\u2019m still in an active stage of improving / implementing different autonomous behavior of my robot.", "\nBesides this I\u2019m trying to make it more gentle, more social\u2026 And all this for a certain good reason! Indeed, I have very specific steady problem while working with robot Valter. My older son very much afraid of it.", "\nNeither I nor my wife never told my son Igor that Valter (the robot) could harm him in any manner. On the contrary, we say all the time that robot Valter loves him and wants to take care of him. Moreover I\u2019m trying as much as possible to make my robot more gentle (speaking to Igor in his specific \u201cchild\u201d language, making him presents like new toys).", "\nSuch a problem results into the following situation: it\u2019s possible to work productivley with robot only at night, thanks robot is pretty quiet while operating. So the overall progress is not so strong as I would like it to be. But I\u2019m constantly doing improvements.", "\nAs nearest super-goals I see the following:", "As a side project I\u2019m working on Virtual Laboratory (web-browser WebGL based real time 3d interactive web application) for technical schools/universities where I used highly detailed textured Valter model.", "\n", "In future plans I have an idea of integration Virtual Laboratory with real Valter.", "I am in awe of your creation! I wish I had an answer for your son\u2019s fear of Valter. I\u2019m sure that when he gets older he will understand that Valter is harmless to him. Also, you have a very understanding wife - Valter doesn\u2019t look inexpensive.", "Could you use the mapping algorithms to make a detailed map of the house and test Valter in the simulator during the day? All of your goals look doable, given enough time and parts.", "I can picture Valter with a tool-belt with various attachments for his hands.", "I would love to see Valter at a science fiction convention sometime.", "Jay", "Wow, that is an amazing robot!!", "About your son fears, you can try inviting him to play with Valter,", "\npainting or putting some stickers on it and even asking him to come up with", "\ntasks to Valter, or teaching him how to operate the robot. In other words,", "\ntry to make him see it as a toy instead of an entity.", "Valter does does looks kinda scary, being big, tall, metalic and we almost", "\ncannot see its eyes. I\u2019ve read somewhere that large, round and white eyes", "\nprovide some comfort for viewers, (eg the usual kids cartoons, animes,", "\nrobots as Nao\u2026), even if the other features of a face are not visible", "\n(nose, mouth, ears\u2026).", "Cheers.", "Thanks for advises! I think the most effective way to start overcoming fear of Valter is to start operate with him. I will try to invite my son in teleoperation session with joystick remotely from another room or from outside of apartment. Also I will try to invite some children, friends of Igor, which I know for sure from previous, enjoys how Valter looks and behaves to demonstrate that there is no danger from Valter side. In other words - continue Valter\u2019s socialization process ", "\nThanks for your words of approval!", "This is really an amazing robot! You must have put an incredible amount of work into it. Good job!", "Thanks, Jay for your so positive characteristic of Valter. It\u2019s really important for me. Most of people who knows me and Valter tells exactly the same - time will pretty soon solve the problem with fear of Valter, and moreover will help to eliminate such a problem with my younger son (he is now 1 y.o.).", "\nThanks to my wife, she understands how important is what I\u2019m doing for our children in future in terms of motivation and positive example. I would like them to be well educated and I believe Valter will be a good example of how to solve complex technical issues and steadily achieve final goals. Besides this, you are absolutely right - this is pretty expensive project (~ price of business class vehicle) which became possible to bring it to reality due to what I\u2019m doing every day - commercial / enterprise software development. And despite I could make more money on my everyday work I made a decision to invest part of my time (and money) in future and get deeper into robotics. I hope my sons will appreciatate such a decision when they get grown up.", "\nAbout simulator - I\u2019m hardly working on this right now. Trying to put valuable parts into Gazebo. I had RTABMap assembled detailed map of the house, but I\u2019m planning to create a detailed plan with photo textures in Blender.", "Thanks, Martin. Indeed 6 years of development and assembly and about a price of good car. As I sad before - neither I nor my wife do not regret spending time and money on such an activity since we believe that\u2019s in first turn an investments in education of our children.", "hi, very cool work. I read the robots hand has 12 force feedback sensors. What sensors do you use? Do you control the fingers with servos and wire-ropes?  Is there anywhere more information about the robots hand/fingers?", "Yes, robot has 12 force feedback sensors, 6 on each palm. Yes, I control fingers with servos and metal tiny ropes. I believe it\u2019s better to see once than listen to a pile of verbal explanations.", "\n", "Finger tips and internal palm surface later were covered with silicone to prevent slipping of smooth objects.", "\n", "Force sensors prepared for installation", "And some photos from hands\u2019 assembly stage\u2026", "\n", "Low-level Valter\u2019s control software (left and right hand\u2019s widgets where sensors readings are displayed)", "\n", "While developing Valter I have made a lot of investigations, among which there was a overview of existing sensors of different types. See the results of this overview on my youtube channel", "thanks for the pics. Are these flexiforce sensors? I really dont understand where they are mounted ? On every finger 3 sensors under these black rubber squares? You got 6 fingers, 6x3=18 , or are there any sensors missing on some fingers? And the electronic cables of the sensors are on the backside of the finger?    I see the cables on the backside of the fingers but cannot see the sensors .", "I also builld a hand  ", " , thats why i am so curious ", "flobotics, you do really nice thing - amount of degrees of freedom of fingers on your robotic hand is impressive! I don\u2019t use flexiforce sensors. I\u2019m using sensors like these ", " on fingers. Sensors installed not even on all fingers phalanges. Each palm has 10 finger pads sensors (where black cables going out of phalanges) and two big force sensors on the internal surface of the palm. Look once more at photos.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["make Valter even more social and delicate", "make Valter even more autonomous (unmanned charging from wall 220V outlet with charger module, docking to 12V charger-station)", "improve visual RTABMap Ros navigation (odometry) with backup on-board localization sub-sybstems", "improve object recognition with ORK", "develop and produce special hand-tools / graspers for precise manipulation with small objects"], "url": "https://discourse.ros.org/t/valter-the-robot/2066"},
{"title": "Ros service sample code not running", "thread_contents": ["when i run rosrun pkg_name and ", " it showing nothing in rosservice list and if i close rosrun then fowlling error comming", "\nTraceback (most recent call last):", "\nFile \u201c/home/edu/p/ros/robot/src/robot/src/service_server.py\u201d, line 3, in ", "\nfrom robot.srv import WordCount,WordCountResponse", "\nFile \u201c/home/edu/p/ros/robot/src/robot/src/robot.py\u201d, line 4, in ", "\nrospy.init_node(\u2018topic_publisher\u2019)", "\nFile \u201c/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/client.py\u201d, line 323, in init_node", "\nraise rospy.exceptions.ROSInitException(\u201cinit_node interrupted before it could complete\u201d)", "\nrospy.exceptions.ROSInitException: init_node interrupted before it could complete", "Following our ", " please ask questions on ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-service-sample-code-not-running/2173"},
{"title": "Rosjava log4j error", "thread_contents": ["After not having used my rosjava nodes for a year or so I am now getting this error when I run a node,", "log4j:WARN No appenders could be found for logger (org.ros.internal.node.client.Registrar).", "\nlog4j:WARN Please initialize the log4j system properly.", "\nlog4j:WARN See ", " for more info.", "I have searched fro a resolution but can\u2019t find one. Manually configuring a properties file doesn\u2019t help.", "Any idea why I am getting this error?", "Please ask debugging questions on ", " this is a forum for announcement and general interest discussions.", "If you\u2019d like more guidance please see our ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosjava-log4j-error/2179"},
{"title": "PR2_Controllers in ros-kinetic", "thread_contents": ["Looks like most of the pr2 packages have not been scaled into ros-kinetic. What are the plans for this?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pr2-controllers-in-ros-kinetic/2182"},
{"title": "Linorobot - 2WD, 4WD, Mecanum and Ackermann Steering ROS platforms", "thread_contents": ["Hi guys,", "Just like to share my project I started almost a year ago. It\u2019s a suite of various ROS compatible platforms mainly 2WD, 4WD, Mecanum Drive and Car-Like robots.", "I have posted a tutorial on how to build each platform at  ", " . Since it\u2019s supposed to be homebrew, most of the parts are easily accessible and comparatively cheap. The controller\u2019s using a Teensy 3.2 board and uses a Radxa Rock Pro as its main computer. It\u2019s also got a XV-11 Lidar for mapping and localization so users can have an end-to-end learning experience with ROS. I tried to make the design as modular and scalable as possible so it can be used for much bigger robots as well (hopefully).", "Please feel free to criticise, suggest or ping me for interested collaborators or mentors. ", "Thank you.", "Wow, your project documentation is fantastic! Thanks for taking all the time to write it up so nicely. Congrats on getting all that working; it\u2019s great to see a full system end-to-end from firmware all the way up to map-building.", "Happy to contribute something ", "This is amazing. I am going to try to make some of these and check them out.", "Love it! Hope to have time for making my own ", "I\u2019m planning to support more motor drivers that has higher rating ie BTS7960 (43A) ", " .", "As well as support for Kinect for more sensor options.", "Will keep you guys posted!", "Happy Building! ", "You have done so much work.", "I have been recently developing a similar robot - the LoCoRo (low cost robot) project. I am using a Raspberry Pi 3 as the computer (others could be substituted). Similar to your Linorobot, the foundation is PWM motor drive and motor controllers. (The use of ESCs allow for high current motors). It too supports 2WD (akerman and differential drive) and 4WD (differential and Mecanum drive). (The mecanum drive currently used 3D printed wheels which I plan to open source soon). I am only at the teleop phase of development. I will read about your work to help with ideas for developing autonomous capabilities.", "I choose the RPi for its ubiquity and with the expectation it will have more than enough CPU to handle computer vision (which I am learning is suboptimal). I only recently started to consider other proximity sensors for mapping.", "Thank you for sharing all that you have done.", "Hi guys, I\u2019ve released beta v1.0 of this project. I did a major revamp to support more motor drivers, IMUs and LIDARs in the future.", "Due to maintainability issues I\u2019ve also merged all the supported bases (2WD, 4WD, Mecanum drive, and Ackermann steering) into one repo .", "You can check out the release notes here: ", "\nand the new wiki here: ", "Do join our small forum if you need help ", " ", "Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/linorobot-2wd-4wd-mecanum-and-ackermann-steering-ros-platforms/772"},
{"title": "Best practice on maintaining a repository", "thread_contents": ["Hi guys,", "Just need some advice regarding my project ", " . I\u2019ve posted it here last month: ", " .", "Basically it supports 2WD, 4WD, Mecanum, and Ackermann steering. Currently, each base has its own repository which includes the microcontroller codes, Nav Stack launch and param files. Since the kinematics of each robot is almost similar and can be written in one unified code, shall I just store it one repository or retain all the repos and keep them independent?", "Keeping it separate will be much appreciated since others can test each package individually and add support for new hardware. I am now planning to incorporate your robot platform for education research. I will start by building the 2wd robot during the new year break.  Your project is a great initiative for those of us who are not so much familiar with the hardware part and can learn immensely by learning to make the robots and using ROS. I strongly suggest you to make a google group for the linorobot so that people interested in the project can have open discussion. Just my suggestion.", "First of all I wanted to mention, that your project really looks fantastic and your attention to detail when it comes to documentation is very impressive. Big Thanks, I would have really loved to have those resources when I was in school learning how to build a robot.", "But to answer your question: Neither way is perfect. As mentioned in the previous comment having all parts separated makes it easier to swap out a component. But in my experience it can be quite painful to maintain a bunch of repositories, that are dependant on each other.", "This is mainly due to the fact, that you need to track, which versions of one repository will work with another repositories version. In the past I relied mostly on syncing them by time and making sure, that the current master branches will work together. As ROS currently does not have a method for resolving versions (e.g. with semantic versioning) other then the main distributions (indigo, jade, etc.) it can be very difficult to deal with the introduction of a breaking change in one of your repositories.", "If you put your project in one repository you can make sure, that every commit on master is in a consistent state, where all components work well together. There are other solutions for that, like for example git subrepos, but they are often more complicated. Also code sharing between different platforms is way easier when they are in the same repository.", "Also if somebody wants to add a new platform/hardware they can fork your repository and maybe even merge it back, which would allow you to see, whether a new feature breaks a different platform (I would only do that, if the new platform would be adopted by several people, otherwise just forking would definitely suffice, adding the benefit that your changes wouldn\u2019t necessarily break other platforms, as somebody could choose whether to upgrade).", "So in conclusion: If you want to share code between the platforms, or there are currently dependencies between repositories it would make sense to switch to a monorepo. If the code in each repository is completely independent of each other it makes sense to put them in different repositories.", "Thank you so much for your advice. I\u2019m currently refactoring all the platforms to support a much higher motor driver, hence bigger robots and for more modularity. I\u2019ll consider these advices once I have all the versions stable. Hope you can join our discussions. Just created a google groups here: ", "Thanks for the suggestion. I just created a Google group. Feel free to post if you have any question or want to share any latest robot you\u2019ve built. Here\u2019s the group: ", " . See ya! ", "Can\u2019t agree more with ", "\u2019s ", " (that already got 3 likes btw). Adding some comments:", "I can\u2019t help but to stress out that reducing maintenance cost for mid/longer term is worth being considered even at the earlier active development phase. So many packages in ROS are abandoned unfortunately (I assume it could be an inevitable nature of software life-cycle though). Reducing the maintenance cost is not the one-stop solution for that issue but can help both the maintainers to keep the maintenance work and finding new active maintainers.", "For consolidating packages/repositories,", "Thanks for the great advice guys. As what you have advised, I did release a beta v1.0 of this project. There was a major revamp to support more motor drivers, IMUs and LIDARs in the future.", "Due to maintainability issues I\u2019ve also merged all the supported bases (2WD, 4WD, Mecanum drive, and Ackermann steering) into one repo .", "You can check out the release notes here: ", "\nand the new wiki here: ", "Do join our small forum if you need help ", " ", "Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "FYI MoveIt! projects recently consolidated several repositores of various modules into one larger repo. You can see ", ".", "Separating packages just to allow end-users to easily swap one-by-one sounds like to me adding ", " with very few advantages. I believe there are number of driver packages that handle multiple particular devices under the same product family.", "\n"], "url": "https://discourse.ros.org/t/best-practice-on-maintaining-a-repository/959"},
{"title": "Rosshow: ASCII/Unicode art viewers for sensor message types", "thread_contents": ["Hi all! First post here and first attempt to contribute so please advise if I should be posting elsewhere. I\u2019m an MIT graduate and co-founder of a robotics startup in Palo Alto.", "One of the problems I have had is that often one wants to simply visualize the output of various sensor message topics without having to fire up some complex visualization tool to answer simple questions such as whether a LIDAR is functioning or whether a camera is saturating. In many cases one simply wants to do this over ssh.", "I created a tool to visualize LaserScan, Image, and NavSatFix topics using Unicode/ASCII art. PointCloud2 supported as well but no rotation of the view yet. Currently very pre-alpha and hacky. Would welcome advice on the best way to structure this, and contributions!", "Contribute to rosshow development by creating an account on GitHub.", "Great work and great repo!", "Why don\u2019t you make it a ROS package?", "I can help if you need help on that.", "Awesome.", "I second the suggestion to make it a ROS package.", "I wanted to give it a try, but unfortunately it relies on python 3, where the default python version for ROS (at least on Ubuntu) is 2.", " Good idea! Will work on that but probably not free until next week; if you want to take a stab at converting it to a package in the next few days let me know!", " Thanks for the feedback! I used a package I made called python-termgraphics (", ") for drawing, and I targeted dual Python2/3 compatibility with that. I guess I just need to make the ROS part with dual compatibility. Is there any reason ROS is using Python 2 as the default still? (I use Python 3 for most of my robot code for future-proofing especially with TensorFlow and other new packages pushing for Python 3 support first.)", "Another option I was considering is to eventually just port the whole thing to C++ (especially useful for people who may run the image viewer on RPis or other embedded hardware).", "I love it! great!", "\nit seems possible to implement rqt_plot easily!", "Is there any reason ROS is using Python 2 as the default still?", "I think it is mostly the vast amount of legacy code and also target platform support / defaults. I believe python 3.5 and up compatibility is strongly encouraged for recent ROS distros but python 2.7 is still the default.", "There is some info here: ", "ROS 2 is already Python 3 only, so it is worth making/keeping your Python software in a condition where it can be executed on both Python 2 and Python 3.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosshow-ascii-unicode-art-viewers-for-sensor-message-types/2212"},
{"title": "What's the real consideration btw Secure ROS and SROS from their design?", "thread_contents": ["Now there are 2 projects which involves security consideration for ROS, one is secure ROS ( ", " ) and the other is SROS ( ", "), from its introduction, we know the Secure ROS employ IPSec and SROS defines different levels of security concept/policies. however, it\u2019s really easy to make things complicated for a guy who hasn\u2019t dug their details, so\u2026here to pose several question as follows:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Why are there two different projects for ROS security at the beginning from the design perspective? and is it possible to consolidate them into one in the future ?", "Does each of continue to maintain or develop in parallel if no integration or trend to lean one of them ?"], "url": "https://discourse.ros.org/t/whats-the-real-consideration-btw-secure-ros-and-sros-from-their-design/2274"},
{"title": "Rqt dynamic reconfigure for a single server", "thread_contents": ["Launch a control panel for a single dynamic reconfigure server with controls very similar to rqt_image_view- there is a dropdown combobox with every server listed and a refresh button.", "The slider behavior is currently different from rqt_reconfigure in that it continually sends updates when moved, but I\u2019d like to make that configurable to optionally only update when the slider is released (does this node itself need a cfg?).", "Tools for use with dynamic reconfigure with ROS. Contribute to lucasw/dynamic_reconfigure_tools development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rqt-dynamic-reconfigure-for-a-single-server/2284"},
{"title": "[Experimental] UDP Broadcast transport layer", "thread_contents": ["Hi all,", "I made a proof of concept, meaning still experimental, of the ", " and how it could work.", "The final aim is to make a multimaster implementation with UDP Broadcast to avoid WiFi struggle.", "I would like some advice of what is wrong and how to improve.", "multimaster_udp - UDP broadcast transport layer, with view of reimplementing the multimaster with UDP to run over WiFi", "This is pretty cool stuff, and I like your goal of moving towards multi-master. How does it relate to ", "?", "It does not relate to UDPROS at all\u2026 ", "This is mainly a proof of concept to broadcast messages on IP layer instead of the Application layer. This avoid to struggle network on massive multi-robot environnement.", "It does not relate to UDPROS at all\u2026 ", "Then in what ways would you say they currently differ? Does UDPROS not yet support UDP ", "? For example, does the xmlrpc negotiation of transport layer for UDPROS not currently enable UDP broadcasting when more than one subscribers connect to the same port for the same topic for the same publisher?", "UDPROS is unicast only:", "Badly, UDPROS is unicast only. While it reduces the overhead over TCP, due to the unreliable connection, it can be improved to forward general information to a lot of robots. For 50 robots over the WiFi, while UDPROS will send 50 packets, it is possible to send only one broadcasted packet.", "Again, this is only a proof of concept, to have a basic communication running. On this base, we can think about the problems encountered and how to tackle them and finally implement it inside ros_comm, for C++ and Python.", ": you probably already found it, but just in case: ", ".", "Old, but trying to address the same limitations, I believe.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Topic negotiation -  Both the Subscriber/Publisher uses a TCP service to get the publishing port from the ", "\n", "Publisher serialize the packets into a Msg message then broadcast the packets to the port given by the organizer.", "The subscriber binds to the port, deserialize packets and either send it to the callback or publish it on the local master", "Not secure (publisher ip not checked, not crypted)", "64k maximum packet size (from IP protocol), tested only for non fragmented packets of less than 1k", "ROS Indigo", "Python 2.7"], "url": "https://discourse.ros.org/t/experimental-udp-broadcast-transport-layer/2226"},
{"title": "Introducing ROS tutorial for a new robotic platform", "thread_contents": ["Hi guys, I\u2019m Dominik, a founder of a new development platform for robots called Husarion. Our platform is ROS compatible and we created a step-by-step ", ". Check it out here:", "By the way, we just started a ", " for our robotic controller at Crowd Supply. Check it out!", "It would be great if you could let me know what do you think about the tutorial and the product we\u2019ve made. Thanks!", "Looks great! We will try it out!", "I looked over the site, looking for your explanations and \u201cabout\u201d and I still have a couple of questions:", "is your primary offering your new hardware platform or your software library?", "Other than the new CORE2 hardware, what advantage if any does your software environment provide for ROS programmers using other board?", "I would appreciate your thoughts on those questions, thank you!", "Hi Pito, here are answers to your questions:", "We are working on additional features for our cloud platform to make building and managing ROS-powered robots much easier.", "I\u2019m sorry Donowak, but I\u2019ve read the website about 2-3 times, and the reply you sent just now. What value add do you provide? I\u2019m interested in the feel and look of things you have, but I need some help clarifying the value, what are some intended uses of the product etc?", "TheDash, thanks for your question. In a few words the value of our product is as follows:", "Two new ROS tutorials are ready:", "\n7. Path planning", "\n8. Unknown environment exploration", "Check it out: ", " ", "Hi, we\u2019ve posted a new update about security aspects of our robot prototyping platform: ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "We offer the hardware, software and cloud platform. All tailored for robotics.", "Right now our software and cloud platform are designed only for CORE2 and RoboCORE controllers (it was the first generation of our hardware that we introduced on Kickstarter more than 2 years ago). In the near future, we are going to make our software & cloud compatible with other hardware controllers popular in robotics. The first step towards this is making our library created for real-time board of CORE2-ROS controller opened. Find it here: ", " - this framework is based on RTOS (Real Time Operating System), and uses advanced peripherals of a microcontroller (STM32F4) to realize multiple control & communication tasks in real-time in an efficient way.", "web IDE & remote firmware update", "all robots listed in a sinlge place", "hosting and creating web UI for your robot (especially useful in development of telepresence robots, but not only)", "web Linux console for CORE2-ROS - it\u2019s a secure connection between a web browser and your CORE2-ROS based robot", "easy sharing your robot with other Husarion cloud users or simply through a link (in this case people who open a link can control a robot through a web UI)", "hFramework library based on RTOS for real-time part of CORE2-ROS controller. It\u2019s open source: ", "\n", "Arduino compatibility layer for hFramework", "CORE2 = STM32F4 (ARM-Cortex M4, 168MHz, microcontroller) + 4 x DC motor interface (built-in H-bridge) + 4 x quadrature encoder interface + 6 x servo interface + USB + Wi-Fi + 4 x UART + 3 x I2C + SPI + CAN + 42 GPIO", "CORE2-ROS = CORE2 + ASUS Tinker Board or RaspberryPi 3", "Adapter board for LEGO Mindstorms elements", "ROSbot = CORE2-ROS + LiDAR + camera + 4 x DC motors with encoders + IMU + aluminium cover"], "url": "https://discourse.ros.org/t/introducing-ros-tutorial-for-a-new-robotic-platform/2087"},
{"title": "Petrone Drone ROS Package", "thread_contents": ["Byrobot\u2019s Petrone is one of the drones with competitive prices about $70.", "They offer a document containing Bluetooth LE protocol which can control the drone in Korean.", "\n(", ")", "So I implement a python package supporting ros and publish it on github.", "In my personal opinion, this provides better performance than other public DIY drones like crazyflie with respect to camera quality, flight controlling, etc.", "Python Implementation for Byrobot Petrone Controller Protocol to use in ROS - ildoonet/PyByrobotPetrone", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/petrone-drone-ros-package/2372"},
{"title": "New Video Recorder Package for ROS", "thread_contents": ["Recently, I published a ros project which provides video recorder for ros.", "Since I feel the inconveniences while using other existing packages for debugging and sharing result video, I created this.", "Feel free to comment! Thanks", "![image|668x500]", "\n(/uploads/ros/original/1X/a5a5f1f159d8bc84781aaaa1ace373e975a9ff06.jpg)", "\n          ", "\n", "ros-video-recorder - Image Record Package for ROS", "Looks nice. Thanks for sharing!", "Do you have any plan to debianize the package?", "Nice idea. I will try.", "Sweet job, thanks for sharing it!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-video-recorder-package-for-ros/2361"},
{"title": "Human Pose Estimation Deep Learning Model (OpenPose) ROS Package", "thread_contents": ["I have started to implement ", " as a ROS Package.", "It listens Image Topic and broadcast estimated pose as it process the image.", "See Here : ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/human-pose-estimation-deep-learning-model-openpose-ros-package/2407"},
{"title": "Amazing board CrazyPi designed for ROS with a 4k Lidar kit implement visual SLAM", "thread_contents": ["CrazyPi board based on Rockchip RK3128 processor support various boards and accessories to design Robotics and ", " project running Ubuntu and Robot Operating System - ROS.", "\nCrazyPi Kit is designed for ROS, with a 4K Lidar, implement the visual SLAM easily.", "\nHead to our Kickstarter page:", "\n", "CrazyPi Team", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/amazing-board-crazypi-designed-for-ros-with-a-4k-lidar-kit-implement-visual-slam/2421"},
{"title": "Capstone Project: Differential Drive UGV. How to use diff_drive_controller", "thread_contents": ["Hi everybody,", "\nLong time lurker, first time writing.", "\nSorry for the long post. Since I may have more questions in the future I figured I would write out a full description of our project so far, in case it may provide any assistance.", "For my Computer Science Senior Design project we are creating an automated UGV with ROS support. It runs through two wheelchair motors, 2 caster wheels, a SICK LMS100 LiDAR, a Roboteq HDC2450 Motor Driver, and a Raspberry Pi as the controlling computer, with ROS-Kinetic.", "We have an external PC on the same network that is handling the heavy computation. We are using the Hector-SLAM algorithm for localization and mapping. This SLAM algorithm is approximating and publishing our TF.", "One big issue that is being fixed right now: This first implementation had no encoders. We are in the process of ordering encoders and we will be fixing them to the exposed rear shaft of the wheelchair motors. These will connect in directly to the motor driver.", "All of our current code is located at ", "Okay! So, I was able to write some quick ROS nodes to get motor values sent from a custom-made game controller to the wheels. It drives around great but we, of course, need to now add more automation and the ros-control and ros-navigation stacks look perfect for our use.", "So it looks like the best bet for me is the diff_drive_controller but I\u2019m having trouble understanding how to implement it. I found a guide that provided instructions into how to create a basic robot class that extends RobotHW. Is this still necessary if I decide to use the diff_drive_controller or will I be, effectively, creating my own controller if I decide to go this route? In the diff_drive_controller documentation I see that joint names are a required parameter. How are these joint names listed to the robot? How does the controller know how to communicate with my motor driver? I assume I still need to create this RobotHW extension but I don\u2019t know exactly how to go about doing this. Also, is the diff_drive_controller a node that I run, once I get the RobotHW extension class running?", "I\u2019ve been reading alot lately and all of this is very new to me. I hope some of you can provide help to speed things up a little bit! I love ROS and have used it for robotic arms in the past, but creating my own mobile robot is a whole different venture.", "Thanks in advance!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/capstone-project-differential-drive-ugv-how-to-use-diff-drive-controller/2426"},
{"title": "New Gazebo plugin for ROS control to simulate control packet latency: robot_hw_sim_latency", "thread_contents": ["Dear ROS Community,", "The popular robotic simulator, Gazebo seems to lack the feature of simulating the effects of control latency that would make it a fully-fledged cyber-physical system (CPS) simulator.", "\nThe CPS that we address to measure is a robotic arm (UR5) controlled remotely with velocity commands. The main goal is to measure Quality of Control (QoC) related KPIs during various network conditions in a simulated environment.", "We implemented a Gazebo plugin <", "> to make the above measurement feasible by making Gazebo capable to delay internal control and status messages and also to interface with external network simulators to derive even more advanced network effects.", "\nOur preliminary evaluation shows that there is certainly an effect on the behavior of the robotic arm with the introduced network latency in line with our expectations, but more detailed further study is needed.", "Authors of ", " describe some early experiments in linking the OMNET++ simulation framework with the ROS middleware for interacting with robot simulators in order to get within the OMNET++ simulation a robot\u2019s position which is accurately simulated by an external simulator based on ROS. The motivation is to use well-tested and realistic robot simulators for handling all the robot navigation tasks (obstacle avoidance, navigation towards goals, velocity, etc.) and to only get the robot\u2019s position in OMNET++ for interacting with the deployed sensors.", "\nOur goal is the other way around, thus to introduce the effects of the network simulator into the robot simulator.", "The roadmap of Gazebo development shows that version 9.0 arriving at 2018-01-25 will have support to integrate network simulation (ns-3 or EMANE). Further information if this feature will be like \\citep{omnetros} or the one we propose in this paper is not available yet.", "Thanks for any feedback!", "\nBR,", "\nGeza", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Do you know how the above feature of Gazebo 9 relates to this plugin?", "Do you see any major implementation shortcoming of the plugin that should be done in a completely other way?", "With the 10 msec latency setup, the robot arm begins to vibrate. Do you know what causes this artifact and how can we eliminate it?"], "url": "https://discourse.ros.org/t/new-gazebo-plugin-for-ros-control-to-simulate-control-packet-latency-robot-hw-sim-latency/2478"},
{"title": "SMACH Projects", "thread_contents": ["Hello,", "\nMy name is Miguel, a PhD student working on improving the understanding of robotic behaviors. In particular, I am focusing in improving the visualization of SMACH, an API to write state machines. This improvement includes information about the logs of the robotic system/behavior.", "Sadly, it is difficult to run a representative (real) example of a robot behavior without an actual robot and program.", "I ask you, the community, to help me with my research. If you have a project (no matter the size) of a robotic behavior using SMACH, you can help me.", "If you have:", "After I finish my research, I will leave the visualization open for the the community to use it, with documentation on how to use it. I hope this visualization can help you all for your SMACH projects, and maybe for other projects that use state machines.", "Bests,", "\nMiguel", "We have a lot of larger, hierarchical state machines in ", "I just noticed the README is a little out of date, the robot_smach_states and robot_skills repos are merged into this bigger one and no longer separate repos.", "We do keep some logs of the execution, but I can\u2019t access them cuz\u2019 I\u2019m at work and moreover our server is still in transport after RoboCup 2017 in Nagoya.", " That\u2019s great that you\u2019re interested in SMACH. If you\u2019re looking for a SMACH project that will work in simulation, you might be interested in trying to resurrect the PR2 recharge executive. This was used as an integration test that ran fully in gazebo back in the day.", "I also recall someone surveying SMACH use in the community a while back.", "As an aside, there is a currently unreleased ", ": ", " that you might be interested in using and/or contributing to.", "I also recall someone surveying SMACH use in the community a while back.", "Yup, it was probably ", " himself: ", "I was thinking of something from a few years ago, but I\u2019m not sure if it resulted in anything.", " Also, are you aware of / familiar with RAFCON from DLR? ", "Thank you Loy, this is exactly what I was looking for.", "\nIs it possible to have those logs when the team come back?", "\nAnother question, is there a way to sim the robot and the behaviors written using SMACH?", "\nIn particular, I need an example of a big behavior running, including the smach introspection server info (for the structure of the machine and the information of the running program) and the logs of /rosout (for more information about the program itself)", "haha yep, it was me", "\nI have a visualization working but I need to test it now with real data", "If you\u2019re looking for a SMACH project that will work in simulation, you might be interested in trying to resurrect the PR2 recharge executive.", "I looked at it, but I could not make it run. I found this repo: ", " . Do you have any information on how to make that project run? For us it is more interesting because here, in Chile, we have a PR2 for ourselves.", "As an aside, there is a currently unreleased rqt_smach: ", " that you might be interested in using and/or contributing to.", "I will take a look at it, thank you!", "Also, are you aware of / familiar with RAFCON from DLR? ", "This look amazing, I have not look at it yet but I will. Thank you again", " ", " should help you to get started.", "I ran a simulation locally of the RoboCup@Home Help-me-carry challenge, a bag file of that is at ", "thanks!", "\nI will try it ", "You should check out ", " used by Team ViGIR in DRC.", "\n", "I saw FlexBE before, I have to try it, it seems really cool.", "\nDo you have a working example using FlexBE that I can try in my computer?", "You might try ", "This provides a simulation (or hardware) of turtlebot navigating with \u201ccollaborative autonomy\u201d using FlexBE.  It is a bit simpler set up than the full Team ViGIR setup that is available on GitHub.   The demo and visualization runs well on my desktop (Core i7 w/ NVIDIA card).  It runs Gazebo, RViz, and FlexBE UI in addition to several other ROS nodes, so it is best with two monitors.", "For more information : ", " and ", "thank you!", "\nI will try it", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["A repository from where I can download your SMACH ROS project and detailed instructions how to run the project on a robot. Also, detailed instructions on how to install and run the simulator of the robot so I can run the project.", "Rosbags with the information of the SMACH introspection server and /rosout that will be great."], "url": "https://discourse.ros.org/t/smach-projects/2397"},
{"title": "Build your own visual-inertial odometry aided cost-effective open-source autonomous drone", "thread_contents": ["Hello,", "\nWe are pleased to announce our open source packages that make use of all open-source and commercial products (e.g., intel ZR300 and DJI M100 \u2026).", "To make a long story short, please have a look our public repo and we would like to get some feedbacks from this forum since we found a lot of useful information from here while developing the system.", "or youTube link", "(If this post isn\u2019t relevant to the forum, please inform me. I will remove it asap)", "\nBest,", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/build-your-own-visual-inertial-odometry-aided-cost-effective-open-source-autonomous-drone/2509"},
{"title": "Intel Euclid iRobot Explorer", "thread_contents": ["A simple solution for robotics is Intel Euclid with an iRobot (create2,roomba).", "With the Euclid,  all included sensors and an iRobot,  it should be possible to prepare a simple and powerful robot explorer.", "Costs:", "\nEuclid: 400 $", "\n", "Create2: 200 $", "\n", "Roomba: 300-800 $", "\n", "Tailoo USB cable", "\n", "Or you can try to get this one:", "\n", "Clearpath Robotics partners with iRobot to provide the next generation Turtlebot based on the Intel Euclid Development Kit (Kitchener, ON, Canada \u2013 AUG 30, 2017) Clearpath Robotics, a global provider of mobile robots for research and development, in...", "\n", "All you need included.", "First picures are available:", "\n", "EuclidRoomba.JPG is publicly shared", "\n", "\n", "EuclidRoomba2.JPG is publicly shared", "\n", "\n", "Monitor.png is publicly shared", "\n", "\n", "Monitor2.png is publicly shared", "\n", "\n", "SystenInfo.png is publicly shared", "\n", "to be continued\u2026", "The thing missing from your list is the ability to power the Euclid from the base.", "The Create2 base (or Roomba for that matter) do not provide 5V power out, much less at 3A needed by the Euclid.", "\nYou can run on the Euclid battery power, but then your runtime is fairly low.", "One thing we are including in our build is a custom interface board, with microcontroller and regulated power supplies.", "Cheers!", "Hi Ilia,", "\nyes, you are completely right,", "\nI\u2019ve noticed this issue while playing with my RoombaEuclid very early  ", "\nOne more reason to buy the Clearpath solution.", "Do you know, if Clearpath has rosnodes for all this nice builtin sensors available ?", "\nGPS, compass, barometer,\u2026", "Thanks for your suggestion and feedback", "\nCheers", "\nChrimo", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/intel-euclid-irobot-explorer/2569"},
{"title": "Urdf-viz: URDF file viewer", "thread_contents": ["Hi, ROS Users!", "I released ", ", URDF viewer.", "How do you check your URDF files?", "\nlaunch rviz? Isn\u2019t heavy?", "urdf-viz is offline, single binary to visualize urdf file.", "\nIt supports not only moving each joint, but moving arms using inverse kinematics.", "\nAll mesh types are supported.", "See some images", "\n", "\n", "\n", "I\u2019m happy if you try ", " and tell me some issues on github.", "\nI know the GUI interface is not good enough, but it will take more time to fix it.", "Because ", " is written in ", ", it is not installed by", "\napt-get install ros-*.", "\nPlease read ", " to try.", "I think Rust is suitable language for robotics, and started some projects for robotics.", "\nurdf-viz is one of them.", "I already wrote ", ", ", ", ", ", and ", " in Rust.", "Hi,", "Any chance you could post some known-working URDF examples to the repo? urdf-viz is crashing on our URDF, but It\u2019s pretty likely we have errors. It\u2019d be nice to verify that urdf-viz is working as expected on known-good files.", "Thanks,", "\nRick", "P.S. Neat idea for a much-needed tool!", "Thank you for trying.", "I want to try your URDF to check urdf-viz.", "I tried almost all *_descrpition packages on Ubuntu16.04, kinetic.", "\nfor example,", "and other projects\u2026", "but I think this is not enough. (kinetic has less packages than indigo.)", "If you can publish your model, or give me your description package to me, I can help you.", "urdf-viz failed to show thormang3 model which was released for kinetic.", "\nBelow change shows why it failed.", "The crate which is used to parse URDF, which is ", ", is stricter than normal ROS urdf parse programs (and it is difficult to debug).", "\nIt does not allow multiple elements which should be only one.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["pr2_description/", "pepper_description/", "nao_description/"], "url": "https://discourse.ros.org/t/urdf-viz-urdf-file-viewer/2472"},
{"title": "Tangobot (Android-based autonomous robot)", "thread_contents": ["Dear ROS users,", "Have you ever thought of making an Android-based robot, making use of all the great sensors and computational power that phones have, while leveraging all the tools which you already know from the amazing ROS ecosystem? What better than a Tango-powered phone, with it\u2019s added visual odometry system and depth sensing camera?", " is pleased to announce the ", " project: a simple Turtlebot-inspired robot which can do autonomous navigation and obstacle avoidance powered entirely by a Tango-capable Android device \u2013 readily available from ", " (but also fully open-sourced on ", ").", "The main goal of this project is to make it easier for developers to build robot-side ROS applications for Android.", "\nFar from being a complete or fine-tuned solution, it brings together all the components of the standard ROS navigation stack - with the addition of using odometry and depth sensing from Tango - into a single Android application.", "\nThe application has been developed in close cooperation with ", " and with support from Google.", "The main focus of this initial version was only to make all the components work together.", "\nYou can tele-operate the robot as well as have it navigate autonomously around obstacles using an empty map. You can use the existing Android \u201cROS Map Navigation\u201d app to drive it and have a 100% Android experience.", "But this is only the starting point, open for the community to extend and improve. Some ideas for future extensions include: supporting additional mobile bases, configuring mapping and localization using Tango Area Learning or loading existing static maps, coming closer to parity with standard Turtlebot tutorials and developing more client Android applications for interesting use cases of the technology.", "In case you are curious and want to hear more, we will be presenting this project at ROSCon as part of the \u201cDeveloping Android Robots\u201d presentation on Friday at 11:50am.", "We hope that interested developers will be able to use this as a starting point to build their own Android robot applications and libraries, or to extend this initial Tangobot configuration with additional readily-available capabilities.", "As usual within ROS, this is an open source package available in Github and has a tutorials page available in the ROS Wiki. We welcome bug reports and appreciate contributions.", "We hope you will all find this new project useful.", "The Ekumen Apps Team", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tangobot-android-based-autonomous-robot/2660"},
{"title": "Introducing Move Base Flex at ROSCON", "thread_contents": ["Hi folks!", "\nWe (", ", ", ") will introduce Move Base Flex (MBF) at ROSCON next week. MBF is a backwards-compatible replacement for move_base that overcomes many of its limitations, but still can use existing plugins. The key features are:", "Though we are still actively developing it with an interesting list of new features we want to incorporate soon, as replace current costmap_2d with ", ".", "We already provide a ", " (", "), though at ROSCON we will show more advanced use with Behavior Trees.", "Hope to see many of you at ROSCON and hear your comments/critics/suggestions/feature requests!", "Resources:", "Sounds very interesting. Eager to watch your talk!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["exposes action servers for planning, controlling and recovering plus access to the costmaps, allowing an external executive to implement smarter  navigation strategies", "abstracts a generic navigation framework that can be easily extended into particular implementations (in our implementation move_base is just an specialization of this abstract framework)", "\n", " (first release expected in October)"], "url": "https://discourse.ros.org/t/introducing-move-base-flex-at-roscon/2665"},
{"title": "Pheeno: A Low-Cost, ROS-Compatible Swarm Robotic Platform", "thread_contents": ["The field of swarm robotics has grown rapidly in recent years, propelled by technological advances that have enabled the development of new robot platforms that can be deployed in large numbers. However, most of these robots are too expensive for use by researchers and educators, or they do not have ROS support. We at the Autonomous Collective Systems Laboratory would like to introduce our new open-source, open-hardware swarm robotic platform ", " and the ROS packages that we have developed for single or multi-robot applications.", "A Pheeno robot costs about $260 dollars to make with PCB fabrication, 3D printing, and other commercially available components. Pheeno is capable of modular additions such as omnidirectional wheels for holonomic motion, additional sensors, or an LED light ring. We are currently developing a gripper addition that will be released soon! All of our hardware and electronic schematics are provided in our GitHub page.", "On our GitHub page, we offer two ROS packages, ", " and ", ". Currently, our packages support both ROS Indigo and Kinetic. The ", " package contains setup code, tools, and examples for running ROS on a single Pheeno. ", " contains a basic Gazebo model of Pheeno, example code, and a simple testbed model for single-robot and multi-robot 3D simulations. The same code written for the Gazebo simulations can also be run on Pheeno robots in real-world experiments.", "To help new users, documentation is provided for building, programming, and developing controllers for Pheeno. The documentation has sections geared towards new users to learn the basics of ROS. We continually update our documentation with new features and guides.", "We hope everyone can come and take a look! If you have any questions about the platform, please feel free to post or e-mail me!", "\n", "\n", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pheeno-a-low-cost-ros-compatible-swarm-robotic-platform/2698"},
{"title": "ROS for GeoFencing Applications - ROSCon2017 Lightning Talk", "thread_contents": ["This is a transcript of the Lightning Talk by Bill Coon of RoadNarrows at last week\u2019s ROSCon 2017. Please let us know if you have any questions or if you\u2019d like to be posted on our release progress. I\u2019d love to hear from anyone else doing drone or geofencing applications. - Thanks! -", "\n", "The GeoFrenzy ROS package under development by RoadNarrows Robotics allows UAV developers to easily integrate functionality of the Fence Delivery Network developed by GeoNetwork\u2122.", "The Fence Delivery Network is a set of virtual fences that are associated with various permissions and entitlements for UAVs, maintained and stored in the cloud and served up dynamically to clients that can autonomously react appropriately in accordance with any geospatial regulations.", "Through the GeoFrenzy Portal web interface, users are able to create virtual fences and apply sets of rules that UAVs are required or encouraged to abide by. For example, a residential property owner may set up a virtual fence around their yard, and restrict a drone from operating any downward facing cameras within the fence perimeter.", "The GeoFrenzy ROS package we are developing makes interfacing with the Fence Delivery Network and implementing behaviors that adhere to fence entitlements an easy task. The GeoFrenzy Server node subscribes to a NavSatFix message and queries the FDN via the Fencing Agent library and publishes topics relevant to any fences that are nearby. Our GeoFrenzy Virtual Cloud node subscribes to fencing topics and publishes a PointCloud message that represents a 3D scan of any nearby fences, so it is easy to incorporate the virtual fences into a navigation stack as if they were physical barriers. This is useful in the event that a fence has a No-Entry or No-Exit rule. Our Map Server node publishes an occupancy grid version of the fences that provides the same convenience for 2-dimensional navigation stacks. The GeoFrenzy sensor relay node and associated sentinels can be configured to watch specific entitlements such as \u201cCamera Permitted\u201d and automatically emit information for corresponding subsystems to respond to.", "The Alpha version of this package is publicly available on GitHub. It is still under development and a Beta version will be released in the near future.", "We hope to contribute to the progression of employing UAVs for convenience, entertainment, research and more in a safe and sane manner.", "\n", "\n", "GeoNetwork provides vital solutions to a world where drones, robots and autonomous devices are increasingly immersed in the fabric of everyday life. We enable all society (governments, businesses, individuals) to express rules of behavior expected from these smart devices as they transit our spaces\u2014and a means for these devices to comply.", "Using GeoNetwork\u2019s unique SmartFence\u2122 solution, geofences are created for any 3D geometry\u2014air, sea, and land\u2014along with associated rules, and published to edge caches around the world ready for smart device consumption. Our ID & Monitoring solution is suitable for even small autonomous air and ground vehicles.", "RoadNarrows Robotics\u2122 has been developing and providing robotics solutions to the academic and research communities since 2002.  We are a strong believer of open software and common interfaces. Our Hekateros robotic manipulator, Laelaps wheeled robot, and other ROS-enabled platforms have been architected and integrated by our team. RoadNarrows has been working with GeoNetwork on the ROS implementation of the GeoFrenzy project.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-for-geofencing-applications-roscon2017-lightning-talk/2749"},
{"title": "[ADLink DDSBot] The ROS 2.0/1.0 based robots swarm architecture", "thread_contents": ["Hi all,", "We are happy to announce the release of a ROS 2 based robots swarm projects held in ADLINK Technology/PrismTech.", "The typical solution for swarm robots in ROS 1 is using either different namespaces or external communication (outside of ROS middleware). However, by means of ROS2/DDS capability, we can solve this issue more effectively. As shown in figure below, each robot in our architecture is fully-independent, self-awareness and fully autonomous(by navigation stack). Then each robot publishes its global position to ROS2 Topic Layer through ros1_bridge. Therefore, all the others will be aware of its position with the help of custom \u201cswarm_costmap_layer\u201d. The swarm network allows robots to join and leave dynamically without human involving or any beforehand configuration. Furthermore, the developer/user is able to monitor&control all robots through a ROS 2 node.", "\n", "The demo video:", "\n", "Github link:", "\n", "\nIncluding all required files, tutorials, simulation and ready-to-go Pi3 image(ROS Kinetic & r2b2 with opensplice)", "In addition, the robot in our experiments, called DDSBot, is a extremely low-cost platform. The total cost of each robot is lower than 250USD (inc, lidar, Pi3, battery). All necessary document, firmware/software source codes are opened and under Apache 2.0 License.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/adlink-ddsbot-the-ros-2-0-1-0-based-robots-swarm-architecture/2753"},
{"title": "Soft body robot simulation with bullet physics", "thread_contents": ["I have a bullet physics wrapper ros node that allows soft body structures to be anchored to rigid bodies and simulated, this example shows a skid steered vehicle with squishy cubical wheels (for no real reason other than the cubes are easier to generate):", "The soft bodies in that example don\u2019t have surfaces, in rviz the node points and the link lattice connecting them to each other is visualized, also the anchors to the \u2018motor\u2019 rigid bodies are shown as lines as well.", "The rigid and soft body compounds are spawned through service calls, and in the example a python script generates the wheels and chassis.  Every joint/constraint creates a rostopic that allows it to be controlled, in the video separate velocities are being published to each wheel.", "The soft bodies don\u2019t communicate anything other than rviz Markers to ros, though I think bullet provides a center of mass (and orientation?) that could be made into a tf frame.  The rigid bodies positions are broadcast as tfs.", "Currently face (3-node) and tetra (4-node) structures aren\u2019t fully supported, just links (2-node).", "The source is here:", "simple_sim_ros - Minimally featured but fast ROS physics simulation wrapping bullet", "The idea of the bullet wrapping is that it hides nothing about bullet (though only a small subset of the api is exposed) so it can be a platform to experiment with it (and also the simulation boots in about a second and dies with ctrl-c just as fast).", "The main branch is pure rigid body simulation (", " shows off a Stewart platform, support for closed chains demonstrated), later it will be possible to specify whether the sim should support soft bodies or not at launch time.", "I was inspired by the the ", " to add more features to ", " to create a soft material grasping demonstration:", "It takes about 60% of a cpu core so the ability to run faster than real time is limited (though running in parallel is easy).  Running with a lower fidelity sphere or skimping on simulation steps isn\u2019t recommended with soft body simulation, the most likely result is either instability, object penetration, or bodies that move without any external forces other than gravity.", "A ", " takes much less cpu so can be made to run possibly at 10x real time.", "Since I started this project the Bullet Physics Library has added a lot of python support and made a client server architecture that might make a lot of what I\u2019ve done redundant, but there still needs to a be (an even more) thin layer to turn bodies into rviz Markers for visualization, publish their positions into tf, and spawn and manipulate them through service calls and topics.", "I\u2019d like to next turn the gripper into a five fingered hand, and try anchoring soft body finger tips to each to see how well grasping a soft body with another soft body works, as well as grasping a rigid object.", "A Bullet feature unimplemented in bullet_server is the fracturing capability, it would be interesting to make rigid bodies that will shatter under too much strain and see how well the grasping works.", "Hi ", ". Great to see my talk inspired you to do something this cool ", "I\u2019d love to get in touch to see how we can collaborate on moving this forward!", "Sure- you can pm me here on discourse if needed, also take a look at the github repo and feel free to make issues for anything big or small that is missing or broken.  I imagine better motion control and force sensing are required to make this more usable.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/soft-body-robot-simulation-with-bullet-physics/834"},
{"title": "Pushing ROS to get standardized - ROSCon 2017 Lightning Talk", "thread_contents": [", the documentation that will describe how to create standard robot modules.", "I would like to share with all of you the Lightning Talk I made at ROSCon 2017: the transcript + slides. Feel free to ask any questions or give your feedback.", "Thanks!", "Good evening, I am Irati Zamalloa, an industrial design engineer from ", ".", "As all of you know, we are working on H-ROS, the ", ", to offer the world standard modules for building robots.", "The standardization require that every detail fit like a tetris, so I would like to introduce one of the key pieces, the information model.", "It is a set of rules and specifications to create an standard robot modules.", "H-ROS remove components integration, we eliminate the necessary effort to achieve communication between devices, almost 70% time over the total.", "If we translate this to ROS, we need all the modules to contain the same topics, messages services, actions and parameters, to make sure that all of them speak the same language at all levels.", "And the information model is the document that describes all the details.", "We are pushing ROS to get standardized. We believe in ROS, but we also believe that it needs a step more, and get the next level of acceptance. That is why we are working actively with international organizations like ISO or OMG.", "It will help to build a solid infrastructure from the ground up and make the H-ROS information model a reference document to create standard robotic modules.", "Thank you for your time! And if anybody wants to know more about the ", ", come talk to me! Thank you!", "I saw your talk at ROSCon, this is an interesting effort and I\u2019d like to know more about it.", "First of all, what is it that you are trying to get standardized?  Is it the workings of ROS 2 on top of DDS?  Is it parts of H-ROS?  Some combination of both?  I would be interested in the scope of what you\u2019re trying to accomplish, as well as seeing what kinds of documents you\u2019re generating in support of this standardization effort.", "Secondly, you mentioned working with ISO and OMG.  Is this happening in the open, or is it just talks between your company and the standards bodies at this point?", "Anything you could share or expand upon would be appreciated.", "Thanks,", "\nRich", "Hi Rich,", "Thank you for your interest in our standardization process. This work is being pushed by our engineers to a number of OMG and ISO (slow process which involves getting parts of our Information Model at the core of these documents that are being written) working groups. A good example is the work that\u2019s happening in ISO 299.", "The Information Model standardizes ROS messages and it\u2019s written using ROS messages syntax. It could be implemented with different middlewares (DDS is only one option, you could use other choices). With H-ROS we are aiming to create standardized robotics hardware, so, the Information Model is the baseline which ensures that all the modules are able to interoperate (even from different manufacturers, you will only have to connect them together). Accordingy, part of the Information Model is created specifically for H-ROS, but I have to say, that I\u2019m taking care of making it as modular as posible, so that in the future, you can all take advantage of this work. We are working very hard to open the Information Model as soon as posible, please stay tuned.", "Although it\u2019s still a work in progress, our current implementation of the Information Model is based in ROS 2 however we\u2019re interested in getting contributions from the community to port these same abstractions to ROS and other alternatives. Would you be open to collaborate with this? If so, could you share how much time you could commit?", "Thanks,", "\nIrati", "It sounds gorgeous, I\u2019m curious that what the Information Model will really standardize and how to apply that to address gap among various robotics h/w components ?  thank you !", "Hi Roser,", "I\u2019m curious that what the Information Model will really standardize", "The Information Model standardizes logical middleware interfaces between components (or we try to!). We are working on creating a modular interface assigning at least a basic message that covers the main objective of the device. For the other characteristics we use optional messages, so the user wil be able to work with the 100% of possibilities that the component offers.", "how to apply that to address gap among various robotics h/w components ?", "We do this through H-ROS. ", " is the infrastructure for creating interoperable hardware and all that this entails, where, the Information Model is part of the infrastructure.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pushing-ros-to-get-standardized-roscon-2017-lightning-talk/2812"},
{"title": "HyphaROS RaceCar: high speed, low cost autonomous 1/10 racecar (3m/s 600USD)", "thread_contents": ["Hi all,", "After hard working for half of a year, we have successfully replaced all expensive components on MIT RaceCar architecture with low cost choices and still keep the high speed capability on our platform. Instead of using encoder and stereo VO to estimate the odometry, our platform adopt the combination of laser odom generated by RPLidar-A1 and yaw measuement from gy85 IMU with fine tuning EKF fusion.", "\nThe total cost of our platform is only around 600 USD (inc. lidar, car, battery, etc)", "\nThe Youtube link for 3 m/s demo video: ", "\nIn addition, a step by step tutorial for beginner now is available on our github:", "\n", "\nit includes from hardware assembling to software installation and operation.", "Currently, we are integrating teb_local_planner on our platform in order to achieve auto-parking capability and trying to add low-cost VIO to obtain better odometry info.", "Please feel free to leave any suggestion below or open an issue on our github.", "Cheers!", "Note:", "\nHyphaROS Workshop is one of the most famous group in Taiwan for ROS related activities. We have organized several ROS based workshops and are planning to hold a small competition for auto-racecar in public by the end of this year.", "Website: ", " (in Chinese)", "\nFacebook: ", "Woooooooooooooooow, great", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/hypharos-racecar-high-speed-low-cost-autonomous-1-10-racecar-3m-s-600usd/2869"},
{"title": "ROS libraries for the Donkey RC car platform", "thread_contents": ["Hi all, Donkey Car \u2013 ", " \u2013 is a DIY 1/16th scale RC robot car platform equipped with a Raspberry Pi 3 and a monocular camera. It\u2019s easy and inexpensive to put together, and the project is actively maintained by a relatively large community.", "The platform is designed to be a single purpose autonomous racer, but I have found it useful for more general experimentation. For this reason I\u2019ve ported the custom python controller scripts to ROS and created a ready-to-run Ubuntu MATE disk image. The image includes all ROS packages necessary to access the camera and send commands to the vehicle.", "Links to the code repository and disk image can be found here on right side of the page: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-libraries-for-the-donkey-rc-car-platform/2977"},
{"title": "ROS Openlighting for interactive art", "thread_contents": ["So a few years back I decide to build an interactive lighting installation using ROS this resulted in a ton of fun and a pile of code. If there is an interest in these types of tools it might encourage me to finally publish theses as package.", "What it does(videos):", "How does it work:", "\n0. ", "I hope this helps others build cool interactive projects.", "I would be interested, looks cool\u2026 what type of sensors are you using?", "The ", " node can support any PointCloud2 and publishes:", "hey alan, this is great work. where can i buy it? ", "cheers", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Large scale interaction", "Small reactive", "\n", " leveraging the ", "\n", "4 PointCloud2 topics\n", "[ ", "]", "\n", "Visualization markers: ", "\n"], "url": "https://discourse.ros.org/t/ros-openlighting-for-interactive-art/2979"},
{"title": "ROS-A Microtractor build with Open Source Ecology", "thread_contents": ["You can discover more about a person in an hour of play than in a year of conversation. - Plato", "Visit the post for more.", "\n", "\nMicrotractor: A tracked machine that is similar in functionality to a Toro Dingo brand utility machine but has more power. We will use a 16 hp Power Cube \u2013 our hydraulic power unit. Because our build system is modular, we can add a second Power Cube to this tractor for a total of 32 hp for more demanding tasks.", "Interested in learning more about the project?  Next online community meeting is Tuesday October 10th at 6:00 pm PST.", "Here is a link to the Micro Tractor wiki:", "\n", "Join us for our online meeting.  We will be discussing options for automating their tractor with ROS using a RaspberryPi.", "When", "\nTue Oct 10, 2017 6pm \u2013 7pm Pacific Time", "Just want to dial in on your phone?", "Call one of the following numbers:", "\nAustralia: +61.8.7150.1136", "\nBrazil: +55.21.3500.0112", "\nFrance: +33.1.84.88.6478", "\nGermany: +49.89.380.38719", "\nJapan: +81.3.4510.2372", "\nSpain: +34.932.205.409", "\nUK: +44.121.468.3154", "\nUS: +1.512.402.2718", "If you would like to join the Slack channel discussion please email ", ".", "Here is a better introduction:", "Join us for our online meeting.  We will be discussing options for automating their tractor with ROS using a RaspberryPi.", "Big thanks to Ian McMahon for making the Gazebo model ", "When", "\nTue Oct 17, 2017 6pm \u2013 7pm Pacific Time", "Click the following link to join the meeting from your computer: ", "MicroTrac build starts this Thursday Oct 26th.", "Join us for our online meeting. We will be reviewing the automation of their tractor with ROS using a RaspberryPi.", "When", "\nTue Oct 24, 2017 6pm \u2013 7pm Pacific Time", "Click the following link to join the meeting from your computer: ", "Here are a couple pictures of the MicroTrac build.", "\n", "The new tractor is out driving today.", "\n", "Great time working with Open Source Ecology.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-a-microtractor-build-with-open-source-ecology/2830"},
{"title": "OpenR2 ROS", "thread_contents": ["I\u2019ve started a full ROS project for the true automation of an R2-D2 unit.", "\nWe\u2019ve purchased a licensed replica from Side Show but I want to hack it into a full functional screen accurate robot.", "\nWe have a complete set of 1:1 scale blueprints from the original film production.", "\nWe also have purchased and laser-scanned an original dome, radar eye, and holoprojectors.", "\nThrough Q4 we will be creating accurate mesh files for use in simulation in RVIZ and Gazebo.", "I\u2019m excited to see this project develop! Do you have a github repo for the project?", "Very cool! This has also been a dream of mine, which you might have guessed if you ", ".", "For those interested, there is also this video from a few years back.", "\n", "I actually spoke with Bjorn yesterday about any progress he\u2019s made on his R2 ROS project. He said life has gotten in the way and at this point his ROS work is \u201cancient history\u201d as he put it. I was hoping to corral him into the project.", "I actually started with the URDF tutorial. Lol. If you created the tutorial then it looks like your already all in on the project whether you were aware of it or not. ", "It looks really amazing. Do you pretend to release the code? Do you have any repository? If you need it I might help you with it ", "Congratulations by the way ", "We could use any help we can get. The plan is all the code would be Open Source including all the geometry files.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/openr2-ros/3117"},
{"title": "Trinibot - a tiny desktop research robot based on ROS", "thread_contents": ["Hi ROS Community,", "I started a year and half ago building a ROS enabled tiny robot that costs a quarter of other equivalent robots and that has all the goodies for mobile robotics research right here on my desk.", "Besides the usual interruptions and a ton of lessons learnt with low cost hardware, I\u2019m proud to share my results with you, Trinibot V2.0!", "\n", "\n", "\n", "Here\u2019s what she has:", "ROS functionality (so far)", "She runs for about 2 hrs with stops and constant sensor streams over wifi. I still need to redefine the covariances for the various sensors (and a lot of other tests of the entire stack).", "The next steps are:", "\nURDF", "\nNavigation stack", "\nInteractive functionality", "Keen to hear thoughts, questions and critique that will make Trinibot awesome and community friendly!", "Cheers,", "Kesh", "Looks like the depth camera is no longer available. A comment on Amazon said Sony bought Softkinetic and took the cameras off the market. They may be only selling OEM units to include in commercial projects.", "Indeed the DS525 went out of production last month! ", "  I picked it up last year and it was a big disappointment because the camera was well made. In any case I have 4 spare so I\u2019ll survive ", "I\u2019m considering moving to a x86 board with USB3 at some point. Need to find a nice RGBD camera option.", "This seems like a nice robot. I generally prefer larger bots, but as I\u2019m now living in a motorhome I\u2019m going to have to get used to smaller ones.", "The description sounds like this would be useful. I would have added an external I2C port on the controller in order to connect to new low-level sensors without having to work through the USB port.", "You were able to put a lot of robot into a small package.", "I\u2019ve been looking at RGBD cameras so was disappointed to see this one discontinued.", "Thanks ", ", I do have an I2C port that can be exposed. Any specific sensors you have in mind? I can provide a data and mechanical interface to mount these sensors.", "You\u2019re welcome. I tend to buy odd sensors from Sparkfun or Adafruit.", "\nSparkfun has a \u201cstandard\u201d that can be used for I2C sensors. However,", "\nsince most of the sensors that I use have their own quirks I mostly just", "\nwire things myself. Most of them use either 5v or 3.3v. Adafruit is", "\nusually better at supporting a wider range of voltages by putting", "\nvoltage regulation on the sensor boards.", "I use these companies so that I can avoid having to deal with very small", "\nsurface-mount components and have access to some components that would", "\nbe difficult to get or use. Plus I\u2019m better at software than hardware. ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Quad core RPi3 running ROS Kinetic on Ubuntu Mate 16.04", "Softkinetic DS525 Time of Flight - 0.15 to 1m @ 320x240 with a 720p RGB camera", "Filtered IMU and magnetometer", "Temperature and humidity sensors.", "In-built 3.6Ah 12V rechargeable battery", "WiFi and Bluetooth", "Additional USB port and DC 5V socket for peripherals", "8 channel 12 bit ADC for analog sensors", "A joystick for some interaction", "A 8x8 RGB panel to generate markers and bitmaps", "Differential wheel rate control from 1 to 10cm/sec and 0.2 to 1.57 deg/sec with odometry", "Two additional motors (open-loop) for a robot arm or other actuation", "Can be ported to other robot differential wheel chassis with DC servos upto 12V 1A.", "Develop and test while charging!", "Full TF tree", "Sensor messages", "GMapping", "EKF localization"], "url": "https://discourse.ros.org/t/trinibot-a-tiny-desktop-research-robot-based-on-ros/3177"},
{"title": "Autonomous navigation using navigation stack + hector_slam", "thread_contents": ["Hello everyone!", "\nive got a question regarding navigation stack together with SLAM and path planning.", "\nMy setup: 4 Wheeled Robot with Rplidar A2 mounted on top. No Odometry at all \u2026 .", "Right now hector_slam works and i can create a map as well as localizing my robot in this map thanks to my Lidar (SLAM). Now i want to combine hector_slams /map with move_base and navigation stack.", "\nAs i understood, move_base needs odometry information for the navigation part. What parameters do i have to tweak so hector_slam can publish the needed odometry information? when i run hector_slam together with move_base i still cant declare paths in rviz.", "\nregards", "\nVladi", "Thanks for your question. However we ask that you please ask questions on", "\n", " following our support guidelines:", "\n", "ROS Discourse is for news and general interest discussions. ROS Answers", "\n", " provides a forum which can be filtered by tags to", "\nmake sure the relevant people can find and/or answer the question, and not", "\noverload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/autonomous-navigation-using-navigation-stack-hector-slam/3246"},
{"title": "ROS Object Analytics Release - V0.3.0", "thread_contents": ["Hi All,", "\nWe are happy to announce the initial release of the Object Analytics packages. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. It consumes ", " data delivered by RGB-D camera, publishing topics on ", ", ", ", and ", " in 3D camera coordination system. Packages and features provided by below 4 projects with tag v0.3.0:", "We have tested on RealSense D400 series camera, Astra camera, and Microsoft Kinect v1 camera. See README of each project for how to setup the environment. Welcome feedbacks and participation.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\no ", ": ROS publisher and tests for object tracking in a ROS ", " message, and for object localization 3D in a ROS PointCloud2 message from an RGB-D camera. ", " is adopted for object tracking, with regular rectification from object detection results. Object segmentation over pointcloud2 image locates the detected objects in 3D camera coordination system.", "\no ", ": ROS message interface for the object analytics features", "\no ", ": ROS launch interface for the overall object analytics and its nodelet sub-modules", "\no ", ": ROS sample app with visual outputs for the results of object tracking and object localization 3D", "\n", "\no ", ": ROS service, publisher, and tests for object inference in a ROS Image message from an RGB camera. Yolo2 model fitting into ", " is adopted for object detection", "\no ", ": ROS sample app with visual outputs for the results of object detection", "\n", ": ROS message interface common for all object detection packages of various detection framework and hardware", "\n", ": ROS launch interface common for all object detection packages of various detection framework and hardware"], "url": "https://discourse.ros.org/t/ros-object-analytics-release-v0-3-0/3283"},
{"title": "Netatmo Weatherstation node", "thread_contents": ["Hi all,", "I\u2019ve developed a little node for sending Netatmo Wetherstation data to ROS. You can find it in ", ".", "I\u2019m using it in a personal project, but it may be useful for somebody.", "Enjoy ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/netatmo-weatherstation-node/3342"},
{"title": "Raspberry Pi Raspbian Stretch Lite ROS + OpenCV Disk Image", "thread_contents": ["Hi All,", "For those who don\u2019t want to / can\u2019t use Ubuntu Mate with their Raspberry Pi ROS project, I created an image preloaded with ROS and OpenCV 3.3.1 installed.", "Roscore is automatically started at bootup via systemctl (so you don\u2019t even need to log in to start master).", "You can find the image through the ", " - please give us a Medium \u201cclap\u201d or two or 10 to help spread the word!", "BTW - this is (small) progress towards a budget ROS robot that I\u2019m designing - aka ", ".  I\u2019m looking for people who want to collaborate so feel free to reach out!", "Thanks!", "\nJack", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/raspberry-pi-raspbian-stretch-lite-ros-opencv-disk-image/3399"},
{"title": "Dynamic object and reading out coordinates", "thread_contents": ["Hello forum members,", "My name is Koen and I am a student in mechatronics. I am currently programming a UR5 robot in ROS for a school project. The objective is to move to robot from point A to B, but when an object passes through the trajectory, the robot must stop or reduce speed.", "I am working together with people who do the vision part. They programmed a QR code detection and this QR code will be  the object in Rviz. The vision system has to detect the object and let the robot know if the object is in its trajectory or in its work space. The vision group is currently not far enough to test the system so I would like to use a dynamic object which is moving through the area. I am able to create a fixed object, but to avoid this object it is not a big challenge. I would like to have a dynamic object to test this system. It could be a rectilinear movement or a unpredictable movement. Is this possible to get in ROS and are there already tutorials available for this?", "Also I would like to read out the coordinates of the QR code. These read out coordinates can then be passed on to the UR5 so that they can avoid the object. A kind of real time tracker.", "Are the above issues possible to solve and are there tutorials available? It would help me a lot to finish my school project.", "Very kind regards and thank you in advance.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/dynamic-object-and-reading-out-coordinates/3408"},
{"title": "ROS# on github.com/siemens/ros-sharp", "thread_contents": [" with ROS from within your Windows app:", "Subscribe and publish topics, call and advertize services, set and get parameters and use all features provided by rosbridge_suite. (See code example below.)", " your robot\u2019s URDF model as a Gameobject in Unity3D (", ").", "Import the data either directly from the ROS system using the robot_description service or via a URDF file that you copied into your Unity Asset folder.", " your real Robot via Unity3D (", ")", " your Robot\u2019s actual state and sensor Data in Unity3D (", ") (see image below).", " your robot in Unity3D with the data provided by the URDF and without using a connection to ROS (see video and post below).", "Beside visual components as meshes and textures, also Joint parameters and masses, CoMs, Inertia and Collider specifications of Rigidbodies are imported.", "Please do not hesitate to try it out yourself and to get in touch with us!", "\nWe are very interested in your feedback, applications, improvement suggestions, and contributions!", "ROS# Development Team (", "), Siemens AG, Corporate Technology, 2017", "A simple Publisher/Subscriber Example:", "My post above was limited to 5 links and 2 pictures only ", "\n\u2026here is some additional media :", "(2)", "\n", "\n(3)", "\n", "We get Texture2Ds in Unity which can be converted to Jpeg or Pngs and further base64 encodings. Can these video feed be transferred over RosBridge to convert it into an image topic?", "Hi pushkalkatara,", "\nyou opened an issue on our Github project site with the same question yesterday. We already gave you an answer there:", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ", a .NET API to ROS using rosbridge_suite on the ROS side.", "\n", ", a URDF file parser for .NET applications.", "\n", ", a Unity Asset package providing Unity-specific extensions to RosBridgeClient and UrdfImporter.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", " ROS# is useful for a wide variety of applications. Think about Machine Learning, Human-Machine Interaction, Tele-Engineering, Virtual Prototyping, Robot Fleet Operation, Gaming and Entertainment!"], "url": "https://discourse.ros.org/t/ros-on-github-com-siemens-ros-sharp/3405"},
{"title": "ROS Intel Movidius NCS Release \u2013 V0.5.0", "thread_contents": ["Hi All,", "We are happy to announce the v0.5.0 release of ROS Intel Movidius NCS package.", "\nThe Movidius\u2122 Neural Compute Stick (NCS) is a tiny fanless deep learning device that you can use to learn AI programming at the edge. NCS is powered by the same low power high performance Movidius\u2122 Vision Processing Unit (VPU) that can be found in millions of smart security cameras, gesture controlled drones, industrial machine vision equipment, and more.", "\nThis project is a ROS wrapper for NC API of NCSDK, providing the following features:", "\n\u2022\tA ROS service for object classification and detection of a static image file", "\n\u2022\tA ROS publisher for object classification and detection of a video stream from a RGB camera", "\n\u2022\tDemo applications to show the capabilities of ROS service and publisher", "\n\u2022\tSupport multiple CNN models of Caffe and Tensorflow, including", "\no\tCNN models for object classification", "\n\uf0a7\tAlexNet", "\n\uf0a7\tGoogleNet", "\n\uf0a7\tSqueezeNet", "\n\uf0a7\tInception_V1", "\n\uf0a7\tInception_V2", "\n\uf0a7\tInception_V3", "\n\uf0a7\tInception_V4", "\n\uf0a7\tMobileNet", "\no\tCNN models for object detection", "\n\uf0a7\tMobileNet_SSD", "\n\uf0a7\tTinyYolo_V1", "\nThis project has been open sourced in github: ", ". Please refer to README file for more details about this project. We have tested it on RealSense D400 series camera and Microsoft HD-300 USB camera. Welcome feedback and participation.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-intel-movidius-ncs-release-v0-5-0/3530"},
{"title": "Industrial_ci 0.5.0 available. ABI compatibility check, catkin_lint, ARM support etc", "thread_contents": ["Newest version 0.5.0 of ", " is available now. Notable changes: ABI compatibility check feature, ", " feature, and ", " platform support. Check out changelog for more detail:", "Current users who use it via ", " as recommended don\u2019t need to do anything to get an update.", "\nNew users can start from ", ".", "Your issue reports and requests for industrial_ci are welcomed, no matter how insignificance you think it is (it\u2019s NOT trivial for us in most cases). Feel free to ", ".", "Isaac", " 0.5.1 just released. This includes a workaround for a recent upstream regression of ", " (", ").", "\n", " users may need to take action:", "Have a carefree holidays with CI in your projects!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["[fix] ROS Prerelease Test by updating to the latest Docker CE ", "\n", "[capability] ABI check based on abi-compliance-checker ", "\n", "Follow ", " for adding ABI check to your packages.", "\n", "[capability] catkin_lint feature. ", "\n", "[capability] implemented ROSDEP_SKIP_KEYS feature ", "\n", "[enhance] Add support for ARM platform ", "\n", "Many more bug fixes.", "Contributors: Mathias L\u00fcdtke, Miguel Prada, Isaac I.Y. Saito", "No action required for ", " users (both public and private versions).", "All users on ", " who uses SSH forwarding need to add a small change in their ", ".\n", "Install ", ". See ", ".", "\n"], "url": "https://discourse.ros.org/t/industrial-ci-0-5-0-available-abi-compatibility-check-catkin-lint-arm-support-etc/3403"},
{"title": "Project Wayfarer", "thread_contents": ["Starting to work on a project I\u2019ve dubbed Wayfarer. My community has a large spread of concrete hike and bike paths that wander through neighborhoods and forested areas. My goal is a robot that can autonomously navigate these pathways.", "The chassis is a ", ", the same as I used in the NASA Sample Return Robot (SRR) challenge. The current plan is to use an", " (quad core  64 bit Intel\u00ae Atom\u2122 x5-Z8350 Processor 1.92 GHz as the main brain. This came in a Intel RealSense Kit with an R200 RealSense. Originally was going to use multiple Raspberry Pi 3s but the Up is much nicer.", "I offload motor control to a Pololu Simple Motor Controller (SMC) and a Pololu Maestro servo / analog in / digital out controller. I\u2019ve completed porting the Windows code for them from the SRR to Linux. Switched to Linux so I can work with ROS. Also competed in the Space Robotics Challenge so have a fair amount of ROS experience at higher levels. As you can see I\u2019m now looking at lower levels for controlling the base.", "A long way to go on this but I\u2019m retired so can spend a lot of time on it. The first task is to get base level ROS support for the Pololu controllers. My thought is to write a \u2018driver\u2019 for both controllers that accepts messages to control the motors / maestro outputs and publishes the state of the controlled devices.", "The unknown is what messages to use for this.", "Above the motor controller presumably a twist message will arrive at a higher controller that gets translated into a motor command message for each motor. My thought is that each motor is controlled by a separate node. Since there might be multiple motors the controller nodes could accept a ", " with the ", " of each motor. The SMC can control velocity and acceleration so can use that from this message.", "Similarly, the servos can be positioned with the same message using speed, acceleration, and position.", "If the maestro is used for digital output a joint message is heavy. But it should still be a name and value array. Don\u2019t see anything standard that works. Any thoughts? Same thing for reporting on digital inputs.", "Also, the SMC has analog and RC inputs plus a lot of status information (voltage, temp, errors). Maybe the latter should be done through a service rather than published?", "Okay, a lot of thinking on the fly here. Time to stop and think. Appreciate any comments or suggestions on how to proceed.", "Rud", "I have no experience with any of them, but a quick ", " Google turned up quite some results (", " and ", " among others).", "In my set-up, ros_control (", ") on RPi receives join position / velocity commands. Then RPi relays updates/state requests to Arduino over a serial link. When commands are received, Arduino uses Pololu API to set/get parameters on Mini Maestro servo controller. Same thing for VNH5019 motor driver.", "ros_control and controller packages are pretty good.", "For now, I abandoned the use of rosserial and use a custom protocol and serial I/O between RPi and Arduino. Actually, I just wrote a blog post about it, if interested take a look at ", ".", "Cheeres", "Thanks. I\u2019ve looked at those project but they are limited to controlling servos, despite the \u2018motor\u2019 in their descriptions. I also want to access the digital and analog capabilities of the Maestro. They don\u2019t address the Simple Motor Controller at all.", "Rud", "The problem you are working, low level control of a mobile base, has been done more than a few times.", "What you need to implement is called a \u201cbase controller\u201d.  This accepts messages from ROS and controls motors based on the messages.    Almost certainly you will be using the \u201cmove base\u201d package to do the motion planning.   this package expects your base controller to accept \u201cTwist\u201d massages.  Even if some other package is used the \u201cstandard\u201d is to use \u201ctwist\u201d messages.", "So to answer your question about \u201cwhat message type?\u201d the answer should be \u201ctwist\u201d.", "If you are using the Wild Thumper chassis it is a good choice for a small outdoor robot.  I am using the same chassis.  (For those who don\u2019t know, this is a 6 wheel drive chassis, one gear motor per wheel)  One modification you will need.  You really do want to put encoders on the center pair of wheels.  Don\u2019t bother with the end pairs.  You only need the encoders on the center.   As you know the center axel caries most of the weight and the front and rear wheels are intended to slip and skid.  Encoders are only useful on the center axel.", "As you know, all three motors on the left are wired in parallel as are al the right motors.  You can in effect read the Thumper as a two wheel robot, just like \u201cTurtlebot\u201d and most of the turtlebot tutorials apply.", "You can add the encoders yourself or buy a Thumper with encoders from the factory in China.  I added them myself but would 100% not recommend it.  It took a lot of work.", "Study the diagram and documentation for \"Move Base\"", "\n", "All your questions about what could be a service, what massage types and so on. are answered by the above.", "This is the wrong l=place to discus the design of Yhumper/ROS robot design.   If anyone wants to discus Wild Thumper under ROS (including 3D printed parts, batteries and what not) email me at ", " and we can find a suitable public forum.", "This is a great chassis but almost all of them end up being just R/C cars.", "Mind to share where you got the encoders from? I still have the original four motors from my wt chassis (4wd) and like to equip them with encoders.", "Also, what is the problem discussing a full implementation of a ROS base controller here?", "For Rud_Merriam: Do you already have a speed control and odometry for your robot?", "Chris,", "While I\u2019m using the Thumper I don\u2019t want to limit this code to that platform so I\u2019m generalizing to handle more than two \u2018motors\u2019, i.e. Thumper has 3 per side but controlled as one.", "I think ", " is a level higher than than I\u2019m talking about.", "Something has to translate ", " into the speed control commands for the Simple Motor Controller (SMC). ", " contains the necessary information so probably is the way I\u2019ll go.", "I have motors with encoders but they are not installed, in part because I don\u2019t have anything that will read the encoders. I may see if a script on the Maestro can handle it but have my doubts.", "I\u2019d be interested in discussing other aspects of the project elsewhere but IMO this project section is appropriate for this discussion. There have been similar ones on other projects.", "Most \u2018robots\u2019 tend to end up as RC cars. I\u2019ve already used the Thumper for an autonomous competition.", "Rud", "H,", "I have speed controller but not odometry. Pololu sells replacement motors with encoders and I\u2019m going to use two of them. Also need to determine how to capture the encoder information. Might fake odometry with timing if I get to the point of wanting to work with ", ".", "Rud", "I don\u2019t mind here but this forum I thought was for announcements about ROS", "\nitself.  Not for people who use ROS.    Eventually whoever manages this", "\nlist will jump in and tell us.", "As for how to obtain a Wild Thumper with encoders there are several options", "The 'Thumper is made in China by a company called \u201cDagu Robots\u201d  Dagu has a", "\nlot of interesting products  One is this motor.", "\n", "\nIt is a Wild Thumper motor with encoder.", "Also if you email Dagu Customer service and ask.  They will sell you a wild", "\nthumper with this motor replacing the standard motor.   They charge not", "\nmuch more than the cost of the motors and BELEIVE ME it is worth it to have", "\nthen do the work", "Those are two ways where you have to deal with the Chinese factory.", "The other way is to notice that the motors Dagu is selling are just normal", "\n25D gear motors.   You can buy these from many different vendors.  One is", "\nPololu.", "\n", "Pololu is your source for robot kits, robot parts, robot electronics, and custom manufacturing services, including custom laser cutting and mylar solder paste stencils for SMT soldering.", "\n", "You can buy the samemorte and the same encoder from Pololu.    But Pololu", "\nhas something else:  They sell motors with no gearboxes.  These come with", "\nand without encoders.   I bought a 25D, 6 volt motor with encoder and not", "\ngearbox and swapped two of these into my stock wild thumper.    Logically", "\nit should have been a screwdriver job and take about 15 minutes   But the", "\nmotors in the wild thumper are press fit into this black plastic holders", "\nanother is no room for the encoder leads.", "One last comment.   You can NOT simple add encoders to the stock Wild", "\nThumper.  Encoders require the motor to have a \u201cback shaft\u201d.  This back", "\nshaft is an extension of the motor shaft out the rear of the motor.   Wild", "\nthumper meters lack this so you have to replace the motor.      When I", "\nfound you can buy the motor with no gearbox, that is what I got.", "Doing it all over again, I would simply buy the wild thumper from Dagu", "\ndirectly and save a ton of work.  But if yu already own a wild thumper then", "\nyou have to swap the motor on the center  on each side.", "Back to ROS\u2026", "You are going to need a source of odometry.  So people are using \u201cvirtual", "\nodometry\u201d by computing optical flow from video cameras but I think you need", "\nreal ground truth to calibrate everything else.   BTW I just discovered 360", "\ndegree cameras.  These have a true SPHERICAL field of view.  Yes they see", "\nEVERYTHING.   I want to place one of these on this robot.  I am working on", "\nsoftware right now to computer optical flow.  With a spherical camera, I", "\nthink(?) vehicle heading is simply the average optical flow times -1.", "\nThese camera cost under $100.    So far i have a helmet mounted camera and", "\nI\u2019m walking, collecting test data.", "Yes.  What you are building is called a \u201cbase controller\u201d.   Typically", "\neverything done at a lower level than \u201ctwist\u201d is done outside of ROS.", "\nROS is really not very good with real time.", "I had the same goal. wring this controller once and reusing it.   My other", "\nplatform has four wheels and each has it\u2019s own encoders and each wheel is", "\ncontrol independently.", "Here is how I did it\u2026", "My Base Controller runs on an ARM Cortex-4 micro controller.  These are", "\nlike Arduino but 40X more powerful at 1/2 the cost.  They run a ROS node", "\nusing ROS Serial and connect to the larger Linux based system with a serial", "\ninterface.", "The base controller, waits for \u201ctwist\u201d messages.  For each message it", "\ncomputes the speed of every motor and stores this as \u201ctarget speed\u201d.   It", "\nis not hard only a little trigonometry is needed and we only get at most 20", "\ntwist  messages per second", "Also running the same Cortex-M are one PID servo control loop for each", "\nmotor that neds to be controlled.  The PID loop looks at the target speed,", "\ncompared this to the measured speed and then does what PID controllers do,", "\nadjusts the motor voltage to reduce the error.  I have one loop per wheel.", "Finally the last part.  An interrupt service routine was called each time", "\nan encoder sends a pulse.  Atevery plus it updates the measured position of", "\nthe wheel.   This can be as fast as 11,000 times per second", "In short, the measured speed is kept up to date by an interruption", "\nhandler.  The target speed to kept updated by the twist message handler.", "\nThen the PID controller runs 20 times per second and looks at the", "\ndifference between measured the target speeds and sets the motor voltage up", "\nor down so as to make target equal measured.", "You are forced to use a small micro controller if yu are dealing with wheel", "\nencoders.  Any computer running linux would never handle the interrupt rate.", "One final thing.   Recently found out that ARM has a feature where it can", "\nhandle quadrature encoders in hardware.  I don\u2019t need to us an interrupt", "\nhandler.   I\u2019ll change this some day. It will save a ton of CPU time  I", "\ncould simply read the well position from a hardware register.", "The same Cortex-M also sends periodic odomerty messages.  Also it does", "\n\"housekeeping\" things like battery voltage", "To underline that Chris architecture is a common one I\u2019ll add a draft of ", " which is similar.", "I have a 4wd, so 4 motors to control, they are controlled independently by the 4 PWM outputs of an AVR Atmega. For this I use 4 PID controller, which are all calculated on the AVR. The measurements for the PID comes from the 4 quadrature encoders which are polled in the timer overflow interrupt service routine.", "The AVR is connected to the computer using a I2C bus. Input is x translation and z rotation parts of the geometry/Twist message. Output of the AVR are the translation (x,y) and rotation (z) parts of nav_msgs/Odometry. The conversion between the I2C protocol and the corresponding ROS message is done with a node on the board computer.", "Btw if you don\u2019t need much cpu processing you could do everything on a ", ". It has an additional real time processor \u201cPRU\u201d.", "Hope you can find this useful. ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/project-wayfarer/3659"},
{"title": "Architecture Test - New with ROS", "thread_contents": ["Hi guys, i\u2019m new here,", "I\u2019m doing a project for my master\u2019s degree, I need some direction .", "\nI have to test one architecture based on ontology.", "\nMy project is: I have to build 3 robots(one leader and 2 slaves) , and they have to communicate with each other. They have a mission of walk a path diverting from obstacles. The leader robot have to check in a cloud robotic if all robots have same architecture( which i\u2019m testing) and if they have potential to walk a path, then they will do the mission.", "Can you guys help with good materials/documentations/books and directions?", "\nThank you.", "Not sure of your constraints\u2026 you mentioned cloud, but it\u2019s not clear to me if that means they must be Internet connected.", "Otherwise, this sounds like an interesting application for a mesh network with xbee or similar radio modules.  That\u2019s on my list of things to try once I master ROS itself and have my robot(s) cruising around outdoors.", "If you\u2019re not familiar with xbee radios, they are available at most hobbyist electronics suppliers in many forms depending on your communications requirements.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/architecture-test-new-with-ros/3748"},
{"title": "Quadcopter shooting high power lasers controlled with motion capture (Otus tracker)", "thread_contents": ["Hi! This project will document my attempts at shooting at baloons with a high power laser from an autonomous quadcopter. I will provide the documentation to replicate the whole system. I already made a simple quadcopter controller that can follow a path as you can see there:", "\n          ", "\n", "The code is available ", ".", "I am the lead developer of the motion capture system ", ", and the mechanical developer of the dynamometer for motors and propellers RCbenchmark ", ". A collegue made a ROS plugin for the Otus tracker and software. I learned ROS to code the quadcopter controller. We also coded another example with a ground small robot following another tracker with ROS:", "First, I want so talk about safety. High power lasers are dangerous and can cause permanent eye damage, fire and burns. We are performing all tests in a closed, locked room with safety glasses. If you work with lasers, make sure you have quality safety glasses. For this project, I will be using a high power laser and an off the shelf quadcopter.", "\nCurrently, I am sending the command through an arduino acting as a ROS node. The Arduino then sends the command to a receiver, and finally to the quadcopter. The advantage of this method is that an operator can still control the UAV by simple releasing a switch from the controller. The disadvantage is that the update rate is limited to 50 Hz, the setup adds latency. Consequently, the quadcopter cannot be controlled as precisely. The Otus has a resolution of 1 mm or better and a latency under 3 ms, but the latency from the uplink prevents us from using a controller that is too aggressive. If this causes problem, I will switch to Xbee communication.", "I will be using a 500  mW laser to pop balloons.", "I am planning to document this project and make a nice video. At the end, I hope to publish the complete specs and code so the project can be replicated quickly. I have also been talking with a few profs and researchers in the field to see if there is interest in buying a kit that includes everything you need to fly a quadcopter with motion capture. It would reduce the time to start research on quadcopter from weeks or months to half a day, while staying affordable for many researchers. The platform could even be used in undergrad courses. Please contact me if you are interested.", "Right now, I am testing mounting the balloons and conducting test with the laser to determine the time required to pop a balloon. I will do my best to update frequently. Please don\u2019t hesitate to comment and share your suggestions!", "Cheers,", "Here is the BOM for now:", "Quadcopter: ", " (125 USD)", "\nReceiver: Orange RX R618 XL (10 USD)", "\nController: Spektrum DX7s", "\nComputer to controller interface: Arduino copy (5 USD/board) with PPM cable made from an audio cable.", "\n", " (2180 USD)", "\nSpare props (About 10 USD)", "\nSport netting (About 100 USD)", "\nLaser: ", " (30 USD)", "\nGlasses: ", " (3x 25 USD)", "\nBlack Balloon: (Dollar store close to Halloween or Amazon)", "\nPWM controlled relay (8 USD)", "Total: about 2500 USD + transmitter (100 to 500 USD), including the quadcopter and the motion capture equipment.", "\nMost of this is reusable for many other projects if you have a quadcopter lab.", "Robots with lasers oh yea.", "I did some tests. The lasers I ordered are for cutting or engraving. This means that their focal point is relativety narrow. I have to focus them, so set of points that will pop the balloon is a line about 15 cm long in front of the quadcopter. That should be good enough for now. I also have two relays that will be in series with the laser: one will be controlled from a manual wireless transmitter, and one will be controlled from the quad. This is for safety.", "Just have to say\u2026 this is awesome\u2026 and scary at the same time\u2026 the technology that we have at our disposal is so powerful and cheap that it\u2019s scary to think what will happen with these devices in the next few years.", "Stay safe, and keep the lasers pointed away from you!", "Thanks! We are still working on this, but we were a bit delayed by other projects. Also, I switched to a PX4 integration with ROS. It makes a lot more sense in term of implementation. The PX4 is a lot more optimized for real time flight, and it has a good ROS integration.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/quadcopter-shooting-high-power-lasers-controlled-with-motion-capture-otus-tracker/2987"},
{"title": "Rosimport : python is dynamic", "thread_contents": ["Hi everyone,", "Python is a dynamic language, and ROS integration with python should fit the language habitual usage and be dynamic.", "\nBased on that main thought, catkin should be made completely redundant in python.", "I have been working last year on two projects as proof of concept for this idea :", " so that the whole workspace setup is useless for python. You just work on your project in its virtualenv, and ros libraries on the system are detected and added to your sys.path at run time.", " so that ROS message code is generated on import time (in /tmp if you re looking for it)", "With these, the whole artificial ", ", and ", ", is now totally unneeded for python ROS code. A python dev dev can follow the usual python development flow : create a virtual environment and just run a REPL or some python code. I personally believe that, done well, this has the potential to greatly simplify and speed up python ROS development (usually the main point of using python)", "These projects are tested and working, however they are in need of feedback, documentation and general love and care.", "\nSo if you are interested in this kind of experiment, give these a try, and let me know what you think !", "How does this look to someone who has a workspace with a mix of C++ and Python packages? With Catkin if I have that, and I have messages in the Python package, Catkin will ensure they are available from the C++ package. Based on your description of pyros, I get the impression that it\u2019s only suitable for the equivalent of a pure-Python workspace, and if I have stuff that needs to be shared with C++ packages then I need to deal with that in a workspace for that. Is this correct? It\u2019s not necessarily a bad thing, I just want to be clear on the limitations.", "Cool work man! As I also mainly work on ROS via Python I find your work very interesting, altho as I tend to work with teams that also code in differente languages a catkin_ws is usually needed.", "You may be interested in my project: ", "In there I made a handy function (", " ) that given a message definition (the .msg / .srv) and the package name, generates a fake package that can be compiled in a catkin_ws for any language to use that message. It also has tools to \u201csteal\u201d from a robot these definitions.", "I gave a lightning talk about it in ROSCON:  ", " (seems like the link to start at the 20m 12s mark does not work, sorry!).", "Indeed these focus primarily on pure-python environments for proper ROS", "\npython integration.", "The setup layout assumption that I work under is the usual software one :", "So for ROS and C++/Python that means I usually expect the C++ packages are", "\nbuilt and deployed to the machine when a dev work on a python package (on", "\nits own, in a virtualenv), relying on these packages.", "That being said, messages are still the same, and they are located in the", "\nsame place, so if someone wanted to go through the trouble of writing a", "\nCMakeLists.txt file for them, he/she could get the C++ code part to compile", "\nand run while having the python code working there, all from the same", "\nworkspace. But I see more risks than benefits in doing that, so I dont want", "\nto encourage it. Very few actually know or even care about how dependency", "\ntrees are resolved in static and dynamic time for both C++ and Python and", "\nare able to solve a problem that may occur with a mixed complex setup.", "Also I understand that the usual way to do things with ROS is to have a", "\nseparate dedicated package for message definitions only, so in that case it", "\nis not an issue, even in the same workspace.", "rosimport is known to work only at the development phase so far. I still", "\nneed to work out some details for packaging (with pip) python packages that", "\nuse it.", "Interesting stuff !", "Thinking about your TODO for instant connection to a robot, it looks like you got the \u201cdiscovering\u201d messages part working already there, and you could use a similar trick than rosimport to import the messages in your code (instead of generating a workspace)", "You might want to have a look at ", "\nMaybe there are some parts we could factor out, or maybe patch or standardise some kind of API in genpy and genmsg via a REP, to enable this kind of dynamic tools\u2026", "Thanks for putting this up ", ", I was really interested in your work in pyros, and found catkin_pip to be a very useful jumping off point.", "We were faced with related challenges w.r.t shipping python packages, and I ended up breaking out and releasing our solution (", ").", "It comes at it from the opposite direction, which is bundling a virtualenv with dependencies in each package as part of the CMake build. I\u2019m also looking forward to poking at snappy instead, which may offer a better layer in which to bundle a virtualenv (bunch of ROS packages + one agglomerated virtualenv inside a standalone snap).", "Catkin_virtualenv look interesting, I shall try it sometimes\u2026 But I hope that eventually we will get rid of the whole build process, completely redundant in python.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "work on one repo at a time ( or tracking old bugs across different", "\npackages and versions is a nightmare )", "know your dependencies and how they integrate with your own code ( or", "\nupgrading your system will break your code without your knowledge )", "continuous integration : update/test/deploy as often and as fast as", "\npossible ( or you will break what already works beyond any hope of repair )"], "url": "https://discourse.ros.org/t/rosimport-python-is-dynamic/3156"},
{"title": "Coursera's Control of Mobile Robots using ROS on Raspberry Pi Robot", "thread_contents": ["Hi,", "Upon launching version 2.0 of our ", " (which is based off a Raspberry Pi and many other standard off-the-shelf components), I decided to use the ROSbots kit to re-visit some of the concepts from Coursera\u2019s Control of Mobile Robots course.  Over time, I plan to implement parts of the programming exercises in that course using ROS on a ROSbots differential drive robot.", "The ", ".", "Love to hear what the community thinks of the effort. As usual, don\u2019t hesitate to reach out with questions, suggestions and feedback.  Want to collaborate? Love to connect as well.", "Thanks!", "\nJack \u201cthe ROSbots Maker\u201d", "I note the mention of Coursera\u2019s Control of Mobile Robots Course, can you please give me details of this course.", "Collen Gura", "Hi Collen, link to MOOCs course here - ", "The course is free. The implementation exercises are in Matlab. I plan on re-implementing in ROS on a ", " while being as faithful to the original exercises as possible.", "You can read more in the link from the original post above.", "Thanks!", "\nJack", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/courseras-control-of-mobile-robots-using-ros-on-raspberry-pi-robot/3952"},
{"title": "Voice Control Via Android or Chrome Capable device", "thread_contents": ["This is a demo of speech control of a ROS based differential drive based robot. The Robot Commander is available for browsers (only tested for Chrome) or Android. Code available via github Ubiquity Robotics. Will run on minimal hardware (Raspberry Pi 3 + Raspicam)  It should be adaptable to any robot. For full functionality, it may require our modified move_base called move_basic, available from the ROS Kinetic Distro. The Robot Commander app is available via the Play Store.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/voice-control-via-android-or-chrome-capable-device/4036"},
{"title": "Controller", "thread_contents": ["Good evening dear community of ROS, I am a Cuban boy of 12 years who by chance discovered the simulator gazebo ros, but due to the poor communication in my country I have limited access to the Internet. Currently I set my goal to develop the simulation of a quadrator in ros, which is able to locate a mobile platform, and land on it, but I have stumbled on the first problem to achieve it, I do not know how to implement a controller that allows me to rotate the quadrator on the z-axis, that is, its yaw, from its rotation matrix.", "Best regards, God bless you.", "PS: I attached a part of the code, in which you can see the way I currently perform the rotation without using the rotation matrix.", "//-------------- YAW CONTROL ----------------", "\nmax_grad = 180.0;", "\nyaw_error = yaw_reference - yaw;", "\nu = kp_yaw*yaw_error/max_grad;", "void image_viewer::Ardrone_odom(const sensor_msgs::Imu::ConstPtr &odom)", "\n{\t", "\nodom_ARdrone_x = odom->orientation.x;", "\nodom_ARdrone_y = odom->orientation.y;", "\nodom_ARdrone_z = odom->orientation.z;", "\nodom_ARdrone_w = odom->orientation.w;", "}", "Hello mate,", "Not trying to self promote here, but this extremely relevant ROS tutorial was written some weeks ago. ", " (Oh, and keep in mind that you should use ", " for such questions instead of discourse!)", "Have fun,", "George", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/controller/4060"},
{"title": "Tractobots, my attempts at field robots", "thread_contents": ["I\u2019ve been discussing my field robot projects on Emlid\u2019s forum (", ") because I initially considered basing my projects on a Navio+.  I didn\u2019t end up using Emlid\u2019s products so it\u2019s been a little strange to keep posting there.  This looks like a more appropriate place for me.", "I am firmly in ROS these days but I don\u2019t use it well.  I hope some discussion here might help me standardize and take better advantage of ROS components.", "Lots of historic information is in the Emlid forum.  I\u2019ll be happy to elaborate on it here but for now, I\u2019ll just start posting what I\u2019m currently doing.", "It\u2019s high time for me to do some vertical tillage with Tractobot02 but I\u2019ve been working on my new little planter tractor (Tractobot03).  I worked a bit in Tractobot02 yesterday.", "\n", "I originally set up Tractobot02 using a monolithic Python script.  It worked great, even at high speeds (19 MPH).  Then I partially converted it to ROS and sent it out to pull a grain cart for harvest and didn\u2019t care much that it couldn\u2019t drive a straight line well.", "I tried to get it working last weekend and found it weaving all over.  Yesterday I finally decided to rip out my PID code and use the ROS PID.  I did it mostly because I wanted to reconfigure it (rqt) on the fly but I also suspected some windup issues so I thought I might as well switch to the ROS PID before investing more effort in it.", "The ROS PID solved the problem!  It was a wonderful change.  I so appreciate being able to use code that has gone through such rigorous development.  Thank you ROS community!", "There\u2019s a lot I want to do beyond the dumb functionality I have now.  I am confident that working with ROS is the best way to ensure that I make progress.", "I added code to make turns.", "\n", "I did all of my work in the \u201cdriver\u201d script today.  I\u2019d like to break out navigation into its own node but I needed to make quick progress today so I was concentrating on just making it work.", "I think in terms of sequences:  raise the tool, throttle down, turn, wait, stabilize, drop tool, throttle up.  (The tractor easily does a better job than I would.  And I\u2019m still tweaking it.)  I\u2019m not programming it that way, though.  Instead, I\u2019m setting the controls based on the current state.", "If things go wrong, this all falls apart quickly, but for now it works.  I will need to do a lot of re-coding before it goes out on its own.", "That video above was intense during the closing approach to the ditch, just before I got to see your turning radius. I just caught up on your previous posts, (awesome work btw!) and will be eagerly following your progress here.", "I agree, this is amazing work. You\u2019ve accomplished more by yourself on a hobbyist\u2019s budget than some agricultural machinery makers I happen to know with half a million Euros to spend. Looking forward to following your saga! ", "BTW: I\u2019m usually working with robots that are much, much smaller. And I\u2019ve inadvertently run them into, over or under things many times. The thought of having a tractor run autonomously without any collision avoidance scares the shit out of me. Since you\u2019re working on an open field, implementing this would be trivial if you just had the right sensor. I see you\u2019re looking into Radar or lidar sensors; that would be ideal. If money wasn\u2019t an issue, I\u2019d probably go with the SICK LD-MRS. Despite being a Lidar, it can scan through dust, rain and snow. Or perhaps a Velodyne Puck, which would give you 360\u00b0 vision; but that one\u2019s also about $8k I think. So perhaps Radar is the better option.", "That turn was scarier than it looked.  I decided to increase the headland for the next field.", "I was using a CaryMart long-range kill switch but this winter I misplaced the remotes and I\u2019ve wanted to build my own kill switches anyway.  I built a LoRa pair a couple weekends ago and I plan to package them next weekend.  Then I\u2019ll feel more comfortable operating Tractobot02 without being in the cab again.", "I started with a small (23 HP) tractor because I was concerned about obstacles.  It did its share of damage and ran into me a few times.  I learned a lot from that, as I expected.  I\u2019m not so willing to let the bigger tractors run into anything.  Fortunately, operating in an open field under constant supervision makes it fairly easy.  I\u2019m hopeful that the auto industry will make robust and reasonably inexpensive pedestrian detectors available soon so I\u2019m waiting for that.", "I\u2019ve been spending a lot (to me) on tools and supplies but it\u2019s amazing how little I have invested in Tractobot02\u2019s equipment.  It\u2019s basically a GPS (~$1K), antennas, a Raspberry Pi 3, Verizon USB stick, and an Arduino Nano.  $2K easily covers it.", "I have learned the importance of a good kill switch that\u2019s independent of the rest of the robot\u2019s control system.  I recently built my own using Adafruit LoRa Feathers.  I put the remote in a magnetic-mount weatherproof GPS case.", "\n", "\n", "In the robot, I simply wired a Feather to an SSR for the ignition.", "\n", "Those little wire antennas should work for well over a mile.  I haven\u2019t tested over half a mile.", "OK\u2026I\u2019m finally butting up against transforms.  I\u2019m going to have to do something more complicated for Tractobot03 because of the location of the satellite antennas.", "\n", "I tried hiring an expert on Upwork but I\u2019m having trouble getting messages there now.  So I might have to figure this out on my own.  I\u2019m overwhelmed.", "I have this very fuzzy understanding that I need to:", "Unfortunately, every one of those steps feels insurmountable right now.  And I don\u2019t even know if I\u2019m on a reasonable path.", "My fallback plan is to do some simple nvector calculations to move the satellite-derived position half way between the primary antenna and the secondary antenna.  (The position of the secondary antenna is given as meters of latitude and meters of longitude from the primary antenna.)  Then I can just pretend that my antenna is on the centerline of the tractor (like my others have been).", "Of course it\u2019s likely that there will be a slight offset, so I\u2019ll tweak the \u201chalf way\u201d calculation as necessary.  And I will have tilt data so I really should use it to calculate the ground-level position.  I can imagine this eventually working well enough but it would be so nice to have transforms working so that I can do this cleanly.", "I will gratefully welcome assistance/advice.  And I\u2019m quite willing to pay for help!", "Although I\u2019m sure it\u2019s possible to do, I\u2019d strongly recommend not mounting your GPS antennas on a movable link compared to your odometry and IMU. Most localization implementations assume fixed offsets for those elements and I doubt that you will want to be implementing your own localization estimator with extra parameters to estimate and deal with latency etc.", "From your picture it looks like there\u2019s a reasonable amount of space on the 80/20 at the top of your roll cage for the antennas. Or you could also use the root of the arms instead of the middle point. Or use the front brush guard.", "Thank you for the recommendations, tfoote.", "I intend for the loader to be in a fixed position during operation (such that the antennas are pointing straight up).  I did think about putting the antennas on the extrusion but I want the increased precision of having them farther forward, plus it gets them away from the cameras and antennas I\u2019ll have on the extrusion.  The grill guard wobbles a lot and does not offer the spread I need.  (It will also be shadowed when the loader is lifted.  I\u2019ll have a fertilizer tank up front.)", "I only use GPS - no odometry or IMU.  That\u2019s part of the problem I bump into whenever I try to use ROS tools instead of the code I\u2019ve written.", "Very nice project! One remark on the killswitch: I hope it is implemented in a fail-open way (no wireless connection -> stop) ", "I guess you are not using any localization filter at the moment. In that case, you don\u2019t need odometry/pose messages, just direct tf transforms.", "Do you get attitude measurements (e.g. compass bearing) from the GPS? I assume so in the following.", "The remaining question is how you fuse the two GPS measurements. Naively, I would simply average them in the UTM frame, and then pretend that there is a single GPS antenna in the center. Then, you could even use your old code as you mentioned.", "Otherwise, you already identified the steps you have to take to transform the data in ROS properly. Here is how I would fill these steps with more details:", "I assume the desire for odometry is to allow use of existing ROS", "\npackages for navigation or other purposes. Many require odometry as an", "\ninput. I was working on my slightly smaller robots last year and ran", "\ninto that requirement. Got distracted by a competition using simulation", "\nso didn\u2019t follow through with them although encoded motors were purchased.", "A quick search found some GPS to odometry discussions so it\u2019s likely to", "\nbe available.", "-73 -", "\n", "\n/Mystic Lake Software/ ", "Thanks for the help, xqms!  I\u2019ll try to respond to your points in order\u2026", "One of the best parts about my new kill switch system is that I can program it.  Yes, I originally made it so that there was a heartbeat and if the switch was hit, the heartbeat stopped and the tractor was killed.  I switched away from that to only transmit a kill signal for a couple reasons.  The main one is that I wanted to save the battery.  I don\u2019t know how long it\u2019ll last if I\u2019m transmitting all of the time but I sure don\u2019t want to run out of battery at the end of a day and be tempted to bypass the kill switch.  I also plan to have several kill switches operating several robots which could be miles apart.  I want kill switches to be able to move in and out of range without killing tractors.  I do consider having one heartbeat switch that stays in the field with the tractor (and is connected to a power source like the command center) and then have other switches that send signals only when activated.  I like keeping it simple for now, though.", "I do not use a localization filter.  I work directly from the GNSS data, which is plenty accurate for me.  I would love to fuse an IMU, though, so I can get higher refresh rates and lower latency.  Yes, \u201cdirect tf transforms\u201d sounds like my current need.", "Yes, I get course and pitch from the receivers.  Here\u2019s the data I get about the base line to the second antenna:", "Right now I\u2019m using nvector for all of my calculations.  I think about switching to UTM but I\u2019m a little concerned about conversion errors, especially in fields which are over a mile in one dimension.  I suppose that if I calculate my lines in nvector and then convert them to UTM, I should be OK.", "Alright, modeling the robot in URDF seems to be the place to start.  I\u2019ve tried it a few times and not gotten far.  I\u2019m sure it\u2019s something I can do.  I will give it a shot.", "The next steps overwhelm me right now.  Good to know I\u2019m on the right path, though.", "Thank you!", "Rud,", "Yes, it seems like the tools I find always want odometry.  I have read some discussions about using GPS only but they get complicated quickly.  I suspect that the issue is that typical GPS data is very noisy and more input is needed to provide a usable position.  I already have very usable position and pose data.", "We have a need for a kill switch as well, and found that the only suitable wireless kill switches start close to 1000\u20ac. Thus we also decided to build our own kill switch (using heartbeat and an active kill signal for below reasons). We also ordered Featherboards and will use an 868MHz signal. We calculated (most pessimistic assessment, heart beat at 100Hz) that our 2200mAh battery will last at least 10 hours. Judging from your picture I would assume that you have a larger battery, you should have no problem sending a heart beat signal for the whole day at 10Hz.", "Yes it is a different category, but the run-stop of our PR2 runs for about a year on 4 AA-cells. It\u2019s a bit clumsy, but I would never trade more battery time for security, especially when running a tractor.", "We plan to put code, parts list and 3D-printable model on thingiverse or an open git repo once our student is done building it.", "Heartbeat: obviously required in case the battery runs out or the signal is out of range", "\nActive Kill Signal: in case someone duplicates the heart beat signal, the active kill signal blocks the Heartbeat. Reset switch will be placed on the receiver. This way, multiple kill switches can be used for a single receiver, as well as multiple receivers for one kill switch (signal will have an ID to avoid interfering with multiple setups)", "I didn\u2019t know about the n-vector representation - cool!", "It seems that averaging positions in n-vector (i.e. finding the center between the two antennas) is just the normalized mean of the two vectors: ", "I would use that to calculate the position of a virtual antenna in the middle of the two.", "As ", " suggested it is extremely unusual to have GPS antennas on a movable part of any robot. You really wonna have an exact known offset between your antenna(s) and point in which you control your tractor (probably center of the rear axis, that is base_link). If you use very precise GPS (< 5cm) then you can very well just continue using it alone. That is how we did navigation here: ", ".", "You can use this function to convert your Lat/Lon into utm: ", ".", "Since you have 2 antennas you will be able to infer rotation of the tractor as you already figured it out above. Otherwise ", " gave you all the right steps above.", "If you need some help with the initial URDF and TF transforms I can find some time next weekend.", "xqms,", "Yes, n-vector has been a lifesaver for me.  I can cobble together very useful code from the recipes given.", "I was thinking I\u2019d use the interpolation example (", ") so that I could easily handle an offset in case it\u2019s not quite centered.", "Kyler, here is the UTM conversion i use. Works to the millimeter - even to convert back. If you want the conversion back, i can post that too. Its written in C# but can easily be changed to whatever language.", "In terms of position of your hitch etc, most robots are no where as large as the project you have here so there are a couple considerations.", "You need a pivot distance, ie the distance from the back wheels (pivot point) to your antenna. your heading will be based off the movement of the antenna and since your antenna is not on the pivot point of your vehicle, the antenna will sweep in an arc and circle that is larger than your pivot point arc.", "The hitch point of your vehicle should be based on the distance from your pivot point to the hitch pin. In your tractor example note that the hitch moves in the opposite direction as your antenna.", "Some example code to easily determine position based on your antenna heading in UTM\u2026", "Some example code", "Again, while its in C#, easily translated to your required code.", "Hope this helps your quest.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["If I am within 14 meters of the end line, increment the path offset and reverse the direction.", "If I have a large steering correction to make, raise the tool, set the throttle low.", "If I don\u2019t have a large steering correction to make, increment a counter so I know when I\u2019ve stabilized.", "If I\u2019ve stabilized for a bit, drop the tool and set the throttle high.", "Establish a world frame and a base frame.", "Define the base, the loader arms on which the antennas are mounted, and the rear hitch.", "Translate the satellite navigation data into odometry and pose messages.", "Use transforms to determine the hitch location using the satellite data.", "Model your robot using URDF, including coordinate frames for base_link (somewhere in the middle of your tractor), gps_link (location of your GPS antenna) and rear hitch. For starters, you can use fixed joints for everything.", "Start a robot_state_publisher, which takes care of publishing these transforms", "Check that the transforms look okay using rviz (fixed frame: base_link, add a TF display).", "Write a node which converts GPS lat/lon measurements into some Cartesian system (UTM?) and publishes a transform world -> base_link. I\u2019m not aware of a ready-to-use implementation for your use case, but it should not be hard to implement.", "\nThe tricky part is the correction for the mounting offset: Ask tf for the transform gps_link -> base_link and multiply that from the right side onto the world->gps_link transform (UTM) to obtain world->base_link.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "the resulting angle of trailing equipment and its position is based on a following algorithm, like a trailer behind a truck. Its a constantly decaying angle as the pulling vehicle moves forward. The trailing hitch length needs to be known of course, and once the heading of your implement is known and since you know the width of your implement you can now know where the extremes of your implement are by using the above sin/cos method."], "url": "https://discourse.ros.org/t/tractobots-my-attempts-at-field-robots/1486"},
{"title": "FACSvatar (facial expression to avatar animation) ROS integration", "thread_contents": ["I currently have a project that combines OpenFace, Blender, Unity 3D and Python modules to transfer a person\u2019s facial expression to an avatar: ", "At the moment I\u2019m using ", " as Messaging Broker, but I want to tap into the resourceful ROS community, and therefore want to make it ROS compatible.", "Currently, the framework is just simply transferring the facial expression from user to avatar, but a future goal is to use the facial expression as input that can be analysed, and it\u2019s the agent that generates the facial expressions. Think of Affective Computing.", "Most of this framework is similar to Robotics projects, and the facial expression generation could even be used for robots that have advance facial \u2018muscles\u2019 and are FACS (Facial Action Coding Unit) Compatible. So for my framework to be useful to many people, I want to integrate it with ROS.", "Transfer / analysis of facial expressions in an easy to run framework (even for non-ROS users), but offer advance customizability (for ROS users with Human-Agent Interaction goals).", "Probably use ", " for the modules to run cross-platform and preventing the need of installing dependencies: ", "Develop main project in ROS. Put every module of the framework in a Docker container and use Docker compose to easily connect all the modules in an easy-to-run way.", "Then use ROS-bridge to connect modules for visualization outside the docker container which runs cross-platform.", "Put Roscore and Ros-bridge in a docker container, so it can run cross-platform on both Ubuntu and Windows (or anywhere where Docker runs). Then only some boiler-plate code has to be added to a module to communicate with ROS in a connection class. The rest of the module doesn\u2019t need any knowledge of how the messages reach their destination or where they come from. Possibly put modules in Docker containers for easy deployment and connect everything with Docker compose.", "Example of ROS communicator over Rosbridge: ", "I\u2019m aware of ROS through the Windows Subsystem for Linux (WSL), however that breaks the requirement of \u201cRun out-of-the-box\u201d.", "It seems [idea 2] fits my purpose the best, however I would like some feedback if you think this is feasible. Or maybe I overlooked some easier solution? Please let me know all your thoughts ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Unity 3D integration for visualizing avatar\u2019s facial expressions", "Blender integration for creating animations out of facial data.", "Python modules for transferring/converting facial data & Deep Learning", "Cross-platform (at least Linux (Ubuntu) & Windows, preferably Android)", "Run out-of-the-box with minimal set-up, but still can be configured for everyone\u2019s needs.", "Rely on popular tools, to prevent going out-of-date fast.", "Modular, only use the modules necessary for your project.", "Easy integration with other ROS modules", "Main development in Ubuntu (Windows is run-only)", "Hard to switch to another message broker, because fully dependent on ROS", "Easy for non-ROS users to modify and run code.", "Easy to swap out ROS for another message broker", "No catkin_build for Python modules", "Can support any language, as long as it can communicate with ROS bridge", "Matches current implementation", "Harder integration with ROS (but can still communicate with ROS projects)", "Cannot release as ROS module", "? Double messaging?", "ROS bridge has no libraries for Python (or most other languages)", "Boilerplate code has no maintainer"], "url": "https://discourse.ros.org/t/facsvatar-facial-expression-to-avatar-animation-ros-integration/4132"},
{"title": "Announcing catmux - A helper package to startup complex systems in multiple shells", "thread_contents": ["Hi there,", "Have you ever had a larger robotic systems with many different launchfiles to startup? So you end up opening multiple shells and starting launchfiles or nodes in each of them instead of having the comfort of starting just one command.", "This is where catmux jumps in. It helps you managing mutliple shells running different commands using tmux without the need of actually knowing too much about tmux. Just create a yaml-file that specifies which commands you want to run in which window and your robot-startup is just one call away.", "\nThe package is relatively new, but has been tested in production for two months now, which is why I decided to announce it here. You\u2019ll find it on my github page:", "A catkin package that wraps launching ROS applications with a tmux script - fmauch/catmux", "If you think, this could be useful, please have a try and give feedback.  Currently. I am especially interested in your opinion whether this package benefits from being a catkin package or if I should rather make it ROS independent and release it as pure python package. Currently the ROS dependency comes in only from being able to pass paths to files using the \u2018package://\u2026\u2019 syntax.", "If you say, that this package is completely useless, as one can also launch everything in one single launchfile, my daily workflow often requires to inspect output of single nodes separately or easily restart components of a whole systems. Yes, i know that this could all be achieved using the built-in logging helpers and using rosnode kill and so on, but in my opinion it has great benefit to be able to do all this at command line level.", "I am also aware that I reimplemented many roslaunch-features such as argument passing, launching things on external machines and so on, which is why I am also very curious whether you think, this is an idiotic idea or if this actually makes sense in your opinion.", "That all being said: I\u2019m looking forward to any feedback, might it be here, a gitlab issue or a personal message.", "Cheers", "\nFelix", "This is probably going to be useful! I\u2019ll try it out in the future and give feedback. ", "I personally think it should be ROS-independent/standalone, even if it\u2019s only used with ROS, because it adds an unnecessary layer. For example, having it under ROS makes the commands quite verbose. For ", ", ", " is quite long (all ", " commands start with ", ", so it could really be simplified to just ", ").", "This looks interesting, we also use tmux-sessions in a similar way at Magazino. Have you thought about using services so that everything is started automatically when the robot is turned on?", "You mean services as in upstart / systemd? As I work at a research lab, most of our robots are multi-purpose with multiple workspaces on them, so this does not make much sense here. But of course nothing stands in the way of using it with those, one would only have to pass the \u2018-d\u2019 flag to the run part and that\u2019s it.", "This looks interesting, will give it a try.", "\nAre you familiar with rosmon, that also improves output and enables easy restart of individual nodes? That\u2019s in a single shell though.", "\n", "ROS node launch & monitor daemon. Contribute to xqms/rosmon development by creating an account on GitHub.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-catmux-a-helper-package-to-startup-complex-systems-in-multiple-shells/4095"},
{"title": "Coursera's Control of Mobile Robots using ROS + Raspberry Pi Robot - Part 2", "thread_contents": ["Hi,", "I published the 2nd post of a multi-part series that implements concepts learned from ", " on to our ", ".", ". Future posts, parts 3 and beyond, will completely implement the CMR assignments.", "On an ongoing basis, I will implement various online robotics courseware and textbooks on our ROSbots robot kit as an effort to learn / re-learn robotics concepts and theory, welcoming others to join along with their own ROSbots kit.", "As usual, love to hear what the community thinks of the effort. As usual, don\u2019t hesitate to reach out with questions, suggestions and feedback. Want to collaborate? Love to connect as well.", "Thanks!", "\nJack \u201cthe ROSbots Maker\u201d", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/courseras-control-of-mobile-robots-using-ros-raspberry-pi-robot-part-2/4249"},
{"title": "Rqt tf echo", "thread_contents": ["Display the same fields tf_echo does on the command line but in an rqt widget:", "The time of the transform and the approximate time which the transform was looked up is shown, so a transform that is not getting updated and falling behind in the buffer can be detected.   In the above example the tf time is 0.0 because a static tf is being looked up.", "Individual fields can be hidden with a right click context menu (though those are masked by the context menu of the line edit boxes), and whether they are hidden will be saved in .perspective files.  The source and target frames are also saved and restored.", "There are toolTips for each field, though I find getting them to pop up inconsistent.", "The only error output is the transform time label turns red when the transform is unavailable, I\u2019ll possibly add another field (that also can be hidden) with more verbose exception output.", "TF buffer cache time will also be made configurable.", "I could make the frames also have a dropdown with all the known frames, but I\u2019d like to also make it possible to type in anything and I\u2019m not sure if those two can be combined smoothly.", "View the transform between two frames. Contribute to lucasw/rqt_tf_echo development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rqt-tf-echo/4271"},
{"title": "Catkin_virtualenv", "thread_contents": ["Hello ros-users,", "Ever wish your catkin package could painlessly depend on any python library under the sun?", "Presenting ", ", a tool to define and bundle python dependencies with your catkin python packages.", "We\u2019ve been running an internal ros_buildfarm instance at Locus Robotics for almost 2 years (thanks OSRF!). One ongoing pain point was managing python package dependencies that aren\u2019t in the Debian/Ubuntu apt repositories.", "The workflow for catkin_virtualenv fits fairly well on top of the existing ROS Python-via-CMake pattern:", "More detail is provided in the ", ", or see the provided test packages for toy usage examples. Behind the scenes:", "One important thing to note is that the catkin_virtualenv approach does make your package artifacts noticeably \u2018fatter\u2019 due to the dependency bundling, so be wary of pulling in large dependency trees, and please continue to use rosdep+apt dependencies when possible.", "Please enjoy, and I\u2019m looking forward to feedback from anyone who finds this package useful!", "Paul", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " a set of python requirements in ", "\n", "\n", " a virtualenv at build time by calling CMake macro ", ".", "\n", " your python scripts via ", " as usual.", "add a dependency to your ", " and ", ".", "the generated virtualenv will work transparently in devel- and install-space, and be bundled in any released debian packages.", "package requirements will be propagated to any dependent catkin packages.", "your python scripts will be wrapped in a loader that will work transparently with ", ", ", ", and ", ".", "adding the ", " flag to ", " will enable your python scripts to run in python3. This mostly ", " with a lot of existing ROS components, but does have a few ", " at the moment."], "url": "https://discourse.ros.org/t/catkin-virtualenv/4365"},
{"title": "Create your own secure P2P network for your ROS based robots", "thread_contents": ["Hi ROS community!", "I would like to introduce you to a new software which allows you to create ", ".", "Most of so called IoT solutions are based on client-server architecture. Usually, the server and the software running on it are both owned by a solution provider. User owns the devices he connects (like smartphones, robots or sensors) but the communication is forwarded through a server that doesn\u2019t belong to him.", "That leads to an unwanted situation where users can be spied on by the manufacturer who may aggregate their data and sell it to a third party. It can also lead to users loosing access to their devices when the central server goes down.", "It won\u2019t be a problem any more thanks to the solution we just launched.", " is basically a P2P, VPN network dedicated to autonomous vehicles, robots and drones. Your devices running Husarnet client software communicate directly with each other, without being forwarded through our servers.", "We developed Husarnet with the following assumptions in mind:", "Husarnet beta is now available for testing. For free.", "More details are available on our documentation page: ", " . Under this link you can also find a tutorial showing how to use it in a simple ROS based project.", "I hope you\u2019ll find it useful, and I would be grateful for your feedback.", "Best,", "\nDominik", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " - it goes directly between devices running Husarnet client", "\n", " - thanks to direct connection between devices", "\n", " - at cryptography and runtime level", "\n", " - after initial connection is set up, there\u2019s no web access required", "\n", " - connecting multiple robots and remote servers with additional computing power can in couple easy steps"], "url": "https://discourse.ros.org/t/create-your-own-secure-p2p-network-for-your-ros-based-robots/4401"},
{"title": "Initial release of ros moving object package - V0.3.0", "thread_contents": ["We are happy to announce the initial release of ", ", a ROS node for moving object abstraction. Thanks for ", " and ", " packages to provide an AI solution for object detection, tracking and localization, based on that, we can get object velocity information, which is essential to make intelligent collision avoidance for autonomous path planning.", "You can visit ", " to understand the data pipeline/flow between the corresponding packages.", "To show cases of the moving object usage scenarios, this package also involves a sample node, ca_policy. Ca_policy implements two policies for navigation style:", "By using the inputs of moving object, ca_policy analyzes the obstacles\u2019 types, and arbitrates the suitable navigation policy (coworking with ROS navigation stack by dynamic reconfigure interfaces). Furthermore, based on the HW attributes of robot chassis, ca_policy triggers some sensors for policy notification. In the current version, 2 robot chassis are enabled :", "Please see more from package ", ". You are welcomed to feedback and contribute.", "ROS2 version of this package will come soon.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": for the cases when obstacles are person(s), and", "\n", ": for the other cases"], "url": "https://discourse.ros.org/t/initial-release-of-ros-moving-object-package-v0-3-0/4408"},
{"title": "Free simulation model - mobile robot with LiDAR and RGBD camera", "thread_contents": ["Dear ROS community,", "Couple of weeks ago I\u2019ve posted an information about a launch of a new development platform for ROS - ROSbot 2.0. It\u2019s a mobile robot platform with RPLiDAR A2, Orbecc Astra RGBD camera and CORE2-ROS controller with ASUS Tinker Board single board computer.", "Recently we have introduced Gazebo simulation model of mentioned robot which allows you to test all of its functionalities and development capabilities. Simulation package is available here for free: ", " .", "To get your started we\u2019ve prepared a set of ROS tutorials:", "I would appreciate if you could let me know your impressions from testing the simulation.", "Best,", "\nDominik", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/free-simulation-model-mobile-robot-with-lidar-and-rgbd-camera/4535"},
{"title": "Slamtec RPLIDAR A2M8: Autonomous Wall Painting Robot", "thread_contents": ["Hello Everyone,", "I would like to talk about the use of Slamtec RPLIDAR A2M8 in an autonomous robot that we built as a part of our Capstone Project at the University of Waterloo. RPLIDAR is a 2D laser scanner by Slamtec. It is very compact and well designed for mobile robotic applications.", "Here is a video of unboxing of the Lidar:", "It talks about the features of the Lidar as well.", "The LIDAR was used in our robot as the range sensor for Autonomous Navigation. We used it to map the environment and localize in the environment. Initially the LIDAR was mounted on a turtlebot before it was put on our robot. Here is a video of the LIDAR mapping when mounted on the turtlebot:", "When mounted on our robot, the LIDAR worked out of the box without any problems. All the software - SDK, Firmware, ROS Packages are provided by SLAMTEC, which makes it extremely easy to bring it up and running. If using ROS, the only thing one needs to be careful about is to set up the transform tree correctly for mapping or localization to work properly.", "Here is a final video of our robot on which the LIDAR was used:", "\n", "Find out more about this robot at ", "I would highly recommend the LIDAR. Its extremely easy to use, comes with all necessary support software, and works well without any issues.", "Cheers,", "\nTeam MIST", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/slamtec-rplidar-a2m8-autonomous-wall-painting-robot/4614"},
{"title": "Lidar Lite V3 with ros", "thread_contents": ["Im wondering if anyone would know how i could get sparkfuns Lidar Lite v3 module working with ros. I have it communicating with a raspberry pi, but im unsure how to get it working with ros. Ive been seeing RPlidar around and was wondering if i could somehow use the existing support for RPlidar, but with the Lidar Lite instead of an actual RPlidar module.", "\nHeres a link to the lidar module i currently have", "\n", "This is the LIDAR-Lite v3, a compact optical distance measurement sensor. When space and weight requirements are tight, the LIDAR-Lite v3 soars!", "\n", "Hi, I did a fast checking and it seems there are some projects you could try", "\n", "\n", "regards,", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/lidar-lite-v3-with-ros/4103"},
{"title": "Rosbag sharing place", "thread_contents": ["Hello ROS Users,", "I have a small idea in mind, a rosbag sharing place.", "At work, we always have a problem with choosing hardware and always save .bag for Proof of Concept development and demonstration of algorithm.", "\nI think being able to grab .bag from hardware we don\u2019t own to test it or experiment with other test environment or use it to create easy tutorial is pretty cool.", "But this idea can be achieved only if people share it ", "I wanted to know your thought :", "This place can be the ros wiki, a github org, etc. as long as the community can be involved.", "Thanks for you time,", "\nBest regards,", "This talk from ROSCon 2016 about BagBunker might interest you:", "1720.98 KB", "\n", "\n", "The software has apparently moved to here:", "Contribute to marv-robotics development by creating an account on GitHub.", "That\u2019s correct, thanks Geoff. Let me know if you get into problems running. It does exactly what you need and also allows you to conveniently visualize the content and search for bags.", "Thanks a lot for the link, I totally missed this talk,", "This software is great, it does the job I was looking for (and more).", "\nI will begin with this .", "But I\u2019m still open for other solution if somebody has any.", "Best regards,", "Marv is great, but it doesn\u2019t address your suggestion of having some infrastructure that allows \u2018everyone\u2019 to share bags - unless someone sets up a public instance with enough storage somewhere.", "I\u2019d definitely be interested in that, but that would need (quite) some coordination I believe.", "github for data.", "recently picked up using open science framework for group internal logfile sharing (nothing fancy, small datasets). that should work as a public bag dump as is, even for large amounts of data.", "maybe osf could be talked into running marv within their site if the repository contains .bag files, just like a wiki comes with every repo on osf or github.", "osf site: ", "\nexample repo: ", "I also believe that MARV could be a great interface to a public bag exchange. It is under active development so there are more features to come that are not in the community edition (yet). We have been using their enterprise edition in my previous job and I believe the developers might be open to supporting publicly hosted instances that make the data available to everyone.", "For completeness, there is also ", ", which is also freely available and has a similar goal to MARV, but with (I believe) a subset of features.", "Also, a bag cloud storage project BotBags was announced a while back, but except from this announcements, there was nothing much yet (", ").", "As for a central place to share bags, we would need to ensure funding for the storage and bandwidth. It seems like OSF could be the right place for that (first time I hear about it), however we would need to talk to them. I guess it only makes sense if MARV was running on their servers as well. While in principle it could be possible to run it somewhere else and access the data via their API, it probably would be way too slow.", "Also, from an initial look at their offerings, it seems that the storage is not restricted currently, but the maximum file size is (\u201cIndividual files must be 5GB or less to be uploaded to OSF Storage\u201d), which might be a problem (although splitting bags is of course possible and supported by MARV).", "If we figure out where and how to host it, I\u2019m willing to help setting up and maintaining MARV for that use case.", "Maybe we could also start by hosting a public instance at some university or OSRF with limited storage to get a feel for what would be required. But if you want to get serious, that is a huge task (backups, availability, scaling storage and bandwidth, funding, \u2026).", "I would love to have some site to share ROS bags on. At RoboCup@Home, I collect bag files from all teams in all challenges.", "There is some interesting data in there but it\u2019s a pity to only WeTransfer that data to the teams.", "The OSF seems like the correct option for bags that are related to research results and papers. If they could be convinced to set up an instance of MARV, that would be awesome.", "Nice idea for OSF, it seem a right place for .bag, they also got some API to connect to external application and they can even redirect the project main page to an external website : ", "Github is not the right place for storing large dataset, each repo are limited to 1GB : ", "Would it make sense to start out with links to files hosted in various cloud storage places (google drive, dropbox or any others that aren\u2019t going to charge by the download)?  The links could be on a page within the existing ros wiki.  ", " could have requests for bags from particular sensors or robots, or conversations here on discourse could drive new uploads.", "If an organization with money later wants to make sure those links don\u2019t go dead then all the files could be moved to a centralized location (maybe still google drive or dropbox, but all in one account).  Licensing of the files would ideally already be in place so they could be freely copied without issue.", "I don\u2019t know if that grows into MARV integration very well though, which sounds like it would be a substantial increase in IT maintenance and hosting fees since it wouldn\u2019t leverage the free/low-cost but more constrained hosting services.", "It would be great if every visual sensor could also have a link to a video (e.g. on youtube, though again issues with links going dead later if someone takes their youtube page down) that shows off the entirety of what is in the bag so it can be quickly previewed without downloading gigabytes and incurring bandwidth costs.", "Hi,", "I wanted to let you know that I contacted Botbag, no answer yet (but It\u2019s been only 3 day, so no problem).", "I also contacted OSF, it\u2019s ok to upload bag as long as we don\u2019t upload all bag at once (\u201cIf uploading to OSF Storage, all we ask is that you upload your rosbag gradually and not all at once. If too much data is uploaded in a short amount of time, it will cause us problems.\u201d).", "\nI opened a test project : ", " (I will remove it so don\u2019t store important bag).", "I created a small instance of MARV into a private server : ", " where I uploaded some bag for testing (and removed, only a SR300 bag for find_object_2d is available.)", "There is a lot of problem with MARV at the moment :", "It may look bad but it\u2019s not a big deal. ", "OSF have a cool API for uploading, so a MARV server can act as a proxy to take bag, analyze it, then upload it to OSF and keep only the download link (so all data will be on OSF, the server will only keep the metadata, and the server will throttle upload to OSF to avoid sending too much data).", "I will continue to play a bit with Marv and OSF API. I will fork MARV someday to connect it to OSF and correct some bug, so everyone interested can participate.", "OSF have a cool API for uploading, so a MARV server can act as a proxy to take bag, analyze it, then upload it to OSF and keep only the download link (so all data will be on OSF, the server will only keep the metadata, and the server will throttle upload to OSF to avoid sending too much data).", "I will continue to play a bit with Marv and OSF API. I will fork MARV someday to connect it to OSF and correct some bug, so everyone interested can participate.", "Cool, awesome for getting starting. Your proposal might actually be a good solution, using OSF for long-term storage, but processing the data first on the server where MARV is running. We might need apply some smaller patches to make that work with MARV, but it should not be a big deal.", "Most of the issues with MARV are know problems of the current Beta community edition and are already addressed with the more recent developments. The cool thing about MARV is that it is quite flexible, i.e. if we need a new property such as \u201ccategory\u201d, that can be easily added (even though, I\u2019m not sure if that is not already covered better with tags\u2026). Things like video is already there, and some more visualizations like a trajectory player, also embedded in a map viewer for things like NavSatFix.", "Rather than forking, we should work with Ternaris (MARV developers) to make sure we don\u2019t duplicate efforts and can directly benefit from future updates, so don\u2019t put too much effort in that just yet. I will talk to them in how far they want to support this idea.", "It would be great if every visual sensor could also have a link to a video (e.g. on youtube, though again issues with links going dead later if someone takes their youtube page down) that shows off the entirety of what is in the bag so it can be quickly previewed without downloading gigabytes and incurring bandwidth costs.", "Well that is exactly the kind of thing that MARV is designed to do for you.", "Just a heads up, BotBags is still alive and under development. Look out for more news here very soon!", "Rather than forking, we should work with Ternaris (MARV developers) to make sure we don\u2019t duplicate efforts and can directly benefit from future updates, so don\u2019t put too much effort in that just yet. I will talk to them in how far they want to support this idea.", "So I talked to the guys from Ternaris and they are very positive towards this community effort and are willing to support it, at the very least with a free license for the EE features for a community hosted service.", "After a couple of beta iterations they are also converging towards a stable version, which we should use as basis (instead of the current public beta from last year).", "We are also currently looking into possible solutions for hosting.", "More on all fronts shortly.", "Hi,", "and thank you very much for your thoughts on this!", "We are happy to announce that ", " is online and we are", "\nready to accept datasets for publication. ", " is and will remain", "\nfree of charge for public datasets.", "For the time being we\u2019ll use a manual workflow and are relying on", "\nexternal hosting of bag files.", "To publish your datasets at marvhub, please follow these steps:", "We are starting with bag files. Do you have other file formats that", "\nyou would like to publish at ", "?", "best regards", "\nFlorian", "I\u2019m getting a login page when I open marvhub. Is that intended?", "yes, once we have the first dataset published, the login will be removed and all public data will then be visible without login.", "Just for even more completeness,", " generously hosts large files. ", ", it\u2019s primarily for files for testing, so ", " is a perfect example. No fancy features as discussed in this thread (don\u2019t get me wrong. I\u2019m thumbing up) but I assume it\u2019s safe to expect to last as long as ", " does.", "While shared bag files are useful for some people who are working on projects (i.e. temporary), another important usecase is for testing like continuous integration, which can be endless. So sustainable hosting server or features that enable server portability (e.g. persistent URL as ", " ", " are very much appreciable IMO.", "Awesome, thanks Flo!", "I put together an according yaml for one of our datasets that also has bag files: ", ". Its not super exiting, since the bags contain only one image topic each, but it\u2019s a start.", "Immediate observations:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Is a rosbag sharing place a good idea ?", "Will you take/share .bag ?", "Do you see any problem with sharing .bag ?", "Do you know a good way to upload and share .bag (since they can be very large) ?", "We can\u2019t remove bag (will be fixed soon)", "We can\u2019t upload bag directly from the website", "We need to limit the number of data displayed (the website display all image from the image topic, it will take too much space)", "Tags seem not to work", "We need category for bag (Sensor/Robot/Camera/?)", "We need a way to make MARV point to a remote storage (OSF/Drive/Dropbox/?)", "Need to display more data from bag (video/map/tf/?)", "Put all files of your dataset online, e.g. at OSF", "Create a yaml file to describe your dataset, use ", "\n", "Put that yaml file next to your dataset files", "Post a link to your dataset here", "For this dataset each sequence has 2 bag files (one with the original images, one with the rectified ones). For MARV it would probably make sense to combine them, but this could also just be our task to provide a bag with both. Question is whether it makes sense for marvhub to allow combining multiple bags into one MARV \u201cdataset\u201d like it already does for split bags.", "For each sequence there is additional information like text files with camera calibration and ground truth, as well as the same image data in different formats. Again one could argue, that we should provide a bag file with both calibration groundtruth as ROS topics, but I guess there will always be data that doesn\u2019t natively fit in a bagfile. So the question is if marvhub should support additional per-bag links / files / metadata."], "url": "https://discourse.ros.org/t/rosbag-sharing-place/1956"},
{"title": "Rostful Kinetic Release", "thread_contents": ["Hi Everyone,", "I finally got around releasing ", " (REST API for ROS) on kinetic as a ROS package, along with ", " (Dynamic Python interface for ROS) and other dependencies.", "It is released on ROS packaging system via a ", " ", ", therefore it is ", ".", "The big change in this release is that rostful is now a pure python web project, ", ", with the ROS requirements being moved down the dependency tree to a ", ".", "\nOther multiprocessing systems could potentially be added ( think twisted and friends - maybe even erlang )", "This should hopefully help cleanly integrating python with ROS environments.", "\nDon\u2019t hesitate to ask if there is anything you wonder about all this.", "Try it out, and send some feedback on ", "\nIf you\u2019re interested on helping out, make yourself known, as I wont have much time working on it from now on.", "Thank you all !", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rostful-kinetic-release/4713"},
{"title": "QtQuick support for RViz to build fancy HMIs", "thread_contents": ["Hello everyone,", "Have you ever wondered how to build a fancy touch friendly HMI for ROS including 3D visualization? Well, then you might like this idea, concept, and prototype.", "I\u2019ve been working with Qt UIs for more than 10 years now and I have seen the transition from desktop focus, to mobile with Nokia to industrial and automotive HMIs. The trend is clear, the introduction of smartphones and other touch devices made a focus on user experience easier, more accessible and a necessity for modern HMIs. More and more industrial machines and robots are equipped with touch-enabled tablets rather than mouse and keyboard.", "So far RViz only supports Qt Widgets based interfaces. Qt Widgets are great for normal desktop applications, but they have many drawbacks when it comes to HMIs, embedded mobile, and touch applications.", "Since a few years, Qt also ships with a technology called QtQuick. QtQuick is paradigm shift compared to widgets. QtQuick uses the declarative QML programming language (think of HTML+JS with JSON syntax) to build the UIs rather than XML based descriptions. The rendering happens in OpenGL, everything is based on base types such as Rectangles. This means it is possible to create literally any design you can think of, which is impossible with widgets and hard with HTML5.", "Long story short, since I work on HMIs for industrial machines and robots professionally, I was looking for ways to build UIs with QtQuick and ROS. Using ROS and QtQuick together is not very hard. It\u2019s just a matter of using the ROS bindings for C++ or Python and building an application.", "One of the greatest things about ROS when it comes to UI is RViz. However, RViz strongly depends on QtWidgets internally. Using QtWidgets in combination with QtQuick is possible, and after some research, it turns out that the actual problem is not combining QtQuick and Qt Widgets, but rather combining RViz Ogre-based OpenGL renderer with QtQuick. Long story short, the easy way of adding QtQuick to an rqt/RViz application using QtQuickWidget does not work out.", "That\u2019s why I invested some time into exploring the correct approach of integrating RViz into QtQuick or to be exact, to integrate the RViz render panel into QtQuick. Turns out that is possible, but only with some major architectural changes inside RViz. In many places inside RViz it\u2019s assumed that Qt Widgets are used, this includes things such as changing the mouse pointer or setting properties of the render panel. It\u2019s possible to refactor out the differences between Qt Widgets and QtQuick Items, but incompatibilities still reside at the moment.", "I have created a ", " and  ", "The patches work and I\u2019m already working on an HMI for an industrial robot application. Unfortunately, I can\u2019t show some screenshots or videos of this application at the moment.", "I\u2019m also aware that the changes probably will not make into upstream ROS1 RViz, but I want to use this discourse to spur some interest in QtQuick support for ROS2. In my opinion, QtQuick support would be a major benefactor for I-ROS applications, as Qt is becoming a bigger and bigger player in the industrial embedded HMI space.", "If you wonder what QtQuick applications look like, I encourage you to take a look at the [Qt user stories](", " and some) or at my CNC examples on ", ".", "If you are fellow HMI engineers interested in QtQuick support for RViz please let me know. Maybe we can pull together to get a ready-to-use upstream version of RViz QtQuick going.", "Machine Koder", "It\u2019s a bit old, but this reminded me of ", " (", ").", "S\u00e9verin Lemaignan seems to have done something similar: ", " which is a bit more recent.", "Thank you for the hints.", "Both projects tackle what I described as the easy/straightforward part of integrating the ROS middleware into QtQuick/QML. That\u2019s great, especially if you want to implement part of the application logic inside QML.", "However, there is also a lot of value RViz visualization and plugins such as the MoveIt integration. The QtQuick integration of RViz is, unfortunately, something that can\u2019t be done without modifying RViz itself.", "Of course one could build a new robot visualization from scratch on top of Qt 3D, but that is an unnecessary exercise and it won\u2019t help moving RViz and ROS forward.", "For fancy UIs, there is another project coming up which might be interesting to you: ", " -> Roboware Viewer", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/qtquick-support-for-rviz-to-build-fancy-hmis/4428"},
{"title": "Bot'n Roll OMNI Final Project", "thread_contents": ["Can anyone suggest me where i can find a project that is a omni directional robot with 3 wheels with a claw on the top of the robot. I need a simulation in gazebo and a find a way to command the robot.    I\u2019m new in ROS and i am not comfortable with the complexity of it.  I know a few commands in ROS but is not enough to build this project. Thanks", "Movimento Omnidireccional de forma simples e com precis\u00e3o.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/botn-roll-omni-final-project/4816"},
{"title": "Coursera\u2019s Control of Mobile Robots using ROS + Raspberry Pi Robot - Part 3", "thread_contents": ["Hi,", "I published the 3rd post of a multi-part series that implements concepts learned from ", " (CMR) on to our ", ".", "In this part 3, I write about ", ".", "Disclaimer: I don\u2019t actually showcase any ROS code in this part 3 as I wanted to focus on the equations used. This sets part 4 up nicely to showcase lots of ROS code that implements those equations.", "As usual, I welcome others to join along with their own ROSbots robot kit. Love to hear what the community thinks of the effort.", "Don\u2019t hesitate to reach out with questions, suggestions and feedback. Want to collaborate? Love to connect as well.", "Thanks!", "\nJack \u201cthe ROSbots Maker\u201d", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/coursera-s-control-of-mobile-robots-using-ros-raspberry-pi-robot-part-3/4818"},
{"title": "Handling topic names for secondary IMU and IMU Filter", "thread_contents": ["Hello everyone, I\u2019m relatively new when it comes to ROS, so bare with me here if this is a pretty basic question.", "To summarize my setup, I\u2019m working with a Clearpath Jackal robot that runs ROS Indigo on Ubuntu 14.05 LTS. On board, the robot has a GPS and IMU unit, and starts up its own ROS components for these sensors as well as a controller node for a bluetooth PS3 controller.", "The above works great, but the hurdle I\u2019m running into is when I try to integrate our own IMU unit into the mix, which is a PhidgetSpatial Precision 3/3/3, I believe.", "I have a launch file for our IMU that runs a few nodes, and is pretty much the same as this example here: ", "The problem I\u2019m running into is getting the IMU filter to subscribe to the Phidget IMU topics. The IMU filter is looking for /imu/data_raw and /imu/mag topics to subscribe to, which are published by the IMU itself. For my case, these two topics are already being published by the default IMU on the Jackal. I\u2019d like to be able to run our Phidget IMU launch file alongside the Jackal\u2019s nodes and topics.", "I\u2019ve tried giving the nodes in the Phidget IMU launch file their own namespace, but the IMU filter node never subscribes to the Phidget IMU node\u2019s topics.", "Here is some additional info to make this a little easier to see what\u2019s going on:", "\n", "\n", "/bluetooth_teleop/joy", "\n/cmd_drive", "\n/cmd_vel", "\n/diagnostics", "\n/diagnostics_agg", "\n/diagnostics_toplevel_state", "\n/feedback", "\n/imu/data", "\n/imu/data_raw", "\n/imu/mag", "\n/imu/magnetic_field", "\n/imu_filter/parameter_descriptions", "\n/imu_filter/parameter_updates", "\n/jackal_velocity_controller/cmd_vel", "\n/jackal_velocity_controller/odom", "\n/joint_states", "\n/navsat/fix", "\n/navsat/nmea_sentence", "\n/navsat/time_reference", "\n/navsat/vel", "\n/odometry/filtered", "\n/rosout", "\n/rosout_agg", "\n/set_pose", "\n/status", "\n/tf", "\n/tf_static", "\n/twist_marker_server/feedback", "\n/twist_marker_server/update", "\n/twist_marker_server/update_full", "\n/wifi_connected", "\nHave Phidget IMU node publish /phidget/imu/data_raw and /phidget/imu/mag, which the IMU filter subscribes to instead of /imu/data_raw and /imu/mag.", "Any help would be greatly appreciated, and I can provide any additional details if needed. It\u2019s probably something pretty simple to someone more versed in ROS. I\u2019ve tried using \u2018ns\u2019 for handling the IMU nodes with a different namespace, and messed around some with \u2018remap\u2019, but haven\u2019t had any luck.", "thanks!", "Nick", "Hi, ", ", welcome to ROS! ", "Maintainer of ", " here. There are many ways of doing this. From the command line:", "Inside another launch file:", "\u2026 or by copying and then editing the launch file directly, and adding remappings to the nodes/nodelets, like this:", "When using remappings, remember that you have to remap both the publisher and subscriber, and that \u201cfrom\u201d and \u201cto\u201d don\u2019t mean the direction of the data flow, but \u201cfrom\u201d the old name \u201cto\u201d the new name.", "Also, I\u2019d recommend using ", " from the same directory. With nodelets, it can be harder to see what\u2019s going on using tools like ", ", ", " or ", ".", "P.S.: It\u2019s preferred to ask questions like these (i.e., which have a clear \u201cright\u201d answer) on ", " and keep ", " for other kinds of discussions (open-ended discussions, project announcements and so on).", "Thanks for all the info, ", "!", "I see what you\u2019re saying about posting location. I\u2019ll make sure to post something like this over at answers next time. I\u2019ll try what you mentioned here and provide the outcome.", "thanks again!", "Nick", "The 2nd suggestion ", " provided, which included the IMU launch file with a namespace worked for me! Thanks a lot for the help, and next time I\u2019ll make sure to post something like this over at ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/handling-topic-names-for-secondary-imu-and-imu-filter/4829"},
{"title": "ROS Telescope project", "thread_contents": ["Hello!", "I am newbie in ROS and I am thinking of using ROS as a basis for building a robotic telescope but there is a question about the reference frames that I would like to know the opinion of the experts.", "A telescope points celestial objects tipicaly using equatorial coordinates. Other coordinates are also used such as horizontal, ecliptic or galactic or in general any of the ", "I know al the transformations between this coords systems but I am very confuse how to match to the ROS reference frames (earth, map, odom\u2026) What is the best way to implement this reference frames in ROS?", "Thank you in advance", "There\u2019s documentation on the recommended way to connect those coordinate frames in ", " The default is to use an Earth Centered Earth Fixed frame.", "As you make progress on your project it would be great for you to share it back here. However in the future for questions please ask them on ", " it\u2019s a better location for asking questions and finding solutions.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-telescope-project/4846"},
{"title": "Raspbian Stretch Lite 2018-04-18 + ROS + OpenCV Raspberry Pi Image", "thread_contents": ["Hi All,", "We updated our SD Card image with ROS + OpenCV built and installed on the latest 2018-04-18 Raspbian Stretch Lite release (tested on both the Pi 3 and Pi 3+).", "(Update 11/2/2018) Updated image with 2018\u201310\u201309 Stretch Lite Raspbian\u2026", "\n    ", "Enjoy!", "\nJack \u201cROSbots Maker\u201d", "\n", " makes a ROS + OpenCV robot kit for Makers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/raspbian-stretch-lite-2018-04-18-ros-opencv-raspberry-pi-image/4849"},
{"title": "Books or documentacion for ros", "thread_contents": ["Hi guys, can u recommend some good books or documentation about :", "\n1 - collaborative task with ros", "\n2- cloud robotics with ros", "Thank you .", "I don\u2019t understand what you mean with collaborative task with ROS. Is it like human-robot collaboration? In that case you can look at robots such as Sawyer and Baxter from Rethink Robotics (they have some pretty good tutorials) and academic literature from conferences such as ", " and ", ".", "Regarding cloud robotics, there used to be an EU project (", "), which spawned ", ", which is now a ", ". Sadly the open source Rapyuta seems deprecated and the startup company has not released their platform to the public as far as I know. Alternatively you can look into hosting ROS nodes in the cloud using Kubernetes (container orchestrator which can be hosted in various cloud environments including Google Apps, Amazon Web Services and Windows Azure), there is a (slightly dated) article about that ", ".", "I recommend \u2018ROS Robot Programming\u2019. It includes not only basic ROS programming as topic, service, action but also packages for SLAM, Navigation and Manipulator with TurtleBot3 and OpenManipulator. It definitely helps you.", "Thank you very much!!!", "Thanks for recommendations. When i say collaborative task, i mean multi robots(swarm of robots) doing a task, master-slave, robot - robot communications. Sorry for my english.", "There\u2019s a wiki page with most of the ROS books listed at:", "I\u2019d recommend looking through that listing for resources.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/books-or-documentacion-for-ros/4857"},
{"title": "Move Base Flex released!", "thread_contents": ["Finally, it\u2019s here!", "We (", ", ", ") have released Move Base Flex (MBF) in both Kinetic and Lunar. We introduced MBF last year at ROSCON as a backwards-compatible replacement for move_base (video available ", ").", "Short reminder of the key features:", "We are still actively developing it, so expect significant changes on incoming releases. But the basic ROS and plugins APIs are (we hope) frozen or at least won\u2019t change significantly (and we will in any case properly document any needed porting)", "For newcomers, we provide a ", " (", "), but more advanced tutorials will come.", "Hope you will enjoy composing your clever navigation strategies, and please tell us if you have comments, critics, wanted features\u2026", "Resources:", "Great news! Can\u2019t wait to explore it! Thank you for sharing.", "Great!!! I was impressed with your presentation last year.", "\nI am delighted that you are offering this software as a binary.", "\nI will try your MBF package soon. Thank you for your hard work. ", "I was taking part in Robocup Germany last month where I had a conversation with an employee from Magazino about their navigation packages and he mentioned Move Base Flex!", "So excited to test this on our robot!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["exposes action servers for planning, controlling and recovering plus access to the costmaps, allowing an external executive to implement smarter navigation strategies", "abstracts a generic navigation framework that can be easily extended into particular implementations (in our implementation move_base is just an specialization of this abstract framework)"], "url": "https://discourse.ros.org/t/move-base-flex-released/4741"},
{"title": "ROS2 LEGO MINDSTORMS - interest?", "thread_contents": ["Who\u2019s interested in making ROS2 accessible for kids and teens in a somewhat friendly LEGO MINDSTORMS tutorial?", "I would be interested, what do you have in mind ?", "I would like to join in !!!", "The basic idea is that kids learn the most while playing. So I was thinking: can we make a ROS setup, with a mindstorms robot, that kids can install, run and tweak. I think these elements will be important:", "I want to do this lean and mean.", "I personally have no ROS experience, but I do have a lot of robot and MINDSTORMS experience, python, linux, macos,\u2026 and I can do building instructions.", "What do you think?", "Thanks for bringing this up ", ", certainly an interesting project!. I\u2019d recommend you to look at ", " which can simplify the process of putting the first pieces of code together. This may be specially relevant if you aim to target kids.", "Robot_blockly looks good starting point. Thanks for bringing up ", ". I would like to suggest that we can also into ", " and ", " for more inspiration ", " .", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["During the installation/setup you learn some basic ROS concepts", "The project is cool enough for kids to really want to try it (a mapping project? a maze? a swarm?)", "brainstorm with you guys for an interesting project", "test with kids if they are interested", "build a \u2018rough\u2019 project", "Explain the basics to kids and test if they manage to tweak it", "Make the whole package with installation and building instructions."], "url": "https://discourse.ros.org/t/ros2-lego-mindstorms-interest/5027"},
{"title": "Open-source Autonomous Sailing Robot", "thread_contents": ["We started our open-source (MIT licence) autonomous sailing robot project at University of Southampton. Our ultimate goal is to design a long-persistent ocean observation platform powered by renewable energy. This platform will help us collect data from ocean and have a much better understanding of our ocean environment.", "And of course, our autonomous system was build on top of ROS, here is the our source repository:", "Southampton sailing robot. Contribute to Maritime-Robotics-Student-Society/sailing-robot development by creating an account on GitHub.", "Here are some picture we took at World Robotics Sailing Championship 2016 at Portugal last year. (We are the champion in our class, by the way ", "  )", "And this year, we are still improving our sailing platform in many ways:", "You are welcome to follow our progress on our ", ", and encouraged to join our active discussion on ", " issue pages.", "Last but not least, we are continuously looking sponsors in electronic components, and our trip to World Robotics Sailing Competition 2017 in Spain to defend our champion.", "Cheers  and happy hacking", "Really cool project, I can see that it is still going on despite this message is quite old.", "\nI love the fact that even the boat CAD files are made with open source software (FreeCAD).", "Good luck with the sailing, put some foils on it and you\u2019ll win ", "Indeed, this is a still a on-going project. But this year we will not only participate but also host the World Robotics Sailing Championship (", ") at Southampton, U.K.", "Yes, we are a group of people really love open source software. ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Improve waterproof capability (seawater is a really nightmare to all electronics)", "Develop/ integrate new path planning algorithm(s)", "Get better sensors (wind sensor, GPS, IMU) for our boat"], "url": "https://discourse.ros.org/t/open-source-autonomous-sailing-robot/1311"},
{"title": "Retrofit a PanaRobo AW8006", "thread_contents": ["I\u2019m on a project of build a new controller for my PanaRobo AW8006. I read document about YARC (Yet Another Robot Controller). And then, I tend to follow this document for my project. I used servo Sigma II for my robot. And now I need a Motion Controller for my Rob. I wonder if they really combined PMAC, ROS, MoveIt. Because I rarely can find anthor document about them and their project. Someone can help me on my project. Thank you so much", "I was looking also for combaining ROS with Pmac", "\nI porbally came across the same article: ", "I also found some source code of implenatation of diffrenet project", "\n", "Power Pmac is framework that running on ubuntu 14.04 so it easy to install ROS on it", "\nalso using command line and a pmac app called gpascii you can access most of their functions and data locally or remotely (using SSH).", "\n", "So you can create ROS Pacage with ROS Topics to get commands msgs and send dat", "\n", "There is open souce library for connecting remotely that was developed by their customer:", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/retrofit-a-panarobo-aw8006/4738"},
{"title": "Comunication with Jetson Robot over wifi", "thread_contents": ["I have a few robots powered by a Jetson TX2 boards running ROS, and I would like to communicate with them securely over wifi. I don\u2019t have a lot of networking experience, and am using this project as a means to force myself to learn. I was wondering if you all have some suggestions on technologies I should look at.", "The ", " caught my eye, can that do point-to-point communication or does it need to go though a router? Is my understanding correct that it runs on top of an existing VPN connection set up through OpenVPN? Also it suggests using RMS which also requires rosbridge. Is RMS worth using too?", "Before I go down the rabbit hole installing a wide variety of technology, I was wondering what you all have had success with in the past.", "VPN\u2019s are great to use with ROS. I use tinc because its easy to setup, OpenVPN seems to be the more popular choice. One thing to note  is that the VPN will increase video latency. I\u2019m currently trying to figure a way around that using something like webRTC.", "I\u2019ve used RMS before. Its okay, but it could certainly be more configurable. I stopped using it and write my own web stack based on ROS Java", "Last thing I\u2019ll throw out there is a startup called ROCOS. They are creating a system for cloud connected robots that looks really legit. They are still creating it but once its done you will pay a certain amount per robot per month for this stuff to just work.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/comunication-with-jetson-robot-over-wifi/4866"},
{"title": "Some problems about creating my first msg", "thread_contents": ["I\u2019m a beginner of ros, I met a problems about creating my first topic.", "\nThe flowing is the problem,it just a warning,the real error is that the cmake can not find the *.h which I had included in my *.cpp, but I think it had a relation with this waring :", "\n\u201cPolicy CMP0046 is not set: Error on non-existent dependency in add_dependencies.\u201d", "\nIs it the problem of Cmake\u2019s version ? How can I solve it ?", "\nmy Cmake version is 2.8.3", "I don\u2019t think that the warning and the error are related. The warning is about a CMake policy that is not set. See the ", " for an explanation. This is only a warning. To write your first message, you can follow ", " tutorial. Take a close look at the CMakeLists.txt part of the tutorial.", "Please ask such questions on ", " following the support guidelines: ", ".", "ROS Discourse is for news and general interest discussions. ROS Answers provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Additionally, when you ask questions be sure to include as much information as possible. In this case that would include your ", " file.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/some-problems-about-creating-my-first-msg/4780"},
{"title": "Topics RViz plugin", "thread_contents": ["This package contains a Qt RViz panel that allows users to display topics values dynamically.", "We use RViz as our monitoring software for our industrial processes, this comes in very handy to watch topics in the GUI, without the need to know the command line for end-users.", "Supported built-in types are:", "More information here:", "\n", "The package has been tested against ", " and ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["bool", "duration", "float32", "float64", "int8", "int16", "int32", "int64", "string", "time", "uint8", "uint16", "uint32", "uint64"], "url": "https://discourse.ros.org/t/topics-rviz-plugin/5030"},
{"title": "Jog_control: Jogging package for robot arms", "thread_contents": ["Hi,", "We are now working on ", ", which provides jog control for research and industrial manipulators.", "ROS oriented robot arms sometimes suffer from the lack of the teaching system, which is usually provided by industrial manipulators. Yes, we have MoveIt! and Rviz plug-in, but we know it is quite difficult to specify target hand position and posture by the GUIs while we have to observe the robot is moving in real workspace. We preferably need teaching pendants or joypads to jog the robot. This package is aiming to provide joint jogging, frame jogging, joypad and teaching pendant control.", "Please refer the repository and ", " for further information. We are very appreciated if we can get feedback from various robot users to improve this package.", "Thanks,", "Looks interesting. There is also the ", " package which provides cartesian jogging functionality for robot arms.", "Thanks,", "\nYes, we noticed jog_arm before starting our project. I believe jog_arm and jog_control are not interchangeable and competitive because the purpose and implementation are different. We want to jog the robot cartesian space precisely, as almost industrial manipulators do.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/jog-control-jogging-package-for-robot-arms/5094"},
{"title": "Using the intensity of the Laser return to detect objects", "thread_contents": ["As part of our efforts to promote ROS, the Meetup group SV-ROS (Silicon Valley ROS), has been using the Neato BotVac as a learning platform (Sort of a role your own Turtlebot).  I have been  working with using the intensity of the returned signal from a Neato Botvac XV-11 type LDS (laser distance sensor). Recently, we have been adding intensity to our own version of the neato node to our Git Hub repository, \u201cintro_to_ros.\u201d", "The Neato company uses the intensity to locate their charging stations. It is suspected they actually use a pattern of returned intensity data to locate specific charging stations. What I wish to do is use the intensity to 1) locate bright (highly reflective) objects. 2) actually be able to recognize 2-D markers on objects. This seems possible, and I am wondering if this hasn\u2019t already been done.", "The ultimate goal is to complete the three challenges for the Homebrew Robotics Club \u201cFloor Bot\u201d", "Repository of packages and info for the SV-ROS Intro To ROS training series - SV-ROS/intro_to_ros", "I\u2019d be happy to collaborate with someone on this.  I suspect there is already a clever algorithm out there that can decide that an object is bright for its range, but I am not sure how regular light (inverse square law) compares to laser light from a reflected surface. At any rate, someone needs to figure out how laser brightness is affected with distance from retro-reflective surface.", "\n", "The above chart based on only 6 points shows the intensity of the charging station is at least 2 times that of white paper.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-the-intensity-of-the-laser-return-to-detect-objects/5288"},
{"title": "Static exception handling", "thread_contents": ["Dear all,", "Avoiding dynamic memory allocations is crucial in industrial grade robotics or autonomous driving. Unfortunately, C++ GCC allocates memory when throwing exceptions. We released a library to solve this problem here: ", "Enjoy trying it out! All feedback is welcome.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/static-exception-handling/5326"},
{"title": "Reinforcement Learning of Driving Robots", "thread_contents": ["I just released a reinforcement learning framework for robot swarms of over 100 robots. It uses an ROS 2 service for communication between the reinforcement learning algorithms and the simulation and can therefore easily be adapted to other needs.", "Video:", "\n", "Github:", "\n", "TensorSwarm: A framework for reinforcement learning of robot swarms. - TensorSwarm/TensorSwarm", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/reinforcement-learning-of-driving-robots/5327"},
{"title": "The Autonomous Robot Challenge", "thread_contents": ["Are you interested in AI? Are you a ROS developer? Why not put your knowledge to use by joining the Arm developer community and accepting the challenge - ", "!", "Follow the ", " to see what the prizes and rules are!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/the-autonomous-robot-challenge/5311"},
{"title": "Feedback Wanted: Test framework for verifying topic output (both unit and integration testing)", "thread_contents": ["\nI\u2019ve been working on a project that (I believe) simplifies testing nodes. I have a working proof of concept that permits one to simply supply inputs (currently, just messages but extensible) and verify outputs to expected values. I\u2019ve also been experimenting with methods of supplying parametric rules (near mappings) between input and output for testing a full parameter space.", "I\u2019m looking for feedback, and in the future some testers, to see which direction I should grow this project.", "\nTake for example a node that filters a Twist message ", ". Let\u2019s propose it does two things, first it clamps the message to a certain velocity magnitude. Then, let\u2019s say it subscribes to a Float64 message (or perhaps in the future, a dynamic parameter) ", " that correspondingly considers the change in velocity and clamps acceleration.", "\nThe first and biggest pain point I see in my mind is defining tests. Rather than supplying a few test cases, I\u2019d love to be able to just give the functional mapping of input to output and say \u201cmake sure the node follows this\u201d. However for even relatively few degrees of freedom, this becomes costly in time.", "To test such a node, I could create several extreme test cases to ensure clamping, and others to ensure acceleration limiting.", "However, It would be very nice to be able to describe that velocity magnitude ", " is clamped to a max value, and ", " should correspond with ", ". The risk of such a modality is requiring the developer maintain both the node and the tests, which for complex behavior may be unreasonable.", "One modality I\u2019ve also considered/experimented with is allowing one to provide input, record a node (or many nodes) output, and then use this as a base to test against in future work. However I don\u2019t know how useful this would be in broad application.", "\nFirst, I\u2019m interested in how you might envision using such a tool (if at all), and what features would be most desired.", "Then, I\u2019m seeking any feedback on how to best define an input/output space for testing, and establishing those relationships.", "It could probably be interesting for you to integrate with ", " \u201c(\u2026 which provides) wrapping functional mockup units (FMUs) for co-simulation of physical models into ROS nodes.\u201d to generate input space\u2026", "I like your ideas and think this could be very useful. However, one should keep in mind that writing a complete specification of the expected behavior of a node is as complex as writing the node itself; if you had a complete mapping from input to output, you could just execute it instead of running the node. So writing a full functional mapping of input to output is probably infeasible. In your example, you\u2019re doing something else: You specify invariants that should hold (velocity magnitude is clamped, and dv/dt corresponds with accel_max), while leaving open some degrees of freedom in the behavior of the node. I think this is the way to do it.", "By the way, are you familiar with ", "? An example where it\u2019s used are the  ", ". Here they just test some simple properties of the node\u2019s output topics and check that the final estimated pose is correct. This is probably enough to catch 95% of regression bugs.", "This is quite interesting, thanks for this! I\u2019ll have to look into it further.", "I like your ideas and think this could be very useful. However, one should keep in mind that writing a complete specification of the expected behavior of a node is as complex as writing the node itself; if you had a complete mapping from input to output, you could just execute it instead of running the node. So writing a full functional mapping of input to output is probably infeasible. In your example, you\u2019re doing something else: You specify invariants that should hold (velocity magnitude is clamped, and dv/dt corresponds with accel_max), while leaving open some degrees of freedom in the behavior of the node. I think this is the way to do it.", "This is a great way to put it. My concern is precisely how to express invariants generally enough such that development of the node (or nodes) isn\u2019t hindered by the tests. Cumbersome tests are likely to be ignored if they break and are hard to fix.", "By the way, are you familiar with ", "?", "Yes, should have mentioned that this will ideally become a ", " node where pubs/subs/invariants are supplied via yaml.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/feedback-wanted-test-framework-for-verifying-topic-output-both-unit-and-integration-testing/5336"},
{"title": "ROSbot 2.0 - new development platform for ROS", "thread_contents": ["Hi ROS Community,", "I want to introduce a new ROS development platform which we have offically launched a week ago.", "ROSbot 2.0 is a mobile, autonomous robot platform based on CORE2-ROS controller (with ASUS Tinker Board).", "Here are some of the main features:", "\n", "\n", "You can find full specs, documentation, and tutorials here: ", " .", "I would appreciate your comments and would be glad to answer any questions.", "Btw, we have also prepared a detailed article about ROSbot 2.0 platform on medium, under this link: ", "Do you have a github of all the files and documentation?", "Hi Kevin, ROS tutorials (", ") are here: ", " , documentation is here: ", " . Soon more repos with code, examples etc. for ROSbot will be available.", "Dear ROS community,", "I\u2019ve got a quick update regarding ROSbot 2.0 platform:", "We\u2019ve updated Sharp distance sensors to VL53L0X time-of-flight distance sensor, which offers bigger range, higher accuracy and higher immunity to interferences", "We\u2019ve increased battery capacity from 2600 mAh to 3500 mAh", "We\u2019ve improved the charging process by providing stronger and more reliable chargers", "We are now adding USB Ethernet Adapter to each ROSbot 2.0. set for quick network setup", "The complete set looks like this:", "\n", "Full specs are available here: ", " .", "Cheers!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["aluminium chassis", "4 x DC motors with quadrature encoders drive", "RGDB camera - Orbecc Astra", "LiDAR - RPLIDAR A2", "4 x sharp distance sensors", "extension interfaces grouped in the rear plate", "badass looks ", "\n", "We\u2019ve introduced a new ROS-based web user interface for ROSbot. You can set it up in 5 mins in your local network or via the internet. Use ROSbot in both remote control and autonomous mode, and test its functionalities, without any coding. It\u2019s open source an already on our GitHub page. Web UI works with any ROSbot 2.0 version and is available here:  ", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/rosbot-2-0-new-development-platform-for-ros/3890"},
{"title": "Online ROS", "thread_contents": ["Announcing a new site, Online ROS (", ") to view, edit, compile and execute ROS code in the browser. It is also supports shared collaborative coding. This could be useful for online ROS based interviews, in teaching environments and to store complete working examples.", "The site is in an alpha, proof of concept stage. I\u2019m interested in feedback about how useful others think this would be.", "The code is open source on github and contributions are welcome!", "Regards,", "\nDavid Ball", "it definitely benefits the speed-up for new users to learn the internal of ROS and get hands dirty based on the examples, generally speaking, the example code is shortage for ROS newcomers. please extend more examples covering more for the ROS internal. ", "It\u2019s up again. Thanks for the notification. ", "David", "Since it looks like it is not working anymore, can I recommend the ", " (ROSDS) in its place?", "\nWe built that tool to have a full ROS development environment online, so you can program ROS with any type of computer.", "Additionally, we include simulations of the robots, so you can also test on the robots.", "\nYou can also share your projects, so replicating results is straightforward.", "We are using the ROSDS to run online tutorials of ROS where all the attendants can follow live the exercises and do the coding together with the speaker. Have a look at our ", " every Tuesday. You can participate too, it is completely free!", "Hello,", "Yes the site itself is down as I have moved onto other projects. However, note that the code is still working and available as an open source alternative if you would like to self host. It\u2019s available here: ", "Next steps could be to add more examples (especially ROS2 examples) and improve the user interface. These could be good student projects.", "Regards,", "David Ball", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/online-ros/2733"},
{"title": "ROS RTABMAP error 2 make -j4", "thread_contents": ["Hello", "I am trying to get my kinect working with rtabmap to show the mapping in real time but when I\u2019m trying to install rtabmap according to the instructions given ", " , I am getting  bunch of error while running ", "here is the list of error", "Kindly help me with this problem", "Thanks", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ROS Answers provides a forum which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "If you\u2019d like you can reply with a link to your ", " question, but in the future please start there.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-rtabmap-error-2-make-j4/5632"},
{"title": "Formal verification of ROS nodes using Imandra reasoning engine", "thread_contents": ["Hello! We at ", " are developing tools and techniques for formal verification of distributed systems. For our ROS-related project, we are working on OCaml client for ROS and we are creating tools and wrappers that allow one to formally reason about ROS node models. With our ", " reasoning engine you can formally verify statements about models written in OCaml, and the same model can be compiled and executed as a ROS node.", "Check our medium post:", " where we are describing our approach to the creation of a simple model of a ROS node.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/formal-verification-of-ros-nodes-using-imandra-reasoning-engine/5655"},
{"title": "Hypothesis-ros", "thread_contents": ["As part of the efforts to improve the test infrastructure of ROS I implemented a Python package ", ". It provides \u201cstrategies\u201d (data generators) for ROS messages/parameters and allows to implement strategies for custom messages. These \u201cstrategies\u201d can be used to apply property based testing on the ROS node/nodelet level. The package has Python dependencies only and is deployed on ", ". A ROS release is on the TODO list. Documentation: The best documentation are tests ", "Happy ROS node/nodelet fuzzy testing ", " (Good luck! You\u2019ll need it\u2026 probably.)", " I will only add strategies which I need for work. However PRs to get other strategies added are welcome.", " ", "Dear ROS1 community,", "due to prioritization reasons I\u2019ve dropped maintenance/support for this project. Please consider issue ", ".", "Cheers", "Thanks for the call ", ". Let me get the interest of some colleagues that I believe should be able to help with this.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " (developer @ ", ")", "\n", " (developer @ ", ")"], "url": "https://discourse.ros.org/t/hypothesis-ros/4978"},
{"title": "Results webpage of ROSIN (ROS-Industrial Quality-Assured Robot Software)", "thread_contents": ["Dear ROS community,", "\nIt is now easier to get an overview of ROSIN\u2019s funded Focused Technical Projects (FTPs) at:", "\n", "\n", "Browse through vendor-developed ROS drivers for industrial hardware,", "\ngeneric ROS frameworks for industrial applications and model based", "\ntooling. Thanks to ROSIN support, all these new ROS components are open-sourced for the benefit of the ROS-Industrial community.", "Each entry leads to a minipage that is maintained by the FTP champion, so check back often for updates on the progress of the projects.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/results-webpage-of-rosin-ros-industrial-quality-assured-robot-software/5946"},
{"title": "Series of ROS video projects launched", "thread_contents": ["Hi all the ROS developers,", "\nwe have started a series of videos showing how to do some interesting ROS projects step by step. So far, we have the following 4 projects:", "We have published two videos of each project so far (as for 8th Feb 2018) and we publish a new one of each project every week.", "Hope you like the projects and we are open to receive suggestions, questions, correct errors, propose collaborations, etc.", "\u25b8 Week 1: Get the Robot up and running.", "\n\u25b8 Week 2: Create the Movelt! package for Robot.", "\n\u25b8 Week 3: How to connect your MoveIt Setup with a Real/ Simulated Robot.", "\n\u25b8 Week 4: MoveIt Planning Interface: Obstacles in the robot workspace and how to avoid them.", "\n\u25b8 Week 5: 3D Perception with MoveIt.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": showing how to use the real Aibo robot with ROS. Program in simulation and transfer to real robot.", "\n", ": how to make a simulated wheeled robot from zero, while understanding all the ROS concepts on its way. URDF, XACRO, plugins, etc.", "\n", ": how to configure everything on a drone to make it navigate using LSD-SLAM", "\n", ": Learn how to build the Sentinel Robots from the Matrix for Gazebo Simulator. Learn about advances XACRO techniques.", "\n", " : Learn how to use a Gazebo simulation in Raspberry Pi through OpenVPN.", "\n", "\nIn this series of videos you will learn how to use MoveIt! package with industrial robots."], "url": "https://discourse.ros.org/t/series-of-ros-video-projects-launched/3924"},
{"title": "ROS Books Give Away | ROSCon 2018", "thread_contents": ["Hi Everyone", "I would like to give away my books as a part of ROSCon2018.", "Please register and select your ROS books. Please make sure your e-mail address is correct. The 15 lucky winners will get their selected e-book.", "The list of winners will be announced at the end of ROSCon2018.", "Note: I can\u2019t attend ROSCon 2018:   ", "  Ticket sold out.", "Regards", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-books-give-away-roscon-2018/6193"},
{"title": "Move Base Flex 0.2.0 is here!", "thread_contents": ["It\u2019s a major code refactoring to enable the decoupling of the action executions. This allows us to implement the main new feature: Concurrent running planners, controllers and recovery behaviors.", "\nParallel running plugins are identified by their concurrency slot. You just need to provide a different concurrency_slot on the action goal to run a action goal in parallel with a different plugin. Using a already running concurrency_slot will preempt the previous execution in the old good fashion. Not using the concurrency_slot will lead to the normal behavior of canceling the current goal when sending a new one.", "Other new feature is the check_path_cost service, similar to the check_pose_cost but for a full path.", "Enjoy!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/move-base-flex-0-2-0-is-here/6200"},
{"title": "Robotics Language at RosCon/IROS 2018", "thread_contents": ["We have developed a compiler infrastructure (RoL) at Robot Care Systems that drives our robot LEA and automates the way we write code. RoL allows us to create new high-level languages, generate \u201ccorrect by design\u201d ROS c++ code, Graphical user interfaces, etc. The idea is to abstract vertically (by using/creating new languages) instead of horizontally (by creating libraries for a specific language). We are now in the process of open sourcing our code:", "I will be at RosCon on sunday, and will stay at IROS for the rest of the week. If you want to see a live demo please send me an email: ", "We are very open for discussion and feedback!", "Best regards,", "\nGabriel", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/robotics-language-at-roscon-iros-2018/6211"},
{"title": "Roboteq motor control ROS differential drive ", "thread_contents": ["I been working on this differential drive for roboteq motor controller and I wanted to share.", "\n", "Contribute to scancool/roboteq development by creating an account on GitHub.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/roboteq-motor-control-ros-differential-drive/6214"},
{"title": "Different Control theories and Reinforcement learning simulation in ROS+ Gazbeo", "thread_contents": ["Hello all! I am really glad to share with you my simple implementation of", "\nFuzzy logic , LQR and PID controllers ,most importantly Deep Q Network on Gazebo model using ROS. Please find", "\nthe project link ", "This is super cool! Thanks for sharing.", " you may be interested on this.", "thank you!! Please let me know if you have any comments", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/different-control-theories-and-reinforcement-learning-simulation-in-ros-gazbeo/6229"},
{"title": "First P2P network layer for robots and IoT with first class ROS support", "thread_contents": ["Hi ROS community,", "I am writing this post from iROS2018 conference in Madrid, Spain and\u2026", "I am very excited to announce that during this event we have officially launched Husarnet - first P2P network layer for robots and IoT with first class ROS support.", "Ok, but what is it and why you should even bother checking it out?", "Husarnet is a peer-to-peer VPN network dedicated for robots, autonomous vehicles and industry 4.0 applications. Husarnet allows devices to connect with each other without any central cloud in between - they communicate directly, through the shortest (in term of latency) path through the internet. Thanks to the direct connection latency is low and nobody has the access to the data exchanged between the devices. Husarnet creates a \u201cglobal LAN network\u201d where devices are identified by their IPv6 address, and see each other like they were in the same LAN network, regardless of their location. Our solution provides direct, private, fast, and extremely secure datalink between connected devices such as: robots, servers, laptops, cell phones, and even microcontrollers. If devices are in the same network, they don\u2019t even need access to the Internet to see each other.", "Think of it as a global LAN network for your robotic components.", "If you have your ROS project working in LAN network, you don\u2019t need to change anything in the software, to make it working through the internet - just install Husarnet client on your devices and setup your network in few seconds.", "As it has just launched I would very much appreciate your feedback. You can log in and set-up ", " at ", " .", "All the best!", "\nDominik", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/first-p2p-network-layer-for-robots-and-iot-with-first-class-ros-support/6325"},
{"title": "ROS Additive Manufacturing (RAM)", "thread_contents": ["OS ", "dditive ", "anufacturing (RAM) provides a set of tools (mostly algorithms and GUIs) to help end users create complex trajectories for additive manufacturing. The project is especially focused on printing metallic parts with industrial robots. This project contains multiple tools: path planning algorithm able to generate complex trajectories, visualisation tools, editing tools, etc.", "The application contains an algorithm that we found in a thesis, it allows to generate continuous trajectories from the start of the process to the end of the part (the whole part is built in one shot), this is very interesting for many aspects (material health, no dead-times) but also has some drawbacks (heat management). We developed a set of ROS messages, services, actions, nodes and GUIs that enables this whole application to work.", "Here are screenshots of the application as of today:", "\n", "\n", "The user can modify each pose of the trajectory: robot speed, feed rate, robot pose, laser power, feed rate etc\u2026", "\nExample, deleting pose:", "\n", "\n", "Here is a video demonstrating a use case with arc welding 3D printing:", "\n", "We are actively developing the application and are already using the application on 3 of our robot work-cells! One for arc welding and the two others are confidential projects.", "The code is available here:", "\n", "If you are willing to test the application, help with the development or just curious about the functionalities please leave a message here!", "Hello, ", "!", "\nFirst of all, congratulations for this Project, I\u2019ve been following it and its developments and it is amazing!", "\nI\u2019m doing a Project and I think ROS AM can be helpful, but I\u2019m a newbie.", "\nIt is a FDM 3D printer with a robot arm holding a movable base and a fixed head.", "\nIs it possible to implemente FDM 3D printing with ROS AM? And what about robot parameters and kinematics, that has to be added with a diferente package, right?", "\nI don\u2019t know if this is the best place to post this questions, however, I would be grateful if you could answer.", "Best Regards,", "\nJo\u00e3o Sobral", "Plastic FDM? If so I would recommend using already existing slicers to generate the trajectory, such as ", ". The problem is that they are not compatible with robots. Integrating Slic3r into ROS AM is in our plans but nothing has been done yet; ", "You could also convert the GCode output of Slic3r into a robot program directly but then you don\u2019t get the editing abilities of ROS AM and the possibility to simulate the trajectory in ROS (you could simulate the trajectory with the robot manufacturer off-line programming software).", "The robot parameters etc goes into an other package, for example if you are using a Fanuc robot you would probably want to use the ROS-I ", " or ", " packages.", "Yes, it is plastic FDM.", "\nMy initial references (and problably I\u2019ll end up doing like that) were to follow a regular 3D printing process, using  a slicer to generate GCode and then to convert those targets into robot movement.", "\nHowever, as I found ROS I was trying to implement the Project using its packages, as it seems to be really powerful. My robot arm is a 5 DOF Igus Robolink. As far as I know, there is few development of Igus packages.", "\nHow can I convert the Slic3r GCode output into a robot program? Is there any developed package/software to do that (like Moveit?) or that has to be done from scratch?", "I don\u2019t know much about Igus robots nor about a bridge to ROS.", "You should write a program that reads the GCode and transforms it into robot program (depending on the programming language used on the Igus robot). This is not best as this is a conversion. You could also modify Slic3r so that it does not output GCode but instead directly Igus compatible machine code.", "MoveIt will allow you to simulate the robot and do the path planning. It is not MoveIt that does the connection to the robot or a possible conversion.", "You might want to have a look at ", "Or it\u2019s newer cousin/evolution: ", ".", "Thanks for your responses!", "\nI\u2019ll ckeck out those links.", "Hi,", "\nHow its can development for ROS Indigo?", "Hey all! Anyone here part of the ROS meetup in Seattle, WA, USA? I\u2019m launching Women in 3D Printing in the city and involved in a ROS robotics company here. Was curious to see if there were people working on a 3D printer. Way cool, thank you!", "Edit", "\nMeetup here: ", "Hi,", "\nHow its can development for ROS Indigo?", "I have not tested ROS Additive Manufacturing with ROS Indigo but it should work fine.", "I\u2019m in France so I won\u2019t attend to the meeting in Seattle. It would be nice if you could share with us who was interested in 3D printing, what has been done for metallic 3D printing etc. !", "Here is the meetup! ", "There is also one in Paris: ", "We have meetups all over the world, check the website: ", "Thank you for your interest!", "Hello! I am working now in a similiar project, but with a KuKa youBot. How\u2019s your progress on the conversion from GCode into robot trajectory so far? I would like to know more about that. Thank you!", "Regards", "We are not working with GCode at any stage of the software.", "\nROS Additive Manufacturing generates a trajectory that is a ROS message (defined here: ", ")", "If you want to move a robot there are two options:", "For the first option you need to write a post-processor, that will convert the ROS Additive Manufacturing trajectory into a Kuka (in your case) program that contains the right instructions for your process.", "\nThis process is explained here: ", "For the second option you need to read the ROS Additive Manufacturing trajectory and feed it to MoveIt (we are not using this approach as of today).", "There are post-processors for the Kuka robots inside the ", " repository, I have started a repository that allows to use these post-processors as ROS services: ", " (I\u2019m focusing on Motoman now but merge requests for Kuka are more than welcome!)", "For the second option you need to read the ROS Additive Manufacturing trajectory and feed it to MoveIt", "Just a clarification: you don\u2019t need MoveIt necessarily. If your trajectory is compatible with what your driver accepts (ie: a ", " or some driver-specific representation), then it should be possible to send the trajectory to the driver immediately. No need for MoveIt as an in-between.", "hello thank you for your reply.", "for the second option, how can I get the generated trajectory ROS message? In the example GUI, do I have to export the trajectory message into a bag file and then read the bag file to feed it to MoveIt topic?", "I am new to ROS and hope you can clarify this. Thank you.", "The trajectory is published on a topic.", "\nEvery module publishes a new trajectory after it\u2019s done working with the trajectory.", "So for example if you modify the trajectory, the output of this operation will be the full trajectory (", " type) being published on the ", " topic.", "All you need to do is create a subscriber that listens to the trajectory topic. Each time a new trajectory is generated, modified etc\u2026 you\u2019ll get a new message containing the latest trajectory.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Generate a robot program, upload it on the robot and run it.", "Use a ROS driver, connect to the robot and use MoveIt to move the robot."], "url": "https://discourse.ros.org/t/ros-additive-manufacturing-ram/3170"},
{"title": "TZC: Efficient Inter-Process Communication for Robotics Middleware with Partial Serialization", "thread_contents": ["Hello", "Here is an efficient Inter Process communication, which works better than existing ROS1 and ROS2 framework.", "The TZC can be easily integrated into the ROS framework.", "Have a look", "Courtesy: ", "Any link to the library and code examples to get a feel of it?", "The paper  does not contain any link to the implementation of their transport logic they proposed.", "  I will get back to you regarding the code. I have sent a mail to the author. Hope he will respond soon.", "Regards", "Skimming through it, it would seem that from a very high-level this is similar to using shared-memory for the entire message (ie: like nodelets in ROS 1) but then using pub-sub for the ", " in the shared-memory segment.", "It\u2019s not clear to me from the paper what the benefit is of splitting up the message in a part that uses regular serialisation (\u201ccontrol part\u201d) and a part that gets pushed into shared-memory: it would seem that you get the disadvantages of using a shared message anyway (ie: nodes cannot change \u201ctheir copy\u201d without affecting the message contents for all subscribers), and messages cannot be presented to callbacks without having deserialised the control part.", "The latency improvements are to be expected: (de)serialisation times in ROS 1 and 2 are typically mostly dominated by fields that are larger than the L1 / L2 cache of the CPU used (depending on memory bandwidth), so if you can skip those it will instantly improve message throughput (almost linearly).", "The authors also don\u2019t discuss compatibility with nodes not using this transport. From the description it would seem there is no fallback to full serialisation when communicating with non-extended nodes. Nodelets are certainly not perfect, but at least supported some measure of graceful degradation.", "Edit: re: why split the message: it\u2019s not made very clear, but this sentence seems to provide a possible rationale:", "The ETHZ-ASL framework eliminates copying operations, but multiple serialization operations remain. This is because the whole message is too complicated to be shared within shared memory without serialization.", "\u201ctoo complicated\u201d is a bit vague though.", "I ", " that \u201ctoo complicated\u201d is a simple way to say that we can\u2019t simply share objects in shared memory that use memory allocations and non-continuous segments of memory (in practice, every message which has a string or a vector with non fixed size).", "This is \u201cserialization 101\u201d ", "Sure, only PODs can be directly shared.", "It would have been nice if the authors had made that more explicit though. The paper seems to step over that bit of rationale, which makes it not very obvious why they chose their approach.", "Hi, ", " ", ". I am one of the authors. Thanks for your interest.", "\nThe code is not fully prepared for reading yet. I hope we can sort it out soon.", "Thanks for your comments! Here is some responses. I hope these responses will make my point clearer.", "We have argued in Section II.C that intra-process communication (such as nodelets) is the only efficient solution for now. But the obvious drawback of intra-process communication is fault isolation. Since all modules run within the same process, when any module crashes, the entire system crashes. There are applications that pay more attention to reliability and TZC can provide an option.", "Combining socket and shared-memory may inherit their disadvantages, but also their  advantages. By using socket for the control part, we can use compatible select/poll notification interfaces; and by using shared-memory for the data part, we can skip serialization for most of the data. As for the disadvantages you have mentioned, I don\u2019t consider them serious because:", "\n2.1. If a subscriber need to change the message, it can always copy the message and edit its copy and suffer the copying time. But there are practical callbacks that need not change the message and TZC provides an optimization.", "\n2.2. Shared-memory IPC does not provide proper synchronization (or notification) mechanism. We have to notify the subscribers through another channel. The control part is used for that and it is usually small enough (although it is larger than a reference) to omit its serialization latency.", "The latency improvements are to be expected, ", " we can skip those serialization operations. How to skip serialization for inter-process communication is the main contribution of this paper.", "As shown in the example code, TZC generates new message types and works per topic. You can always publish ROS (1 or 2) messages without TZC.", "About why split the message.  ROS transmits all message data through socket and ETHZ-ASL transmits all message data through shared-memory, but both of them can not avoid serialization. We split the message to avoid serialization of most of the message data (i.e. the data part).", "At glance it looks very similar to ", "\nAre there significant advantages in TZC?", "Thanks for the clarifications ", ".", "Just to make sure: bw-compatibility with nodes not using the new transport is not supported, correct?", "In fact, shm_transport is our previous work and TZC is based on it.", "Soon after we open-sourced shm_transport, we found that it is much like ETHZ-ASL shared-memory framework in terms of performance. Both shm_transport and ETHZ-ASL avoid copying operations by serializing each message into shared-memory at the publisher side and de-serializing it at the subscriber side. Such a mechanism can reduce the latency by 2-3 times comparing with ROS. However, we were not satisfied because the serialization operations are still a performance bottleneck. Thus, TZC is inspired.", "TZC not only avoid copying operations, but also avoid serialization operations for most of the data. Therefore, the latency no longer increases with the message size grows which is good for large message transmission.", "Currently yes. But as we have mentioned in the future work, we are planning to provide this kind of compatibility. For ROS1, the publisher-subscriber link mechanism makes this plan feasible, but  we still have no idea how to support it for ROS2.", "Hi everyone ", ", ", "!", "\nI have just shared the source code of TZC at ", ".", "\nPlease have a try and contact me if there are any concerns or suggestions.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/tzc-efficient-inter-process-communication-for-robotics-middleware-with-partial-serialization/6264"},
{"title": "Precision Agriculture Simulation with a Quadcopter in Gazebo", "thread_contents": ["Greetings everyone,", "For a while I was thinking to develop a project related to Agriculture. Because of the intense labor required during the fruit picking (it worsens at taller trees), I thought it would be good idea to develop an autonomous drone that finds and picks fruits with a small arm attached to its base (or to any convenient location). Since I can\u2019t afford the hardware currently, I consider simulation as a good start.", "So, the software stack will roughly comprise:", "I started with Gazebo simulator. Nevertheless, it is very difficult to find high resolution fruit trees on the web. I created my own tree with Blender, but it is far away from being realistic. So, I\u2019m looking for any contribution on this subject.", "There are a lot of things to tell about the project since I have done quite a work, but to cut it to the chase I\u2019m giving the ", ". Readme is a bit outdated, but it will be updated soon. I will try to post here about the project on a regular basis.", "I\u2019m looking for contributors on any part of the project. All the contributions are highly encouraged.", "I would start by just gathering photos of trees, and trying to detect apples on them.", "That is definitely a stage, but latter in the project. Currently, I\u2019m seeking the ways to do detection on simulation with mildly realistic trees at least.", "You can see the version 1 of exploration ", ".", "Currently, the volume of interest is specified with 6 constants  - XMIN,XMAX,YMIN,YMAX,ZMIN,ZMAX. In future, it would be a good idea to support it to be specified in run time via graphical tools (e.g. hand-drawing through Rviz).", "I\u2019m using MoveIt! to exploit its Motion Planning pipeline in which the planners take the instant ", " into consideration when properly integrated to the system. MoveIt! also supports ", " specification as a plugin (i.e. controller), but  I only use ", " currently by reading the waypoints into a vector of Pose\u2019s and sequentially execute them. Dynamic Collision Checking and Replanning done with the callback attached to action client by checking the validity of path through isPathValid() method of ", " during goal execution of action server. PlanningScene is fetched via ", " service provided by the ", ".", "Also, I want to list some issues that need to be resolved at version 2.", "Orientation fixation of quadcopter before motion. Since Kinect (or any other stereo camera) does not have 360 degree of FOV, the velocity vector and the orientation of the camera should be equal. Otherwise, drone might not see its front exactly; thus couldn\u2019t notice the obstacle in front and thinks the latched path is still valid, whereas it isn\u2019t.", "Implementation of a velocity controller or directly using existing alternatives, if any. By this, motions will be much smoother with respect to position control.", "Implementation of a Frontier approach that determines the next goal from extracted frontiers. Currently, the goals are hardcoded in a way that traverses the faces and corners of the rectangular volume.", "I haven\u2019t recorded yet the version 2 of exploration stack.", "All the To-do\u2019s from version 1 are implemented mostly.", "All the experiments are made in exactly 10 simulation minutes and the metrics is the explored percentage of volume of the VOI. There are two different configurations. First one is the fast-forward option, in which the candidate frontiers with the same distance are eliminated in one pass. The other one is the meaningful separation between candidate and registered frontiers.", "Fast forwarding definitely improves the exploration. However, na\u00efvely increasing the range does not have the same effect on FVF. To be honest, either methods don\u2019t possess a crucial supremacy over each other. Much more intelligent algorithms and approaches are required after this point. Version 3 of the exploration will focus on this feature.", "You can see the version 3 of exploration in this video:", "The problem diagnosed as ", " is mitigated with the addition of randomization into the system. So that, it both curates the distant and closest frontiers of any degree. With a fully randomized decision making process, it naturally boosted up the performance and with this sole trick, the exploration rate has reached to 27-28%. Then, another problem is diagnosed. The frontiers were so granular that almost identical frontiers are behaved as different and drone visited same places repetitively. In order to resolve this issue a grid approach is embraced in which a cell could be visited only once. This approach increased the exploration rate a lot. For example in 25x25 case the exploration rate reached up to 32%. Actually, as long as the grid size is reduced, the exploration rate increases. However, of course there is a saddle point which results in the complete failure of the system regarding to new frontier discovery if overcrossed. Consider 1x1 case to understand that.", "In total; 15x15, 14x14 and 13x13 cases are experimented. Their respective exploration rates were 35.6%, 37.4% and 40.7%. During experimentations, I have diagnosed another interesting feature of the problem. OctoMap is not uniformly investigated in terms of frontiers, therefore a side of the volume remains highly explored whereas the other does remain unexplored. In order to mitigate this problem, and have higher exploration rates in 10 minutes one can embrace a better, more advanced heuristics.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Exploration", "Tree/Fruit Detection", "Fruit Manipulation", "\n", "\n", "\n", "\n", "\n", "\n", "Orientation Fixation is now perfectly handled.", "Velocity controller produces motion commands according to whole trajectory instead of separate waypoints.", "2 Frontier approaches. ", " and ", ". Both approaches have their own problems. For example, first one suffers being trapped in a sub-region of the volume of interest. Even though the previously explored frontiers and their proximity are checked for to eliminate the problem, it stands still. Latter one suffers a similar problem in fact, which I call as ", ". Since it searches for the farthest frontier, it generally returns back to the starting region and then pushes the true unknown field a little bit. In order to choose between them, I have made several experiments:"], "url": "https://discourse.ros.org/t/precision-agriculture-simulation-with-a-quadcopter-in-gazebo/6025"},
{"title": "Use ROS to talk to the VEX Cortex or V5 Brain!", "thread_contents": ["Dear ROS community,", "I would like to announce two new packages that connect ROS with VEX Robotics hardware. More specifically, these packages, as a part of ", ", allow for the VEX Cortex and VEX V5 Brain to send and receive messages to a ROS graph running on Linux. The full post can be found at the link below!", "The VEX microcontrollers are designed to be compatible with a selection of cost-effective, plug-and-play sensors and motors, provided by the VEX platform. I am especially excited about the VEX V5 hardware, which offers integrated motor-sensor modules, vision solutions, and even (eventually) LiDAR. Now, the VEX platform is at your disposal to control with ROS. To learn more about VEX V5, see the links below.", "I hope these serial clients will be the perfect stepping-stone for learners, because using the VEX platform is a perfect low-cost way to be introduced to ROS. I also hope the packages will enable you to create projects with VEX-brand \u2018intelligent simplicity\u2019. With VEX\u2019s end-to-end simple hardware platform, and ROS as the framework, a whole new niche of cheap projects and prototyping is made possible, which will enable more people to try, learn, and create robotic systems.", "The packages ", " and ", " use Purdue Robotics Operating System (PROS) for the VEX kernel and CLI toolchain. The serial clients can be found linked in the post above, complete with instructions for installation, examples, and more. I hope this project helps you with your robotics projects!", "Original Post on the Vex Forums:", "\n", "For more information about VEX V5:", "\n", "The VEX Robotics Design System offers students an exciting platform for learning about areas rich with career opportunities spanning science, technology, engineering and math (STEM). These are just a few of the many fields students can explore by...", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/use-ros-to-talk-to-the-vex-cortex-or-v5-brain/6520"},
{"title": "IMcoders: easy-to-install (and cheap) odometry sensors", "thread_contents": ["Hey there,", "We would like to introduce you our project: ", ".", "A few months ago we started developing some sensors in our spare time to provide odometry data for robot with wheels. One of the main objectives of the project is to develop sensors extremely easy to integrate in already developed devices. The provided odometry data can be used among the output of other sensors to navigate autonomously.", "We presented this project to the ", " under the category ", " so in case you want information about the development process, just take a look here: ", "Here it is a short introduction before going to the problematic:", "If you want to prototype with autonomous navigation on real vehicles the options are quite limited.", "Imagine you want to make the forklift in a warehouse, a tractor at the field or a wheelchair at the school to navigate autonomously, to prototype and get some first hand input data you could easily integrate cameras and get some visual odometry or attach a GPS device for the use cases when you are outside. Perfect, you can navigate using that input but still the system is not reliable enough, maybe there are not features for computing a reliable visual odometry or you are inside a building and there is no GPS signal, wouldn\u2019t be nice to have some encoder input? Let\u2019s add them to our vehicle!.. hmmm not so easy, right? If you are good at mechanics, you could install some encoders in your vehicle, but on \u201coff the shelf\u201d vehicles, the hardware modifications needed to add this sensors are not a real option for all the people. At the moment, there is nothing mechanical and money-wise affordable for everybody. In order to meet this needs the IMcoders are borned.", "For that we are using IMUs but not in a conventional way. The idea here is to attach an IMU to each robot wheel and measure its spatial orientation. Tracking the change of the orientation of the wheel we can infer how fast the wheel is spinning and, if needed, its direction. (Yes, as you probably already notice the Idea is to provide an output very similar to a traditional encoder, just from a different source, hence the name IMCoder = IMU+Encoder)", "You could think this approach has a lot of error due to the nature of IMUs (and of course is not the perfect solution for every use case!) but adding some constraints based on the location of the IMUs on the robot most of the error can be mitigated so that the output provided is stable for most use cases.", "After some simulations, we developed some IMU wireless boards which provide IMU data using the ROS interface:", "It means that combining some of them and using some theory about differential drive steering, we might be able to calculate a reliable odometry. So that\u2019s what it is almost happening. To focus on the odometry calculations we created a simulation environment using gazebo and we attached one IMU (using the gazebo IMU plugin) to each wheel of our simulated differential drive robot. It is almost working as expected: the calculated odometry using our sensors is quite similar to the one provided by the diff_drive plugin for gazebo. We say almost because there is still a mismatching between the output odometry provided by the diff_drive odometry plugin and ours. We guess there is something we are not considering within our calculations for the odometry, so our output it is not as good as expected (it is our first time working with quaternions).", "Summarizing what we are doing:", "We get the IMU absolute orientation as a quaternion in one time instant and also in the next one.", "We compute the quaternion that defines the rotation between the first measurement and second one", "The rotation of the sensor is translated to linear velocity (we know the diameter of the wheels).", "With this information (linear velocity of each wheel) and a little bit of theory about differential driving vehicles, we are able to compute the new position of the robot.", "In the image there are three arrows: the orange one corresponds to the diff_drive gazebo plugin output and the red/green one (they are the same) corresponds to our output.", "It is easy to observe that after some left/right turning the odometry we are computing it is accumulating some error.", ". Concretely where the odometry is being computed.", "In case you want to reproduce the problem ", " in the repository (more precisely, the ", " part) and you will be able to play around with our simulation environment.", "Once the problem is solved we will continue integrating the sensors in a comercial RC car (Parrot Jumping Sumo) and testing them with real data:", "Hi , thanks to share this very interesting and ingenious approach.", "\nWhy do not use directly the rotation rate provided by the IMU? instead of the absolute orientation, which requires extra computations at IMU level, usually involving a 3D compass, which may not work correctly close to wheels and motors.", "\nSo my proposal would be to directly use rotation rate provided by gyros, and with diff-drive forward kinematics compute platform velocities.", "\nThis would lead to an even smipler approach , where a single 1D gyro could solve the problem (ideally).", "\nWhat do you think ?", "Hello ", ",", "Why do not use directly the rotation rate provided by the IMU?", "The point is that the gyro provides just a rotational speed measurement, which drifts. The main advantadge of our approach is that, using the gravity, the computed movement can be corrected. For instance, imagine that the algorithm computed that the wheel moved pi/2 but it really moved 3pi/8. At the end of the movement (when the robot is standing still), the gravity will help to correct the movement because it is always point the same way. Further more, during the movement the measured acceleration will always belongs to the inferior semispace (its z component will always be negative), so we even might be able to make some corrections during the movement. But this approximation is still to be tested.", "That\u2019s the main advantage of having absolute rotation measurements instead of relative ones.", "Thanks for the clarification", "\nok , I see you use \u201cimu.orientation\u201d to compute wheel angular position. This orientation is not a raw measurement of an IMU, its an estimate, often provided by sensor itself, using raw measurements (a_xyz and w_xyz). Using this angular position, as you said, there is no position drift due to velocity noise time integration.", "\nBut I wonder, if your accelerometers are not just exactly on the axis of rotation, additional linear accelerations will be measured by accelerometers, not related with the gravity, so the wheel orientation computations may be corrupted.", "\nWhat do you think about ?", "You are right. We already considered that and we already saw that, for quick translations, the estimated orientation is erratic. Thus, depending on the use case, our sensors will provide a better or worse output depending on the quality of the IMU integrated in the board (sometimes it is just a matter of calibration). Here we are facing a trade-off between the cost of the sensor and the expected accuracy.", "Anyway, for the applications as the ones exposed above, we guess that the accuracy for the sensor we chose (MPU9250) for the first prototype will be more than enough.", "I hope I answered your question. Really thank you for the interest by the way ", "Hey, just a small update:", "Yesterday we\u2019ve been working on the mechanism for mounting the sensors on the wheels. Now it is really easy to mount/unmount them:", "Note we are not using the compass for computing the orientation of the IMU. Thus, we don\u2019t have to worry about the magnets.", "We recorded some datasets and we will post an update as soon as we can.", "Thus, depending on the use case, our sensors will provide a better or worse output depending on the quality of the IMU integrated in the board (sometimes it is just a matter of calibration).", "How are you currently calibrating for the extrinsics of the origin of the IMU sensor with respect to the center of rotation of the wheel axis? After fixating the IMCoder to the wheel in a new position, are you by chance rotating the wheel at a constant velocity, then inferring the rotational speed ", " from the sinusoidal frequency from the two IMU accelerometers axes perpendicular to the wheels axle, then additionally using the amplitude to infer the radial distance ", " from the wheel axis, and the phase of the peak of the waveform to discern the angular position ", "? I suppose the phase between accelerometer axes x, y (assuming z points along the wheel axle) could be used to resolve the rotation of the IMU about the endpoint of the vector ", " if neither ", " or ", " happen necessarily lie along ", ".", "I\u2019m not sure how level the mounting of your fixture is to the wheel, or even if the toy\u2019s wheel and axles would be true (straight); if it\u2019s only roughly perpendicular, the offaxis comentents may then need to be accounted for as well, necessitating a full 6DOF calibration rather than just a 3DOF calibration. Additionally, if positioning is subject to disturbances during reinstallment, like in the case with the rotationally symmetric magnetic clips, then perhaps making the calibration online or as a tracking filter might be appropriate to simplify deployment to arbitrary platforms.", "How are you currently calibrating for the extrinsics of the origin of the IMU sensor with respect to the center of rotation of the wheel axis?", "Hey, sorry for the late answer. Due to the youthness of the project and our limitated time for developing, we are not doing right now any kind of calibration. We already considered what you exposed and the intention is to follow that path but just if necessary.", "The main goal is to have a system providing an odometry good enough for being used in autonomous navigation among other inputs (e.g., visual odometry, GPS, UWB sensors\u2026). Thus, our developing line is going iteration by iteration checking what\u2019s really necessary.", "So now going back to the topic, regarding the perpendicularity of the robot axles: that\u2019s a good point, for a first approximation, we just assumed that they are perpendicular. Last day we recorded some datasets we are probably checking this weekend.", "What we are trying right now is finding out a \u201cground truth\u201d to compare the output of our algorithm against. For that, we thought about computing a visual odometry using the robot\u2019s camera (but we don\u2019t know how good it is) or using aruco markers for getting the position of the robot.", "Do you think we are following the right path? What would be your proposal given our time restrictions?", "What we are trying right now is finding out a \u201cground truth\u201d to compare the output of our algorithm against. For that, we thought about computing a visual odometry using the robot\u2019s camera (but we don\u2019t know how good it is) or using aruco markers for getting the position of the robot.", "Do you think we are following the right path? What would be your proposal given our time restrictions", "I\u2019m not sure as to the scale or distance you\u2019d like to test against to compare your odometry with, (are we talking like looping around a table or a building?), but usually benchmarking against a established SLAM algorithm (as opposed to a odometry method) would still be useful, like cartographer. Try and use a SLAM approach where data association of landmarks would be less of an issue, and when odometry sensing is optional for runtime. If you don\u2019t want to use a LIDAR, or can\u2019t fit one on the platform, but say only a onboard camera, you could use something like this:", "Simultaneous localization and mapping using fiducial markers. - UbiquityRobotics/fiducials", "If the platform is sensor deprived, i.e. you can\u2019t tack on camera on it, you could flip the problem and go the poor-man\u2019s-motion-capture route using a fixed facing camera and a printed fiducial taped to the robot:", "The Why and The What Flying tiny drones indoors is cool, no questions asked. And stuff gets all the more interesting\u00a0when you can accurately control the drone's position in space ", "I\u2019ve used this april tag library before for something related in previous SLAM development and liked the packages features:", "ROS wrapper for the Apriltags visual fiducial tracker - personalrobotics/apriltags", "The Golem Lab at Georgia Tech used something quite like, using a six camera overhead vision system when a proper mocap system was unavailable. Just be sure to disable any autofocusing features if your using a cheap web camera or something.", "This Georgia Tech robot is smart enough to shove furniture out of the way to get where it wants to go", "We recorded some datasets and we will post an update as soon as we can.", "Hey guys, just a small update. After analyzing the datasets and watching at the output we can confirm what we had at simulation.", "We get a valid odometry! Well, at least the output makes sense. The data is a bit noisy and we still have some things to tune but it is possible to see that our approximation is really working.", "Here is a gif for a linear trajectory:", "It is intended that we upload the datasets in case somebody want to test itself.", "Hi there,", "just wanted to say that after lots of hours working, we could finally get everything ready and today we submitted our project to the contest.", "Here is the video we used for it:", "Hope you like it.", "In case you want to know more, all the information is in the project web you can find at the beginning of this post.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/imcoders-easy-to-install-and-cheap-odometry-sensors/5543"},
{"title": "Niryo One: an educational 6 axis robotic arm powered by ROS", "thread_contents": ["Hi Everyone,", "I\u2019m excited to share this project with you and the ROS community. First I want to thank all of the ROS maintainers and developers for their amazing job. Without ROS I\u2019m not sure we would\u2019ve been able to create such a robot in a short time.", "Quick story: after our studies (engineer school) we quickly noticed that industrial robots are way too expensive and complicated for schools and universities. So we decided to create an educational accessible robot that will be easy-to-use. We have successfully finished a Kickstarter campaign in early 2017 (for this campaign I also posted on ROS discourse), and then continued the development of the robot.", "Now, after 2 years of development and beta tests, we are now ready and Niryo One has been officially launched in September! You can now order the robot directly ", ".", "Niryo One is a 3D printed 6 axis robotic arm powered by ROS. We have decided to release all the STL files for 3D printing and (what\u2019s most interesting here) all the ROS packages that we developed for the robot.", "You can find the ROS code ", ". The code is actually running on a Raspberry Pi 3 board inside the robot. We are using Xubuntu for RPi. For now we work on Ubuntu 16.04 and ROS Kinetic, and plan to switch to Ubuntu 18/Melodic for late 2018/early 2019.", "Here are some ROS features and packages that we use:", "So, we basically use all the ROS Industrial stack on this robot.", "There is a simulation mode so you can just install the packages on your computer and see the robot in Rviz. The README on the github repo provides a tuto to install the full Niryo One ROS stack. Also, a gazebo integration will be released for the end of the year.", "Some additional developer interfaces:", "As a side project we also have developed an open source ", ". The goal of the app is to compare the theoretical trajectory given by Moveit! with the real trajectory (listening to the /joint_state topic, directly with the data coming from the motors real position)", "For the end user, there is an available desktop application (Windows/Linux/MacOS) that we named Niryo One Studio. The goal of this app is to simplify the programmation of the robot, so that everyone can use it. We have included the Blockly library (developed by Google) so you can use block programming + move the robot with your hands thanks to the learning mode. Under the hood, the Blockly XML is translated to Python code. This Python code is using the Python API that I described before.", "This app can be used with both the real robot and the simulation mode running on your computer.", "Some additional info about the mechanical/electronic structure:", "You can also find a complete documentation on our website ", ". This doc includes:", "If you\u2019re interested in Niryo One (for a ROS development project, or to teach robotics), feel free to ", ", where you can also get more info (tech specs, tutorials, etc).", "Thanks for reading this! That was quite a long post I confess.", "I\u2019d be happy to answer to all your questions and get your feedback ", "Hi", "\nI\u2019m very interested in your robotic arm and want to make one by myself. but I can\u2019t find the stepper motor model. pls tell me if it\u2019s convenient. thank you.", "Hi ", ",", "We use NEMA 17 stepper motors along with a custom electronics board on the back, including an Arduino-compatible microcontroller and a position sensor with a magnet on the motor shaft. You can get the layout overview on ", ", as well as the code running on the microcontroller. Now, we don\u2019t sell the motors or boards separately, but you can find a Maker Kit on our website, with all the Niryo-custom parts (so you have to buy the rest + 3D print the robot using the open source STL files).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["URDF", "Moveit! for motion planning", "ros_control with a FollowJointTrajectory controller. For the hardware interface part of ros_control, we have developed a CAN bus driver, and a Dynamixel driver compatible with XL-320 and XL-430 models", "rosbridge_suite to communicate with non-ROS environment. This is particularly useful for us to connect the desktop application using the roslibjs library", "joy for Xbox control", "\n", ". The goal of this API is to hide ROS so that developers can directly give commands with Python.", "\n", ".", "\n", " (SPI-CAN bus interface) for Raspberry Pi", "The robot is entirely 3D printed (about 60h printing time)", "First 3 axis are powered by a stepper motor with a custom Arduino board. We can get the position of the motor to make a close loop control", "Axis 4 and 5 are powered by Dynamixel XL-430 (I\u2019ve seen it\u2019s the same motors used on the Turtlebot 3 Burger)", "Axis 6 + grippers are powered by Dynamixel XL-320 motors. We have developed multiple tools that you can all plug at the end of the arm.", "There is a panel connector at the back of the robot, with some pins connected to the Raspberry Pi GPIOs. So you can use those pins to easily communicate with the outside (see ", " on how to control multiple Niryo One robots with an Arduino board)", "Assembly guide (+soon maintenance guide)", "Updating software tutorial", "Debugging the robot", "User manual", "Developer tutorials", "Maker tutorials"], "url": "https://discourse.ros.org/t/niryo-one-an-educational-6-axis-robotic-arm-powered-by-ros/6289"},
{"title": "Raspberry Pi ROS + OpenCV robot kit for Makers", "thread_contents": ["Hi All,", "I\u2019ve been putting together a ROS+OpenCV robot kit for hobbyist ie \u201cMakers\u201d. What does that mean? Well the thought is - simple ROS software stack that can be extended but relatively easy to kick off and have the \u201chardware\u201d to something hello-worldy quickly; tolerant to a level of \u201cscrappiness\u201d ie wires hanging out, not drop-proof tolerant; lastly economical and extensible using hardware that\u2019s readily available via any of your favorite online sources.", "Under the name ROSbots, ", ".", "Love to hear the communities thoughts and feedback.", "Thanks!", "\nJack \u201cROSbots Maker\u201d", "Hi Jack,", "I feel that is similar to the \u201cduckiebot\u201d developed by ", ".", "Also, I think that  image processing with a raspberry is a little complex because you would lost many frames (works like 10 fps if you apply some filters with decent quality (640x480)).", "It could be interesting use a odroid ux4 or some main board with more power and a lower cost.", "Hi Rodrigo, thanks for the note.", "I actually got the Duckietown ROS software stack running on our ", ".  Our ROSbots kit is functionally equivalent to the duckiebot.", "I reached out to the duckietown team members to see if we can work together.  ", ".  I believe our two efforts complement each other very well (we are focused on HW, them on curriculum framework for ROS).", "As for vision on a RPi, totally agree.  I\u2019m thinking at having a ROS container and/or VM that runs on a laptop/server (maybe even in the cloud) do the heavy lifting computations, while pointing at the ROS_MASTER_URI on the ROSbots Pi running ROS.  Assuming you have the bandwidth to stream the images from the Pi Camera to the machine at 15-30 FPS (which shouldn\u2019t be a problem compressed at 640x480), you can run vision, localization, whatevers with another ROSbots \u201cbrain\u201d living off the actual RPi.  Hope that makes sense!", "Appreciate the odroid suggestion but I\u2019m trying to avoid requiring anyone to buy a >$50 SBC and/or co-processor.", "Let me know if you have further feedback or suggestions.", "BTW - what are you working on using ROS?  Love to learn more.", "Thanks", "\nJack \u201cROSbots Maker\u201d", "I reached out to the duckietown team members to see if we can work together.  If you know anyone on the Duckietown team, would appreciate an intro.  I believe our two efforts complement each other very well (we are focused on HW, them on curriculum framework for ROS).", "I\u2019m participate of duckietown Chile, but i don\u2019t  know anybody in the MIT team or another team.", "As for vision on a RPi, totally agree.  I\u2019m thinking at having a ROS container and/or VM that runs on a laptop/server (maybe even in the cloud) do the heavy lifting computations, while pointing at the ROS_MASTER_URI on the ROSbots Pi running ROS.  Assuming you have the bandwidth to stream the images from the Pi Camera to the machine at 15-30 FPS (which shouldn\u2019t be a problem compressed at 640x480), you can run vision, localization, whatevers with another ROSbots \u201cbrain\u201d living off the actual RPi.  Hope that makes sense!", "That make sense, is a very good idea process the data in a powerful computer to get a better result, could be in a docker it\u2019s a easy and quickly form to test.", "BTW - what are you working on using ROS?  Love to learn more.", "I am a undergraduate of Computer Science student and I am working with a Parrot Ardrone with [pharo] (", ") and [LRP] (", ") , a live programming language designed for the creation of the behavioral layer of robots. I am also part of duckietown Chile where we use robots to teach ROS to second grade engineering students.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/raspberry-pi-ros-opencv-robot-kit-for-makers/3747"},
{"title": "Game developed using RVIZ", "thread_contents": ["Interaction b/w real World and virtual World to play classic pac man game - NalinLuthra/pac_bot", "Any video? not that much to see in that repo. The README is actually from the aurco_ros package", "Video would be nice for sure \u2013 sounds like a fun project!", "It looks like they linked a subdirectory ", ".", " I have a suggestion for you, however: you seem to have packed an entire catkin workspace into your repo\u2026 I\u2019d suggest you divide it, as this is the usual case for ROS packages.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["We have successfully designed a well developed physical replication of PacMan using physical hardware using indoor Navigation and visualising physical world using ROS with RVIZ"], "url": "https://discourse.ros.org/t/game-developed-using-rviz/6802"},
{"title": "Swift ros client", "thread_contents": ["I have published a Swift Ros client implementation on Github at ", "Hi Thomas, that is very interesting implementation. I would like to interview you for the ", " so you can share with the audience about its development, usage, maintenance, applications, etc. Please if you are interested contact me on (", ")", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/swift-ros-client/6812"},
{"title": "Finding objects in an unknown environment - ROS tutorial", "thread_contents": ["Hi ROS community,", "I would like to introduce our newest free ROS tutorial where you will learn how to use LIDAR and RGB-D camera of your robot in the same time to explore an unknown environment in order to find a predefined object based on a reference model. You can find it here: ", " .", "It is a first tutorial from a new series showing you how to use Robot Operating System to solve real life robotic problems. You can run that in Gazebo, ROS Development Studio by ", ", or ROSbot 2.0 platform.", "I hope it will be useful for you and I would be grateful for your feedback ", " .", "Best,", "\nDominik", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/finding-objects-in-an-unknown-environment-ros-tutorial/6847"},
{"title": "ROS Agriculture Community Meeting", "thread_contents": ["ROS Agriculture community meeting tomorrow night.  Tuesday, Nov 7th from 6-7pm PST", "\nClick the following link to join the meeting from your computer: ", "We will be reviewing the OSE MicroTrac automation with a RaspberryPi and a few other community projects.", "Jump in if you can make it.", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Nov 14th from 6-7pm  Pacific Time", "\nClick the following link to join the meeting from your computer: ", "Big thanks to Kyler Laird for reviewing his tractor automation.  Check out his YouTube channel here:  ", "If you would like to join the ROS-A Slack channel please email me at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Nov 21th from 6-7pm Pacific Time", "\nClick the following link to join the meeting from your computer: ", "Interested in IOT for agriculture?  We will cover a few of the technologies that are being used.", "If you have a project demo or package you would like to show email us at ", "Matt", "Hello Guys,", "I really want to attend this meeting, but it is night time as per Indian", "\nTime.", "\nDo you planning the video recording for  this.", "With best regards,", "\nSaurabh", "ROS Agriculture community meeting tomorrow night. Tuesday, Nov 28th from 6-7pm Pacific Time.", "New format using Zoom", "Join from PC, Mac, Linux, iOS or Android: ", "Or iPhone one-tap :", "\nUS: +16699006833,143104309#  or +14086380968,143104309#", "\nOr Telephone:", "\nDial(for higher quality, dial a number based on your current location)\uff1a", "\nUS: +1 669 900 6833  or +1 408 638 0968  or +1 646 876 9923", "\nMeeting ID: 143 104 309", "\nInternational numbers available: ", "Thank you Sabarinath M S for providing a list of IOT Ag devices: ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Dec 5th from 6-7pm Pacific Time.", "Web meeting link:  ", "Last weeks meeting is posted in the Slack channel.  Or you can view it here ", "If you would like to join the Slack channel please email ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Dec 12th from 6-7pm Pacific Time.", "Web meeting link: ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Jan 9th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have any questions about agriculture robots, please join us or sign up for our slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Jan 16th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have any questions about agriculture robots, please join us or sign up for our slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Jan 23th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have any questions about agriculture robots, please join us or sign up for our slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Jan 30th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have any questions about agriculture robots, please join us or sign up for our slack channel at ", "ROS Agriculture community meeting tomorrow night. Tuesday, Feb 13th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have any questions about agriculture robots, please join us or sign up for our slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Feb 20th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have an interest in agriculture robots please join us.", "Recording from last weeks community meeting ", "Matt", "I am interested to attend this evening\u2019s Agriculture community meeting. What is the time (6-7pm Pacific) for the meeting in GMT?", "ROS Agriculture community meeting tomorrow night. Tuesday, Feb 27th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have an interest in agriculture robots please sign up for our Slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Mar 6th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have an interest in agriculture robots please join us.", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Mar 13th from 6-7pm Pacific Time.", "Web meeting link: ", "If you have an interest in agriculture robots please join us.", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Mar 27th from 6-7pm Pacific Time.", "Web meeting link: ", "  If you have an interest in agriculture robots please join us.", "Sign up for the slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Apr 3rd from 6-7pm Pacific Time.", "Web meeting link: ", "  If you have an interest in agriculture robots please join us.", "Sign up for the slack channel at ", "Matt", "ROS Agriculture community meeting tomorrow night. Tuesday, Apr 10th from 6-7pm Pacific Time.", "Web meeting link: ", "  If you have an interest in agricultural robots please join us.", "Sign up for the slack channel at ", "Matt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-agriculture-community-meeting/3127"},
{"title": "Using your smartphone's built-in sensors as a ROS node", "thread_contents": ["Hi ROS community!", "We have created hNode - a free Android app that allows you to use your smartphone sensors in ROS. The smartphone is connected to the rest robotic system through a P2P, VPN network, so the latency is quite low. You can read more in the ", ". We\u2019ve also created a ", " showing you how to use hNode in your robotic project. Let me know what you think, and I hope it would be helpful in your next robotic project.", "All the best!", "\nDominik", "There\u2019s also an open source ROS Android sensor app designed to provide a proof of concept access to most of the sensors and for prototyping.", "ROS Driver for Android Sensors. Contribute to ros-android/android_sensors_driver development by creating an account on GitHub.", "Wiki page: ", " Though it looks a little bit stale now, and the version in the play store is no longer available.", "Can someone put up the latest APK of the same somewhere . Its a big pain to get Android studio setup , especially with ROS .", "I also found ", " and it nearly works out of the box for android sensor messages to ROS.", "Two important mentions:", "\n-add maven2 to gradle file otherwise it will not work", "\n-build the app from the linux terminal because then Gradle helps a lot", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/using-your-smartphones-built-in-sensors-as-a-ros-node/4305"},
{"title": "Fusion 360 add-in to export URDF", "thread_contents": ["Hi, ROS users!", "I wrote a fusion 360 add-in to export URDF from Fusion 360 models.", "See:", "\n", "For example, you can convert this cool robot arm(", ") into urdf with just one command.", " (upload://hqn8wGJBlaawLC6nvu2KpmyJ8yv.png)", "I\u2019m happy if you try this add-in and give me feedback.", "That\u2019s a very nice work ", ", thanks for sharing.", "I wrote a script to plot joint values written in a CSV file. It requires to model using the asssembly (I guess it is your case too).", "My script can be found in ", "It\u2019s not fully general, but going to a general approach and connect F360 as subscriber node to plot joint values coming from ROS is feasible, IMHO, which I think it\u2019d be beneficial to make the most of the F360 capabilities.", "I\u2019ll defintely try your code!", "Cheers", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/fusion-360-add-in-to-export-urdf/6944"},
{"title": "Agricultural Robot Functional Safety", "thread_contents": ["Jonathan Moore - Director Advanced Systems at exida LLC stopped by the ROS-A community meeting to discuss functional safety for agricultural robots.  Here is a recording of the presentation:  ", "\nFor more information about Functional Safety - Jonathan\u2019s contact: ", "ROS Agriculture community meetings are Tuesday nights from 6-7pm Pacific Time. Web meeting link: ", "Matt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/agricultural-robot-functional-safety/7036"},
{"title": "An Experimental JVM Based Robotics Platform (Work In Progress)", "thread_contents": ["I\u2019ve recently begun working on an experimental JVM based robotics platform that\u2019s loosely inspired by ROS. I think it\u2019d be great to open up robotics development on the JVM, and I\u2019m looking to experiment with how a JVM based approach would work and perform. The project is early stage but I thought I\u2019d start a thread for feedback and to share progress updates.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/an-experimental-jvm-based-robotics-platform-work-in-progress/6688"},
{"title": "Kuri Mayfield Robot", "thread_contents": ["I am working on Kuri from Mayfield robotics (ROS based), attempting to manage its available capabilities and augment them with more higher level tasks like Multi-robot collaboration or human-interaction.", "\nI couldn\u2019t find any topic here concerning this so I would love to get in touch with others who have worked/are working with Kuri.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/kuri-mayfield-robot/7044"},
{"title": "ROS OpenVINO Toolkit Release - Initial Version(V0.3.0)", "thread_contents": ["Hi All,", "We are happy to announce the initial release of \u201c", "\u201d. The ", " toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the Toolkit extends computer vision (CV) workloads across Intel\u00ae hardware, maximizing performance.", "\n", "\nThis project is a ROS wrapper for CV API of ", ", providing the following features:", "Support CPU and GPU. FPGA and Movidius will be supported in the future.", "\nSupport standard USB camera and Intel\u00ae RealSense\u2122 camera.", "\nSupport person-attributes-recognition, including face detection, emotion recognition, age gender recognition and head pose recognition.", "\nDemo application to show above detection and recognition.", "This project has been open sourced in github: ", ". Please refer to ", " file for more details about this project. We have tested it on RealSense D400 series camera and Standard USB camera. More features, such as object detection, vehicle-attributes-recognition and license-plate-recognition are under development and will be included in future release. Stay tuned. Welcome feedback and participation.", "You can also find ROS2 version of OpenVINO Toolkit ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-openvino-toolkit-release-initial-version-v0-3-0/7058"},
{"title": "MARA, the modular cobot powered by ROS 2.0", "thread_contents": ["Dear ROS community,", "After years of work, our team at Acutronic Robotics is thrilled to announce that our newly developed modular collaborative robot, ", ", powered by ROS 2.0 is finally available for pre-order. MARA stands for Modular Articulated Robotic Arm and has been built in collaboration with other manufacturers and out of individual modules that natively run ROS 2.0. The modular robot arm can be physically extended in a seamless manner and delivers industrial-grade features that we\u2019ve been discussing for quite a while in threads like ", ".", "We\u2019ll be announcing more related modularity products in the short-term. All powered by ROS 2.0. Exciting times!", "You can find more information about the robot at ", ". The robot will be displayed within the following days at ", " in Stuttgart so if you\u2019re around, stop by and say hi!", "In case you\u2019re interested on technical details, there\u2019s a lively discussion happening at ", " where our team will be answering questions.", "We\u2019re shocked and excited with the first commercial reactions received. We were originally planning to manufacture a small batch of 20-50 units but it seems we\u2019ll need to plan for more. If anyone in the community is interested I\u2019d recommend pre-ordering now to ensure we can fulfill the request by March 2019.", "Super exciting! Thanks for sharing!", "Got any simulation ready also?", "Sure! Take a look at the following links:", "Hi ", ",", "I just updated the ", " few minutes ago. Feel free to raise any issue in case of failure.", "It\u2019s also possible to move MARA with MoveIT! using bridges. You can find the details ", ".", "Cheers", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " (work in progress)", "\n", " (work in progress)"], "url": "https://discourse.ros.org/t/mara-the-modular-cobot-powered-by-ros-2-0/7095"},
{"title": "Agricultural Field Survey Robot", "thread_contents": ["Hi all,", "I\u2019ve started to build a robot!", "\nAn Agricultural Field Survey Robot (Surveying sensor is a downward facing camera)", "\nIt is has:", "Four wheels and skid steers.", "\nRaspberry3 running ROS Kinetic on Ubuntu Mate", "\nRaspberry Pi v2 Camera running using rospicamnode", "\nAn Arduino collecting data from wheel encoders and passing to Pi3 using rosserial.", "External Android-based RTK GPS unit giving a 2cm accurate fix (in optimal conditions) but only at a 1Hz interval.", "Forward facing Garmin Lidar Lite3 lidar unit - intended to help avoid obstacles", "\nIMU unit is fitted, but I have had trouble getting reliable Heading measurements from it.", "Progress so far:", "The robot drives following twist commands, with PID controllers using the encoder data to achieve desired rates of wheel turns. An odometry topic is produced, but skid steering and the slippy field surface means that this is quickly inaccurate, especially in turning.", "The robot drives around nicely under manually issued commands, or by a simple rule set applied to lidar measurements (e.g. dist <2m stop, dist 2-5m turn right, dist >5m go straight forward)", "I am now trying to move to use robot localization and then navigation to achieve a level of autonomy. This has proven to be a good deal trickier than I had expected!", "The first target is to simply have the robot move from a known start location to a newly specified location, say, 100m away in the field.", "I have been trying to feed the odometry, IMU and GPS data into robot localization to get the robot\u2019s position and orientation pinned down. Then use ros_navigation to navigate across a (for now) empty static map to the target location. I am running the localization and navigation nodes on a laptop over wifi - just in case the Pi3 isn\u2019t up to the task.", "The fusion has been problematic, the heading of the robot is a key piece of information and the IMU seems to be unreliable.  I have been around the loop several times getting the IMU orientation and handedness resolved, but have come to the conclusion that the heading data provided by my IMU is poor. I suspect it is struggling with interference from motors or other electronics on board, though seems to function correctly on the bench.", "ROS setup for localisation/navigation", "\nThe odometry, imu and gps fix data is fed into robot localisation as per instructions here:", "\n", "\nI have two EKFs running and the navsat_transform_node as described. The resultant odometry topic is then fed into a move_base instance along with a map_server.", "I am happy that the move_base end is working - when simulated it sends sensible looking twist commands to the wheels and shows appropriate progress on the map. But my localization isn\u2019t working.", "SO, has anyone got any comments on my setup or any pointers as to how I can get the localisation to behave? or perhaps some alternative approaches? Are there any really common \u2018gotchas\u2019 that I might have fallen foul of?", "For now, I am going to try ignoring the IMU and encoder odometry and see if I can get by with just GPS - not any good long term - but might get me through for now.", "You comments would be appreciated!", "Thanks in advance, Joe.", "Hello Joe,", "I am interested in learning more about this. Do you have an email I can reach you at?", "-Will", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/agricultural-field-survey-robot/3750"},
{"title": "ROS Visual odometry", "thread_contents": ["After this tutorial you will be able to create the system that determines position and orientation of a robot by analyzing the associated camera images. This information can be used in Simultaneous Localisation And Mapping (SLAM) problem that has been at the center of decades", "\nof robotics research.", "If you have everything, grab a cup of coffee, sit down, and read this tutorial.", "If you absolutely have no idea what is ROS, nodes and how they communicate with each other, I strongly recommend you to learn it by reading official documentation and completing tutorials for beginners. Alternatively, you can read our guides. explaining its ", " and teaching how to write simple publisher and subscriber either on ", " or ", ".", "\nSo, the graph of our system looks like this:", "As you can see in this picture, we have Raspberry Camera connected and raspicam creating multimedia pipeline and sending video from camera to gst-launch. The latter then transmit the image to our server over UDP. gscam will broadcast the video to /raspicam/image_raw topic. This image should be rectified with image_proc node. And finally, rectified image is taken by mono_odometer, which handles it and computes position and orientation of the robot publishing this data straight to the /vision/pose topic.", "Firstly, connect your camera to Raspberry. To determine whether it\u2019s working or not, just type:", "If you got ", ", ****then it\u2019s ok and you can follow the next step. Otherwise, you should enable your camera with raspi-config. Furthermore, you can test video streaming with ", ".", "I used ROS kinetic, but you may use anything you want. Check out the ", " to get it working. Installation of ROS is quite straightforward and usually doesn\u2019t produce errors. If you stumbled and got any, you can always ask for help on the ", ".", "In this section we are going to build our environment with every library we need. Here are the list of what we should install:", "The needed packages should be installed using a terminal and the following commands:", "Get the latest stable OpenCV version:", "Build:", "All the following packages should be cloned into ~/odometry/src, so", "Building everything:", "After successful building all packages let\u2019s get our system up and working. Firstly, ssh into Raspberry and start broadcasting video to our server:", "Where  is IP address of your server. If you don\u2019t know it, type:", "and find your network and your IP.", "After that change directory to ~/odometry/gscam/examples and create a new launch file called \u2018raspicam.launch\u2019:", "Paste the following and save it:", "Then launch gscam and see if you can get an image:", "Before Starting Make sure that you have a large ", " with known dimensions.", "\nTo calibrate camera we will use cameracalibrator.py node from package image_calibration which is already installed. To run it for a monocular camera using an 8x6 chessboard with 24mm squares  just type:", "You will see a new window opened which will highlight the checkerboard:. This will open up the calibration window:", "In order to get a good calibration you will need to move the checkerboard around in the camera frame such that:", "As you move the checkerboard around you will see three bars on the calibration sidebar increase in length. When the ", " button lights, you have enough data for calibration and can click ", " to see the results.", "Calibration can take about a minute. The windows might be greyed out but just wait, it is working.", "\nAfter calibration is done, you can save the archive and then extract it. You will need the *.yaml file. Rename it to \u2018raspicam.yaml\u2019 and move it to the \u2018~/odometry/src/gscam/example\u2019 directory. Then open file \u2018raspicam.launch\u2019 that we\u2019ve already created and change it, so that it should looks like this:", "After that you have your camera calibrated and can launch gscam by:", "The raw image from the camera driver is not what is needed for visual processing, but rather an undistorted and (if necessary) debayered image. This is the job of image_proc. For example, if you have topics /raspicam/image_raw and /raspicam/camera_info you would do:", "There will appear a new topic /raspicam/image_rect. It\u2019s exactly what we need.", "This will publish /mono_odometer/pose messages and you can echo them:", "If you want to visualize that messages that is published into /mono_odometer/pose, then you should install and build another one package:", "The ", " is a very simple plugin just displaying an OpenGL 3D view showing a colored cube. You can drag and drop a geometry_msgs/Pose topic onto it from the \u201cTopic Introspection\u201d or \u201cPublisher\u201d plugins to make it visualize the orientation specified in the message.", "Hi Kobylyanskly,", "Really nice work and I am excited to try this! I am trying to use your setup but couldn\u2019t launch raspicam launch file properly. Setup on Pi was fine and the video is broadcasting since I am able to stream the video with command $ gst-launch-1.0 -v udpsrc port=9000 caps=\u2018application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264\u2019 ! rtph264depay ! avdec_h264 ! videoconvert ! autovideosink sync=f", "\nThe problem arises when I run the raspicam.launch file and it returns the message:", "[FATAL] [1515599618.204792312]: GStreamer: cannot link outelement(\u201crtph264depay0\u201d) -> sink", "\n[FATAL] [1515599618.204858007]: Failed to initialize gscam stream!", "Do you have any idea what is wrong? My launch file is exactly the same as your post.", "Any help would be greatly appreciated.", "Erin", "My setup is Ubuntu 14.04, ros indigo, raspberian Jessie with Ros indigo", "Update: I found out this works for me:", "Also worked for me. I used the system with OpenCV3, ROS Kinetic", "What worked for you? Something went wrong with your post?", "In the launch file for raspicam.launch, the given enviroment variable for GSCAM_CONFIG didn\u2019t work for me. I had to modify it to this:", "I met the problem that gscam didn\u2019t stream anything. It\u2019s blocked in the line \u2019 GstSample* sample = gst_app_sink_pull_sample(GST_APP_SINK(sink_));\u2019 Have you ever meet the problem? I will be really grateful if you have some idea.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Introduction", "System architecture", "Preparing the environment", "Calibrating the camera", "Rectifying image", "Getting odometry", "Visualizing pose", "Raspberry PI running Emlid Linux distro with pre-installed ROS", "Raspberry PI Camera", "Server running Linux", "OpenCV", "libviso2", "gscam", "image_common", "image_pipeline", "vision_opencv", "checkerboard on the camera\u2019s left, right, top and bottom of field of view\n", "X bar - left/right in field of view", "Y bar - top/bottom in field of view", "Size bar - toward/away and tilt from the camera", "\n", "checkerboard filling the whole field of view", "checkerboard tilted to the left, right, top and bottom"], "url": "https://discourse.ros.org/t/ros-visual-odometry/2465"},
{"title": "Industrial_ci 0.7.0 released", "thread_contents": [" is a set of scripts that allow you to set up CI (continuous integration) with less effort. It is not limited to \u201cindustrial\u201d usage despite its name.", "Any questions/suggestions are welcomed at the ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["[capability] Add clang-format check (", ", ", ")", "[capability] Re-use previous local CI run ", "\n", "[capability] Allow reuse Docker image ", "\n", "[capability] Automatically set ROS_DISTRO from Docker image if not specified ", "\n", "[capability] ROS melodic support ", "\n", "[deprecation] USE_DEB, UBUNTU_OS_CODE_NAME", "[fix] Run Travis CI locally ", ", ", "\n", "[maintenance] remove rosdep hack ", "\n", "[improve] Better output text ", "\n", "[improve] Speed up CI by skipping Docker pull ", "\n", "Contributors: Felix Messmer, Isaac I.Y. Saito, Jonathan Hechtbauer, Mathias L\u00fcdtke, Miguel Prada, Wolfgang Merkt"], "url": "https://discourse.ros.org/t/industrial-ci-0-7-0-released/7314"},
{"title": "Aztarna, a footprinting tool for robots", "thread_contents": ["Hello ROS community,", "Our team at Alias Robotics is proud to present you our work on a footprinting tool for robotic networks. We have recently published a ", " with the results on our first tests with amazing results. We have also released the aztarna tool on ", ", so if you are interested you can check it out.", "I hope you find it as interesting as we do and if you wanna try it and give us your opinion or feedback you will be more than welcome.", "Thank you!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/aztarna-a-footprinting-tool-for-robots/7332"},
{"title": "3D Solid State Lidar Demo", "thread_contents": ["Jeff Sampson gives us a demo of the new 3D Solid State Lidar from Hypersen ", "The full demo is located here: ", "What 3D sensors are you using on your outdoor robot?", "ROS Agriculture community meetings are Tuesday nights from 6-7pm Pacific Time. Web meeting link: ", "Matt", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/3d-solid-state-lidar-demo/7371"},
{"title": "Release: Rviz Cinematographer - Create Professionally Looking Camera Trajectories Easily ", "thread_contents": ["Visualizing 3D data as a 2D projection - a common case in Rviz - usually comes with a tradeoff.", "\nStatic visualizations suffer at least from a partial loss of depth information for the audience.", "\nDynamic visualizations on the contrary are either tedious to generate or seem like the deadline was closer than expected.", "I\u2019ve build an rqt plugin to conveniently specify camera motions in Rviz in a matter of minutes.", "\nA short example trajectory - created with 5 clicks, including one to trigger the camera motion - is provided below.", "A more sophisticated example is presented on the ", ".", "Just clone it into your workspace, build it and use it along with Rviz.", "\nNo non-ROS dependencies.", "\nDetailed instructions and illustrations, if needed, are provided on the project page.", "I\u2019ve added an option to ", " - using the images rendered by rviz during a camera motion induced by the Rviz Cinematographer.", "\nYou can choose the ", ".", "\nThe motion is rendered frame by frame guaranteeing smooth motions in the video ", " - e.g. due to large point clouds or slow hardware.", "Furthermore the code was successfully ", ".", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/release-rviz-cinematographer-create-professionally-looking-camera-trajectories-easily/6703"},
{"title": "Robotics Beginner - Hexapod", "thread_contents": ["Hi All", "I am totally new to the world of robotics and like most of my other projects usually come to me on a whim, this time it was a TV show set in space that featured a homicidal Hexapod ", "So this is my current endeavor to build from scratch a Hexapod Robot, very basic at first with it being controlled via a simple controller, the homicidal part comes later ", " Looking at 18DoF.", "Whats the best low cost platform to get started with ROS as I get more proficient and my ideas expand to the realm of total autonomy then systems like the intel nuc and Dynamixel Servos may be called for but at the moment a simple control system that can run ROS and some cheap 15kg Torque Servos.", "From other DIY projects I have a 3D printer and a CNC machine that I can use to create my custom parts for the body.", "So looking for help and points in the right direction from other Hexapod Builders and other Robotics enthusiasts and professionals alike.", "Cheers", "Druid", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/robotics-beginner-hexapod/7496"},
{"title": "BICA, a building block for robotic applications", "thread_contents": ["Hi,", "We have just released BICA, a powerful framework based on ROS to develop robotic behaviors. It comes with a visual editor of finite state machines.", "We use it both in our industrial projects and in the software we use, together with ROSPlan, in the RoboCup SSPL with the Pepper robot.", "Contribute to IntelligentRoboticsLabs/BICA development by creating an account on GitHub.", "I hope someone finds it useful. Contributions are welcome.", "Francisco.", "Wow, I see a relatively large number of projects dealing with Behavior Design recently\u2026", "Just to name few:", "RAFCON uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. It ships with a graphical user interface supporting the creation of state machines and contains IDE like debugging mechanisms. Alternatively,...", "Behavior Trees Library in C++. Batteries included. - BehaviorTree/BehaviorTree.CPP", "Contains the behavior engine FlexBE. Contribute to team-vigir/flexbe_behavior_engine development by creating an account on GitHub.", "Dear Davide,", "I agree. After all, developing behaviors for robots is the Holy Grail of mobile robotics. The development of robotic behavior is still a very active field of research, although it is a problem in which many people have been working for more than 40 years: reactive, hybrid, three-level architectures, sumbsuption, \u2026 All of them try to solve the same problem: generate apparently intelligent behaviors.", "ROS provides a great environment to develop software for robots, but it does not implement the generation of behaviors for complex tasks. Until recently, Smach and ROSPlan were the only outstanding projects to address this problem.", "I think it is positive that many of the approaches that have been developed over these years are released to the community as ROS packages. It is a way for these software not to die with the project that saw them born. As they say in the YARP paper, sometimes projects are black holes of code, of which only papers survive.", "Best", "\nFrancisco", "I think it is positive that many of the approaches that have been developed over these years are released to the community as ROS packages.", "I 100% agree ", "As they say in the YARP paper, sometimes projects are black holes of code, of which only papers survive.", "Not so sure I agree on this one ", "Thank you for sharing your work, I am sure a lot of people will find it useful.", "I want to add my own work (a bit of self advertising it is not bad ", " )", "\n", "Besides, more than visual editor, we add live programming to develop robotic behaviors.", "\nI just hope to add a bit more of data about this topic", " very cool. You should ", " do some self advertising, what all these projects miss, IMO, is some momentum  ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/bica-a-building-block-for-robotic-applications/7327"},
{"title": "Hal_ros_control: simple, powerful ros_control hardware interface", "thread_contents": ["My team has written a simple but very flexible ros_control hardware", "\ninterface that runs Dave Coleman\u2019s ", " in", "\nMachinekit HAL + RTAPI.  It\u2019s really cool, and we believe it can be", "\nused to control a lot of different robot hardware with configuration", "\nonly, and little or no coding.  We use it in our 6 DOF robot arms with", "\nsimulated and EtherCAT drivers.  While it\u2019s still brand-new, it plays", "\nout joint trajectories beautifully.", "A real-time ros_control implementation with Machinekit HAL and boilerplate - zultron/hal_ros_control", "Machinekit HAL provides an RT threading environment where \u2018components\u2019", "\nare loaded and \u2018pins\u2019 are connected through \u2018signals\u2019 in a simple", "\nconfiguration file.  Out of the box it includes hundreds of", "\npre-defined virtual components such as joint controllers, PID", "\ncontrollers, encoders, scale & offset, filters, pos/vel/acc limiters", "\nand logic gates.  It talks to many types of real hardware, like", "\nEtherCAT, BeagleBone GPIO (incl. PRU support), and Zynq SOCs.", "\nMachinekit HAL is an offshoot of LinuxCNC HAL, well-proven to adapt an", "\nenormous variety complex physical hardware configurations for use with", "\nits CNC controller application; Machinekit improves and extends HAL", "\nfor general automation applications, including robots.", "The ", " enables adaptation to many", "\napplications by setting up ros_control and its joint limits and", "\ntransmissions interfaces purely through configuration from the ROS", "\nparameter server.", "By complementing the boilerplate with Machinekit HAL as the hardware", "\ninterface, we hope to eliminate most coding tasks in robot hardware", "\nintegration and let you spend your time on more important things.", "Sounds great! Thanks for sharing!", "Great work! You can find my showcase application moving an industrial robot with AC servos here: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/hal-ros-control-simple-powerful-ros-control-hardware-interface/7418"},
{"title": "BennyBot build... an autonomous bot to deliver my garbage to the curb", "thread_contents": ["Hi all,", "I\u2019m new to ROS and using a simple domestic \u201cproblem\u201d as a platform to build a robot from the ground up while I learn ROS and robotics in general.", "I have a formal background in electronics, plus 15+ years building business applications, and I also have a lot of mechanical and fabrication background\u2026 so, I thought robotics would be a fun way to put all these skills together\u2026 and presumably I will find some opportunities somewhere along the way to make this somewhat profitable.", "BennyBot\u2026 which is just a play on my last name (Benson)\u2026 is (going to be) an autonomous platform that will find my garbage bin, load it up, transport it to the end of my driveway and drop it off at the curb on collection day, then go retrieve the empty bin after it\u2019s been emptied.  Every second week, it will do the same with the compost bin.", "I\u2019m still very early stages, but I have some of the mechanical, electronic, and ROS nodes operational\u2026 the mechanical and electronics are relatively easy for me\u2026 ROS is my learning curve, but it\u2019s all making sense so far.", "I am using one (or more) raspberryPi 3 SBCs, one Arduino, two Dimension Engineering 2x32 Sabertooth motor controllers with Kangaroo X2 PID controllers, 4 AndyMark PG71 gear motors, a single RPLidar A2 scanner, some sort of IMU (currently experimenting with an LSM9DS1 from SparkFun), and probably a 3d camera, and maybe a GPS for fun.", "My platform is a bit unique in that the rear wheels are not fixed to the frame, they pivot on a swingarm.  The idea is that this will allow the platform to \u201ckneel\u201d at one end in order to pull the garbage/compost bin on board, then raise up again once the bin is properly positioned.  What makes this unique is that the wheels themselves are used to raise and lower the platform\u2026 there is a locking mechanism on the swingarm\u2026 when locked in the upgright position, the platform can travel around to transport the bin.  When unlocked, if I drive the front and rear wheels in opposite directions, the frame will kneel down at one end as the swingarms swing upward.  Driving the front and rear wheels towards each other will lift the platform, followed by locking the swingarms in place in preparation for travel.  I know this is hard to envision\u2026 so here are some videos that might help:", "This is the swingarm with a servo motor that locks/unlocks it to the robot frame.  The servo is controlled by the Arduino using the RosSerial Arduino bridge\u2026 I can publish messages to lock and unlock the swingarm from a terminal window , this was my first real robot movement controlled by ROS\u2026 I\u2019m calling that Benny\u2019s birthday ", "\n", "This is a side view of the frame, with both wheels being controlled by ROS using the kangaroo_x2_driver by smd-ros-devel.", "I am interested in building a robot that can carry a larger weight. But I have no knowledges in electronics so I am confused on how Raspberry Pi will control the motor because, in comparison with Turtlebot, this robot will have a more powerfull motor. So which is the \u201cOpenCV\u201d you used? Or, how did you replace it?", "\nThank you", "RaspberryPi, actually a couple of RaspberryPis\u2019 are running ROS, but the actual \u201cheavy lifting\u201d of powering the motors is being handled by a pair of motor controllers from Dimension Engineering.  I am using a pair of their 2x32 amp Sabretooth motor controllers, along with their KangarooX2 PID modules which allow for closed loop operation by monitoring the encoders on the motors.", "With this combination, I am able to issue speed commands from ROS to the KangarooX2 module which takes care of maintaining a constant speed for me (it looks at the load on the motors and will vary the power to each motor to maintain the commanded speed).", "I do not have the odometry feedback to ROS figured out yet, but I think others may have already accomplished this.", "This is still a work in progress\u2026 I don\u2019t have all the answers yet, but happy to pass on what I have learned so far.", "Duane", "I am struggling to make a general bill of material for the hardware required. But I don\u2019t progress at all. So I would like to ask for some advice.", "\nMy plan is to have a robot that can carry a larger weight (>1,5TO). For this project I assume that I need two motors to about 0.8kw/each, 380Volt, DC. And now comes the questions:", "\n-what kind of motor do you use?", "\n-how many sabretooth do I need (not clear which type)? In fact why do you use a pair of sabertooth? Or can be replace with something else?", "\n-how many kangaroox2 do I need? This question is link with the previous one. Because this large equipment need many Ampers I need big batteries (I was thinking at some car batteries to 12 or 24 Volt). But than I need a converter, do I? In fact two: one from battery to kangaroo and one from sabertooth to motor, no?", "\n-how many raspberrypi? Why do you have 2 raspberry? Probably to have a better computation, but how do you connect them? One is master and one is slave?", "Other questions (cables, encoders, etc) remain for later after clarifying those", "Thank you", "\nAdrian", "Hi Adrian,", "It sounds like you might be in a bit over your head with this project, but everybody has to start somewhere\u2026 you may have a number of steep learning curves ahead of you.", "The sabertooth controllers drive 2 channels (i.e. 2 motors) at 32 amps, up to 30volts per motor.  I have 4 independent drive motors, which requires 2 sabertooth controllers.  The Kangaroox2 is an add-on to the sabertooth which provides the closed loop PID control and there is one required for each sabertooth controller.  Theoretically, the two sabertooth controllers could deliver a maximum of 128 amps to all 4 motors (32A x 4), this assumes that the motors are big enough to require that much current draw, it will be the motors themselves and their rated current at stall which determine the current draw from the sabertooth.  I am using 4 x PG71 gearmotors, which have a stall current of 22 amps each, I haven\u2019t pushed them to stall currents yet, but the sabertooth is rated to safely deliver up to the 22 amp max of these motors.  The sabertooth is not very well supported in ROS\u2026 I\u2019m not sure it\u2019s the best choice, but it\u2019s certainly not impossible to make it work.", "I am using a single 12V car battery to power my robot.  You mention 380 volts which is certainly a possibility, but you need to be careful with this type of configuration\u2026 380 VDC is very dangerous!  I use a single 12 V bus to power the 12 V equipment, and I have a downconverter that converts the 12VDC to 5VDC for my electronics\u2026 these are easily found on any robotic suppliers website.", "I have 2 raspberry pi for now until I can determine how much processing power will be required\u2026 I am a beginner with ROS, so I won\u2019t be surprised if this is not enough to perform full SLAM navigation.  Once you learn ROS, it\u2019s easy to understand how the processing is distributed across multiple devices\u2026 it\u2019s the whole publish/subscribe scheme that makes this possible.  The multiple processors will need to be networked\u2026 EtherCat seems to be the standard here, but it\u2019s an area I haven\u2019t tackled yet.  I do have an Arduino that connects to the Pi by USB\u2026 the Arduino is controlling my stepper motors by ROS events published by the Raspberry Pi.", "BOM is very much up to you\u2026 my high level components are:", "2 - Sabertooth 2x32 motor controllers", "\n2 - Kangaroo X2 PID controllers", "\n4 - PG71 gearmotors", "\n2 - Raspberry Pi 3", "\n1 - Arduino Uno", "\n2 - HS485 (I think) servo motors", "\n1 - 12VDC to 5VDC converter", "\n1 - Main contactor to isolate the battery", "\n1 - Main circuit breaker for protection", "\n1 - Fuse block to protect individual circuits", "\nVarious other components such as LiDAR, switches, cameras, etc.", "Hey, Duane. How do you stay with your project? I would like to talk to you if you agree, but in a private way. I did something with my project and it would be useful to share.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/bennybot-build-an-autonomous-bot-to-deliver-my-garbage-to-the-curb/3896"},
{"title": "Rvis2AR Project", "thread_contents": ["Hej guys,", "\nWe are working on a project for visualizing ROS-Data, supporting current AR/ VR devices like the Hololens and Rift.", "The Project is built on the ROS Sharp framework (", ") and we plan to include DDS at some point when we got it running.", "Those are our set goals for the project:", "\u2022 ", "\nThe software module should provide a stable communication link between the Unity", "\napplication running on AR/VR gear (Hololens) and ROS, enabling to send and receive live", "\ndata.", "\u2022 ", "\nThe module should already support current off-the shelf AR/VR gear. We start to integrate", "\nthe Microsoft Hololens, but plan also to use HTC Vive and Oculus Rift.", "\u2022 ", "\nWe want to demonstrate the capabilities of AR/VR interfaces for a ROSin running robot", "\nby documenting a showcase an providing tutorials for setup and use.", "\u2022 ", "\nThe software should include a toolkit for building AR/VR interfaces to the robot (roughly", "\nin the direction of rviz for AR/VR)", "\u2022 ", "\nThe software should be able to enabling multiple users to see the same virtual content,", "\nso they can use this information for discussions.", "The project still in development, but feel free to check it out:", "Best regards ", "Supported by ", "- ROS-Industrial Quality-Assured Robot Software Components. See also:", "\n", "enable a visualization platform for AR / VR devices, especially for the Microsoft Hololens", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rvis2ar-project/7687"},
{"title": "[TUTORIAL] Raspberry Pi and ROS - Installing RPLidar", "thread_contents": ["Hello ROS community!  I\u2019m working on developing a series of \u201cHowTo\u2019s\u201d around ROS for the beginner.   My company makes ", ", and we have wanted to adapt ROS for some time.", "My first HowTo is ", ". I go step by step through the installation instructions, with all the commands necessary for getting the LIDAR sensor installed on the Raspberry Pi.  The directions on Github had a few small errors or incomplete steps, and I think the  blog post helps fill in the blanks.", "I am working on a whole series as I setup a dedicated image to adapt our robots to the ROS OS.  Our ultimate goal is to make it easy to use ROS on our Raspberry Pi robots (which are only $100), so that we can open up the ROS world to a wider audience.", "My future plans are to show installation of the ydlidar, installing ROS on the Pi, and then do some example projects.  Any suggestions on projects we could do or tutorials we could walk through that would help open up the Pi and the GoPiGo3 to the ROS community, please let me know!", "Your project is most welcome. I am new to Robotics, but am clear on what I want my robot to do as a farmer.", "I write not on on behalf of ROS agriculture for I have no such authority to do so. However, may I suggest that you consider current efforts by the ROS Agriculture Community to convert farm vehicles into autonomous farm vehicles ranging from tractors pulling grain bins, planters, trailers, ploughing disks to combine harvesters. Ideally, a scalable Raspberry Pi Robot is most welcome. Bear in mind that it is crucial for the farmer to make the robot as he has to deal with converting the tractor to autonomous vehicle. Here lies a value proposition for a project that provides the processes of how to make such robot in addition to how ROS works. Would it be considered unattractive to sell robot processes and provide related ROS tutorials as opposed to selling robots? May be not. The lifespan of robot sales not linked to a particular industry may turnout to be short.", "In my view, there are three things of great importance to the farmer. Up-skilling his knowledge of ROS as an operating system on the one hand, development of software that runs the farm vehicle, and acquiring the critical knowledge of how to modify the robot to understand where it is, ability for robot to avoid obstacles, including the electronic/mechanical aspects of the vehicle(robot).", "In conclusion, you may want to review the \u2018value proposition\u2019 of your project at this early stage if you are planning in upgrading it to a sustainable business venture.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/tutorial-raspberry-pi-and-ros-installing-rplidar/7720"},
{"title": "ROS package for Anki Vector now available", "thread_contents": ["Hi everyone,", "Lately I\u2019ve been working on this package for Vector, It\u2019s my first public ROS package, and I\u2019ll be really happy to get some professional feedback ", "Still got some work to do but it\u2019s pretty stable and available on my Github repo:", "Unofficial ROS package for Anki Vector home robot. Contribute to betab0t/vector_ros development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-package-for-anki-vector-now-available/7923"},
{"title": "ROS Beginner Tutorials", "thread_contents": ["Hello everyone\u2026", "\nMy name is Vibhutha Kumarage from Sri Lanka. I\u2019m gonna create youtube channel for ROS tutorials. As the first step, I created a simple obstacle avoidance robot simulation with Gazebo + ROS and uploaded the source files. My concept of creating videos is re-tutor text tutorials in ROS documentation and other sources as video contents. My target audience is the ROS beginners who wants a jump start in the ROS learning process. I am expecting feedback for this project and guidance to make an impact to ROS world.", "\nThank you.", "\nFirst Video link :  ", "Hey\u2026 nice to see another Sri Lankan here\u2026 I\u2019m Imesh and recently started learning ROS\u2026 as a beginner I must say it is bit hard to get the first few steps into the ROS. Please keep up the good work\u2026", "\nCan I ask where ar u from in Sri Lanka ??", "Wow, I\u2019m also a Sri Lankan and I hope to study ROS, I just started as a beginner and I hope u guys are able to help me.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-beginner-tutorials/3032"},
{"title": "ROSject of the week", "thread_contents": ["This week, we would like to share with you a ROS project created by ", ": ", "This ROSject shows how to use a wheeled robot (a ROSbot) to search for specific objects in a given area. In their own words:", " defines a mission in which robot has to ", ". For this purpose it is necessary to use two different approaches, one for exploration and second for object recognition. [\u2026] Besides launching them together, it is ", " by the object recognition process. Task is considered as finished when object is succesfully recognized or all obstacles were checked with no object detection.", "You can get the whole code, simulation and notebook with instructions (everything running off-the-shelf), by ", ".", "Let us know if you are missing any feature in the project or if you would like to contribute to make it better.", "\n", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosject-of-the-week/7953"},
{"title": "Introduction", "thread_contents": ["Hello, I am Abhishek Mishra from bhubaneswar, India. I have previously worked in ROS kinetic and made project on path planning of drone using OMPL plugin and simulated the same in V-REP. I am interested in add path planning libraries in addition to the existing ones.", "Hello Abhishek, I have used MoveIt to control our robotic arm. I will be interested to know how you are planning to use your algorithm and how it will be better than existing ones.", "\nPlease feel free to call me at 8095971082", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/introduction/8138"},
{"title": "Developing with SROS", "thread_contents": ["I\u2019ve been receiving a number of private messages about ", ", and so rather then all this valuable discussion accumulating dust in my inbox, I though I\u2019d start a thread here for the rest of the community\u2019s benefit.", "I should think for now, Q&A about the design of SROS would also be appropriate here, as SROS is bound to change, and so archiving such topics on ", " may no yet be the best place for early developments.", "So please give us shoutout or followups about SROS here so we can all stay on the same page. I\u2019ll try and be as active as gradschool will afford, but posting here may enable someone else to respond when my inbox backs up.", "Hello ruffsl,", "The following is my current understanding on SROS workflow:", "I hope you could answer a few questions on SROS for me.", "Do you assume a one-to-one synchronous TLS communication between the nodes using topics for communication (similar to ROS services)?", "If I understand correctly the keyserver only generates the keys, certificates for the nodes (based on the user-defined configuration, policies etc.) but it does not sign the node\u2019s public key certificate i.e.  keyserver does not act as CA for nodes.  In the current SROS implementation, does the rosmaster register the node and sign its certificate acting as a CA when a node connects to the rosmaster for the first time?", "\nIf yes, does the rosmaster authenticate or check the node\u2019s authorization rights before it registers the node and verifies its certificate?", "In your ROScon slides (", ") , you mention in your ToDo slide (Slide ", ")   \u201cHarden Master and Slave API calls where caller\u2019s privilege must be checked before response\u201d. Have you already dealt with modifying ROS API functions in SROS to include an authentication token?", "I am thinking of an SROS extension which includes modifying the Master API call (register call) from a node to the master, to include a node-authentication-token. This would mean that the node registration would be successful at the master only if this token is verified correctly. In addition to your answers to the above questions, it would be great to know your thoughts on this idea.", "Thank you.", "I am thinking of an SROS extension which includes modifying the Master API call (register call) from a node to the master, to include a node-authentication-token. This would mean that the node registration would be successful at the master only if this token is verified correctly. In addition to your answers to the above questions, it would be great to know your thoughts on this idea.", "I doubt that approach will work in ROS 2, because there is no direct equivalent to ", ".", "Maybe some of the ROS 2 developers will suggest ways to automate secure node creation in a more portable fashion.", "Hi!", "\nWe\u2019ve uploaded your Humanoids 2016 paper on SROS from the Towards Humanoid Robots OS workshop. The link is right on the SROS line ", " ", "\nHope it helps give some technical insight regarding doubts that have arisen!!", "Hi ", ",", "\nGreat questions! I\u2019ll try to answer each in turn, but first let me clarify some possible misnomers.", "If we are starting from scratch, then:", "Upon starting the SROS keyserver, it would supposedly have note initialized keystore, and thus would generate some certificate authorities (CA) as specified by the default config. You could also give it a CA to start with or tell it to make an intermediate CA with it as well. By default currently it\u2019ll generate a root CA, and then use that to make an intermediate mater CA. The default config tells the keyserver to reserve the master CA for ROS nodes, but again this is all stuff you could reconfigure or customize.", "\n", ", this master certificate for CAs are not the same ones that are used by say the ROS master node from transport. Giving a certificate CA and transport use-permissions would be dangerous, so we always separate the roles of CA and trasport certs. Thats why in the ", ", the tree command listed two master certs, on in the keystore\u2019s CApath, and another in the master node\u2019s individual nodestore.", "If you then ran sroscore, then the rosmaster, roslaunch, and roslog nodes would start and connect to the keyserver before they could talk to each other. Node request certificate and key pairs from from the keyserver, ", " they do not do this by sending a whole certificate signing request (CSR). With the intent of making the modifications to the ROS client library as light as possible to support SROS, I chose to not make the client responsible for generating the CSR, as this would bring a host of other dependencies for PKI generating into client library itself.", "\nInstead, I deferred that responsibility to the keyserver, as only it is fully aware of the policy profile context necessary for embedding the access control extensions into the X.509 certificate. If nodes did submit CSRs, then we\u2019d have to also enable the keyserver the ability to scrutinize those CSRs for permission escalations, just like your web certificate authority provider would do if you were requesting a SSL cert for a https domain. Rather than doing all of that, the node simply provides its ROSgraph own domain name, and the keyserver uses this to lookup the applicable certificate recipe.", "\nNow you may ask yourself, how does this solve the chicken or the egg problem of establishing trust if any old node can ask the keyserver using any old namespace? Well it doesn\u2019t, but we provide some methods to do this. First is that the user can specify secretes (tokens you might say) to the keyserver (via shell environment) to use to cipher private keys for a given node namespace, then requesting nodes should also know that secret by the user (via handcuffed briefcase or ssh etc.) at runtime so the node can decrypt its own received private key.", "\nAnother way is to enable the keyservers client-verification mode it uses for TLS connections from ROS nodes. So this would require nodes to use an intermediate certificate (signed from a CA that the keyserver would trust) to connect to the keyserver at all. However the really shouldn\u2019t be running your keyserver on an exposed network to begin with.", "Once the nodes have the keys and certs, then upon their p2p connection they verify each other\u2019s certificates. ", ", the signature does not necessarily need to be from a \u201cmaster\u201d CA, it just needs to be from a CA that is already trusted by being in the CApath for the keystore of the verifying node. By checking that the cert signatures are from trusted CAs within their own CA path, this would allow for a multiple of other schemes such as signing certain subgraphs of the ROS network with different levels of CAs, or giving only certain CA public certs to a limited set CApath for special node keystore.", "I\u2019ll rework the SROS wiki entry about the keyserver to make this more clear:", "\n", "Ok, for your questions then.", "Do you assume a one-to-one synchronous TLS communication between the nodes using topics for communication (similar to ROS services)?", "Well because TLS is sort of a stateful connection, there is a handshaking process, this requires the use of TCP. So essentially all topic, service and parameter level API call in SROS currently use TLS/TCP for the network layer. I am not fully aware of the best methods to do this for UDP based connections, but perhaps someone else might want to interject here about that.", "If I understand correctly the keyserver only generates the keys, certificates for the nodes (based on the user-defined configuration, policies etc.) but it does not sign the node\u2019s public key certificate i.e.  keyserver does not act as CA for nodes.  In the current SROS implementation, does the rosmaster register the node and sign its certificate acting as a CA when a node connects to the rosmaster for the first time?", "\nIf yes, does the rosmaster authenticate or check the node\u2019s authorization rights before it registers the node and verifies its certificate?", "Currently the keyserver does in fact fulfill that role of signing, as it uses the CAs you give it to sign the public certificates it generates. When nodes connect to the master node, they will already have had their certificates properly signed, otherwise the master node would just reject the TLS connection at the handshake stage. However, the master does indeed check that the node is authorised to register or deregister a topic (among other API actions) by scrutinizing the access control extensions in the certificate of the TLS connection.", "In your ROScon slides (", ") , you mention in your ToDo slide (Slide ", ")   \u201cHarden Master and Slave API calls where caller\u2019s privilege must be checked before response\u201d. Have you already dealt with modifying ROS API functions in SROS to include an authentication token?", "This follows from what I mentioned for the previous question, in that nodes, including the master node, have validators that are used to check if the other nodes is authorised to serve or call a given API function. You can see some of this in the source code here:", "\n", "\n", "\nSo I\u2019ve added basic checks to most of the API, but there is the bigger todo/question as to how we should safe gard the rest of the less abvious API calls. Personally I think defining a OID for the remaining API functions and checking for it presence in a certificate might be a good way for designating permission for their execution or calling. However I\u2019d like to avoid getting too verbose and overloading the certificate payload with so many rules. So perhaps we could partition the API call into categories and alot OIDs to those groupings (like some call only a master can serve, and some only a slave can call).", "The other question is whether we should filter or censor certain API function returns. For instance, some API calls are used by node to ascertain the existence of a parameter. Here is one such example where the master considers the requesting node\u2019s permissions and redacts any permeameter namespaces the node was not authorised to read from:", "\n", "\n", "I am thinking of an SROS extension which includes modifying the Master API call (register call) from a node to the master, to include a node-authentication-token. This would mean that the node registration would be successful at the master only if this token is verified correctly. In addition to your answers to the above questions, it would be great to know your thoughts on this idea.", "Hmm\u2026 well what has been implemented is a bit different from what you mentioned. The way I\u2019ve approached this so far is to try and have minimal impact to the existing ROS API (other that to enforce access control policies). I get away with this by leveraging the context of the IP socket connection to ascertain the certificate credentials there. By piggybacking on the transport security layer, I don\u2019t need to exchange additional meta information that would necessitate extending the existing ROS API, and I also get the added bonus in the assurance that the PKI elements (the X.509 certificate) is valid (in trusted signature, expiration time, revocation etc.), so I don\u2019t need to verify the policy information again.", "I doubt that approach will work in ROS 2, because there is no direct equivalent to roscore.", "Maybe some of the ROS 2 developers will suggest ways to automate secure node creation in a more portable fashion.", "I think it would be best to keep the policy definitions somewhat autonomous, in the fact that every node/identity is branded with the policies it was given at birth like branded cattle, so that anyone who later encounters the node, wondering out in the wild, and can interpret the policy/branding will know how to handle it, avoiding the need for some centralized policy lookup authority to recognize what the connecting peer can do. I think keeping in the realm of PKI would also be good for transitioning to ", " (that also use PKI) for ROS2.", "Thanks ", " , I\u2019ll make an update note of this on the SROS announcement thread I made earlier.", "Thank you very much ", " for the clarifications and detailed answers.", "\nI now have a much better understanding on the initial setup for the nodes in SROS. ", "Do you assume a one-to-one synchronous TLS communication between the nodes using topics for communication (similar to ROS services)?", "Well because TLS is sort of a stateful connection, there is a handshaking process, this requires the use of TCP. So essentially all topic, service and parameter level API call in SROS currently use TLS/TCP for the network layer. I am not fully aware of the best methods to do this for UDP based connections, but perhaps someone else might want to interject here about that.", "For securing UDP connections, something like ", " could be used.", ": you might be interested in the ", " presentation by Bernhard Dieber (Joanneum Research) from the last ROS-Industrial Conference (similar security considerations, but also addressing the ", " side).", "I\u2019ve been debating with some folks about the best method circulating policy restrictions in SROS for enforcing access control. The two of them so far are:", "I\u2019d like to invite the rest of the community to put forth their own opinion, and so I have started a short wiki entry expanding upon the approaches. Please feel free to reply with your remarks here and/or concisely clarify the comparison on the wiki as you see them:", "To be honest I\u2019ll admit my bias for Certificate Embedding.", "\nNot only is this what I\u2019ve developed in SROS so far, but I also see it as:", "But don\u2019t be afraid to play devil\u2019s advocate.", "I was watching some talks on the developing TLS 1.3 protocol [1], just checking on what\u2019s been happening in recent drafts, and noticed that the authentication step in the connection handshake is now encrypted. Here is another video slightly more introductory [2].", "This is really cool as it can bring privacy to certificate extensions, preventing say a client\u2019s access policy being revealed to a passive attacker. I\u2019m not yet sure if I understand to what this extends to the server\u2019s certificate, or for active attackers. For that I may have to follow the mailing list discussions [3] more closely or check out a current implementation.", "Perhaps with TLS 1.3, this might void some of my remarks on the potential drawbacks I discussed earlier about SROS\u2019s use of pigging backing on the transport layer encryption. Also, the reduced number of round trips would also help improve the connection time between SROS nodes.", "1 ", " | ", ":02", "2 ", " | 4th slide or ", ":00", "3 ", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["SROS keyserver first creates keypairs and certificates for the root and master nodes.", "When publisher/subscriber nodes are launched for the first time, the keyserver creates keypairs and certificates for them. Subsequently, nodes send their certificates to the master and master signs/verifies the certificates and registers the nodes.", "Finally, these nodes can use these master-verified certificates to create a TLS channel between them and have an encrypted communication.", "\n", "\n", "\n", "\n", "\n", "\n", "SROS keyserver first creates keypairs and certificates for the root and master nodes.", "When publisher/subscriber nodes are launched for the first time, the keyserver creates keypairs and certificates for them. Subsequently, nodes send their certificates to the master and master signs/verifies the certificates and registers the nodes.", "Finally, these nodes can use these master-verified certificates to create a TLS channel between them and have an encrypted communication.", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "def unregisterSubscriber(self, context, f):", "    def check_permitted(instance, caller_id, topic, topic_type):", "        policy, allowed = self._engine.check_profile(context, 'topics', 'subscriber', topic, caller_id)", "        if allowed:", "            return f(instance, caller_id, topic, topic_type)", "        else:", "            raise PolicyInvalid(\"ERROR: policy invalid for given action\")", "    return check_permitted", "\n", "def registerPublisher(self, context, f):", "    def check_permitted(instance, caller_id, topic, topic_type, caller_api):", "        policy, allowed = self._engine.check_profile(context, 'topics', 'publisher', topic, caller_id)", "        if allowed:", "            return f(instance, caller_id, topic, topic_type, caller_api)", "        else:", "            raise PolicyInvalid(\"ERROR: policy invalid for given action\")", "    return check_permitted", "\n", "def unregisterPublisher(self, context, f):", "    def check_permitted(instance, caller_id, topic, topic_type):", "\n", "def hasParam(self, context, f):", "    def check_permitted(instance, caller_id, key):", "        policy, allowed = self._engine.check_profile(context, 'parameters', 'reader', key, caller_id)", "        if allowed:", "            return f(instance, caller_id, key)", "        else:", "            raise PolicyInvalid(\"ERROR: policy invalid for given action\")", "    return check_permitted", "\n", "def getParamNames(self, context, f):", "    def check_permitted(instance, caller_id):", "        code, msg, val = f(instance, caller_id)", "        val = self._engine.filter_policy(context, 'parameters', 'reader', val, caller_id)", "        return code, msg, val", "    return check_permitted", "\n", "\n", "##################################################################################", "# SERVICE PROVIDER", "\n", "Decentralized", "Certificate Embedding", "Centralized", "Online Arbiter", "More Secure", "Harder to circumvent or exploit", "Less Invasive", "Modification can be kept out of client library", "Autonomous", "Access control is self contained and validated in TLS"], "url": "https://discourse.ros.org/t/developing-with-sros/861"},
{"title": "gym-gazebo migration to ROS2 with MARA", "thread_contents": ["Dear ROS Community,", "I am going to start the task of migrating gym-gazebo to ROS2, give it a new structure and test it with the ", ".", "In case you don\u2019t know about gym-gazebo, it is basically a toolkit for developing and comparing reinforcement learning algorithms using ROS and Gazebo. A few robots have been successfully tested in this environment, but the community has focused particularly in the Turtlebot. A couple example videos of this robot:  ", ", ", ". I invite you to also take a look at the ", ".", "Everything here is ROS1 based and the general structure is not very convenient. The goal is to migrate to ROS2 and use the under development ros2 version of the ", ". Once this is completed, installation and usage tutorials will be created for the community.", "Please feel free to share your thoughts, specially if you are familiar with the ros2 gazebo_ros_pkgs branch. Take a look at the ", " for a more technical debate and share your interest if you want to get involved in the project!", "I\u2019m glad to hear you\u2019re doing this. Have you thought about using openai/gym as a library instead of forking it? Forking it seems to make it near impossible to get updates as openai moves forward. I realize that was the \u201crecommended way\u201d openai had suggested to use their repo, but it seems problematic.", "Sure, I don\u2019t even know when was the last time updates form OpenAI Gym where merged. It makes more sense to update the structure to a more convenient one, while still using the gym library for some features like monitoring the environment. The core idea behind the gazeboEnv superclass and specific Envs for each scenario would be kept the same as is it now, following the OpenAI gym.", "Have you thought about using openai/gym as a library instead of forking it? Forking it seems to make it near impossible to get updates as openai moves forward.", "Interesting, did you struggle due to the fork yourself ", "?", "I think that using it as a library is a pretty valid suggestion. ", " and ", ", you guys have been maintaining lots of algorithms and gym-gazebo together with them, what\u2019s your view on this?", "That is a very valid suggestion. We will look into how to make gym_gazebo more of a stand alone package and use necessary libraries such as openai/gym", " - I wouldn\u2019t say I struggled, but I did notice that there was a big delta between the forked version you had and the original. There were bugs I ran into in the fork that had already been fixed in the upstream, and due to the big delta, it wasn\u2019t easy to port them. If you maintain a fork, then you probably at least need to rebase it to the main repo on a regular basis to get the latest bug fixes. That in itself is time consuming, which is why I suggested using it as a library instead of forking it.", "I hope that answer helps explain my reasoning.", "Thanks for input ", ", that makes sense. We\u2019ll try our best.", "Just to let everyone know we have released our gym-gazebo2 and ros2learn toolkits, check the announcement at: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gym-gazebo-migration-to-ros2-with-mara/7119"},
{"title": "ROSject of the week: TALOS human size humanoid robot controlled with ROS", "thread_contents": ["Find here a ", " containing the full simulation and controllers of the ", " from ", ". Click on the link to get a working copy of the code.", "By clicking on the link you will get a fully working copy of the code which you can test with any type of computer. ", " .", "\nThe whole robot is controlled through a ", " , but you don\u2019t need to install anything in your computer to make it work. The rosject includes the walking controllers among other things (complete documentation included).", "You can use the rosject for your own experiments with walking humanoids.", "Go ahead and let us know what you think. Also, indicate if you would like us to include any additional functionality.", "Here a video example: ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosject-of-the-week-talos-human-size-humanoid-robot-controlled-with-ros/8450"},
{"title": "New INA219 I2C driver for ROS", "thread_contents": ["I wrote a driver for the Texas Instruments INA219 current and voltage monitor (used in, for example: ", "). I\u2019m often surprised that stuff like this doesn\u2019t exist already. I found a couple of Python implementations but for something as simple and \u201clowly\u201d as reading bytes of I2C and spitting them out on a ROS topic, Python consumes a bit much in terms of resources. I have especially users of e.g. Raspberry Pis and other small embedded hardware in mind for this. Same for the other I2C nodes I posted recently.", "ROS node for Texas Instruments INA219 current/power monitor - dheera/ros-power-ina219", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-ina219-i2c-driver-for-ros/8453"},
{"title": "Animating robots with Blender", "thread_contents": ["I\u2019ve played around lately with using Blender to animate robot movements. I wrote a short blog post about this topic, including some videos of the current state: ", "The execution of the movement is done with ", ". In general, I think there is potential in using Blender and things such as animation nodes as an alternative way to program robots.", "I\u2019ve also thought about integrating it with MoveIt to feature motion planning.", "Simulation and debugging of recorded moves might be another application. The new render engine EEVEE looks very promising for this purpose. Too bad the Blender community plans to remove the game engine, which is also used by MORSE, it might have been a good alternative for Ogre in RViz.", "Wow this is awesome! Thank you for posting this it is exactly what I am looking for! I\u2019m actually building an Art robot for a permanent installation, and I already use Blender, but I am new to ROS.", "I think another interesting flavor of this that I would like to explore is using IMU\u2019s to basically do non-optical motion capture.", "Wow that sounds interesting. Is it also a robot arm or something else?", "I think there is big potential for using to animate animatronics or to create natural movements for humanoid robots, to be used in more complex applications.", "It\u2019s more like an animatronic. So It won\u2019t be picking up anything, but waving its arms around and gesticulating like C3PO (hopefully). ", "I also found a couple other projects that also use blender, but not ROS:", "This controller uses Blender and Arduino to control a robotic arm in real time by calculating its Inverse Kinematics angles from the model. While this is focused on controlling a scara by moving the tip other uses like changing the pose of a humanoid...", "and", "I got the hackaday project working with a couple hobby servos, but I I\u2019m not good enough with python to be able to figure out how to get Blender to work with ROS.", "also here is another project using IMUs to do motion capture in realtime in Blender:", "\n", "Active motion capture suit which reads data from IMUs in real time and sends them to Blender over Bluetooth. Low cost and open-source - alvaroferran/MotioSuit", "\n", "the animation can be recorded, and then perhaps that can get translated to ROS?", "Alexander can you assist me with getting this to work? I\u2019d really like experiment with it. Do you have a code repository you can share?", "Thanks", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/animating-robots-with-blender/7636"},
{"title": "New Project \"RDBOX(A Robotics Developers BOX)\": Auto-build secure, scalable Wi-Fi network and Kubernetes cluster for ROS Robots", "thread_contents": ["We are released the new ", " ", " : Auto-build secure, scalable Wi-Fi network and Kubernetes cluster for ROS Robots. Enhance your ROS robot\u2019s roslaunch by leveraging K8s.", "For more information, visit our homepage or the GitHub repository.", "\n", "\n", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Automatic build script for cloud computing resources.\n", "We also offer an on-premise version that runs on VirtualBox. For more information, visit our ", ".", "\n", "An original Raspberry Pi SD image that provides network and Kubernetes functions at the edge (the field where the robot acts).\n", "Debian flavor Linux distribution based on ", "\n", "\n", "Providing know-how for utilization in the form of a tutorial."], "url": "https://discourse.ros.org/t/new-project-rdbox-a-robotics-developers-box-auto-build-secure-scalable-wi-fi-network-and-kubernetes-cluster-for-ros-robots/8426"},
{"title": "ROS Based Fully Autonomous Airport Snow Plow", "thread_contents": ["I am super excited to share with the ROS community our ROS based fully autonomous airport snow plow.  This is the first autonomous snow plow operating at an Airport in North America.", "It\u2019s been named \u201cOtto\u201d (not to be confused with ", ") by the technical staff at the airport and I think its a great name for an autonomous vehicle.", "\n", "We are long time ROS developers and I thank the community as a whole for making ROS the platform of choice for companies like ours.  Using ROS has enabled us to concentrate on what matters most to our business and has saved us countless years of development time.", "If anyone is interested in seeing a video of the plow operating can check out this link.", "\n", " Full article: ", "It would be great to get feedback from other groups using ROS in heavy vehicles like the ones we are automating.", "Shawn Schaerer", "\n", "Hi Shawn!", "Thanks for sharing!", "Love the project and will share it forward at Weekly Robotics (", ")!", "Couple of questions:", "I\u2019ll be looking forward to follow this project!", "Thanks for the support!", "Hey Shawn, fantastic stuff mate.", "Looks like you made some great progress specifically with using ROS with outdoors heavy machinery. I understand some of those challenge quite well. I work for a company called Universal Field Robots (", ") , developing autonomous field robots based on existing outdoor machinery, and similarly we are using ROS to power these beasts. We have currently focused on excavators in particular.", "Keep up the good work, look forward to see where this project goes!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Do you use a standard GPS or an RTK?", "Are there any particular packages that you think saved you a considerable amount of time?", "Do you need any certification to operate at the airports?", "We do use RTK GPS.", "In terms of packages, I don\u2019t think there was one in particular, but all of the drivers and ROS infrastructure were the big help.  RQT and RVIZ are two tools we use the most.", "Yes, we have a certification process with the airport and we need to work worth Transport Canada on operation of the plow.  Autonomy is so new that most governments have not caught up to what is going on yet."], "url": "https://discourse.ros.org/t/ros-based-fully-autonomous-airport-snow-plow/8344"},
{"title": "ROSject of the week: the Fetch It! Challenge simulation and control", "thread_contents": ["Hello ROS Developers,", "\nthis week we want to share the rosject we have created for the great competition organized by Fetch Robotics: ", ".", "Here you have a rosject with the full system ready to participate without having to install anything in your computer, everything running on the cloud, and for free: ", "This rosject will allow you to:", "As always, you can download all that code into your local machine at anytime, loosing though, the cloud capabilities.", "Any questions, suggestions or doubts?", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Start with the full environment set up, everything ready to work from minute 1, only waiting for your intelligent algorithms that will beat the competition", "It is git synchronized, so you will be able to do a simple ", " to get any update done to the simulation, correction of bugs, additional capabilities, etc", "Easily change between the execution of your code in the simulation and in the real robot. With two clicks you can switch in which system you want to execute your code.", "Use extra cloud computers to speed your development. ", " showing the performance differences between our different cloud computer configurations."], "url": "https://discourse.ros.org/t/rosject-of-the-week-the-fetch-it-challenge-simulation-and-control/8628"},
{"title": "ROS wiki for Intel Robotics Open Source Project", "thread_contents": ["A new ROS wiki about the Intel\u00ae Robotics Open Source Project was created at ", ".", "\nWith this wiki, you can find the information of key projects we are working on under ROS umbrella.", "Your feedbacks and contributions are highly welcome!", "Thanks,", "\nCathy", "Cathy,", "I posted a question under the tb3 category that pertains to Intel ROS Project.", "Your post is a year and a half old. I hope there is someone monitoring this project that can help me.", " Please don\u2019t try to add onto an unrelated thread to get more attention for your question. It\u2019s bad etiquette and clutters up our community forums. Please see the Etiquette section of our support guidelines:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " ROS node for object categorization backend with Intel\u00ae Movidius\u2122 NCS stick.", "\n", " Just announced at ", ". This is based on 3D camera and ", " ROS nodes to provide object classification, detection, localization and tracking information. We\u2019d do future optimization and integrate VPU accelerated Movidius\u2122 NCS solution soon.", "\n", " ROS node for object detection backend based on GPU optimized ", " solution.", "\n", " ROS node for launch file configuration.", "\n", " ROS package for object related message definitions.", "Expect more\u2026"], "url": "https://discourse.ros.org/t/ros-wiki-for-intel-robotics-open-source-project/3296"},
{"title": "Dorna Arm Ros OSS https://github.com/rakutentech/dorna_arm_ros", "thread_contents": ["Hey all,", "This is my first time posting so I hope I am doing it correct.", "I have released OSS for the ", " Robot to integrate with ROS, Gazebo and Moveit that can be found ", ".", "The Dorna arm is a very low cost 5 DOF robotic arm so is great for everyone.", "This is my first OSS project so I know there is going to be lots of problems and look forward to learning and contributing more in the future.", "Thanks,", "Mitch", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/dorna-arm-ros-oss-https-github-com-rakutentech-dorna-arm-ros/10519"},
{"title": "Real time safe logger", "thread_contents": ["Hello everyone,", "Recently I created a small project ", " to log generic data (e.g doubles, std vectors and eigen vectors and matrices) trough real time publishers ", ".", "\nTo see how it works check out this ", " file.", "I hope this can be useful to have a quick and simple way to log numeric data from real time controllers. In the future I intend to extend the project to support more data types.", "Let me know if you find it useful ", "Gen", "Thanks for sharing ", ".", "We also developed also a real time publisher for numeric data earlier this year as a part of a ROSIN proposal: ", "Besides the publisher, which you already have, we have some integrations for storing and displaying the data for short term (PlotJuggler) and long term (Graphite/Grafana) which may be useful to you.", "Nice work! Thank you ", " for sharing it.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/real-time-safe-logger/10565"},
{"title": "Yet Another ROS 2 Navigation Robot: Raspberry Pi Mouse", "thread_contents": ["There are already two posts about robots enabled ROS 2 Dashing with ", ".", "I\u2019m happy to announce another ROS 2 Navigation robot. It\u2019s a ", " made by ", " (a Japanese robot maker). A 2D LiDAR ", " is also attached on the top of the robot.", "ROS 2 Cartographer SLAM with RasPi Mouse and RPLiDAR A1M8", "\n", "ROS 2 Navigation with RasPi Mouse and RPLiDAR A1M8", "\n", "All packages:", "\n", "ROS 2 pakcages for Raspberry Pi Mouse V3. Contribute to youtalk/raspimouse_ros2 development by creating an account on GitHub.", "\n", "ROS 2 driver of Raspberry Pi Mouse:", "\n", "ROS 2 node for Raspberry Pi Mouse V2 from RT Corporation - gbiggs/raspimouse2", "\n", "ROS 2 migrated RPLiDAR driver:", "\n", "Contribute to youtalk/rplidar_ros development by creating an account on GitHub.", "\n", "Finally I\u2019m giving a special thanks to ROBOTIS members. The forked ", " package works fine for the Raspberry Pi Mouse too.", "Great to see more robots working with Nav2! ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/yet-another-ros-2-navigation-robot-raspberry-pi-mouse/10670"},
{"title": "Sparse Bundle Adjustment Python Wrapper", "thread_contents": ["I recently made a ", " over the excellent ", " library (", " for linux wheels). I\u2019m using it in a graph slam package. So far only the SBA2d piece is exposed (example on github) but I will be working on exposing more bits and pieces as I build my SLAM package.", "Sharing it here in case it\u2019s useful to someone. If you have requests please post here or open an issue on Github ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/sparse-bundle-adjustment-python-wrapper/10699"},
{"title": "The NASA Curiosity Rover ROSject is now open source", "thread_contents": ["Dear ROS developers, the NASA Curiosity Rover ROSject (", ") is now open source for everyone. Ready-to-go project including:", "\nThis is a port to Gazebo-ROS of the Curiosity Mars Rover, including velocity control, mast control, odometry, camera and mars terrains.", "It is based on the official 3D models issued published by the NASAJPL itself, including the Mars terrain map, which is based in real martial holography.", "This ROSject is a first basic version of the Curiosity rover that went to Mars, check out all the footage and images generated by the real robot: ", ".", "This ROSject serves as the foundation to further upgrades to make it as close as the real robot as possible. Its open for the community to make its own upgrades and contributions through ROSjects or the Git: ", ".", "This first version has totally functional 6 wheel drive and suspension, arm control and mast movement. It also has installed a Camera on the mas to start creating Vido Based navigation around Mars.", "\nWelcome to NASAJPL-ROSJECTS!", "I see there was some discussion over on a JPL related form about adding to this package:", "Hi Rover Builders, We have just published the first version of a Gazebo simulation of the real Curiosity Rover, based on the 3D models published by NA", "Was there any progress in updating the 3D model/mass to use a more official metrics of the rover?", "Robot Garden wants to go with a more realistic/authentic look for our Curiosity Rover mock up. So, we decided to not use the predrilled aluminum chann", "Hi Rufus,", "\nsorry we did not have any update yet. Just ", " at that time.", "Let me know if somebody wants to help in improving the model. We created everything from scratch and published as open-source.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS code", "Gazebo simulation", "Mars environment", "Rviz config files", "Jupyter notebook with instructions"], "url": "https://discourse.ros.org/t/the-nasa-curiosity-rover-rosject-is-now-open-source/7635"},
{"title": "Rosject of the week: the Dogbot quadruped robot with walking controllers", "thread_contents": ["Hi all ROS developers,", "\nwe at ", " have collaborated with ", " to create a running simulation of their cool quadruped DogBot. You can get everything ready to launch in this rosject (including documentation about how to launch the simulation and some basic controllers and walking behaviors that allow you to move the robot around).", "Get ", ".", "Remember that the rosject will run with a single click on the cloud on any type of computer without requiring installation.", "You can also download the full code to your local machine and make it work there.", "Everything in that rosject is published as Open Source, so basically, you can do whatever you want with it. If possible, please give us some credit!", "It may be of your interest this video that shows how to add pressure sensors to the DogBot simulation", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/rosject-of-the-week-the-dogbot-quadruped-robot-with-walking-controllers/10735"},
{"title": "Announcing Offline Support for AWS CloudWatch ROS Nodes", "thread_contents": ["Hey all,", "At ", " we\u2019ve launched a new feature to the ROS AWS ", " and ", " packages. Specifically, when your robot loses internet connectivity data is buffered on disk to be uploaded later when connectivity is restored. We\u2019re excited about this feature because it allows platforms to reliably sync data while operating on the edge.", "For more details and demos please visit ", "Thanks!", "\nDevin and the AWS RoboMaker Team", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/announcing-offline-support-for-aws-cloudwatch-ros-nodes/10812"},
{"title": "Gazebo model collision plugin", "thread_contents": ["Hi,", "Just wanted to throw out there I just put up a section of a larger project that I think might have reuse.", "The Gazebo model collision plugin is a gazebo plugin you place in your URDF that will advertise a topic giving you information if your robot is in collision with an obstacle in gazebo. You can do this with a collision object to estimate the robot footprint, or with the mesh of the link itself which is more useful for finer control. You can load this plugin into any/all links and receive really fine understanding of what exactly was in collision and in collision with what named object in the environment for analysis.", "This was motivated by some research I\u2019ve been conducting on deep reinforcement learning but I\u2019m sure there\u2019s better uses for it. It\u2019s a pretty quick plugin that just exposes the collision information to ROS for an external program to utilize.", "A gazebo plugin definable from URDF to inform a client of a collision with an object - SteveMacenski/gazebo_model_collision_plugin", "Thanks, for sharing this ", ". I think I have a few use case\u2019s for this plugin. ", "Anytime, its pretty basic and figured it is useful.", "Feel free to star and file tickets as you have them!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gazebo-model-collision-plugin/10814"},
{"title": "Blog post 2: ROS 2 Dashing with Navigation", "thread_contents": ["I\u2019m really excited to announce our next ", ", featuring our OpenRover running ROS 2 with the Navigation stack! We did a comparison of different DDS implementations in a typical deployment over wifi, and recorded some ", " ", ".", "The code we used is available on github:", "I worked very hard on this and would love to hear what you guys think!", "Awesome job! Great to see Nav2 working well with your Rover! I\u2019m going to pull your repos file and drive it around . ", "Also, I want to add that we\u2019ve done some testing with CycloneDDS and it\u2019s looking good in our testing too.", "Hi ", ",", "I think the scenario you are testing is really complex, and it is hard to extract any conclusions regarding performance.", "A couple of months ago you were ", " and this time OpenSplice often shows start-up times higher than 10 min, according to your blog post.", "In such a complex system, with components that are right now on development and have problems using any DDS implementation (see for example ", "), I think the best approach is try to isolate the issues and understand the reasons behind the numbers/hangs you are getting.", "Because the symptoms you are describing it seems you are probably losing discovery multicast announcements. Multicast is not very reliable in some wifi routers, and take into account the discovery announcement period is 30 seconds by default in most implementations. This could lead to long discovery times. Could this issue also produce timeouts in your scenario? You said just re-starting the rviz solves the hang issue, and this could indicate that is the case.", "In July, we contacted you through email offering help to debug these problems, and here I extend the invitation. Let\u2019s have a meeting to discuss how to isolate/reproduce the issues.", "Let\u2019s have a meeting to discuss how to isolate/reproduce the issues.", "If you would like to have an isolated and reproducible example, I\u2019d welcome any input on:", "From the README.md on the demo repo, only one command is needed to start the demo:", "Perhaps we could open a ticket or a new discourse thread as well?", "Hi ", ",", "Happy to help. I have read the issue you are referencing, and yes, it could be related to losing samples in the initial multicast discovery traffic.", "We do have an issue we are using to try to isolate the wifi multicast discovery problems:", "\n", "\n", "And also we are preparing a document with some configuration hints for this kind of scenarios. I will share it also here this week.", "Right now our team is testing intensively this kind of scenarios, with different configurations and routers, so we should have more information soon.", "I think the scenario you are testing is really complex, and it is hard to extract any conclusions regarding performance.", "A couple of months ago you were ", " and this time OpenSplice often shows start-up times higher than 10 min, according to your blog post.", "It\u2019s absolutely complex! OpenSplice Community 6.9 works great, and more reliably than Fast RTPS with a ", " of nodes (at least at the time that post was authored). But introducing the ROS2 Navigation stack makes OSPL frustratingly slow and unreliable.", "Here\u2019s the issue writeup ", ". Some attempted fixes are discussed in the GitHub issue, namely changing ", ", enabling ", ", and tuning high-water mark settings. Restarting RViz resolves bringup issues in Fast RTPS sometimes, but not in OpenSplice. After some analysis of ", ", ", " ", ".  I welcome and will try any tuning suggestions - just shoot over a ", " file. Better yet: ", " so it works better out of the box.", "It does seem that the issues go away in OpenSplice 6.10.2p4, even with shared memory disabled, but that (1) is unavailable to install via ", " or ", " and (2) requires a commercial license. I\u2019m hoping that a future release of OpenSplice Community provides a more usable foundation for ROS 2.", "Hi ", ",", "We have at this point 2 engineers of our core team working on characterize the performance on wifi networks of the multicast traffic, so we should have more information very soon. I will keep you posted.", "If in this case Shared Mem transport could help, we have good news, we are developing already the shared memory transport, and the goal is to provide an initial version for our next release (Open Source, we have no commercial edition).", "If in this case Shared Mem transport could help, we have good news, we are developing already the shared memory transport, and the goal is to provide an initial version for our next release (Open Source, we have no commercial edition).", "Good to hear! I\u2019ll keep my recommendations up to date when that\u2019s out.", "What do you mean you have no commercial edition? What is ADLINK Vortex OpenSplice 6.10.2p4 if not a commercial variant of Vortex OpenSplice Community Edition?", "Hi ", ",", "I am the CEO of eProsima, the company behind Fast RTPS. We have no commercial edition, all the features of Fast RTPS are open source.", "I am the CEO of eProsima, the company behind Fast RTPS. We have no commercial edition, all the features of Fast RTPS are open source.", "Oops! I misunderstood and thought you were with ADLINK! I\u2019m sorry for the confusion. I can\u2019t find any emails from eProsima but I\u2019ll be very happy to give any diagnostic info and try out any suggestions for tuning Fast RTPS. Feel free to email me at ", ".", "Hi ", ",", "Sure, we used the general info email in the past. We will contact you directly on this email with some hints.", "Hey ", ",", "I dropped the ball on responding to your email on July 31st. I apologize for that. Glad you and Dan could connect here. Dan is our local expert on DDS stuff so he is the person to be in contact with. I\u2019d love to see all the DDS\u2019s in the sub 10s bring up group. Lets see if we can make that happen.", "Hi guys,", "Good news, we have been working with RoverRobotics, and now our bring-up time is around 10-15 seconds, that is probably the minimum time required because the timing of their system.", "It seems some routers can cause problems when sending and listening to multicast through different interfaces (for example wifi and cable). See ", " for a full explanation:", "We provided a fix as a workaround, and now is already in Dashing if you build from sources, and will be incorporated to Dashing next sync very soon.", " could you check if this solves your issue too?", "Please note that we\u2019ve backported the relevant DDSI enhancements in our commercial 6.10 release (w.r.t. retransmission of fragments) to our 6.9 community-edition \u2026 and are awaiting some feedback w.r.t. those resolving the issues here (upon which we\u2019ll also update the pre-built binaries on the community-website) \u2026", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/blog-post-2-ros-2-dashing-with-navigation/10656"},
{"title": "Cartographer Tuning", "thread_contents": ["Hi Everyone", "Can anyone please tell me more about running cartographer slam in low latency, I referred google docs but I didn\u2019t get much understanding there", "Thank You", "Thanks for your question. However we ask that you please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/cartographer-tuning/10841"},
{"title": "PC Hardware for ROS Project Development & Simulation", "thread_contents": ["There are some general guidelines on the hardware required for ROS projects and competitions, like the NASA Space Robotics 2 or ARIAC, but not much on how to maximize a development system. It would be good to understand the interactions to balance price and performance. Adding this information to the ROS Wiki might be useful.", "There are two aspects that I see. First is the development aspect of compile times. Second is the performance of Gazebo when testing via simulation.", "Development is straightforward, I believe, the fastest CPU, memory, and disk. An SSD really makes a difference as I just found out on my current, vintage '14 system.", "A GTX GPU is required for Gazebo. What I don\u2019t understand is the interaction between the GPU and the rest of the system. The fastest GPU you can afford is the best option. But does main memory speed play a role? The GPU has to be pulling data from memory so a fast MB / memory would seem a good idea. What about CPU? I have no idea the level of interaction.", "Could it be summarized that good gaming machine is best for doing simulation work?", "What factors am I missing?", "A GTX GPU is not required and I am not even sure that Gazebo uses any GTX feature yet.", "To get a nice simulation rendering, a Nvidia GPU is a must but if the visual part is not so important, Gazebo works on Intel internal GPU.", "Usually, if I do not need the visual part or if Rviz is enough to visualize TF and sensors data, I only run gzserver and the needed performances are much lower.", "Gazebo is makes use of multiple threads, but still the bottleneck is on single core speed.", "\nFor compilation typically you\u2019ll want more cores, but if you are iterating on code making small modifications, you won\u2019t benefit a lot from mass parallelization, since you\u2019ll only compile the code that you have changed. Probably an 8-core is more than enough.", "I\u2019d recommend a 9700K if you are into intel. Lately I\u2019m pushing more for AMD, for a lower budget you can get a 3700x or 3800x. But you\u2019d be losing some single core speed for more threads.", "16GB of RAM is a must, 32GB is not too much.", "And like ", " said, a GTX is probably too much, any low or mid end GPU will be enough for gazebo and rviz.", "edit: Yeah a gaming pc is a good starting point, just cut costs on the GPU, since it\u2019s the most critical aspect of a gaming PC, and the least important for you. If you go with an AMD Ryzen, a fast RAM (up to 3733Mhz) will do wonders for your CPU.", "\nGlad to see you\u2019ll participate also on Phase 2!", "The system requirements for SRC1 and ARIAC specify GTX. The SRC2 just says an Nvidia GPU.", "The Gazebo site tutorial just says an Nvidia GPU.", "Interesting idea to just run gzserver and RVIZ.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pc-hardware-for-ros-project-development-simulation/10910"},
{"title": "ESMERA Project - up to 200 000 euros of funding", "thread_contents": ["Within the European project ESMERA, SNCF (French Railways Company) has defined a challenge that could be solved by introducing a robotics application.", "Please have a look on the challenge C2.A1 in the category \u201cESMERA Challenges Construction\u201d.", "\u201cROS powered solutions\u201d will be welcomed.", "We may have the right platform for answering this call. Is  more information about the topology of the tunnel to inspect? Ground surface assumption? Network connectivity ? Environmental constraints?", "Hello,", "Nice to hear that we have  a platform to answer this call. I can\u2019t unfortunetly answer directly your questions. All the tenderer should have the same informations and we can\u2019t guarantee this with this forum.", "If you send me an email (", "), I\u2019ll put you through to an ESMERA team member. She\u2019ll answer your questions (maybe on a dedicated ESMERA forum).", "\nThis is not very flexible (sorry for this !!!) but at the end, we get your answers\u2026", "Thanks for your answers!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/esmera-project-up-to-200-000-euros-of-funding/10912"},
{"title": "Azure Kinect ROS sensor driver is now available", "thread_contents": ["Hi everyone,", "I\u2019m here to announce that we\u2019ve just released a ROS node to add compatibility for the new Azure Kinect Developer Kit (", ").", "This node provides compatibility with ROS Melodic Morenia on both Ubuntu 18.04 and Windows 10. It provides access to a number of sensor streams from the Azure Kinect, including:", "The source code for the node is available here:", "A ROS sensor driver for the Azure Kinect Developer Kit. - microsoft/Azure_Kinect_ROS_Driver", "We\u2019re looking forward to hearing your feedback!", "That looks super cool! Are there any plans to share some bag files created with this sensor? Would love to see it in action!", "I don\u2019t have plans to release any bag files at the moment: releasing recorded camera data is a bit tricky. However, we have published some sample recordings in the native Azure Kinect recording format (mkv). It would be possible to add Azure Kinect mkv playback to the ROS node so that it can play Azure Kinect recordings into ROS.", "You can find the example recordings here: ", "And information on the playback API can be found here: ", "I don\u2019t have plans to release any bag files at the moment: releasing recorded camera data is a bit tricky.", "Is that a technical limitation, or a legal one?", "  The tricky part is due to the group policy on videos. My team will be working with the Kinect in a different environment next week and will capture rosbags and video.", "Ok, clear. Thanks Lou.", "Looking forward to those bags.", "Does it support ROS Kinetic?", "Not officially, no. The underlying Azure Kinect SDK only officially supports Ubuntu 18.04. The ROS driver only supports the matching release for Ubuntu 18.04, which is Melodic.", "Anecdotally, I\u2019ve heard that some people have had success getting this working on Kinetic, but you\u2019d need to first install the Sensor SDK and get that working before you could try to build the ROS Driver.", "Where can I get more information about things like bandwidth requirements, outdoor performance, and if multiples of these things can be looking at the same area?", "More information about the Azure Kinect itself can be found on its ", ".", "To answer your specific questions:", "I see. That makes a lot of sense. Thank you for the response and the links.", "The d435 actually has really good performance outdoors with the projector turned off (as far as realsenses go anyway).", "Yes, the newer RealSense cameras (like the ZR300 and the D435) work outdoors since they can fallback to a pure-stereo depth approach when their IR projectors get washed out by the sun.", "Azure Kinect doesn\u2019t have a stereo fallback mode, so once the IR projector gets washed out there\u2019s no more depth data.", "Something I\u2019ve not yet tried is using ", " to extract stereo depth from the IR and RGB cameras. If the scene was well lit with both IR and visible light (outdoors in sunlight) you might be able to recover some depth data.", "I\u2019m sure what the Kinect lacks in ability because of being a structured light sensor it more than makes up for with accuracy (these point clouds look beautiful!)", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Raw infrared, color, and depth Images", "Registered depth and color Images", "A PointCloud2, optionally colored using the color camera", "An IMU stream", "Factory-captured intrinsic and extrinsic calibration data for the color and depth cameras, as well as the IMU", "\n", " The Azure Kinect requires USB3.0 bandwidth. To my knowledge, this is a hard requirement: the camera won\u2019t function at all on USB2", "\n", " There\u2019s support in the SDK for running multiple cameras in the same space by using a sync cable to avoid interference. More information about the various multi-camera setups can be found ", " and ", ". There isn\u2019t support for multi-camera mode in the ROS driver, but it would be reasonably simple to add it.", "\n", " I\u2019ve never used the Azure Kinect outside, but it\u2019s fundamentally similar to other IR-projector depth cameras, which typically have poor performance outside."], "url": "https://discourse.ros.org/t/azure-kinect-ros-sensor-driver-is-now-available/9782"},
{"title": "Vapor-master, a distributed high availability rosmaster", "thread_contents": ["Announcing vapor-master", " is a drop in replacement for rosmaster enabling high availability ROS service discovery. Vapor removes the single point of failure fundamental to ROS1, enabling new options for achieving scale without sacrificing stability.", "Vapor makes it possible to scale ROS 1 workflows to cloud scales, support hot-swap scenarios, and accomplish redundant handover. All without significantly modifying legacy code.", "Have a look at our ", " to get started using vapor-master today.", "Checkout the ", " or download the ", " for rapid setup.", "Hi Alan,", "\nThank you, this seems like an interesting addition and idea for ROS1 users. I will try to test it soon!", "\nI have a few questions that hopefully you can answer:", "Cheers,", "\nMart\u00ed", "Excellent questions:", "Unfortunately for some reason it does not work with Kinetic Kame.", "\u2026 ", " set a parameter. Subsequently doing", "\u2026 does return", "And", "crashes with the following error message on rosparam\u2019s side:", "\u2026 and no error message on vapor\u2019s side.", "The original rosmaster works.", "/edit: ", " apparently ", " work (comment corrected). The parameter is set in the database. Interestingly though I\u2019ve got ", " parameters with the very same keyPath, one for each testing call of ", " and ", ".", "Thanks for the bug report!", "We\u2019ll look into it. Track our progress at ", "Hi ", "sorry about the lame bug, we\u2019ve merged a fix ", " and have deployed to ", ".", "You can update your installation with:", "To prevent this type of bug in the future we\u2019ll be adding protocol level integration tests. Check out ", "A talk was delivered at the ", ". The video can be ", "Hi Alan,", "Is there any tutorial out that can help me to set up a redundant vapor-master setup? My use case is mobile robots and I want to have two instances of vapor-master running in to provide continuous ros master availability. I have one powerful main computer and one single board computer in the robot. The intention is the primary master runs on the main computer and a second master runs on the SBC. Also is it possible to run multiple instances of vapor-master on the same computer?", "Best regards", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Is the vapor-master computing overhead noticeable?", "When a new vapor-master is connected, does it have all topics, services, etc, available? or what is public for each vapor-master is customisable?", "Security wise, are you adding any kind of extra authentication?", "Does it work with ROS Kinectic? Are you planning on releasing the debian packages for it?", "On some work loads we have seen higher than desired CPU hits. Specifically about 10% higher than rosmaster for certain rospy publishers. This is due in part to an upstream bug in rospy and a lack of adequate caching on the vapor side, ", ". Typically most pronounced on small systems like raspi. Typically not noticeable on modern x86 systems.", "This is a great idea, a middleware/filter feature here similar to the approach in the server in rosbridge_suite would make a lot of sense for some use cases. We\u2019d welcome a feature request so we can design it out together.", "Security was sadly a non-goal of this rev. We also have some tools at a different layer coming in the near future so stay tuned. We expected a desire to become SROS compliant, not presently on the road map on my side but we should explore it more. Fundamentally the security threat model is very similar to stock ROS 1 presently, with the exception of the securing the db connection via SSL and mongo user+token. Major concern from my side is that all security features must have excellent UX\u2026 poor UX in security is nearly as bad as no security. So lets continue discussion in issues and make sure we can deliver great UX.", "Vapor should happily run on kinetic, we just haven\u2019t tested there. Would love feedback if you\u2019ve tried it."], "url": "https://discourse.ros.org/t/vapor-master-a-distributed-high-availability-rosmaster/7375"},
{"title": "Semi-regular Glamour Shots Thread", "thread_contents": [" we\u2019re going to try doing a semi regular \u201cROS glamour shots\u201d thread in ROS Projects. What is a glamour shot? It is a screen capture, tutorial, video, or image of whatever you are working on in ROS or Gazebo. We all do this in our private work forums but we should do it out in the open more.", "There are five rules:", "I feel like I shouldn\u2019t repost the two things I\u2019ve recently posted (one in the attached thread and another ", ") and instead I present that one time we drove a robot in VR back in grad school (4 years ago I think).", "The area is digitized using a kinect and ElasticFusion + a home grown mesh simplification algorithm, the VR rendering was done through blender, and AMCL localizes the robot in this environement against an existing occupancy grid map.", "ROS melodic, ZED-M camera and Livox Lidar. Nvidia Xavier processor.", "Interesting, Is wiring for the wheel motors running through the the rod?", "Glad this thread is alive !", "We did some videos of our robots with different use case (well, the usage is arguable) just for fun with my intern this year (and also to teach them ROS a bit).", "I might post them on a regular basis, I want to see more cool robot!", "Just to begin with something basic, we did the good old \" I\u2019m not a robot \" with a dobot :", "\n", "The catch here : We didn\u2019t use a camera for recognition, we plugged our control software with a small ros node that send keyboard (1-9 and enter) command to the robot driver with prerecorded cartesian trajectory. We just had to launch it, the robot press submit, we press the correct button to pass the captcha, and press enter so the robot can do the last trajectories and submit the captcha. The robot is just semi-autonomous but it was fun to make !", ", a robot I\u2019m building as a learning platform for ROS.", "\nIt is additionally serving the role of development/test robot for a new version of ", " based on STM32.", "\nIt is equipped with a RPLidar A1, a Realsense r200, a BNO055 imu, current/voltage monitor and is powerd by a a Jetson Nano.", "Yea the box is supported by a pair of 1\" carbon tubes so all the wiring runs through those to the wheels. It has about 8 inches of articulation.", "Hi!", "Erwhi Hedgehog is actually the smallest  (120x120mm) ROS robot that achieve autonomous navigation and vision tasks.", "This is main repo to the project:", "\n", "\nIt is completely opensource and openhardware.", "\nIt works on ROS, Gazebo and, if you need, AWS Robomaker.", "Learn more on my website:", "\n", "Gazebo: ", "Intel spot: ", "Nice documentation gbr1!", "Well since we post pictures of our loved ones\u2026", "\nHere comes my ", ":", "\n", "ROS Kinetic running on a i.MX6 ARM Cortex-A9 Quad Core, main sensor is a rplidar a2 (image still shows the XTion). Odometry is corrected with a BNO055 IMU. It is powered by two batteries in parallel. I/O is done by additional AVRs microcontrollers.", "That\u2019s such a cute robot (and a beautiful photograph).", "Is there a way to buy just some of the components? Or is it \u201cfull kit\u201d vs \u201cprint/source your own\u201d?", " thank you for your interest in Erwhi Hedgehog!", "\nI\u2019m working on \u201chow to buy\u201d option and understanding what could be better for a student like me (e.g. open an online shop, sell on tindie, etc.).", "\nPlease feel free to write to me an email (my address is ", ") with the list of parts you need.", "A first kit of the robot (without sengi carrier board and other \u201crobotic parts\u201d) was released by UPBoard/AAEON:", "\n", "UP Squared RoboMaker Developer Kit\t\t\t\tThe UP Squared RoboMaker Developer kit is the easiest way to get started with your robotics project powered by AWS RoboMaker. Utilize the power of AI Core X with Intel\u00ae Movidius\u2122 Myriad\u2122 X VPU, Intel\u00ae RealSense\u2122...", "\n", "\nIn fact demos I did for this kit are the same of Erwhi, and Intel IoT \u201ccommercial\u201d is with Erwhi.", "Holy moly. That is incredible! Keep us posted. I would love to see this thing walking.", "I for one welcome our arachnid robot overlords.", "massive garage", "You must not live in California ", " otherwise I\u2019d love to help as I could!", "We are OUTX Polaris we are building a Big USV.", "All of navigation stack is open source.", "aibo official website", "This arachnid robot overlord has a giant red emergency off switch on its side. I think you\u2019re safe.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["It must be something you or your team made.", "It must use ROS or Gazebo.", "You need to include at least a one sentence description.", "Nothing is too big or too small. We want everything from home brew bots to fleets of autonomous cars.", "Be excellent to one another.", "Modelling Pneumatic cylinders forces in Gazebo", "Experimenting with Gait", "I\u2019ll then move on to a motion plan and controller.", "\nIt\u2019s an art project and when it walks it\u2019ll be covered in floral suits which are slowly growing;", "\n", "\n", "\nSome more info at ", "\nAdvice is welcome. It\u2019s a bit robotically lonely in Cape Town."], "url": "https://discourse.ros.org/t/semi-regular-glamour-shots-thread/10983"},
{"title": "Visual topic selector for production of filtered BAG files", "thread_contents": ["In my organization we often filter .BAG files in many cases by variety of topics. To improve our workflow I developed ", " hosted on Github. So now I want to kindly ask the community for opinion about is such tool useful (especially for beginners) or not?", "Wow, an entire TUI written using bash and the rosbag CLI. Having a nice TUI like this is awesome.", "Nice! Apparently a lot of us need this kind of tools:)", "Create a rosbag from a given one, using a simple GUI - facontidavide/rosbag_editor", "Having a TUI alternative is great!", "What a god-send.", "I have had countless occasions where I record a ROS bag with everything in it only to wish I could easily filter out the space consuming camera topics so I can send the bag file to another developer in another part of the world without spending a long time uploading, downloading and trying to store giant bag files. Its usually quick to verify the camera topics have been processed correctly in to another topic and once that\u2019s done they just get in the way.", "Is it possible to filter out specific transforms using this? Like if I want to remove the map to Odom corrections in a bag file to try running a localization algorithm on it? I currently use rosbag filter for that. The experience is less than ideal.", " if you can find a nice UI to do that, I will be happy to implement it.", "UPDATED: I did implement it, have a look", " , you can find TFs of your need and then unselect them in topic list. After that they will not be kept in result .BAG file. But if these TFs set up using single node, they will be removed together. To remove some needed specific TFs you unfortunately need to recreate TF publisher with excluded TFs", "An addition: using CI engine I can provide Debian packages for my script easily. Actually I am a novice with ROS buildfarm, so if you can direct me to good guide, I can try to promote my package into ROS main package tree", "looks nice for me, we will give it a shot!!!", "thanks,", "\ntomoya", "Actually I am a novice with ROS buildfarm, so if you can direct me to good guide, I can try to promote my package into ROS main package tree", "You should take a look at this ", "Found an expression for TFs, trying to integrate", "\n", "There are a couple of problems with that expression:", "Anyways, here\u2019s a small script that does all of this using the ", ", which I can highly recommend for filtering rosbags:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["It is obviously not going to work for TF messages with more than 5 transforms.", "It doesn\u2019t take the ", " topic into account.", "It filters out every transform that has ", " as the ", ". This is okay for ", ", since it usually only has one child (", "). In general though, I\u2019d say it\u2019s more convenient to filter out a specific child frame. The only drawback is that it\u2019s a bit counter-intuitive at first that you have to specify ", " when you actually want to \u201cremove the ", " frame\u201d."], "url": "https://discourse.ros.org/t/visual-topic-selector-for-production-of-filtered-bag-files/11142"}
{"title": "ROS on the new Jetson Nano with STEEReoCAM\u2122 camera", "thread_contents": ["We tested and we published a tutorial about how to install ROS and configure it to be used with our STEEReoCAM\u2122  MIPI stereo camera.", "Let me know what you think about:", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-on-the-new-jetson-nano-with-steereocam-camera/11196"},
{"title": "ROS2AR project ", "thread_contents": ["Hey All,", "We would like to share our preliminary results on the AR visualization for ROS enabled robots using Android devices including smart glasses (e.g ", ").", "The project makes use of the ", " engine, and currently we are working on porting our solution to ROS2.", "The current demo was tested on both Epson BT-200 smart glass and Samsung Tab A6 and a number of different types of robots including AGVs (Pioneer 3), UAVs (ArDrone2) and robot arms (Cyton Gamma 1500) as well, but they are intended to work on different robot platforms as well.", "In our first ", " you can visualize with an Android enabled AR device the list of nodes which should be running on your robot. The recognition of the robot can be done either by associating a QR code or by training the visual model of your robot using the ", ".", "The project is still under development, but you can access it here:", "\n", "This work was Supported by ", " ROS-Industrial Quality-Assured Robot Software Components.", "Best regards,", "\nLevente", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2ar-project/11345"},
{"title": "Another ROS1 One line install", "thread_contents": ["Hello,", "I\u2019ve been working on a single line installation since I find myself re/installing ROS a lot :", "Script to install ROS in one line. Contribute to mlauret/ros_easy_install development by creating an account on GitHub.", "I tought it can be usefull for newcomer/student/learner so I made it user friendly and trying my best to make it error proof (You can\u2019t use it if you tried to install ROS before since the script check for previous installation in different way)", "At the moment it work only for ubuntu with Kinetic and Melodic.", "I\u2019m aware that an another one line installer exist, but it\u2019s not maintained, not error proof (no check for previous installation) and I wanted something even simpler in the vein of ", ".", "The installation can be done like this : ", "the only customization is on the sudo password passing (cli, env variable or file), since this installation work only in user mode.", "Feel free to get the scripts and customize it if you need !", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/another-ros1-one-line-install/11355"},
{"title": "FXIMU Sensor for ROS - Open Source Project", "thread_contents": ["Greetings;", "We made an open source imu sensor for ros, that utilizes FXOS8700 and FXAS21002 sensors from NXP semiconductors, coupled with a Texas Instruments TM4C123GH6PM Arm Cortex based MCU.", "The set of sensors from the NXP semiconductors is said to have 100 times better noise characteristics, then what is available on the market.", "We have ported ROS\u2019s imu_complementary_filter code to run on the TM4C123GH6PM platform, to achieve:", "Since the complementary filter is running at an embedded level, there is nearly no latency. It attaches directly and powered by USB port.", "Calibration is done by putting the unit in calibration mode using the params file, and running a set of scripts provided, the ros publisher is turned into normal serial data forwareded to another virtual serial port using the program socat. So once the values are forwarded to virtual serial port in ASCII format, one can use off the shelf calibration utilities, such as Paul Stoffregen\u2019s MotionCal. Basically you can calibrate your sensor, without taking it off from the robot, or updating firmware,", "We are working on a V2 of the board, that includes cosmetic and ergonomic changes, and the addition of a i2c port.", "It runs extremely well, and has more space for development. I built it for a robot I was building, then noticed it could be useful to others, so decided to develop this as a seperate project. Because the filter is fed at 400hz after being read from i2c, it offers nearly zero latency.", "I have worked with various IMUs (I will not name them here.), including ones that include on-board digital motion processors, and the results we are getting are much better than what is available on the market that is the same class of device.", "Souirce code available at ", "Prior version source code available at ", " and ", "All are welcome to parrallel develop this project, and I provide circuit boards for people who can solder SMD components, along with montage diagrams and BOM. Right now github does not contain schematics nor gerbers, but just because I am building a new version with slight improvements.", "We are also asking for the help of ROS community to further develop and maybe fund this project so we can have them produced in SeedStudio or allike manufacturing house.", "People who like to replicate results are also welcome and will be helped.", "Best Regards,", "\nCan Altineller", "Hello Can,", "I follow your IMU project on different platforms. I would be happy to learn more, especially how to get to such a board or how to create one.", "\nYou say that there are no schematics and gerber-files on Git, because you are working on a new design \u2026", "Is it possible to get the \u201cold\u201d design? I would then check to see if I can etch myself a pcb.", "Best regards,", "\nThomas", "This is fantastic. Just to clarify this IMU supports ROS 1, ROS 2, both? It isn\u2019t immediately apparent in the read me?", "I am really curious about what sort of help you would need to produce these boards and what sort of quantity you would like to produce them at? Is the $20 price a retail price or production price at low quantity? Do you plan to make the boards open hardware, not just open source?", "This is currently ROS1, but I would love to port it to ROS2 as well, with the help of the community.", "20$ is just for parts, at low quantity, i.e. 1 by 1. If produced  by the 1000\u2019s, I would say it would cost 12-15USD.", "I plan to make everything opensource, both hardware and software, and I plan to make and sell the boards myself, and looking for collaborators to maybe pay for production costs at a pcb production house.", "But of course, first some other people need to verify my work and see if it is actually useful.", "Best regards,", "\nCan", "Hello ", "Per your request, I am posting them right now.", "Best regards,", "\nCan", "the hardware files are at ", "This is outstanding. How does it compare to the BNO085?", "Indeed, the BNO055 is very robust against magnetic interference. I wonder if anyone knows the algorithm behind that.", "I tested a BNO055 once. It has drift problem, fximu does not. precision is higher for the chipset I am using.", "Also you can not modify the digital motion processing on the BNO055.", "mpu9150 and bno055 was two chipsets that I was disappointed with the DMP, my original plan was to use a bno055, and dont do a lot of processing in my main loop - the bno055 did not work out for me, so I developed a board with a seperate processor, which I could do the filtering myself.", "In this filter, roll and pitch are not affected by magnetic interference, and it has magnetic interference rejection according to video in ", "I however tested this and found out it is not exactly the same as the video in the original code.", "Setting the mag gain low, protects againist magnetic interference, but not we have the filter code opensourced, we can maybe work on this all together, and make the magnetic interference rejection better.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Up to 400Hz read of acceletometer, magnetometer and gyro sensors", "Up to 400Hz update of the complementary filter", "Output is published at /imu/data and /imu/mag as sensor_msgs/Imu and sensor_msg/MagneticField message types, and output rate is determined by output_rate_divider parameter", "Hard iron and soft iron correction", "Everything is configurable by rosparams, from hard iron soft iron calibration to sensor_read rate, output_rate_divider, to gyro and accel sensor parameters, and all the parameters in imu_complementary_filter. (with the exception of tf) See: ", "\n", "Green led is on when at steady state", "It requires no serial driver, and uses no serial ftdi, but rather emulates a virtual serial port as a usb hid device, provided with the TM4C123", "It has an expansion port that can be used as serial, or analog input,  or gpio. PA0 to PA7 on TM4C123GH6PM", "It costs 20USD to build in components, and is open source."], "url": "https://discourse.ros.org/t/fximu-sensor-for-ros-open-source-project/11339"},
{"title": "Graph RViz plugin", "thread_contents": ["This is a RViz plugin to draw graphs from topics values.", "\n", "\n", "\n", "We have tested it on Melodic but I\u2019m pretty sure it will work on Kinetic too.", "This package is much less sophisticated compared to ", " but it has the advantage of being integrated into RViz.", "Feel free to open issues if you have problems or open merge requests if you want to fix the code or add new features!", "We added a new panel that allows to plot histograms of ", ":", "\n", "It currently works with mono8 and mono16 images but it can easily be upgraded to support more images formats.", "Hi,", "\nThis is very nice integration with rviz. Can the current UI support saving plots in different formats (jpg, png, svg) for later use ?", "It\u2019s possible to export graphs from the panel that allows to draw lines but not for the histogram panel.", "\nIt would be very easy to add; merge requests are welcome!", "Hi", "\nYes that feature would be very helpful. Thanks", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/graph-rviz-plugin/5692"},
{"title": "Semi-regular Glamour Shots Thread for 10/25/2019", "thread_contents": ["\nIt has been two weeks. Time for a glamour shots thread. With ROSCon coming up we\u2019re hoping to see a lot of activity. If you have any cool images or video from your current project now is the time to post it.", "What is a glamour shot? It is a screen capture, tutorial, video, or image of whatever you are working on in ROS or Gazebo. We all do this in our private work forums but we should do it out in the open more.", "There are five rules:", "Currently I am working on localization and planning trajectories using crazyflie2.1 platform.", "\nCheckout the video of its initial teleoperation test here:", "That\u2019s great. I would love to build an open source vicon system to do external drone tracking. If only I had more time.", " here!  Working hard on our aftermarket autopilot for commercial lawn mowers.  Here\u2019s us working at the field with some of our team\u2019s helpers:", "And here\u2019s our latest video:", "Using ROS: localization, our open source ROS planners, and a hardened, fast booting Ubuntu kernel.", ":", "Using ROS: localization, our open source ROS planners, and a hardened, fast booting Ubuntu kernel.", "Can you talk more about the hardened, fast booting Ubuntu?", "\u201cJohn you have to mow the lawn before you go riding\u201d.", "\n", "  Building Autonomous robots for the lazy man.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["It must be something you or your team made.", "It must use ROS or Gazebo.", "You need to include at least a one sentence description.", "Nothing is too big or too small. We want everything from home brew bots to fleets of autonomous cars.", "Be excellent to one another."], "url": "https://discourse.ros.org/t/semi-regular-glamour-shots-thread-for-10-25-2019/11228"},
{"title": "Raspbian Stretch Lite + ROS + OpenCV Raspberry Pi Disk Image", "thread_contents": ["Hi,", "We created a new SD Card image with ROS + OpenCV built and installed on the latest Raspbian Stretch Lite 2018-03-13 release.", "(Update 11/2/2018) Updated image with 2018\u201310\u201309 Stretch Lite Raspbian\u2026", "\n    ", "Enjoy!", "\nJack \u201cROSbots Maker\u201d", "Awesome! Thank you very much for the image and especially the fabric file!", "You welcome! Wondering what project you will be using the image for. Love to learn!", "Thanks", "\nJack \u201cROSbots Maker\u201d", "Hi Jack,", "Whats the default username and password for logging in.", "\nI just booted up and", "\nusername: \u201cpi\u201d", "\npassword: \u201crosbots\u201d", "\ndoes not work.", "Hey mate, the password is \u201crosbots!\u201d", "Unfortunately, my raspi shows \u201c-bash: /home/pi/ros_catkin_ws/build/opt/ros/kinetic/share/rosbash/rosbash: Structure needs cleaning\u201d", "What should I do?", "Compile the source codes from scratch?", "Hi Jack,", "\nI tried to install vrpn_client_ros using ", " but it tells me cannot locate the package. I am pretty sure that this package exists. Do you have any suggestions to solve this?", "Many Thanks! Amazing Work! Wishing I could try it on my new project.", "You\u2019re a saint! I am going to see if I can\u2019t you some more visibility.", "Wow. Does this have any GUI built-in\u2026? such as xfce or startx ??", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/raspbian-stretch-lite-ros-opencv-raspberry-pi-disk-image/4294"},
{"title": "Cozmo with ROS2 without SDK", "thread_contents": ["Hi there,", "for all of you using Cozmo from Anki", "I implemented a simple wrapper for using it with ROS2 without depending on the Cozmo SDK. Please, check it out and let me know what you think:", "At the moment, it allows to move the robot (wheels, head and lift) using a modified version of the teleop_twist_keyboard and publishes the images from the camera to a topic.", "Feel free to contribute to the repo.", "That\u2019s super cool, and the price point is pretty good for the features (~$115US on Amazon). Looks like there is quite a bit more possible. Pycosmo supports:", "Sensors:", "Actuators:", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Hi,", "Good news! So far Cozmo publishes:", "Now that it is working, the rest is quite straightforward (and boring ", ") so some help with the implementation would be much appreciated.", "That seems like is going to be a lot of work but probably worth the effort. Have you tried connecting to the cubes with PyCozmo?", "Not yet but I will try. In theory the pycozmo wrapper I am using for it supports it. I just have to convert the data to ROS2 msgs.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Camera", "Cliff sensor", "Accelerometers", "Gyro", "Battery voltage", "Cube battery voltage", "Cube accelerometers", "Wheel motors", "Head motor", "Lift motor", "Backpack LEDs", "IR LED", "OLED display", "Speaker - work progress", "Cube LEDs", "Platform LEDs", "Odometry", "Image", "Imu (orientation, gyro and accel)"], "url": "https://discourse.ros.org/t/cozmo-with-ros2-without-sdk/11689"},
{"title": "Semi-regular Glamour Shots Thread #3", "thread_contents": [" Which ", " can we get some new Glamour shots of cool projects and visuals? I personally haven\u2019t been able to work with any hardware in a while so could use a Friday robot pick me up.", "As ", " put so nicely in the previous thread, the rules are:", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/semi-regular-glamour-shots-thread-3/11829"},
{"title": "ROS2 Planning System", "thread_contents": ["Dear ROS community,", "I want to share with you my ", " project. I have just finished the first functional version, and I am in the documentation phase. If someone finds it interesting, you are encouraged to contribute.", "This project is the result of my experience of several years with ", ", which has inspired this project. It is a complete implementation in ROS2 that offers an alternative design with the functionality that we wanted during our developments.", "Enjoy and ", " !!!", "\nFrancisco", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros2-planning-system/11844"},
{"title": "OpenR2.org ROS project", "thread_contents": ["We are looking for people that may be interested in developing some open source ROS implementations for our OpenR2 project.", "We have four projects that we want to get started.", "\nThree of the projects have ROS components.", "-Head bearing system", "\n-Leg drop system", "\n-Center foot steering system", "We have a 5 year access to Dassault Systems 3DExperience environment in the cloud. We will be using this to managed the projects and create the geometry of each system.", "If anyone is interesting contributing thier ROS skill while getting the change to learn the latest CAD an PLM software, check out our Facebook group.", "Robert", " Can you explain your project a bit more? ", " does not have any content. Also I, like many other ROS users don\u2019t use facebook. That facebook group link is behind a registration wall.", "ROS users don\u2019t use facebook", "+1 for non-facebook user and would potentially be willing to get involved.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/openr2-org-ros-project/11862"},
{"title": "Experimental Python package index for ROS", "thread_contents": ["Hi, Python ROS users!", "I\u2019d like to introduce an experimental ", " which hosts basic ROS Python packages that can run without ROS installation.", "\nThis enables to", "You can try ropy as follows:", "Most of the packages are written in pure Python, so they can be installed into multi platforms and also Python 3 environment.", "\nA few packages(tf2_py and PyKDL) which need to be built with a c++ library are offered also as binary wheels(only Linux and OSX), you don\u2019 t need to build them by yourself.", "All packages are built by a script in ", ".", "Please enjoy and any feedback is welcome!", "Best,", "This looks super useful. I wish this was around when I needed a Ros free version of TF2 (though that need led to ", ").", "Why not put the packages on the actually PyPI?", " This is great to see. Thanks for sharing. If you\u2019d like to pursue it further I\u2019d suggest looking at if it can integrate with ", " which supports generating indexes of packages for different platforms. A pypi index could be another platform onto which to extend support.", "Why not put the packages on the actually PyPI?", "I think that the private python index is more appropriate for this until it\u2019s a more formalized effort. If you put something on PyPi it should be a commitment to maintaining it. And there\u2019s also likely a lot of potential naming collision and name grabbing if you automatically post all of these.", "That\u2019s a fair point. I do like the idea of a ROS/ROS2 python package index. I think it would fix a number of my pain points. Open to help as needed. ", " if you would like help with the Superflore integration if that\u2019s something you\u2019re interested in please hit me up.", "Thank you very much for your feedbacks ", ", ", " !", "I didn\u2019t know ", " and I found it interesting to integrate with it.", "\nI will take a deeper look and try something.", "Great job ", "! I would also be down to help if you needed any. ", "Hi ", ", cool initiative!", "I have been working on getting ROS packages on conda/conda-forge. This is like pip + virtualenv but on steroids, and works for native packages as well. There are many benefits that I mention in my blog post:", "\n", "Finally ROS can be  conda-installed The initial ROS packages are available for OS X and Linux, and soon Windows! More on  why this is\u2026", "\n    ", "\n", "Maybe we can join forces somehow? I would be happy to get more contributors to this adventure ", "Being a primarily pip/virtualenv user with a passing interest in Conda, I\u2019d like to see both paths pursued, ideally with common build strategies. I\u2019m very interested in helping with such an effort", "I understand the need to run separated package management tools for everything that runs inside ROS, to keep a tight control on it, but a python package that is detached from ROS releases should ", " go into any ROS-specific package manager, it needs to go to ", " (and then also ", " for convenience).", "Regarding the commitment to maintain it, I agree it is a must, but I also think it\u2019s not so difficult to commit to that: at least, on my side, I offer to help with maintenance if this package moves into proper distribution channels. I am currently the maintainer of ", ", which we developed exactly for the purpose of using ROS from outside ROS in Python, so, I\u2019d gladly support this effort as well.", "EDIT: corrected ", " instead of ", "I assume by ", " you mean ", ". I both agree and disagree. I think it\u2019s fine to put such packages on ", ", but if they get put on another index I think that\u2019s ok too. The user would simply need to edit their ", " to ensure that the new index gets searched as well (which you could provide a script for),very similar to how the ", " source is added.", "Correct, I meant ", ", I fixed it.", "I think you\u2019re answer doesn\u2019t quite answer the point. It is clearly possible to ", " upload it to another index, it\u2019s also possible to ", "-install it directly from git for that matter, and from a zip file on a website, and probably a number of other exotic means. The point I\u2019m trying to make is that, the default way of installation should be the default package index of the python community, ie ", "; any non-standard steps to installation will lead to less adoption.", "Cheers", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["try ROS(rospy) in a Python way such as a virtualenv&pip", "easily integrate ROS with an existing Python project"], "url": "https://discourse.ros.org/t/experimental-python-package-index-for-ros/10366"},
{"title": "Qt rosbag exporter", "thread_contents": ["We have open sourced a Qt GUI that allows users to export data topics (", ", ", ", ", " etc.) to CSV file and videos (", " from bag files very easily.", "The software is user oriented (non programmer) so it is very easy to use and features a desktop launcher when installed:", "Main features are:", "The project is available at ", " under a BSD-3-Clause-Attribution license", "Feel free to use it and create merge request / issues!", "That is great! Thank you!", "Awesome! Great work!", "Thanks for announcing this. I think many people will find it useful.", "For better community visibility I\u2019d recommend adding it to be indexed in one or more rosdistros. Tutorial ", " or if you\u2019re up for it a ", " If you do the full release bloom will prompt you to add source and documentation elements which will mean you don\u2019t need to manually add it to the index as in the first link.", "Once it\u2019s indexed you can create a wiki page for it at: ", " and the header will be auto populated with the metadata.", "With it indexed and having a wiki page many more people are likely to find it and use it.", "It would be really useful if this could handle complex types.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Export topics to CSV files (multiple topics inside one CSV file is not implemented, each topic goes into one CSV file)", "Export H264 encoded videos from ", "\n", "Progress bar to display the progress of the export", "Ability to export only a portion of a bag file (eg: from second 25 to second 60)", "Ability to change the video frame rate when exporting the video", "Ability to cancel exporting video/CSV"], "url": "https://discourse.ros.org/t/qt-rosbag-exporter/4964"},
{"title": "New PCA9685 driver for ROS", "thread_contents": ["I wrote a driver for the PCA9685 I2C PWM driver chip. This chip is intended to be used with LED applications but is also used in a number of servo and motor controller boards, including a few Adafruit and Waveshare boards among others.", "A couple of notable features I added that I haven\u2019t seen done in other similar packages:", "Contribute to dheera/ros-pwm-pca9685 development by creating an account on GitHub.", "Hi dheera,", "\nThanks for sharing your package. I am new to ROS so I\u2019m still confusing after reading your readme file. You\u2019re using Int32MultiArray as the message type, but in your description, the subscriber take 16 values for each channel. How are those two related. If I want to write send command to this node, how do I define the values of label, size, stride, data_offset, and data respectively to specify the channel?", "\nThanks much,", "\nZhouqiao", "Hi Zhouqiao,", "\nThe code currently only reads the data field and expects 16 values. It doesn\u2019t actually read label/stride/data_offset, so it\u2019ll work without setting those, but it\u2019s probably most \u201ccorrect\u201d to just define it as a one-dimensional array of 16 values. i.e.", "dim[0].label = \u201ccommand\u201d", "\ndim[0].size = 16", "\ndim[0].stride = 16", "Thanks, works perfect.", "Hi Dheera,", "\nI have not been able to get this to work.", "\nI have the Adafruit servo board working outside of ROS but I need to make it work within ROS as this project does.", "\nI am fairly new to ROS but I do have several nodes working.", "\nI work in python.", "\nIs there something particular to get a cpp script working?", "The approach I took was to download your project in a catkin workplace and compile.", "If you could guide me on how to install your files in an existing project, that would be awsome.", "\nThanks.", "Marc Boudreau via ROS ", " \u65bc 2019\u5e7412\u670819\u65e5 \u9031\u56db \u4e0a\u53488:42\u5beb\u9053\uff1a", "Hi,", "Thanks for replying, I appreciate it", "Like I said, I am new to cpp nodes.  I did not realize that you do not place the file extension \u201c.cpp\u201d as you need to place the \u201c.py\u201d when using python.", "I now have the node running!", "However, I have yet to figure out the ", " nomenclature.", "I have tried variations of:", "rostopic pub Int32MultiArray/command  {data:[32767,32767,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]}", "but always get errors:", "Usage: rostopic pub /topic type [args\u2026]", "rostopic: error: no such option: -]", "Le jeu. 19 d\u00e9c. 2019 \u00e0 17:08, Dheera Venkatraman via ROS ", " a \u00e9crit :", "it works!", "\nThis makes me happy!", "Thanks for sharing your work!", "Le jeu. 19 d\u00e9c. 2019 \u00e0 17:08, Dheera Venkatraman via ROS ", " a \u00e9crit :", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The ability to update only certain channels in a ROS command, and not update all of them. This allows multiple ROS nodes to publish to the same command topic but take charge of different subsets of the 16 channels.", "A timeout. If a channel isn\u2019t updated within that time, it gets set to 0. This is useful to avoid servos burning out if your control logic crashes, and also as a heartbeat timeout in the case of motor controllers.", "Did you get a compile or run error, and what is the error?", "What hardware do you have the servo board connected to?", "Are you able to see the servo board using i2cdetect?"], "url": "https://discourse.ros.org/t/new-pca9685-driver-for-ros/8299"},
{"title": "ROS Mapping and Navigation Robot Build with Nvidia Jetson Nano", "thread_contents": ["Hello Everyone\u2026", "Recently I started learning ROS and I was able to do a differential drive robot build with 2D Mapping with Gmapping and 3D mapping with RTAB-Map with Microsoft Kinect v1 with Nvidia Jetson Nano. So I\u2019m here to share my experience with you\u2026 I posted a video on youtube of the build i did. I named it NavBot.", "Check this out\u2026", "\n", "So i was able perform SLAM with Gmapping and i had to build gmapping from the source as it is not updated for ROS melodic. I added gmapping source package files into my project git so it will be easy to use. I used Differential drive controller from ROS Control to get odometry.", "Here is the Link to project GitHub repository", "\n", "Indoor Mapping and Navigation Robot Build with ROS and Nvidia Jetson Nano - imeshsps/ros-navbot", "\n", "If i visualize 3D point Clouds in Nano, It seems to get stuck so i did visualizations in my laptop connected to ROS master running on the Nano. Other than that everything worked fine.", "So if you are doing a similar build, I think this will help and if you having problems related to this, Please comment it here i\u2019m really happy to help.", "Thanks.", "Hey! Could you guide me on how to configure it? Thanks!", "Help me a little bip here!", "Nano can be used without external PC for making the map?", "On Raspberry PI only getting the rosbag is really doable", "Hello, what driver you used for the kinect? in my case openni for melodic didnt produce managable point clouds and after searching i found that openni in melodic doesnt work with arm processors,", "\nfreenect didnt compile in melodic", "\nso i used docker and it work\u2026", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-mapping-and-navigation-robot-build-with-nvidia-jetson-nano/9680"},
{"title": "Open Mobile Manipulator Project ", "thread_contents": ["Hello!!", "I would like to share my Graduation project", "It is a cheap educational mobile manipulator, like a diy approach to a turtlebot", "\ncosting approximately 600-700 but that depends", "\nIt can be helpfull to teach amcl, move_base,  rtabmap, moveit, hardware interface\u2026", "It consists of", "Jetson Nano (100) - TX2 (600)", "\n4x Geared DC Motors with encoders (4x30 = 120)", "\narduino due(30) (with rosserial for pid speed and publish encoder ticks, it also reads the encoders directly)", "\n1 ydlidar X4 (100-300)", "\n1 kinect v1(50) - realsense(200)", "\n3 dc-dc buck converters (30)", "\n2 L298N(15) - sabertooth (60)", "\n1 lipo 5000 mah (50)", "\n1 I2c servo driver (20)", "\n1 diy robotic arm (200) no feedback - with feedback (800\u2026)", "\n1 robot base could be just wood with metal rods to make 2 levels and motor brackets", "youtube video", "\n", "A cheap educational DIY mobile manipulator - ROS , with instructions on how to build it, starting code and demos - panagelak/Open_Mobile_Manipulator", "Any advice much appreciated!!", "\nThanks", "another video ", "\nReal Time Appearance Mapping (RTABMap) and Localization with Robot KidNap (3D features)", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/open-mobile-manipulator-project/12062"},
{"title": "Sphero RVR Needs ROS Interface", "thread_contents": [" has a new robot the RVR. This is a serious, robust, and capable small tank robot for a good price.", "Creating the ROS interface would be a good undergrad or advanced high school project. I\u2019d pursue it myself but have other activities over the next months that limit the time I can spend on it. Plus, bluntly, I don\u2019t know enough about ROS internals, working with URDF, etc to address this.", "Here\u2019s why I think the RVR should receive attention.", "It provides a serial port for control by another controller mounted on the chassis. The serial API provides access to control the motors and read gyro, accel, imu, speed, velocity, and location. Sphero is saying a 1st quarter 2020 release will provide access to the magnetometer and other drive functions. The API also accesses power and drive status, and controls LEDs. The Sphero link above provides access to the documentation on the API.", "Sphero has Python code that runs on the Raspberry Pi. I\u2019ve reverse engineered the protocol for C++. I\u2019ve run my code on an Up Board which is a quad core 1.7 Ghz SBC mounted on the RVR. My code is at ", ".", "If someone is interested I can be of some assistance.", "What do we need to do?", "\nRight now I am using roslibpy to publish topics to rosbridge since RVR is in Python 3 and kinetic and Melodic for Python 2 for the PI images I am using.", "I\u2019m not sure what is needed which is why I posted the message. Maybe someone can suggest needs to be done? Some thoughts / questions;", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Need to be able to use Move Base.", "Is Differential Drive suitable? It needs URDF definitions from looking at the code.", "How to get from Twist to the specific motor commands?"], "url": "https://discourse.ros.org/t/sphero-rvr-needs-ros-interface/12163"},
{"title": "Weekend Hardware Happy Hour!", "thread_contents": ["We just wrapped up our first full week of the year and many of us are finally getting the chance to hack on the hardware that we picked up over the holidays. In this thread I would like to know what sort of hardware people received over the holidays, what they did with it (or plan to do with it), and how it all went. If the holidays meant you got a solid week to write code and tinker we would love to know what you made.", "I\u2019ll go first. Over the holiday I picked up a used Oculus Dev Kit 2. I want to see if I can get video from my TurtleBot to stream to the Oculus. My stretch goal is to see if I can get stereo pair of cameras that roughly match the stereo baseline of the human visual system. For the holidays I also received a vintage set of letter stamps that I want to use to bling out my TurtleBot with a brass name badge.", "I wrote some ROS2 drivers for the Ouster lidars! ", "Also bought an Anki Vector. Not really a project but also pictured. Wanted to use it for algorithm development. Ended up annoying me ", " much it stays powered off on my desk.", "Video showing basic setup (feel free to subscribe ", " I\u2019m posting a set of videos on depth cameras and CES soonish ).", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/weekend-hardware-happy-hour/12261"},
{"title": "ROS acceleration using Xilinx AI Edge computing", "thread_contents": ["Future Robots are smarter and capable of making decisions in real-time. Decision making requires the use of AI for performing classification and semantic segmentation. Such AI algorithms require the use of dedicated hardware for making decisions at the edge. This group is meant for discussing subjects related to the integration of the:", "The following slack workspace (", ") is available for online discussing.", "\nXilinx Edge AI framework,", "The Xilinx Edge AI Platform provides comprehensive tools and models which utilize unique deep compression and hardware-accelerated Deep Learning technology.", "The platform provides efficient, convenient and economical inference deployments for embedded-CPU-based FPGAs.", "The Xilinx AI team consists of renowned researchers and experienced professionals known for their pioneering work in the field of deep learning.", "I would tempted to encourage people start by looking at Ultra96.", "Thanks ", ". I have experience using the following boards:", "I would like to encourage you to share your experience using the Ultra96", "\nKind Regards", "\nPedro", "The Ultra96-V2 updates and refreshes the Ultra96 product that was released in 2018. Like Ultra96, the Ultra96-V2 is an Arm-based, Xilinx Zynq UltraScale+ \u2122 MPSoC development board based on the Linaro", "\n", "\n", "The Ultra96-V2 updates and refreshes the Ultra96 product that was released in 2018. Like Ultra96, the Ultra96-V2 is an Arm-based, Xilinx Zynq UltraScale+ \u2122 MPSoC development board based on the Linaro 96Boards Consumer Edition (CE) specification....", "\n", "\n", "Avnet has announced the new Ultra96 development board, compatible with the 96Boards Consumer Edition specification from Linaro. The Ultra96\u2026", "\n", "\n", "Ultra96\u2122 is an ARM-based, Xilinx Zynq UltraScale+\u2122 MPSoC development board based on the Linaro 96Boards specification. The 96Boards\u2019 specifications are open and define a standard board layout for development platforms that can be used by software...", "\n", "\n", "One of the best Edge AI platforms out there today following 96Boards specification.", "For full disclosure  - I am the author of 96Boards specification.", ",", "\nHave you come across any Ultra96 project where ROS is used? If so can you share it here?", "\u2026 I am also totally biased, as I created ", " platform.", "We wanted to use the EV version of the Zynq MPSoC to allow the \u201cEdge-AI\u201d to benefit from the integrated codecs, etc. + give more fabric and more memory. The VCS platform is also in Open Source, but has a much higher cost, intended for industrial application.", "Even Xilinx like Sundance\u2019s \u2018story\u2019 and efforts - see ", " - and", "The Ultra96 is a \u2018gift\u2019 from Farnell/Avnet to help R&D and I can only recommend buying one to learn about Xilinx Vitis and Edge-AI.", "i got some experience on ", ", including ROS1 to publish the sensor data.", "Hi ", ",", "\nCan I kindly ask you to share your experience here? Perhaps a simple tutorial.", "In my experience the xilinx workflow works really well, IF the model you are trying to deploy was built with the same version of caffe or tensorflow as your DECENT and DNNC. Trying to deploy pre-trained models you find online can be tricky and time-consuming (unless they come from the AI model zoo of course ", " then you are one step removed from a DPU kernel). Deploying your own models should be fine if you align your environments.", "I\u2019m also interested to see if and when pytorch support arrives. I\u2019m currently working with DNNDK on the ZCU102, maybe the newer VITIS AI (built on top of DNNDK) has support for more frameworks and easier integration of models built with/against different versions of tensorflow and caffe?", "Hi ", ",", "\nPlease join our growing community on slack ", ".", "\nLets make our life easier.", "\nBw,", "\nPedro", "we did internally put some login in PL for doing pre-process for imaging (not so much i can share), and the CPU receives that pre-processed data and publishes as ROS topic. this is long time ago, but if i remember correctly, i did use ROS kinetic.", "hope that helps you.", "Hi ", ",", "\nCan you please join our slack group ", "Hi ", ", thank you for the invite, however we got a ZCU102 loaner to do some testing for a project. We will be handing that back over soon. So I am not sure if Ill be handling any FPGAs for the foreseeable future. I am interested in the subject in general, but I dont think I\u2019ll be contributing much.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["ROS", "ROS2", "OpenCV4", "Librealsense2", "Xilinx DPU TRM"], "url": "https://discourse.ros.org/t/ros-acceleration-using-xilinx-ai-edge-computing/11995"},
{"title": "Quaternion and dual quaternion operations", "thread_contents": ["I developed a ", " and conversions from and to ROS Pose and Transform messages in Python. Dual quaternions are an alternative to homogeneous transformation matrices that provide many benefits such as compactness, a direct relation to screw parameters, easy normalization, and an extension to quaternion SLERP for translations + rotations. They\u2019re also mathematically pleasing ", ". I\u2019d love to hear if there\u2019s interest in getting this released.", "I\u2019m working on releasing this package to ", " with ", ". Since dual quaternions consist of two quaternions, the package relies on an underlying quaternion python package, which AFAICT doesn\u2019t exist in ", ". I was using ", " at first but that package will be hard to release into ", " because of its dependencies. I am considering releasing ", " (with some modifications) or a custom package. I wanted to hear from the community what they want to see in such a package or if another similar package is in the works. As it stands, I would be releasing 4 packages to keep the python code separated from the ROS code: ", ", ", ", ", ", and ", ".", "Just a heads-up, I almost only use C++ and I work mostly with ROS2, so I\u2019m not sure how a lot of stuff works in ROS1 and python, in case this is a very silly question", "Is there a plan to somehow integrate this with TF, or how and when would this be useful in a robot system? Sorry for my ignorance ", " I believe you if you tell me dual quaternions are awesome, but the only usage of quaternions I\u2019m aware of in ROS is \u201cbehind the scenes\u201d for TF.", "Good job either way ^^", "Is there a plan to somehow integrate this with TF, or how and when would this be useful in a robot system? Sorry for my ignorance ", " I believe you if you tell me dual quaternions are awesome, but the only usage of quaternions I\u2019m aware of in ROS is \u201cbehind the scenes\u201d for TF.", "TF is designed to hide the implementation so you can use any linear math library you want with tf. There are tutorials for adding support for both ", " and ", " Once you have the bridge to the linear math library that you want you can keep operating with that libraries primatives. Both Bullet and Eigen provide Quaternion classes. It looks like this can add quaternion support to numpy.", "I am planning on writing a C++ interface to this as well, very similar to how ", " 's ", " package is set up. The reason for starting with Python is that there aren\u2019t any stable dual quaternion packages out there unlike in C++ as many graphics folks use dual quaternions regularly.", "Your question \u2018why should we care\u2019 is super valid and the main reason I\u2019m promoting this package! The answer depends on your use case. When you\u2019re using transformations mostly to convert points from one frame to another and other basics, you\u2019re probably not going to be better off switching. If you\u2019re however writing a new algorithm that heavily relies on transformations (e.g. new Unscented Kalman Filter for SLAM, an optimization algorithm for sensor extrinsic calibration), dual quaternions might be interesting as they can greatly simplify the code and improve speed because their manifold fits the underlying problem well. Most optimization algorithms for minimizing some kind of transformation error have struggled with representations that split up translation and rotation (matrices, euler angles or quaternions + translation vector) because they require separate optimizations and additional work to tie those together. Since dual quaternions combine translation and rotation intuitively, you can do \u2018simultaneous optimization\u2019. To see how dual quaternions behave (take a step on the manifold), take a look at the visualization gif in the readme. Compare that with matrix interpolation or its Lie algebra (tangent at the identity) or with quaternion + translation interpolation where the interpolation is a straight line and rotation happens uniformly along that line.", "\nIf this sounds interesting, definitely take a look at the ", ". I\u2019m still working on my explanation. Let me know if you have questions or suggestions!", "TF is designed to hide the implementation so you can use any linear math library you want with tf. There are tutorials for adding support for both ", " and ", " Once you have the bridge to the linear math library that you want you can keep operating with that libraries primatives. Both Bullet and Eigen provide Quaternion classes. It looks like this can add quaternion support to numpy.", "Interesting, once dual quaternions is released it would be great to get those bridges in place with TF.", "\nRegarding the quaternion support to numpy, as I stated I\u2019m looking into switching to ", " as opposed to ", " since the latter requires ", " and ", ". However the former package doesn\u2019t integrate into numpy as tightly as ", " does. I prefer ", " but I don\u2019t know if and how I can release those requirements. Would love to hear your thoughts on this.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/quaternion-and-dual-quaternion-operations/12221"},
{"title": "Python script works locally but not through SSH", "thread_contents": ["Hello everyone!", "I\u2019m trying to link a Raspberry pi with pycroft with a Turtlebot. I have created a skill and I want that when I said \u201cgo to somewhere\u201d the turtlebot goes to this place.", "I have a python script and it works fine if firstly I connect via ssh and then I execute python go_to_specific_point_on_map.py", " python go_to_specific_point_on_map.py", "But if I tried to do all in one command, I get:", "\nImportError: No module named \u2018rospy\u2019", "$ ssh ", " python go_to_specific_point_on_map.py", "I think it is a problem due to .bashrc is not loaded and neither do the alias. I don\u2019t know exactily what the problem is and how to solve it.", "Can somebody help me? please.", "I don\u2019t know how to get it to work.", "Thank you very much and best regards", "Your title is not very descriptive of the project you are doing. (short) Problems that are ROS related are best asked on ROS answers. Problems that are not ROS related need to be asked somewhere differently all together.", "If you maybe rename the topic and give more details you can start a discussion in ROS projects and get people to contribute and help you with minor issues of your project (This is what ROS Discourse is best used for).", "Thank you very much and sorry. I\u2019m really new using ROS and my English is not too good. For both reason my question is too poor.", "I will try to explain better. I\u2019m the most interested in solving my problem.", "Thank you again", "Regards", "I\u2019m trying to link a Raspberry pi with pycroft with a Turtlebot via SSH. I have created a skill and I want that when I said \u201cgo to somewhere\u201d the turtlebot goes to this place.", "No problem with this part. The problem come here.", "I have a python script and it works fine if firstly I connect via ssh and then I execute python go_to_specific_point_on_map.py", "But if I tried to do all in one command, I get: ImportError: No module named \u2018rospy\u2019", "I have tried the arunp9294\u2019s solution but I get the same error. $ ssh ", " \u201csource ~/.bashrc; python go_to_specific_point_on_map.py\u201d", "The file go_to_specific_point_on_map.py is here: ", "I think it is a problem due to .bashrc is not loaded and the alias neither. I don\u2019t know exactily what the problem is and how to solve it.", "Can somebody help me? please.", "I don\u2019t know how to get it to work.", "Thank you very much and best regards", "Give a try to ", ".", "If that doesn\u2019t work, post your .bashrc here. Anyways, this is not the best way to use ROS. It\u2019d be better in most cases to install ROS also on your computer and send the command via ROS with ROS_MASTER_URI set to the turtlebot\u2026", "I have tried to ", "  but nothing happens.", "The screen appears waiting for ever. Here there is a capture of what is happening", "\n                        ", "\n\n", "I have tried to install ROS in the pycroft but I have had problems too. I have followed these steps.", "\n", "At the end of the process I get an error and nothing works. The error is:", "\n\u201c\u2018boost/tr1/unordered_set.hpp\u2019 file not found\u201d", "I though it would be easier to do what I want using SSH but I\u2019m having problems with all the options.", "Thank you again and regards", "Post the bashrc. It seems there has to be something rotten in there.", "Also, if it\u2019s not Raspberry Pi Zero or 1, you\u2019d better go with installing ROS via the package manager as mentioned in ", " .", "Thenk you again.", "The .bashrc file is this:", "\n", "\n", "I can try to install ROS again but I was afraid because I didn\u2019t want to break my mycroft installation.", "Thank you very much", "You\u2019re not sourcing ros anywhere in the .bashrc \u2026", "And don\u2019t fear installing ROS via package manager. It will tell you what\u2019s about to happen when you type ", " or so. If you see some packages are about to be deleted, you can always cancel the installation without any harm being done.", "In the future, please ask questions on ", " following our support guidelines: ", "ROS Discourse is for news and general interest discussions. ", " provides a Q&A site which can be filtered by tags to make sure the relevant people can find and/or answer the question, and not overload everyone with hundreds of posts.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/python-script-works-locally-but-not-through-ssh/12283"},
{"title": "RobotsForRobots: CES 2020 Robotics Trends and Review", "thread_contents": ["Hi,", "I travelled to CES this year and wanted to give you all an inside look at some of the amazing companies and trends in robotics this year. This video highlights a few companies I really like building the future of robotics technology and hardware. Let me know what you think, I\u2019d love any feedback I can get! If you liked it, please subscribe and share with your colleagues!", "I wouldn\u2019t normally shamelessly plug, but there\u2019s a few ROS Veterans featured including ", " and ", ".", "Ouster: ", "Apex.AI: ", "Samsung: ", "LG: ", "Ubiquity Robotics: ", "SLAMCore: ", "- Steve", "For someone who\u2019s never been to CES, that was pretty cool! I can see that the constant lights and noise might get overwhelming after a while, but I guess that\u2019s what LG\u2019s booth entrance is for. ", "Some constructive (I hope) criticism:", "LG\u2019s booth entrance", "I have to say, that was one of the highlights of the show for me. I didn\u2019t get to use even 10% of the footage of that that I took. Video doesn\u2019t do it justice.", "Thanks for the feedback! 100% agree on the me-on-camera stuff. I need to work on my \u201cstage presence\u201d.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["The Samsung segment sounded like a sales pitch, possibly due to too much marketing speak; perhaps your script just needs to be smoother?", "Look at the lens, not the view finder", "Smile!"], "url": "https://discourse.ros.org/t/robotsforrobots-ces-2020-robotics-trends-and-review/12364"},
{"title": "Code for building ROS nodes in Bazel", "thread_contents": ["I\u2019ve put together a demo project for building ROS (1) nodes with Bazel, Google\u2019s build system that\u2019s quite good for large monorepos of code. Many things are still missing, most notably integration with rospack and friends. However, it\u2019s a good starting point if you\u2019re tired of catkin clean.", "Build ROS code with Bazel. Contribute to nicolov/ros-bazel development by creating an account on GitHub.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/code-for-building-ros-nodes-in-bazel/12411"},
{"title": "Gundam Global Challenge - Research Open Simulator", "thread_contents": ["Here is an interesting ROS project related to Gundam.", "GUNDAM GLOBAL CHALLENGE", "\n", "\n", "\n", "ROS packages for GUNDAM robots. Contribute to gundam-global-challenge/gundam_robot development by creating an account on GitHub.", "\n", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gundam-global-challenge-research-open-simulator/12428"},
{"title": "ROS package that publishes the MPU-9255 data into a Topic used in Raspberry PI 3", "thread_contents": ["I\u2019ve been working with a raspberry pi 3 and with an IMU MPU-9255. It works with ROS kinetic.", "\nThe package allows to publish the data of the IMU, uses complementary filter to reduce noise and allows to store the data in a .bag file.", "\nNow I am working to integrate the offset data of the sensor before publishing on the topic and then get the orientation angles.", "ROS MPU9255 Node", "\nC++ ROS node wrapper for the mpu9255 gyroscope / accelerometer/magnetometer.", "\nReads and  publish accelerometer, gyroscopic and magnetic data.", "More information and suggestions:  ", "Is this appicable for the MPU6050?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-package-that-publishes-the-mpu-9255-data-into-a-topic-used-in-raspberry-pi-3/5536"},
{"title": "New BNO055 I2C driver for ROS", "thread_contents": ["The Bosch BNO055 is an excellent IMU at its price point and extremely common among hobbyists and industry alike. Many are using it in Jetson TX1/TX2/Xavier boards over I2C, as well as other boards that have exposed I2C ports.", "I found a RS232 driver for ROS, as well as an I2C driver that depended on RTIMULib, but that project looks seemingly defunct and cumbersome as an install requirement.", "So for all of you BNO055 fans out there I wrote a ROS driver that doesn\u2019t depend on anything except libic2-dev. You might need to give permissions to the ROS user by adding it to the i2c group as well.", "BNO055 I2C driver for ROS. Contribute to dheera/ros-imu-bno055-i2c development by creating an account on GitHub.", "I\u2019ve tested this on a TX2. It should work out of the box on other Jetson boards. I haven\u2019t yet tested it on a Raspberry Pi but you\u2019ll need to slow down the I2C clock to use it with a BNO055 as the Pi doesn\u2019t support clock stretching.", "The calibration service isn\u2019t implemented yet. I welcome pull requests or contributions.", "NIce! do you think this will work natively with the Jetson Nano as well? Or will it has the same clock speed issues as a Raspberry Pi?", "Thanks!", "If you refer to the ", ", that is probably unique to the rpi.", "I can confirm that the Jetson Nano does ", " have the clock stretching bug.", "(I can also confirm that this driver works on a Raspberry Pi with ", " i2c, although it will consume the majority of one core doing so. If you don\u2019t need data at 100Hz, you can reduce that to cut down on CPU usage. This is not an issue with Jetsons or other platforms that support the full I2C spec including clock stretching.)", "Does the Raspberry Pi 4 suffer the same clock stretching bug? Or did they fix it in the new silicon?", "The rpi4 has another core so it ", ".", "But I guess ", " can better answer that question.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-bno055-i2c-driver-for-ros/7940"},
{"title": "MOCAP4ROS2: Motion Capture Systems in ROS2", "thread_contents": ["Dear ROS community,", " is a Focused Technical Project (FTP) funded by ", " and coordinated with ", ". The goal is to standardize the integration of different motion capture systems in ROS2.", "I want to open a discussion with the members of the ROS community and companies interested in motion capture systems. We want to receive feedback on the proposed design and open a  discussion about messages, formats, and processes. We are currently doing some experiments to decide the adequate QoS of the communication between nodes.", "We will support ", " and ", ", that will be used in Eurobench. Our design lets to replace any MOCAP system (for example, to use ", " instead of Vicon), or to incorporate new ones. The next figure shows the current design:", "We want that this project will be useful to the ROS community beyond the funding projects, and this is the reason why we would like to incorporate this discussion to everyone interested in using or contributing to this project.", "Best regards", "\nFrancisco", "To continue with the discussion, ", " are the messages that we are using:", "Markers.msg", "Marker.msg (currently it has more fields, but it is dependent on Vicon, so we are removing it)", "For ", ", we will use ", ", but in the Composer Layer, we have to provide a config file (.yaml) with the distance between sensors.", "Hi ", ", looks like a great initiative!", "As a point of reference, some time ago I put together ", ", which had a similar goal of bridging MOCAP into ROS in a vendor-neutral way.", "Being not particularly interested in writing vendor-specific clients, I used ", " as an interface - that project is not particularly active anymore, it may be useful. Certainly there are a lot of hoops to jump through to link against VICON SDK blobs in an otherwise open-source project. Kudos to VICON engineers for adding velocity and acceleration output to their VRPN server implementation on request.", "My other main take away was - consider your interfaces carefully. Some consumers may need /tf*, some may want to subscribe information regarding just one object and would prefer you didn\u2019t pollute the /tf tree of all connected nodes (since /tf can be quite chatty).", "Hello ", ", this is pretty cool work. Regarding ", ", it seems like some of the fields you removed as being specific for Vicon are not that uncommon for other mocap systems, like the Qualisys system we are using. Maybe it would be better to keep those fields (e.g. ", " and ", ") and just keep them blank for systems which do not support that specific feature? Probably we would need to use an integer as datatype for ", " as it should be nullable\u2026", "Also for our purposes it would be useful to include the marker id in the message.", "The Qualisys SDK also supports integrating force plates, which might be useful for some users.", "Thanks for the feedback", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", ": Nodes of this layer are dependent on each MOCAP System vendor. All vision-based nodes would publish messages in the same format. The same for IMU-based systems, or any other type of MOACP.", "\n", ": Nodes of this layer are independent of a particular MOCAP System. The output of these nodes are TFs, and maybe some metadata.", "\n", ": This layer contains applications that use the information from MOCAP Systems. In the case of Eurobench, there will be a skeleton composer and other performance measurement components."], "url": "https://discourse.ros.org/t/mocap4ros2-motion-capture-systems-in-ros2/12308"},
{"title": "\"Deterministic\" navigation in ROS", "thread_contents": ["We are working on a research project where we are investigating the ROS Navigation Stack in an industrial setting (", "). The partners in the project want autonomy to a certain level. For most areas in a plant, the behaviour of the robot must be predictable.", "We have been looking into commercial solutions such as Navitec and Bluebotics and they have graphical tools to guide the navigation behaviour. In certain areas, the robot is free to navigate. In other areas, the robot is only allowed to follow a virtual line. The tools are some sort of vector-drawing tools, where you can draw routes and areas on top of a map. The planners use this additional information to come up with appropriate paths.", "We are wondering if there are open source tools that can do these kind of things. We did not find them yet. If they are not there, it might be interesting to develop such a tool in the ROS ecosystem.", "It\u2019s very early days still, but we just opened up our in-progress \u201ctraffic editor\u201d for this type of thing. Documentation is currently non-existent, publicly-viewable examples are not there yet, etc., but it\u2019s coming along.", "You can specify a floorplan image, draw lanes on it, trace the walls if you want, and then export the path data to YAML and/or a simulation model for Gazebo. The GUI is built using QtWidgets in C++ and saves all the data to YAML. The exporters are Python scripts.", "Again, it\u2019s just a work-in-progress in a public repo, not a polished product. But I think this style of robot operations is becoming a common use case in many domains, so hopefully the editor can become useful.", "Contribute to osrf/traffic-editor development by creating an account on GitHub.", "This is just an editor for these paths; it doesn\u2019t touch the problem of actually following them with real robots.", "Cheers", "Thank you. Your description is close to what I was looking for, so I will definitely have a closer look at your repository. We want to plan our routes on top of the maps generated by Gmapping or Carthographer. But essentially, those are also just bitmaps. We want the following flow: the robot drives around and builds a map, then the operator draws routes/paths, restricted areas, and free-to-navigate areas, speed zones, etc. on top of that map, and finally, the robot uses that data as input to the planner.", "I think it is logical to separate the editor from the functionality of the planner. The interesting part is to have a common data format for the routes in YAML/XML/etc. Do you use a standard for storing the routes/paths? Currently, we have a group of students working on a planner that follows predefined paths.", "Ps. I tried to build your code, but I got some errors. I will have a look at it tomorrow.", "Currently the annotations are just stored in a YAML format of our own dreaming. There are Python \u201cgenerators\u201d in the repo which process that YAML into other \u201coutput formats\u201d such as Gazebo worlds (XML) or a \u201csimpler\u201d YAML format that is just the navigation data (lanes, etc.), intended to be consumed by nav stacks. We can create generators for any other formats or navstacks; I wasn\u2019t aware of standards for this, but if there are, we can certainly convert the data to whatever format is desired. It\u2019s just Python ", "I added a GitHub Action to the repo now which does an automatic build on Ubuntu 18.04 every code commit, so if the badge is green on the repo, it should build (at least on Ubuntu 18.04). Please create an issue ticket if you\u2019re seeing any build errors with details about your platform. Cheers!", "The build errors were my fault\u2026 I was first trying it out on an Ubuntu 16.04 machine. On 18.04, the project builds without problems.", "This application is really similar to what we have in mind. We are now playing around with it. The application really helps to discuss the requirements with colleagues and we are sharpening our user requirements. I will get in touch for next steps. I think it would be nice to join efforts to further develop the tool.", "Web-based GUI plz.", "Preferably as composable widgets.", "(It would make it easier to integrate the tool into application-specific UIs.)", "Yeah, maybe rev 2 ", "Currently we\u2019re running this tool \u201coffline\u201d to create map files and export various products (simulation models, navstack configs, etc.) in the local filesystem, so it would seem that a web-based approach would make things a fair bit more complex. But maybe that\u2019s just because I\u2019m a dinosaur.", "There is probably room for both \u201coffline\u201d and \u201conline\u201d editors in the ecosystem eventually.", "But maybe that\u2019s just because I\u2019m a dinosaur.", "You, like me, are a dinosaur.", "All the young mammals (including at my company) are running around building beautiful-looking UIs that run locally but are accessed through a web browser.", "I don\u2019t pretend to know how it works (OK, I do a little bit but just because I\u2019m curious), but I do know that they seem to whip up new UIs in a matter of hours using nothing but libraries with weird names they found on GitHub.", "I certainly agree that there is room for both. I\u2019m more interested in the (system) interfaces side\u2026 the data formats taken in and spat out, any online communication channels, etc.", "I am also ok with an off-line tool and personally I have most experience in that direction (Qt and Java). However, I agree that it would be nice to have web-based tools to edit the maps. I saw a demo of the MiR 100 robot and it looked like they already had a web-based UI for editing the map and for fleet management. However, when I look at their website I can\u2019t find anything about it.", "We also came up with two other features that might be interesting for this tool. One is support for .pgm files, because that is the format that ROS uses to store maps. The other is to support curves, because our robots need to have smooth trajectories and no sharp turns. This could also be solved at the planner level, but I think it is better to do that in the editor. I did some experiments in Java with curves and used 4 vertices to define bezier curves.", "Would it be interesting if I integrate the curves in your traffic-editor?", "I have written this very web-based application twice now for different companies, but both in proprietary formats.", "\nWhat you\u2019re looking for, if you don\u2019t want to write your own custom UI/system, is QGIS (", "). At its core, what you\u2019re looking to do is pretty standard GIS stuff. QGIS is designed for these kinds of spatial data workflows. It contains everything you need to digitize, manage, analyze, and ultimately serialize (eg. into GeoJSON) your robot \u201ctraffic plan\u201d.", "Also be aware of QGIS-ROS, which I wrote to help bridge QGIS into the ROS world (", ") ROSCon presentation linked in the readme.", "I strongly encourage trying to utilize available open source GIS tooling (that\u2019s been in development for decades) before deciding to make a ROS-specific flavouring of a subset of these tools.  These aren\u2019t novel spatial data authoring problems we\u2019re trying to solve.", "Thanks ", " ! Yes certainly, adding curves/splines would be great, since they are actually physically realizable by robots. We started off with straight-line segments just because they are super easy and because (in my limited experience) it seems that is what many/most companies are currently doing in their proprietary editors anyway.", "Thanks ", " for the feedback. Indeed, it seems that every robot company has their own internal proprietary editor. Thank you for the pointer to QGIS; it\u2019s an interesting idea to use a GIS system for this. I guess I had been stuck in a mental rut that \u201cGIS is for outdoor large-scale maps expressed in lat/lon,\u201d but I see the overlap with indoor mapping now. GIS seems particularly relevant for indoor+outdoor robot operations (deliveries to loading docks, etc.). I guess the purely-indoor, large-building domain still feels \u201ca bit different\u201d to me, in that multi-level buildings are so much more constrained than entire city maps, so an \u201cintentionally limited\u201d UI subset of something like QGIS might make the tool easier to use and the workflows more straightforward. The input (in my limited experience) is often a pile of PDF floorplans provided by the building operator, rather than satellite imagery or other traditional GIS data. But I\u2019ll definitely dive deeper into the GeoJSON RFC and QGIS to challenge those assumptions. Thanks for the pointers!", "Perhaps there is enough interest in this area to create a new ROS Discourse category called \u201cMulti-Robot Operations\u201d or something like that. It wouldn\u2019t necessarily be locked to a particular robot software platform (ROS1 / ROS2 / various other options) but instead about creating higher-level tools to deal with multiple robots sharing the same space, no matter what software is running on the robots themselves. I\u2019ll create a category proposal now and anyone interested can help evolve the definition and direction. Cheers!", "If you\u2019re going to consider GeoJSON as an option for interchange, just know that you can completely ignore long,lat in the spec and just pretend it\u2019s x,y. Been doing this for many projects well over a decade and can say it works fine, assuming you\u2019re utilizing tools that don\u2019t just assume a CRS like WGS84.  There\u2019s hundreds of geodata formats so pick what makes sense, but resist making your own. GeoJSON is great because of the countless tools and libraries already available for it.", "For QGIS all you need to do is set a custom CRS to a planar Cartesian system (just set everything to zeroes). Everything else just works including the extensive suite of raster and vector tools (see my ROSCon talk for examples of QGIS showing an indoor facility with robots in real-time).", "Orthorectified imagery or PDF floorplans are basically the same thing when you think about it. Set control points from known fiducials and begin drawing. Thinking all the way back to school, we\u2019d just rasterize the PDF and run it through QGIS akin to how one would an unrectified satellite/aerial image.Though often a SLAM map is used as the \u201cground reference\u201d and you draw on your vectors relative to it.", "Yes, there is a lot to be said about a limited UI, which is probably why I keep end up making them. Depends what your goals, timelines, etc. are.  QGIS may fill a gap shorter or longer term, and it\u2019s always a phenomenal analysis and data processing tool.", "Feel free to email me with any questions you\u2019ve got with getting started.", "Thanks! Ticket added: ", "The QGIS approach is definitely interesting. I think it is better to use existing tools if they are available. I will have a look at the software to see if it is possible to use in our projects.", "Currently, we are aiming at ", " as the fleet/traffic manager, because it is open source and one of our partners already has experience with it. However, OpenTCS manages on an very abstract level. It uses a graph of the environment and there is no direct connection between the information in OpenTCS and, for example, a planner in the ROS Navigation Stack.", "Our current approach is to have one data set with maps based on sensor data (from GMapping or Carthographer) with an overlay of the paths and areas where the robot can navigate. From that data set  we can export graph data to OpenTCS and use the whole structure in ROS Navigation.", "It\u2019s been over 8 years since I was involved in anything that used GIS, but I do recall that back then the GIS community was making a ", " big push into indoor GIS. A quick Google search turned up plenty of results so I\u2019m guessing the community didn\u2019t give up. I can\u2019t remember details, unfortunately, but there were open format specifications for buildings and built spaces and things like that.", "I dived into the QGIS software and it really has a steep learning curve. However, based on my experiences it definitely has the functionality to draw vectors on top of raster images. That was what I was looking for. I still have to look into the interoperability between QGIS projects and ROS. Thank you ", " for the pointer to QGIS-ROS.", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "I am still interested in your view on the last part of my previous post. We are looking into data exchange between different navigation systems. Specifically, we want to use OpenTCS with our our robots that are running ROS. The standard is really targetted at this topic. However, the standard is not open source and I can\u2019t find fleetmanagers implementing the standard.", "I found a question on this topic on answers: ", " They have a similar conclusion: not open source and no implementations.", "In our project we need functionality to exchange map data. So, we can implement it according to a standard or we can build our own custom solution. At the moment, I don\u2019t know the best solution yet.", "Word of warning, below is a textwall that may be interesting to some, but may not answer the posters question. Read at your own peril.", "I did some research as part off my Master thesis where I created a map using JOSM (java editor for OpenStreetMap) using indoor mapping plugins and then hosted the database (map nodes and relations) locally (could also be hosted online). A simple python node can then be used to query this database using overpass API. I put in information like different floors, hallways, door colours, which sensors could be used in which area. It was all kind of experimental, but technically possible. It gave me a multi-layered map containing the low level x-,y-, occupancy information, a low level topological map, high level topological map all the way up to a high-level semantic map. I never got to the point of actually using the map for navigation though ", " (the mapping effort was luckily enough to graduate on).", "The advantage of creating such a map is that the robot does not have to think a lot. All of the information about traffic rules and which methods of localization and navigation to use in which area are all embedded in the map. The obvious disadvantage is the insane mapping effort required to get such a map. Additionally there is no standard way to this that I am aware of. I came up with my own model, my own hierarchy and own key-value combinations. I just wanted to share my experience and point out that OSM can be used for indoor mapping as well and that OSM has a pretty big open source community so lots of tools and plugins are available.", "Another student from Germany was working on the same project at the time.", "\nThis could be an interesting read for you (summary of a couple of slides):", "\n", "I have also been looking into standard map data formats. I encountered the IEEE Standard for Robot Map Data Representation for Navigation. Based on the name, it seems like the thing I was looking for. However, I am not sure whether to use such standards, because they are closed source and we work in an open source projects.", "There\u2019s nothing to stop you using a closed standard to design open-source software, except in rare situations (such as the AUTOSAR specifications) where the license explicitly forbids it.", "The catch is that only people who have access to the standard will understand your design decisions.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/deterministic-navigation-in-ros/11442"},
{"title": "ROS-A Focused Initiative for Agriculture", "thread_contents": ["Greetings,", "ROS-A is a focused initiative for using ROS in Agriculture.", "The goal is to make close loop autonomous farming a reality.", "We are working on data collection and agricultural data standards.  If you would like to participate in the project we will have a breakout session at ROSCon 2017 to discuss the ROS-A standards.", "Follow us online Twitter ", " or signup to join the conversation at ", ".", "Thank you for supporting open source robotics,", "Matt Droter", "\nFounder", "Tel: 866 565-3025", "\n", "\n", "Hi. I\u2019ve looked at the site and am interested, but am wondering what I\u2019m \u2018signing up\u2019 for. What sort of platform do you use to host the discussion? A forum?", "Additionally: are you primarily focusing on the US, or are users from other countries also welcome?", "Hi,", "Right now we are setup on Slack at ", "US based but this is a global challenge.  Everyone is welcome to participate.", "If you are interested in starting a ROS-A meetup in your location contact us.", "Matt", "I\u2019ve done a lot of work in greenhouse automation and control. As a part of this I\u2019ve done automatic irrigation based on sensor data, HVAC control, supplemental lighting and other things (control of fan rates and pumps).  Is there work being done yet on this front within ROS-A?  If not, is there interest in it under the ROS-A project?", "Kevron,", "There is interest in your project. Can you and I get in touch with each other directly, to discuss possibilities?", "Bill Mania", "Sure.  Can you PM me and we can discuss how to discuss more?", "Hi ", ",", "\nThis is a very interesting initiative. I would like to join and contribute to the project. I am currently involved in a reasonable amount of farming/forestry projects.", "A new SIG datasets group is now active and WE NEED YOU. Please join us on our ROS-A slack -> ", ". Start here ", "I would love to lurk and potentially help out on this effort. I have some experience putting together large forestry data sets and I would love to figure out how to dovetail this effort with the perception working group.", "AFAICT I need someone to invite me to the slack group.", "If our proposal for a new category is accepted, we will keep track of these initiatives in ", " .", "I\u2019ve just posted: ", ".", " ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/ros-a-focused-initiative-for-agriculture/2563"},
{"title": "About the ROS Projects category", "thread_contents": ["ROS Projects - a place for the community to discuss any projects they are (or would like to be) working on, from the smallest of tweaks to the largest of ventures. Somewhere we can come together to get help and feedback; give advice and opinions; publicize and discover; contribute to and learn from the various ROS-related projects that we are all working on.", "This is as proposed ", " If you have feedback or suggestions please follow up in that thread.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/about-the-ros-projects-category/175"}
]