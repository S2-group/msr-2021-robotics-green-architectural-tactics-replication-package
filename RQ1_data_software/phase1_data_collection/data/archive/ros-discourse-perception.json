[
{"title": "Proposal - New Computer Vision Message Standards", "thread_contents": ["Hello computer vision users,", "Please help us define a new set of computer vision specific ROS messages by reviewing the ", " and providing your feedback, either here or on the repository.", "At OSRF, we are in the process of defining a new standard set of ROS messages for the computer vision community, and we\u2019d like your help. This need was identified from our computer vision ", " as a first step towards improving the ROS computer vision ecosystem, so thank you for the feedback!", "The end result of this effort may be a new message package in ", ", a REP, or both. Our goal is to capture as many common computer vision use cases as possible, with the exception of navigation. (We feel that navigation and localization are already well-defined by the community and ", ".) Object recognition and image classification are two primary targets we are hoping to hit, and we want to cover both 2D and 3D use cases.", "The ", " we have created is very much a work in progress, and only with your feedback can we make it better. Any feedback is welcome, bu here are a couple of questions I have identified:", "Thanks!", "Nice effort! What about also define interfaces for annotations?", "We also try to define \u201cgeneric\u201d interfaces:", "image_recognition - Packages for image recognition - Robocup TU/e Robotics", "However, it\u2019s hard to capture things as:", "I think you would also like to define something as detection or feature groups that belong together. Maybe take a look at how ", " does it.", "Currently, what is the difference in roles between the two poses in ", " vs ", " nested inside it?:", "As in, what is the relationship that would afford the use the nested ", " to convey the detection\u2019s frame_id?", "I suppose this is a larger question of semantics or dichotomy, but perhap I\u2019m of the thought that classifications derive from detections, such as ROI\u2019s, as opposed to viersa. In whichever case, I think the relationship should be made clarified if we are starting to nest standard message types.", "To just throw this out here, I\u2019ve been using SPENCER recently, and I\u2019m beginning to really appreciate the message type layout they\u2019ve used. Perhaps we could take some hints from the project:", "\n", "spencer_people_tracking - Multi-modal ROS-based people detection and tracking framework for mobile robots developed within the context of the EU FP7 project SPENCER.", "\n", "Hi,", "\nwhen talking computer vision message standards one thing comes to mind, features.", "\nIt is not unusual to have different nodes exploiting the same kind of features (think SIFT/SURF etc), so that rather than extracting several time the same features, a single \u2018extraction\u2019 node does the job and publishes them. They are in turns exploited by (several?) others. A standard message may not be straightforward and I\u2019m not sure this problem fits the scope of this proposal, however it certainly would be useful.", "Cheers.", "I think features should be regarded as an implementation detail of the classifier/detector and should therefore specifically not exposed in these messages.", "I really like this proposal, there should be a standard for this in my opinion.", "In detail, what I like more about Reinzor\u2019s message definitions, is that there is a shared ", ", which takes the role of the ids+scores combination in ", "I think using the CategorialDistribution reduces a little bit on complexity on the user end, but the difference is very small. Another benefit is that the message definition is reusable.", "This topic\u2019s header is \u2018Computer Vision Message Standards\u2019 however the discussion seems to focus on classification. Did I misunderstood the point or is the title not appropriated ?", "It\u2019s not clear to me if the proposal supports per-pixel segmentations. There is the ", " field of ", " and ", "  which might be used for segmentations, but from the documentation I\u2019m not entirely sure if it is also meant for segmentations or not. The name ", " is a bit confusing to me.", "There might also be other types of detection besides bounding boxes and segmentation that I\u2019m not currently thinking of, though the two seem like a pretty solid base for now.", "Why XML? Why not YAML or JSON, or completely implementation defined? Or what about the name of a tree of parameters on the ROS parameter server?", "It indeed says computer vision and not classification/detection only. My bad.", "\nWhat kind of messages would be needed for tasks beyond those 2?", "No problem ", ", it\u2019s just that I have a specific use case in mind. It is as follows:", "\nLocal features (a point and a descriptor -> SIFT/SURF etc) are one of the basic component of CP and is used for geometry algos as well as for appearance-based algos. In feature-based Visual-SLAM (e.g. ORB-SLAM) you rely on feature both for the poses estimation (geometry) and place recognition (appearance). Those two tasks can be executed in parallel threads. Assuming you are using the same features for both tasks one could communicate a ", " (or such) to the other.", "\nIt is something (a ", ") I have been hackily doing here and there, feeding different classifiers - different processes for that matter.", "\nI am just wondering here if a standardized way of moving such objects around would not make sense ?", "ps : To be fair local features are also used from other sensor readings (e.g. laser scan, point cloud) so my question my be a little out of the scope of this thread.", "Thanks for the awesome feedback, everyone! I\u2019ll try to address everything that was brought up.", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general ", " package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific ", " or similar.", ", thanks for linking to your message definitions! I think that annotations are already covered under the existing implementation. You could provide the bounding box coordinates in a ", " message, and the most likely label as the only result in the class probabilities. If we want to add other information, such as color of the outline, etc. then maybe this would be a better fit for ", " or another package.", "On another note, is human pose estimation standardized enough to make a custom message type for it? Or is it best described by a TF tree, arbitrary set of ", ", or some other existing ROS construct? I\u2019m thinking of the fact that different human detectors provide different levels of fidelity, so it might be difficult to standardize.", ", My idea with having two poses is that the bounding box could actually have a different pose from the expressed object pose. For example, the bounding box center for a coffee mug might have some z-height and be off-center wrt the body of the mug, but the expressed object pose might be centered on the cylindrical portion of the mug and be at the bottom. However, maybe it makes sense to forego the bounding box information, as this could be stored in the object metadata, along with a mesh, etc.", "On the topic of nesting, I\u2019m open to the idea of flattening the hierarchy and having Classification/Detection 2D/3D all include a  new ", " message. I\u2019m not sure how much message nesting is considered standard practice, so I\u2019ll look at some other packages to get an idea.", ", I like the idea to add a standardized ", " or other similar message, as long as there is some common baseline that can cover a lot of feature types. From my own understanding of visual features, there\u2019s usually a lot of variation in how the feature is actually defined and represented, so I\u2019m not able to find a \u201clowest common denominator\u201d from my own experience. If you feel there\u2019s something there that could be broadly useful, please feel free to post it here or make a pull request. I agree with ", " as well, although many classifiers use features internally, this should be hidden in the implementation except in special cases like the SLAM case described.", "I didn\u2019t design the current messages to support per-pixel segmentation, and I\u2019ll have to look into how that is usually represented to get a good idea of how to craft a message for it. My initial guess is that it will be a separate message type from ", " and ", ".", "On the topic of the parameter server, I think it\u2019s worth having a discussion about representation format. From talks with other OSRF folks, I don\u2019t think it\u2019s a good idea to use a tree of parameters; a single parameter would be better. For example, if you are loading the ImageNet class names, that\u2019s 1000 items on the parameter server, just to store the names. Add object meshes, sizes, etc., and it could balloon very quickly.", "While JSON/XML/YAML might work equally well in terms of expressive power, with XML, we can be sure that both C++ and Python will have the ability to read the database. TinyXML is already included as a low-level dependency in ROS C++, but the same can\u2019t be said for a YAML or JSON parser. Rather than allow people to use whatever\u2019s convenient, I think it\u2019s worth it to restrict/recommend everyone to use a format that can be parsed from more languages. We could do it in the REP, but not enforce it, so if someone really wants to use YAML in their Python-only implementation, they could do so. That\u2019s my position, but I\u2019m interested in hearing other ideas.", "I\u2019ve updated the repository with changes as discussed above \u2013 ", " and flattened the message hierarchy accordingly.", "On the topic of dense pixel segmentation, is there a reason that ", " is inadequate?", "In regard to pixel labeling, I\u2019ve also seen ", " used as a means to publish, along with some custom structure to define the mapping between pixel values and labels on a separate topic. It would be cool to also have a message type to publish a array of convex bounding polygon verticies with label IDs. That\u2019s\u200b a common use case when labeling regions of an image, and would be good compressed representation to transmit instead for classification modalities that utilise that format.", "For pixel-based segmentation, I imagine a message PixelSegmentation.msg like", "where the pixel value of each pixel in the mask corresponds to an index in the results-array.", "This looks like it would be a clean implementation. Just to be sure (since I\u2019m a segmentation newbie), the size of ", " would be the number of pixels in the ", "? There\u2019s a distribution for each pixel?", "First, let me start off by noting that although I only created Classification and Detection messages, I think it makes sense to keep this as a general vision_msgs package, and additional computer vision-related messages can be added as time goes on. I think it\u2019s more useful than making a too-specific classification_msgs or similar.", "I am not an expert in this field, but I would like to clarify whether the classification and detection messages are specific to 2D image processing, or if they can also be used for 3D point cloud processing or even 2D laser scan processing. If there is a possibility that they may be used outside of image processing, then perhaps a ", " or similar package actually is appropriate. Just something I think should be considered.", "A CategoricalDistribution for every ", ". E.g pixel [45, 89] has value 21. That means it\u2019s labeling can be found at", "That CategoricalDistribution e.g. determines that pixel is either a pear or banana with corresponding values in the distribution.", "Pixel [45, 90] also has value 21, referring to the same CategoricalDistribution, though pixel [56,94] has value 2 so refers to results[2], which says that pixel is most likely an apple etc.", "The max value of the image +1 corresponds to the length of the .results-array.", "Is it likely that many pixels in the image will have identical distributions? It seems that \u201capple\u201d pixels near the edge of the apple would have a different probability distribution than those near the center. All the ML-based segmentation systems I\u2019ve seen either predict a single output class for a pixel (such as a binary classifier), or they produce probability vector", "It seems like a bit of a halfway solution to define a small set of distributions that the image uses as an index, then transmit that set with every result. I feel that these two options would work based on use case:", "The image is segmented in some small finite set of output classes, which do not have probability distributions that vary in space/time: use an Image message where the lookup value of the pixel is the output class. If desired, static probability distributions for each class can be communicated in a one-time fashion, such as via a single CategoryDistribution[] message, or via the parameter server", "The output segmentation includes varying probability distributions that are calculated per-pixel or per-small region: use a CategoryDistribution of length the size of the image, where each pixel has its own unique distribution that may change every frame.", "Let me know if I missed something! If you have some code available for a use case, that\u2019s really helpful. I\u2019m currently in the process of writing example classifiers to use the Classification/Detection messages and finding it a useful exercise.", "3D point cloud processing generally falls under the topic of \u201ccomputer vision.\u201d But I had not considered laser scan processing, good point. The package name will probably be subject to review from more senior OSRF architects, and we\u2019ll keep that in mind!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Are there major use cases or edge cases not covered by this set of messages?", "Is this set of messages broad enough to encompass both handcrafted and machine learning-based approaches to computer vision?", "\n", ": as a result from ", "\n", "Poses e.g. ", "\n", "\n", "\n", "\n", "\n"], "url": "https://discourse.ros.org/t/proposal-new-computer-vision-message-standards/1819"},
{"title": "Accuracy of SLAM in outdoor environments using 2D versus 3D Lidar?", "thread_contents": ["Hello, I am working on a startup, and we are thinking through LIDAR sensors we might use.  Our platform will be used outdoors, especially in settings like orchards and berries on farms, where you have a lot of leaves, trunks, etc.", "A function of our platform will be the ability to use SLAM to build a map, which the platform can then retrace back through to a set point.  We\u2019ve been debating what sensors we could use to do this, and are considering both 2D and 3D lidars.  For 2D lidars, we are experimenting with the Sweep 360, and the SICK TIM 561.  For 3D lidars, we have been trying to get some early low cost units from Quanergy, which has a 120 degree horizontal/10 degree vertical FOV Lidar.", "Can anyone comment on the accuracy of SLAM in this type of environment using 3D Lidars versus a 2D lidar (likely mounted about 18 inches from the ground)?  In the real world, does the added data generated by 3D Lidars tend to make SLAM mapping more accurate, or no?", "You can solicit sensor placement and unit advice from the community of course, but it is clear that you are going to run into much more fundamental issues later, since you have to solicit this kind of advice from the community to begin with.", "I would look for a perception/SLAM engineer to join your startup.", "The discussion about 2D or 3D environment perception is usually more related to robustness rather than to accuracy. If your environment is highly 3D, meaning ramps, holes, steps, furniture, \u2026 then a 2D SLAM might fail due to a poor representation, not due to inaccuracies. Otherwise, if your environment is flat enough, and features have a strong vertical expansion (walls , posts) , then 2D could be enough robust and very accurate, and generally cheaper in both, dollars and computation.", "\nMy advice would be that think about your environment, and try to imagine if a 2D slice can represent it in a robust way to navigate. Then take a decision (and then accept that you may fail in your decision \u2026)", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/accuracy-of-slam-in-outdoor-environments-using-2d-versus-3d-lidar/1902"},
{"title": "Avoiding Small Obstacles on the Gruond", "thread_contents": ["Hello, I am Working on a Differential Driven Robot who has to navigate through a very unstructured environment. One of the Biggest changes faced now is to avoid obstacles with height less than 3cm(~2inches). These Obstacles are :", "1.Power Cables.", "\n2.Clothes.", "\n3.Small supportive Structures of Furniture.", "As none of the depth sensors we have tried upto date has no capability of measuring heights accurately around this height, my suggestion was to use an image processing based approach to determine the obstacles of this nature. what sort of a methodology would be best for a substantially accurate detection of such an obstacle?", "Hi ", ",", "According to ROS community support guidelines ", " this", "\nquestion should be better asked in ", "Please do not post questions on the ", " they should go", "\nto ROS Answers.", "ROS Discourse is for news and general interest discussions. ROS", "\nAnswers provides a forum which can", "\nbe filtered by tags to make sure the relevant people are included and", "\nnot overload everyone.", "I actually think this topic fits here. Is more like a general discussion not a things with just one answer.", "On the topic:", "From my experience the 3D sensors on the market (Asus Xtion, Orbbec Astra, Kinect) should be kind of accurate enough to go down to 0.3 cm.", "I would say the main problem is the calibration of the sensors / sensor pose and I ran into some cases where the ground is actually not flat / horizontal.", "On the other hand it is most likely easier to make the robot just be able to drive over such small obstacles. You will most likely want to be able to do that, for e.g. door sills.", "Sorry, I guess I was being overzealous\u2026", "Adding to the discussion:", "Indeed I can detect most objects that are 3 cm in height, using the usual", "\nsensors. And, as ", " stated, the biggest issue I had was properly", "\nfiltering the ground, as many sensors have a distortion in measurements and", "\nthat error grows with its distance from the ground. I would not recommend", "\nthe Orbbecs for this task as they suffer from a drift problem that would", "\ninvalidate the calibration after a couple of hours (", "\n", ").", "But if you want go to under 3 cm that would be a bit harder\u2026 Maybe", "\ninstalling the sensor very close to the ground and looking forward?", "\nOr if you want to go with image processing, it would be relatively easy to", "\ndetect objects using color segmentation, for example.", "From my experience the 3D sensors on the market (Asus Xtion, Orbbec Astra, Kinect) should be kind of accurate enough to go down to 0.3 cm.", "Small correction: I guess you meant 3 cm (as per the original question), not 0.3 cm, right?", "Hi ", ",", "\nI agree with ", " answer on the topic. We have made good experiences with structured light (e.g. Asus Xtion) and time-of-flight (e.g. Kinect v2) concerning the precision for mapping of rough terrain. Here\u2019s a video of one experiment ", " and our mapping software is available here: ", ", ", ", ", "Nope, 0.3 cm. I might stretch it a bit with that, but I did some tests with the Asus Xtion before where I got it down to detect reliably obstacles with around ~1 cm without trying too hard (", ", ", " or ", ", basically comes from ", ").", "\nBut this actually turned out to be infeasible because our sensor is mounted quite high and the uneven ground (at least in our office) tilts the robot so much that the sensor accuracy is higher than the variance we get from the ground :\u2019(", "With the Orbbec you probably have to look into the IR image calibration and see if you need additional calibration (and if you can figure out a way to do that, which is probably non-trivial).", "\nI heard that some of the Intel guys are working on doing obstacle segmentation from ground by vision / with their 3D sensor. Maybe contact them.", "Hi,", "\nI felt this as an open ended discussion which would be better for a discussion rather than a issue of a particular ROS system. Regards", "Just an idea: if you could try to establish a known colour or texture of the floor using computer vision then avoid anything that doesn\u2019t look like the floor. That\u2019s what I do when I walk cautiously.", "Probably the most robust floor detector for a \u201cvery unstructured environment\u201d would be a CNN trained to segment floor vs not-floor. If you have the budget, one way to get a training dataset is to create one using amazon turk to manually label a lot of images.", "Here\u2019s a couple slides showing existing deep learning segmentation approaches and software that I think look promising.", "\n", " ", "Here\u2019s a list of deep learning segmentation publications:", "\n", "\nAnother list of deep learning computer vision:", "\n", "awesome-deep-vision - A curated list of deep learning resources for computer vision", "\n", "Consider also using the 3D information together with an image stream for the \u201cnot floor\u201d detection. Filters out already a lot of hard to classify stuff if you only consider points on floor height.", "\nMight even be able to come up with something that does not use a machine learning black box. I would look into frequency analysis of images to differentiate between regular floor patterns and obstacles.", "Always depends what you are doing though, trying out machine learning for a research project is probably nice ", "This was one of the approaches I had in mind. Thank You for sharing. Should try this ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/avoiding-small-obstacles-on-the-gruond/1939"},
{"title": "How to project 3D data into pixel coordinate?", "thread_contents": ["Hello,", "The scenario is I have a RGB-D camera in robot, and by subscribing camera/depth_registered/points (sensor_msgs/PointCloud2) get the point cloud data. What I want is to project the PointCloud2 data into pixel coordinate. My question is that is there any module / library which is hardware independent (the camera could either be Kinect or RealSense or what ever RGB-D camera) could help to do this?", "Thanks", "You don\u2019t need a library for that, just use the ", "\nhere is a small python function:", "\ndef world_to_pixel(self, point):", "\nx = point[0]", "\ny = point[1]", "\nz = point[2]", "\npx = x", "self.K[1][1]/z + self.K[1][2]", "\nreturn np.asarray([px, py])", "K is the 3x3 camera matrix you get from camera_info", "\nself.K[0][0] = cam_info.K[0]", "\nself.K[0][2] = cam_info.K[2]", "\nself.K[1][1] = cam_info.K[4]", "\nself.K[1][2] = cam_info.K[5]", "\nself.K[2][2] = 1", "Even simpler: all the examples you gave of \u201cdepth_registered\u201d point clouds", "\nare ", ": point (u, v) corresponds to pixel (u, v) in the image.", "If you want the points in a different RGB frame and you have the depth", "\nimage, it\u2019s best to register the depth image into that RGB frame and then", "\ncalculate the point cloud (which will also be organized). Both can be done", "\nusing depth_image_proc.", "If you have an arbitrary point cloud, you\u2019ll have to use the pinhole camera", "\nmodel as suggested above.", "P.S.: The proper place to ask this kind of question is ROS answers.", "Also note that when using the pinhole camera model, multiple points can", "\ncorrespond to the same pixel, so you might have to do a z buffer check", "\n(only keep the closest pixel).", " ", " really appreciate your helps. thank you very much.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-project-3d-data-into-pixel-coordinate/2616"},
{"title": "3D reconstruction from several point clouds with camera pose", "thread_contents": ["Hello,", "I have some problem on 3D reconstruction.", "My situation is below.", "I collected The Scene Info that include Pointcloud  and estimated camera pose at that time I captured.", "I want to make map from several point clouds.", "So, I just merged point cloudes to one point cloud.", "My question is how to merge point clouds\u2026 well.", "Thanks.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/3d-reconstruction-from-several-point-clouds-with-camera-pose/3212"},
{"title": "New ROS-compatible 3D sensor based on stereo vision", "thread_contents": ["Nerian Vision Technologies has released SceneScan \u2013 a new ROS-compatible 3D sensor system that is based on stereo vision. Unlike conventional depth cameras, stereo vision is a passive technology, which also works robustly in bright daylight and over long distances.", "SceneScan is an embedded system that uses a powerful FPGA and state-of-the-art image processing algorithms to convert the imagery of a stereo cameras or two conventional cameras into a depth map or 3D point cloud. This can be done at frame rates of up to 100 fps and image resolutions up to 3 megapixels.", "Together with SceneScan, Nerian also released the new Karmin2 stereo camera. Both devices work optimally together and form a fully featured 3D depth sensor system.", "You can learn more about SceneScan at:", "\n", "More information about Karmin2 is available at:", "\n", "A ROS node for SceneScan has already been released and is available from the ROS package servers. Details can be found in the ROS wiki at:", "\n", "Really looks like a very nice product, the fact an FPGA is embedded to do the processing really is awesome!", "\nAs usual, I\u2019m always disappointed to never see a public price tag on these products.", "Your ROS package looks clean and you are providing clear/read-able documentation/examples. I\u2019m pretty sure many people will be interested in your sensor because the integration time in a ROS application is close to zero.", "Best of luck to you with this product!", "Yes, prices are available upon request only, but we do have a competitive pricing scheme. Prices do vary depending on the configuration. A full system consisting of SceneScan, cameras and optics is already available at well below EUR 3,000.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/new-ros-compatible-3d-sensor-based-on-stereo-vision/3251"},
{"title": "Repository consolidated", "thread_contents": ["Purpose is to keep the maintenance effort even lower. For the detail of the decision, please see the referenced ticket for each repo.", "Each old repo is preserved for backward compatibility.", "Admins, please feel free to move this thread to more appropriate category if any (I bet there is but couldn\u2019t think of it).", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["\n", " is no longer hosted at ", ". It is now merged into ", "\n", "\n", "\n", " is no longer hosted at ", ". It is now merged into ", "\n", "\n"], "url": "https://discourse.ros.org/t/repository-consolidated/3695"},
{"title": "Openpose Implementation for Mobile / Embedded", "thread_contents": ["Implementation of Openpose for realtime : Human Pose Estimation", "tf-openpose - Openpose from CMU implemented using Tensorflow with Custom Architecture for fast inference.", "This will run on your laptop or embedded devices (even without gpu).", "It seems to be a good technique to create various application examples.", "[Call for join on opensource project]", "I recently re-implemented CMU\u2019s openpose which is one of the best model to estimate human pose in realtime with powerful 'GPUs\u2019", "\nWith that reimplementation, I have made a couple of changes like tuning network architecture.", "Especially, by using mobilenet\u2019s \u2018Depthwise Separable Convolution\u2019, the model can be improved to run realtime even in an low-power embedded device or laptop without a GPU.", "But I think, there are much room for improvement in speed and accuracy. So I\u2019m finding crews to help this opensource project to research further. Anyone can join via online.", "Anyway you might recommend to extract skeleton points as 3D point coordinates?", "Sorry for the late reply. Actually, I\u2019m working on it although It will take some time\u2026", "I added ROS support on this project as well as other features.", "See ", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/openpose-implementation-for-mobile-embedded/3031"},
{"title": "markerless positional tracking?", "thread_contents": ["Need to find out the position of a stereo camera in the real world relative to a starting reference position via a SLAM algorithm or something else.", "My strereo camera is an inexpensive 120fps @ 480p per side or 60fps @ 60 fps per side.", "Need to run it on a singleboard computer such as Raspberry PI (Odroid and Tinkerboard seem like good candidates) so need to run ROS on Linux to run on these singleboard computers with no PC requirements.", "All I need is to access the position values of the camera each frame.", "Can ROS do this? I found a stereo camera SLAM package for it but not sure if it can provide position data and whether there are more suitable nodes. ", "Thank you.", "PS. If anyone is curious It\u2019s a ELP stereo USB HD camera module which cost around 70USD and can be bought directly from the manufacturer from Alibaba or AliExpress.", "You can try ORB-SLAM ", ".  It has very good results and is faster than method based on point clouds.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/markerless-positional-tracking/3589"},
{"title": "Navigating a KUKA YouBot with Intel Realsense R200", "thread_contents": ["Hello", "I am trying to navigate a KUKA YouBot using a Intel Realsense R200-camera mounted at the front of the robot, connected to a Odroid XU4 (due to only having USB2.0 ports on the YouBot and the intel camera requiring USB 3.0).", "Navigation has to be quite precice, as i have a delivery system in place that places objects in the exact same place every time it gets a request, and i need the YouBot to be placed precicely (+/- 2-3 cm) under this delivery system so the object is placed on the same location on the back of the YouBot (so they can be picked up by the YouBot arm when doing the final delivery)", "Do anyone have suggestions on packages i can use to best solve this task? And are there anyone with experience using this kind of sensor that could provide some information about its accuracy?", "\nThe only alternatives i have considered this far is simply to build a map with rtabmap or gmapping and use the ROS navigation stack to navigate. There are alot of tutorials for this, so it should not be a problem, but i have no information about the navigation accuracy. The youbot is running ros indigo, but i have kinetic installed on the odroid aswell as a newer computer that i plan to use for navigation.", "I am new to linux based operating systems in general aswell as ROS. With a limited timeframe to solve this task i hope to depend on already-buildt navigation packages.", "All comments, help, tips and tricks are greatly appreciated!", "\nThank you!", "Hey ", ",", "there were some people already playing with them. Take a look here:", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/navigating-a-kuka-youbot-with-intel-realsense-r200/4210"},
{"title": "A kit for multiple RealSense D400 sensors in ROS", "thread_contents": ["I was really impressed by the new RealSense depth cameras but the field of view is still a bit narrow. So I made a kit for multiple RealSense in ROS (", "). It comes with two RealSense D415 but can easily be expanded up to 6 cameras. The field of view is essentially doubled to ~140\u00b0 x 42\u00b0. If all six cameras are used, the kit gives full cylindrical coverage. It\u2019s a good way to get started with immersive virtual reality.", "My goal here was to make this as close to plug-and-play as possible. There is just one simplified launch file for all X cameras, and another to display the point clouds in RViz. The tf frames are launched automatically, as well.", "One caveat is that >2 cameras requires additional USB3 busses for higher resolutions/fps.", "This is a shot of my kitchen with 2x D415:", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/a-kit-for-multiple-realsense-d400-sensors-in-ros/4411"},
{"title": "How to identify 3D shapes (Rectangle, Circle and Square) using ROS Laser Scan Data?", "thread_contents": ["What is the simplest way to identify objects using ROS laser scan data ranges? I know we can use Hough Transformations but I haven\u2019t seen an example on how to use these techniques with Laser Scan Data.", "Any help is appreciated\u2026", "Hey!", "You could decompose the point cloud data into primitive shapes using PCL. Check out these links related to PCL if it may be of any help.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/how-to-identify-3d-shapes-rectangle-circle-and-square-using-ros-laser-scan-data/4790"},
{"title": "Slam_karto", "thread_contents": ["Has anyone used lately the slam_karto package?", "\nWhat is the current state? How is it compared to gmapping?", "thx!", "You could find several similar questions at ", "[1][2], and also there is a paper on this topic [3], sample programs [4] and movies [5]", "[1] ", ",", "\n[2] ", "\n[3] ", "\n[4] ", ")[5] ", "Note that at this moment, you could ", " both packages[6][7] and Non-GNU licenses [8].", "[6] ", "\n[7] ", "\n[8] ", "Thanks. very useful!", "FYI, there\u2019s a fair bit of momentum these days behind ", ". Definitely have a look at that one, too.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/slam-karto/4862"},
{"title": "Swiss Ranger 4000", "thread_contents": ["Hi there,", "Would anyone out there have a copy of libmesasr that they could send me?  I\u2019m trying to get an old SwissRanger 4000 working with ROS for a ToF navigation project.  Unfortunately, Mesa Imaging (the company who makes the sensor) was bought out years ago by Heptagon, a member of the ASM group.  Their website documents the SwissRanger line of products as discontinued, and has broken software download links to the old mesa-imaging website.  I emailed ASM support asking if they could send me the Linux driver, but they responded that they no longer support the sensor and couldn\u2019t help me.  I\u2019m turning to the ROS community as my last hope.  If anyone out there has a libmesasr library file and complimentary header file, I would greatly appreciate if they could tarball it and send it to me at jeremy (dot) roy (at) live (dot) ca.", "Thanks,", "Jeremy", "The Wayback Machine appears to have indexed the Mesa Imaging site quite a few times over the years. See ", ".", "You can then navigate to ", " and download the Debian packages. The links worked for me.", "Hi Gavanderhoorn,", "Thanks for the suggestion.  I was able to successfully download and install the Debian packages using the Wayback Machine.  After some testing I was able to communicate with the sensor via ethernet connection.  Now my next step is to catkinize one of the old rosmake drivers.", "Jeremy", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/swiss-ranger-4000/5189"},
{"title": "Discussion on detection of people and obstacles in a dense crowd", "thread_contents": ["Hi,", "I\u2019m not at the point of being able to ask a proper question yet. However, I am starting on a project that will hopefully work as a telepresence robot for a friend of mine who is housebound. This would, hopefully, allow her to attend science fiction conventions.", "The problem is that science fiction conventions are extremely crowded and I don\u2019t want the robot to be able to run into anybody or anything, at least no more than a human would in a similar situation.", "The robot will have a lot of sensors to detect objects at various levels, but I think that vision is the best way to detect people in this situation. And I\u2019m sure that it won\u2019t help that many of the people will be in costume.", "Does anybody have any ideas about how to approach this problem?", "The planner to determine the route of the robot would include the above data, in addition to the goals of the remote user and also keeping me within a certain distance. But right now I am only concerned about detecting the \u201ccrowd.\u201d Finding a way through the crowd is different problem. It is similar to the problem of a child in a similar circumstance with orders to keep their parents in sight and not to run into people.", "Yes, it would be nice if the robot were fully autonomous.  But actually I think a robot like this one would be teleoperated, that is moved with a joystick.    So there is a human driver using the vision system remotely.    Still there would be some lag and human drivers are not perfect.   I think it might be good enough that if the robot detected an imminent collision would simply apply the brakes hard and stop.  The human driver would then figure out what to do.", "I\u2019m using the common HC-SR04 as a last-resort collision sensor.  In theory my robot should never crash as there are sensors to prevent this.  But hc-sr04 is a backup.   Inside the base controller there is a loop that runs once for every twist message.  But the base controller looks at the distance reading from the ultrasonic sensors and decides if it \u201cwants to\u201d execute the twist message.   This decision is made completely outside of ROS.  The point is that we should only detect an obstacle if there is a failure in the ROS based system so the base controller, the process that actually commands the traction motors does the \u201cpinging\u201d itself and will refuse to power into a fixed object.", "You may or may not want to use this same design but if operating in a crown and the robot owner is not within arm\u2019s length with a hand ready to punch an \u201ce-stop\u201d bottom you need maybe TWO not one fail safe checks.   In other words if the ROS planner says \u201cgo\u201d it also must have an OK from TWO independent non-ROS based systems.  Perhaps the second one is a mechanical switch that detects physical contact between a bumper bar and the obstacle.    For your robot  I\u2019m thinking of a ring that encircles the robot and is held by springs and if a spring moves say 1/4 inch it means that your vision system, the humans driver and the ultra sonic sensors have all failed to detect the obstacle.  So this system that uses mechanical switches disconnects the motors from power using a mechanical relay (no software in the loop)", "Safety is hard.  The easy task is to design the machine to be safe when it is operating as designed.  The harder task that is 100% required to operate remotely in a crowd is that the machine remains safe even after unanticipated failure modes.    As an example I had an electrical fire last week in a prototype.  Lithium batteries have high power density and I ended up vaporizing a power cable.  My design was not fail safe, in that obviously there was a failure mode that could cause a fire.", "So your anti collision system must work even if there is a bug in the software and even if there is a mechanical fault.  Vision is CLEARLY to complex to be unconditionally safe but could be a very good primary system given enough  redundant backups.", "Back to my work, I\u2019d like to be able to use one camera but I\u2019m undecided.  getting 3D data from one camera requires very good motion estimation.  I don\u2019t think my IMU and odometry will be good enough so I may need stereo vision.   I think in your mixed indoor/outdoor use case, moving in a crowd you will need stereo vision to get usable depth.  Your obstacles are all moving and you will need to snap the pair of images simultaneously, not sequentially.", "Summary:  I think stereo vision is a great primary sensor.  But for remote operation you\u2019d better have multiple independent backups that are each so simple they can\u2019t fail and being truly independent the chances of both failing at the same time is the product of the probabilities, a tiny number.", "Telepresence seems easy at first, basically it is a remote control car with a webcam glued to the top but the problem is that telepresence by definition means the operator is not present.  I think that implies extreme reliability and safety.", "People tracking for people who don\u2019t look like people all the time, interesting!. In a crowded environment no less.", "You might look into the ", " (who had a robot driving around on Amsterdam Schiphol Airport: ", ")", "Following people in a less crowded environment is a task in RoboCup@Home. At TechUnited, we use vision to detect a person and then a laser scanner at torso-height to track the operator.", "Thank you very much, Loy.", "And I forgot to mention: while most of the people in the crowd are adults, there are many children and probably a few R2D2 robots. This is going to be an interesting project. I suppose I will start out with collisions in my motorhome where there is just me to damage. Then I will add in other factors by inviting other people in and putting random boxes down to simulate a changing environment.", "Then I might bring Groucho outside and see how he handles there, with me close by with a remote cut-off switch. Unfortunately, I have to carry him in and out of the motorhome. This puts an upper limit on his size and weight, though I will make him able to be split into multiple pieces if possible.", "Jay", "I agree about the need for safety.", "In some of my previous robots, I had a \u201creflex system\u201d that handled some failure cases. This was a simple processor that took in input from certain sensors and stopped the robot and then sent a message to the planner about why it had stopped. Until the planner sent the reflex processor a \u201cchill out\u201d message, the robot didn\u2019t move.", "The reflex processor killed the motors with relays so that nothing could turn them on until the reflex processor was satisfied.", "I used touch-sensors and floor sensors to make sure the robot wasn\u2019t heading over some stairs.", "These were very simple robots compared to Groucho and they didn\u2019t use ROS.", "I usually design my robots\u2019 brains with multiple functions/subsystems. The reflex processor is the one that is guaranteed to be a separate processor that is very simple.", "I will use multiple segments to my touch-ring. This will give me a better idea of where the robot was touched the object. I may even have certain touch-sensors control the motor relays themselves after a given amount of pressure.", "I have two Intel RealSense cameras that I plan to use for Groucho. I also have six webcams coming so I\u2019ll be able to handle using stereo vision if need be. I am still designing Groucho\u2019s head. I want it to look metallic while also looking like a caricature of Grouch Marx. Plus I will take some ideas from ", ". This is the most expressive head I\u2019ve seen while still looking like a robot. I have 12 Dynamixel AX-12 servos that can be used for just the head if need be.", "There will be at least one camera in the head for additional vision at a taller height. There will be at least 3 webcams in Groucho, in addition to the two RealSense cameras.", "I\u2019ll be using an Intel NUC (latest generation) for Groucho\u2019s main brain. I may preprocess some of the cameras with another computer if that is needed. Or perhaps use a second NUC for vision processing. At this point, I haven\u2019t done much vision processing so I will have to do more before I can say anything. The base will have enough room for several processors.", "DangerousThink, AKA DT, Jay", "The Jack Robbot program at Stanford is using ROS to understand the robot SLAM in social situations.  For example, A person would never walk between to other people having a conversation.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/discussion-on-detection-of-people-and-obstacles-in-a-dense-crowd/2109"},
{"title": "Your thoughts on Intel Realsense D415/D435", "thread_contents": ["Hi!", "I\u2019m wondering about investing in one Intel Realsense camera. Does anyone have an opinion on how good/bad they are? From what I\u2019ve seen on youtube the sensors seemed quite noisy, on the other hand it looks like they might be good enough for basic tasks and the price is super attractive.", "Would appreciate any insights you might have!", "\nMat", "They are a little bit hard to install. The installation instructions aren\u2019t great and you have to get one package from source. I find the point cloud to be noisy at a distance but very good at shorter range (like, less than 2 meters).", "The ROS wrapper has a lot of functionality. I think it may be the best ROS camera package I\u2019ve used. You can pause any camera at any time, which is cool if bandwidth is a concern.", "The mechanical aspects of the camera are pretty good (robust cable connections, small package size, multiple mounting options). Personally I don\u2019t think there\u2019s a better option at the moment.", "Thanks ", "! Your points are very helpful! Do you think it\u2019s worth paying ~30$ more for D435 to get a global shutter and a slightly larger field of view compared to D415?", "I\u2019m sure I\u2019ll manage incomplete instructions (and maybe even create some docs while setting everything up).", "To me, the wider field of view is definitely worth it. I don\u2019t care about the global shutter, though.", "I wasn\u2019t thrilled with the accuracy of the D435. Emailed Intel and they said the 415 was more accurate (they replied right away which was a nice sign). The 415 still needs filtering but is adequate for me, the 435 was off by >2cm at distances less than 2M and very noisy. Their Windows app has a bunch of filters that were not implemented in ROS last I checked so I just wrote my own.", "\nThere was also an issue where sometimes you have to reset the device. Either soft reset or unplug and plug back in. This was an open issue on their github a while ago, not sure if its still there.", "\nOtherwise I\u2019ve been happy with it.", "Thanks for your advice ", ", ", ", In the end I\u2019ll go with 435 and will create some filters if needed to handle the noise.  Can\u2019t wait to start creating something!", "As far as I know the issue that requires a reset is still there. I ran across a fork of the realsense2-camera package that actually detected this and did the soft reset automatically, but it\u2019s still annoying that you don\u2019t get frames for a moment. Apparently this is a firmware issue that is on their to-do list for some time already.", "Some colleagues and me have mainly been using the D415 camera for a while now. Mainly the slow delivery of the D435 made us start with the D415 instead, but we\u2019ve now received a D435 as well. I haven\u2019t tried the D435 yet, so I can\u2019t really say much about how they compare.", "I\u2019ve been playing around with a D415 in a mobile robot where I use it together with ", " to generate laser scans for amcl and some other ideas for localisation. I was positively surprised by the accuracy of the depth image, where sometimes even up to 15 meters it gives reasonable measurements (less than 30 cm error). However I have only played around, no extensive checks and no tests in poor lighting conditions etc.", " thanks for your insights! Would you mind sharing what kind of computer did you use on the mobile robot? I\u2019m wondering whether anything like Raspberry Pi 3 B+ can handle working with those cameras or should I rather look into Up Board or even Jetson TX2. I know it will mostly depend on what I do with the data but having some reference would be really useful.", "I\u2019m using my laptop currently, it\u2019s a Lenovo ThinkPad T540p with an i7. I have no experience using it with a raspberry pi, sorry about that.", "One thing I forgot to mention that I don\u2019t use the pointcloud from the realsense2-camera, instead I take de depth image and use ", " to convert it into a pointcloud. This results in bigger pointclouds and somewhat higher cpu loads.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/your-thoughts-on-intel-realsense-d415-d435/6467"},
{"title": "Pointcloud / PCL or other libraries", "thread_contents": ["I am curious about the state of PCL and if anyone is using PCL or a similar framework.", "We have developed our own CUDA code for point cloud processing on the ZED but I am looking at using a framework instead of rolling our own going forward, since we want to support 3D LIDAR.", "Any insight would be great.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/pointcloud-pcl-or-other-libraries/6507"},
{"title": "Building a Tensorflow Object Detection and Localization ROS Package", "thread_contents": ["Hey guys, longtime lurker, first time, you know.", "I\u2019ve been working on a project that involves custom building object D&L models and using them to find things in high-res streaming video. I have the model trainer pipeline done, and am building out the actual detection code using ROS (close to done). I\u2019d like to turn this into a package for the community once it\u2019s done, but one thing I\u2019m not clear on is dependency management. Tensorflow is in rapid development, and some of its dependencies may break other dependencies (numpy, etc.). Also, it\u2019s not a simple apt-get to install Tensorflow if you don\u2019t have CuDNN etc. installed first. Does anyone have any recommendations on how to manage these dependencies for a package?", "Cannot waiting for the release. Since you mentioned it also does localization, may I ask how can you get the localization information (x, y and z) from a streaming video? Does it contain point cloud or something?", "I\u2019m doing it on 2d images from a camera, with a depth sensor (tbd). No point clouds.", "so it would be a RGB-D sensor and because most of the well known RGB-D sensors\u2019 ROS wrapper provides registered point cloud data which also contains rgb field, I think you can also use point cloud data as the input.", "Any url to the code, or to a repository? I think it is an amazing project.", "Best,", "\nPaco", "Hi Drew! Glad to hear someone is working on this, it\u2019s something that I\u2019ve felt ROS has needed for a while (and some partial implementations are out there).", "Most ROS pipelines that I\u2019ve seen that depend on TensorFlow, CuDNN, etc. being installed simply provide full instructions for installing the dependencies in their repository. Unfortunately this makes it hard to install via apt-get, and it\u2019s more complicated than cloning a git repository into your catkin workspace, but as long as the installation information is kept up to date, it\u2019s not too bad to copy-paste commands into a terminal.", "I\u2019ve gotten tensorflow and caffe interfaced with ROS before, and I had to install everything manually. I found the tutorials on PyImageSearch to be quite helpful, maybe you could reference those or make your own version that\u2019s more appropriate for your dependencies.", "Also, I believe that tensorflow can be installed via ", " even if you don\u2019t have CUDA installed, it will just install the CPU version of TensorFlow. So you could ask users to install tensorflow via pip, and provide links on how to install the dependencies if your users want more speed.", "Also, it would be great to get people to start standarizing vision pipelines so they are interoperable. May I suggest that you make your detector output messages from the vision_msgs package?", "Paco - I\u2019ll clear some time out next week to clean up the code and put it out on github. I\u2019ll try to get it into a downloadable package for ROS too, if I can squeeze it in. I like to think there aren\u2019t that many dependencies and it should work fine, but I know better than to blindly believe that ", "Kukanani - Pip can install tensorflow with or without GPU (assuming CuDNN, etc.). If you\u2019re doing other data sciencey stuff it can cause versioning / dependency issues by forcing some version upgrades (at least it\u2019s happened to me in the past, not sure how much of that still remains). For a lot of applications, I couldn\u2019t imagine not using GPU for Tensorflow. But then there\u2019s things like mobilenet and the Raspberry Pi, so it could be the case. Right now I\u2019m favoring throwing an error about Tensorflow not being installed and point out the options, than having Tensorflow-CPU or -GPU be a dependency.", "Thanks for recommending vision-msgs - I\u2019ll take a look. The current version of my code only returns pictures with bounding boxes, but returning probabilities / classifications / etc will be easy.", "Ok, I cleaned up the code enough to put an alpha release out on Github. If you\u2019re familiar with making your own object detection models in tensorflow, this should get you the rest of the way.", "ros_object_detector - This is a tensorflow-based object detection and localization package for ROS", "I have a todo list in the readme, and I suggst you look at it first just to get a sense for what\u2019s going on. I\u2019ll be around to answer any questions. Code improvements are always welcome.", "Note that I couldn\u2019t load my demo model, because github has a 100mb limit, so the code will fail when executed because there\u2019s no valid .pb model in there (just a placeholder file). For researchers, I\u2019ll try to get a working, sub-100mb model in there soon.", "Once I get it a little cleaner I\u2019ll make a new announcement of its availability to the community at large.", "Hey Drew, it\u2019s nice to read that some people care about doing things \u201cthe right way \u2122\u201d ", "Have a look here: ", "Hi Drew,", "I am also working in this topic but I did not use Tensorflow. I implement object segmentation and localization from scratch with Eigen and boost.", "\nDemo video: ", "\nIf you are interested, you can check out this project on GSoC:", "\n", "Cheers,", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/building-a-tensorflow-object-detection-and-localization-ros-package/3103"},
{"title": "Low Cost ROS supported Cameras", "thread_contents": ["Hi Guys,", "I wish to know what are the low cost cameras which supports ROS available in the market?", "I have found few of them which starts from 299$ here:", "\nflea 3", "\nflir blackfly", "\nflir chamelion", "And some High End Cameras also:", "\nFlir Ladybug 3,5,5+(All surrounding Capture)", "\nbumble bee (Stereo Camera)", "\nTara - USB 3.0 (Stereo Camera)", "\nNerian Karmin2 (3D Camera)", "\nNerian SceneScan (3D Camera)", "But I want suggestions which are even cheaper than this, Can you suggest few?", "Thanks in advance. Cheers!", " - You can find the list of stereo cameras which supports ROS. What is your use case?", "We used Intel D415 Stereo 3D camera, it works very good.", "\nYou can use this in Open as well as indoor environment.", "\nIt is supported with ROS and will be available at lesser price of $150.", "May be you consider Raspberry Pi-based stereoscopic camera StereoPi. I\u2019m a developer, and we discussing ROS support with Rohan Agrawal (", "), ", " author. You can see current discussion here on GitHub: ", "Here is how current revision looks like:", "\n", "To say briefly first tests are Ok, next step is to add stereoscopic driver.", "\n", "Yes a raspberry pi 3B+ or the Raspberry Pi-based stereoscopic camera StereoPi are easily going to be your most widely available, easy to set up, easy to use options.", "The raspberry pi camera is an excellent camera with 8MP, excellent dynamic range as well as a number of supported high speed modes that allow for very high frame rates.", "You can download a raspberry pi distribution at ", " which has all the camera drivers and ROS (kinetic) nodes all set up. All you have to do is log in to the pi and run the raspicam node that is already there. Obviously being a raspberry pi this topic can be published over WiFi or Ethernet and basically every kind of interface you could possibly want is there.", "The Raspi cam node is open source with a permissive license and is being continuously improved. For example, in addition to the high speed and high res modes already supported, GPU based object tracking will soon be released should that be something you need.", "Please download it - try it - and if you want a new feature - code it and issue it as a pull request!", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/low-cost-ros-supported-cameras/4680"},
{"title": "Gazebo Checkerboard Generator", "thread_contents": ["I\u2019d like to share a script to generate a Gazebo checkerboard model automatically. It can generate checkerboards of any size and with any number of squares. There are more details in the file header.", "The script is simple and straightforward, but I thought it could save some time if anyone is dealing with checkerboards in Gazebo, because of the amount of boilerplate involved with setting up a new model. It could also be used as a jumping-off point for an AR tag generator.", "I wrote the script this past summer, when I worked at Diligent Robotics in Austin, TX writing ROS code for medical assistance robots (they are also ", "!). Diligent is committed to having good simulation environments for testing, and this script was part of that effort.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/gazebo-checkerboard-generator/7061"},
{"title": "find_moving_objects", "thread_contents": ["Hey guys,", "I am fairly new to ROS but have written a package called find_moving_objects. It does what its name suggests, from either a laser scan or point cloud 2 data stream. The wiki page is found here (sources are linked to from the wiki):", "\n", "There is ", ", ", ", of objects. I have tried to write the package with performance computing in mind. The biggest bottleneck is mapping the points of the clouds onto a 2D plane, like in the laser scan case, which is used internally for the detection algorithm. Any suggestions on how to improve this are welcome!", "Two message types are defined which are used to provide information about the detected objects\u2019 positions and velocities in the", "Visualization messages are also published so that RViz can be used for debugging etc.", "I hope that this can be helpful to someone!", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["sensor\u2019s frame, as well as a", "map,", "fixed (typically odom) and", "base (typically base_link) frame."], "url": "https://discourse.ros.org/t/find-moving-objects/7178"},
{"title": "Node/nodelet to merge depth images from two cameras into one", "thread_contents": ["Hi everyone,", "during my work I\u2019ve developed a node/nodelet that merges the depth images of two depth cameras (RGBD, TOF, whatever generates depth images) into a single depth image as it would be seen from an (almost) arbitrary perspective. It basically transforms and combines the point clouds from two depth cameras of a known transformation to each other into one, and uses a pinhole camera model to create the depth image from a third given perspective.", "Here\u2019s the repository on GitHub: ", "Feel free to comment, criticize and contribute! But be aware that this has evolved from the mere need of enhancing the field of view of a depth cam by adding a second one. It\u2019s not optimized in any way. Hence, I\u2019ll certainly be grateful for any contribution especially regarding performance optimization!", "Cheers!", "Hendrik", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/node-nodelet-to-merge-depth-images-from-two-cameras-into-one/7566"},
{"title": "Perception SiG Maintainers Still Maintaining?", "thread_contents": ["This SiG seems like it hasn\u2019t seen any real conversation for a while and the GitHub group definitely needs some love (almost 100 pull requests are dead or un-reviewed across multiple repos). Is anyone still out there? Do we need to recruit some new maintainers?", "No offense intended to any maintainers out there - everyone gets busy and priorities change. However, this is a pretty important group of repos for the entire community and I want to make sure they\u2019re taken care of regularly.", "We\u2019ve started work on some of the ROS2 ports, so there is some activity on that side, but I haven\u2019t taken on any sort of maintainership for the ROS1 bits yet.", "Are you the only maintainer in the group? If so, I can offer my help with maintenance.", " what is the perception sig project? I could not find such project in github. would you please help to share the project link? thanks.", "we are contributing on ros2 perception porting, and tracking the status in the thread here: ", " - I\u2019m specifically talking about the ROS Perception Special Interest Group (formerly at ", "). The focus of this group appears to have been developing/maintaining the drivers/nodes at ", ". However, as I said, there are many repos there in need of attention.", "I have some deep familiarity with slam/open karto & gmapping as well as SBA that I would be more than happy to jump on and help maintain if help are needed in those areas.", ", ", ", ", ", and Ken Torris appear to be the previous or current maintainers of the packages that need issues and PRs reviewed. Not trying to shame anyone, just want to get some visibility and, hopefully, responses from the current maintainers to see what can be done. The focus of this tread is on the maintenance of packages like ", " and ", " which are used by many, many individuals and organizations today, though I am having a specific problem with ", " on 18.04.", " - Is there any way to get this run up the flagpole and get some visibility to have new maintainers handle these packages?", "About image_pipeline, we have already taken resources on porting it for ROS2. We also see there are 74 issues and 29 PRs have no updates for a long time. Don\u2019t know if currently maintainers and OSRF have any plans for maintaining? If you could approve, we would like to be the new maintainer of this package.", " / ", " / ", " - Can anyone help? There are bugs in image_pipeline that are currently breaking important nodes like image_view for Melodic (see ", ") and I can\u2019t get any response from ", " on this issue.", "image_view have quite severe problem in Melodic.", "\nimage_view does not work with this PR but still waiting for review.", "\nI need maintainers to solve this problem.", "\n", "\n", "I tried contacting Vincent Rabaud a couple of days ago at every email I could find for him (the listed maintainer for image_pipeline) and got no response. The current people listed as members of ROS Perception are:", "I\u2019m hoping one of these people have access to help us out by assigning myself or ", " as a maintainer.", "I\u2019m only a maintainer on a small subset of ros-perception. I\u2019ll be the first to admit I don\u2019t spend as much time on maintenance as I\u2019d like, but I try to handle any critical/severe issues quickly.", "I think you had the right idea ", " that someone from Open Robotics needs to make a call and designate a package orphaned and transfer maintainership either to an interested party or to the ", " taskforce.", "The timeline for contacting a maintainer directly is probably more than a few days though.", "Yeah definitely more than a few days is necessary. People may be travelling, on vacation or otherwise out of communication for a few days.", "I\u2019d suggest following the basic outline of what Dirk suggested ", ".", "If you are interested in taking over maintenance of a package please", " - Thanks for the feedback. I agree that giving it more than a few days makes sense. However, the PR fixing the particular bug that caused me to bring this up has been open since June and is causing image_viewer on Melodic to be unusable. I attempted to contact anyone in the group via their Google Group (see link above), via this topic, and now via email. I\u2019ll give it a few more days and then create an issue on the repo. I think the \u201cunable to reach the maintainer\u201d step has passed.", "If you\u2019re unable to reach the maintainer and they\u2019re not actively maintaining please post an issue on the repository as a proposal to take over. This post should explain why you think you\u2019re an appropriate candidate for maintaining the package so that the community knows who you are. This is also a place for public feedback and others to potentially see that there\u2019s an issue and step up as well.", "I would add to that, that it is worth linking to your issue from Discourse. Not everyone who is potentially interested will be watching the issue tracker, so I think that you would catch more interested people by also announcing on Discourse. Discussion should still occur on the issue, though. So perhaps close your thread as soon as you make the announcement to prevent splitting the discussion?", "Regardless of current maintainership status\u2019, I\u2019ll throw it out there that I\u2019m open and willing to help maintain anything SLAM/PCL related in ros-perception organization.  At this point I\u2019ve read just about everything at one time or another and reimplemented sections of them. Always looking for concrete ways to continue to help support the community.", "\n~Steve Macenski  / Simbe Robotics", " and I have taken over maintainership for the image_pipeline repository. Thanks to all involved for helping resolve this.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Isaac I.Y. Saito ", "\n", "Chad Rockey ", "\n", "Dave Hershberger ", "\n", "Jack O\u2019Quin", "Julius Kammerl", "Michael Ferguson ", "\n", "Paul Bovbel ", "\n", "William Woodall ", "\n", "Kentaro Wada ", "\n", "If you\u2019re unable to reach the maintainer and they\u2019re not actively maintaining please post an issue on the repository as a proposal to take over. This post should explain why you think you\u2019re an appropriate candidate for maintaining the package so that the community knows who you are. This is also a place for public feedback and others to potentially see that there\u2019s an issue and step up as well.", "If there\u2019s a consensus from that thread in the tracker and the issue can be resolved locally that\u2019s great.", "If no-one on that thread has access to the repository and the consensus is for a new maintainer to take over please start a thread in the ", " requesting for facilitation of the adoption."], "url": "https://discourse.ros.org/t/perception-sig-maintainers-still-maintaining/7065"},
{"title": "Realsense T265 - hands on review", "thread_contents": ["Hi!", "I\u2019ve made a short blog post describing my experience with ", " tracking camera used with a wheeled mobile robot that I think some of you could find useful: ", "Tl;dr summary of the post: I really like the idea behind this sensor but I found that it was quite difficult to get a \u2018proper\u2019 REP-105 compliant setup with it. In my opinion at this stage it can be a decent unit to have for R&D but I\u2019d say it\u2019s not there yet for commercial applications.", "If you have any tips on using T265 or notice that I do anything wrong in my setup then I would really appreciate your feedback!", "Nice article.", "I was really impressed  with the T265. Having a commercially off the shelf device that does visual odometrytells something about how much VSLAM is improving.", "It worked pretty well in my experiments, but then I had to ask myself: if I have wheel odometry and an IMU, do I need the T265 ?", "I come to the conclusion that the T265 is an amazing device that is not really useful in many practical cases.", "The fact that it is \u201cjust\u201d Visual odometry and I can not reuse maps, makes it less attractive than it could be.", "But I think it is great for non-wheeled robots like drones ans hand-held devices.", "Thank you for this. The coordinate convention makes my head spin, hopefully this will help.", "While impressive, so far I see the t265 as better suited for integration with an external vio/vslam system, which is kind of ironic. It is really amazing how well it works out of the box but I can\u2019t quite figure out a good way to translate that into a production asset.", "For me the greatest promise of T265 was the plug\u2019n\u2019play aspect. I was hoping that I will plug it in to my robot and I will receive a good feedback from it that would allow me to detect wheel slip, especially on uneven/slippery terrains.", "I\u2019m really looking forward to see what happens to it in the future and I really hope Intel doesn\u2019t abandon it.", "I dont get your problem with the TFs. For me the T265 is publishing TFs in compliance with REP 105", "Interesting! Did you set the publish_odom_tf parameter to true in the driver and managed to get a solid tree? Would you be able to share you settings, tf tree etc? I spent close to a week just on this and I\u2019d love to learn how to set it up properly!", "i have just used default settings ", "Yes, this looks good for a system where you use just the camera. How would you integrate it on your robot when it comes to tf structure?", "Where would you connect the camera to base_link? Because the only way I can think of that won\u2019t violate the REP-105 is if you have a camera_link -> base_link transform, which is quite backwards from what I\u2019m used to.", "if you want your standard odom --> base_link --> camera_link you have to handle TF by yourself or use robot_localization package.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/realsense-t265-hands-on-review/11023"},
{"title": "VINS Fusion on a 435i", "thread_contents": ["Some time ago I went down a deep VIO rabbit hole which led me to try out a bunch of different available packages on different hardware. The combo that I found worked best for me was the Realsense d435i along with VINS fusion and I made a video that shows 3D mapping of my apartment complex using only that camera ", " (the map data is integrated using Octomap)", "Are there folks here that use VIO or VISLAM algorithms on their robot? What packages do you use and with what hardware?", "Thank you for sharing!", "\nWe are adventuring into the same rabbit hole very soon ", "Great work. Can you describe the steps you followed to make this work.", "\nI am struggling with the correct setup, having tried different sensors. Can you point me to the right direction or if possible repository (git page) to share your work.", "Thanks", "Most of the hurdles were around getting a good calibration for the camera. Also vins fusion tends to be kind of unstable when using the IMU if your calibration is off (kind of ironic I think) so try running things in stereo only mode first and see if that works (there will be no notion of \u201cup\u201d but it should run quite well otherwise).", "I\u2019ll make a gist with my calibration and launch files later today.", "Hi,", "Thanks for the reply and I really appreciate your kind help in sharing the files. One thing, doesn\u2019t the d435i comes factory calibrated? Is it absolutely necessary to run the calibration prior to testing.", "Thanks", "Alex", "Hi,", "\nAny update on this.", "Thanks", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/vins-fusion-on-a-435i/10937"},
{"title": "Visual Odometry / Helper from RaspberryPI's motion vectors output", "thread_contents": ["Hello,", "The Raspberry Pi has a hardware feature that gets the motion vectors from camera images, and it is done in hardware decoder level. The feature is summarized here: ", "There is also a ", " that will get both the image and the motion vectors as a MotionVectors.msg, which looks like: ", "I have tested it, on a robot, with a front looking camera, and it does work, it is not as good as the other setups I have seen like on nvidia jetsons, (i.e. has more mis-vectors) but I believe it is usable to a level.", "Could we get any usable results from this feature, visual odometry, or such, even if it is a limited one? For example, while using the robot_localization package with imu and odometry as input, could we fuse data from the motion vectors?", "Or could it be used for detecting things like when the robot is stuck and wheels are slipping?", "Best regards,", "\nC.A.", "What exactly do you mean by \u201cmotion vector\u201d here? There are a lot of ways to describe motion but I think what\u2019s going on here is that you are getting vectors of objects relative to screen pixel space (i.e. how much something moves on the screen). A lot of the mpeg encoders calculate some really basic heuristics that allow them to swap out compression techniques based on the underlying activity in a video and I think this is what they mean by \u201cmotion vectors.\u201d From what I read into the docs this is probably a really really rudimentary calculation.", "I don\u2019t think this is going to particularly useful for odometry as you\u2019re going to get a lot of false positives and a really noisy signal. I think you might be able to use this data for some really primitive signals (like is the robot moving or rotating) but I don\u2019t think it would be suitable for visual SLAM even with an IMU. You might also be able to pull out some really basic brightness and color information from the mpeg encoder which you could use for something like line following or ball tracking, but that\u2019s a bit of a conjecture.", "Cool find though! That\u2019s a really useful set of tools.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/visual-odometry-helper-from-raspberrypis-motion-vectors-output/11975"},
{"title": "Low light depth camera in high dust environment", "thread_contents": ["Hello all!", "I am researching what kinda of depth camera to use for my project but I am having a hard time finding some answers. I am building something that will need depth information in low light (and sunlight) but also exists in a high dust environment (this will be in a desert with fine particulate dust). I realize that every once in a while I would need to clean off the sensors, so I thought about putting the depth camera (whichever I end up with) behind some sort of acrylic or glass that would allow for easy wipe down periodically.", "Does anyone have any ideas or experience here? I am new to the various technologies that are available so perhaps this topic exists with answers, but not sure what do search for.", "Thanks for all your help!", "\nRyan", "I have no idea at all, but note that the Mars rovers with solar panels, chose to not automatically wipe the solar panels clean from the dust, because such a wiping action will statically charge the glass/plastic, and attract even more dust.  Plus, trying to wipe sand off, is basically the same thing as rubbing sandpaper on it.", "Underwater applications are pretty similar to this problem since water can have high particle count (turbidity). For these kind of environments a single point light can generate a lot of backscatter from the outgoing light reflecting off suspended particles. Some ideas for how to reduce this backscatter are:", "For underwater applications you can use sonar, the terrestrial equivalents would be ultrasound or radar. I\u2019m not sure ultrasound would be a good choice in the desert since wind can make the microphone signal pretty noisy but maybe radar maybe is an option. There are some commercial off the shelf radar units that wouldn\u2019t require wiping.", "Powered by ", ", best viewed with JavaScript enabled"], "thread_details": ["Place active illumination far away from the camera", "Use a laser fan for active illumination instead of a single point light", "Use a modulated light source / receiver, backscatter show up as white noise instead of a signal"], "url": "https://discourse.ros.org/t/low-light-depth-camera-in-high-dust-environment/12140"},
{"title": "Cameras with 360\u00ba FoV in ROS", "thread_contents": ["Hi,", "I noticed that more and more consmer-grade cameras with 360 FOV are available (Samnsung Gear 360, Nikon KeyMission, kodak pixpro 360, etc.)", "Wonder if anyone successfully managed to do live streaming from these cameras in Ubuntu and to use them in robotic application.", "Cheers", "Davide", "Hi,", "Disclaimer : self-promotion ^^.", "A few years back I was playing with a 360\u00ba imaging using two uEye camera UI-3240CP \u2013 IDS-Imaging each mounted with 185\u00ba FoV fish-eye lens.", "\nYou can find short videos ", ".", "\nSince then plenty of nice camera got to the market as you mentioned, unfortunately I didn\u2019t get to play with any of them.", "Cheers.", "I\u2019m not sure about those cameras, but Econ Systems makes a rig for the TX1 that supports 6 cameras to provide 360FOV.", "I can only talk about the Kodak PIXPRO SP360 (and 4K). They work as normal USB cameras. But connecting them via USB doesn\u2019t give them enough current to keep on forever (they drain more than they charge). Maybe fixable with some kind of powered USB cable?", "My little work can be found here: ", "I have a short experience with SP360 (firmware version 1.0.5). I think it is important to highlight the differences of SP360 and SP360 4K since AFAIK only the SP360 4K version appears as V4L2 device.", "The USB interface in SP360 serves only for storage access purposes. The only way I have found to stream the live image is setting the cam in WiFi mode and sniffing the MJPEG stream. Some instructions on that can be found ", ". Since this stream is actually meant for live preview on a mobile app, the quality and resolution is far from the cam is actually capable of. Although the cam has an HDMI output, it is only enabled in playback mode and does not provide live feed.", "The SP360 4K has a webcam functionality and enables live stream to HDMI (now the external capture cards are also an option).", "Hi,", "does anyone know if the ", " is compatible with the ROS this node or at least with Linux?", "I think ", " was being too modest. Not only did he get the Kodak 360 camera streaming, he wrote RViz plugins to blend and display a spherical image (requires 2 cameras, ~1 hemisphere each) and  use a VR headset.", "Spherical image:", "\n", "VR:", "\n", "Hi, we are using ", ".", "\nWe can use ", " package to obtain image via USB streaming.", "Hi!", "Is the RICOH Theta S working well with the UVC driver on Linux?", "\nWe bought a RICOH Theta V only to realize that it is using the UVC1.5 driver which is not supported. I can\u2019t seem to find any other confirmation besides your post about the compatibility of the \u201cS\u201d model, using UVC1.1, and it would be great if you can confirm that it is working fine before we buy another camera ", "\nThanks!!!", "Hi,", "We are using RICOH Theta S with libuvc_camera, using UVC1.1 (Motion Jpeg mode).  The frame is a  combined 1280x720 image at 14fps. I think you can use other package like gscam or usb_cam.", "I didn\u2019t know about RICOH Theta V, but do you mean you cannot make Theta V work with current driver and packages? I expected V model is upper compatible to Theta S\u2026 but checking the spec sheet now, it seems support only H264 streaming\u2026 (it means we need UVC1.5 or higher, which is not supported by libuvc ", ")", "Thank you for the confirmation! We will go ahead with the S model then.", "I will check periodically what the status of support for uvc1.5 is and post it here when I can make the V model work. I assumed compatibility as well, but they dropped support for 1.1, which is my bad, I should have checked before.", "I seriously doubt my boss will allow me to allocate time to develop the driver, but if it happens I will post the news here.", "It\u2019s wonderful if we can use UVC1.5 and H.264 encoding. I\u2019m waiting for good news!", "what do you mean with combined 1280x720 image, the equirectangular 360\u00ba panorama?", "Also, how does the integration work? Can I plug it to a computer and have the computer handle everything ( turning it On/Off, modes, data handling, etc)", "Has anyone had any success running the Theta S or V with UVC1.5?", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/cameras-with-360-fov-in-ros/1833"},
{"title": "About the Computer Vision / Perception category", "thread_contents": ["Discussion on object recognition, visual sensors, and other computer vision and perception concepts in ROS.", "Powered by ", ", best viewed with JavaScript enabled"], "url": "https://discourse.ros.org/t/about-the-computer-vision-perception-category/1814"}
]